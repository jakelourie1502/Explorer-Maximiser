append_mcts_svs:False
dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 50}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:1
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.Car_Driving_Env.RaceWorld'>
same_env_each_time:True
env_size:[7, 42]
observable_size:[7, 9]
game_modes:1
env_map:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
max_steps:150
actions_size:7
optimal_score:0.86
total_frames:255000
exp_gamma:0.975
atari_env:False
memory_size:30
reward_clipping:False
image_size:[48, 48]
deque_length:3
PRESET_CONFIG:7
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:episodic
rdn_beta:[0.3333333333333333, 2, 6]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
contrast_vector:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 294)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:False
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.003]
 [0.004]
 [0.003]
 [0.004]
 [0.004]
 [0.003]] [[12.707]
 [ 9.151]
 [12.518]
 [ 9.151]
 [12.329]
 [12.267]
 [ 9.151]] [[0.004]
 [0.003]
 [0.004]
 [0.003]
 [0.004]
 [0.004]
 [0.003]]
printing an ep nov before normalisation:  7.673895666528097
printing an ep nov before normalisation:  7.511293824819347
UNIT TEST: sample policy line 217 mcts : [0.  0.4 0.2 0.2 0.2 0.  0. ]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.012]] [[12.401]
 [ 7.415]
 [ 7.415]
 [ 7.415]
 [ 7.415]
 [ 7.415]
 [ 7.415]] [[0.68 ]
 [0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]]
printing an ep nov before normalisation:  12.117605209350586
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  7.675876127723598
Starting evaluation
printing an ep nov before normalisation:  12.584986686706543
siam score:  0.0003387320109389045
maxi score, test score, baseline:  -0.9544454545454546 0.0 0.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.9628629629629629 0.0 0.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.3545690793976634, 0.08884849573766805, 0.2217087875676657, 0.1419926124696671, 0.1419926124696671, 0.050888412357668694]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.38529746207811166, 0.09653980776110882, 0.15429133862450936, 0.15429133862450936, 0.15429133862450936, 0.05528871428725128]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.38529746207811166, 0.09653980776110882, 0.15429133862450936, 0.15429133862450936, 0.15429133862450936, 0.05528871428725128]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.38529746207811166, 0.09653980776110882, 0.15429133862450936, 0.15429133862450936, 0.15429133862450936, 0.05528871428725128]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.38529746207811166, 0.09653980776110882, 0.15429133862450936, 0.15429133862450936, 0.15429133862450936, 0.05528871428725128]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.38529746207811166, 0.09653980776110882, 0.15429133862450936, 0.15429133862450936, 0.15429133862450936, 0.05528871428725128]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.38529746207811166, 0.09653980776110882, 0.15429133862450936, 0.15429133862450936, 0.15429133862450936, 0.05528871428725128]
siam score:  -0.10480169756014196
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.38529746207811166, 0.09653980776110882, 0.15429133862450936, 0.15429133862450936, 0.15429133862450936, 0.05528871428725128]
deleting a thread, now have 2 threads
Frames:  1107 train batches done:  34 episodes:  33
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.38529746207811166, 0.09653980776110882, 0.15429133862450936, 0.15429133862450936, 0.15429133862450936, 0.05528871428725128]
actions average: 
K:  1  action  0 :  tensor([0.3003, 0.1584, 0.1445, 0.0729, 0.1236, 0.1150, 0.0852],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.1045, 0.6039, 0.0777, 0.0388, 0.0423, 0.0687, 0.0641],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1229, 0.1544, 0.3148, 0.0647, 0.0620, 0.2013, 0.0798],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2096, 0.1208, 0.1392, 0.0953, 0.1671, 0.1730, 0.0951],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1833, 0.0556, 0.0870, 0.0649, 0.4455, 0.0822, 0.0815],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0680, 0.0616, 0.1057, 0.0592, 0.0500, 0.6142, 0.0413],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1307, 0.2176, 0.1305, 0.0768, 0.0818, 0.1278, 0.2349],
       grad_fn=<DivBackward0>)
siam score:  -0.15276544
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.38529746207811166, 0.09653980776110882, 0.15429133862450936, 0.15429133862450936, 0.15429133862450936, 0.05528871428725128]
siam score:  -0.21316694
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.38529746207811166, 0.09653980776110882, 0.15429133862450936, 0.15429133862450936, 0.15429133862450936, 0.05528871428725128]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.38529746207811166, 0.09653980776110882, 0.15429133862450936, 0.15429133862450936, 0.15429133862450936, 0.05528871428725128]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
siam score:  -0.30905578
actions average: 
K:  1  action  0 :  tensor([0.3945, 0.0437, 0.1265, 0.0741, 0.1197, 0.1069, 0.1346],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.1013, 0.7169, 0.0403, 0.0331, 0.0403, 0.0330, 0.0351],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2454, 0.0506, 0.2875, 0.0792, 0.1035, 0.1134, 0.1204],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2600, 0.1155, 0.1536, 0.1107, 0.1304, 0.1364, 0.0934],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2814, 0.1221, 0.0628, 0.0250, 0.4315, 0.0283, 0.0489],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0738, 0.0348, 0.1188, 0.0463, 0.0757, 0.5942, 0.0563],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1926, 0.1705, 0.1053, 0.0749, 0.1302, 0.0983, 0.2282],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.38529746207811166, 0.09653980776110882, 0.15429133862450936, 0.15429133862450936, 0.15429133862450936, 0.05528871428725128]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.38529746207811166, 0.09653980776110882, 0.15429133862450936, 0.15429133862450936, 0.15429133862450936, 0.05528871428725128]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.38529746207811166, 0.09653980776110882, 0.15429133862450936, 0.15429133862450936, 0.15429133862450936, 0.05528871428725128]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.38529746207811166, 0.09653980776110882, 0.15429133862450936, 0.15429133862450936, 0.15429133862450936, 0.05528871428725128]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.38529746207811166, 0.09653980776110882, 0.15429133862450936, 0.15429133862450936, 0.15429133862450936, 0.05528871428725128]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.38529746207811166, 0.09653980776110882, 0.15429133862450936, 0.15429133862450936, 0.15429133862450936, 0.05528871428725128]
siam score:  -0.38451886
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.38529746207811166, 0.09653980776110882, 0.15429133862450936, 0.15429133862450936, 0.15429133862450936, 0.05528871428725128]
deleting a thread, now have 1 threads
Frames:  1107 train batches done:  99 episodes:  33
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
actions average: 
K:  4  action  0 :  tensor([0.2095, 0.0896, 0.1546, 0.1496, 0.1055, 0.2045, 0.0867],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.1189, 0.2891, 0.1157, 0.1079, 0.1412, 0.1239, 0.1034],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2858, 0.0908, 0.1175, 0.1199, 0.1471, 0.1547, 0.0842],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2120, 0.2627, 0.1095, 0.1482, 0.0656, 0.1432, 0.0589],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1552, 0.1821, 0.1488, 0.1082, 0.1455, 0.2028, 0.0575],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2094, 0.0542, 0.1155, 0.1399, 0.0968, 0.2959, 0.0882],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2888, 0.1224, 0.0913, 0.1121, 0.1217, 0.1239, 0.1399],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.38529746207811166, 0.09653980776110882, 0.15429133862450936, 0.15429133862450936, 0.15429133862450936, 0.05528871428725128]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.38529746207811166, 0.09653980776110882, 0.15429133862450936, 0.15429133862450936, 0.15429133862450936, 0.05528871428725128]
siam score:  -0.41388187
siam score:  -0.4150569
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.38529746207811166, 0.09653980776110882, 0.15429133862450936, 0.15429133862450936, 0.15429133862450936, 0.05528871428725128]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.38529746207811166, 0.09653980776110882, 0.15429133862450936, 0.15429133862450936, 0.15429133862450936, 0.05528871428725128]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
main train batch thing paused
add a thread
from probs:  [0.38529746207811166, 0.09653980776110882, 0.15429133862450936, 0.15429133862450936, 0.15429133862450936, 0.05528871428725128]
Adding thread: now have 2 threads
from probs:  [0.38529746207811166, 0.09653980776110882, 0.15429133862450936, 0.15429133862450936, 0.15429133862450936, 0.05528871428725128]
printing an ep nov before normalisation:  36.73478364944458
using another actor
from probs:  [0.38529746207811166, 0.09653980776110882, 0.15429133862450936, 0.15429133862450936, 0.15429133862450936, 0.05528871428725128]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.041458859800431534, 0.22186118873291436, 0.22186118873291436, 0.2888677680506937, 0.08784803009735573, 0.13810296458569024]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.041458859800431534, 0.22186118873291436, 0.22186118873291436, 0.2888677680506937, 0.08784803009735573, 0.13810296458569024]
siam score:  -0.4310952
deleting a thread, now have 1 threads
Frames:  1689 train batches done:  155 episodes:  54
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.041458859800431534, 0.22186118873291436, 0.22186118873291436, 0.2888677680506937, 0.08784803009735573, 0.13810296458569024]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.041458859800431534, 0.22186118873291436, 0.22186118873291436, 0.2888677680506937, 0.08784803009735573, 0.13810296458569024]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.041458859800431534, 0.22186118873291436, 0.22186118873291436, 0.2888677680506937, 0.08784803009735573, 0.13810296458569024]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.041458859800431534, 0.22186118873291436, 0.22186118873291436, 0.2888677680506937, 0.08784803009735573, 0.13810296458569024]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.041458859800431534, 0.22186118873291436, 0.22186118873291436, 0.2888677680506937, 0.08784803009735573, 0.13810296458569024]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.041458859800431534, 0.22186118873291436, 0.22186118873291436, 0.2888677680506937, 0.08784803009735573, 0.13810296458569024]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.041458859800431534, 0.22186118873291436, 0.22186118873291436, 0.2888677680506937, 0.08784803009735573, 0.13810296458569024]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.041458859800431534, 0.22186118873291436, 0.22186118873291436, 0.2888677680506937, 0.08784803009735573, 0.13810296458569024]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.041458859800431534, 0.22186118873291436, 0.22186118873291436, 0.2888677680506937, 0.08784803009735573, 0.13810296458569024]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.041458859800431534, 0.22186118873291436, 0.22186118873291436, 0.2888677680506937, 0.08784803009735573, 0.13810296458569024]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.041458859800431534, 0.22186118873291436, 0.22186118873291436, 0.2888677680506937, 0.08784803009735573, 0.13810296458569024]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.041458859800431534, 0.22186118873291436, 0.22186118873291436, 0.2888677680506937, 0.08784803009735573, 0.13810296458569024]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.041458859800431534, 0.22186118873291436, 0.22186118873291436, 0.2888677680506937, 0.08784803009735573, 0.13810296458569024]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.04264676210371975, 0.22823474280361658, 0.22823474280361658, 0.2971674213492925, 0.09036938571226466, 0.11334694522748996]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.04264676210371975, 0.22823474280361658, 0.22823474280361658, 0.2971674213492925, 0.09036938571226466, 0.11334694522748996]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.04931091972424755, 0.16431784792147464, 0.20703470696615892, 0.34372865590914886, 0.10451424525891655, 0.1310936242200535]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.05026444108217821, 0.164471214488496, 0.21086771618481256, 0.3500572212737623, 0.1273540131314427, 0.0969853938393082]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.05026444108217821, 0.164471214488496, 0.21086771618481256, 0.3500572212737623, 0.1273540131314427, 0.0969853938393082]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.05026444108217821, 0.164471214488496, 0.21086771618481256, 0.3500572212737623, 0.1273540131314427, 0.0969853938393082]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.05026444108217821, 0.164471214488496, 0.21086771618481256, 0.3500572212737623, 0.1273540131314427, 0.0969853938393082]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.05026444108217821, 0.164471214488496, 0.21086771618481256, 0.3500572212737623, 0.1273540131314427, 0.0969853938393082]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.040056889386086654, 0.2092330564217598, 0.27796087428000205, 0.2092330564217598, 0.15425080213516607, 0.10926532135522567]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.042381661923251476, 0.1632251367303806, 0.2941389011047705, 0.22140903200788717, 0.1632251367303806, 0.1156201315033297]
printing an ep nov before normalisation:  40.428899608038726
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.042381661923251476, 0.1632251367303806, 0.2941389011047705, 0.22140903200788717, 0.1632251367303806, 0.1156201315033297]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.04449520354193128, 0.1713840552326079, 0.30884697789750765, 0.23247868752811884, 0.12139753789991713, 0.12139753789991713]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.047384299911750936, 0.18253685280144089, 0.3289521184319383, 0.18253685280144089, 0.12929493802671452, 0.12929493802671452]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.047384299911750936, 0.18253685280144089, 0.3289521184319383, 0.18253685280144089, 0.12929493802671452, 0.12929493802671452]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.047384299911750936, 0.18253685280144089, 0.3289521184319383, 0.18253685280144089, 0.12929493802671452, 0.12929493802671452]
siam score:  -0.4422438
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.047384299911750936, 0.18253685280144089, 0.3289521184319383, 0.18253685280144089, 0.12929493802671452, 0.12929493802671452]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.047384299911750936, 0.18253685280144089, 0.3289521184319383, 0.18253685280144089, 0.12929493802671452, 0.12929493802671452]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.047384299911750936, 0.18253685280144089, 0.3289521184319383, 0.18253685280144089, 0.12929493802671452, 0.12929493802671452]
printing an ep nov before normalisation:  36.683807373046875
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9713285714285714 -1.0 -0.9713285714285714
probs:  [0.035278728019985814, 0.24381429396822477, 0.24381429396822477, 0.12216854716508536, 0.17746206843923956, 0.17746206843923956]
maxi score, test score, baseline:  -0.9713285714285714 -1.0 -0.9713285714285714
probs:  [0.035278728019985814, 0.24381429396822477, 0.24381429396822477, 0.12216854716508536, 0.17746206843923956, 0.17746206843923956]
actions average: 
K:  4  action  0 :  tensor([0.2552, 0.0525, 0.1195, 0.2133, 0.1229, 0.1103, 0.1263],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.1269, 0.6309, 0.0151, 0.0650, 0.0694, 0.0269, 0.0658],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1524, 0.0886, 0.3144, 0.1350, 0.0970, 0.1165, 0.0960],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1712, 0.1111, 0.1336, 0.1265, 0.2344, 0.0945, 0.1287],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1981, 0.0186, 0.1139, 0.1316, 0.2176, 0.1408, 0.1794],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1368, 0.0505, 0.0776, 0.1664, 0.1022, 0.3006, 0.1659],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1393, 0.0418, 0.1155, 0.1222, 0.1203, 0.0710, 0.3900],
       grad_fn=<DivBackward0>)
siam score:  -0.440354
maxi score, test score, baseline:  -0.9713285714285714 -1.0 -0.9713285714285714
probs:  [0.03758734831400588, 0.09992321425016884, 0.2585963275422201, 0.17264839117569247, 0.2585963275422201, 0.17264839117569247]
maxi score, test score, baseline:  -0.9713285714285714 -1.0 -0.9713285714285714
probs:  [0.03722031185995396, 0.1302103463333475, 0.25701493879706616, 0.1302103463333475, 0.25701493879706616, 0.18832911787921863]
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.041025970675345635, 0.041025970675345635, 0.2833746392953899, 0.1435580997069029, 0.2833746392953899, 0.20764068035162603]
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.041025970675345635, 0.041025970675345635, 0.2833746392953899, 0.1435580997069029, 0.2833746392953899, 0.20764068035162603]
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.041025970675345635, 0.041025970675345635, 0.2833746392953899, 0.1435580997069029, 0.2833746392953899, 0.20764068035162603]
from probs:  [0.041025970675345635, 0.041025970675345635, 0.2833746392953899, 0.1435580997069029, 0.2833746392953899, 0.20764068035162603]
printing an ep nov before normalisation:  18.43322992324829
printing an ep nov before normalisation:  113.13575198908276
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.04437960378138745, 0.04437960378138745, 0.3066034087312645, 0.15532044433710468, 0.22465846968442799, 0.22465846968442799]
using another actor
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.04717782291091326, 0.04717782291091326, 0.3259851250076711, 0.10194354296563364, 0.23885784310243438, 0.23885784310243438]
printing an ep nov before normalisation:  57.68319068913396
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.054647587661011694, 0.054647587661011694, 0.37772407303354444, 0.11810904014490214, 0.11810904014490214, 0.27676267135462795]
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.054647587661011694, 0.054647587661011694, 0.37772407303354444, 0.11810904014490214, 0.11810904014490214, 0.27676267135462795]
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.054647587661011694, 0.054647587661011694, 0.37772407303354444, 0.11810904014490214, 0.11810904014490214, 0.27676267135462795]
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
actions average: 
K:  1  action  0 :  tensor([0.3547, 0.0177, 0.2155, 0.0392, 0.2026, 0.0920, 0.0782],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0591, 0.7386, 0.0261, 0.0458, 0.0349, 0.0453, 0.0502],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1230, 0.0813, 0.4293, 0.0709, 0.0953, 0.1156, 0.0846],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1513, 0.1664, 0.1075, 0.1710, 0.1320, 0.1445, 0.1274],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2074, 0.0423, 0.0677, 0.0826, 0.4195, 0.0946, 0.0860],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0833, 0.0493, 0.1170, 0.0796, 0.0734, 0.5096, 0.0877],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1222, 0.0815, 0.0904, 0.1459, 0.1321, 0.1791, 0.2489],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.054647587661011694, 0.054647587661011694, 0.37772407303354444, 0.11810904014490214, 0.11810904014490214, 0.27676267135462795]
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.054647587661011694, 0.054647587661011694, 0.37772407303354444, 0.11810904014490214, 0.11810904014490214, 0.27676267135462795]
Printing some Q and Qe and total Qs values:  [[1.182]
 [1.182]
 [1.182]
 [1.182]
 [1.182]
 [1.182]
 [1.182]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.182]
 [1.182]
 [1.182]
 [1.182]
 [1.182]
 [1.182]
 [1.182]]
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.054647587661011694, 0.054647587661011694, 0.37772407303354444, 0.11810904014490214, 0.11810904014490214, 0.27676267135462795]
from probs:  [0.054647587661011694, 0.054647587661011694, 0.37772407303354444, 0.11810904014490214, 0.11810904014490214, 0.27676267135462795]
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.04737768811625645, 0.04737768811625645, 0.32571863806721374, 0.12690367381652984, 0.12690367381652984, 0.32571863806721374]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.09809612500023629, 0.04499042890112841, 0.22881783847496331, 0.15878834911350226, 0.15878834911350226, 0.3105189093966675]
actions average: 
K:  0  action  0 :  tensor([0.2586, 0.0708, 0.0832, 0.1328, 0.1840, 0.1071, 0.1635],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0370, 0.8383, 0.0156, 0.0320, 0.0213, 0.0172, 0.0387],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1375, 0.0743, 0.3056, 0.1192, 0.1233, 0.1221, 0.1179],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1456, 0.0610, 0.0866, 0.1667, 0.2222, 0.1850, 0.1329],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1773, 0.0206, 0.0709, 0.1133, 0.3991, 0.1213, 0.0975],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0825, 0.0681, 0.0907, 0.1130, 0.1277, 0.4262, 0.0919],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1462, 0.1352, 0.0977, 0.1025, 0.1520, 0.1287, 0.2377],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.11019555217239374, 0.04090739820863396, 0.28075100808318704, 0.18938201384526177, 0.18938201384526177, 0.18938201384526177]
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.11966871717755484, 0.04441686843545737, 0.3049040371581017, 0.20567083002566555, 0.11966871717755484, 0.20567083002566555]
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
siam score:  -0.48822945
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.11966871717755484, 0.04441686843545737, 0.3049040371581017, 0.20567083002566555, 0.11966871717755484, 0.20567083002566555]
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.11966871717755484, 0.04441686843545737, 0.3049040371581017, 0.20567083002566555, 0.11966871717755484, 0.20567083002566555]
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.11966871717755484, 0.04441686843545737, 0.3049040371581017, 0.20567083002566555, 0.11966871717755484, 0.20567083002566555]
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.11966871717755484, 0.04441686843545737, 0.3049040371581017, 0.20567083002566555, 0.11966871717755484, 0.20567083002566555]
siam score:  -0.48788312
using another actor
maxi score, test score, baseline:  -0.9760904761904762 -1.0 -0.9760904761904762
probs:  [0.1196687166682672, 0.04441686711071286, 0.30490403865609367, 0.20567083044832957, 0.1196687166682672, 0.20567083044832957]
maxi score, test score, baseline:  -0.9760904761904762 -1.0 -0.9760904761904762
probs:  [0.1196687166682672, 0.04441686711071286, 0.30490403865609367, 0.20567083044832957, 0.1196687166682672, 0.20567083044832957]
using explorer policy with actor:  0
using another actor
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.16106819363824684, 0.043500260041429664, 0.29543154632032353, 0.043500260041429664, 0.16106819363824684, 0.29543154632032353]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.1874589413205482, 0.10932376183175137, 0.2767562893077447, 0.10932376183175137, 0.040380956400459786, 0.2767562893077447]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.20584297144443717, 0.12003965361827725, 0.3039039061029056, 0.12003965361827725, 0.044330843771665566, 0.20584297144443717]
maxi score, test score, baseline:  -0.9776777777777778 -1.0 -0.9776777777777778
probs:  [0.235738346929556, 0.12899120470509062, 0.235738346929556, 0.12899120470509062, 0.034802549801150606, 0.235738346929556]
printing an ep nov before normalisation:  42.40923717206178
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.29504385776745584, 0.043529360916930035, 0.29504385776745584, 0.16142678131561414, 0.043529360916930035, 0.16142678131561414]
siam score:  -0.5046394
printing an ep nov before normalisation:  43.0141160218225
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.3344958895900454, 0.04933473205651691, 0.3344958895900454, 0.1830040246503585, 0.04933473205651691, 0.04933473205651691]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.2762131532549523, 0.10972561690969493, 0.2762131532549523, 0.18776664957153436, 0.10972561690969493, 0.040355810099171045]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.27153351305484197, 0.1418541043687187, 0.20264132719033903, 0.20264132719033903, 0.1418541043687187, 0.03947562382704263]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.27153351305484197, 0.1418541043687187, 0.20264132719033903, 0.20264132719033903, 0.1418541043687187, 0.03947562382704263]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.22171058331759777, 0.15120489232651738, 0.22171058331759777, 0.22171058331759777, 0.15120489232651738, 0.03245846539417183]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.22171058331759777, 0.15120489232651738, 0.22171058331759777, 0.22171058331759777, 0.15120489232651738, 0.03245846539417183]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.22171058331759777, 0.15120489232651738, 0.22171058331759777, 0.22171058331759777, 0.15120489232651738, 0.03245846539417183]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.22171058331759777, 0.15120489232651738, 0.22171058331759777, 0.22171058331759777, 0.15120489232651738, 0.03245846539417183]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.27844291627580214, 0.18988505365111252, 0.18988505365111252, 0.18988505365111252, 0.11116695354027735, 0.0407349692305828]
printing an ep nov before normalisation:  52.798848152160645
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.23534340342587085, 0.23534340342587085, 0.1295441062562859, 0.23534340342587085, 0.1295441062562859, 0.034881577209815516]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
siam score:  -0.5576862
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.17616628638454188, 0.2575915982520434, 0.17616628638454188, 0.17616628638454188, 0.17616628638454188, 0.037743256209789064]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.14506615027167563, 0.14506615027167563, 0.26288714878980896, 0.14506615027167563, 0.26288714878980896, 0.03902725160535522]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.16227303105048949, 0.04364486941370587, 0.29408209953580466, 0.16227303105048949, 0.29408209953580466, 0.04364486941370587]
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.83081940242223
siam score:  -0.56374764
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.1748756661248041, 0.03742265194203918, 0.25505659106475004, 0.1748756661248041, 0.25505659106475004, 0.10271283367885244]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.1748756661248041, 0.03742265194203918, 0.25505659106475004, 0.1748756661248041, 0.25505659106475004, 0.10271283367885244]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.19012126988430916, 0.04067601044446344, 0.19012126988430916, 0.19012126988430916, 0.2772976712242191, 0.11166250867839005]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.19012126988430916, 0.04067601044446344, 0.19012126988430916, 0.19012126988430916, 0.2772976712242191, 0.11166250867839005]
siam score:  -0.5595281
maxi score, test score, baseline:  -0.9790666666666666 -1.0 -0.9790666666666666
probs:  [0.2547431863250424, 0.10296641615656202, 0.03742644722017283, 0.2547431863250424, 0.17506038198659019, 0.17506038198659019]
maxi score, test score, baseline:  -0.9794918367346939 -1.0 -0.9794918367346939
probs:  [0.29768019163216514, 0.12030803475579795, 0.04371551246827583, 0.29768019163216514, 0.12030803475579795, 0.12030803475579795]
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.2255692973907408, 0.13265873270144662, 0.04819458298390653, 0.32825992152101297, 0.13265873270144662, 0.13265873270144662]
UNIT TEST: sample policy line 217 mcts : [0.  0.2 0.2 0.2 0.2 0.2 0. ]
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
probs:  [0.293560294834625, 0.043738464378957204, 0.043738464378957204, 0.293560294834625, 0.16270124078641787, 0.16270124078641787]
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
probs:  [0.293560294834625, 0.043738464378957204, 0.043738464378957204, 0.293560294834625, 0.16270124078641787, 0.16270124078641787]
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.23197197126400138, 0.03605605747199719, 0.03605605747199719, 0.23197197126400138, 0.23197197126400138, 0.23197197126400138]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.23197197126400138, 0.03605605747199719, 0.03605605747199719, 0.23197197126400138, 0.23197197126400138, 0.23197197126400138]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.21254818348575885, 0.03176131124336601, 0.1180459548135987, 0.21254818348575885, 0.21254818348575885, 0.21254818348575885]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.21254818348575885, 0.03176131124336601, 0.1180459548135987, 0.21254818348575885, 0.21254818348575885, 0.21254818348575885]
using another actor
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
using another actor
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.2594704682221175, 0.038747913171315526, 0.1440927689910165, 0.2594704682221175, 0.038747913171315526, 0.2594704682221175]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.18490974074932856, 0.049696368136658835, 0.049696368136658835, 0.33300057742034744, 0.049696368136658835, 0.33300057742034744]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.05745296335610132, 0.05745296335610132, 0.05745296335610132, 0.3850940732877974, 0.05745296335610132, 0.3850940732877974]
siam score:  -0.55514014
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.05745296335610132, 0.05745296335610132, 0.05745296335610132, 0.3850940732877974, 0.05745296335610132, 0.3850940732877974]
actions average: 
K:  1  action  0 :  tensor([0.3274, 0.0690, 0.0687, 0.1070, 0.1661, 0.1238, 0.1380],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0804, 0.6323, 0.0296, 0.0811, 0.0557, 0.0567, 0.0641],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0969, 0.0167, 0.4971, 0.0837, 0.0977, 0.1131, 0.0949],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1819, 0.0070, 0.1104, 0.1548, 0.2289, 0.1476, 0.1696],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2169, 0.0460, 0.0710, 0.1333, 0.2380, 0.0970, 0.1978],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0947, 0.0057, 0.0904, 0.0754, 0.1098, 0.5251, 0.0990],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2395, 0.0568, 0.0921, 0.1257, 0.1718, 0.1244, 0.1898],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  63.90481472015381
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.06931016419316441, 0.06931016419316441, 0.06931016419316441, 0.464727343470158, 0.06931016419316441, 0.2580319997571844]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.08755217409501348, 0.08755217409501348, 0.08755217409501348, 0.08755217409501348, 0.08755217409501348, 0.5622391295249327]
from probs:  [0.08755217409501348, 0.08755217409501348, 0.08755217409501348, 0.08755217409501348, 0.08755217409501348, 0.5622391295249327]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.0693758438733474, 0.0693758438733474, 0.0693758438733474, 0.25823449988390834, 0.0693758438733474, 0.464262124622702]
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.0693758438733474, 0.0693758438733474, 0.0693758438733474, 0.25823449988390834, 0.0693758438733474, 0.464262124622702]
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.13320906399082227, 0.04813596371958009, 0.13320906399082227, 0.22567982515521637, 0.13320906399082227, 0.32655701915273677]
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.13320906399082227, 0.04813596371958009, 0.13320906399082227, 0.22567982515521637, 0.13320906399082227, 0.32655701915273677]
printing an ep nov before normalisation:  39.021238119530466
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.049829318350502506, 0.049829318350502506, 0.049829318350502506, 0.33259533780474854, 0.18532136933899537, 0.33259533780474854]
from probs:  [0.049829318350502506, 0.049829318350502506, 0.049829318350502506, 0.33259533780474854, 0.18532136933899537, 0.33259533780474854]
printing an ep nov before normalisation:  26.44312858581543
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.035216729736564326, 0.13081668386754786, 0.13081668386754786, 0.23438330084278003, 0.23438330084278003, 0.23438330084278003]
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.035216729736564326, 0.13081668386754786, 0.13081668386754786, 0.23438330084278003, 0.23438330084278003, 0.23438330084278003]
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.035216729736564326, 0.13081668386754786, 0.13081668386754786, 0.23438330084278003, 0.23438330084278003, 0.23438330084278003]
using another actor
from probs:  [0.04395068403675281, 0.04395068403675281, 0.16335001848747976, 0.2926992974757674, 0.2926992974757674, 0.16335001848747976]
actions average: 
K:  4  action  0 :  tensor([0.2024, 0.0491, 0.0855, 0.1668, 0.2152, 0.0974, 0.1836],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.1204, 0.3896, 0.0551, 0.0972, 0.0948, 0.0963, 0.1466],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1514, 0.0411, 0.1046, 0.1105, 0.2779, 0.1554, 0.1591],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1495, 0.0684, 0.0774, 0.1489, 0.1688, 0.1315, 0.2554],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1034, 0.2469, 0.0574, 0.1201, 0.1982, 0.1013, 0.1726],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1519, 0.0823, 0.0969, 0.1222, 0.1442, 0.2265, 0.1761],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2072, 0.0677, 0.0973, 0.1589, 0.2206, 0.0941, 0.1541],
       grad_fn=<DivBackward0>)
actions average: 
K:  0  action  0 :  tensor([0.4658, 0.0282, 0.0785, 0.1058, 0.1207, 0.0973, 0.1038],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0329, 0.8459, 0.0169, 0.0333, 0.0209, 0.0172, 0.0328],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0908, 0.0329, 0.4693, 0.0778, 0.0837, 0.1155, 0.1300],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1552, 0.0770, 0.1221, 0.1620, 0.1285, 0.1729, 0.1822],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1972, 0.0323, 0.0842, 0.1301, 0.2916, 0.1168, 0.1477],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1349, 0.0729, 0.0754, 0.1160, 0.1003, 0.3628, 0.1377],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1581, 0.1003, 0.1009, 0.1405, 0.1333, 0.1706, 0.1963],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.04538756231960177, 0.04538756231960177, 0.04538756231960177, 0.2879457710137316, 0.2879457710137316, 0.2879457710137316]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.04538756231960177, 0.04538756231960177, 0.04538756231960177, 0.2879457710137316, 0.2879457710137316, 0.2879457710137316]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.04538756231960177, 0.04538756231960177, 0.04538756231960177, 0.2879457710137316, 0.2879457710137316, 0.2879457710137316]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.04538756231960177, 0.04538756231960177, 0.04538756231960177, 0.2879457710137316, 0.2879457710137316, 0.2879457710137316]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.04538756231960177, 0.04538756231960177, 0.04538756231960177, 0.2879457710137316, 0.2879457710137316, 0.2879457710137316]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.04538756231960177, 0.04538756231960177, 0.04538756231960177, 0.2879457710137316, 0.2879457710137316, 0.2879457710137316]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.04538756231960177, 0.04538756231960177, 0.04538756231960177, 0.2879457710137316, 0.2879457710137316, 0.2879457710137316]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.04538756231960177, 0.04538756231960177, 0.04538756231960177, 0.2879457710137316, 0.2879457710137316, 0.2879457710137316]
siam score:  -0.5610707
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.146035367725525, 0.146035367725525, 0.146035367725525, 0.2612860721553512, 0.2612860721553512, 0.039321752512722526]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.146035367725525, 0.146035367725525, 0.146035367725525, 0.2612860721553512, 0.2612860721553512, 0.039321752512722526]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.2878039770493705, 0.2878039770493705, 0.045529356283962825, 0.2878039770493705, 0.045529356283962825, 0.045529356283962825]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
Printing some Q and Qe and total Qs values:  [[1.18 ]
 [1.141]
 [1.195]
 [1.191]
 [1.19 ]
 [1.171]
 [1.104]] [[104.362]
 [103.223]
 [101.197]
 [ 97.412]
 [ 92.546]
 [ 88.721]
 [ 84.999]] [[1.996]
 [1.941]
 [1.968]
 [1.912]
 [1.845]
 [1.774]
 [1.657]]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.2878039770493705, 0.2878039770493705, 0.045529356283962825, 0.2878039770493705, 0.045529356283962825, 0.045529356283962825]
printing an ep nov before normalisation:  52.48398835654122
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.2878039770493705, 0.2878039770493705, 0.045529356283962825, 0.2878039770493705, 0.045529356283962825, 0.045529356283962825]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.2878039770493705, 0.2878039770493705, 0.045529356283962825, 0.2878039770493705, 0.045529356283962825, 0.045529356283962825]
Printing some Q and Qe and total Qs values:  [[0.218]
 [0.282]
 [0.218]
 [0.218]
 [0.218]
 [0.218]
 [0.218]] [[15.35 ]
 [14.772]
 [15.35 ]
 [15.35 ]
 [15.35 ]
 [15.35 ]
 [15.35 ]] [[1.094]
 [1.106]
 [1.094]
 [1.094]
 [1.094]
 [1.094]
 [1.094]]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.2878039770493705, 0.2878039770493705, 0.045529356283962825, 0.2878039770493705, 0.045529356283962825, 0.045529356283962825]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
printing an ep nov before normalisation:  98.13850085110792
printing an ep nov before normalisation:  60.0412033556438
printing an ep nov before normalisation:  32.610271264913365
using another actor
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.2923329300042523, 0.04406543414219306, 0.04406543414219306, 0.2923329300042523, 0.16360163585355464, 0.16360163585355464]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
printing an ep nov before normalisation:  44.567202530064094
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.2923329300042523, 0.04406543414219306, 0.04406543414219306, 0.2923329300042523, 0.16360163585355464, 0.16360163585355464]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.2923329300042523, 0.04406543414219306, 0.04406543414219306, 0.2923329300042523, 0.16360163585355464, 0.16360163585355464]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.2923329300042523, 0.04406543414219306, 0.04406543414219306, 0.2923329300042523, 0.16360163585355464, 0.16360163585355464]
printing an ep nov before normalisation:  26.710094003482254
printing an ep nov before normalisation:  26.885574671805728
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.2923329300042523, 0.04406543414219306, 0.04406543414219306, 0.2923329300042523, 0.16360163585355464, 0.16360163585355464]
from probs:  [0.2923329300042523, 0.04406543414219306, 0.04406543414219306, 0.2923329300042523, 0.16360163585355464, 0.16360163585355464]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
printing an ep nov before normalisation:  43.91087052394083
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.3320394942886179, 0.050035304028027174, 0.050035304028027174, 0.3320394942886179, 0.1858150993386826, 0.050035304028027174]
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.3320394942886179, 0.050035304028027174, 0.050035304028027174, 0.3320394942886179, 0.1858150993386826, 0.050035304028027174]
printing an ep nov before normalisation:  16.631372554863066
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.21764625483218544, 0.05858993975576686, 0.05858993975576686, 0.38893767106832855, 0.21764625483218544, 0.05858993975576686]
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.21764625610512855, 0.0585899370571272, 0.0585899370571272, 0.3889376766183612, 0.21764625610512855, 0.0585899370571272]
printing an ep nov before normalisation:  26.786038670082256
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.21764625610512855, 0.0585899370571272, 0.0585899370571272, 0.3889376766183612, 0.21764625610512855, 0.0585899370571272]
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.21764625610512855, 0.0585899370571272, 0.0585899370571272, 0.3889376766183612, 0.21764625610512855, 0.0585899370571272]
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
printing an ep nov before normalisation:  30.813732611131528
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 15.12629794966495
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.06965697511069133, 0.06965697511069133, 0.06965697511069133, 0.46254622591239164, 0.2588258736448431, 0.06965697511069133]
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.06965697511069133, 0.06965697511069133, 0.06965697511069133, 0.46254622591239164, 0.2588258736448431, 0.06965697511069133]
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.06965697511069133, 0.06965697511069133, 0.06965697511069133, 0.46254622591239164, 0.2588258736448431, 0.06965697511069133]
siam score:  -0.57731193
actions average: 
K:  4  action  0 :  tensor([0.2329, 0.0136, 0.1674, 0.0865, 0.1875, 0.1287, 0.1834],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0829, 0.5614, 0.0424, 0.0576, 0.0838, 0.0986, 0.0733],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1520, 0.0680, 0.1610, 0.1263, 0.2271, 0.1475, 0.1181],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1676, 0.0176, 0.1491, 0.0605, 0.2749, 0.1075, 0.2228],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1976, 0.0354, 0.1254, 0.0956, 0.2899, 0.1197, 0.1364],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1281, 0.0839, 0.1774, 0.0692, 0.1863, 0.2286, 0.1266],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1433, 0.1124, 0.1310, 0.0888, 0.1651, 0.1242, 0.2353],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.04813162174725044, 0.13376963361699007, 0.13376963361699007, 0.32480827548025454, 0.22575120192152476, 0.13376963361699007]
main train batch thing paused
add a thread
Adding thread: now have 4 threads
printing an ep nov before normalisation:  10.538755595159577
printing an ep nov before normalisation:  36.68591450972445
printing an ep nov before normalisation:  11.159887427388462
main train batch thing paused
add a thread
Adding thread: now have 5 threads
printing an ep nov before normalisation:  44.45251200738228
printing an ep nov before normalisation:  80.8492011650996
printing an ep nov before normalisation:  26.059131622314453
using explorer policy with actor:  1
printing an ep nov before normalisation:  85.51018642093442
printing an ep nov before normalisation:  57.49137702747973
Printing some Q and Qe and total Qs values:  [[1.102]
 [1.102]
 [1.102]
 [1.102]
 [1.1  ]
 [1.102]
 [1.102]] [[22.953]
 [22.953]
 [22.953]
 [22.953]
 [24.414]
 [22.953]
 [22.953]] [[2.336]
 [2.336]
 [2.336]
 [2.336]
 [2.457]
 [2.336]
 [2.336]]
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.050613694108291724, 0.050613694108291724, 0.1878442164035964, 0.1878442164035964, 0.3352399625726272, 0.1878442164035964]
printing an ep nov before normalisation:  79.10290454003118
printing an ep nov before normalisation:  56.7845236915191
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.036914524143604445, 0.036914524143604445, 0.23154273792819777, 0.23154273792819777, 0.23154273792819777, 0.23154273792819777]
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.036914524143604445, 0.036914524143604445, 0.23154273792819777, 0.23154273792819777, 0.23154273792819777, 0.23154273792819777]
Printing some Q and Qe and total Qs values:  [[1.226]
 [1.226]
 [1.261]
 [1.226]
 [1.258]
 [1.245]
 [1.226]] [[62.576]
 [62.576]
 [45.34 ]
 [62.576]
 [43.977]
 [43.672]
 [62.576]] [[4.087]
 [4.087]
 [2.928]
 [4.087]
 [2.83 ]
 [2.796]
 [4.087]]
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.036914524143604445, 0.036914524143604445, 0.23154273792819777, 0.23154273792819777, 0.23154273792819777, 0.23154273792819777]
printing an ep nov before normalisation:  68.65316336321747
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.036914524143604445, 0.036914524143604445, 0.23154273792819777, 0.23154273792819777, 0.23154273792819777, 0.23154273792819777]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.045812013212832306, 0.045812013212832306, 0.2875213201205011, 0.045812013212832306, 0.2875213201205011, 0.2875213201205011]
printing an ep nov before normalisation:  20.074162036405863
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.208]
 [0.208]
 [0.208]
 [0.208]
 [0.208]
 [0.208]
 [0.402]] [[15.868]
 [15.868]
 [15.868]
 [15.868]
 [15.868]
 [15.868]
 [30.994]] [[0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [1.582]]
printing an ep nov before normalisation:  51.08617164586383
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.045812013212832306, 0.045812013212832306, 0.2875213201205011, 0.045812013212832306, 0.2875213201205011, 0.2875213201205011]
siam score:  -0.5920002
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.045812013212832306, 0.045812013212832306, 0.2875213201205011, 0.045812013212832306, 0.2875213201205011, 0.2875213201205011]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.045812013212832306, 0.045812013212832306, 0.2875213201205011, 0.045812013212832306, 0.2875213201205011, 0.2875213201205011]
siam score:  -0.5899331
siam score:  -0.5895659
printing an ep nov before normalisation:  21.20603529887873
printing an ep nov before normalisation:  21.354985951027924
Printing some Q and Qe and total Qs values:  [[0.127]
 [0.127]
 [0.127]
 [0.127]
 [0.127]
 [0.127]
 [0.127]] [[18.223]
 [18.223]
 [18.223]
 [18.223]
 [18.223]
 [18.223]
 [18.223]] [[1.617]
 [1.617]
 [1.617]
 [1.617]
 [1.617]
 [1.617]
 [1.617]]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.045812013212832306, 0.045812013212832306, 0.2875213201205011, 0.045812013212832306, 0.2875213201205011, 0.2875213201205011]
printing an ep nov before normalisation:  46.294939599923914
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.045812013212832306, 0.045812013212832306, 0.2875213201205011, 0.045812013212832306, 0.2875213201205011, 0.2875213201205011]
main train batch thing paused
add a thread
Adding thread: now have 6 threads
Printing some Q and Qe and total Qs values:  [[0.619]
 [0.526]
 [0.654]
 [0.526]
 [0.57 ]
 [0.526]
 [0.526]] [[15.445]
 [13.006]
 [22.705]
 [13.006]
 [14.906]
 [13.006]
 [13.006]] [[1.097]
 [0.869]
 [1.534]
 [0.869]
 [1.018]
 [0.869]
 [0.869]]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.16523016760016238, 0.16523016760016238, 0.2945150835855496, 0.04456424601380086, 0.16523016760016238, 0.16523016760016238]
siam score:  -0.5875112
printing an ep nov before normalisation:  14.513088322896897
printing an ep nov before normalisation:  66.71194523078692
printing an ep nov before normalisation:  50.832070414805855
Printing some Q and Qe and total Qs values:  [[0.632]
 [0.645]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.651]] [[36.208]
 [43.914]
 [35.335]
 [34.915]
 [34.093]
 [33.973]
 [33.974]] [[0.632]
 [0.645]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.651]]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.21780278865380043, 0.21780278865380043, 0.38825652861091386, 0.0587126313604951, 0.0587126313604951, 0.0587126313604951]
Printing some Q and Qe and total Qs values:  [[1.227]
 [1.227]
 [1.227]
 [1.227]
 [1.227]
 [1.227]
 [1.227]] [[38.41]
 [38.41]
 [38.41]
 [38.41]
 [38.41]
 [38.41]
 [38.41]] [[2.331]
 [2.331]
 [2.331]
 [2.331]
 [2.331]
 [2.331]
 [2.331]]
printing an ep nov before normalisation:  76.51791052221313
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.14639800859135543, 0.26063953592492795, 0.26063953592492795, 0.14639800859135543, 0.0395269023760779, 0.14639800859135543]
printing an ep nov before normalisation:  43.94132614135742
printing an ep nov before normalisation:  45.69786019301432
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.14639800859135543, 0.26063953592492795, 0.26063953592492795, 0.14639800859135543, 0.0395269023760779, 0.14639800859135543]
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.14639800859135543, 0.26063953592492795, 0.26063953592492795, 0.14639800859135543, 0.0395269023760779, 0.14639800859135543]
printing an ep nov before normalisation:  63.86530770967278
using explorer policy with actor:  1
printing an ep nov before normalisation:  79.14438269336377
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.14639800859135543, 0.26063953592492795, 0.26063953592492795, 0.14639800859135543, 0.0395269023760779, 0.14639800859135543]
actions average: 
K:  2  action  0 :  tensor([0.2977, 0.0049, 0.1206, 0.1639, 0.1269, 0.1073, 0.1788],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0491, 0.7051, 0.0432, 0.0651, 0.0427, 0.0391, 0.0557],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1219, 0.0556, 0.2636, 0.1576, 0.1218, 0.1031, 0.1765],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1500, 0.0813, 0.1017, 0.1816, 0.1544, 0.1254, 0.2057],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2046, 0.0525, 0.0797, 0.1091, 0.4005, 0.0603, 0.0933],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0964, 0.0301, 0.1573, 0.1286, 0.1141, 0.3422, 0.1313],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1418, 0.1012, 0.1248, 0.1839, 0.1505, 0.1030, 0.1948],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.829]
 [0.829]
 [0.829]
 [0.829]
 [1.039]
 [0.829]
 [0.829]] [[56.867]
 [56.867]
 [56.867]
 [56.867]
 [70.234]
 [56.867]
 [56.867]] [[2.006]
 [2.006]
 [2.006]
 [2.006]
 [2.597]
 [2.006]
 [2.006]]
printing an ep nov before normalisation:  52.73392339105176
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.14639800859135543, 0.26063953592492795, 0.26063953592492795, 0.14639800859135543, 0.0395269023760779, 0.14639800859135543]
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.14639800859135543, 0.26063953592492795, 0.26063953592492795, 0.14639800859135543, 0.0395269023760779, 0.14639800859135543]
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.14639800859135543, 0.26063953592492795, 0.26063953592492795, 0.14639800859135543, 0.0395269023760779, 0.14639800859135543]
printing an ep nov before normalisation:  30.709780589221147
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.14639800859135543, 0.26063953592492795, 0.26063953592492795, 0.14639800859135543, 0.0395269023760779, 0.14639800859135543]
using explorer policy with actor:  1
siam score:  -0.57093287
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.044244941547543576, 0.29183944178846677, 0.29183944178846677, 0.16391561666398966, 0.044244941547543576, 0.16391561666398966]
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.044244941547543576, 0.29183944178846677, 0.29183944178846677, 0.16391561666398966, 0.044244941547543576, 0.16391561666398966]
printing an ep nov before normalisation:  19.84176256310163
printing an ep nov before normalisation:  23.35286587462811
siam score:  -0.56873137
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
printing an ep nov before normalisation:  50.11862572517229
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.04071856391462655, 0.2735848373925536, 0.19078794015595724, 0.19078794015595724, 0.11333277822494821, 0.19078794015595724]
actions average: 
K:  2  action  0 :  tensor([0.2835, 0.0741, 0.1008, 0.1080, 0.1736, 0.1034, 0.1565],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0894, 0.5712, 0.0734, 0.0731, 0.0603, 0.0607, 0.0719],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1636, 0.0576, 0.1655, 0.1425, 0.1319, 0.1735, 0.1654],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1502, 0.1866, 0.1155, 0.1466, 0.1378, 0.1242, 0.1391],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1137, 0.0188, 0.1402, 0.0989, 0.3779, 0.1464, 0.1042],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1337, 0.0269, 0.1315, 0.0943, 0.1464, 0.3721, 0.0951],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1514, 0.0781, 0.1795, 0.1175, 0.1122, 0.1190, 0.2424],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.04071856391462655, 0.2735848373925536, 0.19078794015595724, 0.19078794015595724, 0.11333277822494821, 0.19078794015595724]
printing an ep nov before normalisation:  60.247210181491724
printing an ep nov before normalisation:  32.115341256845184
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.04071856391462655, 0.2735848373925536, 0.19078794015595724, 0.19078794015595724, 0.11333277822494821, 0.19078794015595724]
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.04412898895982601, 0.29656324010584006, 0.20680883969836844, 0.12284504576879858, 0.12284504576879858, 0.20680883969836844]
printing an ep nov before normalisation:  57.568124197621884
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.04412898895982601, 0.29656324010584006, 0.20680883969836844, 0.12284504576879858, 0.12284504576879858, 0.20680883969836844]
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.04412898895982601, 0.29656324010584006, 0.20680883969836844, 0.12284504576879858, 0.12284504576879858, 0.20680883969836844]
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.04412898895982601, 0.29656324010584006, 0.20680883969836844, 0.12284504576879858, 0.12284504576879858, 0.20680883969836844]
Printing some Q and Qe and total Qs values:  [[0.437]
 [0.457]
 [0.457]
 [0.457]
 [0.457]
 [0.438]
 [0.457]] [[30.926]
 [20.721]
 [20.721]
 [20.721]
 [20.721]
 [27.154]
 [20.721]] [[2.023]
 [1.294]
 [1.294]
 [1.294]
 [1.294]
 [1.747]
 [1.294]]
maxi score, test score, baseline:  -0.9862013698630137 -1.0 -0.9862013698630137
probs:  [0.048165156612574346, 0.32375772097844047, 0.13410262292021016, 0.13410262292021016, 0.13410262292021016, 0.2257692536483548]
maxi score, test score, baseline:  -0.9862013698630137 -1.0 -0.9862013698630137
probs:  [0.048165156612574346, 0.32375772097844047, 0.13410262292021016, 0.13410262292021016, 0.13410262292021016, 0.2257692536483548]
Printing some Q and Qe and total Qs values:  [[0.998]
 [0.998]
 [0.991]
 [0.994]
 [0.993]
 [0.998]
 [0.997]] [[25.854]
 [25.854]
 [26.937]
 [29.553]
 [29.291]
 [25.854]
 [27.823]] [[1.985]
 [1.985]
 [2.036]
 [2.179]
 [2.164]
 [1.985]
 [2.089]]
printing an ep nov before normalisation:  59.843214637104104
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.327]
 [0.345]
 [0.3  ]
 [0.327]
 [0.386]
 [0.327]] [[14.975]
 [14.975]
 [29.112]
 [17.002]
 [14.975]
 [28.774]
 [14.975]] [[0.692]
 [0.692]
 [1.38 ]
 [0.761]
 [0.692]
 [1.405]
 [0.692]]
main train batch thing paused
printing an ep nov before normalisation:  15.697176830411909
printing an ep nov before normalisation:  46.542988723172115
printing an ep nov before normalisation:  39.54604485218796
Printing some Q and Qe and total Qs values:  [[1.274]
 [1.173]
 [1.173]
 [1.173]
 [1.173]
 [1.14 ]
 [1.173]] [[15.375]
 [15.954]
 [15.954]
 [15.954]
 [15.954]
 [17.252]
 [15.954]] [[1.987]
 [1.94 ]
 [1.94 ]
 [1.94 ]
 [1.94 ]
 [2.029]
 [1.94 ]]
maxi score, test score, baseline:  -0.9862013698630137 -1.0 -0.9862013698630137
probs:  [0.06995829625341138, 0.4609964896635301, 0.25917032532282414, 0.06995829625341138, 0.06995829625341138, 0.06995829625341138]
printing an ep nov before normalisation:  12.267061471939087
maxi score, test score, baseline:  -0.9862013698630137 -1.0 -0.9862013698630137
maxi score, test score, baseline:  -0.9862013698630137 -1.0 -0.9862013698630137
probs:  [0.06995829625341138, 0.4609964896635301, 0.25917032532282414, 0.06995829625341138, 0.06995829625341138, 0.06995829625341138]
printing an ep nov before normalisation:  32.6348539718909
maxi score, test score, baseline:  -0.9862013698630137 -1.0 -0.9862013698630137
probs:  [0.1465485715677068, 0.26035950955610854, 0.26035950955610854, 0.1465485715677068, 0.1465485715677068, 0.03963526618466258]
printing an ep nov before normalisation:  20.80932321698155
printing an ep nov before normalisation:  9.222294465871528e-05
from probs:  [0.16536928712510174, 0.16536928712510174, 0.2938098617400148, 0.16536928712510174, 0.16536928712510174, 0.0447129897595781]
printing an ep nov before normalisation:  71.25239827056488
maxi score, test score, baseline:  -0.9862013698630137 -1.0 -0.9862013698630137
probs:  [0.1937311985657466, 0.1937311985657466, 0.1937311985657466, 0.1937311985657466, 0.1937311985657466, 0.031344007171267035]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.19373119942538103, 0.19373119942538103, 0.19373119942538103, 0.19373119942538103, 0.19373119942538103, 0.031344002873094856]
printing an ep nov before normalisation:  34.475174034128585
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.19373119942538103, 0.19373119942538103, 0.19373119942538103, 0.19373119942538103, 0.19373119942538103, 0.031344002873094856]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
printing an ep nov before normalisation:  38.750996763322505
Printing some Q and Qe and total Qs values:  [[1.012]
 [1.012]
 [1.012]
 [1.012]
 [1.012]
 [1.012]
 [1.012]] [[23.975]
 [23.975]
 [23.975]
 [23.975]
 [23.975]
 [23.975]
 [23.975]] [[2.196]
 [2.196]
 [2.196]
 [2.196]
 [2.196]
 [2.196]
 [2.196]]
printing an ep nov before normalisation:  36.316887602000214
siam score:  -0.6104244
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.21203995521391866, 0.21203995521391866, 0.21203995521391866, 0.21203995521391866, 0.11947844657752445, 0.03236173256680093]
printing an ep nov before normalisation:  28.097965717315674
actions average: 
K:  2  action  0 :  tensor([0.3463, 0.0582, 0.1033, 0.1054, 0.1562, 0.0976, 0.1331],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0678, 0.7493, 0.0298, 0.0461, 0.0344, 0.0273, 0.0452],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1077, 0.0577, 0.4268, 0.0817, 0.1140, 0.1039, 0.1082],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1951, 0.1037, 0.1356, 0.1560, 0.1401, 0.1208, 0.1487],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1500, 0.0477, 0.1264, 0.1414, 0.2081, 0.1554, 0.1710],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1469, 0.0489, 0.1068, 0.0722, 0.0917, 0.4312, 0.1023],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1433, 0.1381, 0.1170, 0.1149, 0.1299, 0.1291, 0.2275],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.23227999820196119, 0.23227999820196119, 0.23227999820196119, 0.23227999820196119, 0.03544000359607759, 0.03544000359607759]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.23227999820196119, 0.23227999820196119, 0.23227999820196119, 0.23227999820196119, 0.03544000359607759, 0.03544000359607759]
printing an ep nov before normalisation:  19.158735321591486
Printing some Q and Qe and total Qs values:  [[1.19 ]
 [1.19 ]
 [1.19 ]
 [1.19 ]
 [1.19 ]
 [1.19 ]
 [1.222]] [[31.228]
 [31.228]
 [31.228]
 [31.228]
 [31.228]
 [31.228]
 [32.219]] [[1.322]
 [1.322]
 [1.322]
 [1.322]
 [1.322]
 [1.322]
 [1.361]]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.23227999820196119, 0.23227999820196119, 0.23227999820196119, 0.23227999820196119, 0.03544000359607759, 0.03544000359607759]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.2584999708944766, 0.2584999708944766, 0.14564458497596322, 0.2584999708944766, 0.039427751170303525, 0.039427751170303525]
siam score:  -0.6187657
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.2584999708944766, 0.2584999708944766, 0.14564458497596322, 0.2584999708944766, 0.039427751170303525, 0.039427751170303525]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
from probs:  [0.2584999708944766, 0.2584999708944766, 0.14564458497596322, 0.2584999708944766, 0.039427751170303525, 0.039427751170303525]
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.2584999728664651, 0.2584999728664651, 0.14564458452454404, 0.2584999728664651, 0.03942774843803048, 0.03942774843803048]
printing an ep nov before normalisation:  39.77369884988613
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.2501117685871938, 0.2501117685871938, 0.1749037368133282, 0.2501117685871938, 0.03738047871254527, 0.03738047871254527]
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.2501117685871938, 0.2501117685871938, 0.1749037368133282, 0.2501117685871938, 0.03738047871254527, 0.03738047871254527]
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.2501117685871938, 0.2501117685871938, 0.1749037368133282, 0.2501117685871938, 0.03738047871254527, 0.03738047871254527]
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.2501117685871938, 0.2501117685871938, 0.1749037368133282, 0.2501117685871938, 0.03738047871254527, 0.03738047871254527]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
printing an ep nov before normalisation:  70.53705635731231
UNIT TEST: sample policy line 217 mcts : [0.184 0.49  0.041 0.082 0.122 0.041 0.041]
printing an ep nov before normalisation:  43.076537004805274
printing an ep nov before normalisation:  38.899012364753816
printing an ep nov before normalisation:  16.46672344398221
printing an ep nov before normalisation:  44.262633598675464
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
probs:  [0.2584261618847827, 0.2584261618847827, 0.2584261618847827, 0.14573906249411367, 0.03949122592576916, 0.03949122592576916]
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
probs:  [0.2584261618847827, 0.2584261618847827, 0.2584261618847827, 0.14573906249411367, 0.03949122592576916, 0.03949122592576916]
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
probs:  [0.2584261618847827, 0.2584261618847827, 0.2584261618847827, 0.14573906249411367, 0.03949122592576916, 0.03949122592576916]
printing an ep nov before normalisation:  73.19914382472479
printing an ep nov before normalisation:  37.20313283784054
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
probs:  [0.29125846543878603, 0.16424740843808167, 0.29125846543878603, 0.16424740843808167, 0.0444941261231323, 0.0444941261231323]
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
probs:  [0.29125846543878603, 0.16424740843808167, 0.29125846543878603, 0.16424740843808167, 0.0444941261231323, 0.0444941261231323]
siam score:  -0.60397834
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
probs:  [0.33090063686963095, 0.050534696104737944, 0.33090063686963095, 0.1865946379465241, 0.050534696104737944, 0.050534696104737944]
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
probs:  [0.33090063686963095, 0.050534696104737944, 0.33090063686963095, 0.1865946379465241, 0.050534696104737944, 0.050534696104737944]
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
probs:  [0.33090063686963095, 0.050534696104737944, 0.33090063686963095, 0.1865946379465241, 0.050534696104737944, 0.050534696104737944]
printing an ep nov before normalisation:  14.002558648080212
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
printing an ep nov before normalisation:  34.131879806518555
printing an ep nov before normalisation:  33.738126180543304
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
probs:  [0.33090063686963095, 0.050534696104737944, 0.33090063686963095, 0.1865946379465241, 0.050534696104737944, 0.050534696104737944]
printing an ep nov before normalisation:  33.92801943169616
from probs:  [0.33090063686963095, 0.050534696104737944, 0.33090063686963095, 0.1865946379465241, 0.050534696104737944, 0.050534696104737944]
printing an ep nov before normalisation:  41.64480512123212
printing an ep nov before normalisation:  30.41798887738938
printing an ep nov before normalisation:  22.616196066809433
maxi score, test score, baseline:  -0.986912987012987 -1.0 -0.986912987012987
probs:  [0.383040688522892, 0.058479655738554015, 0.383040688522892, 0.058479655738554015, 0.058479655738554015, 0.058479655738554015]
maxi score, test score, baseline:  -0.986912987012987 -1.0 -0.986912987012987
probs:  [0.383040688522892, 0.058479655738554015, 0.383040688522892, 0.058479655738554015, 0.058479655738554015, 0.058479655738554015]
maxi score, test score, baseline:  -0.986912987012987 -1.0 -0.986912987012987
maxi score, test score, baseline:  -0.9870794871794872 -1.0 -0.9870794871794872
maxi score, test score, baseline:  -0.9870794871794872 -1.0 -0.9870794871794872
probs:  [0.4599093397909488, 0.07019269277235893, 0.2593198891196154, 0.07019269277235893, 0.07019269277235893, 0.07019269277235893]
siam score:  -0.61629844
Printing some Q and Qe and total Qs values:  [[1.136]
 [0.38 ]
 [1.079]
 [1.052]
 [1.035]
 [1.073]
 [1.133]] [[31.789]
 [31.477]
 [31.409]
 [30.91 ]
 [31.207]
 [31.019]
 [39.846]] [[1.48 ]
 [0.718]
 [1.416]
 [1.379]
 [1.367]
 [1.402]
 [1.633]]
maxi score, test score, baseline:  -0.9870794871794872 -1.0 -0.9870794871794872
probs:  [0.4599093397909488, 0.07019269277235893, 0.2593198891196154, 0.07019269277235893, 0.07019269277235893, 0.07019269277235893]
maxi score, test score, baseline:  -0.9870794871794872 -1.0 -0.9870794871794872
probs:  [0.4599093397909488, 0.07019269277235893, 0.2593198891196154, 0.07019269277235893, 0.07019269277235893, 0.07019269277235893]
maxi score, test score, baseline:  -0.9870794871794872 -1.0 -0.9870794871794872
probs:  [0.3225925902785783, 0.1344670796812597, 0.22576328335348764, 0.1344670796812597, 0.0482428873241548, 0.1344670796812597]
siam score:  -0.61431366
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9870794871794872 -1.0 -0.9870794871794872
probs:  [0.35304660502938007, 0.05278650577175953, 0.24707245235022035, 0.1471539655384403, 0.05278650577175953, 0.1471539655384403]
actions average: 
K:  2  action  0 :  tensor([0.3147, 0.0100, 0.1079, 0.1114, 0.1847, 0.0969, 0.1744],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0622, 0.7156, 0.0332, 0.0451, 0.0440, 0.0342, 0.0655],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1449, 0.0073, 0.4050, 0.1025, 0.1097, 0.1065, 0.1240],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1515, 0.1552, 0.1738, 0.1231, 0.1618, 0.0967, 0.1379],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1857, 0.0517, 0.1345, 0.1181, 0.2549, 0.1091, 0.1461],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1243, 0.0771, 0.1088, 0.1057, 0.0998, 0.3563, 0.1279],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1649, 0.1122, 0.1602, 0.1129, 0.1470, 0.0862, 0.2168],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.363]
 [0.247]
 [0.288]
 [0.346]
 [0.333]
 [0.313]
 [0.308]] [[43.169]
 [28.384]
 [28.214]
 [32.15 ]
 [35.326]
 [39.778]
 [43.563]] [[0.363]
 [0.247]
 [0.288]
 [0.346]
 [0.333]
 [0.313]
 [0.308]]
maxi score, test score, baseline:  -0.9870794871794872 -1.0 -0.9870794871794872
probs:  [0.35304660502938007, 0.05278650577175953, 0.24707245235022035, 0.1471539655384403, 0.05278650577175953, 0.1471539655384403]
siam score:  -0.61498415
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.877883501660385
printing an ep nov before normalisation:  16.677121626586743
maxi score, test score, baseline:  -0.9872417721518988 -1.0 -0.9872417721518988
probs:  [0.3530466086184182, 0.05278650357881849, 0.24707245389855975, 0.14715396516269247, 0.05278650357881849, 0.14715396516269247]
printing an ep nov before normalisation:  74.69670191826638
printing an ep nov before normalisation:  42.56172774140125
printing an ep nov before normalisation:  42.01265827210655
maxi score, test score, baseline:  -0.9872417721518988 -1.0 -0.9872417721518988
probs:  [0.3530466086184182, 0.05278650357881849, 0.24707245389855975, 0.14715396516269247, 0.05278650357881849, 0.14715396516269247]
maxi score, test score, baseline:  -0.9872417721518988 -1.0 -0.9872417721518988
probs:  [0.3530466086184182, 0.05278650357881849, 0.24707245389855975, 0.14715396516269247, 0.05278650357881849, 0.14715396516269247]
printing an ep nov before normalisation:  24.98693871797837
printing an ep nov before normalisation:  16.032817363739014
maxi score, test score, baseline:  -0.9872417721518988 -1.0 -0.9872417721518988
probs:  [0.3530466086184182, 0.05278650357881849, 0.24707245389855975, 0.14715396516269247, 0.05278650357881849, 0.14715396516269247]
main train batch thing paused
printing an ep nov before normalisation:  24.252243041992188
Printing some Q and Qe and total Qs values:  [[0.993]
 [0.993]
 [1.019]
 [0.993]
 [1.062]
 [0.993]
 [1.121]] [[60.19 ]
 [60.19 ]
 [66.081]
 [60.19 ]
 [67.166]
 [60.19 ]
 [68.31 ]] [[1.208]
 [1.208]
 [1.274]
 [1.208]
 [1.325]
 [1.208]
 [1.392]]
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
probs:  [0.333420279639655, 0.05101496734669063, 0.1881832618889879, 0.1881832618889879, 0.05101496734669063, 0.1881832618889879]
printing an ep nov before normalisation:  26.95954850996291
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
probs:  [0.333420279639655, 0.05101496734669063, 0.1881832618889879, 0.1881832618889879, 0.05101496734669063, 0.1881832618889879]
printing an ep nov before normalisation:  32.31465665749283
printing an ep nov before normalisation:  16.061524152755737
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
probs:  [0.2311185060212609, 0.03776298795747818, 0.2311185060212609, 0.2311185060212609, 0.03776298795747818, 0.2311185060212609]
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
probs:  [0.2311185060212609, 0.03776298795747818, 0.2311185060212609, 0.2311185060212609, 0.03776298795747818, 0.2311185060212609]
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.320138454437256
printing an ep nov before normalisation:  64.77233929646862
printing an ep nov before normalisation:  40.60399329168591
printing an ep nov before normalisation:  31.935395227754018
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
probs:  [0.2311185060212609, 0.03776298795747818, 0.2311185060212609, 0.2311185060212609, 0.03776298795747818, 0.2311185060212609]
printing an ep nov before normalisation:  26.494235606195303
printing an ep nov before normalisation:  15.577797889709473
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
probs:  [0.061521503800328496, 0.061521503800328496, 0.061521503800328496, 0.37695699239934305, 0.061521503800328496, 0.37695699239934305]
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
Printing some Q and Qe and total Qs values:  [[-0.038]
 [-0.136]
 [-0.073]
 [-0.048]
 [-0.029]
 [-0.034]
 [-0.048]] [[26.259]
 [35.177]
 [28.577]
 [35.991]
 [35.957]
 [33.708]
 [35.464]] [[1.011]
 [1.376]
 [1.097]
 [1.506]
 [1.523]
 [1.402]
 [1.479]]
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
probs:  [0.14680539929102004, 0.14680539929102004, 0.14680539929102004, 0.25986184435239307, 0.0398601134221538, 0.25986184435239307]
actions average: 
K:  0  action  0 :  tensor([0.2389, 0.0583, 0.1404, 0.1246, 0.1470, 0.1096, 0.1813],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0206, 0.8449, 0.0124, 0.0433, 0.0210, 0.0301, 0.0277],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1242, 0.0189, 0.4008, 0.0920, 0.0933, 0.1172, 0.1536],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1654, 0.0563, 0.1589, 0.1662, 0.1606, 0.1289, 0.1637],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1773, 0.0499, 0.1127, 0.1093, 0.3427, 0.0923, 0.1157],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1487, 0.0310, 0.1288, 0.1063, 0.1257, 0.3307, 0.1288],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1781, 0.0101, 0.1594, 0.1317, 0.1663, 0.1025, 0.2518],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  51.78243506176956
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
probs:  [0.14680539929102004, 0.14680539929102004, 0.14680539929102004, 0.25986184435239307, 0.0398601134221538, 0.25986184435239307]
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
probs:  [0.04462175843150227, 0.16438545342862623, 0.16438545342862623, 0.29099278813987156, 0.04462175843150227, 0.29099278813987156]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
printing an ep nov before normalisation:  57.47736609001869
printing an ep nov before normalisation:  71.0266998440884
printing an ep nov before normalisation:  72.29392698184039
maxi score, test score, baseline:  -0.9875543209876543 -1.0 -0.9875543209876543
probs:  [0.044621755436038066, 0.1643854533726359, 0.1643854533726359, 0.2909927911913261, 0.044621755436038066, 0.2909927911913261]
line 256 mcts: sample exp_bonus 40.444051874423
maxi score, test score, baseline:  -0.9875543209876543 -1.0 -0.9875543209876543
probs:  [0.044621755436038066, 0.1643854533726359, 0.1643854533726359, 0.2909927911913261, 0.044621755436038066, 0.2909927911913261]
printing an ep nov before normalisation:  57.32840488631081
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9875543209876543 -1.0 -0.9875543209876543
printing an ep nov before normalisation:  64.70647399172233
printing an ep nov before normalisation:  28.653742572650742
maxi score, test score, baseline:  -0.9875543209876543 -1.0 -0.9875543209876543
probs:  [0.05918107249368336, 0.21813864134105346, 0.21813864134105346, 0.3861794998368432, 0.05918107249368336, 0.05918107249368336]
maxi score, test score, baseline:  -0.9875543209876543 -1.0 -0.9875543209876543
probs:  [0.05918107249368336, 0.21813864134105346, 0.21813864134105346, 0.3861794998368432, 0.05918107249368336, 0.05918107249368336]
printing an ep nov before normalisation:  68.7037816227258
maxi score, test score, baseline:  -0.9875543209876543 -1.0 -0.9875543209876543
probs:  [0.05918107249368336, 0.21813864134105346, 0.21813864134105346, 0.3861794998368432, 0.05918107249368336, 0.05918107249368336]
maxi score, test score, baseline:  -0.9875543209876543 -1.0 -0.9875543209876543
probs:  [0.05918107249368336, 0.21813864134105346, 0.21813864134105346, 0.3861794998368432, 0.05918107249368336, 0.05918107249368336]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.797]
 [0.797]
 [0.797]
 [0.797]
 [0.797]
 [0.797]
 [0.797]] [[21.173]
 [21.173]
 [21.173]
 [21.173]
 [21.173]
 [21.173]
 [21.173]] [[0.797]
 [0.797]
 [0.797]
 [0.797]
 [0.797]
 [0.797]
 [0.797]]
line 256 mcts: sample exp_bonus 9.725088611939059
using explorer policy with actor:  0
printing an ep nov before normalisation:  59.88331446171518
from probs:  [0.04693072493627495, 0.2864026083970584, 0.2864026083970584, 0.2864026083970584, 0.04693072493627495, 0.04693072493627495]
Printing some Q and Qe and total Qs values:  [[0.766]
 [0.415]
 [0.752]
 [0.76 ]
 [0.764]
 [0.764]
 [0.749]] [[22.112]
 [15.059]
 [10.903]
 [11.019]
 [12.016]
 [11.919]
 [11.577]] [[0.766]
 [0.415]
 [0.752]
 [0.76 ]
 [0.764]
 [0.764]
 [0.749]]
printing an ep nov before normalisation:  12.069572825634003
printing an ep nov before normalisation:  42.038670149322925
maxi score, test score, baseline:  -0.9875543209876543 -1.0 -0.9875543209876543
probs:  [0.0616816277322325, 0.0616816277322325, 0.3766367445355349, 0.3766367445355349, 0.0616816277322325, 0.0616816277322325]
printing an ep nov before normalisation:  16.13659977912903
printing an ep nov before normalisation:  3.117644047871835
maxi score, test score, baseline:  -0.9875543209876543 -1.0 -0.9875543209876543
probs:  [0.0616816277322325, 0.0616816277322325, 0.3766367445355349, 0.3766367445355349, 0.0616816277322325, 0.0616816277322325]
printing an ep nov before normalisation:  27.254467159737672
printing an ep nov before normalisation:  12.311638922036128
line 256 mcts: sample exp_bonus 19.44583958387375
line 256 mcts: sample exp_bonus 19.715351610386957
printing an ep nov before normalisation:  28.37008467863999
maxi score, test score, baseline:  -0.9881352941176471 -1.0 -0.9881352941176471
maxi score, test score, baseline:  -0.9881352941176471 -1.0 -0.9881352941176471
probs:  [0.09001205092979526, 0.09001205092979526, 0.09001205092979526, 0.5499397453510236, 0.09001205092979526, 0.09001205092979526]
maxi score, test score, baseline:  -0.9881352941176471 -1.0 -0.9881352941176471
using explorer policy with actor:  0
printing an ep nov before normalisation:  11.747584342956543
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]] [[26.606]
 [20.926]
 [20.926]
 [20.926]
 [20.926]
 [20.926]
 [20.926]] [[1.617]
 [1.449]
 [1.449]
 [1.449]
 [1.449]
 [1.449]
 [1.449]]
printing an ep nov before normalisation:  16.043668788560023
printing an ep nov before normalisation:  21.345775220157446
maxi score, test score, baseline:  -0.9881352941176471 -1.0 -0.9881352941176471
probs:  [0.1655503194503489, 0.1655503194503489, 0.1655503194503489, 0.29281390211057157, 0.1655503194503489, 0.0449848200880329]
printing an ep nov before normalisation:  24.92581695150735
using explorer policy with actor:  0
using explorer policy with actor:  0
printing an ep nov before normalisation:  21.95965051651001
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.1655503194245961, 0.1655503194245961, 0.1655503194245961, 0.29281390502062266, 0.1655503194245961, 0.04498481728099289]
printing an ep nov before normalisation:  16.67068362236023
printing an ep nov before normalisation:  34.62352294581657
Printing some Q and Qe and total Qs values:  [[0.682]
 [0.633]
 [0.686]
 [0.693]
 [0.68 ]
 [0.686]
 [0.704]] [[52.829]
 [43.149]
 [56.141]
 [55.982]
 [55.853]
 [57.912]
 [57.97 ]] [[0.682]
 [0.633]
 [0.686]
 [0.693]
 [0.68 ]
 [0.686]
 [0.704]]
printing an ep nov before normalisation:  18.69109791424269
printing an ep nov before normalisation:  31.76037120430979
printing an ep nov before normalisation:  47.00078962348778
printing an ep nov before normalisation:  0.0
Printing some Q and Qe and total Qs values:  [[0.646]
 [0.613]
 [0.659]
 [0.667]
 [0.668]
 [0.669]
 [0.671]] [[42.459]
 [24.157]
 [24.674]
 [31.271]
 [28.179]
 [32.719]
 [34.099]] [[0.646]
 [0.613]
 [0.659]
 [0.667]
 [0.668]
 [0.669]
 [0.671]]
printing an ep nov before normalisation:  56.56651904543723
printing an ep nov before normalisation:  56.35046600486372
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.18824870012159806, 0.05113931111379908, 0.18824870012159806, 0.33297527740760763, 0.18824870012159806, 0.05113931111379908]
printing an ep nov before normalisation:  47.9871990016874
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
using explorer policy with actor:  0
printing an ep nov before normalisation:  49.88406971915217
printing an ep nov before normalisation:  22.631879487044273
UNIT TEST: sample policy line 217 mcts : [0.061 0.02  0.041 0.245 0.143 0.306 0.184]
Printing some Q and Qe and total Qs values:  [[0.799]
 [0.799]
 [0.799]
 [0.785]
 [0.582]
 [0.759]
 [0.799]] [[26.452]
 [26.452]
 [26.452]
 [49.484]
 [43.194]
 [59.789]
 [26.452]] [[1.069]
 [1.069]
 [1.069]
 [1.642]
 [1.279]
 [1.879]
 [1.069]]
printing an ep nov before normalisation:  26.065828800201416
printing an ep nov before normalisation:  44.791048280411665
printing an ep nov before normalisation:  53.8420776776414
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.18824870012159806, 0.05113931111379908, 0.18824870012159806, 0.33297527740760763, 0.18824870012159806, 0.05113931111379908]
printing an ep nov before normalisation:  56.758675113042386
printing an ep nov before normalisation:  31.531354578922535
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.21816710794936406, 0.05925146056275488, 0.05925146056275488, 0.3859114024130072, 0.21816710794936406, 0.05925146056275488]
Printing some Q and Qe and total Qs values:  [[0.575]
 [0.589]
 [0.583]
 [0.582]
 [0.584]
 [0.584]
 [0.583]] [[24.892]
 [20.485]
 [22.263]
 [22.617]
 [22.635]
 [22.924]
 [23.755]] [[0.575]
 [0.589]
 [0.583]
 [0.582]
 [0.584]
 [0.584]
 [0.583]]
printing an ep nov before normalisation:  47.61599840796605
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
printing an ep nov before normalisation:  29.893419343638712
printing an ep nov before normalisation:  23.857660872027473
printing an ep nov before normalisation:  71.03298971528802
using explorer policy with actor:  0
printing an ep nov before normalisation:  17.36580009956215
printing an ep nov before normalisation:  25.466134343227907
maxi score, test score, baseline:  -0.9884057471264368 -1.0 -0.9884057471264368
printing an ep nov before normalisation:  23.06386795223196
using explorer policy with actor:  0
printing an ep nov before normalisation:  35.6597066591818
printing an ep nov before normalisation:  59.70583458297098
using explorer policy with actor:  0
printing an ep nov before normalisation:  61.701462723564696
actions average: 
K:  1  action  0 :  tensor([0.2666, 0.0108, 0.1199, 0.1231, 0.2013, 0.1254, 0.1529],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0875, 0.5759, 0.0595, 0.0668, 0.0740, 0.0486, 0.0877],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1432, 0.0329, 0.2826, 0.1204, 0.1409, 0.1482, 0.1319],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1810, 0.0571, 0.1440, 0.1617, 0.1415, 0.1466, 0.1682],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2112, 0.0135, 0.1274, 0.1232, 0.2434, 0.1241, 0.1571],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1490, 0.0248, 0.1243, 0.1296, 0.1376, 0.2917, 0.1430],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1624, 0.0351, 0.1612, 0.1510, 0.1408, 0.1375, 0.2121],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9884057471264368 -1.0 -0.9884057471264368
probs:  [0.20690247132430875, 0.12345923684967133, 0.04429514260450225, 0.2949814410475375, 0.20690247132430875, 0.12345923684967133]
printing an ep nov before normalisation:  25.218381841963236
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.355]
 [0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]] [[62.965]
 [59.478]
 [41.675]
 [41.675]
 [41.675]
 [41.675]
 [41.675]] [[1.391]
 [1.259]
 [0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.585]]
printing an ep nov before normalisation:  48.08177472011185
printing an ep nov before normalisation:  43.73825742037369
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9884057471264368 -1.0 -0.9884057471264368
probs:  [0.20690247132430875, 0.12345923684967133, 0.04429514260450225, 0.2949814410475375, 0.20690247132430875, 0.12345923684967133]
printing an ep nov before normalisation:  57.42266294788951
printing an ep nov before normalisation:  75.7841323017156
siam score:  -0.70298266
printing an ep nov before normalisation:  35.730820377354334
printing an ep nov before normalisation:  45.79032760613162
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]] [[40.079]
 [39.685]
 [39.685]
 [39.685]
 [39.685]
 [39.685]
 [39.685]] [[0.586]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]]
printing an ep nov before normalisation:  53.31227716163971
maxi score, test score, baseline:  -0.9885363636363637 -1.0 -0.9885363636363637
probs:  [0.2333303318562272, 0.13203619124351845, 0.035936621944281516, 0.2333303318562272, 0.2333303318562272, 0.13203619124351845]
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]] [[43.874]
 [44.8  ]
 [44.8  ]
 [44.8  ]
 [44.8  ]
 [44.8  ]
 [44.8  ]] [[0.598]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.6473515665316
printing an ep nov before normalisation:  35.549889622895975
printing an ep nov before normalisation:  40.91935074817147
maxi score, test score, baseline:  -0.989147311827957 -1.0 -0.989147311827957
maxi score, test score, baseline:  -0.989147311827957 -1.0 -0.989147311827957
probs:  [0.2349515818408584, 0.1649157714057899, 0.035349522100913695, 0.1649157714057899, 0.2349515818408584, 0.1649157714057899]
printing an ep nov before normalisation:  53.1858925694738
maxi score, test score, baseline:  -0.9892617021276596 -1.0 -0.9892617021276596
probs:  [0.17733625786699492, 0.17733625786699492, 0.03800394925094428, 0.17733625786699492, 0.252651019281076, 0.17733625786699492]
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.052381095659605
Printing some Q and Qe and total Qs values:  [[0.585]
 [0.563]
 [0.585]
 [0.585]
 [0.581]
 [0.577]
 [0.574]] [[33.121]
 [37.151]
 [32.204]
 [33.009]
 [34.176]
 [34.485]
 [35.138]] [[0.585]
 [0.563]
 [0.585]
 [0.585]
 [0.581]
 [0.577]
 [0.574]]
Printing some Q and Qe and total Qs values:  [[0.588]
 [0.57 ]
 [0.583]
 [0.585]
 [0.583]
 [0.58 ]
 [0.575]] [[20.609]
 [27.009]
 [22.165]
 [24.236]
 [23.933]
 [23.971]
 [24.787]] [[0.588]
 [0.57 ]
 [0.583]
 [0.585]
 [0.583]
 [0.58 ]
 [0.575]]
maxi score, test score, baseline:  -0.9894833333333334 -1.0 -0.9894833333333334
probs:  [0.11402839154263596, 0.190983920535292, 0.04092063899961152, 0.190983920535292, 0.2720992078518763, 0.190983920535292]
printing an ep nov before normalisation:  38.80134105682373
printing an ep nov before normalisation:  61.07092755558906
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.311475317256495
siam score:  -0.7639923
actions average: 
K:  3  action  0 :  tensor([0.2425, 0.0539, 0.1161, 0.1254, 0.1366, 0.1350, 0.1905],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.1006, 0.3682, 0.0914, 0.0903, 0.1049, 0.1095, 0.1352],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.2162, 0.0152, 0.1480, 0.1248, 0.1646, 0.1465, 0.1847],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1581, 0.1184, 0.1386, 0.1278, 0.1427, 0.1452, 0.1691],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1923, 0.0580, 0.1283, 0.1210, 0.1780, 0.1428, 0.1796],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1961, 0.0302, 0.1351, 0.0947, 0.1134, 0.2956, 0.1349],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1900, 0.0373, 0.1504, 0.1268, 0.1323, 0.1569, 0.2062],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9896959183673469 -1.0 -0.9896959183673469
probs:  [0.15677675077658423, 0.20106761329297654, 0.03655869537494774, 0.15677675077658423, 0.24775257648593055, 0.20106761329297654]
printing an ep nov before normalisation:  35.51270861406237
Printing some Q and Qe and total Qs values:  [[0.143]
 [0.147]
 [0.147]
 [0.147]
 [0.156]
 [0.147]
 [0.156]] [[55.429]
 [46.682]
 [46.682]
 [46.682]
 [55.775]
 [46.682]
 [64.679]] [[1.304]
 [0.997]
 [0.997]
 [0.997]
 [1.329]
 [0.997]
 [1.645]]
maxi score, test score, baseline:  -0.9896959183673469 -1.0 -0.9896959183673469
probs:  [0.16404222962491988, 0.16404222962491988, 0.038248190510281825, 0.16404222962491988, 0.2592377186846458, 0.21038740193031277]
printing an ep nov before normalisation:  27.634402167308266
printing an ep nov before normalisation:  32.64426275945704
printing an ep nov before normalisation:  115.66597569208034
maxi score, test score, baseline:  -0.9896959183673469 -1.0 -0.9896959183673469
probs:  [0.17201446806210002, 0.17201446806210002, 0.04010203364140937, 0.17201446806210002, 0.27184009411019044, 0.17201446806210002]
printing an ep nov before normalisation:  0.40129401990043334
printing an ep nov before normalisation:  43.909733636038645
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
probs:  [0.1720144681014089, 0.1720144681014089, 0.04010203271109971, 0.1720144681014089, 0.27184009488326466, 0.1720144681014089]
using explorer policy with actor:  0
printing an ep nov before normalisation:  14.950792181439352
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.17201446825089933, 0.17201446825089933, 0.04010202917315996, 0.17201446825089933, 0.27184009782324264, 0.17201446825089933]
Printing some Q and Qe and total Qs values:  [[-0.011]
 [-0.012]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]] [[34.19 ]
 [40.038]
 [30.047]
 [30.047]
 [30.047]
 [30.047]
 [30.047]] [[0.804]
 [1.047]
 [0.622]
 [0.622]
 [0.622]
 [0.622]
 [0.622]]
line 256 mcts: sample exp_bonus 51.00456354360285
using explorer policy with actor:  1
STARTED EXPV TRAINING ON FRAME NO.  10933
printing an ep nov before normalisation:  60.163720056092274
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  10933
printing an ep nov before normalisation:  42.27063998328974
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.16313662021593583, 0.16313662021593583, 0.05195847905831398, 0.16313662021593583, 0.35243994164648107, 0.10619171864739746]
line 256 mcts: sample exp_bonus 0.029341665543711315
printing an ep nov before normalisation:  71.24307475992747
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.69532433540176
using another actor
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.0936364407575652, 0.1863655561852474, 0.05050661962841063, 0.1863655561852474, 0.3442556716431925, 0.1388701556003369]
printing an ep nov before normalisation:  21.963014602661133
siam score:  -0.82214797
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.0936364407575652, 0.1863655561852474, 0.05050661962841063, 0.1863655561852474, 0.3442556716431925, 0.1388701556003369]
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.373]
 [0.373]
 [0.373]
 [0.373]
 [0.373]
 [0.377]] [[54.906]
 [54.906]
 [54.906]
 [54.906]
 [54.906]
 [54.906]
 [59.235]] [[1.541]
 [1.541]
 [1.541]
 [1.541]
 [1.541]
 [1.541]
 [1.66 ]]
printing an ep nov before normalisation:  54.23162074473377
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
probs:  [0.048771355765903183, 0.2093627730430069, 0.048771355765903183, 0.2093627730430069, 0.33051068432222513, 0.1532210580599545]
printing an ep nov before normalisation:  55.561381604720154
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
probs:  [0.048771355765903183, 0.2093627730430069, 0.048771355765903183, 0.2093627730430069, 0.33051068432222513, 0.1532210580599545]
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
probs:  [0.051666856678365874, 0.16233460671459077, 0.051666856678365874, 0.2218185223590618, 0.3501785508550249, 0.16233460671459077]
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
probs:  [0.05492892706363917, 0.17260192766174676, 0.05492892706363917, 0.17260192766174676, 0.37233636288748134, 0.17260192766174676]
printing an ep nov before normalisation:  33.72032295947878
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
probs:  [0.05492892706363917, 0.17260192766174676, 0.05492892706363917, 0.17260192766174676, 0.37233636288748134, 0.17260192766174676]
printing an ep nov before normalisation:  29.42019114258457
printing an ep nov before normalisation:  32.585509147262606
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.050297959223706536, 0.1871335247596496, 0.050297959223706536, 0.1871335247596496, 0.3380035072736382, 0.1871335247596496]
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.05029795800116855, 0.1871335249746688, 0.05029795800116855, 0.1871335249746688, 0.3380035090736565, 0.1871335249746688]
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.05407976202392613, 0.2012310485228758, 0.05407976202392613, 0.12590360424365146, 0.36347477466274464, 0.2012310485228758]
printing an ep nov before normalisation:  46.750617027282715
printing an ep nov before normalisation:  57.11392455335194
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.05407976202392613, 0.2012310485228758, 0.05407976202392613, 0.12590360424365146, 0.36347477466274464, 0.2012310485228758]
printing an ep nov before normalisation:  36.395466785724494
printing an ep nov before normalisation:  66.62801687242236
using explorer policy with actor:  1
printing an ep nov before normalisation:  79.49886887675731
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.04400329136558527, 0.22789144260137248, 0.10244836268858737, 0.10244836268858737, 0.295317098054495, 0.22789144260137248]
printing an ep nov before normalisation:  37.257890211094136
from probs:  [0.04400329136558527, 0.22789144260137248, 0.10244836268858737, 0.10244836268858737, 0.295317098054495, 0.22789144260137248]
printing an ep nov before normalisation:  70.12483625642675
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.37286708726928
printing an ep nov before normalisation:  56.68533019078986
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
actions average: 
K:  1  action  0 :  tensor([0.2431, 0.0174, 0.1389, 0.1364, 0.1549, 0.1330, 0.1763],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0311, 0.7902, 0.0295, 0.0402, 0.0256, 0.0305, 0.0530],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1780, 0.0387, 0.1920, 0.1391, 0.1415, 0.1430, 0.1677],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1851, 0.0868, 0.1599, 0.1357, 0.1465, 0.1287, 0.1573],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2165, 0.0049, 0.1644, 0.1421, 0.1860, 0.1360, 0.1501],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1841, 0.0092, 0.1323, 0.1113, 0.1206, 0.3098, 0.1328],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2025, 0.0361, 0.1513, 0.1474, 0.1474, 0.1275, 0.1878],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]] [[35.153]
 [35.153]
 [35.153]
 [35.153]
 [35.153]
 [35.153]
 [35.153]] [[1.19]
 [1.19]
 [1.19]
 [1.19]
 [1.19]
 [1.19]
 [1.19]]
printing an ep nov before normalisation:  19.77407217025757
printing an ep nov before normalisation:  26.415926962490786
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.048060665960585655, 0.31806373815619393, 0.13387559588322043, 0.048060665960585655, 0.13387559588322043, 0.31806373815619393]
printing an ep nov before normalisation:  22.137272357940674
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.048060665960585655, 0.31806373815619393, 0.13387559588322043, 0.048060665960585655, 0.13387559588322043, 0.31806373815619393]
printing an ep nov before normalisation:  89.86424577657007
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.053054427793085196, 0.24707933299243098, 0.1478107768439279, 0.053054427793085196, 0.1478107768439279, 0.3511902577335428]
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.053054427793085196, 0.24707933299243098, 0.1478107768439279, 0.053054427793085196, 0.1478107768439279, 0.3511902577335428]
printing an ep nov before normalisation:  19.66681957244873
printing an ep nov before normalisation:  37.43486287067588
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.053054427793085196, 0.24707933299243098, 0.1478107768439279, 0.053054427793085196, 0.1478107768439279, 0.3511902577335428]
printing an ep nov before normalisation:  29.943561309738076
Printing some Q and Qe and total Qs values:  [[0.175]
 [0.137]
 [0.171]
 [0.168]
 [0.167]
 [0.168]
 [0.167]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.175]
 [0.137]
 [0.171]
 [0.168]
 [0.167]
 [0.168]
 [0.167]]
Printing some Q and Qe and total Qs values:  [[-0.006]
 [-0.004]
 [ 0.   ]
 [-0.007]
 [-0.011]
 [-0.013]
 [-0.022]] [[35.385]
 [37.517]
 [34.761]
 [36.726]
 [36.521]
 [38.647]
 [41.804]] [[1.243]
 [1.403]
 [1.204]
 [1.342]
 [1.323]
 [1.477]
 [1.702]]
printing an ep nov before normalisation:  98.9444218014141
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
probs:  [0.04789406519963863, 0.2854392681336947, 0.2854392681336947, 0.04789406519963863, 0.04789406519963863, 0.2854392681336947]
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
probs:  [0.0627897221028262, 0.37442055579434763, 0.0627897221028262, 0.0627897221028262, 0.0627897221028262, 0.37442055579434763]
printing an ep nov before normalisation:  51.215618360570204
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
probs:  [0.0627897221028262, 0.37442055579434763, 0.0627897221028262, 0.0627897221028262, 0.0627897221028262, 0.37442055579434763]
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
probs:  [0.0627897221028262, 0.37442055579434763, 0.0627897221028262, 0.0627897221028262, 0.0627897221028262, 0.37442055579434763]
Printing some Q and Qe and total Qs values:  [[0.233]
 [0.178]
 [0.235]
 [0.228]
 [0.235]
 [0.234]
 [0.234]] [[62.243]
 [52.595]
 [54.808]
 [55.992]
 [57.859]
 [59.035]
 [60.108]] [[0.233]
 [0.178]
 [0.235]
 [0.228]
 [0.235]
 [0.234]
 [0.234]]
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
probs:  [0.16573382535201012, 0.2916674028306354, 0.16573382535201012, 0.16573382535201012, 0.16573382535201012, 0.04539729576132418]
printing an ep nov before normalisation:  76.2055852589006
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
probs:  [0.23058033988256071, 0.03883932023487864, 0.23058033988256071, 0.23058033988256071, 0.23058033988256071, 0.03883932023487864]
printing an ep nov before normalisation:  47.85498754918967
printing an ep nov before normalisation:  47.04725824680433
printing an ep nov before normalisation:  14.254188537597656
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
probs:  [0.23058033988256071, 0.03883932023487864, 0.23058033988256071, 0.23058033988256071, 0.23058033988256071, 0.03883932023487864]
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.322]
 [0.309]
 [0.296]
 [0.3  ]
 [0.302]
 [0.294]] [[22.261]
 [28.445]
 [20.664]
 [21.056]
 [21.293]
 [21.209]
 [21.733]] [[0.313]
 [0.322]
 [0.309]
 [0.296]
 [0.3  ]
 [0.302]
 [0.294]]
Printing some Q and Qe and total Qs values:  [[0.339]
 [0.339]
 [0.332]
 [0.339]
 [0.339]
 [0.329]
 [0.339]] [[83.81 ]
 [83.81 ]
 [67.044]
 [83.81 ]
 [83.81 ]
 [70.25 ]
 [83.81 ]] [[0.339]
 [0.339]
 [0.332]
 [0.339]
 [0.339]
 [0.329]
 [0.339]]
printing an ep nov before normalisation:  33.01626965192414
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
probs:  [0.23058033988256071, 0.03883932023487864, 0.23058033988256071, 0.23058033988256071, 0.23058033988256071, 0.03883932023487864]
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
probs:  [0.23058033988256071, 0.03883932023487864, 0.23058033988256071, 0.23058033988256071, 0.23058033988256071, 0.03883932023487864]
printing an ep nov before normalisation:  36.20854005661707
printing an ep nov before normalisation:  37.416827109280284
line 256 mcts: sample exp_bonus 21.594284922192344
Printing some Q and Qe and total Qs values:  [[-0.051]
 [-0.031]
 [-0.053]
 [-0.056]
 [-0.059]
 [-0.058]
 [-0.059]] [[24.322]
 [23.883]
 [26.245]
 [26.398]
 [26.776]
 [26.112]
 [27.097]] [[0.401]
 [0.404]
 [0.474]
 [0.477]
 [0.489]
 [0.463]
 [0.5  ]]
printing an ep nov before normalisation:  48.11715852677718
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  10933
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.07 ]
 [-0.058]
 [-0.07 ]
 [-0.07 ]
 [-0.072]
 [-0.068]
 [-0.072]] [[27.91 ]
 [28.026]
 [27.91 ]
 [29.579]
 [30.216]
 [28.789]
 [26.895]] [[1.319]
 [1.344]
 [1.319]
 [1.487]
 [1.55 ]
 [1.409]
 [1.215]]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
deleting a thread, now have 5 threads
Frames:  12626 train batches done:  1476 episodes:  333
printing an ep nov before normalisation:  0.0003054161692261914
line 256 mcts: sample exp_bonus 26.810235643328454
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
printing an ep nov before normalisation:  32.95529918583114
printing an ep nov before normalisation:  47.64429693642896
printing an ep nov before normalisation:  46.44212334256383
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.17690785809189277, 0.03810313859642187, 0.25104219691333696, 0.17690785809189277, 0.25104219691333696, 0.10599675139311888]
printing an ep nov before normalisation:  51.48052230483416
line 256 mcts: sample exp_bonus 48.9520906086551
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.17690785809189277, 0.03810313859642187, 0.25104219691333696, 0.17690785809189277, 0.25104219691333696, 0.10599675139311888]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.17690785809189277, 0.03810313859642187, 0.25104219691333696, 0.17690785809189277, 0.25104219691333696, 0.10599675139311888]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.17690785809189277, 0.03810313859642187, 0.25104219691333696, 0.17690785809189277, 0.25104219691333696, 0.10599675139311888]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.19207503343083554, 0.03717583363963774, 0.248402015173089, 0.19207503343083554, 0.19207503343083554, 0.13819705089476655]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.14606589890271784, 0.039287050283297306, 0.26255191557844904, 0.20301461816640895, 0.20301461816640895, 0.14606589890271784]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.14606589890271784, 0.039287050283297306, 0.26255191557844904, 0.20301461816640895, 0.20301461816640895, 0.14606589890271784]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.14606589890271784, 0.039287050283297306, 0.26255191557844904, 0.20301461816640895, 0.20301461816640895, 0.14606589890271784]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.14606589890271784, 0.039287050283297306, 0.26255191557844904, 0.20301461816640895, 0.20301461816640895, 0.14606589890271784]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.14606589890271784, 0.039287050283297306, 0.26255191557844904, 0.20301461816640895, 0.20301461816640895, 0.14606589890271784]
siam score:  -0.87059945
deleting a thread, now have 4 threads
Frames:  12993 train batches done:  1514 episodes:  338
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.14606589890271784, 0.039287050283297306, 0.26255191557844904, 0.20301461816640895, 0.20301461816640895, 0.14606589890271784]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.14606589890271784, 0.039287050283297306, 0.26255191557844904, 0.20301461816640895, 0.20301461816640895, 0.14606589890271784]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.14606589890271784, 0.039287050283297306, 0.26255191557844904, 0.20301461816640895, 0.20301461816640895, 0.14606589890271784]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.14606589890271784, 0.039287050283297306, 0.26255191557844904, 0.20301461816640895, 0.20301461816640895, 0.14606589890271784]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.14606589890271784, 0.039287050283297306, 0.26255191557844904, 0.20301461816640895, 0.20301461816640895, 0.14606589890271784]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.14606589890271784, 0.039287050283297306, 0.26255191557844904, 0.20301461816640895, 0.20301461816640895, 0.14606589890271784]
siam score:  -0.8745984
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.09681643436534436, 0.041546955632290196, 0.2776983647644313, 0.21472465566252733, 0.21472465566252733, 0.15448893391287954]
printing an ep nov before normalisation:  61.64943106960076
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.106041595136006
printing an ep nov before normalisation:  49.312528694364836
siam score:  -0.87433136
printing an ep nov before normalisation:  18.907269706371196
siam score:  -0.8739743
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.038231774032125916, 0.12045207805854133, 0.20982197373942732, 0.2574859181025668, 0.20982197373942732, 0.16418628232791121]
printing an ep nov before normalisation:  55.572054552786476
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.038231774032125916, 0.12045207805854133, 0.20982197373942732, 0.2574859181025668, 0.20982197373942732, 0.16418628232791121]
printing an ep nov before normalisation:  42.84229960651836
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.038231774032125916, 0.12045207805854133, 0.20982197373942732, 0.2574859181025668, 0.20982197373942732, 0.16418628232791121]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.041805226597612294, 0.08585362952770731, 0.22948972603888643, 0.2816243092170183, 0.22948972603888643, 0.13173738257988934]
Printing some Q and Qe and total Qs values:  [[0.238]
 [0.238]
 [0.301]
 [0.322]
 [0.258]
 [0.228]
 [0.229]] [[48.158]
 [48.158]
 [55.398]
 [54.101]
 [51.665]
 [53.938]
 [60.728]] [[1.157]
 [1.157]
 [1.481]
 [1.455]
 [1.303]
 [1.355]
 [1.6  ]]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.041805226597612294, 0.08585362952770731, 0.22948972603888643, 0.2816243092170183, 0.22948972603888643, 0.13173738257988934]
printing an ep nov before normalisation:  36.520647165330736
actions average: 
K:  2  action  0 :  tensor([0.2210, 0.0141, 0.1207, 0.1429, 0.1660, 0.1573, 0.1779],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0050, 0.8755, 0.0078, 0.0234, 0.0142, 0.0635, 0.0106],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2181, 0.0038, 0.1318, 0.1511, 0.1648, 0.1565, 0.1738],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1610, 0.0435, 0.1428, 0.1546, 0.1530, 0.1842, 0.1609],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1899, 0.0167, 0.1035, 0.1341, 0.2726, 0.1418, 0.1414],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1760, 0.0500, 0.1318, 0.1686, 0.1551, 0.1544, 0.1641],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2113, 0.0579, 0.1068, 0.1500, 0.1604, 0.1486, 0.1649],
       grad_fn=<DivBackward0>)
from probs:  [0.043810924486202184, 0.08997940611650096, 0.24052880273703972, 0.2951726578067164, 0.24052880273703972, 0.08997940611650096]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.07832549029709429, 0.04196373512141946, 0.2395819697718271, 0.284375436292586, 0.2395819697718271, 0.11617139874524607]
printing an ep nov before normalisation:  30.389006932576496
printing an ep nov before normalisation:  29.43454988913395
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
printing an ep nov before normalisation:  49.944272145331674
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.08870511419678778, 0.04751652098584787, 0.27136757104530257, 0.3221071423921122, 0.22278713039410178, 0.04751652098584787]
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.08870511419678778, 0.04751652098584787, 0.27136757104530257, 0.3221071423921122, 0.22278713039410178, 0.04751652098584787]
using explorer policy with actor:  1
printing an ep nov before normalisation:  25.936924647903258
Printing some Q and Qe and total Qs values:  [[0.416]
 [0.478]
 [0.401]
 [0.468]
 [0.402]
 [0.403]
 [0.408]] [[43.396]
 [44.305]
 [44.69 ]
 [42.522]
 [42.964]
 [42.876]
 [42.012]] [[1.653]
 [1.768]
 [1.713]
 [1.655]
 [1.615]
 [1.611]
 [1.566]]
siam score:  -0.87306917
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.09418569269408679, 0.04583584661207618, 0.30860674923170045, 0.30860674923170045, 0.19692911561835985, 0.04583584661207618]
Printing some Q and Qe and total Qs values:  [[0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.364]] [[45.555]
 [45.555]
 [45.555]
 [45.555]
 [45.555]
 [45.555]
 [45.894]] [[1.678]
 [1.678]
 [1.678]
 [1.678]
 [1.678]
 [1.678]
 [1.697]]
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.09418569269408679, 0.04583584661207618, 0.30860674923170045, 0.30860674923170045, 0.19692911561835985, 0.04583584661207618]
printing an ep nov before normalisation:  86.76700463950887
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.09418569269408679, 0.04583584661207618, 0.30860674923170045, 0.30860674923170045, 0.19692911561835985, 0.04583584661207618]
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.09939306740007563, 0.04836624987447978, 0.32568591033967526, 0.32568591033967526, 0.15250261217161432, 0.04836624987447978]
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.10575491677717859, 0.051457643393795556, 0.2825098705571289, 0.34655152047740134, 0.1622684054007002, 0.051457643393795556]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  65.83767414093018
printing an ep nov before normalisation:  42.933138607737334
printing an ep nov before normalisation:  49.38448534310238
printing an ep nov before normalisation:  39.10596151369401
Printing some Q and Qe and total Qs values:  [[0.393]
 [0.447]
 [0.359]
 [0.349]
 [0.349]
 [0.343]
 [0.342]] [[19.026]
 [20.599]
 [20.525]
 [22.212]
 [21.695]
 [20.551]
 [19.518]] [[0.642]
 [0.737]
 [0.647]
 [0.682]
 [0.668]
 [0.633]
 [0.605]]
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.269]
 [0.284]
 [0.295]
 [0.306]
 [0.304]
 [0.296]] [[28.326]
 [27.048]
 [38.338]
 [30.478]
 [29.896]
 [29.365]
 [28.898]] [[0.937]
 [0.863]
 [1.407]
 [1.05 ]
 [1.034]
 [1.007]
 [0.977]]
Printing some Q and Qe and total Qs values:  [[-0.028]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.028]] [[48.619]
 [48.619]
 [48.619]
 [48.619]
 [48.619]
 [48.619]
 [48.619]] [[1.103]
 [1.103]
 [1.103]
 [1.103]
 [1.103]
 [1.103]
 [1.103]]
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.10575491677717859, 0.051457643393795556, 0.2825098705571289, 0.34655152047740134, 0.1622684054007002, 0.051457643393795556]
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.10575491677717859, 0.051457643393795556, 0.2825098705571289, 0.34655152047740134, 0.1622684054007002, 0.051457643393795556]
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.03264544612782
printing an ep nov before normalisation:  54.036896809212216
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.10575491677717859, 0.051457643393795556, 0.2825098705571289, 0.34655152047740134, 0.1622684054007002, 0.051457643393795556]
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.10575491677717859, 0.051457643393795556, 0.2825098705571289, 0.34655152047740134, 0.1622684054007002, 0.051457643393795556]
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.11729951686643962, 0.050302497561101465, 0.2596681828902827, 0.33539619673275295, 0.1870311083883218, 0.050302497561101465]
printing an ep nov before normalisation:  104.54848724304492
printing an ep nov before normalisation:  23.83306258334173
printing an ep nov before normalisation:  33.66136074066162
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.12335902553067014, 0.04439811704085369, 0.29115095607153063, 0.29115095607153063, 0.20554282824456124, 0.04439811704085369]
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
Printing some Q and Qe and total Qs values:  [[0.38 ]
 [0.38 ]
 [0.38 ]
 [0.378]
 [0.377]
 [0.38 ]
 [0.38 ]] [[55.777]
 [55.777]
 [55.777]
 [57.052]
 [56.842]
 [55.777]
 [55.754]] [[2.283]
 [2.283]
 [2.283]
 [2.364]
 [2.35 ]
 [2.283]
 [2.281]]
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.04532541951441837, 0.04532541951441837, 0.28800791381891494, 0.28800791381891494, 0.28800791381891494, 0.04532541951441837]
printing an ep nov before normalisation:  22.166814185499586
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
printing an ep nov before normalisation:  84.03322643835959
using explorer policy with actor:  1
from probs:  [0.1143927020957525, 0.1143927020957525, 0.26975064595291826, 0.1905180945857638, 0.26975064595291826, 0.04119520931689475]
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.457]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]]
maxi score, test score, baseline:  -0.9913529914529915 -1.0 -0.9913529914529915
probs:  [0.1468333892977382, 0.040583689107053074, 0.2573330774960519, 0.2573330774960519, 0.2573330774960519, 0.040583689107053074]
from probs:  [0.1468333892977382, 0.040583689107053074, 0.2573330774960519, 0.2573330774960519, 0.2573330774960519, 0.040583689107053074]
printing an ep nov before normalisation:  52.74275779724121
printing an ep nov before normalisation:  53.05268031549068
Printing some Q and Qe and total Qs values:  [[0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]] [[40.28]
 [40.28]
 [40.28]
 [40.28]
 [40.28]
 [40.28]
 [40.28]] [[1.412]
 [1.412]
 [1.412]
 [1.412]
 [1.412]
 [1.412]
 [1.412]]
printing an ep nov before normalisation:  18.869678501671032
Printing some Q and Qe and total Qs values:  [[0.247]
 [0.287]
 [0.247]
 [0.247]
 [0.247]
 [0.247]
 [0.247]] [[27.19 ]
 [33.456]
 [27.19 ]
 [27.19 ]
 [27.19 ]
 [27.19 ]
 [27.19 ]] [[1.077]
 [1.494]
 [1.077]
 [1.077]
 [1.077]
 [1.077]
 [1.077]]
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
probs:  [0.060283579613384805, 0.060283579613384805, 0.21827826335588282, 0.21827826335588282, 0.3825927344480799, 0.060283579613384805]
printing an ep nov before normalisation:  40.386109689234424
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
probs:  [0.060283579613384805, 0.060283579613384805, 0.21827826335588282, 0.21827826335588282, 0.3825927344480799, 0.060283579613384805]
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
probs:  [0.060283579613384805, 0.060283579613384805, 0.21827826335588282, 0.21827826335588282, 0.3825927344480799, 0.060283579613384805]
printing an ep nov before normalisation:  46.427420376756125
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.151254541138194
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
probs:  [0.060283579613384805, 0.060283579613384805, 0.21827826335588282, 0.21827826335588282, 0.3825927344480799, 0.060283579613384805]
Printing some Q and Qe and total Qs values:  [[0.25 ]
 [0.24 ]
 [0.204]
 [0.204]
 [0.204]
 [0.203]
 [0.204]] [[49.284]
 [53.131]
 [38.166]
 [38.166]
 [48.79 ]
 [49.707]
 [38.166]] [[0.505]
 [0.534]
 [0.348]
 [0.348]
 [0.455]
 [0.463]
 [0.348]]
printing an ep nov before normalisation:  51.78709297772827
printing an ep nov before normalisation:  32.55172280303422
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
probs:  [0.07158097463588922, 0.07158097463588922, 0.07158097463588922, 0.2592501036440029, 0.4544259978124401, 0.07158097463588922]
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
probs:  [0.07158097463588922, 0.07158097463588922, 0.07158097463588922, 0.2592501036440029, 0.4544259978124401, 0.07158097463588922]
line 256 mcts: sample exp_bonus 28.425047922726385
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.07158097137222987, 0.07158097137222987, 0.07158097137222987, 0.2592501068217762, 0.4544260076893042, 0.07158097137222987]
siam score:  -0.87587583
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.05361463291381783, 0.14888320068307223, 0.14888320068307223, 0.14888320068307223, 0.35085256435389334, 0.14888320068307223]
siam score:  -0.87368417
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.371]
 [0.363]
 [0.371]
 [0.371]
 [0.366]
 [0.371]] [[27.38 ]
 [28.038]
 [47.249]
 [28.038]
 [28.038]
 [53.242]
 [28.038]] [[0.893]
 [0.932]
 [1.673]
 [0.932]
 [0.932]
 [1.91 ]
 [0.932]]
printing an ep nov before normalisation:  44.04795402953255
using explorer policy with actor:  1
printing an ep nov before normalisation:  88.81004439654028
printing an ep nov before normalisation:  85.2184399434068
printing an ep nov before normalisation:  66.48301825939097
printing an ep nov before normalisation:  86.42668294939124
deleting a thread, now have 3 threads
Frames:  14505 train batches done:  1696 episodes:  389
printing an ep nov before normalisation:  42.72790370880145
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.09265000620268493, 0.09265000620268493, 0.09265000620268493, 0.09265000620268493, 0.5367499689865752, 0.09265000620268493]
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.1658978250124359, 0.045958526952403746, 0.1658978250124359, 0.1658978250124359, 0.29045017299785264, 0.1658978250124359]
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.1658978250124359, 0.045958526952403746, 0.1658978250124359, 0.1658978250124359, 0.29045017299785264, 0.1658978250124359]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.1658978250124359, 0.045958526952403746, 0.1658978250124359, 0.1658978250124359, 0.29045017299785264, 0.1658978250124359]
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.1658978250124359, 0.045958526952403746, 0.1658978250124359, 0.1658978250124359, 0.29045017299785264, 0.1658978250124359]
siam score:  -0.8691907
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.18850963406373553, 0.052209517506026974, 0.18850963406373553, 0.052209517506026974, 0.3300520627967395, 0.18850963406373553]
printing an ep nov before normalisation:  67.80539135648728
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.18850963406373553, 0.052209517506026974, 0.18850963406373553, 0.052209517506026974, 0.3300520627967395, 0.18850963406373553]
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.18850963406373553, 0.052209517506026974, 0.18850963406373553, 0.052209517506026974, 0.3300520627967395, 0.18850963406373553]
siam score:  -0.8669249
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.19112675266162496, 0.04143422493076527, 0.19112675266162496, 0.11489444687276117, 0.2702910702115987, 0.19112675266162496]
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.1243731170562625, 0.04484546907381631, 0.2069018083588023, 0.1243731170562625, 0.292604680096054, 0.2069018083588023]
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.663]
 [0.64 ]
 [0.652]
 [0.599]
 [0.584]
 [0.614]] [[69.716]
 [69.029]
 [70.983]
 [71.183]
 [72.516]
 [72.765]
 [71.823]] [[2.004]
 [2.012]
 [2.068]
 [2.089]
 [2.089]
 [2.085]
 [2.076]]
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.1243731170562625, 0.04484546907381631, 0.2069018083588023, 0.1243731170562625, 0.292604680096054, 0.2069018083588023]
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.1243731170562625, 0.04484546907381631, 0.2069018083588023, 0.1243731170562625, 0.292604680096054, 0.2069018083588023]
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.1243731170562625, 0.04484546907381631, 0.2069018083588023, 0.1243731170562625, 0.292604680096054, 0.2069018083588023]
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.1243731170562625, 0.04484546907381631, 0.2069018083588023, 0.1243731170562625, 0.292604680096054, 0.2069018083588023]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.1243731170562625, 0.04484546907381631, 0.2069018083588023, 0.1243731170562625, 0.292604680096054, 0.2069018083588023]
printing an ep nov before normalisation:  32.899564583434255
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.15522717299304908, 0.041819818444523454, 0.15522717299304908, 0.15522717299304908, 0.27735817019915354, 0.2151404923771758]
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]]
printing an ep nov before normalisation:  96.89359127046923
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.15522717299304908, 0.041819818444523454, 0.15522717299304908, 0.15522717299304908, 0.27735817019915354, 0.2151404923771758]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.1647381859601959, 0.044376215542886045, 0.10346300102047452, 0.1647381859601959, 0.2943587694865295, 0.2283256420297181]
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.17231016755974538, 0.0401411612189783, 0.12665178355111695, 0.17231016755974538, 0.2688952106549216, 0.21969150945549237]
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.012]
 [0.066]
 [0.029]
 [0.034]
 [0.043]
 [0.012]] [[42.58 ]
 [42.58 ]
 [51.383]
 [46.636]
 [45.124]
 [42.638]
 [42.58 ]] [[0.9  ]
 [0.9  ]
 [1.303]
 [1.078]
 [1.022]
 [0.933]
 [0.9  ]]
printing an ep nov before normalisation:  54.13382257405729
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
printing an ep nov before normalisation:  42.09520018729535
printing an ep nov before normalisation:  43.94288641705768
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.18024648916723604, 0.04198528641537327, 0.08642638729990092, 0.18024648916723604, 0.2812835219474438, 0.2298118260028098]
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]] [[71.856]
 [71.856]
 [71.856]
 [71.856]
 [71.856]
 [71.856]
 [71.856]] [[0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]]
printing an ep nov before normalisation:  54.55830805019329
printing an ep nov before normalisation:  35.510694874117576
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.1391276231723212, 0.04408629453865998, 0.09075837556411852, 0.18928832439564194, 0.2953975000603588, 0.24134188226889955]
printing an ep nov before normalisation:  55.382423194395685
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.14588977935995154, 0.0393227489600199, 0.09165477281712935, 0.20213348984880405, 0.26049960450704757, 0.26049960450704757]
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.15493201095030038, 0.041753931156497996, 0.09733245248381191, 0.21466488639702877, 0.2766518326153323, 0.21466488639702877]
line 256 mcts: sample exp_bonus 46.19414477484895
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
Printing some Q and Qe and total Qs values:  [[0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.059]
 [0.053]
 [0.053]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.059]
 [0.053]
 [0.053]]
from probs:  [0.1647743672819815, 0.044400242661117965, 0.10351253600172085, 0.22830515527632578, 0.2942333314968724, 0.1647743672819815]
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.17552796622113895, 0.04729155970246093, 0.11026479504645484, 0.24320829188377405, 0.31344259209971637, 0.11026479504645484]
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.1755279663399179, 0.0472915581023211, 0.11026479429042704, 0.24320829290976118, 0.3134425940671456, 0.11026479429042704]
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.11796074363493166, 0.050586963201473205, 0.11796074363493166, 0.260194280105566, 0.3353365257881658, 0.11796074363493166]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
printing an ep nov before normalisation:  58.11027340964911
printing an ep nov before normalisation:  48.787522912534826
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.12648001372033021, 0.05423491275775839, 0.12648001372033021, 0.27899744908576013, 0.3595726979580627, 0.05423491275775839]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.12648001372033021, 0.05423491275775839, 0.12648001372033021, 0.27899744908576013, 0.3595726979580627, 0.05423491275775839]
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.437]
 [0.443]
 [0.503]
 [0.503]
 [0.503]
 [0.503]] [[56.262]
 [34.732]
 [43.392]
 [56.262]
 [56.262]
 [56.262]
 [56.262]] [[2.981]
 [1.558]
 [2.11 ]
 [2.981]
 [2.981]
 [2.981]
 [2.981]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.23677158355713
printing an ep nov before normalisation:  57.18172830022593
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.12648001372033021, 0.05423491275775839, 0.12648001372033021, 0.27899744908576013, 0.3595726979580627, 0.05423491275775839]
printing an ep nov before normalisation:  52.48893246675426
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.15463106094876633, 0.09716838242976689, 0.15463106094876633, 0.2759411600444321, 0.2759411600444321, 0.0416871755838363]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.15463106094876633, 0.09716838242976689, 0.15463106094876633, 0.2759411600444321, 0.2759411600444321, 0.0416871755838363]
printing an ep nov before normalisation:  20.53917497396469
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.16480923632403066, 0.1035609685555723, 0.16480923632403066, 0.22828471382952498, 0.29411113494633323, 0.04442471002050818]
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.16480923632403066, 0.1035609685555723, 0.16480923632403066, 0.22828471382952498, 0.29411113494633323, 0.04442471002050818]
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.16480923632403066, 0.1035609685555723, 0.16480923632403066, 0.22828471382952498, 0.29411113494633323, 0.04442471002050818]
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.16480923632403066, 0.1035609685555723, 0.16480923632403066, 0.22828471382952498, 0.29411113494633323, 0.04442471002050818]
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.6151008605957
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.16480923632403066, 0.1035609685555723, 0.16480923632403066, 0.22828471382952498, 0.29411113494633323, 0.04442471002050818]
using explorer policy with actor:  1
printing an ep nov before normalisation:  57.59471705909627
using explorer policy with actor:  1
printing an ep nov before normalisation:  29.95084285736084
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
siam score:  -0.858194
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
printing an ep nov before normalisation:  29.667952417452454
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.17598013218744588, 0.11057705144511283, 0.17598013218744588, 0.17598013218744588, 0.3140533026434828, 0.047429249349066706]
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.497]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.497]]
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.18969864107333975, 0.04419767859568301, 0.18969864107333975, 0.18969864107333975, 0.29568082361879355, 0.091025574565504]
Printing some Q and Qe and total Qs values:  [[-0.004]
 [-0.004]
 [-0.004]
 [-0.008]
 [-0.004]
 [-0.004]
 [-0.011]] [[26.359]
 [26.359]
 [26.359]
 [33.346]
 [26.359]
 [26.359]
 [32.956]] [[0.836]
 [0.836]
 [0.836]
 [1.247]
 [0.836]
 [0.836]
 [1.221]]
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
Printing some Q and Qe and total Qs values:  [[-0.039]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.037]] [[51.539]
 [51.539]
 [51.539]
 [51.539]
 [51.539]
 [51.539]
 [61.324]] [[1.099]
 [1.099]
 [1.099]
 [1.099]
 [1.099]
 [1.099]
 [1.504]]
printing an ep nov before normalisation:  5.240615309062946
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.10062026977539
printing an ep nov before normalisation:  87.86221586468838
printing an ep nov before normalisation:  19.832500005787654
printing an ep nov before normalisation:  38.28095601576018
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.23317353108562125, 0.03593987774947233, 0.23317353108562125, 0.23317353108562125, 0.16512215361876323, 0.09941737537490061]
printing an ep nov before normalisation:  25.29384136199951
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.23317353108562125, 0.03593987774947233, 0.23317353108562125, 0.23317353108562125, 0.16512215361876323, 0.09941737537490061]
siam score:  -0.8753212
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.23317353108562125, 0.03593987774947233, 0.23317353108562125, 0.23317353108562125, 0.16512215361876323, 0.09941737537490061]
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.23317353108562125, 0.03593987774947233, 0.23317353108562125, 0.23317353108562125, 0.16512215361876323, 0.09941737537490061]
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]] [[47.981]
 [47.981]
 [47.981]
 [47.981]
 [47.981]
 [47.981]
 [47.981]] [[0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]]
printing an ep nov before normalisation:  69.14546366013998
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.20687823179784479, 0.04500197043110299, 0.2921523337678252, 0.12454461610269112, 0.20687823179784479, 0.12454461610269112]
Printing some Q and Qe and total Qs values:  [[0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.486]
 [0.489]
 [0.495]] [[49.113]
 [49.113]
 [49.113]
 [49.113]
 [78.809]
 [78.565]
 [80.284]] [[0.734]
 [0.734]
 [0.734]
 [0.734]
 [1.383]
 [1.381]
 [1.426]]
siam score:  -0.8772325
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.20687823179784479, 0.04500197043110299, 0.2921523337678252, 0.12454461610269112, 0.20687823179784479, 0.12454461610269112]
printing an ep nov before normalisation:  50.183003853717544
printing an ep nov before normalisation:  50.80686109922425
printing an ep nov before normalisation:  47.531244156191185
printing an ep nov before normalisation:  60.65144317224985
printing an ep nov before normalisation:  41.63194112257646
actions average: 
K:  4  action  0 :  tensor([0.2550, 0.0524, 0.1089, 0.1303, 0.1394, 0.1392, 0.1748],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0301, 0.7639, 0.0430, 0.0396, 0.0343, 0.0403, 0.0489],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1503, 0.0204, 0.1633, 0.1330, 0.1644, 0.1805, 0.1881],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2093, 0.0233, 0.1306, 0.1731, 0.1529, 0.1438, 0.1670],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1970, 0.0091, 0.1295, 0.1293, 0.2020, 0.1565, 0.1767],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1755, 0.0483, 0.1161, 0.1346, 0.1307, 0.2417, 0.1531],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1488, 0.0250, 0.1477, 0.1619, 0.1534, 0.1328, 0.2304],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.20687823248140427, 0.04500196836291519, 0.29215233590096434, 0.12454461538665601, 0.20687823248140427, 0.12454461538665601]
printing an ep nov before normalisation:  34.85842413869473
Printing some Q and Qe and total Qs values:  [[0.119]
 [0.145]
 [0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]] [[60.614]
 [43.458]
 [60.614]
 [60.614]
 [60.614]
 [60.614]
 [60.614]] [[0.452]
 [0.294]
 [0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.20687823248140427, 0.04500196836291519, 0.29215233590096434, 0.12454461538665601, 0.20687823248140427, 0.12454461538665601]
printing an ep nov before normalisation:  41.32315453574812
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
printing an ep nov before normalisation:  26.801700592041016
printing an ep nov before normalisation:  8.33182129328236
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
printing an ep nov before normalisation:  57.7674925049284
printing an ep nov before normalisation:  49.856115586000655
actions average: 
K:  3  action  0 :  tensor([0.3704, 0.0146, 0.1012, 0.1305, 0.1219, 0.1312, 0.1301],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0442, 0.6468, 0.0418, 0.0808, 0.0498, 0.0762, 0.0604],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1932, 0.0099, 0.1319, 0.1658, 0.1663, 0.1677, 0.1652],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1654, 0.0781, 0.1218, 0.1661, 0.1391, 0.1550, 0.1744],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1923, 0.0064, 0.1259, 0.1600, 0.2113, 0.1395, 0.1646],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1781, 0.0114, 0.1192, 0.1592, 0.1433, 0.2278, 0.1609],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1665, 0.0163, 0.1334, 0.1629, 0.1451, 0.1630, 0.2128],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.1357166167607869, 0.049031130390880216, 0.31837532018308734, 0.1357166167607869, 0.2254436991436718, 0.1357166167607869]
Printing some Q and Qe and total Qs values:  [[-0.056]
 [-0.056]
 [-0.056]
 [-0.056]
 [-0.056]
 [-0.056]
 [-0.056]] [[39.837]
 [39.837]
 [39.837]
 [39.837]
 [39.837]
 [39.837]
 [44.186]] [[0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.904]]
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.1357166167607869, 0.049031130390880216, 0.31837532018308734, 0.1357166167607869, 0.2254436991436718, 0.1357166167607869]
printing an ep nov before normalisation:  69.28952256353388
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
printing an ep nov before normalisation:  33.664549089799
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.1885340515206922, 0.05255867661020604, 0.1885340515206922, 0.1885340515206922, 0.32928049221751143, 0.05255867661020604]
printing an ep nov before normalisation:  40.87057597645257
line 256 mcts: sample exp_bonus 72.49020930622042
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.21821097808041556, 0.06081674144200322, 0.06081674144200322, 0.21821097808041556, 0.38112781951315927, 0.06081674144200322]
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.419]
 [0.419]
 [0.575]
 [0.577]
 [0.419]
 [0.419]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.419]
 [0.419]
 [0.419]
 [0.575]
 [0.577]
 [0.419]
 [0.419]]
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
printing an ep nov before normalisation:  34.105484103028815
printing an ep nov before normalisation:  40.59784862431248
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.2068714370483713, 0.12458519246104083, 0.04504182269328859, 0.2068714370483713, 0.2920449182878873, 0.12458519246104083]
siam score:  -0.8833307
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.2068714370483713, 0.12458519246104083, 0.04504182269328859, 0.2068714370483713, 0.2920449182878873, 0.12458519246104083]
printing an ep nov before normalisation:  51.58105742058034
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.549]
 [0.61 ]
 [0.549]
 [0.549]
 [0.579]
 [0.557]] [[20.09 ]
 [20.09 ]
 [20.243]
 [20.09 ]
 [20.09 ]
 [21.003]
 [20.949]] [[0.549]
 [0.549]
 [0.61 ]
 [0.549]
 [0.549]
 [0.579]
 [0.557]]
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.25782262766142716, 0.14770133249997128, 0.041250747177231754, 0.14770133249997128, 0.25782262766142716, 0.14770133249997128]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
printing an ep nov before normalisation:  60.78284149150501
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.25782262766142716, 0.14770133249997128, 0.041250747177231754, 0.14770133249997128, 0.25782262766142716, 0.14770133249997128]
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]] [[34.855]
 [34.855]
 [34.855]
 [34.855]
 [34.855]
 [34.855]
 [34.855]] [[1.616]
 [1.616]
 [1.616]
 [1.616]
 [1.616]
 [1.616]
 [1.616]]
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
printing an ep nov before normalisation:  43.16156459646912
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.32759550337924453, 0.05238386958092295, 0.05238386958092295, 0.05238386958092295, 0.32759550337924453, 0.18765738449874209]
printing an ep nov before normalisation:  47.72585289996602
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.32759550337924453, 0.05238386958092295, 0.05238386958092295, 0.05238386958092295, 0.32759550337924453, 0.18765738449874209]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.32759550337924453, 0.05238386958092295, 0.05238386958092295, 0.05238386958092295, 0.32759550337924453, 0.18765738449874209]
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.32759550337924453, 0.05238386958092295, 0.05238386958092295, 0.05238386958092295, 0.32759550337924453, 0.18765738449874209]
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.29193954271401196, 0.12462494069976365, 0.12462494069976365, 0.045081933184791874, 0.20686432135083438, 0.20686432135083438]
printing an ep nov before normalisation:  56.0148809697436
printing an ep nov before normalisation:  72.58116185614162
printing an ep nov before normalisation:  59.200796142965515
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]] [[51.409]
 [51.409]
 [51.409]
 [51.409]
 [51.409]
 [51.409]
 [51.409]] [[2.127]
 [2.127]
 [2.127]
 [2.127]
 [2.127]
 [2.127]
 [2.127]]
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.29193954271401196, 0.12462494069976365, 0.12462494069976365, 0.045081933184791874, 0.20686432135083438, 0.20686432135083438]
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.29193954271401196, 0.12462494069976365, 0.12462494069976365, 0.045081933184791874, 0.20686432135083438, 0.20686432135083438]
printing an ep nov before normalisation:  45.96264055793191
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.328983800447957, 0.18853918824003185, 0.18853918824003185, 0.05269931741597371, 0.18853918824003185, 0.05269931741597371]
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
printing an ep nov before normalisation:  48.012658338378316
line 256 mcts: sample exp_bonus 23.15484216460478
printing an ep nov before normalisation:  25.043712792098475
printing an ep nov before normalisation:  44.75943895442887
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.29183610387184333, 0.20685690123123002, 0.20685690123123002, 0.12466390195588169, 0.12466390195588169, 0.04512228975393313]
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.29183610387184333, 0.20685690123123002, 0.20685690123123002, 0.12466390195588169, 0.12466390195588169, 0.04512228975393313]
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.29183610387184333, 0.20685690123123002, 0.20685690123123002, 0.12466390195588169, 0.12466390195588169, 0.04512228975393313]
printing an ep nov before normalisation:  30.624432768106455
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.29183610387184333, 0.20685690123123002, 0.20685690123123002, 0.12466390195588169, 0.12466390195588169, 0.04512228975393313]
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.29183610387184333, 0.20685690123123002, 0.20685690123123002, 0.12466390195588169, 0.12466390195588169, 0.04512228975393313]
printing an ep nov before normalisation:  56.53233167089098
printing an ep nov before normalisation:  55.42244477840999
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.23215481503781832, 0.23215481503781832, 0.23215481503781832, 0.13312395652534473, 0.13312395652534473, 0.03728764183585566]
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.23215481503781832, 0.23215481503781832, 0.23215481503781832, 0.13312395652534473, 0.13312395652534473, 0.03728764183585566]
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.23215481503781832, 0.23215481503781832, 0.23215481503781832, 0.13312395652534473, 0.13312395652534473, 0.03728764183585566]
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.23215481503781832, 0.23215481503781832, 0.23215481503781832, 0.13312395652534473, 0.13312395652534473, 0.03728764183585566]
printing an ep nov before normalisation:  29.092214086393295
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.037]
 [0.037]
 [0.037]
 [0.037]
 [0.037]
 [0.078]] [[24.078]
 [24.078]
 [24.078]
 [24.078]
 [24.078]
 [24.078]
 [30.413]] [[0.982]
 [0.982]
 [0.982]
 [0.982]
 [0.982]
 [0.982]
 [1.455]]
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.23215481503781832, 0.23215481503781832, 0.23215481503781832, 0.13312395652534473, 0.13312395652534473, 0.03728764183585566]
Printing some Q and Qe and total Qs values:  [[0.063]
 [0.027]
 [0.062]
 [0.06 ]
 [0.059]
 [0.057]
 [0.057]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.063]
 [0.027]
 [0.062]
 [0.06 ]
 [0.059]
 [0.057]
 [0.057]]
Printing some Q and Qe and total Qs values:  [[0.31 ]
 [0.312]
 [0.312]
 [0.314]
 [0.313]
 [0.311]
 [0.315]] [[42.186]
 [43.036]
 [43.036]
 [46.589]
 [46.672]
 [46.719]
 [46.439]] [[0.31 ]
 [0.312]
 [0.312]
 [0.314]
 [0.313]
 [0.311]
 [0.315]]
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.23353023829507943, 0.23353023829507943, 0.16558822196298248, 0.16558822196298248, 0.16558822196298248, 0.036174857520893756]
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.23353023829507943, 0.23353023829507943, 0.16558822196298248, 0.16558822196298248, 0.16558822196298248, 0.036174857520893756]
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.404]
 [0.414]
 [0.391]
 [0.387]
 [0.372]
 [0.354]] [[25.748]
 [32.914]
 [28.9  ]
 [30.559]
 [31.389]
 [30.943]
 [32.598]] [[0.396]
 [0.404]
 [0.414]
 [0.391]
 [0.387]
 [0.372]
 [0.354]]
printing an ep nov before normalisation:  16.804681641065045
using explorer policy with actor:  1
printing an ep nov before normalisation:  12.63136105042745
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.23353023829507943, 0.23353023829507943, 0.16558822196298248, 0.16558822196298248, 0.16558822196298248, 0.036174857520893756]
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.23353023829507943, 0.23353023829507943, 0.16558822196298248, 0.16558822196298248, 0.16558822196298248, 0.036174857520893756]
printing an ep nov before normalisation:  60.82262194642665
printing an ep nov before normalisation:  40.60110822995791
maxi score, test score, baseline:  -0.9920875 -1.0 -0.9920875
probs:  [0.2335302393007526, 0.2335302393007526, 0.1655882219467623, 0.1655882219467623, 0.1655882219467623, 0.03617485555820789]
printing an ep nov before normalisation:  46.64051802800962
maxi score, test score, baseline:  -0.9920875 -1.0 -0.9920875
printing an ep nov before normalisation:  44.65677675564933
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.17765935472378175, 0.25055823341833255, 0.17765935472378175, 0.17765935472378175, 0.17765935472378175, 0.03880434768654034]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.17765935472378175, 0.25055823341833255, 0.17765935472378175, 0.17765935472378175, 0.17765935472378175, 0.03880434768654034]
Printing some Q and Qe and total Qs values:  [[0.483]
 [0.483]
 [0.467]
 [0.483]
 [0.483]
 [0.446]
 [0.483]] [[67.918]
 [67.918]
 [67.879]
 [67.918]
 [67.918]
 [66.558]
 [67.918]] [[1.817]
 [1.817]
 [1.8  ]
 [1.817]
 [1.817]
 [1.729]
 [1.817]]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.17765935472378175, 0.25055823341833255, 0.17765935472378175, 0.17765935472378175, 0.17765935472378175, 0.03880434768654034]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.17765935472378175, 0.25055823341833255, 0.17765935472378175, 0.17765935472378175, 0.17765935472378175, 0.03880434768654034]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.17765935472378175, 0.25055823341833255, 0.17765935472378175, 0.17765935472378175, 0.17765935472378175, 0.03880434768654034]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
printing an ep nov before normalisation:  37.76380861940786
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.17765935472378175, 0.25055823341833255, 0.17765935472378175, 0.17765935472378175, 0.17765935472378175, 0.03880434768654034]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.20684919569264107, 0.291734516609599, 0.20684919569264107, 0.12470211093429437, 0.12470211093429437, 0.04516287013653014]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.20684919569264107, 0.291734516609599, 0.20684919569264107, 0.12470211093429437, 0.12470211093429437, 0.04516287013653014]
printing an ep nov before normalisation:  47.32843900935741
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.20684919569264107, 0.291734516609599, 0.20684919569264107, 0.12470211093429437, 0.12470211093429437, 0.04516287013653014]
Printing some Q and Qe and total Qs values:  [[0.36 ]
 [0.524]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]] [[61.299]
 [55.074]
 [61.299]
 [61.299]
 [61.299]
 [61.299]
 [61.299]] [[0.36 ]
 [0.524]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.24675674699748518, 0.34802960351162404, 0.05385606792293393, 0.14875075682251152, 0.14875075682251152, 0.05385606792293393]
maxi score, test score, baseline:  -0.9922664122137405 -1.0 -0.9922664122137405
probs:  [0.24675674871050707, 0.34802960739074174, 0.05385606551006171, 0.14875075643931387, 0.14875075643931387, 0.05385606551006171]
printing an ep nov before normalisation:  32.50041398757557
using another actor
printing an ep nov before normalisation:  71.79084828591218
printing an ep nov before normalisation:  47.30489253997803
maxi score, test score, baseline:  -0.9922664122137405 -1.0 -0.9922664122137405
printing an ep nov before normalisation:  57.72922282277574
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.20887322306167413, 0.4887693338917133, 0.0755893607616531, 0.0755893607616531, 0.0755893607616531, 0.0755893607616531]
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.523]
 [0.482]
 [0.426]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.523]
 [0.482]
 [0.426]]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.202576741003933, 0.3602994204460405, 0.12747070317435685, 0.12747070317435685, 0.0547117290269559, 0.12747070317435685]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.202576741003933, 0.3602994204460405, 0.12747070317435685, 0.12747070317435685, 0.0547117290269559, 0.12747070317435685]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.83438511537246
printing an ep nov before normalisation:  78.12717914581299
siam score:  -0.865345
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.22534637216842918, 0.3177308206585116, 0.13589476331295064, 0.13589476331295064, 0.049238517234207184, 0.13589476331295064]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.24673286677660708, 0.34789024681941516, 0.14878683213198413, 0.053901611070004726, 0.053901611070004726, 0.14878683213198413]
Printing some Q and Qe and total Qs values:  [[0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.259]] [[17.63 ]
 [17.63 ]
 [17.63 ]
 [17.63 ]
 [17.63 ]
 [17.63 ]
 [17.173]] [[0.892]
 [0.892]
 [0.892]
 [0.892]
 [0.892]
 [0.892]
 [0.838]]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.24673286677660708, 0.34789024681941516, 0.14878683213198413, 0.053901611070004726, 0.053901611070004726, 0.14878683213198413]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.28818777901254494, 0.28818777901254494, 0.16538749706302563, 0.04642472392442944, 0.04642472392442944, 0.16538749706302563]
printing an ep nov before normalisation:  60.01392279689229
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.32711790195104284, 0.32711790195104284, 0.05268094748274351, 0.05268094748274351, 0.05268094748274351, 0.18772135364968387]
printing an ep nov before normalisation:  45.407139325051496
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.731]
 [0.652]
 [0.652]
 [0.617]
 [0.652]
 [0.632]] [[42.311]
 [54.697]
 [42.311]
 [42.311]
 [45.981]
 [42.311]
 [40.558]] [[1.31 ]
 [1.781]
 [1.31 ]
 [1.31 ]
 [1.391]
 [1.31 ]
 [1.235]]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.2181342554489329, 0.38013060046852465, 0.06120029621120323, 0.06120029621120323, 0.06120029621120323, 0.2181342554489329]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.2181342554489329, 0.38013060046852465, 0.06120029621120323, 0.06120029621120323, 0.06120029621120323, 0.2181342554489329]
printing an ep nov before normalisation:  58.413556983913736
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
printing an ep nov before normalisation:  13.68745510183318
printing an ep nov before normalisation:  20.987037196853038
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.1725899877531084, 0.26789191452203803, 0.1725899877531084, 0.040305223730564915, 0.12713829960177309, 0.21948458663940718]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.09122560003150447, 0.2942435117390849, 0.18956240101486374, 0.044258769711094444, 0.13963756359254306, 0.24107215391090928]
printing an ep nov before normalisation:  69.34946905021545
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.09122560003150447, 0.2942435117390849, 0.18956240101486374, 0.044258769711094444, 0.13963756359254306, 0.24107215391090928]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.09122560003150447, 0.2942435117390849, 0.18956240101486374, 0.044258769711094444, 0.13963756359254306, 0.24107215391090928]
printing an ep nov before normalisation:  71.1063089173808
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.09122560003150447, 0.2942435117390849, 0.18956240101486374, 0.044258769711094444, 0.13963756359254306, 0.24107215391090928]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.09122560003150447, 0.2942435117390849, 0.18956240101486374, 0.044258769711094444, 0.13963756359254306, 0.24107215391090928]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.09122560003150447, 0.2942435117390849, 0.18956240101486374, 0.044258769711094444, 0.13963756359254306, 0.24107215391090928]
from probs:  [0.09122560003150447, 0.2942435117390849, 0.18956240101486374, 0.044258769711094444, 0.13963756359254306, 0.24107215391090928]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.11787190459539, 0.28359735470631264, 0.1981451694928682, 0.0423205964565874, 0.11787190459539, 0.2401930701534517]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.11787190459539, 0.28359735470631264, 0.1981451694928682, 0.0423205964565874, 0.11787190459539, 0.2401930701534517]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.11787190459539, 0.28359735470631264, 0.1981451694928682, 0.0423205964565874, 0.11787190459539, 0.2401930701534517]
printing an ep nov before normalisation:  61.67319919974553
printing an ep nov before normalisation:  57.51780033111572
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.12304418695852987, 0.2960499874048945, 0.206843871549738, 0.04417389557856979, 0.12304418695852987, 0.206843871549738]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.1269999048848524, 0.26742540429465955, 0.21915413887253804, 0.04026650819055965, 0.1269999048848524, 0.21915413887253804]
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  50.65251351214846
actions average: 
K:  4  action  0 :  tensor([0.3355, 0.0315, 0.0991, 0.1180, 0.1288, 0.1324, 0.1547],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0456, 0.7279, 0.0427, 0.0385, 0.0529, 0.0606, 0.0318],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1631, 0.0860, 0.1530, 0.1378, 0.1243, 0.1884, 0.1474],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2171, 0.0027, 0.1343, 0.1483, 0.1622, 0.1603, 0.1750],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1708, 0.0145, 0.1084, 0.1350, 0.2984, 0.1242, 0.1486],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2279, 0.0041, 0.1231, 0.1644, 0.1417, 0.1701, 0.1687],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2018, 0.0473, 0.1324, 0.1360, 0.1359, 0.1567, 0.1899],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.48 ]
 [0.488]
 [0.482]
 [0.472]
 [0.479]
 [0.463]] [[47.461]
 [45.951]
 [50.437]
 [50.995]
 [48.329]
 [49.924]
 [49.332]] [[0.765]
 [0.75 ]
 [0.814]
 [0.815]
 [0.771]
 [0.799]
 [0.776]]
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.693]
 [0.469]
 [0.46 ]
 [0.619]
 [0.482]
 [0.588]] [[69.223]
 [67.926]
 [73.457]
 [73.29 ]
 [68.962]
 [71.65 ]
 [69.021]] [[0.767]
 [0.979]
 [0.802]
 [0.792]
 [0.914]
 [0.8  ]
 [0.883]]
Printing some Q and Qe and total Qs values:  [[0.624]
 [0.667]
 [0.592]
 [0.55 ]
 [0.463]
 [0.462]
 [0.439]] [[58.434]
 [60.918]
 [60.459]
 [61.598]
 [62.039]
 [61.517]
 [60.451]] [[0.893]
 [0.96 ]
 [0.881]
 [0.85 ]
 [0.767]
 [0.76 ]
 [0.728]]
printing an ep nov before normalisation:  50.60109739250493
printing an ep nov before normalisation:  52.441071737194854
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
Printing some Q and Qe and total Qs values:  [[0.222]
 [0.234]
 [0.211]
 [0.21 ]
 [0.211]
 [0.215]
 [0.212]] [[55.294]
 [58.122]
 [55.535]
 [52.555]
 [54.833]
 [56.862]
 [56.57 ]] [[0.448]
 [0.482]
 [0.439]
 [0.415]
 [0.433]
 [0.453]
 [0.448]]
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
probs:  [0.046456511823949186, 0.3086515377981889, 0.19893300385204543, 0.046456511823949186, 0.1465673399232041, 0.25293509477866327]
printing an ep nov before normalisation:  56.34721891215736
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
probs:  [0.046456511823949186, 0.3086515377981889, 0.19893300385204543, 0.046456511823949186, 0.1465673399232041, 0.25293509477866327]
printing an ep nov before normalisation:  55.42274719825741
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
probs:  [0.04910310312086247, 0.32627813478249146, 0.21029104461024056, 0.04910310312086247, 0.15493356975530256, 0.21029104461024056]
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
probs:  [0.044131670032120436, 0.29557214369628987, 0.20660089916896843, 0.0829570372890879, 0.16413735064456492, 0.20660089916896843]
printing an ep nov before normalisation:  72.63168811798096
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
from probs:  [0.04602368482286186, 0.3082765659200426, 0.2154793926087324, 0.08651861499227954, 0.1282223490473513, 0.2154793926087324]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
probs:  [0.04213628680613169, 0.2796605363436789, 0.22923231105724612, 0.08684673377790564, 0.13289182095779148, 0.22923231105724612]
printing an ep nov before normalisation:  58.701528859881364
printing an ep nov before normalisation:  49.061705147817946
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
probs:  [0.037509441153730866, 0.24560126290047746, 0.24560126290047746, 0.08723726620350515, 0.1384495039413317, 0.24560126290047746]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
actions average: 
K:  2  action  0 :  tensor([0.2066, 0.0193, 0.1131, 0.1631, 0.1766, 0.1469, 0.1745],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0396, 0.7549, 0.0237, 0.0523, 0.0386, 0.0454, 0.0454],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1712, 0.0279, 0.1975, 0.1481, 0.1505, 0.1492, 0.1555],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2176, 0.0319, 0.1090, 0.1734, 0.1649, 0.1405, 0.1626],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1709, 0.0193, 0.0974, 0.1766, 0.2482, 0.1373, 0.1503],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1843, 0.0134, 0.1285, 0.1728, 0.1643, 0.1580, 0.1787],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2036, 0.0798, 0.0906, 0.1492, 0.1485, 0.1490, 0.1793],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
probs:  [0.039660982152819615, 0.2022130647003525, 0.259731493909479, 0.0922513618005511, 0.1464116035273188, 0.259731493909479]
printing an ep nov before normalisation:  53.20268100301453
Printing some Q and Qe and total Qs values:  [[0.915]
 [0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]] [[36.954]
 [27.858]
 [27.858]
 [27.858]
 [27.858]
 [27.858]
 [27.858]] [[2.496]
 [1.589]
 [1.589]
 [1.589]
 [1.589]
 [1.589]
 [1.589]]
printing an ep nov before normalisation:  40.662036846991896
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.03966098045154428, 0.20221306517650584, 0.25973149515610744, 0.092251360803738, 0.14641160325599697, 0.25973149515610744]
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.13069542067355
printing an ep nov before normalisation:  68.14351166823072
siam score:  -0.8730072
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.03966098045154428, 0.20221306517650584, 0.25973149515610744, 0.092251360803738, 0.14641160325599697, 0.25973149515610744]
printing an ep nov before normalisation:  56.8935117360077
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
siam score:  -0.8706518
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.04192631506652828, 0.21379425586223022, 0.2746090656822477, 0.09753064885337309, 0.09753064885337309, 0.2746090656822477]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.05082585885257172, 0.1877289090350899, 0.3330567623057638, 0.11827074386896, 0.05082585885257172, 0.25929186708504287]
printing an ep nov before normalisation:  28.385941643802614
printing an ep nov before normalisation:  94.82795061116141
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.04519165683921112, 0.20579847495841014, 0.28975203897526386, 0.12431413341264001, 0.04519165683921112, 0.28975203897526386]
printing an ep nov before normalisation:  14.314734125082767
printing an ep nov before normalisation:  32.74326094480086
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.03969018999392853, 0.20221644128917193, 0.25967521699961105, 0.14644762957021595, 0.09229530514746144, 0.25967521699961105]
printing an ep nov before normalisation:  32.98471685488378
siam score:  -0.8622084
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.044745580226752825, 0.22804272888206403, 0.2928447511339423, 0.1651466484611238, 0.10407364283499315, 0.1651466484611238]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.044745580226752825, 0.22804272888206403, 0.2928447511339423, 0.1651466484611238, 0.10407364283499315, 0.1651466484611238]
printing an ep nov before normalisation:  29.098380139596394
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.03899216686134712, 0.24959524411834066, 0.24959524411834066, 0.1773294823144705, 0.10715838027303069, 0.1773294823144705]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.03899216686134712, 0.24959524411834066, 0.24959524411834066, 0.1773294823144705, 0.10715838027303069, 0.1773294823144705]
printing an ep nov before normalisation:  57.0267915725708
printing an ep nov before normalisation:  40.25683139428554
printing an ep nov before normalisation:  23.808445930480957
using explorer policy with actor:  1
printing an ep nov before normalisation:  65.48929686087571
printing an ep nov before normalisation:  47.136472969884764
printing an ep nov before normalisation:  37.15608443484533
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
probs:  [0.04545245800147159, 0.29106827390904605, 0.20678833707801572, 0.12495129696672551, 0.12495129696672551, 0.20678833707801572]
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
probs:  [0.04545245800147159, 0.29106827390904605, 0.20678833707801572, 0.12495129696672551, 0.12495129696672551, 0.20678833707801572]
printing an ep nov before normalisation:  38.12299491335771
printing an ep nov before normalisation:  65.2749809348757
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
actions average: 
K:  0  action  0 :  tensor([0.2014, 0.0192, 0.1042, 0.1608, 0.1908, 0.1508, 0.1728],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0087, 0.9275, 0.0117, 0.0114, 0.0110, 0.0162, 0.0136],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1669, 0.0168, 0.1743, 0.1308, 0.1790, 0.1629, 0.1693],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1695, 0.0058, 0.1282, 0.1517, 0.1850, 0.1564, 0.2033],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1781, 0.0085, 0.1227, 0.1480, 0.1999, 0.1468, 0.1961],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1973, 0.0048, 0.1216, 0.1464, 0.1818, 0.1529, 0.1952],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1754, 0.0112, 0.1286, 0.1360, 0.1699, 0.1413, 0.2376],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.366]
 [0.479]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]] [[45.408]
 [64.85 ]
 [45.408]
 [45.408]
 [45.408]
 [45.408]
 [45.408]] [[1.047]
 [1.652]
 [1.047]
 [1.047]
 [1.047]
 [1.047]
 [1.047]]
printing an ep nov before normalisation:  61.19559427450865
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
line 256 mcts: sample exp_bonus 33.07516793924358
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
probs:  [0.05109372205222496, 0.33417734922769016, 0.18853287437654567, 0.18853287437654567, 0.11883158998349683, 0.11883158998349683]
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
probs:  [0.05109372000216149, 0.3341773521990391, 0.18853287476441344, 0.18853287476441344, 0.11883158913498626, 0.11883158913498626]
Printing some Q and Qe and total Qs values:  [[-0.026]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.033]
 [-0.026]] [[25.599]
 [25.599]
 [25.599]
 [25.599]
 [25.599]
 [20.12 ]
 [25.599]] [[1.307]
 [1.307]
 [1.307]
 [1.307]
 [1.307]
 [0.855]
 [1.307]]
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
probs:  [0.04953881739650714, 0.316909904293206, 0.13611612172496187, 0.22520291313540122, 0.13611612172496187, 0.13611612172496187]
using explorer policy with actor:  1
printing an ep nov before normalisation:  19.421292288635616
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
probs:  [0.04953881739650714, 0.316909904293206, 0.13611612172496187, 0.22520291313540122, 0.13611612172496187, 0.13611612172496187]
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
probs:  [0.04953881739650714, 0.316909904293206, 0.13611612172496187, 0.22520291313540122, 0.13611612172496187, 0.13611612172496187]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
probs:  [0.05437489978065937, 0.34791769288516017, 0.14942685183354515, 0.14942685183354515, 0.14942685183354515, 0.14942685183354515]
Printing some Q and Qe and total Qs values:  [[0.3  ]
 [0.383]
 [0.403]
 [0.426]
 [0.457]
 [0.431]
 [0.425]] [[45.032]
 [44.362]
 [43.732]
 [46.027]
 [50.496]
 [50.67 ]
 [45.226]] [[1.361]
 [1.413]
 [1.404]
 [1.533]
 [1.77 ]
 [1.752]
 [1.495]]
printing an ep nov before normalisation:  27.87095121465004
printing an ep nov before normalisation:  28.956629684711288
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
probs:  [0.05437489978065937, 0.34791769288516017, 0.14942685183354515, 0.14942685183354515, 0.14942685183354515, 0.14942685183354515]
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
probs:  [0.06007725490835239, 0.3844798124336567, 0.16512189258321286, 0.16512189258321286, 0.06007725490835239, 0.16512189258321286]
printing an ep nov before normalisation:  24.913794178917126
using explorer policy with actor:  1
printing an ep nov before normalisation:  57.425434255275846
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.134]
 [0.134]
 [0.134]
 [0.134]
 [0.134]
 [0.134]] [[36.416]
 [48.701]
 [48.701]
 [48.701]
 [48.701]
 [48.701]
 [48.701]] [[1.756]
 [2.898]
 [2.898]
 [2.898]
 [2.898]
 [2.898]
 [2.898]]
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
probs:  [0.05340758668957771, 0.3275794229453555, 0.18853513455849638, 0.18853513455849638, 0.05340758668957771, 0.18853513455849638]
printing an ep nov before normalisation:  32.882994777568456
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
probs:  [0.05148109104750293, 0.05148109104750293, 0.2818522422858304, 0.2818522422858304, 0.05148109104750293, 0.2818522422858304]
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.515]] [[60.314]
 [60.314]
 [60.314]
 [60.314]
 [60.314]
 [60.314]
 [60.314]] [[1.675]
 [1.675]
 [1.675]
 [1.675]
 [1.675]
 [1.675]
 [1.675]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
actions average: 
K:  3  action  0 :  tensor([0.2374, 0.0261, 0.0999, 0.1369, 0.2122, 0.1379, 0.1496],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0365, 0.7276, 0.0253, 0.0694, 0.0307, 0.0368, 0.0736],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1908, 0.0265, 0.2445, 0.1239, 0.1278, 0.1494, 0.1372],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1933, 0.0372, 0.1288, 0.1660, 0.1436, 0.1691, 0.1620],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1751, 0.0290, 0.1197, 0.1433, 0.2408, 0.1353, 0.1567],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.2330, 0.0295, 0.0993, 0.1349, 0.1154, 0.2407, 0.1473],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2062, 0.0597, 0.1071, 0.1527, 0.1439, 0.1408, 0.1896],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
probs:  [0.24650045100179074, 0.1490755255235663, 0.054319776085842084, 0.054319776085842084, 0.1490755255235663, 0.3467089457793925]
printing an ep nov before normalisation:  71.24282688641001
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
probs:  [0.24650045100179074, 0.1490755255235663, 0.054319776085842084, 0.054319776085842084, 0.1490755255235663, 0.3467089457793925]
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
printing an ep nov before normalisation:  42.34082013834548
printing an ep nov before normalisation:  51.97429348049314
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
probs:  [0.24650045100179074, 0.1490755255235663, 0.054319776085842084, 0.054319776085842084, 0.1490755255235663, 0.3467089457793925]
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
probs:  [0.27231154378089756, 0.05999672461865001, 0.05999672461865001, 0.05999672461865001, 0.1646797257333688, 0.3830185566297835]
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
probs:  [0.27231154378089756, 0.05999672461865001, 0.05999672461865001, 0.05999672461865001, 0.1646797257333688, 0.3830185566297835]
printing an ep nov before normalisation:  43.64413261413574
printing an ep nov before normalisation:  47.716511885800514
siam score:  -0.856202
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
probs:  [0.20907709479581268, 0.07614888722684607, 0.07614888722684607, 0.07614888722684607, 0.07614888722684607, 0.48632735629680296]
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
probs:  [0.20268558599961867, 0.05500705340423121, 0.12783482290332648, 0.12783482290332648, 0.12783482290332648, 0.35880289188617076]
line 256 mcts: sample exp_bonus 63.8165793839637
printing an ep nov before normalisation:  61.584297823440515
siam score:  -0.8564872
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
probs:  [0.20268558599961867, 0.05500705340423121, 0.12783482290332648, 0.12783482290332648, 0.12783482290332648, 0.35880289188617076]
Printing some Q and Qe and total Qs values:  [[-0.026]
 [-0.025]
 [-0.036]
 [-0.032]
 [-0.036]
 [-0.036]
 [-0.021]] [[30.936]
 [29.922]
 [32.061]
 [32.233]
 [32.061]
 [32.061]
 [29.47 ]] [[1.658]
 [1.556]
 [1.762]
 [1.784]
 [1.762]
 [1.762]
 [1.514]]
line 256 mcts: sample exp_bonus 52.36763488652369
printing an ep nov before normalisation:  50.274023442719304
Starting evaluation
siam score:  -0.8533783
printing an ep nov before normalisation:  59.1045310110392
printing an ep nov before normalisation:  42.647706041135955
printing an ep nov before normalisation:  60.1527570885816
Printing some Q and Qe and total Qs values:  [[0.299]
 [0.299]
 [0.299]
 [0.328]
 [0.343]
 [0.411]
 [0.435]] [[47.53 ]
 [47.53 ]
 [47.53 ]
 [57.538]
 [56.78 ]
 [57.283]
 [58.129]] [[0.299]
 [0.299]
 [0.299]
 [0.328]
 [0.343]
 [0.411]
 [0.435]]
printing an ep nov before normalisation:  75.02680624522993
printing an ep nov before normalisation:  55.75000286102295
printing an ep nov before normalisation:  55.13718157130066
printing an ep nov before normalisation:  46.906126799248774
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.369]
 [0.341]
 [0.331]
 [0.336]
 [0.317]
 [0.319]] [[47.22 ]
 [55.71 ]
 [50.71 ]
 [50.867]
 [50.409]
 [44.277]
 [44.88 ]] [[0.326]
 [0.369]
 [0.341]
 [0.331]
 [0.336]
 [0.317]
 [0.319]]
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.28732692839110724, 0.04710331641244854, 0.04710331641244854, 0.16556975519644426, 0.16556975519644426, 0.28732692839110724]
printing an ep nov before normalisation:  44.188783555211366
printing an ep nov before normalisation:  43.663494865429385
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.588]
 [0.579]
 [0.614]
 [0.612]
 [0.612]
 [0.613]] [[52.164]
 [56.22 ]
 [52.539]
 [52.617]
 [52.698]
 [50.705]
 [52.407]] [[0.573]
 [0.588]
 [0.579]
 [0.614]
 [0.612]
 [0.612]
 [0.613]]
printing an ep nov before normalisation:  51.90023725840917
Printing some Q and Qe and total Qs values:  [[0.6  ]
 [0.593]
 [0.587]
 [0.629]
 [0.631]
 [0.626]
 [0.609]] [[18.687]
 [55.921]
 [18.715]
 [23.04 ]
 [24.942]
 [25.504]
 [29.896]] [[0.6  ]
 [0.593]
 [0.587]
 [0.629]
 [0.631]
 [0.626]
 [0.609]]
maxi score, test score, baseline:  -0.9929555555555556 -1.0 -0.9929555555555556
probs:  [0.3271789815615373, 0.053621028550044275, 0.053621028550044275, 0.18852632044612472, 0.18852632044612472, 0.18852632044612472]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9930972789115646 -1.0 -0.9930972789115646
probs:  [0.32717899781411586, 0.05362101710367723, 0.05362101710367723, 0.1885263226595099, 0.1885263226595099, 0.1885263226595099]
printing an ep nov before normalisation:  34.988223811967536
printing an ep nov before normalisation:  16.48368001000147
using another actor
printing an ep nov before normalisation:  15.386840566539043
printing an ep nov before normalisation:  57.35748291015625
printing an ep nov before normalisation:  15.135374584873464
maxi score, test score, baseline:  -0.9932774834437086 -1.0 -0.9932774834437086
probs:  [0.21093068260379091, 0.12160867734950451, 0.034668592235331794, 0.21093068260379091, 0.21093068260379091, 0.21093068260379091]
maxi score, test score, baseline:  -0.9932774834437086 -1.0 -0.9932774834437086
printing an ep nov before normalisation:  21.361887381671014
maxi score, test score, baseline:  -0.9934064935064936 -1.0 -0.9934064935064936
probs:  [0.2109306853025389, 0.12160867460234846, 0.034668584187495924, 0.2109306853025389, 0.2109306853025389, 0.2109306853025389]
printing an ep nov before normalisation:  18.369893558411192
printing an ep nov before normalisation:  15.847112141617616
printing an ep nov before normalisation:  32.42845058441162
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.444]
 [0.439]
 [0.439]
 [0.44 ]
 [0.439]
 [0.434]] [[18.791]
 [26.22 ]
 [13.846]
 [14.282]
 [14.502]
 [14.415]
 [14.518]] [[0.43 ]
 [0.444]
 [0.439]
 [0.439]
 [0.44 ]
 [0.439]
 [0.434]]
maxi score, test score, baseline:  -0.9934483870967742 -1.0 -0.9934483870967742
printing an ep nov before normalisation:  68.70859622955322
maxi score, test score, baseline:  -0.9934483870967742 -1.0 -0.9934483870967742
probs:  [0.23162449383612818, 0.1335338152245248, 0.03805888804256577, 0.23162449383612818, 0.23162449383612818, 0.1335338152245248]
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.2316244952298161, 0.13353381451365104, 0.03805888528324959, 0.2316244952298161, 0.2316244952298161, 0.13353381451365104]
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.2316244952298161, 0.13353381451365104, 0.03805888528324959, 0.2316244952298161, 0.2316244952298161, 0.13353381451365104]
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.2316244952298161, 0.13353381451365104, 0.03805888528324959, 0.2316244952298161, 0.2316244952298161, 0.13353381451365104]
printing an ep nov before normalisation:  16.424384486170258
maxi score, test score, baseline:  -0.9936106918238994 -1.0 -0.9936106918238994
probs:  [0.2316244993046994, 0.13353381243518697, 0.038058877215528, 0.2316244993046994, 0.2316244993046994, 0.13353381243518697]
printing an ep nov before normalisation:  14.160499659773627
maxi score, test score, baseline:  -0.9936106918238994 -1.0 -0.9936106918238994
probs:  [0.2316244993046994, 0.13353381243518697, 0.038058877215528, 0.2316244993046994, 0.2316244993046994, 0.13353381243518697]
line 256 mcts: sample exp_bonus 4.8420886658812865
printing an ep nov before normalisation:  5.816525460102184
printing an ep nov before normalisation:  59.22289418608319
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
probs:  [0.23162450062871595, 0.1335338117598498, 0.038058874594152574, 0.23162450062871595, 0.23162450062871595, 0.1335338117598498]
printing an ep nov before normalisation:  5.156377583304987
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
probs:  [0.25682357706338826, 0.1480551865043406, 0.04218728636020157, 0.25682357706338826, 0.1480551865043406, 0.1480551865043406]
printing an ep nov before normalisation:  3.915883178664501
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
probs:  [0.25682357706338826, 0.1480551865043406, 0.04218728636020157, 0.25682357706338826, 0.1480551865043406, 0.1480551865043406]
Printing some Q and Qe and total Qs values:  [[0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.353]] [[18.74]
 [18.74]
 [18.74]
 [18.74]
 [18.74]
 [18.74]
 [18.74]] [[2.02]
 [2.02]
 [2.02]
 [2.02]
 [2.02]
 [2.02]
 [2.02]]
siam score:  -0.84612757
printing an ep nov before normalisation:  37.25591858256855
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.21180532968058
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.28724367981141025, 0.1655852584770282, 0.047171061711561525, 0.28724367981141025, 0.047171061711561525, 0.1655852584770282]
printing an ep nov before normalisation:  37.26193619762572
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.28724367981141025, 0.1655852584770282, 0.047171061711561525, 0.28724367981141025, 0.047171061711561525, 0.1655852584770282]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  39.83608961105347
printing an ep nov before normalisation:  38.50117756300241
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.28724367981141025, 0.1655852584770282, 0.047171061711561525, 0.28724367981141025, 0.047171061711561525, 0.1655852584770282]
printing an ep nov before normalisation:  37.93859499510329
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.3270473539198685, 0.18852276800059606, 0.053692171039171684, 0.18852276800059606, 0.053692171039171684, 0.18852276800059606]
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.32704735836593213, 0.1885227686064905, 0.05369216790729822, 0.1885227686064905, 0.05369216790729822, 0.1885227686064905]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
probs:  [0.3780417618592561, 0.06204667005619356, 0.06204667005619356, 0.2179091139860816, 0.06204667005619356, 0.2179091139860816]
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
printing an ep nov before normalisation:  31.09459860881479
siam score:  -0.8611147
printing an ep nov before normalisation:  45.30368830109017
maxi score, test score, baseline:  -0.9938393939393939 -1.0 -0.9938393939393939
probs:  [0.06744546763593719, 0.06744546763593719, 0.06744546763593719, 0.3651090647281256, 0.06744546763593719, 0.3651090647281256]
maxi score, test score, baseline:  -0.9938393939393939 -1.0 -0.9938393939393939
probs:  [0.09600618865136797, 0.09600618865136797, 0.09600618865136797, 0.09600618865136797, 0.09600618865136797, 0.5199690567431601]
maxi score, test score, baseline:  -0.9938393939393939 -1.0 -0.9938393939393939
probs:  [0.09600618865136797, 0.09600618865136797, 0.09600618865136797, 0.09600618865136797, 0.09600618865136797, 0.5199690567431601]
maxi score, test score, baseline:  -0.9938393939393939 -1.0 -0.9938393939393939
probs:  [0.09600618865136797, 0.09600618865136797, 0.09600618865136797, 0.09600618865136797, 0.09600618865136797, 0.5199690567431601]
maxi score, test score, baseline:  -0.9938393939393939 -1.0 -0.9938393939393939
probs:  [0.09600618865136797, 0.09600618865136797, 0.09600618865136797, 0.09600618865136797, 0.09600618865136797, 0.5199690567431601]
from probs:  [0.09600618865136797, 0.09600618865136797, 0.09600618865136797, 0.09600618865136797, 0.09600618865136797, 0.5199690567431601]
maxi score, test score, baseline:  -0.9938393939393939 -1.0 -0.9938393939393939
probs:  [0.16613179442617576, 0.16613179442617576, 0.16613179442617576, 0.047390157037223705, 0.16613179442617576, 0.2880826652580732]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  32.18437501932227
Printing some Q and Qe and total Qs values:  [[0.26]
 [0.26]
 [0.26]
 [0.26]
 [0.26]
 [0.26]
 [0.26]] [[22.045]
 [22.045]
 [22.045]
 [22.045]
 [22.045]
 [22.045]
 [22.045]] [[0.26]
 [0.26]
 [0.26]
 [0.26]
 [0.26]
 [0.26]
 [0.26]]
maxi score, test score, baseline:  -0.9938393939393939 -1.0 -0.9938393939393939
probs:  [0.22878653162760537, 0.04242693674478925, 0.22878653162760537, 0.04242693674478925, 0.22878653162760537, 0.22878653162760537]
maxi score, test score, baseline:  -0.9938393939393939 -1.0 -0.9938393939393939
Printing some Q and Qe and total Qs values:  [[-0.004]
 [-0.004]
 [-0.003]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]] [[28.922]
 [28.922]
 [31.956]
 [28.922]
 [28.922]
 [28.922]
 [28.922]] [[1.685]
 [1.685]
 [1.997]
 [1.685]
 [1.685]
 [1.685]
 [1.685]]
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.346]
 [0.346]
 [0.362]
 [0.346]
 [0.346]
 [0.346]] [[26.068]
 [26.068]
 [26.068]
 [24.145]
 [26.068]
 [26.068]
 [26.068]] [[0.346]
 [0.346]
 [0.346]
 [0.362]
 [0.346]
 [0.346]
 [0.346]]
printing an ep nov before normalisation:  20.317911449362647
maxi score, test score, baseline:  -0.9938393939393939 -1.0 -0.9938393939393939
maxi score, test score, baseline:  -0.9938393939393939 -1.0 -0.9938393939393939
probs:  [0.23154736762788347, 0.13359023088251704, 0.0381774353513155, 0.13359023088251704, 0.23154736762788347, 0.23154736762788347]
maxi score, test score, baseline:  -0.9938393939393939 -1.0 -0.9938393939393939
probs:  [0.23154736762788347, 0.13359023088251704, 0.0381774353513155, 0.13359023088251704, 0.23154736762788347, 0.23154736762788347]
printing an ep nov before normalisation:  39.59453657421428
from probs:  [0.23154736762788347, 0.13359023088251704, 0.0381774353513155, 0.13359023088251704, 0.23154736762788347, 0.23154736762788347]
maxi score, test score, baseline:  -0.9938393939393939 -1.0 -0.9938393939393939
probs:  [0.25670002234728245, 0.14809581784481626, 0.04231250177098621, 0.14809581784481626, 0.25670002234728245, 0.14809581784481626]
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.2567000243371839, 0.1480958174343667, 0.0423124990225321, 0.1480958174343667, 0.2567000243371839, 0.1480958174343667]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  25.42740241275037
printing an ep nov before normalisation:  40.482232466005584
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.2879871848846902, 0.16613918615267526, 0.04745607050460882, 0.16613918615267526, 0.16613918615267526, 0.16613918615267526]
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.2879871848846902, 0.16613918615267526, 0.04745607050460882, 0.16613918615267526, 0.16613918615267526, 0.16613918615267526]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.601]
 [0.612]
 [0.596]
 [0.596]
 [0.596]
 [0.595]
 [0.598]] [[28.731]
 [49.064]
 [28.422]
 [29.139]
 [29.005]
 [28.836]
 [28.153]] [[0.601]
 [0.612]
 [0.596]
 [0.596]
 [0.596]
 [0.595]
 [0.598]]
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.2879871848846902, 0.16613918615267526, 0.04745607050460882, 0.16613918615267526, 0.16613918615267526, 0.16613918615267526]
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.2879871848846902, 0.16613918615267526, 0.04745607050460882, 0.16613918615267526, 0.16613918615267526, 0.16613918615267526]
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.2879871848846902, 0.16613918615267526, 0.04745607050460882, 0.16613918615267526, 0.16613918615267526, 0.16613918615267526]
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
using explorer policy with actor:  1
printing an ep nov before normalisation:  65.03567695617676
printing an ep nov before normalisation:  45.4943198815836
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9939119760479042 -1.0 -0.9939119760479042
probs:  [0.26849365389111923, 0.19112833001128846, 0.11577249506340132, 0.19112833001128846, 0.19112833001128846, 0.042348861011613996]
printing an ep nov before normalisation:  20.6647682640847
printing an ep nov before normalisation:  50.1274256969817
printing an ep nov before normalisation:  37.55896811761727
printing an ep nov before normalisation:  45.03982472212349
maxi score, test score, baseline:  -0.9939119760479042 -1.0 -0.9939119760479042
maxi score, test score, baseline:  -0.9939119760479042 -1.0 -0.9939119760479042
maxi score, test score, baseline:  -0.9939119760479042 -1.0 -0.9939119760479042
maxi score, test score, baseline:  -0.9939119760479042 -1.0 -0.9939119760479042
probs:  [0.29038317900903454, 0.20670734870179738, 0.12520491658435995, 0.12520491658435995, 0.20670734870179738, 0.0457922904186508]
printing an ep nov before normalisation:  25.5056674934373
siam score:  -0.86219
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
probs:  [0.2869962373768723, 0.16562934278123354, 0.16562934278123354, 0.04737441984189413, 0.2869962373768723, 0.04737441984189413]
printing an ep nov before normalisation:  23.495027871684876
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
probs:  [0.2869962373768723, 0.16562934278123354, 0.16562934278123354, 0.04737441984189413, 0.2869962373768723, 0.04737441984189413]
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
probs:  [0.2869962373768723, 0.16562934278123354, 0.16562934278123354, 0.04737441984189413, 0.2869962373768723, 0.04737441984189413]
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
probs:  [0.2869962373768723, 0.16562934278123354, 0.16562934278123354, 0.04737441984189413, 0.2869962373768723, 0.04737441984189413]
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
probs:  [0.2869962373768723, 0.16562934278123354, 0.16562934278123354, 0.04737441984189413, 0.2869962373768723, 0.04737441984189413]
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.349]
 [0.484]
 [0.451]
 [0.451]
 [0.446]
 [0.484]] [[34.498]
 [35.778]
 [32.92 ]
 [34.942]
 [34.684]
 [34.905]
 [32.92 ]] [[0.744]
 [0.682]
 [0.767]
 [0.769]
 [0.765]
 [0.764]
 [0.767]]
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
probs:  [0.32550368052527306, 0.0537159012561022, 0.18784493518114734, 0.0537159012561022, 0.32550368052527306, 0.0537159012561022]
printing an ep nov before normalisation:  18.12151789665222
printing an ep nov before normalisation:  58.81484910220797
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
probs:  [0.32550368052527306, 0.0537159012561022, 0.18784493518114734, 0.0537159012561022, 0.32550368052527306, 0.0537159012561022]
printing an ep nov before normalisation:  37.66889433104723
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.366]
 [0.397]
 [0.369]
 [0.375]
 [0.398]
 [0.385]] [[72.998]
 [71.201]
 [76.906]
 [78.873]
 [72.998]
 [74.315]
 [73.663]] [[0.661]
 [0.638]
 [0.715]
 [0.702]
 [0.661]
 [0.695]
 [0.676]]
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
printing an ep nov before normalisation:  44.85929528525472
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
probs:  [0.32550368052527306, 0.0537159012561022, 0.18784493518114734, 0.0537159012561022, 0.32550368052527306, 0.0537159012561022]
Printing some Q and Qe and total Qs values:  [[0.2 ]
 [1.04]
 [0.2 ]
 [0.2 ]
 [0.2 ]
 [0.2 ]
 [0.2 ]] [[24.876]
 [ 0.027]
 [24.876]
 [24.876]
 [24.876]
 [24.876]
 [24.876]] [[0.452]
 [1.04 ]
 [0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]]
printing an ep nov before normalisation:  70.78368003879096
printing an ep nov before normalisation:  28.866835025110387
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
from probs:  [0.14813527819402864, 0.14813527819402864, 0.25657821814502396, 0.04243772912786613, 0.25657821814502396, 0.14813527819402864]
printing an ep nov before normalisation:  46.730460144484894
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
probs:  [0.053977108398323516, 0.18850572834657872, 0.18850572834657872, 0.053977108398323516, 0.3265285981636169, 0.18850572834657872]
printing an ep nov before normalisation:  71.40769607173216
printing an ep nov before normalisation:  58.1485082508328
printing an ep nov before normalisation:  60.02305486231978
maxi score, test score, baseline:  -0.9940176470588236 -1.0 -0.9940176470588236
probs:  [0.05397710522001408, 0.18850572896252976, 0.18850572896252976, 0.05397710522001408, 0.32652860267238254, 0.18850572896252976]
printing an ep nov before normalisation:  35.09297561443132
maxi score, test score, baseline:  -0.9940176470588236 -1.0 -0.9940176470588236
probs:  [0.052506032482205825, 0.052506032482205825, 0.2808273008511275, 0.052506032482205825, 0.2808273008511275, 0.2808273008511275]
maxi score, test score, baseline:  -0.9940176470588236 -1.0 -0.9940176470588236
probs:  [0.1336722825036164, 0.038355172699248596, 0.23143342076450618, 0.1336722825036164, 0.23143342076450618, 0.23143342076450618]
maxi score, test score, baseline:  -0.9940176470588236 -1.0 -0.9940176470588236
probs:  [0.1336722825036164, 0.038355172699248596, 0.23143342076450618, 0.1336722825036164, 0.23143342076450618, 0.23143342076450618]
from probs:  [0.1477543588134977, 0.04238578648869387, 0.25582468940303815, 0.04238578648869387, 0.25582468940303815, 0.25582468940303815]
maxi score, test score, baseline:  -0.9940176470588236 -1.0 -0.9940176470588236
probs:  [0.1656568635140825, 0.047509894661659634, 0.1656568635140825, 0.047509894661659634, 0.28683324182425785, 0.28683324182425785]
printing an ep nov before normalisation:  62.104649649493545
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.616]
 [0.593]
 [0.621]
 [0.599]
 [0.607]
 [0.613]] [[54.827]
 [60.317]
 [54.827]
 [62.008]
 [70.65 ]
 [58.323]
 [55.826]] [[1.301]
 [1.463]
 [1.301]
 [1.51 ]
 [1.707]
 [1.404]
 [1.347]]
line 256 mcts: sample exp_bonus 56.41893036774832
printing an ep nov before normalisation:  59.99812367592194
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
printing an ep nov before normalisation:  81.59151121964784
maxi score, test score, baseline:  -0.9940860465116279 -1.0 -0.9940860465116279
probs:  [0.060566114075337116, 0.060566114075337116, 0.16532362169715628, 0.16532362169715628, 0.16532362169715628, 0.38289690675785687]
printing an ep nov before normalisation:  39.42824210350695
printing an ep nov before normalisation:  37.45493966652816
printing an ep nov before normalisation:  74.93213505857017
maxi score, test score, baseline:  -0.9940860465116279 -1.0 -0.9940860465116279
probs:  [0.060566114075337116, 0.060566114075337116, 0.16532362169715628, 0.16532362169715628, 0.16532362169715628, 0.38289690675785687]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  45.981714663123014
printing an ep nov before normalisation:  41.90925121307373
maxi score, test score, baseline:  -0.9940860465116279 -1.0 -0.9940860465116279
probs:  [0.04809661209621681, 0.11145310640342777, 0.1763935130683184, 0.1763935130683184, 0.1763935130683184, 0.3112697422954001]
printing an ep nov before normalisation:  46.77421656391136
maxi score, test score, baseline:  -0.9940860465116279 -1.0 -0.9940860465116279
probs:  [0.04809661209621681, 0.11145310640342777, 0.1763935130683184, 0.1763935130683184, 0.1763935130683184, 0.3112697422954001]
printing an ep nov before normalisation:  41.82957649230957
printing an ep nov before normalisation:  9.873756443290631
maxi score, test score, baseline:  -0.9940860465116279 -1.0 -0.9940860465116279
probs:  [0.04809661209621681, 0.11145310640342777, 0.1763935130683184, 0.1763935130683184, 0.1763935130683184, 0.3112697422954001]
actions average: 
K:  4  action  0 :  tensor([0.3040, 0.0468, 0.1157, 0.1005, 0.1585, 0.1296, 0.1450],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0249, 0.8481, 0.0170, 0.0343, 0.0342, 0.0198, 0.0217],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2071, 0.0158, 0.1384, 0.1293, 0.1629, 0.1836, 0.1630],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1340, 0.1370, 0.1097, 0.1953, 0.1424, 0.1389, 0.1427],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1698, 0.0450, 0.1620, 0.1175, 0.1972, 0.1509, 0.1575],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1898, 0.0309, 0.1282, 0.1361, 0.1609, 0.1911, 0.1630],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1435, 0.0345, 0.1399, 0.1529, 0.1689, 0.1653, 0.1949],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
probs:  [0.0514305441727595, 0.11919094790080155, 0.11919094790080155, 0.1886453617220444, 0.1886453617220444, 0.33289683658154867]
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
probs:  [0.0514305441727595, 0.11919094790080155, 0.11919094790080155, 0.1886453617220444, 0.1886453617220444, 0.33289683658154867]
actions average: 
K:  0  action  0 :  tensor([0.2401, 0.0111, 0.1353, 0.1346, 0.1745, 0.1496, 0.1548],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0801, 0.7069, 0.0375, 0.0336, 0.0519, 0.0287, 0.0613],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2115, 0.0248, 0.1790, 0.1251, 0.1508, 0.1631, 0.1457],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2020, 0.0129, 0.1208, 0.2284, 0.1509, 0.1380, 0.1469],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2746, 0.0013, 0.1193, 0.1278, 0.1834, 0.1363, 0.1573],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.2200, 0.0077, 0.1405, 0.1299, 0.1491, 0.2031, 0.1498],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1912, 0.0458, 0.1519, 0.1384, 0.1359, 0.1345, 0.2023],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  41.97484895125761
printing an ep nov before normalisation:  36.37428517129706
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
probs:  [0.11455145987932147, 0.11455145987932147, 0.17494055040040113, 0.05561752816597832, 0.17494055040040113, 0.3653984512745765]
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
probs:  [0.11455145987932147, 0.11455145987932147, 0.17494055040040113, 0.05561752816597832, 0.17494055040040113, 0.3653984512745765]
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
probs:  [0.11455145987932147, 0.11455145987932147, 0.17494055040040113, 0.05561752816597832, 0.17494055040040113, 0.3653984512745765]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
probs:  [0.11455145987932147, 0.11455145987932147, 0.17494055040040113, 0.05561752816597832, 0.17494055040040113, 0.3653984512745765]
printing an ep nov before normalisation:  40.73177752542941
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
probs:  [0.11455145987932147, 0.11455145987932147, 0.17494055040040113, 0.05561752816597832, 0.17494055040040113, 0.3653984512745765]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
probs:  [0.14775914593606887, 0.0967746577927352, 0.20000251032985616, 0.047004086033765644, 0.20000251032985616, 0.30845708957771795]
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
probs:  [0.14775914593606887, 0.0967746577927352, 0.20000251032985616, 0.047004086033765644, 0.20000251032985616, 0.30845708957771795]
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
probs:  [0.14775914593606887, 0.0967746577927352, 0.20000251032985616, 0.047004086033765644, 0.20000251032985616, 0.30845708957771795]
printing an ep nov before normalisation:  46.05062719349789
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
probs:  [0.155903626043827, 0.10210633814430901, 0.155903626043827, 0.04958993805192134, 0.21102924203962992, 0.32546722967648567]
siam score:  -0.8528386
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.1144119442740291, 0.1144119442740291, 0.1144119442740291, 0.055558125988279916, 0.23647912294076934, 0.3647269182488634]
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.1144119442740291, 0.1144119442740291, 0.1144119442740291, 0.055558125988279916, 0.23647912294076934, 0.3647269182488634]
Printing some Q and Qe and total Qs values:  [[0.07 ]
 [0.056]
 [0.056]
 [0.057]
 [0.052]
 [0.057]
 [0.056]] [[35.553]
 [64.421]
 [45.712]
 [45.171]
 [46.211]
 [43.066]
 [42.494]] [[0.07 ]
 [0.056]
 [0.056]
 [0.057]
 [0.052]
 [0.057]
 [0.056]]
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.1144119442740291, 0.1144119442740291, 0.1144119442740291, 0.055558125988279916, 0.23647912294076934, 0.3647269182488634]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.1144119442740291, 0.1144119442740291, 0.1144119442740291, 0.055558125988279916, 0.23647912294076934, 0.3647269182488634]
printing an ep nov before normalisation:  52.74581459947987
actions average: 
K:  1  action  0 :  tensor([0.2854, 0.0068, 0.1273, 0.1236, 0.1491, 0.1507, 0.1571],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0121, 0.8585, 0.0180, 0.0296, 0.0235, 0.0295, 0.0288],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1883, 0.0157, 0.1543, 0.1381, 0.1593, 0.1755, 0.1688],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1536, 0.0375, 0.1474, 0.1866, 0.1361, 0.1664, 0.1724],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1653, 0.0139, 0.1220, 0.1284, 0.2698, 0.1378, 0.1628],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2115, 0.0193, 0.1263, 0.1225, 0.1508, 0.2004, 0.1692],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1942, 0.0166, 0.1280, 0.1412, 0.1434, 0.1513, 0.2253],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.12194312423815237, 0.12194312423815237, 0.12194312423815237, 0.05921072989995434, 0.18620557697484336, 0.38875432041074515]
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.12194312423815237, 0.12194312423815237, 0.12194312423815237, 0.05921072989995434, 0.18620557697484336, 0.38875432041074515]
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.286]
 [0.342]
 [0.342]] [[43.729]
 [43.729]
 [43.729]
 [43.729]
 [53.325]
 [43.729]
 [43.729]] [[0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.58 ]
 [0.537]
 [0.537]]
printing an ep nov before normalisation:  51.931612493993
maxi score, test score, baseline:  -0.9942181818181818 -1.0 -0.9942181818181818
probs:  [0.12194312299013023, 0.12194312299013023, 0.12194312299013023, 0.059210726901366945, 0.1862055775200824, 0.3887543266081599]
printing an ep nov before normalisation:  43.99143753791581
maxi score, test score, baseline:  -0.9942181818181818 -1.0 -0.9942181818181818
probs:  [0.12194312299013023, 0.12194312299013023, 0.12194312299013023, 0.059210726901366945, 0.1862055775200824, 0.3887543266081599]
maxi score, test score, baseline:  -0.9942181818181818 -1.0 -0.9942181818181818
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.5368],
        [-0.2305],
        [-0.5707],
        [-0.5334],
        [-0.6122],
        [-0.6048],
        [-0.3469],
        [-0.6048],
        [-0.5795],
        [-0.5709]], dtype=torch.float64)
-0.032346567066 -0.5691214198896307
-0.07103438119800001 -0.30150978026770325
-0.032346567066 -0.602999295504888
-0.032346567066 -0.5656999911021644
-0.032346567066 -0.6445449508402101
-0.032346567066 -0.6371076771706602
-0.09703970119800001 -0.44395257623359186
-0.032346567066 -0.6371076771706602
-0.032346567066 -0.6118233247610404
-0.032346567066 -0.6032826219663447
siam score:  -0.8623489
printing an ep nov before normalisation:  26.47317963432794
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.473]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.52 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.546]
 [0.473]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.52 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9942502824858758 -1.0 -0.9942502824858758
probs:  [0.04961259473360728, 0.15592728307331152, 0.15592728307331152, 0.10213711337762801, 0.21102940812742663, 0.325366317614715]
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.445]
 [0.474]
 [0.474]
 [0.492]
 [0.498]
 [0.493]] [[59.388]
 [69.883]
 [64.414]
 [65.198]
 [65.298]
 [63.415]
 [66.362]] [[0.975]
 [1.055]
 [0.998]
 [1.01 ]
 [1.029]
 [1.005]
 [1.048]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.408]
 [0.427]
 [0.427]
 [0.385]
 [0.427]
 [0.427]] [[53.156]
 [57.592]
 [53.156]
 [53.156]
 [51.488]
 [53.156]
 [53.156]] [[2.181]
 [2.408]
 [2.181]
 [2.181]
 [2.046]
 [2.181]
 [2.181]]
printing an ep nov before normalisation:  54.88508607934583
printing an ep nov before normalisation:  57.52200492447619
maxi score, test score, baseline:  -0.9942502824858758 -1.0 -0.9942502824858758
probs:  [0.04961259473360728, 0.15592728307331152, 0.15592728307331152, 0.10213711337762801, 0.21102940812742663, 0.325366317614715]
printing an ep nov before normalisation:  44.25374984741211
maxi score, test score, baseline:  -0.9942502824858758 -1.0 -0.9942502824858758
probs:  [0.04637295732760357, 0.12917374271450222, 0.1720705351438589, 0.12917374271450222, 0.21601359080320034, 0.30719543129633275]
printing an ep nov before normalisation:  50.54832458496094
printing an ep nov before normalisation:  52.11866855621338
maxi score, test score, baseline:  -0.9943444444444445 -1.0 -0.9943444444444445
probs:  [0.03751357399868557, 0.15463337924596157, 0.18567718304644398, 0.15463337924596157, 0.21747815279328012, 0.25006433166966713]
printing an ep nov before normalisation:  59.508690591940386
maxi score, test score, baseline:  -0.9943444444444445 -1.0 -0.9943444444444445
probs:  [0.03751357399868557, 0.15463337924596157, 0.18567718304644398, 0.15463337924596157, 0.21747815279328012, 0.25006433166966713]
printing an ep nov before normalisation:  27.278685837268814
printing an ep nov before normalisation:  48.2929653716792
printing an ep nov before normalisation:  55.344553790047655
maxi score, test score, baseline:  -0.9943444444444445 -1.0 -0.9943444444444445
probs:  [0.03751357399868557, 0.15463337924596157, 0.18567718304644398, 0.15463337924596157, 0.21747815279328012, 0.25006433166966713]
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.03868308377535, 0.12820542221845466, 0.1914822558167938, 0.15946719119858685, 0.22427817566959155, 0.2578838713212232]
printing an ep nov before normalisation:  40.59225063975042
Printing some Q and Qe and total Qs values:  [[ 0.038]
 [ 0.016]
 [-0.002]
 [ 0.01 ]
 [ 0.019]
 [-0.007]
 [-0.   ]] [[33.433]
 [28.043]
 [35.883]
 [35.553]
 [33.643]
 [36.492]
 [32.225]] [[0.769]
 [0.476]
 [0.851]
 [0.847]
 [0.76 ]
 [0.878]
 [0.67 ]]
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.03868308377535, 0.12820542221845466, 0.1914822558167938, 0.15946719119858685, 0.22427817566959155, 0.2578838713212232]
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.03868308377535, 0.12820542221845466, 0.1914822558167938, 0.15946719119858685, 0.22427817566959155, 0.2578838713212232]
siam score:  -0.86531794
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.039928138355966654, 0.13234180263403852, 0.19766230429243453, 0.13234180263403852, 0.23151744234709126, 0.2662085097364304]
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.04128939945271526, 0.1368642499457846, 0.17023959456241244, 0.1368642499457846, 0.2394323821822493, 0.2753101239110539]
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.04128939945271526, 0.1368642499457846, 0.17023959456241244, 0.1368642499457846, 0.2394323821822493, 0.2753101239110539]
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.04128939945271526, 0.1368642499457846, 0.17023959456241244, 0.1368642499457846, 0.2394323821822493, 0.2753101239110539]
Printing some Q and Qe and total Qs values:  [[0.081]
 [0.196]
 [0.215]
 [0.178]
 [0.156]
 [0.132]
 [0.207]] [[46.288]
 [39.862]
 [39.296]
 [43.503]
 [45.782]
 [42.989]
 [40.4  ]] [[0.889]
 [0.764]
 [0.762]
 [0.883]
 [0.945]
 [0.817]
 [0.796]]
printing an ep nov before normalisation:  53.451097581564575
printing an ep nov before normalisation:  43.83653705461212
printing an ep nov before normalisation:  0.0022435432089196183
Printing some Q and Qe and total Qs values:  [[-0.129]
 [-0.116]
 [-0.129]
 [-0.129]
 [-0.129]
 [-0.129]
 [-0.129]] [[39.304]
 [48.898]
 [39.304]
 [39.304]
 [39.304]
 [39.304]
 [39.304]] [[0.519]
 [0.842]
 [0.519]
 [0.519]
 [0.519]
 [0.519]
 [0.519]]
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
printing an ep nov before normalisation:  0.007462885605491465
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9944054945054945 -1.0 -0.9944054945054945
probs:  [0.04105257040081939, 0.15221551957036164, 0.15221551957036164, 0.15221551957036164, 0.2307885278187128, 0.27151234306938304]
printing an ep nov before normalisation:  41.257298449500674
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.339]
 [0.112]
 [0.352]
 [0.348]
 [0.149]
 [0.306]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.242]
 [0.339]
 [0.112]
 [0.352]
 [0.348]
 [0.149]
 [0.306]]
maxi score, test score, baseline:  -0.9944054945054945 -1.0 -0.9944054945054945
probs:  [0.04266655617606316, 0.15821411669795918, 0.11880254566723486, 0.15821411669795918, 0.23988628799054507, 0.2822163767702387]
siam score:  -0.8649429
printing an ep nov before normalisation:  48.94526481628418
maxi score, test score, baseline:  -0.9944054945054945 -1.0 -0.9944054945054945
probs:  [0.04266655617606316, 0.15821411669795918, 0.11880254566723486, 0.15821411669795918, 0.23988628799054507, 0.2822163767702387]
printing an ep nov before normalisation:  41.30935304025777
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.72433567047119
maxi score, test score, baseline:  -0.9944054945054945 -1.0 -0.9944054945054945
probs:  [0.04266655617606316, 0.15821411669795918, 0.11880254566723486, 0.15821411669795918, 0.23988628799054507, 0.2822163767702387]
printing an ep nov before normalisation:  52.789950010692586
actions average: 
K:  4  action  0 :  tensor([0.3701, 0.0159, 0.1058, 0.1129, 0.1211, 0.1378, 0.1364],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0437, 0.6835, 0.0375, 0.0693, 0.0400, 0.0437, 0.0823],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2488, 0.0016, 0.1497, 0.1130, 0.1463, 0.1952, 0.1453],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2400, 0.0404, 0.1081, 0.1427, 0.1612, 0.1396, 0.1681],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1861, 0.0215, 0.1134, 0.1300, 0.2593, 0.1322, 0.1575],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2543, 0.0021, 0.1189, 0.1428, 0.1513, 0.1565, 0.1741],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1691, 0.0213, 0.1047, 0.1378, 0.1630, 0.1179, 0.2861],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9944054945054945 -1.0 -0.9944054945054945
probs:  [0.04064636622385257, 0.17257062748881982, 0.12757320504185427, 0.12757320504185427, 0.2658182981018096, 0.2658182981018096]
maxi score, test score, baseline:  -0.9944054945054945 -1.0 -0.9944054945054945
printing an ep nov before normalisation:  85.2026993794305
maxi score, test score, baseline:  -0.9944054945054945 -1.0 -0.9944054945054945
probs:  [0.04064636622385257, 0.17257062748881982, 0.12757320504185427, 0.12757320504185427, 0.2658182981018096, 0.2658182981018096]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9944355191256831 -1.0 -0.9944355191256831
probs:  [0.042556845693286376, 0.1335828999565315, 0.1335828999565315, 0.1335828999565315, 0.2783472272185595, 0.2783472272185595]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  21.30882501602173
printing an ep nov before normalisation:  49.64545239818864
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
probs:  [0.042556843228196084, 0.13358289929941614, 0.13358289929941614, 0.13358289929941614, 0.2783472294367778, 0.2783472294367778]
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
probs:  [0.042556843228196084, 0.13358289929941614, 0.13358289929941614, 0.13358289929941614, 0.2783472294367778, 0.2783472294367778]
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
probs:  [0.042556843228196084, 0.13358289929941614, 0.13358289929941614, 0.13358289929941614, 0.2783472294367778, 0.2783472294367778]
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
probs:  [0.042556843228196084, 0.13358289929941614, 0.13358289929941614, 0.13358289929941614, 0.2783472294367778, 0.2783472294367778]
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
probs:  [0.04460582922965896, 0.14002828358443004, 0.14002828358443004, 0.09176865149696063, 0.2917844760522602, 0.2917844760522602]
from probs:  [0.04460582922965896, 0.14002828358443004, 0.14002828358443004, 0.09176865149696063, 0.2917844760522602, 0.2917844760522602]
maxi score, test score, baseline:  -0.9945236559139785 -1.0 -0.9945236559139785
probs:  [0.04460582797957754, 0.1400282833116143, 0.1400282833116143, 0.09176865072989503, 0.29178447733364943, 0.29178447733364943]
maxi score, test score, baseline:  -0.9945236559139785 -1.0 -0.9945236559139785
probs:  [0.04460582797957754, 0.1400282833116143, 0.1400282833116143, 0.09176865072989503, 0.29178447733364943, 0.29178447733364943]
maxi score, test score, baseline:  -0.9945236559139785 -1.0 -0.9945236559139785
probs:  [0.04460582797957754, 0.1400282833116143, 0.1400282833116143, 0.09176865072989503, 0.29178447733364943, 0.29178447733364943]
printing an ep nov before normalisation:  28.971530941428554
using explorer policy with actor:  1
printing an ep nov before normalisation:  61.25003574444858
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.576]
 [0.505]
 [0.503]
 [0.501]
 [0.5  ]
 [0.495]] [[60.886]
 [54.657]
 [62.3  ]
 [61.81 ]
 [61.42 ]
 [61.32 ]
 [61.07 ]] [[2.2  ]
 [1.985]
 [2.274]
 [2.249]
 [2.229]
 [2.223]
 [2.206]]
maxi score, test score, baseline:  -0.9945524064171123 -1.0 -0.9945524064171123
printing an ep nov before normalisation:  46.300429859976866
maxi score, test score, baseline:  -0.9945524064171123 -1.0 -0.9945524064171123
probs:  [0.05162375336707366, 0.11936666220928574, 0.18868498753620105, 0.11936666220928574, 0.18868498753620105, 0.3322729471419529]
printing an ep nov before normalisation:  1.09344699821321
printing an ep nov before normalisation:  57.62702819091066
siam score:  -0.86973757
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.046225885486963894, 0.12548844681910284, 0.20659432353106086, 0.12548844681910284, 0.20659432353106086, 0.2896085738127087]
siam score:  -0.8695765
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.046225885486963894, 0.12548844681910284, 0.20659432353106086, 0.12548844681910284, 0.20659432353106086, 0.2896085738127087]
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.05515882488181702, 0.14978278380074095, 0.14978278380074095, 0.14978278380074095, 0.14978278380074095, 0.3457100399152193]
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.05515882488181702, 0.14978278380074095, 0.14978278380074095, 0.14978278380074095, 0.14978278380074095, 0.3457100399152193]
Printing some Q and Qe and total Qs values:  [[0.348]
 [0.49 ]
 [0.533]
 [0.473]
 [0.533]
 [0.269]
 [0.494]] [[63.588]
 [72.135]
 [62.57 ]
 [70.242]
 [62.57 ]
 [70.271]
 [74.675]] [[0.926]
 [1.235]
 [1.091]
 [1.182]
 [1.091]
 [0.977]
 [1.289]]
printing an ep nov before normalisation:  93.09278206357561
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.05515882488181702, 0.14978278380074095, 0.14978278380074095, 0.14978278380074095, 0.14978278380074095, 0.3457100399152193]
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
printing an ep nov before normalisation:  0.19318615938288985
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.04538056105105271, 0.16572280592257627, 0.16572280592257627, 0.16572280592257627, 0.16572280592257627, 0.29172821525864223]
printing an ep nov before normalisation:  0.0004071643962788585
printing an ep nov before normalisation:  1.1652720872845634
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.501]
 [0.51 ]
 [0.516]
 [0.518]
 [0.519]
 [0.52 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.524]
 [0.501]
 [0.51 ]
 [0.516]
 [0.518]
 [0.519]
 [0.52 ]]
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.051576287026204946, 0.18839701806731338, 0.051576287026204946, 0.18839701806731338, 0.18839701806731338, 0.3316563717456499]
printing an ep nov before normalisation:  56.21300935745239
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.04618567196320835, 0.20620676145351588, 0.04618567196320835, 0.20620676145351588, 0.20620676145351588, 0.2890083717130356]
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.401]
 [0.302]
 [0.304]
 [0.306]
 [0.338]
 [0.307]] [[43.906]
 [51.474]
 [44.137]
 [44.531]
 [44.195]
 [51.035]
 [44.344]] [[1.223]
 [1.635]
 [1.234]
 [1.252]
 [1.24 ]
 [1.554]
 [1.247]]
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.050243855761536435, 0.13631567202505082, 0.050243855761536435, 0.22436615073140434, 0.22436615073140434, 0.3144643149890676]
printing an ep nov before normalisation:  59.69614247095505
printing an ep nov before normalisation:  46.76852884724919
printing an ep nov before normalisation:  62.83620117413499
printing an ep nov before normalisation:  60.58725582535744
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.411]
 [0.439]
 [0.434]
 [0.434]
 [0.434]
 [0.434]] [[59.839]
 [61.215]
 [55.013]
 [59.839]
 [59.839]
 [59.839]
 [59.839]] [[1.684]
 [1.716]
 [1.496]
 [1.684]
 [1.684]
 [1.684]
 [1.684]]
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.050243855761536435, 0.13631567202505082, 0.050243855761536435, 0.22436615073140434, 0.22436615073140434, 0.3144643149890676]
actions average: 
K:  4  action  0 :  tensor([0.3056, 0.0271, 0.1091, 0.1172, 0.1888, 0.1324, 0.1199],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0356, 0.7964, 0.0237, 0.0349, 0.0383, 0.0324, 0.0387],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2213, 0.0179, 0.1330, 0.1353, 0.1613, 0.1550, 0.1762],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1603, 0.0544, 0.1245, 0.1585, 0.1663, 0.1396, 0.1964],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1673, 0.0745, 0.1053, 0.1619, 0.2163, 0.1200, 0.1546],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1878, 0.0159, 0.1161, 0.1370, 0.1590, 0.2457, 0.1384],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1903, 0.0708, 0.1452, 0.1287, 0.1410, 0.1448, 0.1792],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.050243855761536435, 0.13631567202505082, 0.050243855761536435, 0.22436615073140434, 0.22436615073140434, 0.3144643149890676]
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.512]] [[53.6]
 [53.6]
 [53.6]
 [53.6]
 [53.6]
 [53.6]
 [53.6]] [[0.97]
 [0.97]
 [0.97]
 [0.97]
 [0.97]
 [0.97]
 [0.97]]
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.05508637795215445, 0.14947588426428005, 0.05508637795215445, 0.2460352642847302, 0.14947588426428005, 0.34484021128240083]
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.05508637795215445, 0.14947588426428005, 0.05508637795215445, 0.2460352642847302, 0.14947588426428005, 0.34484021128240083]
printing an ep nov before normalisation:  67.9895757022265
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
Printing some Q and Qe and total Qs values:  [[-0.044]
 [-0.075]
 [-0.065]
 [-0.075]
 [-0.074]
 [-0.078]
 [-0.079]] [[36.758]
 [33.089]
 [36.049]
 [35.229]
 [35.167]
 [35.257]
 [35.415]] [[1.406]
 [1.09 ]
 [1.33 ]
 [1.255]
 [1.252]
 [1.255]
 [1.266]]
UNIT TEST: sample policy line 217 mcts : [0.143 0.327 0.286 0.041 0.061 0.061 0.082]
Printing some Q and Qe and total Qs values:  [[-0.029]
 [-0.034]
 [-0.028]
 [-0.031]
 [-0.032]
 [-0.031]
 [-0.035]] [[14.868]
 [22.259]
 [14.062]
 [13.839]
 [13.814]
 [14.281]
 [14.329]] [[0.192]
 [0.314]
 [0.179]
 [0.172]
 [0.171]
 [0.179]
 [0.176]]
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.05452192301628949, 0.05452192301628949, 0.05452192301628949, 0.32428239976981565, 0.18786943141150023, 0.32428239976981565]
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.1159349956803421, 0.04277589357612242, 0.1159349956803421, 0.26729865520631313, 0.1907568046505671, 0.26729865520631313]
printing an ep nov before normalisation:  38.24927520077156
printing an ep nov before normalisation:  55.95940589904785
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.13660899308649607, 0.0503888548587764, 0.13660899308649607, 0.3149954859714341, 0.22478867991030133, 0.13660899308649607]
printing an ep nov before normalisation:  42.296342849731445
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.285]
 [0.361]
 [0.365]
 [0.243]
 [0.189]
 [0.312]] [[29.723]
 [31.995]
 [24.587]
 [20.188]
 [22.282]
 [31.506]
 [23.553]] [[1.459]
 [1.543]
 [1.194]
 [0.946]
 [0.944]
 [1.418]
 [1.085]]
using another actor
from probs:  [0.13660899308649607, 0.0503888548587764, 0.13660899308649607, 0.3149954859714341, 0.22478867991030133, 0.13660899308649607]
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.14949750431083422, 0.05513490058057413, 0.14949750431083422, 0.3447304775458557, 0.2460047126713276, 0.05513490058057413]
Printing some Q and Qe and total Qs values:  [[0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]] [[16.458]
 [16.458]
 [16.458]
 [16.458]
 [16.458]
 [16.458]
 [16.458]] [[0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]]
printing an ep nov before normalisation:  17.014280481023313
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
siam score:  -0.8618134
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.16577583128998513, 0.04818556156807372, 0.16577583128998513, 0.2860386071419412, 0.2860386071419412, 0.04818556156807372]
printing an ep nov before normalisation:  77.56617481330679
using explorer policy with actor:  0
printing an ep nov before normalisation:  55.02241230713991
printing an ep nov before normalisation:  61.35111576190797
maxi score, test score, baseline:  -0.9946089947089947 -1.0 -0.9946089947089947
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
probs:  [0.1657758312441468, 0.04818555547149968, 0.1657758312441468, 0.2860386132843535, 0.2860386132843535, 0.04818555547149968]
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
probs:  [0.1657758312441468, 0.04818555547149968, 0.1657758312441468, 0.2860386132843535, 0.2860386132843535, 0.04818555547149968]
printing an ep nov before normalisation:  58.28573185464751
printing an ep nov before normalisation:  45.44729219333985
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
probs:  [0.1657758312441468, 0.04818555547149968, 0.1657758312441468, 0.2860386132843535, 0.2860386132843535, 0.04818555547149968]
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.527]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]] [[62.956]
 [60.389]
 [62.956]
 [62.956]
 [62.956]
 [62.956]
 [62.956]] [[0.817]
 [0.843]
 [0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]]
printing an ep nov before normalisation:  16.58428430557251
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
printing an ep nov before normalisation:  44.95372941973819
printing an ep nov before normalisation:  51.326038354848876
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
printing an ep nov before normalisation:  49.71395770868402
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.06319760906994247, 0.06319760906994247, 0.2175243390447167, 0.3753584947007391, 0.2175243390447167, 0.06319760906994247]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.06319760906994247, 0.06319760906994247, 0.2175243390447167, 0.3753584947007391, 0.2175243390447167, 0.06319760906994247]
printing an ep nov before normalisation:  45.53673987110224
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.06319760906994247, 0.06319760906994247, 0.2175243390447167, 0.3753584947007391, 0.2175243390447167, 0.06319760906994247]
printing an ep nov before normalisation:  40.17083246872912
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
printing an ep nov before normalisation:  39.21291444886728
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.05043448306879203, 0.13663185660714555, 0.13663185660714555, 0.3149036973341965, 0.22476624977557497, 0.13663185660714555]
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.05043448306879203, 0.13663185660714555, 0.13663185660714555, 0.3149036973341965, 0.22476624977557497, 0.13663185660714555]
printing an ep nov before normalisation:  56.31450893749929
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.05043448306879203, 0.13663185660714555, 0.13663185660714555, 0.3149036973341965, 0.22476624977557497, 0.13663185660714555]
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.05043448306879203, 0.13663185660714555, 0.13663185660714555, 0.3149036973341965, 0.22476624977557497, 0.13663185660714555]
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.05043448306879203, 0.13663185660714555, 0.13663185660714555, 0.3149036973341965, 0.22476624977557497, 0.13663185660714555]
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.05043448306879203, 0.13663185660714555, 0.13663185660714555, 0.3149036973341965, 0.22476624977557497, 0.13663185660714555]
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.492]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]] [[36.054]
 [41.564]
 [36.054]
 [36.054]
 [36.054]
 [36.054]
 [36.054]] [[1.323]
 [1.708]
 [1.323]
 [1.323]
 [1.323]
 [1.323]
 [1.323]]
using explorer policy with actor:  1
siam score:  -0.8608662
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.061065393229964, 0.061065393229964, 0.1654801355044572, 0.3814288070267004, 0.1654801355044572, 0.1654801355044572]
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.061065393229964, 0.061065393229964, 0.1654801355044572, 0.3814288070267004, 0.1654801355044572, 0.1654801355044572]
printing an ep nov before normalisation:  14.909230190047076
Printing some Q and Qe and total Qs values:  [[0.256]
 [0.292]
 [0.253]
 [0.254]
 [0.255]
 [0.254]
 [0.252]] [[36.838]
 [44.94 ]
 [39.93 ]
 [39.02 ]
 [38.097]
 [37.341]
 [37.483]] [[0.256]
 [0.292]
 [0.253]
 [0.254]
 [0.255]
 [0.254]
 [0.252]]
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.64702541613328
actions average: 
K:  1  action  0 :  tensor([0.2438, 0.0238, 0.1374, 0.1297, 0.1713, 0.1473, 0.1466],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0315, 0.7831, 0.0342, 0.0571, 0.0222, 0.0256, 0.0463],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1963, 0.0013, 0.1485, 0.1357, 0.1675, 0.1706, 0.1801],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1980, 0.1065, 0.0902, 0.2100, 0.1314, 0.1321, 0.1319],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1848, 0.0280, 0.1218, 0.1281, 0.2512, 0.1373, 0.1488],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1648, 0.0138, 0.1244, 0.1272, 0.1237, 0.3096, 0.1366],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2424, 0.0049, 0.1270, 0.1241, 0.1826, 0.1539, 0.1651],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
maxi score, test score, baseline:  -0.9946916666666666 -1.0 -0.9946916666666666
probs:  [0.061065390827793074, 0.061065390827793074, 0.16548013547746596, 0.38142881191201583, 0.16548013547746596, 0.16548013547746596]
printing an ep nov before normalisation:  77.88224696611871
maxi score, test score, baseline:  -0.9946916666666666 -1.0 -0.9946916666666666
probs:  [0.061065390827793074, 0.061065390827793074, 0.16548013547746596, 0.38142881191201583, 0.16548013547746596, 0.16548013547746596]
maxi score, test score, baseline:  -0.9946916666666666 -1.0 -0.9946916666666666
probs:  [0.061065390827793074, 0.061065390827793074, 0.16548013547746596, 0.38142881191201583, 0.16548013547746596, 0.16548013547746596]
printing an ep nov before normalisation:  55.40160935283825
maxi score, test score, baseline:  -0.9946916666666666 -1.0 -0.9946916666666666
probs:  [0.061065390827793074, 0.061065390827793074, 0.16548013547746596, 0.38142881191201583, 0.16548013547746596, 0.16548013547746596]
line 256 mcts: sample exp_bonus 30.286353100978545
printing an ep nov before normalisation:  45.06376490015273
maxi score, test score, baseline:  -0.9946916666666666 -1.0 -0.9946916666666666
probs:  [0.07716470404642752, 0.07716470404642752, 0.2091675986371713, 0.4821735851771186, 0.07716470404642752, 0.07716470404642752]
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
probs:  [0.0771647014994242, 0.0771647014994242, 0.20916759984664196, 0.4821735941556612, 0.0771647014994242, 0.0771647014994242]
printing an ep nov before normalisation:  54.01724412349181
line 256 mcts: sample exp_bonus 32.34541801061484
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.539]
 [0.538]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.539]
 [0.538]]
actions average: 
K:  0  action  0 :  tensor([0.2394, 0.0359, 0.1200, 0.1311, 0.1442, 0.1522, 0.1772],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0264, 0.8517, 0.0139, 0.0310, 0.0196, 0.0257, 0.0317],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1840, 0.0090, 0.2337, 0.1123, 0.1203, 0.1616, 0.1792],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1950, 0.0031, 0.1259, 0.1474, 0.1480, 0.1646, 0.2160],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1677, 0.0018, 0.1229, 0.1333, 0.2654, 0.1479, 0.1610],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1887, 0.0052, 0.1298, 0.1150, 0.1262, 0.2708, 0.1641],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2019, 0.0385, 0.1164, 0.1417, 0.1315, 0.1465, 0.2234],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  41.14391803741455
from probs:  [0.07479824143882198, 0.07479824143882198, 0.25716901865692215, 0.44363801558778987, 0.07479824143882198, 0.07479824143882198]
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.06969901852449523, 0.06969901852449523, 0.3606019629510095, 0.3606019629510095, 0.06969901852449523, 0.06969901852449523]
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.09827018824463964, 0.09827018824463964, 0.5086490587768018, 0.09827018824463964, 0.09827018824463964, 0.09827018824463964]
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.167511994257346
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.1925270644244881, 0.1925270644244881, 0.1925270644244881, 0.1925270644244881, 0.1925270644244881, 0.03736467787755932]
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.1925270644244881, 0.1925270644244881, 0.1925270644244881, 0.1925270644244881, 0.1925270644244881, 0.03736467787755932]
from probs:  [0.1925270644244881, 0.1925270644244881, 0.1925270644244881, 0.1925270644244881, 0.1925270644244881, 0.03736467787755932]
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.2278954697143678, 0.04420906057126436, 0.2278954697143678, 0.2278954697143678, 0.2278954697143678, 0.04420906057126436]
printing an ep nov before normalisation:  70.40735244750977
siam score:  -0.86561936
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
printing an ep nov before normalisation:  40.37127694149726
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.27919663823263974, 0.05413669510069359, 0.27919663823263974, 0.27919663823263974, 0.05413669510069359, 0.05413669510069359]
printing an ep nov before normalisation:  27.487750337716676
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.2791966435871897, 0.05413668974614363, 0.2791966435871897, 0.2791966435871897, 0.05413668974614363, 0.05413668974614363]
printing an ep nov before normalisation:  7.612209547607449
printing an ep nov before normalisation:  85.25638679770609
printing an ep nov before normalisation:  16.000470125574587
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.360326599519098, 0.06983670024045102, 0.360326599519098, 0.06983670024045102, 0.06983670024045102, 0.06983670024045102]
siam score:  -0.867097
printing an ep nov before normalisation:  66.71279675916489
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.09840710822814247, 0.09840710822814247, 0.5079644588592876, 0.09840710822814247, 0.09840710822814247, 0.09840710822814247]
Printing some Q and Qe and total Qs values:  [[0.643]
 [0.643]
 [0.643]
 [0.691]
 [0.643]
 [0.643]
 [0.643]] [[74.957]
 [74.957]
 [74.957]
 [59.915]
 [74.957]
 [74.957]
 [74.957]] [[0.643]
 [0.643]
 [0.643]
 [0.691]
 [0.643]
 [0.643]
 [0.643]]
printing an ep nov before normalisation:  66.54641451504496
using explorer policy with actor:  1
printing an ep nov before normalisation:  53.880726312808704
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.349]
 [0.349]
 [0.352]
 [0.349]
 [0.349]
 [0.349]] [[20.101]
 [20.101]
 [20.101]
 [20.588]
 [20.101]
 [20.101]
 [19.279]] [[0.349]
 [0.349]
 [0.349]
 [0.352]
 [0.349]
 [0.349]
 [0.349]]
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.03746267837786799, 0.19250746432442642, 0.19250746432442642, 0.19250746432442642, 0.19250746432442642, 0.19250746432442642]
printing an ep nov before normalisation:  24.184976941095414
printing an ep nov before normalisation:  40.041476694253134
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.03746267837786799, 0.19250746432442642, 0.19250746432442642, 0.19250746432442642, 0.19250746432442642, 0.19250746432442642]
printing an ep nov before normalisation:  30.373494625091553
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.03746267837786799, 0.19250746432442642, 0.19250746432442642, 0.19250746432442642, 0.19250746432442642, 0.19250746432442642]
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.03746267837786799, 0.19250746432442642, 0.19250746432442642, 0.19250746432442642, 0.19250746432442642, 0.19250746432442642]
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.355]
 [0.355]
 [0.43 ]
 [0.355]
 [0.355]
 [0.355]] [[35.14 ]
 [35.14 ]
 [35.14 ]
 [47.897]
 [35.14 ]
 [35.14 ]
 [35.14 ]] [[0.946]
 [0.946]
 [0.946]
 [1.398]
 [0.946]
 [0.946]
 [0.946]]
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
probs:  [0.05426036247913931, 0.2790729708541941, 0.05426036247913931, 0.05426036247913931, 0.2790729708541941, 0.2790729708541941]
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.13402238186040963, 0.23092436412740433, 0.13402238186040963, 0.03918214389696775, 0.23092436412740433, 0.23092436412740433]
printing an ep nov before normalisation:  33.4311979948188
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.13402238186040963, 0.23092436412740433, 0.13402238186040963, 0.03918214389696775, 0.23092436412740433, 0.23092436412740433]
printing an ep nov before normalisation:  21.358629714399058
printing an ep nov before normalisation:  43.348468131059406
Printing some Q and Qe and total Qs values:  [[-0.022]
 [ 0.291]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]] [[40.474]
 [36.747]
 [40.474]
 [40.474]
 [40.474]
 [40.474]
 [40.474]] [[0.645]
 [0.878]
 [0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]]
Printing some Q and Qe and total Qs values:  [[0.096]
 [0.114]
 [0.087]
 [0.067]
 [0.073]
 [0.081]
 [0.006]] [[20.882]
 [21.918]
 [21.918]
 [24.859]
 [20.826]
 [20.029]
 [19.271]] [[0.652]
 [0.724]
 [0.698]
 [0.834]
 [0.625]
 [0.59 ]
 [0.475]]
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.1484013509857962, 0.1484013509857962, 0.1484013509857962, 0.043375785820791525, 0.2557100806109099, 0.2557100806109099]
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.61 ]
 [0.553]
 [0.467]
 [0.421]
 [0.587]
 [0.419]] [[42.936]
 [51.678]
 [51.01 ]
 [45.154]
 [44.479]
 [48.943]
 [43.704]] [[0.757]
 [1.087]
 [1.019]
 [0.839]
 [0.782]
 [1.02 ]
 [0.767]]
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.1484013509857962, 0.1484013509857962, 0.1484013509857962, 0.043375785820791525, 0.2557100806109099, 0.2557100806109099]
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.1884112050241241, 0.055044703098384634, 0.1884112050241241, 0.055044703098384634, 0.1884112050241241, 0.32467697873085843]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.18841120563274272, 0.055044699974146793, 0.18841120563274272, 0.055044699974146793, 0.18841120563274272, 0.32467698315347826]
printing an ep nov before normalisation:  35.25733709335327
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.18841120563274272, 0.055044699974146793, 0.18841120563274272, 0.055044699974146793, 0.18841120563274272, 0.32467698315347826]
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.474]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]] [[42.642]
 [45.019]
 [42.642]
 [42.642]
 [42.642]
 [42.642]
 [42.642]] [[0.885]
 [0.799]
 [0.885]
 [0.885]
 [0.885]
 [0.885]
 [0.885]]
printing an ep nov before normalisation:  48.956637411718376
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.19106278551668013, 0.04306023496254696, 0.19106278551668013, 0.11627426263028307, 0.19106278551668013, 0.2674771458571296]
printing an ep nov before normalisation:  0.4366953551368624
Printing some Q and Qe and total Qs values:  [[0.383]
 [0.402]
 [0.383]
 [0.372]
 [0.371]
 [0.369]
 [0.383]] [[33.691]
 [26.533]
 [33.691]
 [27.327]
 [27.464]
 [25.868]
 [33.691]] [[2.749]
 [1.965]
 [2.749]
 [2.024]
 [2.038]
 [1.857]
 [2.749]]
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.576]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]]
printing an ep nov before normalisation:  54.90318775177002
printing an ep nov before normalisation:  45.76395329889756
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.971]
 [0.537]
 [0.53 ]
 [0.529]
 [0.527]
 [0.517]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.524]
 [0.971]
 [0.537]
 [0.53 ]
 [0.529]
 [0.527]
 [0.517]]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.16579566385294922, 0.04559727555996037, 0.16579566385294922, 0.16579566385294922, 0.16579566385294922, 0.29122006902824266]
printing an ep nov before normalisation:  34.44303370463592
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.11185429757812014, 0.048539422082427904, 0.176516298084359, 0.176516298084359, 0.176516298084359, 0.3100573860863749]
printing an ep nov before normalisation:  66.89798776555662
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.13853771173691157, 0.060100737672254836, 0.060100737672254836, 0.21864355759017826, 0.13853771173691157, 0.38407954359148894]
printing an ep nov before normalisation:  49.935813316928474
printing an ep nov before normalisation:  43.72823715209961
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.13853771173691157, 0.060100737672254836, 0.060100737672254836, 0.21864355759017826, 0.13853771173691157, 0.38407954359148894]
actions average: 
K:  2  action  0 :  tensor([0.2383, 0.0093, 0.1060, 0.1191, 0.1476, 0.1478, 0.2319],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0255, 0.7850, 0.0527, 0.0179, 0.0245, 0.0453, 0.0491],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1609, 0.0632, 0.1522, 0.1265, 0.1558, 0.1732, 0.1682],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1533, 0.0593, 0.1040, 0.1705, 0.1631, 0.1586, 0.1912],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1788, 0.0030, 0.1041, 0.1232, 0.2508, 0.1528, 0.1872],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1819, 0.0075, 0.1236, 0.1332, 0.1670, 0.2189, 0.1679],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2093, 0.0157, 0.1130, 0.1233, 0.1926, 0.1570, 0.1891],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.15617579014230912, 0.04989796325073742, 0.10248334634813036, 0.21101062635764226, 0.15617579014230912, 0.3242564837588717]
printing an ep nov before normalisation:  13.020529065813339
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.15617579014230912, 0.04989796325073742, 0.10248334634813036, 0.21101062635764226, 0.15617579014230912, 0.3242564837588717]
Printing some Q and Qe and total Qs values:  [[1.417]
 [0.839]
 [0.839]
 [0.839]
 [0.839]
 [0.839]
 [0.839]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.417]
 [0.839]
 [0.839]
 [0.839]
 [0.839]
 [0.839]
 [0.839]]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.16523644055603223, 0.05278756510391057, 0.1084263316036586, 0.16523644055603223, 0.16523644055603223, 0.3430767816243341]
printing an ep nov before normalisation:  60.81276178853953
Printing some Q and Qe and total Qs values:  [[-0.067]
 [-0.067]
 [-0.067]
 [-0.068]
 [-0.068]
 [-0.068]
 [-0.068]] [[43.064]
 [21.015]
 [19.906]
 [19.292]
 [18.799]
 [17.899]
 [16.665]] [[1.239]
 [0.413]
 [0.372]
 [0.348]
 [0.329]
 [0.295]
 [0.249]]
printing an ep nov before normalisation:  67.25598680335958
using explorer policy with actor:  1
printing an ep nov before normalisation:  53.82890307879823
printing an ep nov before normalisation:  62.01362019151176
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
actions average: 
K:  1  action  0 :  tensor([0.2600, 0.0037, 0.1195, 0.1361, 0.1527, 0.1608, 0.1673],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0173, 0.8966, 0.0156, 0.0222, 0.0024, 0.0032, 0.0427],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1725, 0.0034, 0.1490, 0.1526, 0.1734, 0.1827, 0.1664],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1870, 0.0084, 0.1075, 0.2265, 0.1622, 0.1427, 0.1658],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1676, 0.0151, 0.1334, 0.1489, 0.2012, 0.1559, 0.1779],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1489, 0.0411, 0.1575, 0.1311, 0.1283, 0.2250, 0.1681],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1928, 0.0333, 0.1297, 0.1334, 0.1492, 0.1523, 0.2094],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.23725058755201278, 0.06524833694420092, 0.06524833694420092, 0.1503536171928584, 0.06524833694420092, 0.4166507844225261]
printing an ep nov before normalisation:  73.13250826922797
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.23725058755201278, 0.06524833694420092, 0.06524833694420092, 0.1503536171928584, 0.06524833694420092, 0.4166507844225261]
printing an ep nov before normalisation:  59.00607010565085
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]] [[48.619]
 [48.619]
 [48.619]
 [48.619]
 [48.619]
 [48.619]
 [48.619]] [[49.07]
 [49.07]
 [49.07]
 [49.07]
 [49.07]
 [49.07]
 [49.07]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.1752038242630758, 0.05598757244078021, 0.11498118158995699, 0.1752038242630758, 0.11498118158995699, 0.3636424158531543]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.1752038242630758, 0.05598757244078021, 0.11498118158995699, 0.1752038242630758, 0.11498118158995699, 0.3636424158531543]
printing an ep nov before normalisation:  16.350170061782062
printing an ep nov before normalisation:  46.429277625605856
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.1752038242630758, 0.05598757244078021, 0.11498118158995699, 0.1752038242630758, 0.11498118158995699, 0.3636424158531543]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.1752038242630758, 0.05598757244078021, 0.11498118158995699, 0.1752038242630758, 0.11498118158995699, 0.3636424158531543]
Printing some Q and Qe and total Qs values:  [[0.164]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]] [[76.889]
 [75.34 ]
 [75.34 ]
 [75.34 ]
 [75.34 ]
 [75.34 ]
 [75.34 ]] [[1.107]
 [1.073]
 [1.073]
 [1.073]
 [1.073]
 [1.073]
 [1.073]]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.18872679109542081, 0.051956019648426235, 0.11963640139539299, 0.18872679109542081, 0.11963640139539299, 0.33131759536994604]
printing an ep nov before normalisation:  54.356205362829115
printing an ep nov before normalisation:  75.64888886859855
siam score:  -0.8465062
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.18872679109542081, 0.051956019648426235, 0.11963640139539299, 0.18872679109542081, 0.11963640139539299, 0.33131759536994604]
printing an ep nov before normalisation:  50.38882226585375
printing an ep nov before normalisation:  34.5551582095663
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.513]
 [0.517]
 [0.512]
 [0.516]
 [0.511]
 [0.512]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.533]
 [0.513]
 [0.517]
 [0.512]
 [0.516]
 [0.511]
 [0.512]]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.16557033251255834, 0.06141858787225633, 0.06141858787225633, 0.16557033251255834, 0.16557033251255834, 0.3804518267178123]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.16557033251255834, 0.06141858787225633, 0.06141858787225633, 0.16557033251255834, 0.16557033251255834, 0.3804518267178123]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.16557033251255834, 0.06141858787225633, 0.06141858787225633, 0.16557033251255834, 0.16557033251255834, 0.3804518267178123]
printing an ep nov before normalisation:  38.23742866516113
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.16557033251255834, 0.06141858787225633, 0.06141858787225633, 0.16557033251255834, 0.16557033251255834, 0.3804518267178123]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.16557033251255834, 0.06141858787225633, 0.06141858787225633, 0.16557033251255834, 0.16557033251255834, 0.3804518267178123]
actions average: 
K:  2  action  0 :  tensor([0.2804, 0.0070, 0.1238, 0.1274, 0.1565, 0.1483, 0.1565],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0081, 0.9073, 0.0036, 0.0064, 0.0015, 0.0018, 0.0714],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1518, 0.0477, 0.1493, 0.1347, 0.1448, 0.1661, 0.2057],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1972, 0.0119, 0.1360, 0.1473, 0.1576, 0.1697, 0.1804],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1410, 0.0132, 0.1067, 0.1232, 0.3316, 0.1308, 0.1536],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1207, 0.0154, 0.1468, 0.1249, 0.1293, 0.3145, 0.1484],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1997, 0.0240, 0.1158, 0.1504, 0.1541, 0.1655, 0.1906],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.901]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]] [[49.644]
 [45.157]
 [45.157]
 [45.157]
 [45.157]
 [45.157]
 [45.157]] [[2.19 ]
 [1.651]
 [1.651]
 [1.651]
 [1.651]
 [1.651]
 [1.651]]
printing an ep nov before normalisation:  33.50317935055063
Printing some Q and Qe and total Qs values:  [[-0.013]
 [-0.021]
 [ 0.035]
 [-0.018]
 [-0.021]
 [-0.017]
 [-0.021]] [[31.766]
 [33.466]
 [46.016]
 [34.01 ]
 [33.466]
 [30.2  ]
 [33.466]] [[0.593]
 [0.652]
 [1.201]
 [0.676]
 [0.652]
 [0.528]
 [0.652]]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.21729697079972426, 0.0638072066910872, 0.0638072066910872, 0.0638072066910872, 0.21729697079972426, 0.3739844383272899]
printing an ep nov before normalisation:  68.8983469961086
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
printing an ep nov before normalisation:  6.095520496897393
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.27845823828818966, 0.05487509504514366, 0.05487509504514366, 0.05487509504514366, 0.27845823828818966, 0.27845823828818966]
Printing some Q and Qe and total Qs values:  [[0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]] [[66.986]
 [66.986]
 [66.986]
 [66.986]
 [66.986]
 [66.986]
 [66.986]] [[2.155]
 [2.155]
 [2.155]
 [2.155]
 [2.155]
 [2.155]
 [2.155]]
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.287]
 [0.31 ]
 [0.293]
 [0.297]
 [0.298]
 [0.294]] [[22.628]
 [23.106]
 [45.724]
 [23.667]
 [22.474]
 [21.907]
 [19.662]] [[0.786]
 [0.81 ]
 [1.783]
 [0.84 ]
 [0.794]
 [0.771]
 [0.673]]
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.538]
 [0.538]
 [0.435]
 [0.538]
 [0.538]
 [0.538]] [[44.778]
 [44.778]
 [44.778]
 [56.364]
 [44.778]
 [44.778]
 [44.778]] [[1.564]
 [1.564]
 [1.564]
 [2.158]
 [1.564]
 [1.564]
 [1.564]]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.23218507537164867, 0.16600486455853516, 0.16600486455853516, 0.16600486455853516, 0.23218507537164867, 0.037615255581097184]
printing an ep nov before normalisation:  70.83488418012044
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.457]
 [0.457]
 [0.488]
 [0.457]
 [0.457]
 [0.457]] [[39.207]
 [39.207]
 [39.207]
 [60.359]
 [39.207]
 [39.207]
 [39.207]] [[1.058]
 [1.058]
 [1.058]
 [1.754]
 [1.058]
 [1.058]
 [1.058]]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.23218507537164867, 0.16600486455853516, 0.16600486455853516, 0.16600486455853516, 0.23218507537164867, 0.037615255581097184]
printing an ep nov before normalisation:  66.04720269103987
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
printing an ep nov before normalisation:  69.76461877210457
printing an ep nov before normalisation:  74.36054651356206
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.2219195920490513, 0.17374222114009394, 0.17374222114009394, 0.17374222114009394, 0.2219195920490513, 0.0349341524816156]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.21613566800270698, 0.17810171220163074, 0.17810171220163074, 0.17810171220163074, 0.21613566800270698, 0.03342352738969386]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.21613566800270698, 0.17810171220163074, 0.17810171220163074, 0.17810171220163074, 0.21613566800270698, 0.03342352738969386]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.21613566800270698, 0.17810171220163074, 0.17810171220163074, 0.17810171220163074, 0.21613566800270698, 0.03342352738969386]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.23374024336636962, 0.1926064538902609, 0.1926064538902609, 0.1926064538902609, 0.1523036500601348, 0.03613674490271291]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.23374024336636962, 0.1926064538902609, 0.1926064538902609, 0.1926064538902609, 0.1523036500601348, 0.03613674490271291]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.23374024336636962, 0.1926064538902609, 0.1926064538902609, 0.1926064538902609, 0.1523036500601348, 0.03613674490271291]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.2433543727745502, 0.20052771644536715, 0.20052771644536715, 0.20052771644536715, 0.1174440031667531, 0.03761847472259536]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.2433543727745502, 0.20052771644536715, 0.20052771644536715, 0.20052771644536715, 0.1174440031667531, 0.03761847472259536]
siam score:  -0.86033785
printing an ep nov before normalisation:  63.25719743768416
printing an ep nov before normalisation:  50.268770071804845
printing an ep nov before normalisation:  55.11523784050854
siam score:  -0.8601761
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.60390712850438
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.437]
 [0.398]
 [0.402]
 [0.426]
 [0.405]
 [0.385]] [[41.625]
 [45.436]
 [36.002]
 [37.954]
 [44.005]
 [39.701]
 [34.927]] [[0.835]
 [0.877]
 [0.688]
 [0.723]
 [0.843]
 [0.754]
 [0.659]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.2651767951392916, 0.21850762183258332, 0.21850762183258332, 0.1727812601078299, 0.08404495418652576, 0.04098174690118633]
printing an ep nov before normalisation:  55.636800771134176
siam score:  -0.8554842
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
Printing some Q and Qe and total Qs values:  [[0.51 ]
 [0.51 ]
 [0.51 ]
 [0.51 ]
 [0.485]
 [0.51 ]
 [0.51 ]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [43.536]
 [ 0.   ]
 [ 0.   ]] [[0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.941]
 [0.375]
 [0.375]]
siam score:  -0.8566272
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.29102567260696854, 0.18961908751260464, 0.23980499952359083, 0.18961908751260464, 0.04496557642211566, 0.04496557642211566]
printing an ep nov before normalisation:  26.9208682251805
printing an ep nov before normalisation:  19.522478046467306
printing an ep nov before normalisation:  30.881952847135608
line 256 mcts: sample exp_bonus 44.32988026067555
printing an ep nov before normalisation:  19.658308029174805
printing an ep nov before normalisation:  45.846426332381895
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.2922970241649187, 0.12390575014976403, 0.24891049693142162, 0.20640046519759203, 0.04461471335622051, 0.08387155020008313]
printing an ep nov before normalisation:  65.64785513618111
printing an ep nov before normalisation:  59.571226638638066
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.379079413709796
Printing some Q and Qe and total Qs values:  [[0.245]
 [0.268]
 [0.231]
 [0.228]
 [0.231]
 [0.233]
 [0.245]] [[41.429]
 [54.325]
 [41.469]
 [50.752]
 [41.642]
 [41.89 ]
 [41.202]] [[0.792]
 [1.148]
 [0.779]
 [1.016]
 [0.783]
 [0.792]
 [0.786]]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.2922970241649187, 0.12390575014976403, 0.24891049693142162, 0.20640046519759203, 0.04461471335622051, 0.08387155020008313]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.2922970241649187, 0.12390575014976403, 0.24891049693142162, 0.20640046519759203, 0.04461471335622051, 0.08387155020008313]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.3044915124221543, 0.08736666496450957, 0.25929409519627716, 0.2150097571062761, 0.04647130534627339, 0.08736666496450957]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.3044915124221543, 0.08736666496450957, 0.25929409519627716, 0.2150097571062761, 0.04647130534627339, 0.08736666496450957]
printing an ep nov before normalisation:  56.64890705674571
printing an ep nov before normalisation:  49.09465303014575
Printing some Q and Qe and total Qs values:  [[0.495]
 [0.519]
 [0.498]
 [0.494]
 [0.519]
 [0.491]
 [0.519]] [[62.35 ]
 [44.513]
 [71.001]
 [65.539]
 [44.513]
 [63.33 ]
 [44.513]] [[0.918]
 [0.715]
 [1.03 ]
 [0.957]
 [0.715]
 [0.926]
 [0.715]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.323054480437624, 0.10242044781749478, 0.26622450233850004, 0.1559405844431699, 0.04993953714571639, 0.10242044781749478]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.34095545044263365, 0.05270052378949776, 0.28097513237137545, 0.16457768344101226, 0.05270052378949776, 0.10809068616598312]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.34095545044263365, 0.05270052378949776, 0.28097513237137545, 0.16457768344101226, 0.05270052378949776, 0.10809068616598312]
from probs:  [0.34095545044263365, 0.05270052378949776, 0.28097513237137545, 0.16457768344101226, 0.05270052378949776, 0.10809068616598312]
maxi score, test score, baseline:  -0.9949980392156863 -1.0 -0.9949980392156863
probs:  [0.36225846089551983, 0.05598622789539099, 0.23607430089946727, 0.17485624307959965, 0.05598622789539099, 0.1148385393346313]
printing an ep nov before normalisation:  22.525668223048203
printing an ep nov before normalisation:  36.06571674346924
from probs:  [0.3299739124209029, 0.05198472689757523, 0.2583916971486463, 0.18822695148574142, 0.05198472689757523, 0.11943798514955896]
maxi score, test score, baseline:  -0.9950219512195122 -1.0 -0.9950219512195122
probs:  [0.3299739151177148, 0.0519847250037485, 0.258391698663369, 0.18822695184178212, 0.0519847250037485, 0.11943798436963715]
printing an ep nov before normalisation:  49.14143417983494
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  64.34229458778188
maxi score, test score, baseline:  -0.9950219512195122 -1.0 -0.9950219512195122
probs:  [0.3299739151177148, 0.0519847250037485, 0.258391698663369, 0.18822695184178212, 0.0519847250037485, 0.11943798436963715]
printing an ep nov before normalisation:  38.49480251958338
maxi score, test score, baseline:  -0.9950219512195122 -1.0 -0.9950219512195122
probs:  [0.29146823683768747, 0.04510176754490255, 0.24022401122478826, 0.18999452275273904, 0.09246153542836966, 0.14074992621151303]
maxi score, test score, baseline:  -0.9950219512195122 -1.0 -0.9950219512195122
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.30688873740305517, 0.04748197204833649, 0.20004397068269592, 0.20004397068269592, 0.09734851529128331, 0.14819283389193305]
printing an ep nov before normalisation:  79.2141496996634
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.358]] [[43.167]
 [43.167]
 [43.167]
 [43.167]
 [43.167]
 [43.167]
 [43.167]] [[0.99]
 [0.99]
 [0.99]
 [0.99]
 [0.99]
 [0.99]
 [0.99]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.30688873740305517, 0.04748197204833649, 0.20004397068269592, 0.20004397068269592, 0.09734851529128331, 0.14819283389193305]
printing an ep nov before normalisation:  55.71241855621338
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.341272669887994, 0.05278924772877154, 0.22245177521647339, 0.22245177521647339, 0.1082452842215162, 0.05278924772877154]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.341272669887994, 0.05278924772877154, 0.22245177521647339, 0.22245177521647339, 0.1082452842215162, 0.05278924772877154]
printing an ep nov before normalisation:  44.274888038635254
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.30514591510117, 0.08764498118007906, 0.2155613720207998, 0.2155613720207998, 0.12945584032219215, 0.04663051935495921]
siam score:  -0.8632365
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.30514591510117, 0.08764498118007906, 0.2155613720207998, 0.2155613720207998, 0.12945584032219215, 0.04663051935495921]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.31902058667581495, 0.0916257888578938, 0.17990847506955773, 0.22536094717853333, 0.13533857523454298, 0.04874562698365726]
printing an ep nov before normalisation:  44.0272177527676
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
printing an ep nov before normalisation:  42.057926106776186
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
printing an ep nov before normalisation:  1.564417281574606
printing an ep nov before normalisation:  49.649977684020996
siam score:  -0.86403394
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.3061998037815498, 0.09717835280725252, 0.1996398483828878, 0.2523923015505425, 0.09717835280725252, 0.047411340670514855]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.3061998037815498, 0.09717835280725252, 0.1996398483828878, 0.2523923015505425, 0.09717835280725252, 0.047411340670514855]
printing an ep nov before normalisation:  44.502573013305664
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.3061998037815498, 0.09717835280725252, 0.1996398483828878, 0.2523923015505425, 0.09717835280725252, 0.047411340670514855]
printing an ep nov before normalisation:  46.8402148804919
printing an ep nov before normalisation:  60.05338912197748
printing an ep nov before normalisation:  39.05537764460585
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.3229097903165767, 0.10247662080309904, 0.15597981728695312, 0.2661646179665725, 0.10247662080309904, 0.0499925328236997]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.3229097903165767, 0.10247662080309904, 0.15597981728695312, 0.2661646179665725, 0.10247662080309904, 0.0499925328236997]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.3229097903165767, 0.10247662080309904, 0.15597981728695312, 0.2661646179665725, 0.10247662080309904, 0.0499925328236997]
printing an ep nov before normalisation:  50.426264487711485
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.34080388283329965, 0.10815033559020179, 0.1646196431734783, 0.28091287067171006, 0.05275663386565511, 0.05275663386565511]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
printing an ep nov before normalisation:  36.312618255615234
Printing some Q and Qe and total Qs values:  [[0.294]
 [0.344]
 [0.329]
 [0.325]
 [0.317]
 [0.287]
 [0.317]] [[60.128]
 [52.028]
 [57.272]
 [61.121]
 [59.57 ]
 [61.583]
 [58.963]] [[0.569]
 [0.545]
 [0.578]
 [0.61 ]
 [0.587]
 [0.575]
 [0.581]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.467]
 [0.513]
 [0.5  ]
 [0.502]
 [0.492]
 [0.502]
 [0.494]] [[50.516]
 [49.378]
 [50.343]
 [50.352]
 [49.944]
 [49.952]
 [47.543]] [[0.764]
 [0.798]
 [0.795]
 [0.797]
 [0.783]
 [0.793]
 [0.759]]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.20004348176588085, 0.14822876991597098, 0.20004348176588085, 0.3067510071597555, 0.047532254434071386, 0.09740100495844042]
printing an ep nov before normalisation:  36.68406012357586
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.15632845524107164, 0.15632845524107164, 0.21097703266335924, 0.3235206376518314, 0.050124616099647114, 0.10272080310301893]
printing an ep nov before normalisation:  49.74615512345426
using explorer policy with actor:  1
printing an ep nov before normalisation:  24.63819067308224
line 256 mcts: sample exp_bonus 39.861673976849694
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.10853670823375056, 0.16518349074417196, 0.22293021077906688, 0.3418541490687514, 0.05295873294050869, 0.10853670823375056]
printing an ep nov before normalisation:  51.94470670115673
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
printing an ep nov before normalisation:  27.19451350565843
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.06835942545202, 0.21330207110597993, 0.1401405452044564, 0.4414791073335036, 0.06835942545202, 0.06835942545202]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.06835942545202, 0.21330207110597993, 0.1401405452044564, 0.4414791073335036, 0.06835942545202, 0.06835942545202]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.325]
 [0.124]
 [0.13 ]
 [0.137]
 [0.325]
 [0.325]] [[56.552]
 [55.232]
 [57.508]
 [56.748]
 [55.935]
 [55.232]
 [55.232]] [[1.973]
 [1.968]
 [1.849]
 [1.828]
 [1.806]
 [1.968]
 [1.968]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.11143491966883733, 0.21874089536421565, 0.11143491966883733, 0.38766812442921783, 0.11143491966883733, 0.05928622120005449]
Printing some Q and Qe and total Qs values:  [[1.197]
 [1.22 ]
 [1.223]
 [1.222]
 [1.219]
 [1.218]
 [1.211]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.197]
 [1.22 ]
 [1.223]
 [1.222]
 [1.219]
 [1.218]
 [1.211]]
printing an ep nov before normalisation:  59.12942187560175
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.1307824622871893, 0.1990243522676069, 0.06381612165219935, 0.4117784798536159, 0.1307824622871893, 0.06381612165219935]
printing an ep nov before normalisation:  64.47625000715887
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.1307824622871893, 0.1990243522676069, 0.06381612165219935, 0.4117784798536159, 0.1307824622871893, 0.06381612165219935]
Printing some Q and Qe and total Qs values:  [[0.222]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]] [[39.223]
 [13.815]
 [13.815]
 [13.815]
 [13.815]
 [13.815]
 [13.815]] [[1.042]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  75.54813642288113
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.1307824622871893, 0.1990243522676069, 0.06381612165219935, 0.4117784798536159, 0.1307824622871893, 0.06381612165219935]
printing an ep nov before normalisation:  56.2756713167317
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.13880091905616468, 0.21858315089250394, 0.06050994388966065, 0.38279512321584547, 0.13880091905616468, 0.06050994388966065]
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]] [[72.503]
 [72.503]
 [72.503]
 [72.503]
 [72.503]
 [72.503]
 [72.503]] [[2.479]
 [2.479]
 [2.479]
 [2.479]
 [2.479]
 [2.479]
 [2.479]]
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.442]
 [0.38 ]
 [0.464]
 [0.443]
 [0.296]
 [0.442]] [[35.614]
 [31.108]
 [29.029]
 [39.155]
 [44.513]
 [36.356]
 [31.508]] [[1.009]
 [0.893]
 [0.737]
 [1.28 ]
 [1.501]
 [0.985]
 [0.911]]
printing an ep nov before normalisation:  37.89894606807781
Printing some Q and Qe and total Qs values:  [[0.376]
 [0.413]
 [0.394]
 [0.365]
 [0.358]
 [0.326]
 [0.338]] [[49.493]
 [49.408]
 [50.068]
 [50.514]
 [49.882]
 [49.525]
 [49.196]] [[1.764]
 [1.797]
 [1.814]
 [1.809]
 [1.768]
 [1.716]
 [1.711]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.16536695111379549, 0.20706108145134092, 0.124452150315268, 0.292878126417942, 0.16536695111379549, 0.04487473958785813]
siam score:  -0.85471725
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.16536695111379549, 0.20706108145134092, 0.124452150315268, 0.292878126417942, 0.16536695111379549, 0.04487473958785813]
printing an ep nov before normalisation:  70.07584129646726
printing an ep nov before normalisation:  40.15426740612872
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
printing an ep nov before normalisation:  46.92696599060454
printing an ep nov before normalisation:  33.89788998253351
printing an ep nov before normalisation:  55.76348938265197
printing an ep nov before normalisation:  47.18647480010986
Printing some Q and Qe and total Qs values:  [[0.328]
 [0.328]
 [0.355]
 [0.328]
 [0.381]
 [0.328]
 [0.328]] [[46.637]
 [46.637]
 [48.34 ]
 [46.637]
 [56.108]
 [46.637]
 [46.637]] [[1.264]
 [1.264]
 [1.363]
 [1.264]
 [1.714]
 [1.264]
 [1.264]]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.16536695111379549, 0.20706108145134092, 0.124452150315268, 0.292878126417942, 0.16536695111379549, 0.04487473958785813]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.17242174066041613, 0.2158960361306964, 0.12976004884378578, 0.305377110302536, 0.12976004884378578, 0.04678501521877985]
Printing some Q and Qe and total Qs values:  [[0.299]
 [0.295]
 [0.295]
 [0.3  ]
 [0.295]
 [0.295]
 [0.295]] [[50.052]
 [42.044]
 [42.044]
 [47.024]
 [42.044]
 [42.044]
 [42.044]] [[1.456]
 [1.191]
 [1.191]
 [1.358]
 [1.191]
 [1.191]
 [1.191]]
printing an ep nov before normalisation:  35.714226425633065
line 256 mcts: sample exp_bonus 47.71827017232084
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
probs:  [0.17995729684350098, 0.2253330718271497, 0.09172662326418476, 0.31872787082262083, 0.13542966718665023, 0.0488254700558935]
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
probs:  [0.17995729684350098, 0.2253330718271497, 0.09172662326418476, 0.31872787082262083, 0.13542966718665023, 0.0488254700558935]
from probs:  [0.17995729684350098, 0.2253330718271497, 0.09172662326418476, 0.31872787082262083, 0.13542966718665023, 0.0488254700558935]
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
probs:  [0.1900177529892765, 0.2401196689089336, 0.09259736092327786, 0.29118508321166103, 0.14085232147933388, 0.04522781248751716]
printing an ep nov before normalisation:  27.202715101638418
actions average: 
K:  1  action  0 :  tensor([0.2782, 0.0013, 0.1203, 0.1241, 0.1690, 0.1445, 0.1624],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0011,     0.9309,     0.0031,     0.0246,     0.0005,     0.0039,
            0.0359], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1936, 0.0019, 0.1454, 0.1539, 0.1579, 0.1561, 0.1912],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1974, 0.0060, 0.1283, 0.1867, 0.1564, 0.1477, 0.1776],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1885, 0.0041, 0.1201, 0.1400, 0.2406, 0.1433, 0.1634],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1182, 0.0202, 0.1285, 0.1422, 0.1075, 0.3419, 0.1416],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1727, 0.0267, 0.1453, 0.1570, 0.1572, 0.1599, 0.1812],
       grad_fn=<DivBackward0>)
Starting evaluation
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
probs:  [0.1900177529892765, 0.2401196689089336, 0.09259736092327786, 0.29118508321166103, 0.14085232147933388, 0.04522781248751716]
Printing some Q and Qe and total Qs values:  [[0.287]
 [0.311]
 [0.308]
 [0.291]
 [0.279]
 [0.305]
 [0.292]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.287]
 [0.311]
 [0.308]
 [0.291]
 [0.279]
 [0.305]
 [0.292]]
line 256 mcts: sample exp_bonus 0.0
using explorer policy with actor:  0
printing an ep nov before normalisation:  38.014213244120285
Printing some Q and Qe and total Qs values:  [[0.253]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]] [[63.763]
 [65.313]
 [65.313]
 [65.313]
 [65.313]
 [65.313]
 [65.313]] [[0.253]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]]
using explorer policy with actor:  0
using explorer policy with actor:  1
printing an ep nov before normalisation:  62.86799423787934
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
probs:  [0.19001775328393564, 0.24011966983580982, 0.09259735998862474, 0.29118508478291294, 0.14085232115359148, 0.045227810955125416]
Printing some Q and Qe and total Qs values:  [[0.183]
 [0.126]
 [0.129]
 [0.129]
 [0.128]
 [0.126]
 [0.126]] [[32.467]
 [21.986]
 [19.322]
 [18.996]
 [17.604]
 [18.679]
 [14.865]] [[0.183]
 [0.126]
 [0.129]
 [0.129]
 [0.128]
 [0.126]
 [0.126]]
printing an ep nov before normalisation:  11.404106014023984
Printing some Q and Qe and total Qs values:  [[0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]] [[2.581]
 [2.612]
 [2.612]
 [2.612]
 [2.612]
 [2.612]
 [2.612]] [[0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]]
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
probs:  [0.19001775328393564, 0.24011966983580982, 0.09259735998862474, 0.29118508478291294, 0.14085232115359148, 0.045227810955125416]
printing an ep nov before normalisation:  23.10908338773369
printing an ep nov before normalisation:  0.5699409989148307
printing an ep nov before normalisation:  30.957635931579993
printing an ep nov before normalisation:  25.961985036355326
printing an ep nov before normalisation:  32.78815452357051
line 256 mcts: sample exp_bonus 0.9722438006475664
Printing some Q and Qe and total Qs values:  [[0.249]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]] [[38.993]
 [29.018]
 [29.018]
 [29.018]
 [29.018]
 [29.018]
 [29.018]] [[0.249]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]]
printing an ep nov before normalisation:  12.150994821835042
printing an ep nov before normalisation:  17.245947876166152
using explorer policy with actor:  0
printing an ep nov before normalisation:  27.058720589755456
printing an ep nov before normalisation:  10.451467278579685
Printing some Q and Qe and total Qs values:  [[0.789]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]] [[ 3.278]
 [23.049]
 [23.049]
 [23.049]
 [23.049]
 [23.049]
 [23.049]] [[0.789]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]]
printing an ep nov before normalisation:  13.281274099011675
actions average: 
K:  1  action  0 :  tensor([0.2265, 0.0096, 0.1418, 0.1299, 0.1732, 0.1502, 0.1688],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0108, 0.9022, 0.0112, 0.0365, 0.0131, 0.0129, 0.0133],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1682, 0.0030, 0.1784, 0.1326, 0.1633, 0.1722, 0.1822],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2149, 0.0027, 0.1218, 0.1580, 0.1633, 0.1851, 0.1543],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1594, 0.0047, 0.1027, 0.1041, 0.4043, 0.1187, 0.1062],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2022, 0.0088, 0.1327, 0.1259, 0.1529, 0.2173, 0.1602],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2457, 0.0015, 0.1260, 0.1309, 0.1587, 0.1590, 0.1782],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9951153110047847 -1.0 -0.9951153110047847
probs:  [0.14813469848051103, 0.25253924982668896, 0.0973824860205633, 0.3062473603749642, 0.14813469848051103, 0.04756150681676158]
Printing some Q and Qe and total Qs values:  [[0.11 ]
 [0.108]
 [0.113]
 [0.123]
 [0.112]
 [0.101]
 [0.119]] [[26.327]
 [29.417]
 [23.465]
 [23.919]
 [23.783]
 [23.104]
 [22.768]] [[0.34 ]
 [0.386]
 [0.299]
 [0.316]
 [0.303]
 [0.281]
 [0.294]]
printing an ep nov before normalisation:  0.01220450492070313
printing an ep nov before normalisation:  33.55898893231753
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.14813469752922706, 0.25253925423470686, 0.09738248246406146, 0.306247367539931, 0.14813469752922706, 0.04756150070284652]
printing an ep nov before normalisation:  1.1725850600896592
printing an ep nov before normalisation:  16.37132167816162
printing an ep nov before normalisation:  12.09869041580594
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]] [[32.866]
 [51.244]
 [51.244]
 [51.244]
 [51.244]
 [51.244]
 [51.244]] [[0.639]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]]
Printing some Q and Qe and total Qs values:  [[0.528]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]] [[35.051]
 [50.364]
 [50.364]
 [50.364]
 [50.364]
 [50.364]
 [50.364]] [[0.528]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]]
printing an ep nov before normalisation:  54.661448314480054
printing an ep nov before normalisation:  37.550331015199205
printing an ep nov before normalisation:  17.768979741027263
printing an ep nov before normalisation:  24.395442008972168
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.1563743725350395, 0.21096301268317977, 0.10279663313038051, 0.32328963760339496, 0.1563743725350395, 0.05020197151296576]
Printing some Q and Qe and total Qs values:  [[0.904]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]] [[34.436]
 [44.921]
 [44.921]
 [44.921]
 [44.921]
 [44.921]
 [44.921]] [[0.904]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]]
printing an ep nov before normalisation:  41.86234845001508
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.1563743725350395, 0.21096301268317977, 0.10279663313038051, 0.32328963760339496, 0.1563743725350395, 0.05020197151296576]
Printing some Q and Qe and total Qs values:  [[0.842]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]] [[38.23 ]
 [41.627]
 [41.627]
 [41.627]
 [41.627]
 [41.627]
 [41.627]] [[0.842]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]]
Printing some Q and Qe and total Qs values:  [[0.836]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]] [[38.563]
 [41.154]
 [41.154]
 [41.154]
 [41.154]
 [41.154]
 [41.154]] [[0.836]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]]
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.1563743725350395, 0.21096301268317977, 0.10279663313038051, 0.32328963760339496, 0.1563743725350395, 0.05020197151296576]
Printing some Q and Qe and total Qs values:  [[0.798]
 [0.659]
 [0.659]
 [0.659]
 [0.659]
 [0.659]
 [0.659]] [[38.644]
 [40.018]
 [40.018]
 [40.018]
 [40.018]
 [40.018]
 [40.018]] [[0.798]
 [0.659]
 [0.659]
 [0.659]
 [0.659]
 [0.659]
 [0.659]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.304]
 [0.304]
 [0.304]
 [0.339]
 [0.313]
 [0.565]
 [0.304]] [[49.311]
 [58.188]
 [49.311]
 [49.813]
 [51.575]
 [55.674]
 [49.311]] [[1.396]
 [1.675]
 [1.396]
 [1.446]
 [1.476]
 [1.857]
 [1.396]]
Printing some Q and Qe and total Qs values:  [[0.644]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]] [[34.542]
 [25.024]
 [25.024]
 [25.024]
 [25.024]
 [25.024]
 [25.024]] [[0.644]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]]
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]] [[35.809]
 [29.171]
 [29.171]
 [29.171]
 [29.171]
 [29.171]
 [29.171]] [[0.574]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]]
printing an ep nov before normalisation:  38.99833369565499
printing an ep nov before normalisation:  20.255208015441895
printing an ep nov before normalisation:  39.20772670726478
line 256 mcts: sample exp_bonus 34.38976981692302
printing an ep nov before normalisation:  12.808399200439453
printing an ep nov before normalisation:  11.491189002990723
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.11525937387989686, 0.1753411393996053, 0.11525937387989686, 0.36251894736484663, 0.1753411393996053, 0.05628002607614902]
Printing some Q and Qe and total Qs values:  [[0.68 ]
 [0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.669]] [[38.612]
 [38.236]
 [38.236]
 [38.236]
 [38.236]
 [38.236]
 [38.236]] [[0.68 ]
 [0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.669]]
printing an ep nov before normalisation:  32.0974806539144
siam score:  -0.85281265
Printing some Q and Qe and total Qs values:  [[0.828]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]] [[28.501]
 [18.682]
 [18.682]
 [18.682]
 [18.682]
 [18.682]
 [18.682]] [[0.828]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]]
printing an ep nov before normalisation:  13.163508176803589
line 256 mcts: sample exp_bonus 31.61057049051584
printing an ep nov before normalisation:  29.171150658762777
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]] [[36.055]
 [29.171]
 [29.171]
 [29.171]
 [29.171]
 [29.171]
 [29.171]] [[0.626]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]]
printing an ep nov before normalisation:  69.77635623067437
printing an ep nov before normalisation:  13.04312193883075
siam score:  -0.852836
Printing some Q and Qe and total Qs values:  [[0.888]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]] [[34.481]
 [38.88 ]
 [38.88 ]
 [38.88 ]
 [38.88 ]
 [38.88 ]
 [38.88 ]] [[0.888]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]]
printing an ep nov before normalisation:  46.822547912597656
printing an ep nov before normalisation:  39.54427753560712
using explorer policy with actor:  0
printing an ep nov before normalisation:  12.586630784866408
printing an ep nov before normalisation:  15.662662844533948
printing an ep nov before normalisation:  46.726327237396426
from probs:  [0.11525937387989686, 0.1753411393996053, 0.11525937387989686, 0.36251894736484663, 0.1753411393996053, 0.05628002607614902]
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.492]
 [0.499]
 [0.498]
 [0.498]
 [0.499]
 [0.499]] [[44.906]
 [52.635]
 [49.362]
 [49.535]
 [49.131]
 [44.757]
 [38.325]] [[0.502]
 [0.492]
 [0.499]
 [0.498]
 [0.498]
 [0.499]
 [0.499]]
printing an ep nov before normalisation:  56.45439147949219
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.1308330946846214, 0.1990424373447235, 0.1308330946846214, 0.4115407740934992, 0.06387529959626717, 0.06387529959626717]
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.509]
 [0.499]
 [0.509]
 [0.509]
 [0.509]
 [0.509]] [[45.841]
 [45.841]
 [40.889]
 [45.841]
 [45.841]
 [45.841]
 [45.841]] [[0.509]
 [0.509]
 [0.499]
 [0.509]
 [0.509]
 [0.509]
 [0.509]]
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.1308330946846214, 0.1990424373447235, 0.1308330946846214, 0.4115407740934992, 0.06387529959626717, 0.06387529959626717]
printing an ep nov before normalisation:  53.62702063175364
printing an ep nov before normalisation:  53.44558673288544
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.1308330946846214, 0.1990424373447235, 0.1308330946846214, 0.4115407740934992, 0.06387529959626717, 0.06387529959626717]
printing an ep nov before normalisation:  43.33144450633455
printing an ep nov before normalisation:  47.465412746280556
printing an ep nov before normalisation:  42.64185543284007
Printing some Q and Qe and total Qs values:  [[0.647]
 [0.674]
 [0.647]
 [0.647]
 [0.647]
 [0.645]
 [0.642]] [[45.245]
 [47.902]
 [45.245]
 [45.245]
 [45.245]
 [45.606]
 [44.448]] [[0.647]
 [0.674]
 [0.647]
 [0.647]
 [0.647]
 [0.645]
 [0.642]]
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
printing an ep nov before normalisation:  21.436045708038783
printing an ep nov before normalisation:  23.231515884399414
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
probs:  [0.1308330940811372, 0.19904243788997497, 0.1308330940811372, 0.41154077821750895, 0.06387529786512089, 0.06387529786512089]
printing an ep nov before normalisation:  24.532772898265293
printing an ep nov before normalisation:  38.08198813397791
printing an ep nov before normalisation:  56.156583455742926
printing an ep nov before normalisation:  23.47691059112549
using explorer policy with actor:  0
printing an ep nov before normalisation:  16.135527193970194
printing an ep nov before normalisation:  19.179173068909606
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9953128440366973 -1.0 -0.9953128440366973
printing an ep nov before normalisation:  46.697025059175274
using explorer policy with actor:  1
printing an ep nov before normalisation:  57.82607160006868
printing an ep nov before normalisation:  46.81586909931009
maxi score, test score, baseline:  -0.995375113122172 -1.0 -0.995375113122172
probs:  [0.14882599819675632, 0.19774298995085302, 0.14882599819675632, 0.35013823349246337, 0.05366021423878516, 0.10080656592438589]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  42.71964913245115
printing an ep nov before normalisation:  30.482224309592453
printing an ep nov before normalisation:  0.20013636119244893
using explorer policy with actor:  0
printing an ep nov before normalisation:  24.921156537760723
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
Printing some Q and Qe and total Qs values:  [[0.693]
 [0.693]
 [0.727]
 [0.736]
 [0.726]
 [0.693]
 [0.693]] [[43.115]
 [43.115]
 [48.414]
 [41.247]
 [37.933]
 [43.115]
 [43.115]] [[0.693]
 [0.693]
 [0.727]
 [0.736]
 [0.726]
 [0.693]
 [0.693]]
maxi score, test score, baseline:  -0.9954947136563876 -1.0 -0.9954947136563876
probs:  [0.15638919150037803, 0.21095807244424028, 0.15638919150037803, 0.323214056100185, 0.05022791402777331, 0.10282157442704545]
maxi score, test score, baseline:  -0.9954947136563876 -1.0 -0.9954947136563876
probs:  [0.15638919150037803, 0.21095807244424028, 0.15638919150037803, 0.323214056100185, 0.05022791402777331, 0.10282157442704545]
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
probs:  [0.15638919137852922, 0.21095807296935343, 0.15638919137852922, 0.32321405795619185, 0.05022791264729014, 0.10282157367010622]
maxi score, test score, baseline:  -0.9955331877729258 -1.0 -0.9955331877729258
probs:  [0.15638919125775189, 0.21095807348985063, 0.15638919125775189, 0.3232140597958823, 0.05022791127894289, 0.10282157291982046]
printing an ep nov before normalisation:  21.482962661161157
Printing some Q and Qe and total Qs values:  [[0.278]
 [0.251]
 [0.276]
 [0.268]
 [0.282]
 [0.279]
 [0.254]] [[39.907]
 [30.514]
 [25.305]
 [32.633]
 [31.974]
 [31.595]
 [23.216]] [[0.907]
 [0.646]
 [0.541]
 [0.716]
 [0.714]
 [0.701]
 [0.467]]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.15638919113803215, 0.21095807400579134, 0.15638919113803215, 0.3232140616194701, 0.050227909922572145, 0.1028215721761021]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.15638919113803215, 0.21095807400579134, 0.15638919113803215, 0.3232140616194701, 0.050227909922572145, 0.1028215721761021]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.10863859637059405, 0.22290074609978633, 0.1652406798012595, 0.3415157396281875, 0.05306564172957846, 0.10863859637059405]
line 256 mcts: sample exp_bonus 54.57689235259622
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.10863859637059405, 0.22290074609978633, 0.1652406798012595, 0.3415157396281875, 0.05306564172957846, 0.10863859637059405]
printing an ep nov before normalisation:  49.014879495567904
printing an ep nov before normalisation:  39.76248504297033
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
Printing some Q and Qe and total Qs values:  [[-0.086]
 [-0.087]
 [-0.086]
 [-0.086]
 [-0.086]
 [-0.086]
 [-0.086]] [[39.111]
 [54.604]
 [39.111]
 [39.111]
 [39.111]
 [39.111]
 [39.111]] [[0.684]
 [1.277]
 [0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.684]]
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.387]
 [0.443]
 [0.449]
 [0.444]
 [0.449]
 [0.442]] [[51.18 ]
 [53.814]
 [52.818]
 [50.477]
 [53.02 ]
 [50.477]
 [52.163]] [[1.198]
 [1.219]
 [1.247]
 [1.186]
 [1.253]
 [1.186]
 [1.228]]
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]] [[52.102]
 [52.102]
 [52.102]
 [52.102]
 [52.102]
 [52.102]
 [52.102]] [[1.45]
 [1.45]
 [1.45]
 [1.45]
 [1.45]
 [1.45]
 [1.45]]
Printing some Q and Qe and total Qs values:  [[0.237]
 [0.19 ]
 [0.244]
 [0.24 ]
 [0.24 ]
 [0.237]
 [0.244]] [[24.148]
 [58.341]
 [36.363]
 [28.516]
 [28.547]
 [24.148]
 [28.356]] [[0.494]
 [1.157]
 [0.755]
 [0.588]
 [0.588]
 [0.494]
 [0.589]]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.12844247247200455, 0.27723035573348215, 0.12844247247200455, 0.35372978627829804, 0.056077456522105364, 0.056077456522105364]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.15608983597187778, 0.2659866537617147, 0.050153263868162136, 0.3224902063046014, 0.10264002004682193, 0.10264002004682193]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.15608983597187778, 0.2659866537617147, 0.050153263868162136, 0.3224902063046014, 0.10264002004682193, 0.10264002004682193]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.10843326822189231, 0.2810115592470504, 0.05298018371831221, 0.3407084523689605, 0.10843326822189231, 0.10843326822189231]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.10843326822189231, 0.2810115592470504, 0.05298018371831221, 0.3407084523689605, 0.10843326822189231, 0.10843326822189231]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.12961607938166061, 0.2596936620270032, 0.046769047577780345, 0.3046890522502347, 0.12961607938166061, 0.12961607938166061]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.12961607938166061, 0.2596936620270032, 0.046769047577780345, 0.3046890522502347, 0.12961607938166061, 0.12961607938166061]
printing an ep nov before normalisation:  44.39483178219689
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.12961607938166061, 0.2596936620270032, 0.046769047577780345, 0.3046890522502347, 0.12961607938166061, 0.12961607938166061]
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.448]
 [0.468]
 [0.45 ]
 [0.429]
 [0.435]
 [0.431]] [[57.524]
 [52.857]
 [54.688]
 [57.356]
 [56.872]
 [55.528]
 [48.168]] [[2.454]
 [2.132]
 [2.276]
 [2.439]
 [2.385]
 [2.3  ]
 [1.796]]
UNIT TEST: sample policy line 217 mcts : [0.367 0.02  0.02  0.061 0.408 0.082 0.041]
from probs:  [0.12961607938166061, 0.2596936620270032, 0.046769047577780345, 0.3046890522502347, 0.12961607938166061, 0.12961607938166061]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.13560352404921638, 0.22549179894872193, 0.04892554468183576, 0.31877208422179315, 0.13560352404921638, 0.13560352404921638]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.13560352404921638, 0.22549179894872193, 0.04892554468183576, 0.31877208422179315, 0.13560352404921638, 0.13560352404921638]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.13560352404921638, 0.22549179894872193, 0.04892554468183576, 0.31877208422179315, 0.13560352404921638, 0.13560352404921638]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.13560352404921638, 0.22549179894872193, 0.04892554468183576, 0.31877208422179315, 0.13560352404921638, 0.13560352404921638]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.13560352404921638, 0.22549179894872193, 0.04892554468183576, 0.31877208422179315, 0.13560352404921638, 0.13560352404921638]
printing an ep nov before normalisation:  20.381007541417283
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.1410353198590699, 0.24027053205234153, 0.045344222386986224, 0.29127928598346253, 0.1410353198590699, 0.1410353198590699]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
using explorer policy with actor:  1
from probs:  [0.16519395431977393, 0.28890179145878536, 0.04590425422144072, 0.28890179145878536, 0.16519395431977393, 0.04590425422144072]
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.17619261734517777, 0.3081436073791415, 0.0489541626695709, 0.24156283259136127, 0.17619261734517777, 0.0489541626695709]
Printing some Q and Qe and total Qs values:  [[0.6  ]
 [0.656]
 [0.617]
 [0.612]
 [0.608]
 [0.6  ]
 [0.6  ]] [[61.032]
 [64.572]
 [65.107]
 [63.599]
 [62.231]
 [61.032]
 [61.032]] [[1.199]
 [1.302]
 [1.27 ]
 [1.245]
 [1.222]
 [1.199]
 [1.199]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.427]
 [0.423]
 [0.41 ]
 [0.402]
 [0.396]
 [0.366]] [[46.18 ]
 [54.933]
 [44.922]
 [45.001]
 [60.556]
 [44.595]
 [44.379]] [[1.107]
 [1.48 ]
 [1.156]
 [1.146]
 [1.634]
 [1.119]
 [1.082]]
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.516]
 [0.49 ]
 [0.482]
 [0.491]
 [0.489]
 [0.491]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.505]
 [0.516]
 [0.49 ]
 [0.482]
 [0.491]
 [0.489]
 [0.491]]
printing an ep nov before normalisation:  29.92614791096207
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.240712721965814
Printing some Q and Qe and total Qs values:  [[0.567]
 [0.66 ]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]] [[51.878]
 [65.831]
 [51.878]
 [51.878]
 [51.878]
 [51.878]
 [51.878]] [[0.975]
 [1.262]
 [0.975]
 [0.975]
 [0.975]
 [0.975]
 [0.975]]
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.19165578588489815, 0.24444535237228487, 0.03889305808511432, 0.24444535237228487, 0.19165578588489815, 0.0889046654005198]
printing an ep nov before normalisation:  31.427541598188377
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.22721484798577996, 0.1657518606614607, 0.046089407463672155, 0.28980559654540655, 0.1657518606614607, 0.10538642668221998]
maxi score, test score, baseline:  -0.9956264957264958 -1.0 -0.9956264957264958
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.02  0.02  0.02  0.878 0.02  0.02  0.02 ]
printing an ep nov before normalisation:  65.63998864925168
maxi score, test score, baseline:  -0.9956264957264958 -1.0 -0.9956264957264958
probs:  [0.2272148487964652, 0.16575186064921257, 0.04608940584925101, 0.2898055981941268, 0.16575186064921257, 0.10538642586173186]
maxi score, test score, baseline:  -0.9956264957264958 -1.0 -0.9956264957264958
printing an ep nov before normalisation:  50.1536683372849
maxi score, test score, baseline:  -0.9956264957264958 -1.0 -0.9956264957264958
maxi score, test score, baseline:  -0.9956264957264958 -1.0 -0.9956264957264958
probs:  [0.2272148487964652, 0.16575186064921257, 0.04608940584925101, 0.2898055981941268, 0.16575186064921257, 0.10538642586173186]
maxi score, test score, baseline:  -0.9956264957264958 -1.0 -0.9956264957264958
probs:  [0.2272148487964652, 0.16575186064921257, 0.04608940584925101, 0.2898055981941268, 0.16575186064921257, 0.10538642586173186]
maxi score, test score, baseline:  -0.9956264957264958 -1.0 -0.9956264957264958
probs:  [0.24181579343331663, 0.17640084440488948, 0.0490443064734397, 0.30843101675584306, 0.11215401946625554, 0.11215401946625554]
printing an ep nov before normalisation:  44.645686353617585
maxi score, test score, baseline:  -0.9956264957264958 -1.0 -0.9956264957264958
probs:  [0.24181579343331663, 0.17640084440488948, 0.0490443064734397, 0.30843101675584306, 0.11215401946625554, 0.11215401946625554]
maxi score, test score, baseline:  -0.9956264957264958 -1.0 -0.9956264957264958
probs:  [0.24181579343331663, 0.17640084440488948, 0.0490443064734397, 0.30843101675584306, 0.11215401946625554, 0.11215401946625554]
using explorer policy with actor:  1
from probs:  [0.24181579343331663, 0.17640084440488948, 0.0490443064734397, 0.30843101675584306, 0.11215401946625554, 0.11215401946625554]
maxi score, test score, baseline:  -0.9956264957264958 -1.0 -0.9956264957264958
probs:  [0.2661821227053155, 0.19070765434856865, 0.04376621153012705, 0.2661821227053155, 0.1165809443553366, 0.1165809443553366]
Printing some Q and Qe and total Qs values:  [[0.101]
 [0.102]
 [0.089]
 [0.088]
 [0.089]
 [0.089]
 [0.091]] [[23.961]
 [31.742]
 [22.567]
 [18.156]
 [17.818]
 [16.206]
 [13.976]] [[0.101]
 [0.102]
 [0.089]
 [0.088]
 [0.089]
 [0.089]
 [0.091]]
maxi score, test score, baseline:  -0.9956264957264958 -1.0 -0.9956264957264958
probs:  [0.28792005717823443, 0.20627883123492563, 0.04733131169927972, 0.20627883123492563, 0.12609548432631734, 0.12609548432631734]
printing an ep nov before normalisation:  49.490445080111854
maxi score, test score, baseline:  -0.9956264957264958 -1.0 -0.9956264957264958
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
probs:  [0.3130291753904518, 0.22426483178810855, 0.05144929557115676, 0.13708556575009434, 0.13708556575009434, 0.13708556575009434]
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
probs:  [0.2843067655707058, 0.2843067655707058, 0.04972472858698307, 0.04972472858698307, 0.16596850584231115, 0.16596850584231115]
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
probs:  [0.2843067655707058, 0.2843067655707058, 0.04972472858698307, 0.04972472858698307, 0.16596850584231115, 0.16596850584231115]
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
probs:  [0.2843067655707058, 0.2843067655707058, 0.04972472858698307, 0.04972472858698307, 0.16596850584231115, 0.16596850584231115]
actions average: 
K:  4  action  0 :  tensor([0.2434, 0.0635, 0.1240, 0.1099, 0.1779, 0.1504, 0.1308],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0520, 0.6551, 0.0642, 0.0576, 0.0523, 0.0688, 0.0500],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2361, 0.0078, 0.1465, 0.1304, 0.1745, 0.1548, 0.1498],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1591, 0.0142, 0.1331, 0.2277, 0.1848, 0.1424, 0.1386],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2298, 0.0141, 0.1334, 0.1297, 0.1979, 0.1518, 0.1434],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1845, 0.0370, 0.1203, 0.1193, 0.1601, 0.2510, 0.1278],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2523, 0.0239, 0.1275, 0.1077, 0.1487, 0.1399, 0.1999],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  62.46364769879855
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
probs:  [0.2843067655707058, 0.2843067655707058, 0.04972472858698307, 0.04972472858698307, 0.16596850584231115, 0.16596850584231115]
printing an ep nov before normalisation:  54.36372861836449
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
using explorer policy with actor:  1
printing an ep nov before normalisation:  22.775733114464742
printing an ep nov before normalisation:  11.520597523156084
maxi score, test score, baseline:  -0.9956627118644068 -1.0 -0.9956627118644068
probs:  [0.26649241229744935, 0.19095906397347095, 0.1167625890711549, 0.04386780671098287, 0.19095906397347095, 0.19095906397347095]
printing an ep nov before normalisation:  0.00021312475780632667
Printing some Q and Qe and total Qs values:  [[-0.087]
 [-0.087]
 [-0.089]
 [-0.09 ]
 [-0.089]
 [-0.087]
 [-0.087]] [[15.107]
 [15.107]
 [14.534]
 [14.905]
 [14.81 ]
 [15.107]
 [15.107]] [[0.052]
 [0.052]
 [0.04 ]
 [0.046]
 [0.046]
 [0.052]
 [0.052]]
maxi score, test score, baseline:  -0.9956627118644068 -1.0 -0.9956627118644068
probs:  [0.2579970851558689, 0.2023248953389056, 0.0939106309585015, 0.04111759786891282, 0.2023248953389056, 0.2023248953389056]
maxi score, test score, baseline:  -0.9956627118644068 -1.0 -0.9956627118644068
probs:  [0.21723651421219306, 0.21723651421219306, 0.0952323365673916, 0.035821606583836214, 0.21723651421219306, 0.21723651421219306]
from probs:  [0.21723651421219306, 0.21723651421219306, 0.0952323365673916, 0.035821606583836214, 0.21723651421219306, 0.21723651421219306]
maxi score, test score, baseline:  -0.9956627118644068 -1.0 -0.9956627118644068
probs:  [0.23148578770333356, 0.23148578770333356, 0.10147403488701098, 0.0381639639503666, 0.23148578770333356, 0.16590463805262168]
maxi score, test score, baseline:  -0.9956805907172996 -1.0 -0.9956805907172996
probs:  [0.23148578863882757, 0.23148578863882757, 0.10147403394612631, 0.03816396209576677, 0.23148578863882757, 0.16590463804162406]
maxi score, test score, baseline:  -0.9956805907172996 -1.0 -0.9956805907172996
using explorer policy with actor:  1
siam score:  -0.85894036
maxi score, test score, baseline:  -0.9956805907172996 -1.0 -0.9956805907172996
probs:  [0.23148578863882757, 0.23148578863882757, 0.10147403394612631, 0.03816396209576677, 0.23148578863882757, 0.16590463804162406]
maxi score, test score, baseline:  -0.9956805907172996 -1.0 -0.9956805907172996
probs:  [0.23148578863882757, 0.23148578863882757, 0.10147403394612631, 0.03816396209576677, 0.23148578863882757, 0.16590463804162406]
printing an ep nov before normalisation:  50.1132382683624
printing an ep nov before normalisation:  33.756013224064844
printing an ep nov before normalisation:  22.668066943116393
printing an ep nov before normalisation:  44.788615464490675
maxi score, test score, baseline:  -0.99571589958159 -1.0 -0.99571589958159
maxi score, test score, baseline:  -0.99571589958159 -1.0 -0.99571589958159
Printing some Q and Qe and total Qs values:  [[-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]] [[28.198]
 [28.198]
 [28.198]
 [28.198]
 [28.198]
 [28.198]
 [28.198]] [[56.395]
 [56.395]
 [56.395]
 [56.395]
 [56.395]
 [56.395]
 [56.395]]
maxi score, test score, baseline:  -0.99571589958159 -1.0 -0.99571589958159
probs:  [0.2653980011158203, 0.2653980011158203, 0.11632880905911094, 0.04373859379671404, 0.2653980011158203, 0.04373859379671404]
maxi score, test score, baseline:  -0.99571589958159 -1.0 -0.99571589958159
maxi score, test score, baseline:  -0.99571589958159 -1.0 -0.99571589958159
probs:  [0.2653980011158203, 0.2653980011158203, 0.11632880905911094, 0.04373859379671404, 0.2653980011158203, 0.04373859379671404]
maxi score, test score, baseline:  -0.99571589958159 -1.0 -0.99571589958159
probs:  [0.2653980011158203, 0.2653980011158203, 0.11632880905911094, 0.04373859379671404, 0.2653980011158203, 0.04373859379671404]
Printing some Q and Qe and total Qs values:  [[0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.149]
 [0.118]] [[23.964]
 [23.964]
 [23.964]
 [23.964]
 [23.964]
 [25.912]
 [23.964]] [[0.949]
 [0.949]
 [0.949]
 [0.949]
 [0.949]
 [1.124]
 [0.949]]
maxi score, test score, baseline:  -0.99571589958159 -1.0 -0.99571589958159
siam score:  -0.8507271
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
probs:  [0.2056714530558586, 0.28698483420114623, 0.12578462245698238, 0.04728712804243334, 0.28698483420114623, 0.04728712804243334]
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
probs:  [0.2056714530558586, 0.28698483420114623, 0.12578462245698238, 0.04728712804243334, 0.28698483420114623, 0.04728712804243334]
printing an ep nov before normalisation:  51.89074576389692
siam score:  -0.85324264
printing an ep nov before normalisation:  56.13717449064186
Printing some Q and Qe and total Qs values:  [[0.57 ]
 [0.615]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]] [[50.362]
 [62.953]
 [50.362]
 [50.362]
 [50.362]
 [50.362]
 [50.362]] [[1.588]
 [1.989]
 [1.588]
 [1.588]
 [1.588]
 [1.588]
 [1.588]]
printing an ep nov before normalisation:  62.74657456466507
printing an ep nov before normalisation:  38.11624526977539
using explorer policy with actor:  1
actions average: 
K:  4  action  0 :  tensor([0.2948, 0.0059, 0.1085, 0.1257, 0.1796, 0.1320, 0.1534],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0296, 0.8153, 0.0275, 0.0306, 0.0302, 0.0220, 0.0447],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1788, 0.0062, 0.1753, 0.1354, 0.1694, 0.1735, 0.1615],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1926, 0.0207, 0.1127, 0.1478, 0.1804, 0.1778, 0.1680],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1676, 0.1276, 0.0798, 0.0973, 0.2820, 0.1110, 0.1346],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1564, 0.0693, 0.1383, 0.1295, 0.1595, 0.1893, 0.1578],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2193, 0.0462, 0.1190, 0.1237, 0.1569, 0.1277, 0.2072],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.995750622406639 -1.0 -0.995750622406639
probs:  [0.07851410890752211, 0.20899542788140954, 0.07851410890752211, 0.07851410890752211, 0.47694813648850193, 0.07851410890752211]
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.295]
 [0.3  ]
 [0.302]
 [0.302]
 [0.297]
 [0.294]] [[40.512]
 [42.984]
 [41.522]
 [41.692]
 [41.726]
 [41.964]
 [41.64 ]] [[1.477]
 [1.628]
 [1.545]
 [1.557]
 [1.559]
 [1.568]
 [1.546]]
siam score:  -0.856004
maxi score, test score, baseline:  -0.995750622406639 -1.0 -0.995750622406639
probs:  [0.07671539519191287, 0.25556279126195014, 0.07671539519191287, 0.07671539519191287, 0.43757562797039845, 0.07671539519191287]
printing an ep nov before normalisation:  47.602815160272044
printing an ep nov before normalisation:  39.57806081417609
printing an ep nov before normalisation:  44.294841683684204
siam score:  -0.85465795
printing an ep nov before normalisation:  31.314284801483154
maxi score, test score, baseline:  -0.995750622406639 -1.0 -0.995750622406639
probs:  [0.13714171913424453, 0.22419570704494232, 0.13714171913424453, 0.05158866204959177, 0.31279047350273237, 0.13714171913424453]
maxi score, test score, baseline:  -0.995750622406639 -1.0 -0.995750622406639
probs:  [0.13714171913424453, 0.22419570704494232, 0.13714171913424453, 0.05158866204959177, 0.31279047350273237, 0.13714171913424453]
from probs:  [0.16632660452661166, 0.16632660452661166, 0.16632660452661166, 0.05002535262765177, 0.2846682292659015, 0.16632660452661166]
maxi score, test score, baseline:  -0.995750622406639 -1.0 -0.995750622406639
probs:  [0.18821867758069274, 0.05659746807003451, 0.18821867758069274, 0.05659746807003451, 0.3221490311178528, 0.18821867758069274]
siam score:  -0.8606285
printing an ep nov before normalisation:  33.994781642924465
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.43783241397601
siam score:  -0.85975057
printing an ep nov before normalisation:  18.249185225569722
printing an ep nov before normalisation:  19.357442286204524
using explorer policy with actor:  1
siam score:  -0.86101896
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.21005448939727728, 0.03700048011071595, 0.21005448939727728, 0.12278156230017509, 0.21005448939727728, 0.21005448939727728]
siam score:  -0.85997254
printing an ep nov before normalisation:  56.44781028369413
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
printing an ep nov before normalisation:  22.610478401184082
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.23014428686920402, 0.04052873467621696, 0.23014428686920402, 0.13451920235808545, 0.23014428686920402, 0.13451920235808545]
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.1920446598333833, 0.03977670083308339, 0.1920446598333833, 0.1920446598333833, 0.1920446598333833, 0.1920446598333833]
Printing some Q and Qe and total Qs values:  [[0.238]
 [1.401]
 [0.238]
 [0.238]
 [0.238]
 [0.238]
 [0.238]] [[50.158]
 [ 0.124]
 [50.158]
 [50.158]
 [50.158]
 [50.158]
 [50.158]] [[2.042]
 [1.401]
 [2.042]
 [2.042]
 [2.042]
 [2.042]
 [2.042]]
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.22654794787192478, 0.0469041042561504, 0.22654794787192478, 0.22654794787192478, 0.22654794787192478, 0.0469041042561504]
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.22654794787192478, 0.0469041042561504, 0.22654794787192478, 0.22654794787192478, 0.22654794787192478, 0.0469041042561504]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
printing an ep nov before normalisation:  49.24835184009968
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.2761772242784254, 0.05715610905490792, 0.2761772242784254, 0.05715610905490792, 0.2761772242784254, 0.05715610905490792]
printing an ep nov before normalisation:  28.101767249424103
Printing some Q and Qe and total Qs values:  [[0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.389]
 [0.35 ]
 [0.35 ]] [[43.134]
 [43.134]
 [43.134]
 [43.134]
 [61.098]
 [43.134]
 [43.134]] [[1.232]
 [1.232]
 [1.232]
 [1.232]
 [1.73 ]
 [1.232]
 [1.232]]
printing an ep nov before normalisation:  60.5709465706625
actions average: 
K:  2  action  0 :  tensor([0.2819, 0.0145, 0.1219, 0.1332, 0.1451, 0.1426, 0.1607],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0203, 0.8775, 0.0172, 0.0201, 0.0149, 0.0180, 0.0320],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1552, 0.0061, 0.2045, 0.1362, 0.1449, 0.1625, 0.1905],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1697, 0.0232, 0.1129, 0.2434, 0.1411, 0.1258, 0.1840],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2239, 0.0253, 0.1073, 0.1163, 0.2510, 0.1261, 0.1501],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2065, 0.0045, 0.1189, 0.1256, 0.1452, 0.2322, 0.1671],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2041, 0.0173, 0.1109, 0.1404, 0.1571, 0.1528, 0.2173],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.2761772242784254, 0.05715610905490792, 0.2761772242784254, 0.05715610905490792, 0.2761772242784254, 0.05715610905490792]
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.2761772242784254, 0.05715610905490792, 0.2761772242784254, 0.05715610905490792, 0.2761772242784254, 0.05715610905490792]
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.2544351724736222, 0.14875472670606438, 0.14875472670606438, 0.04486547493456267, 0.2544351724736222, 0.14875472670606438]
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
probs:  [0.32192744343944446, 0.056737503550174116, 0.18819918315340245, 0.056737503550174116, 0.18819918315340245, 0.18819918315340245]
printing an ep nov before normalisation:  30.915842056274414
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
probs:  [0.32192744343944446, 0.056737503550174116, 0.18819918315340245, 0.056737503550174116, 0.18819918315340245, 0.18819918315340245]
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
probs:  [0.32192744343944446, 0.056737503550174116, 0.18819918315340245, 0.056737503550174116, 0.18819918315340245, 0.18819918315340245]
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
probs:  [0.32192744343944446, 0.056737503550174116, 0.18819918315340245, 0.056737503550174116, 0.18819918315340245, 0.18819918315340245]
siam score:  -0.86351895
maxi score, test score, baseline:  -0.9958183673469387 -1.0 -0.9958183673469387
probs:  [0.3219274480220726, 0.05673750030553882, 0.1881991837889499, 0.05673750030553882, 0.1881991837889499, 0.1881991837889499]
maxi score, test score, baseline:  -0.9958183673469387 -1.0 -0.9958183673469387
probs:  [0.370678987607397, 0.06531299762788081, 0.2166910097544803, 0.06531299762788081, 0.2166910097544803, 0.06531299762788081]
maxi score, test score, baseline:  -0.9958183673469387 -1.0 -0.9958183673469387
probs:  [0.370678987607397, 0.06531299762788081, 0.2166910097544803, 0.06531299762788081, 0.2166910097544803, 0.06531299762788081]
printing an ep nov before normalisation:  18.86366331997013
actions average: 
K:  2  action  0 :  tensor([0.3148, 0.0296, 0.1092, 0.1257, 0.1639, 0.1309, 0.1260],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0048, 0.9648, 0.0069, 0.0047, 0.0036, 0.0024, 0.0127],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1914, 0.0113, 0.1726, 0.1512, 0.1876, 0.1249, 0.1610],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1986, 0.0652, 0.1422, 0.2149, 0.1264, 0.1213, 0.1314],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1899, 0.0074, 0.1460, 0.1483, 0.2447, 0.1241, 0.1397],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1808, 0.0222, 0.1646, 0.1280, 0.1336, 0.2431, 0.1277],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1902, 0.0639, 0.1294, 0.1389, 0.1119, 0.1118, 0.2539],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9958349593495935 -1.0 -0.9958349593495935
probs:  [0.37067899448450886, 0.06531299421132217, 0.21669101144076228, 0.06531299421132217, 0.21669101144076228, 0.06531299421132217]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.37719716601953
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.4368396407864261, 0.0769507834200674, 0.0769507834200674, 0.0769507834200674, 0.25535722553330437, 0.0769507834200674]
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]] [[64.466]
 [48.944]
 [48.944]
 [48.944]
 [48.944]
 [48.944]
 [48.944]] [[1.509]
 [1.149]
 [1.149]
 [1.149]
 [1.149]
 [1.149]
 [1.149]]
printing an ep nov before normalisation:  50.54242566593204
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.4368396407864261, 0.0769507834200674, 0.0769507834200674, 0.0769507834200674, 0.25535722553330437, 0.0769507834200674]
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.4368396407864261, 0.0769507834200674, 0.0769507834200674, 0.0769507834200674, 0.25535722553330437, 0.0769507834200674]
printing an ep nov before normalisation:  58.12602228171832
printing an ep nov before normalisation:  39.483802318573
Printing some Q and Qe and total Qs values:  [[0.174]
 [0.144]
 [0.189]
 [0.191]
 [0.191]
 [0.191]
 [0.19 ]] [[53.758]
 [32.919]
 [37.554]
 [31.725]
 [30.287]
 [29.206]
 [26.152]] [[1.061]
 [0.548]
 [0.7  ]
 [0.568]
 [0.535]
 [0.509]
 [0.437]]
actions average: 
K:  1  action  0 :  tensor([0.3034, 0.0389, 0.1234, 0.1220, 0.1387, 0.1333, 0.1403],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0504, 0.7228, 0.0410, 0.0508, 0.0455, 0.0397, 0.0498],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1585, 0.0027, 0.1680, 0.1447, 0.1642, 0.1845, 0.1773],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1950, 0.0311, 0.1186, 0.1799, 0.1465, 0.1472, 0.1817],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2009, 0.0203, 0.1322, 0.1434, 0.1774, 0.1581, 0.1677],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1447, 0.0042, 0.1116, 0.1285, 0.1468, 0.3059, 0.1584],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1859, 0.0328, 0.1240, 0.1292, 0.1407, 0.1417, 0.2457],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.3125549437284067, 0.13719681067349013, 0.051728140781178, 0.13719681067349013, 0.22412648346994493, 0.13719681067349013]
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.3125549437284067, 0.13719681067349013, 0.051728140781178, 0.13719681067349013, 0.22412648346994493, 0.13719681067349013]
Printing some Q and Qe and total Qs values:  [[0.3]
 [0.3]
 [0.3]
 [0.3]
 [0.3]
 [0.3]
 [0.3]] [[48.132]
 [48.132]
 [48.132]
 [48.132]
 [48.132]
 [48.132]
 [48.132]] [[0.3]
 [0.3]
 [0.3]
 [0.3]
 [0.3]
 [0.3]
 [0.3]]
printing an ep nov before normalisation:  36.02463715700906
printing an ep nov before normalisation:  30.46501073520055
actions average: 
K:  1  action  0 :  tensor([0.3316, 0.0034, 0.1054, 0.1358, 0.1324, 0.1376, 0.1539],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0212, 0.8362, 0.0266, 0.0372, 0.0207, 0.0257, 0.0324],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1778, 0.0186, 0.2008, 0.1474, 0.1530, 0.1312, 0.1711],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1668, 0.0719, 0.1193, 0.1566, 0.1946, 0.1333, 0.1576],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2052, 0.0071, 0.1323, 0.1655, 0.1761, 0.1271, 0.1867],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2003, 0.0167, 0.1187, 0.1419, 0.1441, 0.2092, 0.1691],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2142, 0.0289, 0.1260, 0.1463, 0.1497, 0.1402, 0.1947],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  37.661861229240024
printing an ep nov before normalisation:  32.68980953694566
siam score:  -0.8612004
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.3125549437284067, 0.13719681067349013, 0.051728140781178, 0.13719681067349013, 0.22412648346994493, 0.13719681067349013]
printing an ep nov before normalisation:  29.360148481674834
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.3125549437284067, 0.13719681067349013, 0.051728140781178, 0.13719681067349013, 0.22412648346994493, 0.13719681067349013]
Printing some Q and Qe and total Qs values:  [[0.24 ]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.531]] [[60.922]
 [56.846]
 [56.846]
 [56.846]
 [56.846]
 [56.846]
 [39.169]] [[0.24 ]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.531]]
from probs:  [0.2543842223327782, 0.14876814350376608, 0.04492712482314528, 0.14876814350376608, 0.2543842223327782, 0.14876814350376608]
printing an ep nov before normalisation:  50.952302996866045
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
Printing some Q and Qe and total Qs values:  [[0.338]
 [0.418]
 [0.343]
 [0.452]
 [0.331]
 [0.354]
 [0.323]] [[31.913]
 [35.775]
 [31.619]
 [43.51 ]
 [34.164]
 [31.713]
 [31.187]] [[0.718]
 [0.892]
 [0.715]
 [1.115]
 [0.765]
 [0.729]
 [0.685]]
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.3210956953584882, 0.1877676101275799, 0.05668033305181455, 0.05668033305181455, 0.3210956953584882, 0.05668033305181455]
printing an ep nov before normalisation:  42.55773834893948
line 256 mcts: sample exp_bonus 39.551314837159
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.3210956953584882, 0.1877676101275799, 0.05668033305181455, 0.05668033305181455, 0.3210956953584882, 0.05668033305181455]
printing an ep nov before normalisation:  16.96453526674759
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.37051787782619994, 0.21665977553454294, 0.06538752370157137, 0.06538752370157137, 0.21665977553454294, 0.06538752370157137]
printing an ep nov before normalisation:  59.16611656529141
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.37051787782619994, 0.21665977553454294, 0.06538752370157137, 0.06538752370157137, 0.21665977553454294, 0.06538752370157137]
line 256 mcts: sample exp_bonus 43.56513333320618
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
printing an ep nov before normalisation:  36.966642480611824
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.70043819323654
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.3531574953658095, 0.3531574953658095, 0.07342125231709527, 0.07342125231709527, 0.07342125231709527, 0.07342125231709527]
printing an ep nov before normalisation:  35.09635481140847
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.3531574953658095, 0.3531574953658095, 0.07342125231709527, 0.07342125231709527, 0.07342125231709527, 0.07342125231709527]
siam score:  -0.8600181
printing an ep nov before normalisation:  37.94791738227781
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.2843578724577786, 0.16633883600986735, 0.16633883600986735, 0.05028678350275206, 0.16633883600986735, 0.16633883600986735]
printing an ep nov before normalisation:  52.18044308443699
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.2843578724577786, 0.16633883600986735, 0.16633883600986735, 0.05028678350275206, 0.16633883600986735, 0.16633883600986735]
printing an ep nov before normalisation:  57.79725634403651
Printing some Q and Qe and total Qs values:  [[0.162]
 [0.166]
 [0.158]
 [0.149]
 [0.145]
 [0.138]
 [0.154]] [[41.951]
 [42.529]
 [41.64 ]
 [43.224]
 [43.928]
 [44.377]
 [42.772]] [[0.641]
 [0.658]
 [0.629]
 [0.658]
 [0.67 ]
 [0.674]
 [0.652]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.22638978956484204, 0.047220420870315895, 0.22638978956484204, 0.047220420870315895, 0.22638978956484204, 0.22638978956484204]
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.209975610512369, 0.03721971262369547, 0.209975610512369, 0.12287784532682869, 0.209975610512369, 0.209975610512369]
printing an ep nov before normalisation:  61.866958169355044
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.209975610512369, 0.03721971262369547, 0.209975610512369, 0.12287784532682869, 0.209975610512369, 0.209975610512369]
printing an ep nov before normalisation:  51.14553880664514
printing an ep nov before normalisation:  51.17577493468686
Printing some Q and Qe and total Qs values:  [[0.258]
 [0.329]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]] [[54.089]
 [53.729]
 [54.089]
 [54.089]
 [54.089]
 [54.089]
 [54.089]] [[1.21]
 [1.27]
 [1.21]
 [1.21]
 [1.21]
 [1.21]
 [1.21]]
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.209975610512369, 0.03721971262369547, 0.209975610512369, 0.12287784532682869, 0.209975610512369, 0.209975610512369]
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.23001370400123125, 0.04076114432950454, 0.13459887183340086, 0.13459887183340086, 0.23001370400123125, 0.23001370400123125]
UNIT TEST: sample policy line 217 mcts : [0.347 0.122 0.367 0.082 0.041 0.02  0.02 ]
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.23001370400123125, 0.04076114432950454, 0.13459887183340086, 0.13459887183340086, 0.23001370400123125, 0.23001370400123125]
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.24720679134737264, 0.1086512772080619, 0.041077071541758395, 0.1086512772080619, 0.24720679134737264, 0.24720679134737264]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.2657782917357843, 0.11680813129925385, 0.04415465141422432, 0.11680813129925385, 0.2657782917357843, 0.1906725025156993]
printing an ep nov before normalisation:  62.97403547293037
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.2657782917357843, 0.11680813129925385, 0.04415465141422432, 0.11680813129925385, 0.2657782917357843, 0.1906725025156993]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.6246],
        [-0.6380],
        [-0.3953],
        [-0.6343],
        [-0.0000],
        [-0.5461],
        [-0.5793],
        [-0.6426],
        [-0.5765],
        [-0.3659]], dtype=torch.float64)
-0.032346567066 -0.6569875459019582
-0.032346567066 -0.6703179264538828
-0.070771701198 -0.46603868805720217
-0.032346567066 -0.6666515432054957
-0.36293138303399963 -0.36293138303399963
-0.09703970119800001 -0.6431086205609798
-0.09703970119800001 -0.6763108964957443
-0.09703970119800001 -0.7396611511350936
-0.032346567066 -0.6088741927023225
-0.032346567066 -0.3982062892952314
siam score:  -0.8645439
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.2657782917357843, 0.11680813129925385, 0.04415465141422432, 0.11680813129925385, 0.2657782917357843, 0.1906725025156993]
printing an ep nov before normalisation:  56.64623402480981
printing an ep nov before normalisation:  49.0318498474371
from probs:  [0.2657782917357843, 0.11680813129925385, 0.04415465141422432, 0.11680813129925385, 0.2657782917357843, 0.1906725025156993]
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.286983237834381, 0.12612163035535498, 0.04766863326517299, 0.12612163035535498, 0.286983237834381, 0.12612163035535498]
printing an ep nov before normalisation:  35.5779242515564
printing an ep nov before normalisation:  0.004427974143084157
Printing some Q and Qe and total Qs values:  [[0.367]
 [0.44 ]
 [0.38 ]
 [0.378]
 [0.384]
 [0.374]
 [0.386]] [[30.867]
 [36.705]
 [28.963]
 [28.775]
 [30.321]
 [28.87 ]
 [29.79 ]] [[0.367]
 [0.44 ]
 [0.38 ]
 [0.378]
 [0.384]
 [0.374]
 [0.386]]
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
printing an ep nov before normalisation:  51.35800876590132
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
printing an ep nov before normalisation:  27.342846393585205
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.2270284411101732, 0.1658304920274653, 0.046419859670957436, 0.1658304920274653, 0.28925492715225537, 0.10563578801168341]
printing an ep nov before normalisation:  46.85243661134732
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.2270284411101732, 0.1658304920274653, 0.046419859670957436, 0.1658304920274653, 0.28925492715225537, 0.10563578801168341]
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.2270284411101732, 0.1658304920274653, 0.046419859670957436, 0.1658304920274653, 0.28925492715225537, 0.10563578801168341]
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.2270284411101732, 0.1658304920274653, 0.046419859670957436, 0.1658304920274653, 0.28925492715225537, 0.10563578801168341]
Printing some Q and Qe and total Qs values:  [[0.275]
 [0.335]
 [0.327]
 [0.282]
 [0.318]
 [0.282]
 [0.278]] [[37.891]
 [41.403]
 [42.965]
 [40.922]
 [42.699]
 [40.791]
 [40.413]] [[1.057]
 [1.257]
 [1.312]
 [1.185]
 [1.292]
 [1.179]
 [1.161]]
printing an ep nov before normalisation:  48.64615424218794
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.2270284411101732, 0.1658304920274653, 0.046419859670957436, 0.1658304920274653, 0.28925492715225537, 0.10563578801168341]
siam score:  -0.865855
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.2474583033222371, 0.1775575134436927, 0.04116572831482167, 0.1775575134436927, 0.2474583033222371, 0.1088026381533187]
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.2474583033222371, 0.1775575134436927, 0.04116572831482167, 0.1775575134436927, 0.2474583033222371, 0.1088026381533187]
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.2474583046025343, 0.17755751361627764, 0.041165726326023035, 0.17755751361627764, 0.2474583046025343, 0.10880263723635315]
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.26573428137356303, 0.19066820248587882, 0.04419780465625333, 0.11683271505537086, 0.26573428137356303, 0.11683271505537086]
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.26573428137356303, 0.19066820248587882, 0.04419780465625333, 0.11683271505537086, 0.26573428137356303, 0.11683271505537086]
printing an ep nov before normalisation:  34.933392666279794
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
printing an ep nov before normalisation:  55.44774528687306
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
Printing some Q and Qe and total Qs values:  [[0.209]
 [0.209]
 [0.209]
 [0.201]
 [0.195]
 [0.209]
 [0.209]] [[31.927]
 [31.927]
 [31.927]
 [30.01 ]
 [31.783]
 [32.172]
 [31.927]] [[0.867]
 [0.867]
 [0.867]
 [0.79 ]
 [0.847]
 [0.875]
 [0.867]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.494]
 [0.433]
 [0.42 ]
 [0.393]
 [0.392]
 [0.398]] [[38.764]
 [40.067]
 [38.285]
 [38.481]
 [39.278]
 [38.736]
 [37.803]] [[0.795]
 [0.892]
 [0.796]
 [0.787]
 [0.776]
 [0.764]
 [0.752]]
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.3403336266161269, 0.14961009024305755, 0.0565742188415629, 0.0565742188415629, 0.3403336266161269, 0.0565742188415629]
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.3403336266161269, 0.14961009024305755, 0.0565742188415629, 0.0565742188415629, 0.3403336266161269, 0.0565742188415629]
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.3403336266161269, 0.14961009024305755, 0.0565742188415629, 0.0565742188415629, 0.3403336266161269, 0.0565742188415629]
printing an ep nov before normalisation:  35.13175131304784
printing an ep nov before normalisation:  22.846034783880047
printing an ep nov before normalisation:  49.811158180236816
printing an ep nov before normalisation:  44.66785266891085
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.34033363037132186, 0.14961008987424307, 0.05657421646103774, 0.05657421646103774, 0.34033363037132186, 0.05657421646103774]
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.30706835646097475, 0.049316000569931226, 0.11218242883603978, 0.11218242883603978, 0.30706835646097475, 0.11218242883603978]
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.01579284667969
actions average: 
K:  2  action  0 :  tensor([0.3230, 0.0140, 0.1137, 0.1188, 0.1536, 0.1182, 0.1588],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0232, 0.8552, 0.0180, 0.0258, 0.0283, 0.0166, 0.0328],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1173, 0.0219, 0.1892, 0.1644, 0.1278, 0.1937, 0.1858],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1529, 0.0163, 0.1405, 0.1664, 0.1660, 0.1834, 0.1745],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1938, 0.0056, 0.0947, 0.1256, 0.3160, 0.1212, 0.1431],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1727, 0.0160, 0.1387, 0.1352, 0.1530, 0.2052, 0.1793],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1733, 0.0043, 0.1315, 0.1351, 0.1764, 0.1431, 0.2362],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.2767128633146218, 0.056600586344188415, 0.1287783248629483, 0.1287783248629483, 0.3525293142711047, 0.056600586344188415]
printing an ep nov before normalisation:  42.141924964057075
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.2767128633146218, 0.056600586344188415, 0.1287783248629483, 0.1287783248629483, 0.3525293142711047, 0.056600586344188415]
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.397]
 [0.403]
 [0.398]
 [0.397]
 [0.397]
 [0.397]] [[42.323]
 [38.771]
 [52.197]
 [43.285]
 [41.946]
 [38.771]
 [38.771]] [[1.098]
 [0.982]
 [1.4  ]
 [1.122]
 [1.079]
 [0.982]
 [0.982]]
printing an ep nov before normalisation:  51.28855988767441
printing an ep nov before normalisation:  47.66715597950469
printing an ep nov before normalisation:  36.819195539766454
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.29824727488800795, 0.060997118847572626, 0.13879459548956005, 0.060997118847572626, 0.37996677307971416, 0.060997118847572626]
Printing some Q and Qe and total Qs values:  [[0.055]
 [0.055]
 [0.055]
 [0.055]
 [0.055]
 [0.055]
 [0.055]] [[23.456]
 [23.456]
 [23.456]
 [23.456]
 [23.456]
 [23.456]
 [23.456]] [[1.489]
 [1.489]
 [1.489]
 [1.489]
 [1.489]
 [1.489]
 [1.489]]
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.23691515907754135, 0.06632158421993864, 0.15092490183224655, 0.06632158421993864, 0.4131951864303961, 0.06632158421993864]
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.618]
 [0.578]
 [0.558]
 [0.557]
 [0.557]
 [0.557]] [[58.938]
 [52.099]
 [65.796]
 [59.274]
 [58.45 ]
 [56.975]
 [55.08 ]] [[1.235]
 [1.149]
 [1.394]
 [1.239]
 [1.221]
 [1.19 ]
 [1.15 ]]
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.23691515907754135, 0.06632158421993864, 0.15092490183224655, 0.06632158421993864, 0.4131951864303961, 0.06632158421993864]
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.414]
 [0.445]
 [0.511]
 [0.476]
 [0.471]
 [0.472]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.494]
 [0.414]
 [0.445]
 [0.511]
 [0.476]
 [0.471]
 [0.472]]
printing an ep nov before normalisation:  55.89176482170285
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.25804883804445145, 0.12013441698819192, 0.12013441698819192, 0.12013441698819192, 0.32871573147823674, 0.05283217951273606]
Printing some Q and Qe and total Qs values:  [[0.32]
 [0.32]
 [0.32]
 [0.32]
 [0.32]
 [0.32]
 [0.32]] [[28.308]
 [28.308]
 [28.308]
 [28.308]
 [28.308]
 [28.308]
 [28.308]] [[0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
printing an ep nov before normalisation:  42.11289737922663
printing an ep nov before normalisation:  54.512304067611694
printing an ep nov before normalisation:  55.220403327531805
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
printing an ep nov before normalisation:  46.08422310703712
Printing some Q and Qe and total Qs values:  [[0.247]
 [0.255]
 [0.247]
 [0.243]
 [0.233]
 [0.223]
 [0.237]] [[39.477]
 [20.087]
 [17.58 ]
 [16.917]
 [15.095]
 [14.122]
 [14.041]] [[1.232]
 [0.623]
 [0.535]
 [0.51 ]
 [0.442]
 [0.401]
 [0.413]]
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.20261860676190588, 0.12910761012709232, 0.12910761012709232, 0.12910761012709232, 0.3532857734183843, 0.05677278943843297]
printing an ep nov before normalisation:  37.970961289309955
printing an ep nov before normalisation:  29.330781846168737
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.513]
 [0.455]
 [0.448]
 [0.451]
 [0.457]
 [0.452]] [[46.38 ]
 [47.022]
 [47.783]
 [47.947]
 [46.389]
 [45.348]
 [45.077]] [[0.468]
 [0.513]
 [0.455]
 [0.448]
 [0.451]
 [0.457]
 [0.452]]
actions average: 
K:  3  action  0 :  tensor([0.2605, 0.0212, 0.1247, 0.1278, 0.1646, 0.1458, 0.1555],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0283, 0.8081, 0.0272, 0.0350, 0.0227, 0.0281, 0.0506],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.2379, 0.0030, 0.1340, 0.1371, 0.1600, 0.1608, 0.1672],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1777, 0.0094, 0.1363, 0.1845, 0.1571, 0.1577, 0.1773],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1695, 0.0140, 0.1150, 0.1454, 0.2515, 0.1374, 0.1672],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1491, 0.0243, 0.1259, 0.1334, 0.1515, 0.2532, 0.1624],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1680, 0.0268, 0.1458, 0.1362, 0.1647, 0.1585, 0.1999],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.15033452201293035, 0.15033452201293035, 0.15033452201293035, 0.15033452201293035, 0.3417268421739046, 0.056935069774374104]
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.15033452201293035, 0.15033452201293035, 0.15033452201293035, 0.15033452201293035, 0.3417268421739046, 0.056935069774374104]
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.15033452201293035, 0.15033452201293035, 0.15033452201293035, 0.15033452201293035, 0.3417268421739046, 0.056935069774374104]
siam score:  -0.8638935
printing an ep nov before normalisation:  42.58714674653209
Printing some Q and Qe and total Qs values:  [[0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]]
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
probs:  [0.15033452165254219, 0.15033452165254219, 0.15033452165254219, 0.15033452165254219, 0.34172684603681464, 0.05693506735301658]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
probs:  [0.06279204899766314, 0.1658221575799258, 0.1658221575799258, 0.1658221575799258, 0.3769494292648962, 0.06279204899766314]
printing an ep nov before normalisation:  41.50109989621355
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
probs:  [0.06279204899766314, 0.1658221575799258, 0.1658221575799258, 0.1658221575799258, 0.3769494292648962, 0.06279204899766314]
siam score:  -0.86763924
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
probs:  [0.06999575078987304, 0.06999575078987304, 0.18487093004606184, 0.18487093004606184, 0.4202708875382571, 0.06999575078987304]
printing an ep nov before normalisation:  58.83009121399793
printing an ep nov before normalisation:  41.89638713219758
Printing some Q and Qe and total Qs values:  [[0.36 ]
 [0.392]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]] [[32.598]
 [44.308]
 [32.598]
 [32.598]
 [32.598]
 [32.598]
 [32.598]] [[0.36 ]
 [0.392]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]]
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
probs:  [0.12029077511170018, 0.052926690412860106, 0.18874137730568386, 0.18874137730568386, 0.3290090047523719, 0.12029077511170018]
printing an ep nov before normalisation:  34.062625356113394
from probs:  [0.12029077511170018, 0.052926690412860106, 0.18874137730568386, 0.18874137730568386, 0.3290090047523719, 0.12029077511170018]
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
probs:  [0.12912758006879455, 0.05680930576037214, 0.20261227815638466, 0.12912758006879455, 0.35319567587685957, 0.12912758006879455]
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.386]
 [0.386]
 [0.456]
 [0.465]
 [0.386]
 [0.386]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.386]
 [0.386]
 [0.386]
 [0.456]
 [0.465]
 [0.386]
 [0.386]]
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
probs:  [0.13936734176537974, 0.06130833462576331, 0.13936734176537974, 0.13936734176537974, 0.38122229831271787, 0.13936734176537974]
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
probs:  [0.13936734176537974, 0.06130833462576331, 0.13936734176537974, 0.13936734176537974, 0.38122229831271787, 0.13936734176537974]
printing an ep nov before normalisation:  28.27321791630421
maxi score, test score, baseline:  -0.995978431372549 -1.0 -0.995978431372549
probs:  [0.1393673412618637, 0.06130833268250704, 0.1393673412618637, 0.1393673412618637, 0.3812223022700382, 0.1393673412618637]
printing an ep nov before normalisation:  40.242482224071225
printing an ep nov before normalisation:  65.33852524975352
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
probs:  [0.15034702609578962, 0.056983500969375725, 0.15034702609578962, 0.15034702609578962, 0.3416283946474659, 0.15034702609578962]
printing an ep nov before normalisation:  62.77505150654653
actions average: 
K:  4  action  0 :  tensor([0.2686, 0.0148, 0.1290, 0.1297, 0.1537, 0.1268, 0.1774],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0123, 0.8585, 0.0233, 0.0200, 0.0109, 0.0509, 0.0241],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1557, 0.0022, 0.1974, 0.1380, 0.1530, 0.1944, 0.1593],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1696, 0.0290, 0.1306, 0.1920, 0.1558, 0.1417, 0.1814],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1892, 0.0489, 0.1335, 0.1261, 0.1589, 0.1417, 0.2016],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1627, 0.1039, 0.1327, 0.1138, 0.1321, 0.1945, 0.1603],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1810, 0.1099, 0.1114, 0.1457, 0.1384, 0.1340, 0.1795],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
probs:  [0.15034702609578962, 0.056983500969375725, 0.15034702609578962, 0.15034702609578962, 0.3416283946474659, 0.15034702609578962]
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
probs:  [0.16582937894600555, 0.06284298930473986, 0.06284298930473986, 0.16582937894600555, 0.3768258845525037, 0.16582937894600555]
printing an ep nov before normalisation:  48.27733193055185
printing an ep nov before normalisation:  21.440763379897447
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
probs:  [0.16582937894600555, 0.06284298930473986, 0.06284298930473986, 0.16582937894600555, 0.3768258845525037, 0.16582937894600555]
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
probs:  [0.16582937894600555, 0.06284298930473986, 0.06284298930473986, 0.16582937894600555, 0.3768258845525037, 0.16582937894600555]
from probs:  [0.16582937894600555, 0.06284298930473986, 0.06284298930473986, 0.16582937894600555, 0.3768258845525037, 0.16582937894600555]
printing an ep nov before normalisation:  49.663052504869384
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
probs:  [0.18874000637012445, 0.05296202711860906, 0.12031221523940001, 0.12031221523940001, 0.32893352966234185, 0.18874000637012445]
Printing some Q and Qe and total Qs values:  [[0.17 ]
 [0.249]
 [0.043]
 [0.254]
 [0.253]
 [0.044]
 [0.249]] [[44.178]
 [40.405]
 [39.983]
 [45.757]
 [44.093]
 [37.077]
 [37.946]] [[0.17 ]
 [0.249]
 [0.043]
 [0.254]
 [0.253]
 [0.044]
 [0.249]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
probs:  [0.1887400070700199, 0.05296202351329392, 0.12031221376960712, 0.12031221376960712, 0.3289335348074521, 0.1887400070700199]
printing an ep nov before normalisation:  27.360861195423386
printing an ep nov before normalisation:  54.651011690603646
printing an ep nov before normalisation:  66.8667086491844
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
probs:  [0.1887400070700199, 0.05296202351329392, 0.12031221376960712, 0.12031221376960712, 0.3289335348074521, 0.1887400070700199]
printing an ep nov before normalisation:  21.85643304766529
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.33518182688799
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]] [[41.193]
 [41.193]
 [41.193]
 [41.193]
 [41.193]
 [41.193]
 [41.193]] [[2.123]
 [2.123]
 [2.123]
 [2.123]
 [2.123]
 [2.123]
 [2.123]]
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
probs:  [0.16583648474416532, 0.06289392635401435, 0.16583648474416532, 0.16583648474416532, 0.37670269305947546, 0.06289392635401435]
siam score:  -0.86418885
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
probs:  [0.16583648474416532, 0.06289392635401435, 0.16583648474416532, 0.16583648474416532, 0.37670269305947546, 0.06289392635401435]
printing an ep nov before normalisation:  36.97465983060687
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
probs:  [0.16583648474416532, 0.06289392635401435, 0.16583648474416532, 0.16583648474416532, 0.37670269305947546, 0.06289392635401435]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
probs:  [0.16583648474416532, 0.06289392635401435, 0.16583648474416532, 0.16583648474416532, 0.37670269305947546, 0.06289392635401435]
printing an ep nov before normalisation:  14.639132022857666
Printing some Q and Qe and total Qs values:  [[0.149]
 [0.176]
 [0.165]
 [0.176]
 [0.176]
 [0.176]
 [0.176]] [[51.967]
 [36.762]
 [41.428]
 [36.762]
 [36.762]
 [36.762]
 [36.762]] [[1.233]
 [0.942]
 [1.029]
 [0.942]
 [0.942]
 [0.942]
 [0.942]]
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.16583648472428275, 0.06289392386867866, 0.16583648472428275, 0.16583648472428275, 0.37670269808979434, 0.06289392386867866]
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.16583648472428275, 0.06289392386867866, 0.16583648472428275, 0.16583648472428275, 0.37670269808979434, 0.06289392386867866]
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
printing an ep nov before normalisation:  32.63703933261107
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.07010250255411296, 0.07010250255411296, 0.1848688573460302, 0.1848688573460302, 0.4199547776456007, 0.07010250255411296]
printing an ep nov before normalisation:  23.181403821062293
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.07443638975371059, 0.07443638975371059, 0.3511272204925788, 0.07443638975371059, 0.3511272204925788, 0.07443638975371059]
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.07443638386253534, 0.07443638386253534, 0.3511272322749293, 0.07443638386253534, 0.3511272322749293, 0.07443638386253534]
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.07443638386253534, 0.07443638386253534, 0.3511272322749293, 0.07443638386253534, 0.3511272322749293, 0.07443638386253534]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
printing an ep nov before normalisation:  52.90942192077637
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.1028913665279385, 0.1028913665279385, 0.4855431673603075, 0.1028913665279385, 0.1028913665279385, 0.1028913665279385]
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.462]
 [0.415]
 [0.467]
 [0.446]
 [0.347]
 [0.392]] [[45.046]
 [51.541]
 [49.444]
 [49.899]
 [49.374]
 [46.526]
 [54.219]] [[1.67 ]
 [1.979]
 [1.832]
 [1.906]
 [1.86 ]
 [1.625]
 [2.037]]
printing an ep nov before normalisation:  46.45784073721792
printing an ep nov before normalisation:  38.06777943261296
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
probs:  [0.10289136093770622, 0.10289136093770622, 0.48554319531146894, 0.10289136093770622, 0.10289136093770622, 0.10289136093770622]
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
printing an ep nov before normalisation:  53.90698313007607
printing an ep nov before normalisation:  42.336355942521756
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.037]
 [ 0.004]
 [ 0.003]
 [-0.002]
 [-0.002]
 [ 0.003]
 [-0.001]] [[36.285]
 [27.876]
 [22.852]
 [24.162]
 [24.859]
 [22.852]
 [28.099]] [[0.882]
 [0.608]
 [0.419]
 [0.463]
 [0.49 ]
 [0.419]
 [0.612]]
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
probs:  [0.19183733159108543, 0.19183733159108543, 0.19183733159108543, 0.19183733159108543, 0.19183733159108543, 0.04081334204457289]
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.509428885609527
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
probs:  [0.19183733159108543, 0.19183733159108543, 0.19183733159108543, 0.19183733159108543, 0.19183733159108543, 0.04081334204457289]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
probs:  [0.19183733159108543, 0.19183733159108543, 0.19183733159108543, 0.19183733159108543, 0.19183733159108543, 0.04081334204457289]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
probs:  [0.22597163454228697, 0.22597163454228697, 0.22597163454228697, 0.22597163454228697, 0.04805673091542602, 0.04805673091542602]
printing an ep nov before normalisation:  35.710102167329154
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
probs:  [0.22597163454228697, 0.22597163454228697, 0.22597163454228697, 0.22597163454228697, 0.04805673091542602, 0.04805673091542602]
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
probs:  [0.22597163454228697, 0.22597163454228697, 0.22597163454228697, 0.22597163454228697, 0.04805673091542602, 0.04805673091542602]
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
probs:  [0.22597163454228697, 0.22597163454228697, 0.22597163454228697, 0.22597163454228697, 0.04805673091542602, 0.04805673091542602]
printing an ep nov before normalisation:  43.00496246113134
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
probs:  [0.13475372751217066, 0.229756092533633, 0.229756092533633, 0.229756092533633, 0.13475372751217066, 0.04122426737475972]
printing an ep nov before normalisation:  46.84187005566364
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.399]
 [0.392]
 [0.392]
 [0.388]
 [0.385]
 [0.386]] [[33.199]
 [39.544]
 [33.673]
 [33.731]
 [33.861]
 [34.381]
 [32.94 ]] [[0.757]
 [0.929]
 [0.782]
 [0.783]
 [0.783]
 [0.792]
 [0.759]]
printing an ep nov before normalisation:  43.20057375988726
using another actor
siam score:  -0.8685588
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
probs:  [0.13475372681205686, 0.22975609391770072, 0.22975609391770072, 0.22975609391770072, 0.13475372681205686, 0.04122426462278417]
maxi score, test score, baseline:  -0.9960977186311787 -1.0 -0.9960977186311787
probs:  [0.13475372611730196, 0.2297560952911802, 0.2297560952911802, 0.2297560952911802, 0.13475372611730196, 0.041224261891855365]
printing an ep nov before normalisation:  39.42897518198066
printing an ep nov before normalisation:  68.6498117955802
using explorer policy with actor:  1
using explorer policy with actor:  0
printing an ep nov before normalisation:  40.46727994735874
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9960977186311787 -1.0 -0.9960977186311787
probs:  [0.13475372611730196, 0.2297560952911802, 0.2297560952911802, 0.2297560952911802, 0.13475372611730196, 0.041224261891855365]
using explorer policy with actor:  0
printing an ep nov before normalisation:  43.22548580341575
maxi score, test score, baseline:  -0.9961121212121212 -1.0 -0.9961121212121212
printing an ep nov before normalisation:  43.492240920090666
actions average: 
K:  2  action  0 :  tensor([0.3340, 0.0080, 0.1047, 0.1234, 0.1402, 0.1419, 0.1478],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0134, 0.9368, 0.0112, 0.0108, 0.0097, 0.0085, 0.0096],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1572, 0.0029, 0.2884, 0.1314, 0.1337, 0.1500, 0.1363],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1607, 0.0485, 0.1230, 0.1838, 0.1606, 0.1493, 0.1742],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1975, 0.0115, 0.1339, 0.1268, 0.2459, 0.1430, 0.1414],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2269, 0.0096, 0.1167, 0.1480, 0.1571, 0.2108, 0.1309],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2174, 0.0138, 0.1311, 0.1513, 0.1640, 0.1547, 0.1677],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
probs:  [0.04552951596571562, 0.25342342869861106, 0.1486706819727356, 0.25342342869861106, 0.25342342869861106, 0.04552951596571562]
from probs:  [0.04552951596571562, 0.25342342869861106, 0.1486706819727356, 0.25342342869861106, 0.25342342869861106, 0.04552951596571562]
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
probs:  [0.03880136439095906, 0.23109428766267331, 0.16600288469472965, 0.23109428766267331, 0.23109428766267331, 0.10191288792629134]
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.07207246717196
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
printing an ep nov before normalisation:  45.78120189796151
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
probs:  [0.03880136439095906, 0.23109428766267331, 0.16600288469472965, 0.23109428766267331, 0.23109428766267331, 0.10191288792629134]
printing an ep nov before normalisation:  68.8867889745406
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
probs:  [0.03880136439095906, 0.23109428766267331, 0.16600288469472965, 0.23109428766267331, 0.23109428766267331, 0.10191288792629134]
siam score:  -0.86292875
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
probs:  [0.03880136439095906, 0.23109428766267331, 0.16600288469472965, 0.23109428766267331, 0.23109428766267331, 0.10191288792629134]
maxi score, test score, baseline:  -0.9961264150943396 -1.0 -0.9961264150943396
printing an ep nov before normalisation:  36.235477355464326
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
probs:  [0.04145157373553318, 0.2469237458665658, 0.10888859433238467, 0.2469237458665658, 0.2469237458665658, 0.10888859433238467]
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]] [[60.146]
 [60.146]
 [60.146]
 [60.146]
 [60.146]
 [60.146]
 [60.146]] [[1.832]
 [1.832]
 [1.832]
 [1.832]
 [1.832]
 [1.832]
 [1.832]]
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
probs:  [0.044542849303170415, 0.2653876380054833, 0.1170252414926487, 0.1906313917005656, 0.2653876380054833, 0.1170252414926487]
Printing some Q and Qe and total Qs values:  [[0.353]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]] [[56.844]
 [42.734]
 [42.734]
 [42.734]
 [42.734]
 [42.734]
 [42.734]] [[1.575]
 [1.218]
 [1.218]
 [1.218]
 [1.218]
 [1.218]
 [1.218]]
printing an ep nov before normalisation:  70.63400224514606
printing an ep nov before normalisation:  60.14272391135571
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
probs:  [0.044542849303170415, 0.2653876380054833, 0.1170252414926487, 0.1906313917005656, 0.2653876380054833, 0.1170252414926487]
printing an ep nov before normalisation:  44.380221527611894
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
probs:  [0.04813408389362119, 0.28683773738122875, 0.12647784708955528, 0.20603624227301978, 0.20603624227301978, 0.12647784708955528]
Printing some Q and Qe and total Qs values:  [[0.669]
 [0.698]
 [0.672]
 [0.674]
 [0.669]
 [0.614]
 [0.572]] [[ 6.958]
 [ 6.43 ]
 [ 2.901]
 [ 9.674]
 [ 8.112]
 [23.833]
 [15.88 ]] [[0.669]
 [0.698]
 [0.672]
 [0.674]
 [0.669]
 [0.614]
 [0.572]]
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]]
maxi score, test score, baseline:  -0.9961546816479401 -1.0 -0.9961546816479401
maxi score, test score, baseline:  -0.9961546816479401 -1.0 -0.9961546816479401
probs:  [0.0481340817310818, 0.28683773957366077, 0.12647784635633927, 0.20603624299128942, 0.20603624299128942, 0.12647784635633927]
maxi score, test score, baseline:  -0.9961546816479401 -1.0 -0.9961546816479401
probs:  [0.0481340817310818, 0.28683773957366077, 0.12647784635633927, 0.20603624299128942, 0.20603624299128942, 0.12647784635633927]
maxi score, test score, baseline:  -0.9961546816479401 -1.0 -0.9961546816479401
probs:  [0.0481340817310818, 0.28683773957366077, 0.12647784635633927, 0.20603624299128942, 0.20603624299128942, 0.12647784635633927]
maxi score, test score, baseline:  -0.9961546816479401 -1.0 -0.9961546816479401
probs:  [0.0413396804437676, 0.22969237650130417, 0.13479159502616, 0.22969237650130417, 0.22969237650130417, 0.13479159502616]
maxi score, test score, baseline:  -0.9961546816479401 -1.0 -0.9961546816479401
probs:  [0.0413396804437676, 0.22969237650130417, 0.13479159502616, 0.22969237650130417, 0.22969237650130417, 0.13479159502616]
printing an ep nov before normalisation:  47.570582520246454
maxi score, test score, baseline:  -0.9961546816479401 -1.0 -0.9961546816479401
probs:  [0.04559121498614552, 0.25337746869529065, 0.14868516394183706, 0.25337746869529065, 0.25337746869529065, 0.04559121498614552]
maxi score, test score, baseline:  -0.9961546816479401 -1.0 -0.9961546816479401
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
using explorer policy with actor:  1
siam score:  -0.86002016
maxi score, test score, baseline:  -0.9961546816479401 -1.0 -0.9961546816479401
probs:  [0.04559121498614552, 0.25337746869529065, 0.14868516394183706, 0.25337746869529065, 0.25337746869529065, 0.04559121498614552]
printing an ep nov before normalisation:  46.733804238169796
printing an ep nov before normalisation:  45.47592639923096
from probs:  [0.04559121498614552, 0.25337746869529065, 0.14868516394183706, 0.25337746869529065, 0.25337746869529065, 0.04559121498614552]
maxi score, test score, baseline:  -0.9961546816479401 -1.0 -0.9961546816479401
probs:  [0.05091156961325531, 0.28301690560724235, 0.1660715247795023, 0.28301690560724235, 0.1660715247795023, 0.05091156961325531]
maxi score, test score, baseline:  -0.9961546816479401 -1.0 -0.9961546816479401
probs:  [0.06627592431254194, 0.21628062427797157, 0.21628062427797157, 0.368610978506431, 0.06627592431254194, 0.06627592431254194]
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.228]
 [0.457]
 [0.458]
 [0.452]
 [0.449]
 [0.449]] [[13.612]
 [21.641]
 [11.72 ]
 [12.377]
 [14.289]
 [13.968]
 [11.801]] [[0.505]
 [0.228]
 [0.457]
 [0.458]
 [0.452]
 [0.449]
 [0.449]]
printing an ep nov before normalisation:  56.085980691850565
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9961546816479401 -1.0 -0.9961546816479401
probs:  [0.07795958298389256, 0.07795958298389256, 0.25446159524730555, 0.4337000728171242, 0.07795958298389256, 0.07795958298389256]
maxi score, test score, baseline:  -0.996168656716418 -1.0 -0.996168656716418
probs:  [0.07795957936514497, 0.07795957936514497, 0.25446159882883973, 0.4337000837105805, 0.07795957936514497, 0.07795957936514497]
printing an ep nov before normalisation:  30.532524420693452
printing an ep nov before normalisation:  50.58859002550374
maxi score, test score, baseline:  -0.996168656716418 -1.0 -0.996168656716418
Printing some Q and Qe and total Qs values:  [[0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.329]] [[39.109]
 [39.109]
 [39.109]
 [39.109]
 [39.109]
 [39.109]
 [59.305]] [[0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.712]
 [1.141]]
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
probs:  [0.10336840846243121, 0.10336840846243121, 0.48315795768784386, 0.10336840846243121, 0.10336840846243121, 0.10336840846243121]
printing an ep nov before normalisation:  74.03311182202869
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.493]
 [0.446]
 [0.442]
 [0.441]
 [0.437]
 [0.445]] [[21.198]
 [53.026]
 [22.316]
 [22.554]
 [22.202]
 [21.434]
 [20.094]] [[0.771]
 [1.754]
 [0.798]
 [0.801]
 [0.789]
 [0.762]
 [0.73 ]]
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.50110775108866
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.90146540297979
printing an ep nov before normalisation:  49.75175857543945
printing an ep nov before normalisation:  40.8350921350289
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
probs:  [0.19176266930669508, 0.04118665346652444, 0.19176266930669508, 0.19176266930669508, 0.19176266930669508, 0.19176266930669508]
printing an ep nov before normalisation:  25.807620712314048
printing an ep nov before normalisation:  52.92701853877926
printing an ep nov before normalisation:  32.30630638317603
siam score:  -0.85880876
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
probs:  [0.19176266930669508, 0.04118665346652444, 0.19176266930669508, 0.19176266930669508, 0.19176266930669508, 0.19176266930669508]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
probs:  [0.19176266930669508, 0.04118665346652444, 0.19176266930669508, 0.19176266930669508, 0.19176266930669508, 0.19176266930669508]
printing an ep nov before normalisation:  26.619426334688825
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
probs:  [0.27443541631832674, 0.05889791701500657, 0.05889791701500657, 0.27443541631832674, 0.27443541631832674, 0.05889791701500657]
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
probs:  [0.27443541631832674, 0.05889791701500657, 0.05889791701500657, 0.27443541631832674, 0.27443541631832674, 0.05889791701500657]
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
probs:  [0.27443541631832674, 0.05889791701500657, 0.05889791701500657, 0.27443541631832674, 0.27443541631832674, 0.05889791701500657]
Printing some Q and Qe and total Qs values:  [[-0.04 ]
 [-0.039]
 [-0.038]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.039]] [[54.613]
 [18.551]
 [26.473]
 [18.551]
 [18.551]
 [18.551]
 [18.551]] [[0.83 ]
 [0.185]
 [0.328]
 [0.185]
 [0.185]
 [0.185]
 [0.185]]
maxi score, test score, baseline:  -0.9961962962962964 -1.0 -0.9961962962962964
probs:  [0.349879078375621, 0.07506046081218949, 0.07506046081218949, 0.07506046081218949, 0.349879078375621, 0.07506046081218949]
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.20471166306145
maxi score, test score, baseline:  -0.9962099630996311 -1.0 -0.9962099630996311
probs:  [0.10348661479973702, 0.10348661479973702, 0.10348661479973702, 0.10348661479973702, 0.48256692600131496, 0.10348661479973702]
Printing some Q and Qe and total Qs values:  [[0.27 ]
 [0.258]
 [0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.277]] [[52.571]
 [40.991]
 [43.904]
 [43.904]
 [43.904]
 [43.904]
 [43.904]] [[0.27 ]
 [0.258]
 [0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.277]]
maxi score, test score, baseline:  -0.9962099630996311 -1.0 -0.9962099630996311
probs:  [0.05778073290774507, 0.1880460965828013, 0.1880460965828013, 0.1880460965828013, 0.3203002444361059, 0.05778073290774507]
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.488]
 [0.569]
 [0.491]
 [0.905]
 [0.469]
 [0.418]] [[47.555]
 [63.73 ]
 [69.676]
 [62.881]
 [40.864]
 [53.038]
 [55.688]] [[0.85 ]
 [1.015]
 [1.149]
 [1.01 ]
 [1.225]
 [0.899]
 [0.872]]
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
printing an ep nov before normalisation:  49.32500400959826
actions average: 
K:  0  action  0 :  tensor([0.2912, 0.0029, 0.1218, 0.1287, 0.1620, 0.1421, 0.1514],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0219, 0.9003, 0.0260, 0.0087, 0.0075, 0.0168, 0.0188],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1950, 0.0119, 0.1931, 0.1401, 0.1595, 0.1567, 0.1437],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1754, 0.0080, 0.1348, 0.1847, 0.1674, 0.1646, 0.1650],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1848, 0.0023, 0.1258, 0.1343, 0.2660, 0.1325, 0.1542],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.2298, 0.0021, 0.1107, 0.1269, 0.1384, 0.2675, 0.1246],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2050, 0.0031, 0.1265, 0.1435, 0.1439, 0.1336, 0.2444],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  48.608310409310924
printing an ep nov before normalisation:  44.99927361666176
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
probs:  [0.07811314904702064, 0.07811314904702064, 0.25432343980408634, 0.07811314904702064, 0.433223964007831, 0.07811314904702064]
printing an ep nov before normalisation:  27.54092346633037
printing an ep nov before normalisation:  0.4841894380683698
printing an ep nov before normalisation:  35.72404384613037
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
probs:  [0.1374587020376303, 0.1374587020376303, 0.22377947387074756, 0.05242629993336318, 0.31141812008299824, 0.1374587020376303]
printing an ep nov before normalisation:  0.0
from probs:  [0.1374587020376303, 0.1374587020376303, 0.22377947387074756, 0.05242629993336318, 0.31141812008299824, 0.1374587020376303]
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
probs:  [0.14896039416740336, 0.14896039416740336, 0.25363571100128735, 0.04584739549521521, 0.25363571100128735, 0.14896039416740336]
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
probs:  [0.14896039416740336, 0.14896039416740336, 0.25363571100128735, 0.04584739549521521, 0.25363571100128735, 0.14896039416740336]
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
probs:  [0.14896039416740336, 0.14896039416740336, 0.25363571100128735, 0.04584739549521521, 0.25363571100128735, 0.14896039416740336]
maxi score, test score, baseline:  -0.9962369963369964 -1.0 -0.9962369963369964
probs:  [0.14896039374194353, 0.14896039374194353, 0.2536357130910422, 0.045847392592084896, 0.2536357130910422, 0.14896039374194353]
Printing some Q and Qe and total Qs values:  [[-0.046]
 [-0.036]
 [-0.054]
 [-0.051]
 [-0.05 ]
 [-0.051]
 [-0.049]] [[22.449]
 [15.725]
 [14.2  ]
 [13.54 ]
 [14.312]
 [14.26 ]
 [14.203]] [[0.674]
 [0.371]
 [0.282]
 [0.254]
 [0.291]
 [0.287]
 [0.287]]
printing an ep nov before normalisation:  37.52671440471822
maxi score, test score, baseline:  -0.9962369963369964 -1.0 -0.9962369963369964
probs:  [0.14896039374194353, 0.14896039374194353, 0.2536357130910422, 0.045847392592084896, 0.2536357130910422, 0.14896039374194353]
maxi score, test score, baseline:  -0.9962369963369964 -1.0 -0.9962369963369964
probs:  [0.16637581047947603, 0.16637581047947603, 0.2832999977299673, 0.05119676035212844, 0.16637581047947603, 0.16637581047947603]
printing an ep nov before normalisation:  70.16293562610818
printing an ep nov before normalisation:  63.716597979907164
siam score:  -0.8612312
printing an ep nov before normalisation:  53.41577937255065
maxi score, test score, baseline:  -0.9962369963369964 -1.0 -0.9962369963369964
probs:  [0.18803553362978245, 0.05784982105448924, 0.3201937570016741, 0.05784982105448924, 0.18803553362978245, 0.18803553362978245]
siam score:  -0.8609944
printing an ep nov before normalisation:  63.28577168475698
using another actor
maxi score, test score, baseline:  -0.9962369963369964 -1.0 -0.9962369963369964
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
Printing some Q and Qe and total Qs values:  [[0.366]
 [0.417]
 [0.432]
 [0.435]
 [0.557]
 [0.432]
 [0.39 ]] [[55.215]
 [59.649]
 [57.886]
 [58.609]
 [51.199]
 [57.886]
 [58.782]] [[1.225]
 [1.417]
 [1.376]
 [1.402]
 [1.288]
 [1.376]
 [1.362]]
printing an ep nov before normalisation:  62.34774589538574
printing an ep nov before normalisation:  49.63548965282797
maxi score, test score, baseline:  -0.9962369963369964 -1.0 -0.9962369963369964
probs:  [0.05912635528707843, 0.05912635528707843, 0.27420697804625493, 0.05912635528707843, 0.27420697804625493, 0.27420697804625493]
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.675421090897252
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  60.490557687383
maxi score, test score, baseline:  -0.9962369963369964 -1.0 -0.9962369963369964
probs:  [0.10372178737246796, 0.10372178737246796, 0.10372178737246796, 0.10372178737246796, 0.10372178737246796, 0.4813910631376603]
maxi score, test score, baseline:  -0.9962369963369964 -1.0 -0.9962369963369964
probs:  [0.10372178737246796, 0.10372178737246796, 0.10372178737246796, 0.10372178737246796, 0.10372178737246796, 0.4813910631376603]
printing an ep nov before normalisation:  49.87588688859646
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
probs:  [0.16637815360023928, 0.16637815360023928, 0.16637815360023928, 0.16637815360023928, 0.05126144009551981, 0.2832259455035231]
line 256 mcts: sample exp_bonus 32.44655205217319
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.49436950683594
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
UNIT TEST: sample policy line 217 mcts : [0.143 0.388 0.102 0.143 0.102 0.061 0.061]
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.468]] [[37.356]
 [37.356]
 [37.356]
 [37.356]
 [37.356]
 [37.356]
 [23.81 ]] [[0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.551]]
printing an ep nov before normalisation:  56.82520877843021
printing an ep nov before normalisation:  60.135650634765625
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
probs:  [0.11727170443303742, 0.19081727256709274, 0.19081727256709274, 0.19081727256709274, 0.04480768877154259, 0.2654687890941417]
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
probs:  [0.11727170443303742, 0.19081727256709274, 0.19081727256709274, 0.19081727256709274, 0.04480768877154259, 0.2654687890941417]
actions average: 
K:  3  action  0 :  tensor([0.2646, 0.0469, 0.1088, 0.1082, 0.1860, 0.1199, 0.1656],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0192, 0.8494, 0.0376, 0.0236, 0.0165, 0.0232, 0.0305],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1156, 0.0063, 0.3131, 0.1152, 0.1210, 0.1661, 0.1626],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1962, 0.0014, 0.1264, 0.1462, 0.2132, 0.1427, 0.1738],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2181, 0.0275, 0.1346, 0.1206, 0.2097, 0.1291, 0.1604],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.2006, 0.0058, 0.1492, 0.1433, 0.1710, 0.1719, 0.1582],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1725, 0.0174, 0.1568, 0.1402, 0.1672, 0.1622, 0.1838],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
probs:  [0.13488493645423133, 0.22953417679491536, 0.22953417679491536, 0.22953417679491536, 0.04162759670679124, 0.13488493645423133]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
siam score:  -0.86231214
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
probs:  [0.06664274510127682, 0.36783001821639, 0.21612087323988977, 0.21612087323988977, 0.06664274510127682, 0.06664274510127682]
siam score:  -0.86209184
using explorer policy with actor:  1
actions average: 
K:  4  action  0 :  tensor([0.2911, 0.0349, 0.1134, 0.1301, 0.1377, 0.1606, 0.1322],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0328, 0.7178, 0.0300, 0.0774, 0.0499, 0.0611, 0.0309],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1754, 0.0967, 0.1762, 0.1216, 0.1345, 0.1470, 0.1486],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1590, 0.0711, 0.1126, 0.2206, 0.1495, 0.1214, 0.1658],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1704, 0.0124, 0.1121, 0.1344, 0.2978, 0.1288, 0.1441],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2254, 0.0205, 0.1164, 0.1334, 0.1611, 0.1690, 0.1743],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1701, 0.0030, 0.1335, 0.1534, 0.1641, 0.1587, 0.2172],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  62.9754523046553
siam score:  -0.8575722
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
probs:  [0.1489970747112749, 0.1489970747112749, 0.2534891940445984, 0.2534891940445984, 0.1489970747112749, 0.04603038777697863]
printing an ep nov before normalisation:  60.65570135904602
printing an ep nov before normalisation:  55.00025965171851
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
probs:  [0.1489970747112749, 0.1489970747112749, 0.2534891940445984, 0.2534891940445984, 0.1489970747112749, 0.04603038777697863]
printing an ep nov before normalisation:  57.84337933981176
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.498]
 [0.404]
 [0.404]
 [0.442]
 [0.404]
 [0.404]] [[56.966]
 [57.257]
 [56.966]
 [56.966]
 [62.482]
 [56.966]
 [56.966]] [[1.761]
 [1.867]
 [1.761]
 [1.761]
 [2.008]
 [1.761]
 [1.761]]
printing an ep nov before normalisation:  38.59966415515875
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
probs:  [0.1489970747112749, 0.1489970747112749, 0.2534891940445984, 0.2534891940445984, 0.1489970747112749, 0.04603038777697863]
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
probs:  [0.1489970747112749, 0.1489970747112749, 0.2534891940445984, 0.2534891940445984, 0.1489970747112749, 0.04603038777697863]
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
probs:  [0.1489970747112749, 0.1489970747112749, 0.2534891940445984, 0.2534891940445984, 0.1489970747112749, 0.04603038777697863]
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.318]
 [0.307]
 [0.309]
 [0.308]
 [0.309]
 [0.311]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.306]
 [0.318]
 [0.307]
 [0.309]
 [0.308]
 [0.309]
 [0.311]]
printing an ep nov before normalisation:  29.380563978604297
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
printing an ep nov before normalisation:  47.05385297817777
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.19080363078978735, 0.19080363078978735, 0.2653801046763267, 0.19080363078978735, 0.1173158645512285, 0.044893138403082775]
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.25305746229327597, 0.25305746229327597, 0.25305746229327597, 0.14878402384016817, 0.046021794640002014, 0.046021794640002014]
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.25305746229327597, 0.25305746229327597, 0.25305746229327597, 0.14878402384016817, 0.046021794640002014, 0.046021794640002014]
printing an ep nov before normalisation:  34.16535521889466
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.25305746229327597, 0.25305746229327597, 0.25305746229327597, 0.14878402384016817, 0.046021794640002014, 0.046021794640002014]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  43.18804968115724
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.25305746229327597, 0.25305746229327597, 0.25305746229327597, 0.14878402384016817, 0.046021794640002014, 0.046021794640002014]
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
printing an ep nov before normalisation:  49.57915574839564
Printing some Q and Qe and total Qs values:  [[-0.015]
 [-0.027]
 [-0.022]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]] [[63.969]
 [57.271]
 [67.226]
 [57.271]
 [57.271]
 [57.271]
 [57.271]] [[1.093]
 [0.954]
 [1.148]
 [0.954]
 [0.954]
 [0.954]
 [0.954]]
printing an ep nov before normalisation:  47.90322177250705
maxi score, test score, baseline:  -0.9962898916967509 -1.0 -0.9962898916967509
probs:  [0.18763454002565458, 0.31916029109566435, 0.31916029109566435, 0.058014959261005604, 0.058014959261005604, 0.058014959261005604]
printing an ep nov before normalisation:  38.198113441467285
maxi score, test score, baseline:  -0.9963028776978418 -1.0 -0.9963028776978418
probs:  [0.21605676695497023, 0.3675197411724376, 0.21605676695497023, 0.06678890830587403, 0.06678890830587403, 0.06678890830587403]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9963028776978418 -1.0 -0.9963028776978418
maxi score, test score, baseline:  -0.9963028776978418 -1.0 -0.9963028776978418
probs:  [0.2059262021992102, 0.28638021850047257, 0.2059262021992102, 0.12663818613419803, 0.12663818613419803, 0.048491004832710745]
printing an ep nov before normalisation:  30.33181090557946
printing an ep nov before normalisation:  43.60079639190644
printing an ep nov before normalisation:  49.602086293355185
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.6178143493375
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9963028776978418 -1.0 -0.9963028776978418
probs:  [0.24713978662016275, 0.17773791725575103, 0.17773791725575103, 0.17773791725575103, 0.17773791725575103, 0.041908544356833326]
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.477]
 [0.49 ]
 [0.493]
 [0.49 ]
 [0.485]
 [0.472]] [[38.287]
 [45.816]
 [35.869]
 [35.832]
 [35.976]
 [36.268]
 [36.55 ]] [[0.473]
 [0.477]
 [0.49 ]
 [0.493]
 [0.49 ]
 [0.485]
 [0.472]]
actions average: 
K:  3  action  0 :  tensor([0.2341, 0.0031, 0.1241, 0.1386, 0.1750, 0.1456, 0.1795],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0354, 0.7911, 0.0147, 0.0367, 0.0197, 0.0142, 0.0882],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1621, 0.0007, 0.1500, 0.1388, 0.1778, 0.1817, 0.1889],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1641, 0.0321, 0.1256, 0.1592, 0.1693, 0.1798, 0.1699],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1827, 0.0211, 0.1239, 0.1484, 0.2005, 0.1458, 0.1777],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1492, 0.0016, 0.0972, 0.1361, 0.1286, 0.3557, 0.1317],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2058, 0.0188, 0.1260, 0.1424, 0.1666, 0.1356, 0.2050],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9963028776978418 -1.0 -0.9963028776978418
probs:  [0.24713978662016275, 0.17773791725575103, 0.17773791725575103, 0.17773791725575103, 0.17773791725575103, 0.041908544356833326]
printing an ep nov before normalisation:  37.74119853973389
printing an ep nov before normalisation:  66.78514945501183
printing an ep nov before normalisation:  30.64178379432491
printing an ep nov before normalisation:  32.32978820800781
maxi score, test score, baseline:  -0.9963412811387901 -1.0 -0.9963412811387901
probs:  [0.28632387328999376, 0.12665788510291548, 0.205912379239254, 0.12665788510291548, 0.205912379239254, 0.04853559802566718]
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.88201126969307
line 256 mcts: sample exp_bonus 69.6305164251971
maxi score, test score, baseline:  -0.9963412811387901 -1.0 -0.9963412811387901
probs:  [0.28632387328999376, 0.12665788510291548, 0.205912379239254, 0.12665788510291548, 0.205912379239254, 0.04853559802566718]
maxi score, test score, baseline:  -0.9963412811387901 -1.0 -0.9963412811387901
probs:  [0.28632387328999376, 0.12665788510291548, 0.205912379239254, 0.12665788510291548, 0.205912379239254, 0.04853559802566718]
Printing some Q and Qe and total Qs values:  [[-0.037]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]] [[43.004]
 [30.832]
 [30.832]
 [30.832]
 [30.832]
 [30.832]
 [30.832]] [[1.02 ]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]]
actions average: 
K:  4  action  0 :  tensor([0.2838, 0.0353, 0.1290, 0.1215, 0.1577, 0.1338, 0.1390],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0241, 0.8533, 0.0180, 0.0266, 0.0124, 0.0109, 0.0547],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1403, 0.0815, 0.1616, 0.1407, 0.1670, 0.1522, 0.1567],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1954, 0.0024, 0.1519, 0.1422, 0.1848, 0.1536, 0.1697],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2146, 0.0423, 0.1255, 0.1367, 0.2062, 0.1318, 0.1430],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1932, 0.0161, 0.1549, 0.1363, 0.1418, 0.2011, 0.1566],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2107, 0.0488, 0.1170, 0.1330, 0.1830, 0.1180, 0.1895],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9963412811387901 -1.0 -0.9963412811387901
probs:  [0.22940869087025872, 0.13495833185409922, 0.22940869087025872, 0.13495833185409922, 0.22940869087025872, 0.04185726368102527]
maxi score, test score, baseline:  -0.9963412811387901 -1.0 -0.9963412811387901
probs:  [0.2529665771015781, 0.1488115127835812, 0.2529665771015781, 0.04614437795584227, 0.2529665771015781, 0.04614437795584227]
maxi score, test score, baseline:  -0.9963412811387901 -1.0 -0.9963412811387901
probs:  [0.2529665771015781, 0.1488115127835812, 0.2529665771015781, 0.04614437795584227, 0.2529665771015781, 0.04614437795584227]
maxi score, test score, baseline:  -0.9963412811387901 -1.0 -0.9963412811387901
maxi score, test score, baseline:  -0.9963412811387901 -1.0 -0.9963412811387901
probs:  [0.2529665771015781, 0.1488115127835812, 0.2529665771015781, 0.04614437795584227, 0.2529665771015781, 0.04614437795584227]
maxi score, test score, baseline:  -0.9963412811387901 -1.0 -0.9963412811387901
probs:  [0.16611297415482756, 0.16611297415482756, 0.28238840164094686, 0.05149862420422563, 0.28238840164094686, 0.05149862420422563]
maxi score, test score, baseline:  -0.9963412811387901 -1.0 -0.9963412811387901
probs:  [0.16611297415482756, 0.16611297415482756, 0.28238840164094686, 0.05149862420422563, 0.28238840164094686, 0.05149862420422563]
printing an ep nov before normalisation:  59.838109804003146
Starting evaluation
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.425]
 [0.401]
 [0.397]
 [0.368]
 [0.378]
 [0.446]] [[46.646]
 [50.456]
 [49.661]
 [49.559]
 [45.789]
 [46.106]
 [48.462]] [[1.191]
 [1.269]
 [1.221]
 [1.215]
 [1.072]
 [1.093]
 [1.231]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.616]
 [0.346]
 [0.343]
 [0.345]
 [0.345]
 [0.349]
 [0.349]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.616]
 [0.346]
 [0.343]
 [0.345]
 [0.345]
 [0.349]
 [0.349]]
maxi score, test score, baseline:  -0.9963412811387901 -1.0 -0.9963412811387901
probs:  [0.05815395190778396, 0.18761868940109205, 0.318959727437778, 0.05815395190778396, 0.318959727437778, 0.05815395190778396]
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]] [[34.46 ]
 [14.245]
 [14.245]
 [14.245]
 [14.245]
 [14.245]
 [14.245]] [[0.639]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]]
maxi score, test score, baseline:  -0.9963412811387901 -1.0 -0.9963412811387901
probs:  [0.05815395190778396, 0.18761868940109205, 0.318959727437778, 0.05815395190778396, 0.318959727437778, 0.05815395190778396]
printing an ep nov before normalisation:  53.54276778319669
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]] [[60.396]
 [66.553]
 [66.553]
 [66.553]
 [66.553]
 [66.553]
 [66.553]] [[0.482]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]]
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]] [[48.329]
 [47.146]
 [47.146]
 [47.146]
 [47.146]
 [47.146]
 [47.146]] [[0.487]
 [0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]]
Printing some Q and Qe and total Qs values:  [[0.407]
 [0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]] [[51.735]
 [51.571]
 [51.571]
 [51.571]
 [51.571]
 [51.571]
 [51.571]] [[0.407]
 [0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]]
printing an ep nov before normalisation:  66.05316378025287
printing an ep nov before normalisation:  45.81448780974256
printing an ep nov before normalisation:  57.32120789229899
printing an ep nov before normalisation:  43.49521758841522
using explorer policy with actor:  0
printing an ep nov before normalisation:  13.925075841703162
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]] [[38.42 ]
 [41.256]
 [41.256]
 [41.256]
 [41.256]
 [41.256]
 [41.256]] [[0.542]
 [0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]]
Printing some Q and Qe and total Qs values:  [[0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]] [[45.719]
 [45.719]
 [45.719]
 [45.719]
 [45.719]
 [45.719]
 [45.719]] [[0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9963412811387901 -1.0 -0.9963412811387901
printing an ep nov before normalisation:  22.032454201183658
printing an ep nov before normalisation:  47.379219146815785
siam score:  -0.86260885
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]] [[33.13 ]
 [13.803]
 [13.803]
 [13.803]
 [13.803]
 [13.803]
 [13.803]] [[0.519]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]]
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]] [[40.595]
 [42.564]
 [42.564]
 [42.564]
 [42.564]
 [42.564]
 [42.564]] [[0.512]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]]
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.376]] [[34.707]
 [21.988]
 [21.988]
 [21.988]
 [21.988]
 [21.988]
 [21.988]] [[0.598]
 [0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.376]]
printing an ep nov before normalisation:  55.049230094579826
maxi score, test score, baseline:  -0.9963412811387901 -1.0 -0.9963412811387901
probs:  [0.06693475269268506, 0.21599255913755233, 0.21599255913755233, 0.06693475269268506, 0.3672106236468402, 0.06693475269268506]
line 256 mcts: sample exp_bonus 52.457347679137015
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.55581188201904
actions average: 
K:  3  action  0 :  tensor([0.2815, 0.0148, 0.1185, 0.1251, 0.1663, 0.1345, 0.1593],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0058, 0.9523, 0.0067, 0.0125, 0.0034, 0.0071, 0.0122],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1225, 0.0589, 0.3283, 0.0877, 0.0996, 0.1454, 0.1577],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1794, 0.0961, 0.1157, 0.1372, 0.1616, 0.1242, 0.1858],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2112, 0.0082, 0.1229, 0.1476, 0.2064, 0.1394, 0.1643],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1697, 0.0421, 0.1349, 0.1349, 0.1388, 0.2376, 0.1419],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2217, 0.0512, 0.1235, 0.1312, 0.1540, 0.1476, 0.1707],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.73 ]
 [0.684]
 [0.663]
 [0.667]
 [0.665]
 [0.655]
 [0.667]] [[38.907]
 [25.558]
 [14.199]
 [13.906]
 [14.562]
 [14.373]
 [14.984]] [[0.73 ]
 [0.684]
 [0.663]
 [0.667]
 [0.665]
 [0.655]
 [0.667]]
printing an ep nov before normalisation:  25.390186309814453
Printing some Q and Qe and total Qs values:  [[0.64 ]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]] [[52.772]
 [52.153]
 [52.153]
 [52.153]
 [52.153]
 [52.153]
 [52.153]] [[0.64 ]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]]
maxi score, test score, baseline:  -0.9963539007092199 -1.0 -0.9963539007092199
probs:  [0.0669347491493277, 0.21599256089004454, 0.21599256089004454, 0.0669347491493277, 0.3672106307719278, 0.0669347491493277]
Printing some Q and Qe and total Qs values:  [[0.7  ]
 [0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]] [[45.056]
 [37.429]
 [37.429]
 [37.429]
 [37.429]
 [37.429]
 [37.429]] [[0.7  ]
 [0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  24.49214220046997
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
Printing some Q and Qe and total Qs values:  [[0.672]
 [0.606]
 [0.584]
 [0.594]
 [0.587]
 [0.575]
 [0.576]] [[37.087]
 [24.293]
 [13.221]
 [18.282]
 [16.474]
 [12.561]
 [15.466]] [[0.672]
 [0.606]
 [0.584]
 [0.594]
 [0.587]
 [0.575]
 [0.576]]
printing an ep nov before normalisation:  51.45232853935106
printing an ep nov before normalisation:  44.413768164274074
Printing some Q and Qe and total Qs values:  [[0.73 ]
 [0.685]
 [0.663]
 [0.667]
 [0.665]
 [0.654]
 [0.656]] [[35.787]
 [28.74 ]
 [13.071]
 [12.459]
 [12.34 ]
 [12.348]
 [15.357]] [[0.73 ]
 [0.685]
 [0.663]
 [0.667]
 [0.665]
 [0.654]
 [0.656]]
printing an ep nov before normalisation:  17.82805734955517
printing an ep nov before normalisation:  51.49455942635438
Printing some Q and Qe and total Qs values:  [[0.66 ]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]] [[39.302]
 [15.521]
 [15.521]
 [15.521]
 [15.521]
 [15.521]
 [15.521]] [[0.66 ]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  11.460988391196025
Printing some Q and Qe and total Qs values:  [[0.694]
 [0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.628]] [[46.619]
 [47.4  ]
 [47.4  ]
 [47.4  ]
 [47.4  ]
 [47.4  ]
 [47.4  ]] [[0.694]
 [0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.628]]
printing an ep nov before normalisation:  50.24439299294663
printing an ep nov before normalisation:  41.23369195657671
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]] [[15.521]
 [25.653]
 [25.653]
 [25.653]
 [25.653]
 [25.653]
 [25.653]] [[0.598]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]]
printing an ep nov before normalisation:  23.867350880370452
printing an ep nov before normalisation:  30.468862742175602
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.615]
 [0.588]
 [0.588]
 [0.586]
 [0.576]
 [0.564]] [[35.833]
 [32.561]
 [16.976]
 [17.947]
 [12.34 ]
 [13.366]
 [15.605]] [[0.653]
 [0.615]
 [0.588]
 [0.588]
 [0.586]
 [0.576]
 [0.564]]
printing an ep nov before normalisation:  48.927419297055366
printing an ep nov before normalisation:  47.4001504029599
Printing some Q and Qe and total Qs values:  [[0.764]
 [0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]] [[45.588]
 [40.225]
 [40.225]
 [40.225]
 [40.225]
 [40.225]
 [40.225]] [[0.764]
 [0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]]
maxi score, test score, baseline:  -0.9963664310954063 -1.0 -0.9963664310954063
printing an ep nov before normalisation:  16.785914931723827
printing an ep nov before normalisation:  48.29024246596088
Printing some Q and Qe and total Qs values:  [[0.637]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]] [[33.522]
 [21.471]
 [21.471]
 [21.471]
 [21.471]
 [21.471]
 [21.471]] [[0.637]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]]
printing an ep nov before normalisation:  21.373193212789573
printing an ep nov before normalisation:  19.23425040549425
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.148]
 [0.26 ]
 [0.215]
 [0.171]
 [0.194]
 [0.288]] [[24.925]
 [23.843]
 [18.528]
 [19.059]
 [18.643]
 [17.648]
 [15.943]] [[0.893]
 [0.646]
 [0.581]
 [0.554]
 [0.496]
 [0.486]
 [0.523]]
printing an ep nov before normalisation:  41.80867481726261
printing an ep nov before normalisation:  13.397251223501511
line 256 mcts: sample exp_bonus 14.78382444184493
printing an ep nov before normalisation:  30.690894004328317
printing an ep nov before normalisation:  43.513969547022135
line 256 mcts: sample exp_bonus 38.65238405143665
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  15.655963973735222
maxi score, test score, baseline:  -0.9964034965034965 -1.0 -0.9964034965034965
probs:  [0.04191457889096067, 0.22937746570955314, 0.22937746570955314, 0.1349765119901899, 0.22937746570955314, 0.1349765119901899]
printing an ep nov before normalisation:  11.510317325592041
printing an ep nov before normalisation:  12.210047245025635
printing an ep nov before normalisation:  30.690894004328317
printing an ep nov before normalisation:  36.756104117990844
maxi score, test score, baseline:  -0.9964034965034965 -1.0 -0.9964034965034965
probs:  [0.04191457889096067, 0.22937746570955314, 0.22937746570955314, 0.1349765119901899, 0.22937746570955314, 0.1349765119901899]
actor:  1 policy actor:  1  step number:  68 total reward:  0.01999999999999924  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.885]
 [0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]] [[48.149]
 [48.893]
 [48.893]
 [48.893]
 [48.893]
 [48.893]
 [48.893]] [[0.885]
 [0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]]
printing an ep nov before normalisation:  13.500807285308838
maxi score, test score, baseline:  -0.9964156794425088 -1.0 -0.9964156794425088
printing an ep nov before normalisation:  18.81955288217133
maxi score, test score, baseline:  -0.9964156794425088 -1.0 -0.9964156794425088
printing an ep nov before normalisation:  26.017844071989934
printing an ep nov before normalisation:  56.143768004300256
Printing some Q and Qe and total Qs values:  [[0.863]
 [0.7  ]
 [0.7  ]
 [0.7  ]
 [0.7  ]
 [0.7  ]
 [0.7  ]] [[49.403]
 [46.009]
 [46.009]
 [46.009]
 [46.009]
 [46.009]
 [46.009]] [[0.863]
 [0.7  ]
 [0.7  ]
 [0.7  ]
 [0.7  ]
 [0.7  ]
 [0.7  ]]
printing an ep nov before normalisation:  73.44970524901589
maxi score, test score, baseline:  -0.9964156794425088 -1.0 -0.9964156794425088
using explorer policy with actor:  0
using explorer policy with actor:  1
printing an ep nov before normalisation:  22.63584176492335
printing an ep nov before normalisation:  61.3099139638435
printing an ep nov before normalisation:  16.846829591099986
printing an ep nov before normalisation:  12.585599422454834
printing an ep nov before normalisation:  21.16549847717646
using explorer policy with actor:  0
printing an ep nov before normalisation:  20.78333473113642
Printing some Q and Qe and total Qs values:  [[0.847]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]] [[45.846]
 [32.031]
 [32.031]
 [32.031]
 [32.031]
 [32.031]
 [32.031]] [[0.847]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]]
Printing some Q and Qe and total Qs values:  [[0.707]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]] [[46.505]
 [38.909]
 [38.909]
 [38.909]
 [38.909]
 [38.909]
 [38.909]] [[0.707]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]]
maxi score, test score, baseline:  -0.9964156794425088 -1.0 -0.9964156794425088
probs:  [0.08244595611234815, 0.08577665700698289, 0.5744474158597204, 0.08577665700698289, 0.08577665700698289, 0.08577665700698289]
Printing some Q and Qe and total Qs values:  [[0.947]
 [0.947]
 [0.947]
 [0.947]
 [0.947]
 [0.947]
 [0.947]] [[58.809]
 [58.809]
 [58.809]
 [58.809]
 [58.809]
 [58.809]
 [58.809]] [[0.947]
 [0.947]
 [0.947]
 [0.947]
 [0.947]
 [0.947]
 [0.947]]
maxi score, test score, baseline:  -0.9964156794425088 -1.0 -0.9964156794425088
printing an ep nov before normalisation:  69.67632836232842
printing an ep nov before normalisation:  32.03140516498504
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.959]
 [0.954]
 [0.954]
 [0.954]
 [0.954]
 [0.954]
 [0.954]] [[58.537]
 [57.643]
 [57.643]
 [57.643]
 [57.643]
 [57.643]
 [57.643]] [[0.959]
 [0.954]
 [0.954]
 [0.954]
 [0.954]
 [0.954]
 [0.954]]
printing an ep nov before normalisation:  63.455626805692084
printing an ep nov before normalisation:  40.45452900125728
printing an ep nov before normalisation:  75.15761852264404
Printing some Q and Qe and total Qs values:  [[0.824]
 [0.944]
 [0.926]
 [0.944]
 [0.944]
 [0.944]
 [0.944]] [[48.216]
 [49.806]
 [50.462]
 [49.806]
 [49.806]
 [49.806]
 [49.806]] [[0.824]
 [0.944]
 [0.926]
 [0.944]
 [0.944]
 [0.944]
 [0.944]]
from probs:  [0.08272125218334683, 0.08606309265356225, 0.5763682176726197, 0.08606309265356225, 0.08272125218334683, 0.08606309265356225]
maxi score, test score, baseline:  -0.9964156794425088 -1.0 -0.9964156794425088
probs:  [0.08488684077010296, 0.08816390676066813, 0.5689655083093328, 0.08816390676066813, 0.0816559306385599, 0.08816390676066813]
maxi score, test score, baseline:  -0.9964156794425088 -1.0 -0.9964156794425088
probs:  [0.08488684077010296, 0.08816390676066813, 0.5689655083093328, 0.08816390676066813, 0.0816559306385599, 0.08816390676066813]
maxi score, test score, baseline:  -0.9964156794425088 -1.0 -0.9964156794425088
probs:  [0.08488684077010296, 0.08816390676066813, 0.5689655083093328, 0.08816390676066813, 0.0816559306385599, 0.08816390676066813]
Printing some Q and Qe and total Qs values:  [[0.731]
 [0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.639]] [[35.5  ]
 [23.387]
 [23.387]
 [23.387]
 [23.387]
 [23.387]
 [23.387]] [[0.731]
 [0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.639]]
printing an ep nov before normalisation:  41.83244701872475
Printing some Q and Qe and total Qs values:  [[0.908]
 [0.984]
 [0.984]
 [0.984]
 [0.984]
 [0.984]
 [0.984]] [[48.458]
 [49.007]
 [49.007]
 [49.007]
 [49.007]
 [49.007]
 [49.007]] [[0.908]
 [0.984]
 [0.984]
 [0.984]
 [0.984]
 [0.984]
 [0.984]]
Printing some Q and Qe and total Qs values:  [[0.928]
 [0.903]
 [0.883]
 [0.903]
 [0.903]
 [0.903]
 [0.903]] [[43.894]
 [45.027]
 [45.044]
 [45.027]
 [45.027]
 [45.027]
 [45.027]] [[0.928]
 [0.903]
 [0.883]
 [0.903]
 [0.903]
 [0.903]
 [0.903]]
maxi score, test score, baseline:  -0.9964156794425088 -1.0 -0.9964156794425088
probs:  [0.08488684077010296, 0.08816390676066813, 0.5689655083093328, 0.08816390676066813, 0.0816559306385599, 0.08816390676066813]
Printing some Q and Qe and total Qs values:  [[0.618]
 [0.753]
 [0.753]
 [0.586]
 [0.753]
 [0.753]
 [0.753]] [[13.131]
 [26.379]
 [26.379]
 [11.785]
 [26.379]
 [26.379]
 [26.379]] [[0.618]
 [0.753]
 [0.753]
 [0.586]
 [0.753]
 [0.753]
 [0.753]]
Printing some Q and Qe and total Qs values:  [[0.918]
 [0.895]
 [0.895]
 [0.895]
 [0.895]
 [0.895]
 [0.895]] [[43.813]
 [45.027]
 [45.027]
 [45.027]
 [45.027]
 [45.027]
 [45.027]] [[0.918]
 [0.895]
 [0.895]
 [0.895]
 [0.895]
 [0.895]
 [0.895]]
line 256 mcts: sample exp_bonus 41.80288299852451
Printing some Q and Qe and total Qs values:  [[0.839]
 [0.561]
 [0.569]
 [0.566]
 [0.562]
 [0.565]
 [0.585]] [[36.09 ]
 [26.076]
 [18.869]
 [17.43 ]
 [16.794]
 [16.197]
 [16.157]] [[0.839]
 [0.561]
 [0.569]
 [0.566]
 [0.562]
 [0.565]
 [0.585]]
Printing some Q and Qe and total Qs values:  [[0.828]
 [0.609]
 [0.623]
 [0.623]
 [0.621]
 [0.619]
 [0.623]] [[31.977]
 [25.709]
 [12.482]
 [11.785]
 [11.557]
 [11.87 ]
 [13.184]] [[0.828]
 [0.609]
 [0.623]
 [0.623]
 [0.621]
 [0.619]
 [0.623]]
printing an ep nov before normalisation:  16.793632437123783
printing an ep nov before normalisation:  30.906225102923425
maxi score, test score, baseline:  -0.9964156794425088 -1.0 -0.9964156794425088
probs:  [0.08488684077010296, 0.08816390676066813, 0.5689655083093328, 0.08816390676066813, 0.0816559306385599, 0.08816390676066813]
siam score:  -0.8556662
Printing some Q and Qe and total Qs values:  [[0.782]
 [0.559]
 [0.574]
 [0.574]
 [0.572]
 [0.571]
 [0.574]] [[31.876]
 [25.709]
 [12.482]
 [11.785]
 [11.557]
 [13.502]
 [13.184]] [[0.782]
 [0.559]
 [0.574]
 [0.574]
 [0.572]
 [0.571]
 [0.574]]
Printing some Q and Qe and total Qs values:  [[0.784]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]] [[35.096]
 [13.131]
 [13.131]
 [13.131]
 [13.131]
 [13.131]
 [13.131]] [[0.784]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]]
siam score:  -0.8559222
maxi score, test score, baseline:  -0.9964156794425088 -1.0 -0.9964156794425088
maxi score, test score, baseline:  -0.9964156794425088 -1.0 -0.9964156794425088
probs:  [0.08488684077010296, 0.08816390676066813, 0.5689655083093328, 0.08816390676066813, 0.0816559306385599, 0.08816390676066813]
using explorer policy with actor:  0
using explorer policy with actor:  0
printing an ep nov before normalisation:  39.00984847243363
maxi score, test score, baseline:  -0.9964277777777778 -1.0 -0.9964277777777778
maxi score, test score, baseline:  -0.9964277777777778 -1.0 -0.9964277777777778
probs:  [0.08488684140676689, 0.08816390890616192, 0.5689655020871023, 0.08816390890616192, 0.081655929787645, 0.08816390890616192]
printing an ep nov before normalisation:  40.442385061614374
maxi score, test score, baseline:  -0.9964870307167235 -1.0 -0.9964870307167235
probs:  [0.08488684452469263, 0.08816391941323497, 0.5689654716151066, 0.08816391941323497, 0.08165592562049598, 0.08816391941323497]
printing an ep nov before normalisation:  45.26121245806883
maxi score, test score, baseline:  -0.9964986394557823 -1.0 -0.9964986394557823
probs:  [0.08488684513551141, 0.08816392147162326, 0.5689654656454853, 0.08816392147162326, 0.08165592480413354, 0.08816392147162326]
maxi score, test score, baseline:  -0.9965887417218543 -1.0 -0.9965887417218543
probs:  [0.0851657317684338, 0.08845360259921584, 0.5708371622871788, 0.0851657317684338, 0.08192416897752182, 0.08845360259921584]
maxi score, test score, baseline:  -0.9965887417218543 -1.0 -0.9965887417218543
probs:  [0.08722177607180301, 0.09044778875298297, 0.5637557624655508, 0.08722177607180301, 0.08090510788487715, 0.09044778875298297]
Printing some Q and Qe and total Qs values:  [[0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]] [[37.581]
 [37.581]
 [37.581]
 [37.581]
 [37.581]
 [37.581]
 [37.581]] [[1.21]
 [1.21]
 [1.21]
 [1.21]
 [1.21]
 [1.21]
 [1.21]]
maxi score, test score, baseline:  -0.9965887417218543 -1.0 -0.9965887417218543
probs:  [0.08722177607180301, 0.09044778875298297, 0.5637557624655508, 0.08722177607180301, 0.08090510788487715, 0.09044778875298297]
Printing some Q and Qe and total Qs values:  [[0.442]
 [0.518]
 [0.453]
 [0.46 ]
 [0.469]
 [0.468]
 [0.468]] [[33.792]
 [50.936]
 [29.418]
 [37.833]
 [32.709]
 [32.582]
 [30.742]] [[0.442]
 [0.518]
 [0.453]
 [0.46 ]
 [0.469]
 [0.468]
 [0.468]]
printing an ep nov before normalisation:  55.52970428353554
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]] [[57.09 ]
 [50.927]
 [50.927]
 [50.927]
 [50.927]
 [50.927]
 [50.927]] [[1.319]
 [1.114]
 [1.114]
 [1.114]
 [1.114]
 [1.114]
 [1.114]]
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]] [[60.494]
 [60.494]
 [60.494]
 [60.494]
 [60.494]
 [60.494]
 [60.494]] [[2.459]
 [2.459]
 [2.459]
 [2.459]
 [2.459]
 [2.459]
 [2.459]]
maxi score, test score, baseline:  -0.9965996699669967 -1.0 -0.9965996699669967
probs:  [0.08732532154576292, 0.0906379823627998, 0.5632343995096313, 0.08732532154576292, 0.08083899267324318, 0.0906379823627998]
siam score:  -0.86159563
printing an ep nov before normalisation:  24.972158234144587
maxi score, test score, baseline:  -0.9965996699669967 -1.0 -0.9965996699669967
probs:  [0.08732532154576292, 0.0906379823627998, 0.5632343995096313, 0.08732532154576292, 0.08083899267324318, 0.0906379823627998]
printing an ep nov before normalisation:  16.645044214565132
printing an ep nov before normalisation:  50.978753959723576
printing an ep nov before normalisation:  28.693593651310792
siam score:  -0.8595888
maxi score, test score, baseline:  -0.9966105263157895 -1.0 -0.9966105263157895
probs:  [0.0873253231178657, 0.0906379852984505, 0.5632343915919058, 0.0873253231178657, 0.08083899157546191, 0.0906379852984505]
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]] [[46.395]
 [46.711]
 [46.711]
 [46.711]
 [46.711]
 [46.711]
 [46.711]] [[2.468]
 [2.507]
 [2.507]
 [2.507]
 [2.507]
 [2.507]
 [2.507]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  54.91747887005906
printing an ep nov before normalisation:  46.96860936328525
printing an ep nov before normalisation:  14.209980964660645
Printing some Q and Qe and total Qs values:  [[0.472]
 [0.472]
 [0.472]
 [0.497]
 [0.518]
 [0.472]
 [0.472]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.472]
 [0.472]
 [0.472]
 [0.497]
 [0.518]
 [0.472]
 [0.472]]
maxi score, test score, baseline:  -0.9966105263157895 -1.0 -0.9966105263157895
probs:  [0.09008116973386072, 0.09343966048225716, 0.5593424466602557, 0.08676998167206143, 0.08028557171770442, 0.09008116973386072]
maxi score, test score, baseline:  -0.9966105263157895 -1.0 -0.9966105263157895
probs:  [0.09016935391665086, 0.09357195294397981, 0.5590295536498258, 0.0868146788192844, 0.08024510675360833, 0.09016935391665086]
printing an ep nov before normalisation:  45.49577236175537
printing an ep nov before normalisation:  43.89627105819742
printing an ep nov before normalisation:  50.999335855042894
maxi score, test score, baseline:  -0.9966105263157895 -1.0 -0.9966105263157895
probs:  [0.09047700348641032, 0.09047700348641032, 0.5609392783339594, 0.0871108656086812, 0.08051884559812843, 0.09047700348641032]
maxi score, test score, baseline:  -0.9966105263157895 -1.0 -0.9966105263157895
probs:  [0.09057063884078095, 0.09057063884078095, 0.5606465676263456, 0.0871601726941914, 0.08048134315712005, 0.09057063884078095]
maxi score, test score, baseline:  -0.9966105263157895 -1.0 -0.9966105263157895
probs:  [0.0906654480859159, 0.0906654480859159, 0.560350187253587, 0.08721009793437914, 0.08044337055428627, 0.0906654480859159]
printing an ep nov before normalisation:  55.492877675985824
maxi score, test score, baseline:  -0.9966105263157895 -1.0 -0.9966105263157895
probs:  [0.0906654480859159, 0.0906654480859159, 0.560350187253587, 0.08721009793437914, 0.08044337055428627, 0.0906654480859159]
Printing some Q and Qe and total Qs values:  [[0.115]
 [0.184]
 [0.184]
 [0.184]
 [0.184]
 [0.184]
 [0.184]] [[65.438]
 [56.86 ]
 [56.86 ]
 [56.86 ]
 [56.86 ]
 [56.86 ]
 [56.86 ]] [[0.998]
 [0.92 ]
 [0.92 ]
 [0.92 ]
 [0.92 ]
 [0.92 ]
 [0.92 ]]
maxi score, test score, baseline:  -0.9966105263157895 -1.0 -0.9966105263157895
probs:  [0.09076145343637308, 0.09076145343637308, 0.5600500677714114, 0.08726065302712421, 0.08040491889234502, 0.09076145343637308]
maxi score, test score, baseline:  -0.9966105263157895 -1.0 -0.9966105263157895
probs:  [0.09076145343637308, 0.09076145343637308, 0.5600500677714114, 0.08726065302712421, 0.08040491889234502, 0.09076145343637308]
printing an ep nov before normalisation:  45.890207290649414
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9966105263157895 -1.0 -0.9966105263157895
maxi score, test score, baseline:  -0.9966105263157895 -1.0 -0.9966105263157895
probs:  [0.08762240726376931, 0.09118186986762652, 0.5617396527393259, 0.08762240726376931, 0.08065179299788232, 0.09118186986762652]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
probs:  [0.08762240900930227, 0.09118187312503759, 0.5617396439486665, 0.08762240900930227, 0.08065179178265387, 0.09118187312503759]
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
probs:  [0.08762240900930227, 0.09118187312503759, 0.5617396439486665, 0.08762240900930227, 0.08065179178265387, 0.09118187312503759]
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.572]
 [0.425]
 [0.421]
 [0.42 ]
 [0.419]
 [0.42 ]] [[58.108]
 [60.247]
 [47.063]
 [46.031]
 [45.248]
 [44.224]
 [43.781]] [[1.21 ]
 [1.304]
 [0.889]
 [0.865]
 [0.848]
 [0.826]
 [0.819]]
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.469]
 [0.469]
 [0.495]
 [0.498]
 [0.469]
 [0.469]] [[58.357]
 [58.357]
 [58.357]
 [74.127]
 [74.54 ]
 [58.357]
 [58.357]] [[1.175]
 [1.175]
 [1.175]
 [1.427]
 [1.437]
 [1.175]
 [1.175]]
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
probs:  [0.08767854361595584, 0.09128495805814275, 0.5614570146517963, 0.08767854361595584, 0.08061598200000655, 0.09128495805814275]
printing an ep nov before normalisation:  41.9870240347726
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
printing an ep nov before normalisation:  24.526542125822957
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
probs:  [0.08773540543115124, 0.09138937842848265, 0.5611707239693551, 0.08773540543115124, 0.08057970831137728, 0.09138937842848265]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
probs:  [0.08779300867820088, 0.09149516035558143, 0.5608807002891047, 0.08779300867820088, 0.08054296164333066, 0.09149516035558143]
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
probs:  [0.08779300867820088, 0.09149516035558143, 0.5608807002891047, 0.08779300867820088, 0.08054296164333066, 0.09149516035558143]
printing an ep nov before normalisation:  49.183439892369236
Printing some Q and Qe and total Qs values:  [[0.393]
 [0.44 ]
 [0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]] [[28.224]
 [53.097]
 [28.224]
 [28.224]
 [28.224]
 [28.224]
 [28.224]] [[0.711]
 [1.305]
 [0.711]
 [0.711]
 [0.711]
 [0.711]
 [0.711]]
line 256 mcts: sample exp_bonus 38.25673555525484
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
printing an ep nov before normalisation:  18.206105907070622
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
probs:  [0.08797041491840593, 0.09182094709381973, 0.5599874865673282, 0.08797041491840593, 0.08042978940822045, 0.09182094709381973]
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
probs:  [0.08803113378104363, 0.091932450500833, 0.5596817762314575, 0.08803113378104363, 0.08039105520478931, 0.091932450500833]
printing an ep nov before normalisation:  45.19356727600098
printing an ep nov before normalisation:  42.64286994934082
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
probs:  [0.08803113378104363, 0.091932450500833, 0.5596817762314575, 0.08803113378104363, 0.08039105520478931, 0.091932450500833]
printing an ep nov before normalisation:  54.604056467452665
printing an ep nov before normalisation:  30.94577193260193
printing an ep nov before normalisation:  63.6131825932226
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
probs:  [0.08872291966771056, 0.08872291966771056, 0.5640856102079218, 0.08872291966771056, 0.08102271112123603, 0.08872291966771056]
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.448]
 [0.434]
 [0.43 ]
 [0.429]
 [0.43 ]
 [0.43 ]] [[33.621]
 [47.352]
 [36.124]
 [35.479]
 [34.875]
 [33.621]
 [33.621]] [[0.847]
 [1.198]
 [0.912]
 [0.892]
 [0.877]
 [0.847]
 [0.847]]
printing an ep nov before normalisation:  51.65639206077022
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
probs:  [0.08886635603776184, 0.08886635603776184, 0.5635746335563416, 0.08886635603776184, 0.08095994229261101, 0.08886635603776184]
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
probs:  [0.08886635603776184, 0.08886635603776184, 0.5635746335563416, 0.08886635603776184, 0.08095994229261101, 0.08886635603776184]
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
from probs:  [0.08893956713680902, 0.08893956713680902, 0.5633138268848102, 0.08893956713680902, 0.08092790456795375, 0.08893956713680902]
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.533]
 [0.494]
 [0.487]
 [0.466]
 [0.409]
 [0.444]] [[59.547]
 [57.708]
 [59.039]
 [59.096]
 [59.244]
 [59.141]
 [60.44 ]] [[1.339]
 [1.344]
 [1.342]
 [1.338]
 [1.32 ]
 [1.26 ]
 [1.333]]
using another actor
from probs:  [0.08929953788043428, 0.08524934286430598, 0.5655966429376562, 0.08929953788043428, 0.08125540055673498, 0.08929953788043428]
using explorer policy with actor:  0
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.61626674756947
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.170893657581544
printing an ep nov before normalisation:  39.00409936904907
maxi score, test score, baseline:  -0.996642671009772 -1.0 -0.996642671009772
probs:  [0.08559576159500959, 0.08559576159500959, 0.5678980275693075, 0.08966244434578247, 0.08158556054910843, 0.08966244434578247]
maxi score, test score, baseline:  -0.996642671009772 -1.0 -0.996642671009772
probs:  [0.08562569946615549, 0.08562569946615549, 0.567692962362793, 0.08974699949441484, 0.08156163971606636, 0.08974699949441484]
siam score:  -0.86452574
printing an ep nov before normalisation:  66.284746792377
maxi score, test score, baseline:  -0.996642671009772 -1.0 -0.996642671009772
probs:  [0.08562569946615549, 0.08562569946615549, 0.567692962362793, 0.08974699949441484, 0.08156163971606636, 0.08974699949441484]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.368]
 [0.372]
 [0.374]
 [0.374]
 [0.376]
 [0.368]
 [0.361]] [[39.467]
 [43.393]
 [19.051]
 [18.736]
 [14.574]
 [14.293]
 [16.577]] [[0.999]
 [1.081]
 [0.596]
 [0.59 ]
 [0.509]
 [0.495]
 [0.534]]
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
probs:  [0.08185414855918535, 0.08604576414978804, 0.5696531055724512, 0.09029641657969505, 0.08185414855918535, 0.09029641657969505]
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.506]
 [0.432]
 [0.437]
 [0.354]
 [0.432]
 [0.432]] [[57.75 ]
 [52.981]
 [47.256]
 [53.57 ]
 [51.708]
 [47.256]
 [47.256]] [[1.267]
 [1.289]
 [1.052]
 [1.236]
 [1.101]
 [1.052]
 [1.052]]
printing an ep nov before normalisation:  46.592117778465806
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
probs:  [0.08220327890681339, 0.08641280203169134, 0.5720862239083243, 0.08641280203169134, 0.08220327890681339, 0.09068161421466625]
actions average: 
K:  0  action  0 :  tensor([0.2807, 0.0048, 0.0963, 0.1336, 0.1918, 0.1353, 0.1575],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0175, 0.9085, 0.0072, 0.0193, 0.0136, 0.0047, 0.0291],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1554, 0.0032, 0.2583, 0.1396, 0.1460, 0.1532, 0.1443],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1741, 0.0147, 0.1263, 0.1817, 0.1513, 0.1824, 0.1694],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1855, 0.0030, 0.1321, 0.1401, 0.2298, 0.1461, 0.1634],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1935, 0.0037, 0.1348, 0.1485, 0.1639, 0.1731, 0.1825],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1850, 0.0095, 0.1379, 0.1229, 0.1303, 0.1448, 0.2695],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
probs:  [0.08253935935845959, 0.08682436333230287, 0.5743878391175263, 0.08253935935845959, 0.08253935935845959, 0.09116971947479202]
Printing some Q and Qe and total Qs values:  [[0.472]
 [0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.471]
 [0.469]] [[60.603]
 [47.948]
 [47.948]
 [47.948]
 [47.948]
 [53.578]
 [47.948]] [[1.293]
 [1.107]
 [1.107]
 [1.107]
 [1.107]
 [1.191]
 [1.107]]
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
probs:  [0.08252805830980456, 0.08687159654057965, 0.5742679171638522, 0.08252805830980456, 0.08252805830980456, 0.0912763113661545]
Printing some Q and Qe and total Qs values:  [[0.233]
 [0.233]
 [0.233]
 [0.233]
 [0.233]
 [0.233]
 [0.233]] [[63.459]
 [63.459]
 [63.459]
 [63.459]
 [63.459]
 [63.459]
 [63.459]] [[1.515]
 [1.515]
 [1.515]
 [1.515]
 [1.515]
 [1.515]
 [1.515]]
printing an ep nov before normalisation:  56.66834872907121
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.83503202924668
printing an ep nov before normalisation:  49.48931301550873
Printing some Q and Qe and total Qs values:  [[0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.067]] [[51.24]
 [51.24]
 [51.24]
 [51.24]
 [51.24]
 [51.24]
 [51.24]] [[1.4]
 [1.4]
 [1.4]
 [1.4]
 [1.4]
 [1.4]
 [1.4]]
printing an ep nov before normalisation:  68.16526231360324
Printing some Q and Qe and total Qs values:  [[-0.035]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.033]] [[41.045]
 [33.417]
 [33.417]
 [33.417]
 [33.417]
 [33.417]
 [33.417]] [[1.2 ]
 [0.97]
 [0.97]
 [0.97]
 [0.97]
 [0.97]
 [0.97]]
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
probs:  [0.0825049413604006, 0.08696821480311875, 0.5740226098636068, 0.0825049413604006, 0.0825049413604006, 0.09149435125207263]
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]]
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.668]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]] [[54.689]
 [68.137]
 [54.689]
 [54.689]
 [54.689]
 [54.689]
 [54.689]] [[1.187]
 [1.593]
 [1.187]
 [1.187]
 [1.187]
 [1.187]
 [1.187]]
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
probs:  [0.08538494554619291, 0.08973339070669986, 0.5642571576875973, 0.08109647893962385, 0.08538494554619291, 0.09414308157369304]
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
probs:  [0.08538494554619291, 0.08973339070669986, 0.5642571576875973, 0.08109647893962385, 0.08538494554619291, 0.09414308157369304]
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
probs:  [0.08538494554619291, 0.08973339070669986, 0.5642571576875973, 0.08109647893962385, 0.08538494554619291, 0.09414308157369304]
Printing some Q and Qe and total Qs values:  [[0.374]
 [0.415]
 [0.374]
 [0.367]
 [0.298]
 [0.356]
 [0.373]] [[41.442]
 [54.774]
 [41.442]
 [53.707]
 [52.854]
 [54.562]
 [54.049]] [[0.727]
 [1.126]
 [0.727]
 [1.049]
 [0.957]
 [1.06 ]
 [1.064]]
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.503]
 [0.5  ]
 [0.502]
 [0.497]
 [0.494]
 [0.496]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.499]
 [0.503]
 [0.5  ]
 [0.502]
 [0.497]
 [0.494]
 [0.496]]
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.08541191795180465, 0.08981847578851423, 0.5640044502645818, 0.08106614022318767, 0.08541191795180465, 0.09428709782010701]
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.47 ]
 [0.47 ]
 [0.5  ]
 [0.472]
 [0.471]
 [0.469]] [[57.764]
 [57.764]
 [57.764]
 [61.563]
 [56.751]
 [54.236]
 [49.503]] [[1.868]
 [1.868]
 [1.868]
 [2.078]
 [1.822]
 [1.702]
 [1.477]]
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.08541191795180465, 0.08981847578851423, 0.5640044502645818, 0.08106614022318767, 0.08541191795180465, 0.09428709782010701]
printing an ep nov before normalisation:  85.06117097729482
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.0854392859250385, 0.08990480853146716, 0.5637480370620915, 0.08103535673387094, 0.0854392859250385, 0.09443322582249353]
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.0854392859250385, 0.08990480853146716, 0.5637480370620915, 0.08103535673387094, 0.0854392859250385, 0.09443322582249353]
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.08546705913690401, 0.08999241960666364, 0.5634878271394554, 0.08100411743224462, 0.08546705913690401, 0.09458151754782829]
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
printing an ep nov before normalisation:  16.709799766540527
printing an ep nov before normalisation:  62.21154334455677
printing an ep nov before normalisation:  79.03824953676323
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.08633139834031918, 0.08633139834031918, 0.5682782953617825, 0.08170442337188531, 0.08633139834031918, 0.09102308624537442]
actions average: 
K:  4  action  0 :  tensor([0.2321, 0.0395, 0.1333, 0.1477, 0.1546, 0.1391, 0.1536],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0197, 0.8174, 0.0200, 0.0438, 0.0164, 0.0207, 0.0620],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2249, 0.0138, 0.1254, 0.1675, 0.1449, 0.1452, 0.1783],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1587, 0.0024, 0.1247, 0.2437, 0.1593, 0.1504, 0.1609],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1840, 0.0142, 0.1294, 0.1390, 0.2350, 0.1530, 0.1455],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1702, 0.0173, 0.1272, 0.1675, 0.1516, 0.2190, 0.1472],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1726, 0.0259, 0.1443, 0.1551, 0.1699, 0.1678, 0.1644],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
siam score:  -0.85933983
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.08633139834031918, 0.08633139834031918, 0.5682782953617825, 0.08170442337188531, 0.08633139834031918, 0.09102308624537442]
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.70945119200866
printing an ep nov before normalisation:  54.23531446031243
printing an ep nov before normalisation:  48.83124361186507
printing an ep nov before normalisation:  42.304320467853735
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.08928984590159869, 0.08928984590159869, 0.5579876780902137, 0.08022587752240393, 0.08928984590159869, 0.09391690668258623]
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.08928984590159869, 0.08928984590159869, 0.5579876780902137, 0.08022587752240393, 0.08928984590159869, 0.09391690668258623]
printing an ep nov before normalisation:  84.43579677677565
siam score:  -0.86198777
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.08944793398856524, 0.08944793398856524, 0.5573156572790544, 0.08014205123529476, 0.08944793398856524, 0.094198489519955]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.08944793398856524, 0.08944793398856524, 0.5573156572790544, 0.08014205123529476, 0.08944793398856524, 0.094198489519955]
printing an ep nov before normalisation:  64.49884878928414
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
using explorer policy with actor:  1
printing an ep nov before normalisation:  2.3617306334071486
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.08556969139884298, 0.09029938516129879, 0.5626273651553606, 0.08090478796190012, 0.09029938516129879, 0.09029938516129879]
line 256 mcts: sample exp_bonus 10.990502947999882
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.52 ]
 [0.466]
 [0.462]
 [0.468]
 [0.466]
 [0.46 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.47 ]
 [0.52 ]
 [0.466]
 [0.462]
 [0.468]
 [0.466]
 [0.46 ]]
Printing some Q and Qe and total Qs values:  [[0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]] [[71.896]
 [71.896]
 [71.896]
 [71.896]
 [71.896]
 [71.896]
 [71.896]] [[1.398]
 [1.398]
 [1.398]
 [1.398]
 [1.398]
 [1.398]
 [1.398]]
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.08920041978120656, 0.09407237620132643, 0.5537990457298806, 0.07965536230505335, 0.08920041978120656, 0.09407237620132643]
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.0892774668599773, 0.09421376764728527, 0.5534111840353253, 0.07960634695014943, 0.0892774668599773, 0.09421376764728527]
printing an ep nov before normalisation:  60.7347136533575
printing an ep nov before normalisation:  38.68883454205738
printing an ep nov before normalisation:  53.84613436070332
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.08935568568632214, 0.09435730940298255, 0.5530174236615868, 0.07955658615980372, 0.08935568568632214, 0.09435730940298255]
printing an ep nov before normalisation:  77.65436653120543
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.08951574715396883, 0.09465104308781423, 0.552211660314489, 0.0794547592019448, 0.08951574715396883, 0.09465104308781423]
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.08951574715396883, 0.09465104308781423, 0.552211660314489, 0.0794547592019448, 0.08951574715396883, 0.09465104308781423]
Printing some Q and Qe and total Qs values:  [[-0.061]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]] [[50.098]
 [35.615]
 [35.615]
 [35.615]
 [35.615]
 [35.615]
 [35.615]] [[0.762]
 [0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]]
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.08997113563752332, 0.09513259501604904, 0.5550244044565473, 0.07985888869184016, 0.084880381181991, 0.09513259501604904]
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.08997113563752332, 0.09513259501604904, 0.5550244044565473, 0.07985888869184016, 0.084880381181991, 0.09513259501604904]
actions average: 
K:  4  action  0 :  tensor([0.3726, 0.0391, 0.1064, 0.1143, 0.1227, 0.1182, 0.1267],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0400, 0.7455, 0.0399, 0.0525, 0.0326, 0.0366, 0.0529],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2173, 0.0368, 0.1432, 0.1338, 0.1546, 0.1489, 0.1655],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1917, 0.0029, 0.1513, 0.1379, 0.1897, 0.1688, 0.1577],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1831, 0.0266, 0.1423, 0.1424, 0.1545, 0.1477, 0.2033],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1480, 0.0079, 0.1822, 0.1588, 0.1368, 0.2074, 0.1589],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1803, 0.0141, 0.1437, 0.1107, 0.1218, 0.1250, 0.3044],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  57.61350367066102
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9966845659163988 -1.0 -0.9966845659163988
probs:  [0.09627307168027262, 0.10133416007059268, 0.533069236068115, 0.076708058842525, 0.09128131326790205, 0.10133416007059268]
printing an ep nov before normalisation:  48.85127544403076
printing an ep nov before normalisation:  28.658556938171387
printing an ep nov before normalisation:  48.417349580299614
maxi score, test score, baseline:  -0.9966845659163988 -1.0 -0.9966845659163988
probs:  [0.09627307168027262, 0.10133416007059268, 0.533069236068115, 0.076708058842525, 0.09128131326790205, 0.10133416007059268]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9966845659163988 -1.0 -0.9966845659163988
probs:  [0.09660322406842285, 0.10179377753375282, 0.5317877179957972, 0.07653772879305361, 0.0914837740752208, 0.10179377753375282]
printing an ep nov before normalisation:  61.84267952267647
siam score:  -0.8494968
maxi score, test score, baseline:  -0.9966845659163988 -1.0 -0.9966845659163988
probs:  [0.0967719449572566, 0.10202866018095182, 0.531132811728333, 0.07645068342136052, 0.09158723953114607, 0.10202866018095182]
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.421]
 [0.407]
 [0.398]
 [0.408]
 [0.392]
 [0.364]] [[53.826]
 [50.804]
 [48.404]
 [50.172]
 [47.011]
 [46.068]
 [47.512]] [[1.878]
 [1.76 ]
 [1.656]
 [1.713]
 [1.606]
 [1.554]
 [1.58 ]]
siam score:  -0.84621215
maxi score, test score, baseline:  -0.9966845659163988 -1.0 -0.9966845659163988
printing an ep nov before normalisation:  35.600526173688145
maxi score, test score, baseline:  -0.9966948717948718 -1.0 -0.9966948717948718
probs:  [0.0969431743880307, 0.1022670350993069, 0.5304681684188804, 0.07636234371893602, 0.09169224327553907, 0.1022670350993069]
maxi score, test score, baseline:  -0.9966948717948718 -1.0 -0.9966948717948718
probs:  [0.0969431743880307, 0.1022670350993069, 0.5304681684188804, 0.07636234371893602, 0.09169224327553907, 0.1022670350993069]
maxi score, test score, baseline:  -0.9966948717948718 -1.0 -0.9966948717948718
probs:  [0.0997682544229637, 0.10493786318228297, 0.5207324246122478, 0.07495413237823113, 0.09466946222199123, 0.10493786318228297]
printing an ep nov before normalisation:  35.09810924530029
maxi score, test score, baseline:  -0.9966948717948718 -1.0 -0.9966948717948718
probs:  [0.0997682544229637, 0.10493786318228297, 0.5207324246122478, 0.07495413237823113, 0.09466946222199123, 0.10493786318228297]
actor:  1 policy actor:  1  step number:  73 total reward:  0.09333333333333227  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9966948717948718 -1.0 -0.9966948717948718
probs:  [0.07514231263128589, 0.07736954417451275, 0.2565070390670522, 0.06445160122379706, 0.44915995872883935, 0.07736954417451275]
printing an ep nov before normalisation:  55.141827243754555
printing an ep nov before normalisation:  32.69081595340072
printing an ep nov before normalisation:  30.825157629437705
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
probs:  [0.07514230940322934, 0.07736954414478703, 0.2565071927916508, 0.0644515826437523, 0.44915982687179357, 0.07736954414478703]
printing an ep nov before normalisation:  31.705852131041823
printing an ep nov before normalisation:  22.66030273647668
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
probs:  [0.07530987940461344, 0.07530987940461344, 0.2570799312431238, 0.06459526960278257, 0.4501629505648718, 0.07754208977999485]
printing an ep nov before normalisation:  21.790482009284357
printing an ep nov before normalisation:  22.349273741453146
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
printing an ep nov before normalisation:  56.55737054928553
siam score:  -0.8532306
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
probs:  [0.07587619489493602, 0.073657977562876, 0.25370665325969194, 0.06508087054557707, 0.4535530829358665, 0.07812522080105244]
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
printing an ep nov before normalisation:  30.108667551121776
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
probs:  [0.07587619489493602, 0.073657977562876, 0.25370665325969194, 0.06508087054557707, 0.4535530829358665, 0.07812522080105244]
Printing some Q and Qe and total Qs values:  [[-0.03 ]
 [-0.033]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]] [[30.559]
 [32.909]
 [30.559]
 [30.559]
 [30.559]
 [30.559]
 [30.559]] [[1.56]
 [1.68]
 [1.56]
 [1.56]
 [1.56]
 [1.56]
 [1.56]]
printing an ep nov before normalisation:  22.35683047493744
printing an ep nov before normalisation:  25.888885518294984
actor:  1 policy actor:  1  step number:  77 total reward:  0.23999999999999921  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
siam score:  -0.8545891
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
probs:  [0.059500147696950216, 0.058126693293048703, 0.16798962161949432, 0.05281600293129617, 0.29334647899844524, 0.3682210554607654]
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
probs:  [0.05958074972582707, 0.05684882328521588, 0.1682175190036085, 0.05288752994632961, 0.29374457245939023, 0.3687208055796286]
Printing some Q and Qe and total Qs values:  [[0.32 ]
 [0.352]
 [0.387]
 [0.383]
 [0.387]
 [0.32 ]
 [0.32 ]] [[41.484]
 [37.58 ]
 [34.464]
 [34.719]
 [34.569]
 [41.484]
 [41.484]] [[1.523]
 [1.352]
 [1.225]
 [1.235]
 [1.231]
 [1.523]
 [1.523]]
siam score:  -0.8537752
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
probs:  [0.05958074972582707, 0.05684882328521588, 0.1682175190036085, 0.05288752994632961, 0.29374457245939023, 0.3687208055796286]
printing an ep nov before normalisation:  48.6356925140607
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
probs:  [0.059771148962756586, 0.057030472675497136, 0.16555560963552607, 0.05305649205897091, 0.29468495438583475, 0.36990132228141465]
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
probs:  [0.059947642954613724, 0.05719885583987412, 0.1660451648650734, 0.05321311452350168, 0.2925995983791088, 0.3709956234378282]
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
probs:  [0.06029661915490133, 0.05753179474775632, 0.16701314752578986, 0.053522799357396025, 0.28847628739375153, 0.37315935182040494]
siam score:  -0.8516121
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
probs:  [0.061626623761723454, 0.058843600412146715, 0.1674457415952047, 0.05349872113150263, 0.2855946391401023, 0.3729906739593202]
printing an ep nov before normalisation:  44.042931390397655
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
probs:  [0.061626623761723454, 0.058843600412146715, 0.1674457415952047, 0.05349872113150263, 0.2855946391401023, 0.3729906739593202]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]] [[44.002]
 [43.944]
 [43.944]
 [43.944]
 [43.944]
 [43.944]
 [43.944]] [[1.823]
 [1.826]
 [1.826]
 [1.826]
 [1.826]
 [1.826]
 [1.826]]
printing an ep nov before normalisation:  46.10880350591595
maxi score, test score, baseline:  -0.9967152866242038 -1.0 -0.9967152866242038
probs:  [0.06174511316498386, 0.05892996096493404, 0.16718399648187235, 0.053523377269474176, 0.28546521665376456, 0.37315233546497106]
maxi score, test score, baseline:  -0.9967152866242038 -1.0 -0.9967152866242038
probs:  [0.06174511316498386, 0.05892996096493404, 0.16718399648187235, 0.053523377269474176, 0.28546521665376456, 0.37315233546497106]
Printing some Q and Qe and total Qs values:  [[0.365]
 [0.462]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]] [[44.357]
 [47.042]
 [44.357]
 [44.357]
 [44.357]
 [44.357]
 [44.357]] [[1.565]
 [1.801]
 [1.565]
 [1.565]
 [1.565]
 [1.565]
 [1.565]]
maxi score, test score, baseline:  -0.9967152866242038 -1.0 -0.9967152866242038
probs:  [0.06174511316498386, 0.05892996096493404, 0.16718399648187235, 0.053523377269474176, 0.28546521665376456, 0.37315233546497106]
line 256 mcts: sample exp_bonus 59.79944293489504
maxi score, test score, baseline:  -0.9967152866242038 -1.0 -0.9967152866242038
probs:  [0.06174511316498386, 0.05892996096493404, 0.16718399648187235, 0.053523377269474176, 0.28546521665376456, 0.37315233546497106]
printing an ep nov before normalisation:  47.77836224524279
siam score:  -0.8525059
printing an ep nov before normalisation:  40.706580925486335
maxi score, test score, baseline:  -0.9967253968253968 -1.0 -0.9967253968253968
probs:  [0.061745088658236504, 0.05892992816260633, 0.16718421911539172, 0.05352332853523714, 0.2854654439961639, 0.37315199153236434]
maxi score, test score, baseline:  -0.9967253968253968 -1.0 -0.9967253968253968
maxi score, test score, baseline:  -0.9967253968253968 -1.0 -0.9967253968253968
maxi score, test score, baseline:  -0.9967253968253968 -1.0 -0.9967253968253968
probs:  [0.061745088658236504, 0.05892992816260633, 0.16718421911539172, 0.05352332853523714, 0.2854654439961639, 0.37315199153236434]
maxi score, test score, baseline:  -0.9967253968253968 -1.0 -0.9967253968253968
maxi score, test score, baseline:  -0.9967253968253968 -1.0 -0.9967253968253968
probs:  [0.06159251416731414, 0.058757488241691545, 0.16777568521036457, 0.053312736464005805, 0.2868915720565677, 0.3716700038600563]
actions average: 
K:  0  action  0 :  tensor([0.3050, 0.0010, 0.1117, 0.1291, 0.1436, 0.1401, 0.1696],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0124, 0.8961, 0.0205, 0.0261, 0.0047, 0.0147, 0.0255],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1275, 0.0024, 0.2516, 0.1375, 0.1356, 0.1615, 0.1838],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1729, 0.0025, 0.1207, 0.1982, 0.1511, 0.1541, 0.2005],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1924, 0.0025, 0.1083, 0.1238, 0.3064, 0.1138, 0.1528],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1723, 0.0024, 0.1363, 0.1399, 0.1345, 0.2403, 0.1744],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1311, 0.0265, 0.1542, 0.1418, 0.1212, 0.1258, 0.2993],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  93 total reward:  0.013333333333332864  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  62.36898678290171
printing an ep nov before normalisation:  53.47213420877719
siam score:  -0.85684526
printing an ep nov before normalisation:  32.66962921426472
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]] [[62.31]
 [62.31]
 [62.31]
 [62.31]
 [62.31]
 [62.31]
 [62.31]] [[1.865]
 [1.865]
 [1.865]
 [1.865]
 [1.865]
 [1.865]
 [1.865]]
maxi score, test score, baseline:  -0.9967253968253968 -1.0 -0.9967253968253968
probs:  [0.0679205913796473, 0.06590949069924915, 0.14324443431227069, 0.06204711190908058, 0.22774244151480433, 0.43313593018494784]
maxi score, test score, baseline:  -0.9967253968253968 -1.0 -0.9967253968253968
maxi score, test score, baseline:  -0.9967253968253968 -1.0 -0.9967253968253968
probs:  [0.06798749267971022, 0.06498826733296173, 0.1433856748796233, 0.06210821650330262, 0.22796707559606058, 0.4335632730083414]
maxi score, test score, baseline:  -0.9967354430379747 -1.0 -0.9967354430379747
probs:  [0.0679874874892081, 0.06498825569732468, 0.14338578655025352, 0.06210819867862866, 0.22796712471340136, 0.4335631468711837]
maxi score, test score, baseline:  -0.9967354430379747 -1.0 -0.9967354430379747
probs:  [0.0679874874892081, 0.06498825569732468, 0.14338578655025352, 0.06210819867862866, 0.22796712471340136, 0.4335631468711837]
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]] [[67.628]
 [67.628]
 [67.628]
 [67.628]
 [67.628]
 [67.628]
 [67.628]] [[2.191]
 [2.191]
 [2.191]
 [2.191]
 [2.191]
 [2.191]
 [2.191]]
maxi score, test score, baseline:  -0.9967354430379747 -1.0 -0.9967354430379747
probs:  [0.0679874874892081, 0.06498825569732468, 0.14338578655025352, 0.06210819867862866, 0.22796712471340136, 0.4335631468711837]
printing an ep nov before normalisation:  63.681805961474254
maxi score, test score, baseline:  -0.9967354430379747 -1.0 -0.9967354430379747
probs:  [0.06791281678964584, 0.06486602360512708, 0.14450677139001408, 0.061940295050456715, 0.22839805702620064, 0.4323760361385556]
maxi score, test score, baseline:  -0.9967354430379747 -1.0 -0.9967354430379747
probs:  [0.06804910526516786, 0.06499618571136717, 0.1447970719110631, 0.06206457421930689, 0.22684789028567193, 0.4332451726074231]
printing an ep nov before normalisation:  50.655867567945776
maxi score, test score, baseline:  -0.9967354430379747 -1.0 -0.9967354430379747
maxi score, test score, baseline:  -0.9967354430379747 -1.0 -0.9967354430379747
probs:  [0.06804910526516786, 0.06499618571136717, 0.1447970719110631, 0.06206457421930689, 0.22684789028567193, 0.4332451726074231]
line 256 mcts: sample exp_bonus 41.00715098923214
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.46 ]
 [0.443]
 [0.443]
 [0.443]
 [0.443]
 [0.443]] [[49.462]
 [48.425]
 [45.258]
 [45.258]
 [45.258]
 [45.258]
 [45.258]] [[1.731]
 [1.746]
 [1.596]
 [1.596]
 [1.596]
 [1.596]
 [1.596]]
siam score:  -0.85051423
maxi score, test score, baseline:  -0.9967354430379747 -1.0 -0.9967354430379747
probs:  [0.0679443935825197, 0.06487060408648453, 0.14521701395816047, 0.061918951921417595, 0.22782873667682216, 0.43222029977459553]
maxi score, test score, baseline:  -0.9967354430379747 -1.0 -0.9967354430379747
maxi score, test score, baseline:  -0.9967354430379747 -1.0 -0.9967354430379747
probs:  [0.0679443935825197, 0.06487060408648453, 0.14521701395816047, 0.061918951921417595, 0.22782873667682216, 0.43222029977459553]
Printing some Q and Qe and total Qs values:  [[0.368]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]] [[32.939]
 [23.158]
 [23.158]
 [23.158]
 [23.158]
 [23.158]
 [23.158]] [[1.571]
 [1.059]
 [1.059]
 [1.059]
 [1.059]
 [1.059]
 [1.059]]
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.529]
 [0.439]
 [0.497]
 [0.502]
 [0.488]
 [0.439]] [[45.083]
 [54.734]
 [45.083]
 [58.28 ]
 [59.15 ]
 [59.594]
 [45.083]] [[1.495]
 [1.99 ]
 [1.495]
 [2.106]
 [2.148]
 [2.152]
 [1.495]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
siam score:  -0.852411
Printing some Q and Qe and total Qs values:  [[0.387]
 [0.38 ]
 [0.381]
 [0.387]
 [0.387]
 [0.387]
 [0.387]] [[63.407]
 [63.517]
 [66.028]
 [62.985]
 [62.985]
 [62.985]
 [62.985]] [[2.092]
 [2.089]
 [2.181]
 [2.077]
 [2.077]
 [2.077]
 [2.077]]
maxi score, test score, baseline:  -0.9967454258675079 -1.0 -0.9967454258675079
probs:  [0.06801212420444683, 0.06393716905263996, 0.14536205132628785, 0.06198065085061352, 0.22805622622996663, 0.4326517783360452]
printing an ep nov before normalisation:  52.890402598264465
maxi score, test score, baseline:  -0.9967454258675079 -1.0 -0.9967454258675079
probs:  [0.06804450682421473, 0.06393356452067701, 0.1460775343631173, 0.06195976771930957, 0.22748540313249124, 0.43249922344019015]
Printing some Q and Qe and total Qs values:  [[0.495]
 [0.499]
 [0.495]
 [0.495]
 [0.495]
 [0.467]
 [0.488]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.495]
 [0.499]
 [0.495]
 [0.495]
 [0.495]
 [0.467]
 [0.488]]
maxi score, test score, baseline:  -0.9967454258675079 -1.0 -0.9967454258675079
probs:  [0.06794077392498837, 0.06380200001422148, 0.1465020950795659, 0.06181484035507842, 0.22846110557306587, 0.4314791850530799]
printing an ep nov before normalisation:  63.67112815861156
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9967454258675079 -1.0 -0.9967454258675079
maxi score, test score, baseline:  -0.9967454258675079 -1.0 -0.9967454258675079
probs:  [0.06794077392498837, 0.06380200001422148, 0.1465020950795659, 0.06181484035507842, 0.22846110557306587, 0.4314791850530799]
maxi score, test score, baseline:  -0.9967454258675079 -1.0 -0.9967454258675079
probs:  [0.06695543727207953, 0.06386935629639567, 0.1466569412041485, 0.06188009438712482, 0.22870265753403662, 0.4319355133062148]
maxi score, test score, baseline:  -0.9967454258675079 -1.0 -0.9967454258675079
probs:  [0.06695543727207953, 0.06386935629639567, 0.1466569412041485, 0.06188009438712482, 0.22870265753403662, 0.4319355133062148]
maxi score, test score, baseline:  -0.9967454258675079 -1.0 -0.9967454258675079
maxi score, test score, baseline:  -0.9967454258675079 -1.0 -0.9967454258675079
probs:  [0.06695543727207953, 0.06386935629639567, 0.1466569412041485, 0.06188009438712482, 0.22870265753403662, 0.4319355133062148]
maxi score, test score, baseline:  -0.9967454258675079 -1.0 -0.9967454258675079
probs:  [0.06695543727207953, 0.06386935629639567, 0.1466569412041485, 0.06188009438712482, 0.22870265753403662, 0.4319355133062148]
printing an ep nov before normalisation:  53.949803163332824
maxi score, test score, baseline:  -0.9967553459119497 -1.0 -0.9967553459119497
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9967553459119497 -1.0 -0.9967553459119497
probs:  [0.06684490231020569, 0.06373803021832292, 0.14708331331787627, 0.061735366529956974, 0.2296815291106068, 0.43091685851303135]
printing an ep nov before normalisation:  71.48143893723714
from probs:  [0.06684490231020569, 0.06373803021832292, 0.14708331331787627, 0.061735366529956974, 0.2296815291106068, 0.43091685851303135]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9967553459119497 -1.0 -0.9967553459119497
probs:  [0.06698004286648582, 0.06386687705735976, 0.1473809960889616, 0.06186015649165379, 0.22812241304716527, 0.43178951444837366]
printing an ep nov before normalisation:  47.42530343670749
maxi score, test score, baseline:  -0.9967553459119497 -1.0 -0.9967553459119497
probs:  [0.06705995050017574, 0.06394306326155534, 0.14636258501381827, 0.06193394389361459, 0.22839494773830932, 0.43230550959252667]
printing an ep nov before normalisation:  54.64537908485152
actions average: 
K:  4  action  0 :  tensor([0.3002, 0.0145, 0.1326, 0.1193, 0.1429, 0.1387, 0.1518],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0077, 0.9501, 0.0089, 0.0104, 0.0049, 0.0046, 0.0135],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1384, 0.0038, 0.2001, 0.1402, 0.1814, 0.1416, 0.1945],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1349, 0.0555, 0.1198, 0.2336, 0.1511, 0.1431, 0.1620],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1564, 0.0435, 0.1445, 0.1604, 0.2044, 0.1300, 0.1608],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1099, 0.0071, 0.1306, 0.1293, 0.1353, 0.3245, 0.1632],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1545, 0.0414, 0.1622, 0.1491, 0.1775, 0.1443, 0.1709],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9967553459119497 -1.0 -0.9967553459119497
probs:  [0.06705995050017574, 0.06394306326155534, 0.14636258501381827, 0.06193394389361459, 0.22839494773830932, 0.43230550959252667]
maxi score, test score, baseline:  -0.9967553459119497 -1.0 -0.9967553459119497
maxi score, test score, baseline:  -0.9967553459119497 -1.0 -0.9967553459119497
probs:  [0.06705995050017574, 0.06394306326155534, 0.14636258501381827, 0.06193394389361459, 0.22839494773830932, 0.43230550959252667]
line 256 mcts: sample exp_bonus 26.84833792300608
printing an ep nov before normalisation:  37.4279144802702
maxi score, test score, baseline:  -0.9967652037617555 -1.0 -0.9967652037617555
probs:  [0.06727384055001481, 0.06414698509095727, 0.1456439039231078, 0.06213144029174137, 0.22711723022164265, 0.43368659992253616]
maxi score, test score, baseline:  -0.9967652037617555 -1.0 -0.9967652037617555
probs:  [0.06727384055001481, 0.06414698509095727, 0.1456439039231078, 0.06213144029174137, 0.22711723022164265, 0.43368659992253616]
printing an ep nov before normalisation:  56.95537443312312
maxi score, test score, baseline:  -0.9967652037617555 -1.0 -0.9967652037617555
probs:  [0.06727384055001481, 0.06414698509095727, 0.1456439039231078, 0.06213144029174137, 0.22711723022164265, 0.43368659992253616]
printing an ep nov before normalisation:  60.06181115730766
Printing some Q and Qe and total Qs values:  [[0.343]
 [0.42 ]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]] [[40.523]
 [57.776]
 [40.523]
 [40.523]
 [40.523]
 [40.523]
 [40.523]] [[0.343]
 [0.42 ]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]]
printing an ep nov before normalisation:  34.62391785621504
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
probs:  [0.0674095413137141, 0.06225673605073199, 0.14593813055956723, 0.06225673605073199, 0.2275760646843334, 0.43456279134092113]
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
probs:  [0.06815950568258886, 0.06303711967898197, 0.1462245063775995, 0.06205308236776273, 0.2273804961310464, 0.4331452897620205]
printing an ep nov before normalisation:  50.12059673234443
printing an ep nov before normalisation:  56.37991663899842
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
probs:  [0.06823922983975171, 0.0631108405986722, 0.14522469533763166, 0.06212565003393849, 0.22764682110580034, 0.4336527630842056]
printing an ep nov before normalisation:  35.20329677647319
printing an ep nov before normalisation:  37.36066120784999
Printing some Q and Qe and total Qs values:  [[0.437]
 [0.56 ]
 [0.531]
 [0.494]
 [0.539]
 [0.272]
 [0.479]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.437]
 [0.56 ]
 [0.531]
 [0.494]
 [0.539]
 [0.272]
 [0.479]]
maxi score, test score, baseline:  -0.9967847352024922 -1.0 -0.9967847352024922
printing an ep nov before normalisation:  47.32405075657651
printing an ep nov before normalisation:  52.680187919223656
printing an ep nov before normalisation:  52.193202428311274
printing an ep nov before normalisation:  64.22690132062577
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]] [[62.397]
 [52.632]
 [52.632]
 [52.632]
 [52.632]
 [52.632]
 [52.632]] [[2.152]
 [1.754]
 [1.754]
 [1.754]
 [1.754]
 [1.754]
 [1.754]]
printing an ep nov before normalisation:  67.50343862342056
using another actor
Printing some Q and Qe and total Qs values:  [[ 0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.007]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.007]]
printing an ep nov before normalisation:  20.265442070774142
actions average: 
K:  1  action  0 :  tensor([0.2810, 0.0036, 0.1143, 0.1419, 0.1437, 0.1361, 0.1794],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0251, 0.7917, 0.0178, 0.0528, 0.0171, 0.0249, 0.0706],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1790, 0.0051, 0.1784, 0.1287, 0.1384, 0.1869, 0.1834],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1702, 0.0041, 0.1429, 0.1595, 0.1818, 0.1629, 0.1786],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1809, 0.0273, 0.1123, 0.1678, 0.1889, 0.1289, 0.1940],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1958, 0.0143, 0.1119, 0.1307, 0.1471, 0.2430, 0.1573],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1823, 0.0334, 0.0933, 0.1443, 0.1231, 0.1329, 0.2907],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9967944099378883 -1.0 -0.9967944099378883
probs:  [0.06745617588186406, 0.06237574579869136, 0.1458103440687323, 0.06237574579869136, 0.22658028242329317, 0.43540170602872774]
using another actor
maxi score, test score, baseline:  -0.9968040247678018 -1.0 -0.9968040247678018
probs:  [0.06756107262422605, 0.062430214533930585, 0.14551738378486065, 0.062430214533930585, 0.22628509095052968, 0.43577602357252243]
maxi score, test score, baseline:  -0.9968040247678018 -1.0 -0.9968040247678018
printing an ep nov before normalisation:  38.02593013688036
maxi score, test score, baseline:  -0.9968040247678018 -1.0 -0.9968040247678018
probs:  [0.06756107262422605, 0.062430214533930585, 0.14551738378486065, 0.062430214533930585, 0.22628509095052968, 0.43577602357252243]
Printing some Q and Qe and total Qs values:  [[0.66 ]
 [0.657]
 [0.65 ]
 [0.664]
 [0.659]
 [0.65 ]
 [0.65 ]] [[55.702]
 [39.18 ]
 [42.241]
 [40.725]
 [39.216]
 [31.848]
 [31.848]] [[0.66 ]
 [0.657]
 [0.65 ]
 [0.664]
 [0.659]
 [0.65 ]
 [0.65 ]]
maxi score, test score, baseline:  -0.9968135802469136 -1.0 -0.9968135802469136
probs:  [0.06756106679465697, 0.062430197993547765, 0.1455174957063043, 0.062430197993547765, 0.2262851351237683, 0.43577590638817487]
printing an ep nov before normalisation:  14.543507099151611
maxi score, test score, baseline:  -0.9968135802469136 -1.0 -0.9968135802469136
probs:  [0.06756106679465697, 0.062430197993547765, 0.1455174957063043, 0.062430197993547765, 0.2262851351237683, 0.43577590638817487]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9968135802469136 -1.0 -0.9968135802469136
probs:  [0.06756106679465697, 0.062430197993547765, 0.1455174957063043, 0.062430197993547765, 0.2262851351237683, 0.43577590638817487]
printing an ep nov before normalisation:  28.929107189178467
maxi score, test score, baseline:  -0.9968135802469136 -1.0 -0.9968135802469136
using another actor
printing an ep nov before normalisation:  46.87976186890501
maxi score, test score, baseline:  -0.9968325153374233 -1.0 -0.9968325153374233
probs:  [0.06745475905568775, 0.06228936264592374, 0.1459356973112414, 0.06228936264592374, 0.22724638204539974, 0.4347844362958237]
maxi score, test score, baseline:  -0.9968325153374233 -1.0 -0.9968325153374233
probs:  [0.06745475905568775, 0.06228936264592374, 0.1459356973112414, 0.06228936264592374, 0.22724638204539974, 0.4347844362958237]
siam score:  -0.8439773
from probs:  [0.06745475905568775, 0.06228936264592374, 0.1459356973112414, 0.06228936264592374, 0.22724638204539974, 0.4347844362958237]
maxi score, test score, baseline:  -0.9968325153374233 -1.0 -0.9968325153374233
probs:  [0.06810574418288061, 0.06194355728414867, 0.14664051727235544, 0.06293680451453638, 0.22800697793511476, 0.4323663988109641]
printing an ep nov before normalisation:  29.059932809057095
printing an ep nov before normalisation:  44.37381866093116
maxi score, test score, baseline:  -0.9968325153374233 -1.0 -0.9968325153374233
probs:  [0.06810574418288061, 0.06194355728414867, 0.14664051727235544, 0.06293680451453638, 0.22800697793511476, 0.4323663988109641]
using explorer policy with actor:  1
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.5973],
        [-0.6083],
        [-0.5769],
        [-0.5795],
        [-0.6167],
        [-0.5512],
        [-0.4109],
        [-0.3791],
        [-0.4615],
        [-0.4719]], dtype=torch.float64)
-0.032346567066 -0.6296501825587004
-0.032346567066 -0.640683582650914
-0.070771701198 -0.6476817697025609
-0.09703970119800001 -0.6765305510110898
-0.032346567066 -0.6490903868324771
-0.09703970119800001 -0.6482871982220175
-0.084359833866 -0.4952418175727705
-0.083839701198 -0.462904816817401
-0.08410238119800001 -0.5455717780147921
-0.09703970119800001 -0.568904794578318
maxi score, test score, baseline:  -0.9968325153374233 -1.0 -0.9968325153374233
maxi score, test score, baseline:  -0.9968325153374233 -1.0 -0.9968325153374233
probs:  [0.06865372088476016, 0.06145968525509609, 0.14775476420499736, 0.06344751088960854, 0.22970791281047626, 0.4289764059550616]
printing an ep nov before normalisation:  50.06716796129254
printing an ep nov before normalisation:  0.0
printing an ep nov before normalisation:  53.63548686733998
printing an ep nov before normalisation:  64.79337816513859
printing an ep nov before normalisation:  56.06409988739131
printing an ep nov before normalisation:  43.89260237903155
maxi score, test score, baseline:  -0.9968512195121951 -1.0 -0.9968512195121951
probs:  [0.06865371512344023, 0.061459650296870734, 0.14775499013191176, 0.06344748399894914, 0.2297080019360625, 0.4289761585127656]
printing an ep nov before normalisation:  21.282421488830252
printing an ep nov before normalisation:  52.77517387869469
using explorer policy with actor:  1
printing an ep nov before normalisation:  60.48463821411133
maxi score, test score, baseline:  -0.9968512195121951 -1.0 -0.9968512195121951
probs:  [0.0684627988425988, 0.06117538324674517, 0.14859049841790542, 0.06318901124033631, 0.23160693915708777, 0.42697536909532646]
maxi score, test score, baseline:  -0.9968512195121951 -1.0 -0.9968512195121951
probs:  [0.0684369914005378, 0.061095373861087925, 0.1491606604242386, 0.06210304685669867, 0.2327945563338203, 0.4264093711236167]
Printing some Q and Qe and total Qs values:  [[0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.655]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.655]]
maxi score, test score, baseline:  -0.9968512195121951 -1.0 -0.9968512195121951
probs:  [0.0684369914005378, 0.061095373861087925, 0.1491606604242386, 0.06210304685669867, 0.2327945563338203, 0.4264093711236167]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9968512195121951 -1.0 -0.9968512195121951
probs:  [0.0684369914005378, 0.061095373861087925, 0.1491606604242386, 0.06210304685669867, 0.2327945563338203, 0.4264093711236167]
using explorer policy with actor:  1
printing an ep nov before normalisation:  63.16543036114033
maxi score, test score, baseline:  -0.9968512195121951 -1.0 -0.9968512195121951
probs:  [0.0684369914005378, 0.061095373861087925, 0.1491606604242386, 0.06210304685669867, 0.2327945563338203, 0.4264093711236167]
from probs:  [0.0684369914005378, 0.061095373861087925, 0.1491606604242386, 0.06210304685669867, 0.2327945563338203, 0.4264093711236167]
siam score:  -0.8442537
printing an ep nov before normalisation:  38.17765639533988
printing an ep nov before normalisation:  35.808652587205756
using explorer policy with actor:  0
printing an ep nov before normalisation:  35.27388181204415
Printing some Q and Qe and total Qs values:  [[0.222]
 [0.132]
 [0.156]
 [0.19 ]
 [0.179]
 [0.132]
 [0.132]] [[35.715]
 [25.771]
 [29.185]
 [30.902]
 [31.593]
 [25.771]
 [25.771]] [[1.478]
 [0.849]
 [1.058]
 [1.185]
 [1.212]
 [0.849]
 [0.849]]
maxi score, test score, baseline:  -0.9968604863221885 -1.0 -0.9968604863221885
probs:  [0.06741915425986969, 0.06116201158858734, 0.14932372082622886, 0.06217078840269386, 0.23304899283917463, 0.4268753320834455]
printing an ep nov before normalisation:  59.772222117452046
printing an ep nov before normalisation:  37.036694206794
maxi score, test score, baseline:  -0.996869696969697 -1.0 -0.996869696969697
probs:  [0.06741914934444934, 0.06116199409560752, 0.14932383560307524, 0.06217077293746873, 0.23304903977088132, 0.42687520824851777]
siam score:  -0.8368155
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.483]
 [0.528]
 [0.448]
 [0.443]
 [0.445]
 [0.437]
 [0.438]] [[36.625]
 [43.395]
 [37.845]
 [40.001]
 [40.345]
 [40.234]
 [37.088]] [[1.949]
 [2.528]
 [2.009]
 [2.175]
 [2.204]
 [2.187]
 [1.94 ]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9968788519637463 -1.0 -0.9968788519637463
printing an ep nov before normalisation:  65.61917413671497
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9968788519637463 -1.0 -0.9968788519637463
maxi score, test score, baseline:  -0.9968788519637463 -1.0 -0.9968788519637463
maxi score, test score, baseline:  -0.9968788519637463 -1.0 -0.9968788519637463
probs:  [0.06731731638680874, 0.061020195354049726, 0.14974510037167044, 0.06203541748133551, 0.23400483904946584, 0.42587713135666966]
printing an ep nov before normalisation:  35.11218881446787
printing an ep nov before normalisation:  32.554073333740234
maxi score, test score, baseline:  -0.9968788519637463 -1.0 -0.9968788519637463
probs:  [0.06735349473468187, 0.06100335862238192, 0.15047523322694573, 0.062027127843101326, 0.23338796641145113, 0.425752819161438]
maxi score, test score, baseline:  -0.9968788519637463 -1.0 -0.9968788519637463
probs:  [0.06735349473468187, 0.06100335862238192, 0.15047523322694573, 0.062027127843101326, 0.23338796641145113, 0.425752819161438]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9968788519637463 -1.0 -0.9968788519637463
actions average: 
K:  0  action  0 :  tensor([0.3196, 0.0079, 0.1012, 0.1219, 0.1662, 0.1486, 0.1346],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0021, 0.9876, 0.0017, 0.0027, 0.0020, 0.0016, 0.0023],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1262, 0.0189, 0.1591, 0.1506, 0.1439, 0.2087, 0.1926],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1583, 0.0212, 0.1167, 0.1970, 0.1563, 0.1873, 0.1631],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2078, 0.0128, 0.1293, 0.1512, 0.2017, 0.1524, 0.1447],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.2561, 0.0010, 0.1190, 0.1412, 0.1572, 0.1737, 0.1520],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1716, 0.0311, 0.1270, 0.1735, 0.1473, 0.1543, 0.1951],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  4.588593185417267e-06
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.426]
 [0.547]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]] [[42.794]
 [71.048]
 [42.794]
 [42.794]
 [42.794]
 [42.794]
 [42.794]] [[1.083]
 [1.984]
 [1.083]
 [1.083]
 [1.083]
 [1.083]
 [1.083]]
printing an ep nov before normalisation:  72.76509001645368
printing an ep nov before normalisation:  53.92271326751853
maxi score, test score, baseline:  -0.9968788519637463 -1.0 -0.9968788519637463
printing an ep nov before normalisation:  74.47232189206062
maxi score, test score, baseline:  -0.9968788519637463 -1.0 -0.9968788519637463
probs:  [0.06737274601782298, 0.06092098625623293, 0.15058069778767233, 0.06196113924611891, 0.23400083217980194, 0.42516359851235097]
maxi score, test score, baseline:  -0.9968788519637463 -1.0 -0.9968788519637463
probs:  [0.06737274601782298, 0.06092098625623293, 0.15058069778767233, 0.06196113924611891, 0.23400083217980194, 0.42516359851235097]
maxi score, test score, baseline:  -0.9968788519637463 -1.0 -0.9968788519637463
probs:  [0.06737274601782298, 0.06092098625623293, 0.15058069778767233, 0.06196113924611891, 0.23400083217980194, 0.42516359851235097]
line 256 mcts: sample exp_bonus 42.8407219579636
actor:  1 policy actor:  1  step number:  67 total reward:  0.06666666666666621  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9968788519637463 -1.0 -0.9968788519637463
maxi score, test score, baseline:  -0.9968788519637463 -1.0 -0.9968788519637463
maxi score, test score, baseline:  -0.9968788519637463 -1.0 -0.9968788519637463
maxi score, test score, baseline:  -0.9968788519637463 -1.0 -0.9968788519637463
probs:  [0.07115603298807169, 0.06623040803216429, 0.13468151733848424, 0.06702451750217989, 0.19836899346449513, 0.4625385306746047]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.463]
 [0.536]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]] [[50.691]
 [58.396]
 [50.691]
 [50.691]
 [50.691]
 [50.691]
 [50.691]] [[1.126]
 [1.396]
 [1.126]
 [1.126]
 [1.126]
 [1.126]
 [1.126]]
maxi score, test score, baseline:  -0.9968788519637463 -1.0 -0.9968788519637463
maxi score, test score, baseline:  -0.9968788519637463 -1.0 -0.9968788519637463
probs:  [0.0717299985069793, 0.06600736467174455, 0.1344592380711168, 0.06758897028820533, 0.19923586431470075, 0.46097856414725324]
maxi score, test score, baseline:  -0.9968788519637463 -1.0 -0.9968788519637463
probs:  [0.07179693037939816, 0.06606894676946869, 0.13365071449034716, 0.06765203094177412, 0.19942199438745326, 0.46140938303155854]
using explorer policy with actor:  1
using another actor
printing an ep nov before normalisation:  48.209122279046454
maxi score, test score, baseline:  -0.9968879518072289 -1.0 -0.9968879518072289
probs:  [0.07179693057978308, 0.06606893671828276, 0.13365079084372308, 0.06765202372388884, 0.1994220064003984, 0.4614093117339238]
maxi score, test score, baseline:  -0.996896996996997 -1.0 -0.996896996996997
probs:  [0.07179693077897553, 0.06606892672763849, 0.13365086673727822, 0.06765201654948329, 0.19942201834088855, 0.46140924086573587]
printing an ep nov before normalisation:  67.13011732512769
printing an ep nov before normalisation:  28.850221960012057
printing an ep nov before normalisation:  36.052584648132324
maxi score, test score, baseline:  -0.996896996996997 -1.0 -0.996896996996997
probs:  [0.07248960069341848, 0.06600430363032822, 0.13324033237920832, 0.06756245941821354, 0.19974129215567565, 0.4609620117231559]
printing an ep nov before normalisation:  21.760017531258722
maxi score, test score, baseline:  -0.996896996996997 -1.0 -0.996896996996997
maxi score, test score, baseline:  -0.996896996996997 -1.0 -0.996896996996997
probs:  [0.07255579272683665, 0.06606456287612116, 0.13244802495674177, 0.06762414407402033, 0.1999238947795499, 0.46138358058673024]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  49.60218920457237
siam score:  -0.84110063
Printing some Q and Qe and total Qs values:  [[0.254]
 [0.127]
 [0.127]
 [0.127]
 [0.127]
 [0.127]
 [0.127]] [[35.916]
 [32.013]
 [32.013]
 [32.013]
 [32.013]
 [32.013]
 [32.013]] [[1.752]
 [1.373]
 [1.373]
 [1.373]
 [1.373]
 [1.373]
 [1.373]]
maxi score, test score, baseline:  -0.996896996996997 -1.0 -0.996896996996997
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.321]
 [0.321]
 [0.321]
 [0.321]
 [0.321]
 [0.321]] [[63.118]
 [55.509]
 [55.509]
 [55.509]
 [55.509]
 [55.509]
 [55.509]] [[1.78 ]
 [1.442]
 [1.442]
 [1.442]
 [1.442]
 [1.442]
 [1.442]]
Printing some Q and Qe and total Qs values:  [[0.322]
 [0.307]
 [0.325]
 [0.326]
 [0.326]
 [0.323]
 [0.32 ]] [[30.846]
 [36.135]
 [31.421]
 [30.72 ]
 [30.583]
 [30.207]
 [29.871]] [[0.322]
 [0.307]
 [0.325]
 [0.326]
 [0.326]
 [0.323]
 [0.32 ]]
maxi score, test score, baseline:  -0.9969059880239521 -1.0 -0.9969059880239521
probs:  [0.07249058677180097, 0.06596019248276787, 0.13274414118502345, 0.06752918331844465, 0.2006269416165074, 0.4606489546254557]
maxi score, test score, baseline:  -0.9969059880239521 -1.0 -0.9969059880239521
maxi score, test score, baseline:  -0.9969059880239521 -1.0 -0.9969059880239521
probs:  [0.07249058677180097, 0.06596019248276787, 0.13274414118502345, 0.06752918331844465, 0.2006269416165074, 0.4606489546254557]
printing an ep nov before normalisation:  49.561745322871325
printing an ep nov before normalisation:  61.26286404920684
Printing some Q and Qe and total Qs values:  [[-0.063]
 [-0.069]
 [-0.069]
 [-0.069]
 [-0.069]
 [-0.069]
 [-0.069]] [[53.129]
 [52.116]
 [52.116]
 [52.116]
 [52.116]
 [52.116]
 [52.116]] [[1.575]
 [1.525]
 [1.525]
 [1.525]
 [1.525]
 [1.525]
 [1.525]]
maxi score, test score, baseline:  -0.9969059880239521 -1.0 -0.9969059880239521
probs:  [0.07236010035229147, 0.0657513571708971, 0.13333655127683694, 0.06733917209110223, 0.20203378066755234, 0.4591790384413199]
printing an ep nov before normalisation:  50.43626367186617
printing an ep nov before normalisation:  31.253831403815795
Printing some Q and Qe and total Qs values:  [[0.579]
 [0.59 ]
 [0.564]
 [0.575]
 [0.574]
 [0.57 ]
 [0.577]] [[38.616]
 [38.31 ]
 [39.04 ]
 [38.622]
 [38.134]
 [38.13 ]
 [36.327]] [[0.579]
 [0.59 ]
 [0.564]
 [0.575]
 [0.574]
 [0.57 ]
 [0.577]]
Printing some Q and Qe and total Qs values:  [[0.892]
 [0.892]
 [0.892]
 [0.892]
 [0.892]
 [0.892]
 [0.327]] [[31.768]
 [31.768]
 [31.768]
 [31.768]
 [31.768]
 [31.768]
 [55.134]] [[0.95]
 [0.95]
 [0.95]
 [0.95]
 [0.95]
 [0.95]
 [0.66]]
siam score:  -0.8395642
maxi score, test score, baseline:  -0.9969149253731343 -1.0 -0.9969149253731343
maxi score, test score, baseline:  -0.9969149253731343 -1.0 -0.9969149253731343
probs:  [0.07087577697779268, 0.06596907154436861, 0.13377860206692258, 0.06756215772405175, 0.20111228627580258, 0.46070210541106177]
Printing some Q and Qe and total Qs values:  [[0.176]
 [0.054]
 [0.054]
 [0.054]
 [0.054]
 [0.054]
 [0.054]] [[40.103]
 [31.867]
 [31.867]
 [31.867]
 [31.867]
 [31.867]
 [31.867]] [[1.38]
 [1.01]
 [1.01]
 [1.01]
 [1.01]
 [1.01]
 [1.01]]
printing an ep nov before normalisation:  59.36132797057368
printing an ep nov before normalisation:  39.2412313595265
maxi score, test score, baseline:  -0.9969149253731343 -1.0 -0.9969149253731343
printing an ep nov before normalisation:  44.843943384954414
maxi score, test score, baseline:  -0.9969238095238095 -1.0 -0.9969238095238095
probs:  [0.07104423202347131, 0.066125833835227, 0.13409692138770826, 0.06691912386558897, 0.20001511997192, 0.4617987689160845]
maxi score, test score, baseline:  -0.9969238095238095 -1.0 -0.9969238095238095
probs:  [0.07104423202347131, 0.066125833835227, 0.13409692138770826, 0.06691912386558897, 0.20001511997192, 0.4617987689160845]
printing an ep nov before normalisation:  43.42603739276604
printing an ep nov before normalisation:  56.47725481987642
printing an ep nov before normalisation:  39.62338447570801
maxi score, test score, baseline:  -0.9969238095238095 -1.0 -0.9969238095238095
maxi score, test score, baseline:  -0.9969238095238095 -1.0 -0.9969238095238095
probs:  [0.07184514276436524, 0.06615245689119134, 0.1347994448178339, 0.06615245689119134, 0.19906055170265838, 0.4619899469327597]
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.353]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]] [[36.256]
 [43.509]
 [36.256]
 [36.256]
 [36.256]
 [36.256]
 [36.256]] [[0.347]
 [0.353]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]]
maxi score, test score, baseline:  -0.9969238095238095 -1.0 -0.9969238095238095
probs:  [0.07184514276436524, 0.06615245689119134, 0.1347994448178339, 0.06615245689119134, 0.19906055170265838, 0.4619899469327597]
maxi score, test score, baseline:  -0.9969238095238095 -1.0 -0.9969238095238095
probs:  [0.07184514276436524, 0.06615245689119134, 0.1347994448178339, 0.06615245689119134, 0.19906055170265838, 0.4619899469327597]
maxi score, test score, baseline:  -0.9969238095238095 -1.0 -0.9969238095238095
maxi score, test score, baseline:  -0.9969238095238095 -1.0 -0.9969238095238095
probs:  [0.07184514276436524, 0.06615245689119134, 0.1347994448178339, 0.06615245689119134, 0.19906055170265838, 0.4619899469327597]
printing an ep nov before normalisation:  56.424879562667876
siam score:  -0.8484554
maxi score, test score, baseline:  -0.9969238095238095 -1.0 -0.9969238095238095
probs:  [0.07177652513290197, 0.06604993583892552, 0.13510575850023576, 0.06604993583892552, 0.19974957951973984, 0.4612682651692714]
Printing some Q and Qe and total Qs values:  [[0.299]
 [0.189]
 [0.189]
 [0.169]
 [0.154]
 [0.15 ]
 [0.16 ]] [[28.522]
 [36.353]
 [36.353]
 [36.347]
 [36.585]
 [36.381]
 [34.634]] [[1.462]
 [2.165]
 [2.165]
 [2.144]
 [2.154]
 [2.129]
 [1.958]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.885441986510514
printing an ep nov before normalisation:  34.83423685948196
maxi score, test score, baseline:  -0.9969326409495549 -1.0 -0.9969326409495549
maxi score, test score, baseline:  -0.9969326409495549 -1.0 -0.9969326409495549
probs:  [0.07170787929241035, 0.0659473624666781, 0.13541227633595398, 0.0659473624666781, 0.20043890487683305, 0.4605462145614466]
printing an ep nov before normalisation:  29.1764681551728
printing an ep nov before normalisation:  14.80427622795105
maxi score, test score, baseline:  -0.9969326409495549 -1.0 -0.9969326409495549
probs:  [0.07170787929241035, 0.0659473624666781, 0.13541227633595398, 0.0659473624666781, 0.20043890487683305, 0.4605462145614466]
actions average: 
K:  2  action  0 :  tensor([0.4015, 0.0033, 0.1206, 0.1018, 0.1060, 0.1273, 0.1396],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0303, 0.7539, 0.0482, 0.0412, 0.0293, 0.0521, 0.0449],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1277, 0.0108, 0.2879, 0.1323, 0.1345, 0.1565, 0.1503],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1464, 0.0167, 0.1727, 0.2495, 0.1446, 0.1353, 0.1349],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2319, 0.0177, 0.1060, 0.1058, 0.3199, 0.1024, 0.1162],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1920, 0.0099, 0.1404, 0.1397, 0.1173, 0.2383, 0.1625],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2700, 0.0020, 0.1535, 0.1208, 0.1655, 0.1285, 0.1597],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  3.4705178373997114
maxi score, test score, baseline:  -0.9969326409495549 -1.0 -0.9969326409495549
probs:  [0.07245988723651843, 0.06593243284027352, 0.135090397188237, 0.0667119855149843, 0.19935860180202306, 0.4604466954179635]
printing an ep nov before normalisation:  34.7375927311406
maxi score, test score, baseline:  -0.9969326409495549 -1.0 -0.9969326409495549
line 256 mcts: sample exp_bonus 19.606710424292185
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.579]
 [0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]] [[45.121]
 [47.159]
 [45.121]
 [45.121]
 [45.121]
 [45.121]
 [45.121]] [[1.214]
 [1.45 ]
 [1.214]
 [1.214]
 [1.214]
 [1.214]
 [1.214]]
maxi score, test score, baseline:  -0.9969414201183432 -1.0 -0.9969414201183432
probs:  [0.07245988867683462, 0.06593242299188036, 0.1350904731100409, 0.06671197701476501, 0.19935861264219099, 0.46044662556428806]
printing an ep nov before normalisation:  83.4015016628657
maxi score, test score, baseline:  -0.9969414201183432 -1.0 -0.9969414201183432
probs:  [0.07239578521932198, 0.06582995498427663, 0.1353944748727918, 0.06661409076075497, 0.20004034419738279, 0.4597253499654718]
printing an ep nov before normalisation:  73.55314983574131
Printing some Q and Qe and total Qs values:  [[0.135]
 [0.135]
 [0.135]
 [0.109]
 [0.135]
 [0.135]
 [0.135]] [[41.361]
 [41.361]
 [41.361]
 [52.553]
 [41.361]
 [41.361]
 [41.361]] [[0.711]
 [0.711]
 [0.711]
 [1.   ]
 [0.711]
 [0.711]
 [0.711]]
printing an ep nov before normalisation:  44.87458636740798
maxi score, test score, baseline:  -0.9969414201183432 -1.0 -0.9969414201183432
probs:  [0.07239578521932198, 0.06582995498427663, 0.1353944748727918, 0.06661409076075497, 0.20004034419738279, 0.4597253499654718]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9969501474926253 -1.0 -0.9969501474926253
probs:  [0.07239578664074088, 0.06582994510654838, 0.1353945508782592, 0.06661408223244718, 0.20004035523666192, 0.45972527990534245]
printing an ep nov before normalisation:  59.87572300184473
printing an ep nov before normalisation:  36.346232891082764
printing an ep nov before normalisation:  31.685332716699694
printing an ep nov before normalisation:  69.3949135861746
Printing some Q and Qe and total Qs values:  [[0.898]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.898]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]]
printing an ep nov before normalisation:  33.69855281656919
actor:  1 policy actor:  1  step number:  69 total reward:  0.17333333333333323  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9969501474926253 -1.0 -0.9969501474926253
printing an ep nov before normalisation:  58.04809019520252
printing an ep nov before normalisation:  52.06752339712452
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9969501474926253 -1.0 -0.9969501474926253
maxi score, test score, baseline:  -0.9969501474926253 -1.0 -0.9969501474926253
probs:  [0.074896632722705, 0.0697305465690319, 0.12373248804113603, 0.07034751545681131, 0.17411662354722998, 0.4871761936630856]
printing an ep nov before normalisation:  50.29730854795808
printing an ep nov before normalisation:  16.91418711932613
maxi score, test score, baseline:  -0.9969501474926253 -1.0 -0.9969501474926253
probs:  [0.07542190714230482, 0.06966076143360014, 0.12413812098711614, 0.07088393462016164, 0.17320383886581903, 0.4866914369509983]
printing an ep nov before normalisation:  70.15750261803882
maxi score, test score, baseline:  -0.9969588235294118 -1.0 -0.9969588235294118
probs:  [0.07537684374478792, 0.06958300881423826, 0.1243694510371657, 0.07081312238760339, 0.17371343545748916, 0.4861441385587156]
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.64133711107638
actor:  1 policy actor:  1  step number:  70 total reward:  0.07333333333333258  reward:  1.0 rdn_beta:  2.0
siam score:  -0.84025025
maxi score, test score, baseline:  -0.9969588235294118 -1.0 -0.9969588235294118
probs:  [0.07664043379558247, 0.071762895065608, 0.11788485004413637, 0.07279846592122892, 0.1594250730546927, 0.5014882821187515]
maxi score, test score, baseline:  -0.9969588235294118 -1.0 -0.9969588235294118
probs:  [0.07664043379558247, 0.071762895065608, 0.11788485004413637, 0.07279846592122892, 0.1594250730546927, 0.5014882821187515]
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.89428115961831
maxi score, test score, baseline:  -0.9969588235294118 -1.0 -0.9969588235294118
probs:  [0.07664043379558247, 0.071762895065608, 0.11788485004413637, 0.07279846592122892, 0.1594250730546927, 0.5014882821187515]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]] [[77.495]
 [77.495]
 [77.495]
 [77.495]
 [77.495]
 [77.495]
 [77.495]] [[2.288]
 [2.288]
 [2.288]
 [2.288]
 [2.288]
 [2.288]
 [2.288]]
printing an ep nov before normalisation:  52.002023651002105
printing an ep nov before normalisation:  30.76854944229126
maxi score, test score, baseline:  -0.9969674486803519 -1.0 -0.9969674486803519
probs:  [0.07660285009812633, 0.0716980478193492, 0.11807778439620524, 0.07273940711420634, 0.15985008631038122, 0.5010318242617317]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9969674486803519 -1.0 -0.9969674486803519
maxi score, test score, baseline:  -0.9969674486803519 -1.0 -0.9969674486803519
probs:  [0.07660285009812633, 0.0716980478193492, 0.11807778439620524, 0.07273940711420634, 0.15985008631038122, 0.5010318242617317]
printing an ep nov before normalisation:  56.503756516542154
maxi score, test score, baseline:  -0.9969674486803519 -1.0 -0.9969674486803519
probs:  [0.07667986537526032, 0.07177012329568051, 0.11819657055462919, 0.072812531380517, 0.1590046123413504, 0.5015362970525626]
maxi score, test score, baseline:  -0.9969674486803519 -1.0 -0.9969674486803519
probs:  [0.07664266172253598, 0.0717056030388864, 0.11839035569506014, 0.07275381082734705, 0.15942544344515006, 0.5010821252710204]
printing an ep nov before normalisation:  54.90386469344416
maxi score, test score, baseline:  -0.9969674486803519 -1.0 -0.9969674486803519
probs:  [0.07664266172253598, 0.0717056030388864, 0.11839035569506014, 0.07275381082734705, 0.15942544344515006, 0.5010821252710204]
siam score:  -0.8469746
maxi score, test score, baseline:  -0.9969674486803519 -1.0 -0.9969674486803519
probs:  [0.07664266172253598, 0.0717056030388864, 0.11839035569506014, 0.07275381082734705, 0.15942544344515006, 0.5010821252710204]
maxi score, test score, baseline:  -0.9969674486803519 -1.0 -0.9969674486803519
printing an ep nov before normalisation:  40.950504671715706
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.434]
 [0.362]
 [0.416]
 [0.38 ]
 [0.323]
 [0.371]] [[44.234]
 [52.644]
 [47.613]
 [51.485]
 [52.231]
 [52.953]
 [52.583]] [[0.629]
 [0.849]
 [0.678]
 [0.809]
 [0.788]
 [0.745]
 [0.786]]
siam score:  -0.84829557
maxi score, test score, baseline:  -0.9969674486803519 -1.0 -0.9969674486803519
probs:  [0.0766054485239414, 0.07164106622721249, 0.11858419055770031, 0.07269507520740971, 0.15984638252757055, 0.5006278369561654]
maxi score, test score, baseline:  -0.9969674486803519 -1.0 -0.9969674486803519
probs:  [0.0766054485239414, 0.07164106622721249, 0.11858419055770031, 0.07269507520740971, 0.15984638252757055, 0.5006278369561654]
maxi score, test score, baseline:  -0.9969674486803519 -1.0 -0.9969674486803519
probs:  [0.07696990424632058, 0.07150535161322459, 0.11881881734084822, 0.07307162466092726, 0.15995339646778245, 0.49968090567089696]
maxi score, test score, baseline:  -0.9969674486803519 -1.0 -0.9969674486803519
Printing some Q and Qe and total Qs values:  [[0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]] [[37.534]
 [37.534]
 [37.534]
 [37.534]
 [37.534]
 [37.534]
 [37.534]] [[0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]]
printing an ep nov before normalisation:  40.7057209270924
printing an ep nov before normalisation:  56.11562648270409
maxi score, test score, baseline:  -0.9969674486803519 -1.0 -0.9969674486803519
printing an ep nov before normalisation:  12.434846115703701
maxi score, test score, baseline:  -0.9969674486803519 -1.0 -0.9969674486803519
probs:  [0.0764350948694406, 0.07154673493173712, 0.11888763417385875, 0.07311391614318768, 0.1600460641444269, 0.49997055573734894]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.013]
 [-0.021]
 [-0.011]
 [-0.024]
 [-0.027]
 [-0.022]
 [-0.053]] [[25.494]
 [28.2  ]
 [24.06 ]
 [28.616]
 [28.202]
 [26.015]
 [30.493]] [[1.001]
 [1.106]
 [0.944]
 [1.12 ]
 [1.1  ]
 [1.014]
 [1.169]]
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.479]
 [0.413]
 [0.467]
 [0.442]
 [0.466]
 [0.466]] [[32.911]
 [42.352]
 [35.796]
 [42.332]
 [67.124]
 [38.501]
 [47.077]] [[0.841]
 [1.175]
 [0.913]
 [1.163]
 [1.885]
 [1.047]
 [1.305]]
maxi score, test score, baseline:  -0.9969760233918129 -1.0 -0.9969760233918129
maxi score, test score, baseline:  -0.9969760233918129 -1.0 -0.9969760233918129
probs:  [0.07687145292650593, 0.07148472331992688, 0.11923710031394408, 0.07355707025137512, 0.159310217554441, 0.499539435633807]
printing an ep nov before normalisation:  35.648422716850696
maxi score, test score, baseline:  -0.9969760233918129 -1.0 -0.9969760233918129
maxi score, test score, baseline:  -0.9969760233918129 -1.0 -0.9969760233918129
maxi score, test score, baseline:  -0.9969760233918129 -1.0 -0.9969760233918129
probs:  [0.0769952924847448, 0.07159986981542392, 0.11880767306545814, 0.07367556108438558, 0.15857623002351973, 0.5003453735264679]
printing an ep nov before normalisation:  52.47376520116815
siam score:  -0.8422739
printing an ep nov before normalisation:  82.43261483612615
maxi score, test score, baseline:  -0.9969760233918129 -1.0 -0.9969760233918129
maxi score, test score, baseline:  -0.9969760233918129 -1.0 -0.9969760233918129
probs:  [0.07646746270015493, 0.07164076827059745, 0.11887558728830032, 0.07371764740235326, 0.15866690273164039, 0.5006316316069537]
maxi score, test score, baseline:  -0.9969760233918129 -1.0 -0.9969760233918129
probs:  [0.07642957120214085, 0.07157636776711081, 0.119070608236515, 0.07366465346314637, 0.1590804641767192, 0.5001783351543678]
maxi score, test score, baseline:  -0.9969760233918129 -1.0 -0.9969760233918129
printing an ep nov before normalisation:  41.32702350616455
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]] [[71.175]
 [71.175]
 [71.175]
 [71.175]
 [71.175]
 [71.175]
 [71.175]] [[2.177]
 [2.177]
 [2.177]
 [2.177]
 [2.177]
 [2.177]
 [2.177]]
maxi score, test score, baseline:  -0.9969760233918129 -1.0 -0.9969760233918129
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]] [[56.974]
 [54.099]
 [54.099]
 [54.099]
 [54.099]
 [54.099]
 [54.099]] [[2.242]
 [2.107]
 [2.107]
 [2.107]
 [2.107]
 [2.107]
 [2.107]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  72.9354192719902
Printing some Q and Qe and total Qs values:  [[0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]] [[53.118]
 [53.118]
 [53.118]
 [53.118]
 [53.118]
 [53.118]
 [53.118]] [[2.53]
 [2.53]
 [2.53]
 [2.53]
 [2.53]
 [2.53]
 [2.53]]
Printing some Q and Qe and total Qs values:  [[0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]] [[56.672]
 [56.672]
 [56.672]
 [56.672]
 [56.672]
 [56.672]
 [56.672]] [[1.104]
 [1.104]
 [1.104]
 [1.104]
 [1.104]
 [1.104]
 [1.104]]
printing an ep nov before normalisation:  69.98503345663212
maxi score, test score, baseline:  -0.9969760233918129 -1.0 -0.9969760233918129
probs:  [0.07591163253751204, 0.07159730217481525, 0.11878457236980976, 0.0736995077337089, 0.1596846736716668, 0.5003223115124872]
maxi score, test score, baseline:  -0.9969760233918129 -1.0 -0.9969760233918129
probs:  [0.07587106585710993, 0.07153325895884857, 0.11897729985376558, 0.07364690372137718, 0.16009996007023627, 0.4998715115386623]
maxi score, test score, baseline:  -0.9969760233918129 -1.0 -0.9969760233918129
maxi score, test score, baseline:  -0.9969760233918129 -1.0 -0.9969760233918129
printing an ep nov before normalisation:  63.49324314885094
printing an ep nov before normalisation:  56.96049815785585
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.556]
 [0.558]
 [0.556]
 [0.557]
 [0.556]
 [0.556]] [[35.217]
 [39.897]
 [35.915]
 [39.897]
 [35.6  ]
 [39.897]
 [39.897]] [[1.148]
 [1.277]
 [1.138]
 [1.277]
 [1.126]
 [1.277]
 [1.277]]
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.471]
 [0.318]
 [0.359]
 [0.336]
 [0.345]
 [0.386]] [[44.939]
 [27.561]
 [22.877]
 [22.558]
 [22.494]
 [22.402]
 [23.437]] [[1.317]
 [1.049]
 [0.797]
 [0.832]
 [0.807]
 [0.814]
 [0.877]]
from probs:  [0.07586630665385442, 0.07147710145649029, 0.11948330269227869, 0.07361579061316771, 0.16008415390911052, 0.4994733446750982]
maxi score, test score, baseline:  -0.9969760233918129 -1.0 -0.9969760233918129
probs:  [0.07594197896886803, 0.07154838808801152, 0.11960255700697382, 0.07368921421785561, 0.15924557757379112, 0.4999722841444998]
maxi score, test score, baseline:  -0.9969760233918129 -1.0 -0.9969760233918129
probs:  [0.07594197896886803, 0.07154838808801152, 0.11960255700697382, 0.07368921421785561, 0.15924557757379112, 0.4999722841444998]
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.288]
 [0.441]
 [0.595]
 [0.584]
 [0.924]
 [0.471]] [[36.776]
 [48.344]
 [39.6  ]
 [34.006]
 [33.899]
 [32.569]
 [41.73 ]] [[1.241]
 [1.452]
 [1.25 ]
 [1.176]
 [1.161]
 [1.448]
 [1.366]]
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.429]
 [0.416]
 [0.434]
 [0.428]
 [0.434]
 [0.434]] [[47.722]
 [46.585]
 [46.947]
 [48.241]
 [45.618]
 [48.241]
 [48.241]] [[2.362]
 [2.26 ]
 [2.275]
 [2.392]
 [2.185]
 [2.392]
 [2.392]]
printing an ep nov before normalisation:  42.48210278229703
maxi score, test score, baseline:  -0.9969760233918129 -1.0 -0.9969760233918129
maxi score, test score, baseline:  -0.9969760233918129 -1.0 -0.9969760233918129
from probs:  [0.07594197896886803, 0.07154838808801152, 0.11960255700697382, 0.07368921421785561, 0.15924557757379112, 0.4999722841444998]
printing an ep nov before normalisation:  40.84214585175298
printing an ep nov before normalisation:  35.203484591875814
maxi score, test score, baseline:  -0.9969845481049563 -1.0 -0.9969845481049563
probs:  [0.0759774317147976, 0.07155585379960493, 0.11991610345241012, 0.07371031692388667, 0.1588183248611356, 0.5000219692481652]
maxi score, test score, baseline:  -0.9969845481049563 -1.0 -0.9969845481049563
probs:  [0.0759774317147976, 0.07155585379960493, 0.11991610345241012, 0.07371031692388667, 0.1588183248611356, 0.5000219692481652]
maxi score, test score, baseline:  -0.9969845481049563 -1.0 -0.9969845481049563
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9969845481049563 -1.0 -0.9969845481049563
probs:  [0.0759774317147976, 0.07155585379960493, 0.11991610345241012, 0.07371031692388667, 0.1588183248611356, 0.5000219692481652]
printing an ep nov before normalisation:  24.748963395552813
Starting evaluation
maxi score, test score, baseline:  -0.9970098265895954 -1.0 -0.9970098265895954
Printing some Q and Qe and total Qs values:  [[0.366]
 [0.436]
 [0.342]
 [0.307]
 [0.346]
 [0.392]
 [0.357]] [[38.419]
 [51.196]
 [40.008]
 [37.227]
 [37.297]
 [36.756]
 [41.248]] [[0.917]
 [1.359]
 [0.939]
 [0.823]
 [0.864]
 [0.894]
 [0.99 ]]
maxi score, test score, baseline:  -0.9970098265895954 -1.0 -0.9970098265895954
probs:  [0.07597743221105017, 0.07155583099248389, 0.1199162679277153, 0.07371030547159421, 0.158818355748827, 0.5000218076483295]
printing an ep nov before normalisation:  19.384038249754227
maxi score, test score, baseline:  -0.9970098265895954 -1.0 -0.9970098265895954
Printing some Q and Qe and total Qs values:  [[0.074]
 [0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]] [[60.924]
 [63.494]
 [63.494]
 [63.494]
 [63.494]
 [63.494]
 [63.494]] [[0.074]
 [0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]]
Printing some Q and Qe and total Qs values:  [[0.187]
 [0.187]
 [0.187]
 [0.187]
 [0.187]
 [0.187]
 [0.187]] [[15.205]
 [15.205]
 [15.205]
 [15.205]
 [15.205]
 [15.205]
 [15.205]] [[0.187]
 [0.187]
 [0.187]
 [0.187]
 [0.187]
 [0.187]
 [0.187]]
printing an ep nov before normalisation:  49.53978875449543
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 16.13617256690957
printing an ep nov before normalisation:  14.513914585113525
printing an ep nov before normalisation:  39.71251336856818
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.003]
 [0.081]
 [0.005]
 [0.005]
 [0.01 ]
 [0.004]] [[23.855]
 [26.93 ]
 [30.215]
 [13.256]
 [12.585]
 [12.259]
 [12.529]] [[0.004]
 [0.003]
 [0.081]
 [0.005]
 [0.005]
 [0.01 ]
 [0.004]]
printing an ep nov before normalisation:  64.98255988837712
printing an ep nov before normalisation:  34.4878713599795
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.77844615041823
line 256 mcts: sample exp_bonus 17.761838076518547
printing an ep nov before normalisation:  49.9716523916646
printing an ep nov before normalisation:  49.23001136686829
printing an ep nov before normalisation:  48.3461856842041
Printing some Q and Qe and total Qs values:  [[0.382]
 [0.381]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]] [[54.87 ]
 [58.214]
 [54.87 ]
 [54.87 ]
 [54.87 ]
 [54.87 ]
 [54.87 ]] [[0.382]
 [0.381]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]]
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.622]
 [0.552]
 [0.63 ]
 [0.516]
 [0.552]
 [0.531]] [[44.857]
 [40.547]
 [44.857]
 [41.372]
 [41.071]
 [44.857]
 [42.234]] [[0.552]
 [0.622]
 [0.552]
 [0.63 ]
 [0.516]
 [0.552]
 [0.531]]
printing an ep nov before normalisation:  48.0100316816295
printing an ep nov before normalisation:  37.496113777160645
maxi score, test score, baseline:  -0.9970264367816092 -1.0 -0.9970264367816092
probs:  [0.07610020473806615, 0.07167143075132545, 0.11947627808017783, 0.07382940024167998, 0.15809179425529293, 0.5008308919334576]
line 256 mcts: sample exp_bonus 46.64977781166711
printing an ep nov before normalisation:  29.264222658171455
printing an ep nov before normalisation:  53.30236267257167
line 256 mcts: sample exp_bonus 63.76486445018728
maxi score, test score, baseline:  -0.997059090909091 -1.0 -0.997059090909091
probs:  [0.07610020595740556, 0.07167140178793928, 0.11947648787933705, 0.0738293859851633, 0.15809183172377253, 0.5008306866663823]
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.644]
 [0.491]
 [0.475]
 [0.474]
 [0.473]
 [0.582]] [[44.313]
 [33.174]
 [44.733]
 [45.744]
 [46.481]
 [47.435]
 [40.308]] [[0.434]
 [0.644]
 [0.491]
 [0.475]
 [0.474]
 [0.473]
 [0.582]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  42.90480440855802
maxi score, test score, baseline:  -0.9971067039106145 -1.0 -0.9971067039106145
probs:  [0.07610020773539665, 0.07167135955953675, 0.1194767937659454, 0.07382936519936657, 0.15809188635128543, 0.5008303873884691]
printing an ep nov before normalisation:  38.33858013153076
printing an ep nov before normalisation:  57.67177341077098
printing an ep nov before normalisation:  35.17091206115538
Printing some Q and Qe and total Qs values:  [[0.208]
 [0.392]
 [0.392]
 [0.392]
 [0.243]
 [0.308]
 [0.392]] [[35.285]
 [28.643]
 [28.643]
 [28.643]
 [36.091]
 [35.818]
 [28.643]] [[1.606]
 [1.273]
 [1.273]
 [1.273]
 [1.703]
 [1.747]
 [1.273]]
maxi score, test score, baseline:  -0.9971144846796658 -1.0 -0.9971144846796658
probs:  [0.07610020802595853, 0.07167135265905482, 0.11947684375054345, 0.07382936180280089, 0.15809189527774675, 0.5008303384838955]
printing an ep nov before normalisation:  32.49989032745361
printing an ep nov before normalisation:  46.16707235913111
printing an ep nov before normalisation:  17.717354511878213
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9971144846796658 -1.0 -0.9971144846796658
probs:  [0.0761479832484901, 0.07171634259846514, 0.11892350644082773, 0.07387570890245819, 0.1581912346277255, 0.5011452241820334]
printing an ep nov before normalisation:  39.9731172778391
using explorer policy with actor:  0
siam score:  -0.8307737
maxi score, test score, baseline:  -0.997129916897507 -1.0 -0.997129916897507
Printing some Q and Qe and total Qs values:  [[0.699]
 [0.732]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]] [[64.51 ]
 [69.392]
 [64.51 ]
 [64.51 ]
 [64.51 ]
 [64.51 ]
 [64.51 ]] [[2.482]
 [2.732]
 [2.482]
 [2.482]
 [2.482]
 [2.482]
 [2.482]]
printing an ep nov before normalisation:  77.32846994209802
maxi score, test score, baseline:  -0.997129916897507 -1.0 -0.997129916897507
probs:  [0.0761092988903311, 0.0716538945841964, 0.11911415483420326, 0.07382483999451044, 0.1585922399894794, 0.5007055717072795]
siam score:  -0.8308632
Printing some Q and Qe and total Qs values:  [[0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]] [[17.663]
 [17.663]
 [17.663]
 [17.663]
 [17.663]
 [17.663]
 [17.663]] [[0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9971451790633609 -1.0 -0.9971451790633609
probs:  [0.07607060290238075, 0.07159142780694822, 0.11930486046750866, 0.07377395579930866, 0.15899336588777002, 0.5002657871360837]
maxi score, test score, baseline:  -0.9971451790633609 -1.0 -0.9971451790633609
maxi score, test score, baseline:  -0.9971451790633609 -1.0 -0.9971451790633609
probs:  [0.07607060290238075, 0.07159142780694822, 0.11930486046750866, 0.07377395579930866, 0.15899336588777002, 0.5002657871360837]
printing an ep nov before normalisation:  55.67259910779967
maxi score, test score, baseline:  -0.9971451790633609 -1.0 -0.9971451790633609
probs:  [0.07595444318805858, 0.07140395557854352, 0.1198770291921897, 0.0736212313882754, 0.160197411474015, 0.49894592917891784]
printing an ep nov before normalisation:  15.052066447210102
printing an ep nov before normalisation:  31.343147724290844
maxi score, test score, baseline:  -0.9971451790633609 -1.0 -0.9971451790633609
probs:  [0.07595444318805858, 0.07140395557854352, 0.1198770291921897, 0.0736212313882754, 0.160197411474015, 0.49894592917891784]
printing an ep nov before normalisation:  20.938250220409795
printing an ep nov before normalisation:  30.73435286631827
printing an ep nov before normalisation:  49.478535652160645
maxi score, test score, baseline:  -0.9971527472527473 -1.0 -0.9971527472527473
probs:  [0.07599193927083378, 0.07141305747273813, 0.12018857321555426, 0.0736441686673516, 0.15975522393602928, 0.4990070374374931]
printing an ep nov before normalisation:  37.596837608540525
maxi score, test score, baseline:  -0.9971527472527473 -1.0 -0.9971527472527473
probs:  [0.07599193927083378, 0.07141305747273813, 0.12018857321555426, 0.0736441686673516, 0.15975522393602928, 0.4990070374374931]
Printing some Q and Qe and total Qs values:  [[0.297]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]] [[47.549]
 [49.418]
 [49.418]
 [49.418]
 [49.418]
 [49.418]
 [49.418]] [[1.868]
 [1.95 ]
 [1.95 ]
 [1.95 ]
 [1.95 ]
 [1.95 ]
 [1.95 ]]
maxi score, test score, baseline:  -0.9971527472527473 -1.0 -0.9971527472527473
probs:  [0.07599193927083378, 0.07141305747273813, 0.12018857321555426, 0.0736441686673516, 0.15975522393602928, 0.4990070374374931]
printing an ep nov before normalisation:  52.27711646972247
maxi score, test score, baseline:  -0.9971527472527473 -1.0 -0.9971527472527473
probs:  [0.07595354224908153, 0.07135082014716052, 0.12038028940217076, 0.07359354779554879, 0.16015294696364366, 0.49856885344239477]
Printing some Q and Qe and total Qs values:  [[0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]] [[49.006]
 [49.006]
 [49.006]
 [49.006]
 [49.006]
 [49.006]
 [49.006]] [[1.721]
 [1.721]
 [1.721]
 [1.721]
 [1.721]
 [1.721]
 [1.721]]
from probs:  [0.07591513330025633, 0.07128856348911673, 0.12057206514061825, 0.07354291119964015, 0.16055079353395393, 0.4981305333364146]
maxi score, test score, baseline:  -0.9971602739726028 -1.0 -0.9971602739726028
probs:  [0.07591513367371319, 0.07128855655228848, 0.1205721152219856, 0.07354290782482982, 0.16055080255999496, 0.49813048416718797]
siam score:  -0.82165277
printing an ep nov before normalisation:  39.34030188701015
maxi score, test score, baseline:  -0.9971602739726028 -1.0 -0.9971602739726028
probs:  [0.07587671278686993, 0.07122628050514955, 0.1207639508347524, 0.07349225547044642, 0.1609487728528566, 0.49769202754992503]
maxi score, test score, baseline:  -0.9971602739726028 -1.0 -0.9971602739726028
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 39.704599367109275
maxi score, test score, baseline:  -0.9971677595628415 -1.0 -0.9971677595628415
probs:  [0.07592626040561339, 0.0712727787480113, 0.1201892885050431, 0.07354023955569639, 0.1610539787055924, 0.49801745408004344]
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.528]
 [0.612]
 [0.525]
 [0.319]
 [0.557]
 [0.504]] [[50.105]
 [53.298]
 [52.434]
 [43.703]
 [42.818]
 [53.648]
 [54.885]] [[1.589]
 [1.672]
 [1.723]
 [1.307]
 [1.069]
 [1.714]
 [1.707]]
maxi score, test score, baseline:  -0.9971677595628415 -1.0 -0.9971677595628415
probs:  [0.07592626040561339, 0.0712727787480113, 0.1201892885050431, 0.07354023955569639, 0.1610539787055924, 0.49801745408004344]
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.547]
 [0.547]
 [0.58 ]
 [0.54 ]
 [0.547]
 [0.547]] [[42.16 ]
 [42.16 ]
 [42.16 ]
 [48.629]
 [50.36 ]
 [42.16 ]
 [42.16 ]] [[1.534]
 [1.534]
 [1.534]
 [1.859]
 [1.897]
 [1.534]
 [1.534]]
maxi score, test score, baseline:  -0.9971677595628415 -1.0 -0.9971677595628415
maxi score, test score, baseline:  -0.997175204359673 -1.0 -0.997175204359673
Printing some Q and Qe and total Qs values:  [[0.712]
 [0.563]
 [0.458]
 [0.506]
 [0.349]
 [0.506]
 [0.389]] [[38.388]
 [50.537]
 [58.234]
 [41.339]
 [39.564]
 [41.339]
 [52.126]] [[1.813]
 [2.014]
 [2.13 ]
 [1.692]
 [1.484]
 [1.692]
 [1.886]]
line 256 mcts: sample exp_bonus 61.93416059892968
printing an ep nov before normalisation:  74.60269800431568
actions average: 
K:  3  action  0 :  tensor([0.3823, 0.0233, 0.1049, 0.1198, 0.1242, 0.1248, 0.1206],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0415, 0.8540, 0.0234, 0.0243, 0.0160, 0.0128, 0.0279],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1993, 0.0107, 0.1449, 0.1549, 0.1677, 0.1542, 0.1683],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2063, 0.0279, 0.1336, 0.1878, 0.1575, 0.1364, 0.1504],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2574, 0.0041, 0.1020, 0.1150, 0.2392, 0.1235, 0.1589],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.2048, 0.0190, 0.1367, 0.1404, 0.1340, 0.2078, 0.1574],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1959, 0.0459, 0.1472, 0.1371, 0.1365, 0.1548, 0.1826],
       grad_fn=<DivBackward0>)
actions average: 
K:  4  action  0 :  tensor([0.2964, 0.0234, 0.1257, 0.1392, 0.1341, 0.1442, 0.1369],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0405, 0.8454, 0.0185, 0.0343, 0.0232, 0.0148, 0.0234],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1891, 0.0199, 0.2181, 0.1255, 0.1395, 0.1468, 0.1611],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1453, 0.0522, 0.1293, 0.1615, 0.1436, 0.1805, 0.1876],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.3076, 0.0253, 0.1068, 0.1177, 0.2271, 0.1065, 0.1090],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1798, 0.0223, 0.1612, 0.1286, 0.1365, 0.1866, 0.1851],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1687, 0.0408, 0.1482, 0.1405, 0.1340, 0.1746, 0.1934],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.997175204359673 -1.0 -0.997175204359673
probs:  [0.0759272453806392, 0.07122115486369292, 0.12069065801403296, 0.07351424992449795, 0.16099576030796744, 0.49765093150916956]
printing an ep nov before normalisation:  41.63380241019864
printing an ep nov before normalisation:  24.800429344177246
maxi score, test score, baseline:  -0.997175204359673 -1.0 -0.997175204359673
probs:  [0.07644769492610815, 0.07119478040042111, 0.12048289986302056, 0.07403871501623893, 0.1603663332006442, 0.49746957659356694]
maxi score, test score, baseline:  -0.9971826086956522 -1.0 -0.9971826086956522
probs:  [0.07644769616407425, 0.07119477346687807, 0.1204829489675421, 0.07403871250676136, 0.1603663414432124, 0.49746952745153183]
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.408]
 [0.436]
 [0.374]
 [0.418]
 [0.435]
 [0.435]] [[63.975]
 [53.197]
 [56.917]
 [64.844]
 [56.291]
 [44.436]
 [44.436]] [[1.601]
 [1.3  ]
 [1.433]
 [1.595]
 [1.397]
 [1.079]
 [1.079]]
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]] [[65.031]
 [65.031]
 [65.031]
 [65.031]
 [65.031]
 [65.031]
 [65.031]] [[1.996]
 [1.996]
 [1.996]
 [1.996]
 [1.996]
 [1.996]
 [1.996]]
printing an ep nov before normalisation:  23.09445858001709
Printing some Q and Qe and total Qs values:  [[0.398]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]] [[70.905]
 [67.236]
 [67.236]
 [67.236]
 [67.236]
 [67.236]
 [67.236]] [[1.638]
 [1.551]
 [1.551]
 [1.551]
 [1.551]
 [1.551]
 [1.551]]
maxi score, test score, baseline:  -0.9971826086956522 -1.0 -0.9971826086956522
probs:  [0.07641253144203096, 0.07113291143165582, 0.1206715878311933, 0.07399130443090352, 0.16075768258918166, 0.4970339822750347]
maxi score, test score, baseline:  -0.9971826086956522 -1.0 -0.9971826086956522
probs:  [0.07645730470293738, 0.07117458574186819, 0.12074233959684931, 0.07344816099093596, 0.16085196347832156, 0.4973256454890877]
maxi score, test score, baseline:  -0.9971826086956522 -1.0 -0.9971826086956522
probs:  [0.07650708528220491, 0.07122092079231901, 0.1201692990006179, 0.07349597892720662, 0.16095678848280465, 0.49764992751484693]
UNIT TEST: sample policy line 217 mcts : [0.184 0.122 0.061 0.102 0.061 0.102 0.367]
maxi score, test score, baseline:  -0.9971826086956522 -1.0 -0.9971826086956522
using explorer policy with actor:  1
printing an ep nov before normalisation:  68.85725309627357
printing an ep nov before normalisation:  58.434027716939774
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.495]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]] [[54.399]
 [53.556]
 [53.563]
 [53.563]
 [53.563]
 [53.563]
 [53.563]] [[2.045]
 [1.955]
 [1.942]
 [1.942]
 [1.942]
 [1.942]
 [1.942]]
maxi score, test score, baseline:  -0.9971826086956522 -1.0 -0.9971826086956522
probs:  [0.07658385381769292, 0.07129237584602087, 0.12028995538291412, 0.07356972079585442, 0.16011407892635807, 0.49815001523115954]
printing an ep nov before normalisation:  33.675026569959776
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.293]
 [0.273]
 [0.273]
 [0.273]
 [0.3  ]
 [0.273]] [[25.724]
 [30.646]
 [26.562]
 [26.562]
 [26.562]
 [23.849]
 [26.562]] [[1.443]
 [1.859]
 [1.502]
 [1.502]
 [1.502]
 [1.306]
 [1.502]]
printing an ep nov before normalisation:  25.285852819756748
maxi score, test score, baseline:  -0.997189972899729 -1.0 -0.997189972899729
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.466]
 [0.464]
 [0.473]
 [0.481]
 [0.484]
 [0.461]] [[50.186]
 [49.474]
 [50.62 ]
 [50.012]
 [50.13 ]
 [50.224]
 [49.887]] [[1.886]
 [1.836]
 [1.901]
 [1.875]
 [1.89 ]
 [1.898]
 [1.855]]
using another actor
using explorer policy with actor:  0
printing an ep nov before normalisation:  31.276821560412063
maxi score, test score, baseline:  -0.9972045822102426 -1.0 -0.9972045822102426
probs:  [0.07646630362637251, 0.07113577688330183, 0.1210278319090996, 0.07402220838271421, 0.16030097964942536, 0.4970468995490866]
using another actor
from probs:  [0.07646630362637251, 0.07113577688330183, 0.1210278319090996, 0.07402220838271421, 0.16030097964942536, 0.4970468995490866]
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.556]
 [0.531]
 [0.893]
 [0.547]
 [0.531]
 [0.533]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.531]
 [0.556]
 [0.531]
 [0.893]
 [0.547]
 [0.531]
 [0.533]]
printing an ep nov before normalisation:  42.484573726678384
maxi score, test score, baseline:  -0.9972118279569893 -1.0 -0.9972118279569893
probs:  [0.07654209285793102, 0.07120626550645164, 0.1211479119820529, 0.0740955672368589, 0.15946794654857963, 0.49754021586812586]
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.516]
 [0.465]
 [0.409]
 [0.431]
 [0.417]
 [0.404]] [[48.433]
 [49.985]
 [49.719]
 [48.557]
 [49.598]
 [48.09 ]
 [46.934]] [[1.237]
 [1.391]
 [1.331]
 [1.238]
 [1.293]
 [1.23 ]
 [1.18 ]]
printing an ep nov before normalisation:  34.49159853127945
maxi score, test score, baseline:  -0.9972118279569893 -1.0 -0.9972118279569893
probs:  [0.07654209285793102, 0.07120626550645164, 0.1211479119820529, 0.0740955672368589, 0.15946794654857963, 0.49754021586812586]
Printing some Q and Qe and total Qs values:  [[0.21 ]
 [0.223]
 [0.237]
 [0.231]
 [0.223]
 [0.223]
 [0.223]] [[57.685]
 [50.156]
 [50.258]
 [49.728]
 [50.156]
 [50.156]
 [50.156]] [[1.065]
 [0.903]
 [0.919]
 [0.901]
 [0.903]
 [0.903]
 [0.903]]
printing an ep nov before normalisation:  32.980164222565605
using explorer policy with actor:  1
printing an ep nov before normalisation:  27.273281636156526
maxi score, test score, baseline:  -0.9972190348525469 -1.0 -0.9972190348525469
probs:  [0.0765081071687028, 0.07114569863230367, 0.1213361159719061, 0.07404939383133836, 0.15984694625008505, 0.497113738145664]
maxi score, test score, baseline:  -0.9972190348525469 -1.0 -0.9972190348525469
probs:  [0.0765081071687028, 0.07114569863230367, 0.1213361159719061, 0.07404939383133836, 0.15984694625008505, 0.497113738145664]
printing an ep nov before normalisation:  52.471385546594654
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.381]
 [0.561]
 [0.538]
 [0.289]
 [0.243]
 [0.39 ]] [[44.795]
 [47.651]
 [33.125]
 [46.952]
 [47.679]
 [48.597]
 [47.332]] [[1.565]
 [1.625]
 [1.391]
 [1.762]
 [1.533]
 [1.513]
 [1.625]]
maxi score, test score, baseline:  -0.9972190348525469 -1.0 -0.9972190348525469
probs:  [0.0765081071687028, 0.07114569863230367, 0.1213361159719061, 0.07404939383133836, 0.15984694625008505, 0.497113738145664]
maxi score, test score, baseline:  -0.9972190348525469 -1.0 -0.9972190348525469
probs:  [0.07602425176191317, 0.07127949561736331, 0.12090607733008608, 0.07359219122241119, 0.16014786823710603, 0.4980501158311203]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9972190348525469 -1.0 -0.9972190348525469
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]] [[69.43]
 [69.43]
 [69.43]
 [69.43]
 [69.43]
 [69.43]
 [69.43]] [[2.377]
 [2.377]
 [2.377]
 [2.377]
 [2.377]
 [2.377]
 [2.377]]
maxi score, test score, baseline:  -0.9972190348525469 -1.0 -0.9972190348525469
probs:  [0.0764211566427614, 0.07113363489447366, 0.12115255804741082, 0.07399724730182727, 0.16026282770952538, 0.49703257540400153]
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]] [[50.771]
 [46.789]
 [46.789]
 [46.789]
 [46.789]
 [46.789]
 [46.789]] [[0.309]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]]
maxi score, test score, baseline:  -0.9972190348525469 -1.0 -0.9972190348525469
maxi score, test score, baseline:  -0.9972190348525469 -1.0 -0.9972190348525469
probs:  [0.07654617667102062, 0.07124998977584347, 0.12069967153819124, 0.07411829504751524, 0.15953897702245967, 0.4978468899449697]
printing an ep nov before normalisation:  31.164688127600197
printing an ep nov before normalisation:  37.407259941101074
printing an ep nov before normalisation:  41.51970863342285
siam score:  -0.8045215
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.537]
 [0.477]
 [0.477]
 [0.493]
 [0.486]
 [0.477]] [[44.083]
 [48.258]
 [44.083]
 [44.083]
 [41.623]
 [42.429]
 [44.083]] [[0.477]
 [0.537]
 [0.477]
 [0.477]
 [0.493]
 [0.486]
 [0.477]]
printing an ep nov before normalisation:  45.40174285286902
printing an ep nov before normalisation:  43.12048340993568
printing an ep nov before normalisation:  72.56537402204545
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  57.33074188232422
actions average: 
K:  1  action  0 :  tensor([0.4207, 0.0088, 0.1005, 0.1106, 0.1419, 0.1170, 0.1006],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0157, 0.9178, 0.0132, 0.0165, 0.0103, 0.0082, 0.0183],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1760, 0.0036, 0.2781, 0.1238, 0.1376, 0.1489, 0.1321],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1719, 0.0274, 0.1043, 0.1869, 0.1884, 0.1506, 0.1705],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1844, 0.0273, 0.1269, 0.1576, 0.1804, 0.1624, 0.1611],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2284, 0.0078, 0.1453, 0.1450, 0.1482, 0.1918, 0.1335],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2373, 0.0254, 0.1110, 0.1275, 0.1555, 0.1376, 0.2056],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9972333333333333 -1.0 -0.9972333333333333
printing an ep nov before normalisation:  67.97861713469106
printing an ep nov before normalisation:  69.10500792992391
maxi score, test score, baseline:  -0.9972333333333333 -1.0 -0.9972333333333333
maxi score, test score, baseline:  -0.9972333333333333 -1.0 -0.9972333333333333
probs:  [0.07634311098373542, 0.07088922233811791, 0.12181129942293142, 0.07384293561579336, 0.1618068969027007, 0.49530653473672115]
printing an ep nov before normalisation:  50.41145324707031
printing an ep nov before normalisation:  25.343639850616455
Printing some Q and Qe and total Qs values:  [[0.214]
 [0.311]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]] [[30.173]
 [43.178]
 [30.173]
 [30.173]
 [30.173]
 [30.173]
 [30.173]] [[1.135]
 [2.006]
 [1.135]
 [1.135]
 [1.135]
 [1.135]
 [1.135]]
printing an ep nov before normalisation:  54.84304428100586
maxi score, test score, baseline:  -0.9972333333333333 -1.0 -0.9972333333333333
probs:  [0.07634311098373542, 0.07088922233811791, 0.12181129942293142, 0.07384293561579336, 0.1618068969027007, 0.49530653473672115]
printing an ep nov before normalisation:  57.34321700017276
maxi score, test score, baseline:  -0.9972333333333333 -1.0 -0.9972333333333333
probs:  [0.07634311098373542, 0.07088922233811791, 0.12181129942293142, 0.07384293561579336, 0.1618068969027007, 0.49530653473672115]
maxi score, test score, baseline:  -0.9972333333333333 -1.0 -0.9972333333333333
probs:  [0.07634311098373542, 0.07088922233811791, 0.12181129942293142, 0.07384293561579336, 0.1618068969027007, 0.49530653473672115]
printing an ep nov before normalisation:  54.41437614958467
printing an ep nov before normalisation:  67.1780580985887
printing an ep nov before normalisation:  67.18660446255203
Printing some Q and Qe and total Qs values:  [[-0.035]
 [-0.035]
 [-0.041]
 [-0.035]
 [-0.047]
 [-0.049]
 [-0.04 ]] [[52.46 ]
 [56.707]
 [52.447]
 [51.464]
 [30.356]
 [30.278]
 [48.438]] [[0.189]
 [0.215]
 [0.184]
 [0.184]
 [0.046]
 [0.044]
 [0.161]]
printing an ep nov before normalisation:  48.09711446680666
line 256 mcts: sample exp_bonus 57.185798265906016
from probs:  [0.07576808176725612, 0.0709216747162397, 0.12148795657580648, 0.07389352809657995, 0.16239771625676175, 0.49553104258735603]
printing an ep nov before normalisation:  36.85259056365402
printing an ep nov before normalisation:  35.6031703278328
maxi score, test score, baseline:  -0.9972333333333333 -1.0 -0.9972333333333333
probs:  [0.07573153721979403, 0.07086182187315494, 0.12167129705794502, 0.0738479680762827, 0.16277780801514652, 0.4951095677576768]
Printing some Q and Qe and total Qs values:  [[0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.289]] [[25.356]
 [25.356]
 [25.356]
 [25.356]
 [25.356]
 [25.356]
 [22.464]] [[0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.289]]
siam score:  -0.796896
printing an ep nov before normalisation:  20.451719589000522
printing an ep nov before normalisation:  21.83823122638642
maxi score, test score, baseline:  -0.9972333333333333 -1.0 -0.9972333333333333
probs:  [0.075781930150909, 0.07090896871016418, 0.12108625766860859, 0.07389710544269638, 0.1628862249510422, 0.49543951307657963]
printing an ep nov before normalisation:  44.993765371815684
maxi score, test score, baseline:  -0.9972333333333333 -1.0 -0.9972333333333333
probs:  [0.075781930150909, 0.07090896871016418, 0.12108625766860859, 0.07389710544269638, 0.1628862249510422, 0.49543951307657963]
line 256 mcts: sample exp_bonus 48.32138407295656
maxi score, test score, baseline:  -0.9972333333333333 -1.0 -0.9972333333333333
probs:  [0.075781930150909, 0.07090896871016418, 0.12108625766860859, 0.07389710544269638, 0.1628862249510422, 0.49543951307657963]
maxi score, test score, baseline:  -0.9972333333333333 -1.0 -0.9972333333333333
probs:  [0.075781930150909, 0.07090896871016418, 0.12108625766860859, 0.07389710544269638, 0.1628862249510422, 0.49543951307657963]
maxi score, test score, baseline:  -0.9972333333333333 -1.0 -0.9972333333333333
probs:  [0.075781930150909, 0.07090896871016418, 0.12108625766860859, 0.07389710544269638, 0.1628862249510422, 0.49543951307657963]
printing an ep nov before normalisation:  33.403925098103805
maxi score, test score, baseline:  -0.9972333333333333 -1.0 -0.9972333333333333
printing an ep nov before normalisation:  50.60728188479043
printing an ep nov before normalisation:  22.419536113739014
printing an ep nov before normalisation:  59.64537312532002
siam score:  -0.7966024
maxi score, test score, baseline:  -0.9972333333333333 -1.0 -0.9972333333333333
printing an ep nov before normalisation:  17.415557903985643
printing an ep nov before normalisation:  29.22439804947956
printing an ep nov before normalisation:  15.194253187937807
printing an ep nov before normalisation:  29.66915996749094
maxi score, test score, baseline:  -0.9972404255319149 -1.0 -0.9972404255319149
probs:  [0.07615623699750469, 0.07070020521755017, 0.12152019920234962, 0.07426893040695437, 0.16337510406266173, 0.49397932411297957]
printing an ep nov before normalisation:  60.79328410235683
maxi score, test score, baseline:  -0.9972474801061008 -1.0 -0.9972474801061008
probs:  [0.07615623821508727, 0.07070019828032657, 0.1215202475997819, 0.07426892880369206, 0.16337511216517636, 0.49397927493593574]
siam score:  -0.7961958
maxi score, test score, baseline:  -0.9972474801061008 -1.0 -0.9972474801061008
probs:  [0.07615623821508727, 0.07070019828032657, 0.1215202475997819, 0.07426892880369206, 0.16337511216517636, 0.49397927493593574]
maxi score, test score, baseline:  -0.9972474801061008 -1.0 -0.9972474801061008
probs:  [0.07615623821508727, 0.07070019828032657, 0.1215202475997819, 0.07426892880369206, 0.16337511216517636, 0.49397927493593574]
Printing some Q and Qe and total Qs values:  [[0.357]
 [0.429]
 [0.348]
 [0.403]
 [0.35 ]
 [0.359]
 [0.353]] [[42.299]
 [44.658]
 [41.436]
 [45.823]
 [41.52 ]
 [41.592]
 [40.981]] [[1.039]
 [1.183]
 [1.003]
 [1.193]
 [1.008]
 [1.019]
 [0.994]]
maxi score, test score, baseline:  -0.9972474801061008 -1.0 -0.9972474801061008
maxi score, test score, baseline:  -0.9972474801061008 -1.0 -0.9972474801061008
probs:  [0.07615623821508727, 0.07070019828032657, 0.1215202475997819, 0.07426892880369206, 0.16337511216517636, 0.49397927493593574]
maxi score, test score, baseline:  -0.9972474801061008 -1.0 -0.9972474801061008
probs:  [0.07615623821508727, 0.07070019828032657, 0.1215202475997819, 0.07426892880369206, 0.16337511216517636, 0.49397927493593574]
actions average: 
K:  3  action  0 :  tensor([0.4406, 0.0523, 0.0997, 0.0889, 0.1141, 0.1023, 0.1020],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0291, 0.8694, 0.0205, 0.0074, 0.0062, 0.0018, 0.0655],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.2286, 0.0785, 0.1519, 0.1187, 0.1208, 0.1379, 0.1636],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1373, 0.0399, 0.1444, 0.1946, 0.1511, 0.1669, 0.1659],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2227, 0.0273, 0.1381, 0.1269, 0.1954, 0.1462, 0.1433],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.2058, 0.0097, 0.1699, 0.1368, 0.1330, 0.1979, 0.1469],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1855, 0.0325, 0.1771, 0.1311, 0.1361, 0.1555, 0.1823],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  25.18920456057166
siam score:  -0.79599273
maxi score, test score, baseline:  -0.9972474801061008 -1.0 -0.9972474801061008
probs:  [0.07620655301671297, 0.07074690207573592, 0.12093927853217729, 0.07431799451511716, 0.1634831515985465, 0.49430612026171006]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.219]
 [0.219]
 [0.219]
 [0.219]
 [0.219]
 [0.219]] [[38.879]
 [41.486]
 [41.486]
 [41.486]
 [41.486]
 [41.486]
 [41.486]] [[1.208]
 [1.068]
 [1.068]
 [1.068]
 [1.068]
 [1.068]
 [1.068]]
maxi score, test score, baseline:  -0.9972474801061008 -1.0 -0.9972474801061008
maxi score, test score, baseline:  -0.9972474801061008 -1.0 -0.9972474801061008
probs:  [0.07620655301671297, 0.07074690207573592, 0.12093927853217729, 0.07431799451511716, 0.1634831515985465, 0.49430612026171006]
printing an ep nov before normalisation:  49.200895962208776
from probs:  [0.07625647822287587, 0.07079324423627444, 0.12036280799990345, 0.07436668030298231, 0.16359035446550516, 0.4946304347724588]
maxi score, test score, baseline:  -0.9972544973544973 -1.0 -0.9972544973544973
probs:  [0.07625647957066682, 0.07079323745211588, 0.12036285464461771, 0.07436667883783472, 0.16359036282565262, 0.49463038666911224]
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.612]
 [0.527]
 [0.529]
 [0.561]
 [0.54 ]
 [0.551]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.558]
 [0.612]
 [0.527]
 [0.529]
 [0.561]
 [0.54 ]
 [0.551]]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9972544973544973 -1.0 -0.9972544973544973
probs:  [0.07635290069539125, 0.07088273858944871, 0.11986447374864709, 0.07384574306350092, 0.16379740496219916, 0.4952567389408128]
printing an ep nov before normalisation:  47.26916162407732
maxi score, test score, baseline:  -0.9972544973544973 -1.0 -0.9972544973544973
probs:  [0.07635290069539125, 0.07088273858944871, 0.11986447374864709, 0.07384574306350092, 0.16379740496219916, 0.4952567389408128]
maxi score, test score, baseline:  -0.9972614775725593 -1.0 -0.9972614775725593
probs:  [0.07635290217373446, 0.07088273195884792, 0.11986451954694691, 0.0738457408252448, 0.16379741358060068, 0.4952566919146252]
printing an ep nov before normalisation:  39.43883482783918
printing an ep nov before normalisation:  58.553108277475815
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.483]] [[46.633]
 [44.015]
 [44.015]
 [44.015]
 [44.015]
 [44.015]
 [44.015]] [[1.459]
 [1.342]
 [1.342]
 [1.342]
 [1.342]
 [1.342]
 [1.342]]
maxi score, test score, baseline:  -0.9972614775725593 -1.0 -0.9972614775725593
probs:  [0.07636890806610185, 0.07086910098433277, 0.11946793923049864, 0.07384816315362434, 0.16428718559767136, 0.495158702967771]
using another actor
maxi score, test score, baseline:  -0.9972614775725593 -1.0 -0.9972614775725593
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.328]
 [0.321]
 [0.32 ]
 [0.315]
 [0.315]
 [0.315]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.315]
 [0.328]
 [0.321]
 [0.32 ]
 [0.315]
 [0.315]
 [0.315]]
printing an ep nov before normalisation:  33.78588888380263
maxi score, test score, baseline:  -0.9972614775725593 -1.0 -0.9972614775725593
probs:  [0.07638146531458678, 0.07082366106246835, 0.11993498912121088, 0.07383413836569916, 0.16419024615390035, 0.4948354999821346]
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.59 ]
 [0.539]
 [0.561]
 [0.546]
 [0.539]
 [0.539]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.539]
 [0.59 ]
 [0.539]
 [0.561]
 [0.546]
 [0.539]
 [0.539]]
printing an ep nov before normalisation:  34.063801762696656
printing an ep nov before normalisation:  61.22504234313965
maxi score, test score, baseline:  -0.9972684210526316 -1.0 -0.9972684210526316
maxi score, test score, baseline:  -0.9972684210526316 -1.0 -0.9972684210526316
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.481]
 [0.467]
 [0.47 ]
 [0.471]
 [0.458]
 [0.456]] [[55.739]
 [42.188]
 [54.581]
 [54.745]
 [54.29 ]
 [45.179]
 [45.154]] [[1.298]
 [0.998]
 [1.265]
 [1.272]
 [1.262]
 [1.043]
 [1.041]]
Printing some Q and Qe and total Qs values:  [[0.08 ]
 [0.113]
 [0.102]
 [0.067]
 [0.07 ]
 [0.057]
 [0.057]] [[51.944]
 [46.935]
 [50.381]
 [50.243]
 [50.741]
 [46.207]
 [46.207]] [[1.023]
 [0.893]
 [0.994]
 [0.955]
 [0.974]
 [0.813]
 [0.813]]
printing an ep nov before normalisation:  47.649466946824894
maxi score, test score, baseline:  -0.9972684210526316 -1.0 -0.9972684210526316
probs:  [0.07647711500178399, 0.070883696167393, 0.11965623477947483, 0.07391346470268813, 0.16381646764986607, 0.495253021698794]
printing an ep nov before normalisation:  35.73585186280951
printing an ep nov before normalisation:  48.47574659039701
Printing some Q and Qe and total Qs values:  [[0.252]
 [0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]] [[40.857]
 [40.044]
 [40.044]
 [40.044]
 [40.044]
 [40.044]
 [40.044]] [[1.944]
 [1.919]
 [1.919]
 [1.919]
 [1.919]
 [1.919]
 [1.919]]
maxi score, test score, baseline:  -0.9972684210526316 -1.0 -0.9972684210526316
probs:  [0.07652663209071031, 0.07092958533240593, 0.1190856707631257, 0.07396131899315414, 0.1639226335497318, 0.49557415927087217]
printing an ep nov before normalisation:  22.66054512321034
maxi score, test score, baseline:  -0.9972684210526316 -1.0 -0.9972684210526316
probs:  [0.07652663209071031, 0.07092958533240593, 0.1190856707631257, 0.07396131899315414, 0.1639226335497318, 0.49557415927087217]
maxi score, test score, baseline:  -0.9972684210526316 -1.0 -0.9972684210526316
probs:  [0.07652663209071031, 0.07092958533240593, 0.1190856707631257, 0.07396131899315414, 0.1639226335497318, 0.49557415927087217]
maxi score, test score, baseline:  -0.9972684210526316 -1.0 -0.9972684210526316
probs:  [0.07652663209071031, 0.07092958533240593, 0.1190856707631257, 0.07396131899315414, 0.1639226335497318, 0.49557415927087217]
Sims:  50 1 epoch:  53201 pick best:  False frame count:  53201
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.24217960132449
printing an ep nov before normalisation:  57.92737960894186
printing an ep nov before normalisation:  47.45290877328667
Printing some Q and Qe and total Qs values:  [[0.643]
 [0.729]
 [0.643]
 [0.637]
 [0.683]
 [0.643]
 [0.643]] [[64.003]
 [67.393]
 [64.003]
 [72.472]
 [70.404]
 [64.003]
 [64.003]] [[2.27 ]
 [2.49 ]
 [2.27 ]
 [2.599]
 [2.563]
 [2.27 ]
 [2.27 ]]
Printing some Q and Qe and total Qs values:  [[0.361]
 [0.35 ]
 [0.567]
 [0.584]
 [0.367]
 [0.272]
 [0.476]] [[38.179]
 [40.195]
 [38.613]
 [39.796]
 [36.841]
 [35.345]
 [28.811]] [[1.762]
 [1.826]
 [1.984]
 [2.044]
 [1.719]
 [1.568]
 [1.53 ]]
maxi score, test score, baseline:  -0.9972684210526316 -1.0 -0.9972684210526316
probs:  [0.07659280094393411, 0.07096219291977537, 0.11812031998653615, 0.07401210559952803, 0.16451285027431212, 0.4957997302759143]
Printing some Q and Qe and total Qs values:  [[0.545]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]] [[46.166]
 [42.574]
 [42.574]
 [42.574]
 [42.574]
 [42.574]
 [42.574]] [[2.196]
 [1.964]
 [1.964]
 [1.964]
 [1.964]
 [1.964]
 [1.964]]
maxi score, test score, baseline:  -0.9972684210526316 -1.0 -0.9972684210526316
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9972684210526316 -1.0 -0.9972684210526316
probs:  [0.07705652713034579, 0.07082447415593483, 0.11867426598475524, 0.07447022514596525, 0.16413767612827707, 0.49483683145472185]
printing an ep nov before normalisation:  59.83943155472888
maxi score, test score, baseline:  -0.9972684210526316 -1.0 -0.9972684210526316
probs:  [0.07702714331108952, 0.07076607007430186, 0.11883867989251436, 0.07442879791782267, 0.1645137955681172, 0.4944255132361545]
actions average: 
K:  1  action  0 :  tensor([0.3821, 0.0162, 0.1172, 0.1213, 0.1296, 0.1333, 0.1002],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0111, 0.8699, 0.0275, 0.0233, 0.0089, 0.0124, 0.0469],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1564, 0.0424, 0.1830, 0.1744, 0.1591, 0.1617, 0.1230],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2259, 0.0147, 0.1372, 0.1864, 0.1355, 0.1579, 0.1425],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2592, 0.0018, 0.1468, 0.1492, 0.1488, 0.1654, 0.1287],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1966, 0.0014, 0.1463, 0.1194, 0.1294, 0.2841, 0.1227],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1278, 0.0444, 0.1301, 0.1964, 0.1201, 0.1329, 0.2483],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  47.53561319738249
maxi score, test score, baseline:  -0.9972684210526316 -1.0 -0.9972684210526316
probs:  [0.07710599866592004, 0.07083850464613939, 0.11896041332796128, 0.07450498864771109, 0.1636576806004027, 0.49493241411186556]
maxi score, test score, baseline:  -0.9972684210526316 -1.0 -0.9972684210526316
printing an ep nov before normalisation:  37.7467911422076
printing an ep nov before normalisation:  46.241164965769784
siam score:  -0.7733614
printing an ep nov before normalisation:  45.79640273068858
maxi score, test score, baseline:  -0.9972684210526316 -1.0 -0.9972684210526316
probs:  [0.07707693932090848, 0.07078035421072293, 0.11912562470054985, 0.07446385650018149, 0.16403035802544705, 0.49452286724219013]
maxi score, test score, baseline:  -0.9972684210526316 -1.0 -0.9972684210526316
probs:  [0.07707693932090848, 0.07078035421072293, 0.11912562470054985, 0.07446385650018149, 0.16403035802544705, 0.49452286724219013]
maxi score, test score, baseline:  -0.9972684210526316 -1.0 -0.9972684210526316
probs:  [0.07707693932090848, 0.07078035421072293, 0.11912562470054985, 0.07446385650018149, 0.16403035802544705, 0.49452286724219013]
printing an ep nov before normalisation:  29.620779943769104
printing an ep nov before normalisation:  54.78399414253989
maxi score, test score, baseline:  -0.9972753280839896 -1.0 -0.9972753280839896
printing an ep nov before normalisation:  43.739009540870164
printing an ep nov before normalisation:  54.32335522287856
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9972753280839896 -1.0 -0.9972753280839896
probs:  [0.07704787126924728, 0.07072217430012152, 0.1192909456104941, 0.07442270702706008, 0.16440318935993067, 0.4941131124331464]
printing an ep nov before normalisation:  16.726874148852666
printing an ep nov before normalisation:  15.75163273396707
maxi score, test score, baseline:  -0.9972753280839896 -1.0 -0.9972753280839896
probs:  [0.07709761708267868, 0.07076782886482876, 0.11872176335377309, 0.07447075497227094, 0.16450943366442636, 0.49443260206202216]
printing an ep nov before normalisation:  17.40989327430725
maxi score, test score, baseline:  -0.9972753280839896 -1.0 -0.9972753280839896
maxi score, test score, baseline:  -0.9972753280839896 -1.0 -0.9972753280839896
probs:  [0.07709761708267868, 0.07076782886482876, 0.11872176335377309, 0.07447075497227094, 0.16450943366442636, 0.49443260206202216]
maxi score, test score, baseline:  -0.9972753280839896 -1.0 -0.9972753280839896
probs:  [0.07709761708267868, 0.07076782886482876, 0.11872176335377309, 0.07447075497227094, 0.16450943366442636, 0.49443260206202216]
maxi score, test score, baseline:  -0.9972753280839896 -1.0 -0.9972753280839896
probs:  [0.07714699198291705, 0.07081314302138497, 0.11815682501616158, 0.07451844466388124, 0.16461488579343034, 0.4947497095222248]
printing an ep nov before normalisation:  18.346340656280518
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9972821989528796 -1.0 -0.9972821989528796
probs:  [0.07714699480667779, 0.07081313652421564, 0.11815686800064994, 0.07451844361945599, 0.16461489363867096, 0.4947496634103296]
siam score:  -0.76979536
line 256 mcts: sample exp_bonus 45.32419600761889
actions average: 
K:  1  action  0 :  tensor([0.3696, 0.0308, 0.1066, 0.1110, 0.1308, 0.1247, 0.1265],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0094, 0.9150, 0.0065, 0.0109, 0.0064, 0.0052, 0.0466],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2896, 0.0155, 0.1627, 0.1289, 0.1429, 0.1258, 0.1345],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2031, 0.0638, 0.1255, 0.2229, 0.1545, 0.1116, 0.1185],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2167, 0.0409, 0.0986, 0.1330, 0.2589, 0.1331, 0.1187],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2668, 0.0319, 0.1020, 0.1399, 0.1397, 0.1542, 0.1656],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1759, 0.0404, 0.1297, 0.1470, 0.1698, 0.1653, 0.1719],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9972890339425587 -1.0 -0.9972890339425587
maxi score, test score, baseline:  -0.9972890339425587 -1.0 -0.9972890339425587
probs:  [0.07714699761566529, 0.07081313006105548, 0.11815691076016144, 0.07451844258050222, 0.1646149014428113, 0.49474961753980434]
maxi score, test score, baseline:  -0.9972890339425587 -1.0 -0.9972890339425587
probs:  [0.07714699761566529, 0.07081313006105548, 0.11815691076016144, 0.07451844258050222, 0.1646149014428113, 0.49474961753980434]
printing an ep nov before normalisation:  60.7223592306187
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.517]
 [0.529]
 [0.551]
 [0.59 ]
 [0.544]
 [0.553]] [[38.746]
 [40.111]
 [38.676]
 [38.94 ]
 [40.189]
 [40.178]
 [40.505]] [[2.335]
 [2.426]
 [2.319]
 [2.363]
 [2.505]
 [2.459]
 [2.494]]
maxi score, test score, baseline:  -0.9972890339425587 -1.0 -0.9972890339425587
probs:  [0.07714699761566529, 0.07081313006105548, 0.11815691076016144, 0.07451844258050222, 0.1646149014428113, 0.49474961753980434]
maxi score, test score, baseline:  -0.9972890339425587 -1.0 -0.9972890339425587
maxi score, test score, baseline:  -0.9972890339425587 -1.0 -0.9972890339425587
probs:  [0.07714699761566529, 0.07081313006105548, 0.11815691076016144, 0.07451844258050222, 0.1646149014428113, 0.49474961753980434]
maxi score, test score, baseline:  -0.9972890339425587 -1.0 -0.9972890339425587
probs:  [0.07714699761566529, 0.07081313006105548, 0.11815691076016144, 0.07451844258050222, 0.1646149014428113, 0.49474961753980434]
Printing some Q and Qe and total Qs values:  [[0.307]
 [0.342]
 [0.308]
 [0.308]
 [0.308]
 [0.308]
 [0.308]] [[47.687]
 [47.989]
 [46.485]
 [46.485]
 [46.485]
 [46.485]
 [46.485]] [[1.312]
 [1.356]
 [1.274]
 [1.274]
 [1.274]
 [1.274]
 [1.274]]
maxi score, test score, baseline:  -0.9972890339425587 -1.0 -0.9972890339425587
maxi score, test score, baseline:  -0.9972890339425587 -1.0 -0.9972890339425587
actions average: 
K:  0  action  0 :  tensor([0.3561, 0.0324, 0.1194, 0.1279, 0.1241, 0.1299, 0.1102],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0102, 0.9088, 0.0084, 0.0158, 0.0044, 0.0036, 0.0489],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1273, 0.0035, 0.3460, 0.1317, 0.1303, 0.1349, 0.1263],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1928, 0.0419, 0.1494, 0.1595, 0.1334, 0.1460, 0.1770],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2733, 0.0032, 0.1010, 0.1131, 0.2815, 0.0936, 0.1342],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1483, 0.0177, 0.1538, 0.1327, 0.1357, 0.2951, 0.1167],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2228, 0.0321, 0.1171, 0.1294, 0.1231, 0.1372, 0.2382],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9972890339425587 -1.0 -0.9972890339425587
probs:  [0.07714699761566529, 0.07081313006105548, 0.11815691076016144, 0.07451844258050222, 0.1646149014428113, 0.49474961753980434]
printing an ep nov before normalisation:  28.538330092016295
maxi score, test score, baseline:  -0.9972890339425587 -1.0 -0.9972890339425587
probs:  [0.07711833460892406, 0.07075527770742736, 0.11831724011397333, 0.07447766599480293, 0.16498933038017344, 0.4943421511946989]
printing an ep nov before normalisation:  24.922953820474003
actor:  1 policy actor:  1  step number:  75 total reward:  0.0399999999999997  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  4  action  0 :  tensor([0.4459, 0.0242, 0.1068, 0.1106, 0.1063, 0.1002, 0.1060],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0645, 0.7543, 0.0519, 0.0361, 0.0267, 0.0303, 0.0361],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1853, 0.0831, 0.1386, 0.1451, 0.1326, 0.1762, 0.1392],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.3600, 0.0228, 0.1411, 0.1407, 0.1082, 0.1051, 0.1220],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2971, 0.0174, 0.1299, 0.1477, 0.1318, 0.1429, 0.1332],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1677, 0.0664, 0.1458, 0.1369, 0.0978, 0.1973, 0.1882],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1598, 0.0472, 0.1547, 0.1532, 0.1358, 0.1587, 0.1906],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9972890339425587 -1.0 -0.9972890339425587
actions average: 
K:  4  action  0 :  tensor([0.4760, 0.0291, 0.0844, 0.0966, 0.1337, 0.0766, 0.1036],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0509, 0.7866, 0.0286, 0.0482, 0.0306, 0.0252, 0.0299],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1708, 0.0452, 0.1660, 0.1699, 0.1490, 0.1409, 0.1583],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1702, 0.0985, 0.1221, 0.1729, 0.1476, 0.1321, 0.1565],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2028, 0.0173, 0.1479, 0.1656, 0.1495, 0.1392, 0.1779],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1945, 0.0044, 0.1307, 0.1721, 0.1360, 0.2166, 0.1457],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1431, 0.0260, 0.2133, 0.0877, 0.0755, 0.0970, 0.3574],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  63.23611696913579
maxi score, test score, baseline:  -0.9972958333333334 -1.0 -0.9972958333333334
probs:  [0.06962546369110117, 0.06388170754896931, 0.10681456240944723, 0.16542194161292098, 0.14801501046347332, 0.44624131427408786]
line 256 mcts: sample exp_bonus 46.03073414476709
Printing some Q and Qe and total Qs values:  [[0.361]
 [0.418]
 [0.366]
 [0.357]
 [0.37 ]
 [0.364]
 [0.402]] [[39.312]
 [38.722]
 [40.472]
 [39.455]
 [39.493]
 [39.926]
 [38.791]] [[1.97 ]
 [1.977]
 [2.072]
 [1.979]
 [1.994]
 [2.025]
 [1.968]]
maxi score, test score, baseline:  -0.9972958333333334 -1.0 -0.9972958333333334
probs:  [0.06962546369110117, 0.06388170754896931, 0.10681456240944723, 0.16542194161292098, 0.14801501046347332, 0.44624131427408786]
line 256 mcts: sample exp_bonus 39.126166719891664
Printing some Q and Qe and total Qs values:  [[0.526]
 [0.594]
 [0.526]
 [0.597]
 [0.523]
 [0.526]
 [0.532]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.526]
 [0.594]
 [0.526]
 [0.597]
 [0.523]
 [0.526]
 [0.532]]
actions average: 
K:  3  action  0 :  tensor([0.4015, 0.0106, 0.1117, 0.1297, 0.1408, 0.1016, 0.1041],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0447, 0.8177, 0.0244, 0.0464, 0.0263, 0.0156, 0.0250],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1040, 0.0495, 0.3172, 0.1289, 0.1361, 0.1417, 0.1227],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1319, 0.1356, 0.1143, 0.1999, 0.1740, 0.1141, 0.1301],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2148, 0.0143, 0.1235, 0.1291, 0.2697, 0.1141, 0.1344],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1792, 0.0201, 0.1513, 0.1574, 0.1353, 0.2398, 0.1169],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2368, 0.0633, 0.1384, 0.1378, 0.1214, 0.1143, 0.1880],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9972958333333334 -1.0 -0.9972958333333334
probs:  [0.0699708956682634, 0.0636849097042621, 0.10719367220379063, 0.16585412525606158, 0.14843143068493553, 0.44486496648268675]
line 256 mcts: sample exp_bonus 53.64289848267363
maxi score, test score, baseline:  -0.9972958333333334 -1.0 -0.9972958333333334
probs:  [0.0699708956682634, 0.0636849097042621, 0.10719367220379063, 0.16585412525606158, 0.14843143068493553, 0.44486496648268675]
printing an ep nov before normalisation:  53.79069084468352
Printing some Q and Qe and total Qs values:  [[0.619]
 [0.619]
 [0.619]
 [0.619]
 [0.619]
 [0.619]
 [0.619]] [[47.869]
 [47.869]
 [47.869]
 [47.869]
 [47.869]
 [47.869]
 [47.869]] [[1.324]
 [1.324]
 [1.324]
 [1.324]
 [1.324]
 [1.324]
 [1.324]]
siam score:  -0.75298166
printing an ep nov before normalisation:  43.07862525989734
maxi score, test score, baseline:  -0.9972958333333334 -1.0 -0.9972958333333334
probs:  [0.0699708956682634, 0.0636849097042621, 0.10719367220379063, 0.16585412525606158, 0.14843143068493553, 0.44486496648268675]
maxi score, test score, baseline:  -0.9972958333333334 -1.0 -0.9972958333333334
printing an ep nov before normalisation:  53.904573369814855
maxi score, test score, baseline:  -0.9973025974025974 -1.0 -0.9973025974025974
probs:  [0.07000847859051118, 0.06365743089735151, 0.10761650314425862, 0.1656982088107934, 0.14835139881668824, 0.4446679797403972]
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.521]
 [0.558]
 [0.593]
 [0.621]
 [0.557]
 [0.52 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.525]
 [0.521]
 [0.558]
 [0.593]
 [0.621]
 [0.557]
 [0.52 ]]
maxi score, test score, baseline:  -0.9973025974025974 -1.0 -0.9973025974025974
maxi score, test score, baseline:  -0.9973025974025974 -1.0 -0.9973025974025974
probs:  [0.07013668621175813, 0.06377398557001164, 0.10781371424300248, 0.16600198864632942, 0.14679000998444047, 0.4454836153444579]
printing an ep nov before normalisation:  54.20220412759499
maxi score, test score, baseline:  -0.9973025974025974 -1.0 -0.9973025974025974
probs:  [0.07013668621175813, 0.06377398557001164, 0.10781371424300248, 0.16600198864632942, 0.14679000998444047, 0.4454836153444579]
Printing some Q and Qe and total Qs values:  [[0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]] [[52.366]
 [52.366]
 [52.366]
 [52.366]
 [52.366]
 [52.366]
 [52.366]] [[2.382]
 [2.382]
 [2.382]
 [2.382]
 [2.382]
 [2.382]
 [2.382]]
maxi score, test score, baseline:  -0.9973025974025974 -1.0 -0.9973025974025974
probs:  [0.07013668621175813, 0.06377398557001164, 0.10781371424300248, 0.16600198864632942, 0.14679000998444047, 0.4454836153444579]
printing an ep nov before normalisation:  50.223914338231225
maxi score, test score, baseline:  -0.9973025974025974 -1.0 -0.9973025974025974
probs:  [0.07016520681532876, 0.06376896039163457, 0.10804087775591907, 0.16535619547919556, 0.14722266647772655, 0.4454460930801955]
actions average: 
K:  2  action  0 :  tensor([0.4857, 0.0048, 0.0861, 0.0965, 0.1695, 0.0823, 0.0751],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0381, 0.8136, 0.0324, 0.0391, 0.0246, 0.0240, 0.0281],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1385, 0.0101, 0.3520, 0.1225, 0.0974, 0.1731, 0.1064],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1795, 0.0466, 0.1399, 0.2010, 0.1445, 0.1489, 0.1395],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2846, 0.0172, 0.1064, 0.0957, 0.2939, 0.1116, 0.0905],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2709, 0.0059, 0.1692, 0.1424, 0.1441, 0.1473, 0.1202],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1816, 0.0648, 0.1470, 0.1747, 0.1252, 0.1549, 0.1517],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  45.67677713834147
printing an ep nov before normalisation:  44.14050102233887
maxi score, test score, baseline:  -0.9973025974025974 -1.0 -0.9973025974025974
probs:  [0.0702288417471348, 0.06382678334004933, 0.108138928617922, 0.16550632621001643, 0.14644839109612262, 0.4458507289887547]
printing an ep nov before normalisation:  44.24052786640902
Printing some Q and Qe and total Qs values:  [[0.257]
 [0.271]
 [0.258]
 [0.238]
 [0.245]
 [0.229]
 [0.212]] [[28.728]
 [30.64 ]
 [22.898]
 [25.212]
 [26.044]
 [25.386]
 [23.15 ]] [[1.448]
 [1.587]
 [1.066]
 [1.199]
 [1.26 ]
 [1.201]
 [1.037]]
maxi score, test score, baseline:  -0.9973025974025974 -1.0 -0.9973025974025974
probs:  [0.0702288417471348, 0.06382678334004933, 0.108138928617922, 0.16550632621001643, 0.14644839109612262, 0.4458507289887547]
maxi score, test score, baseline:  -0.9973025974025974 -1.0 -0.9973025974025974
probs:  [0.0702288417471348, 0.06382678334004933, 0.108138928617922, 0.16550632621001643, 0.14644839109612262, 0.4458507289887547]
printing an ep nov before normalisation:  36.64489269256592
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9973025974025974 -1.0 -0.9973025974025974
maxi score, test score, baseline:  -0.9973025974025974 -1.0 -0.9973025974025974
maxi score, test score, baseline:  -0.9973025974025974 -1.0 -0.9973025974025974
printing an ep nov before normalisation:  31.37436866760254
maxi score, test score, baseline:  -0.9973025974025974 -1.0 -0.9973025974025974
probs:  [0.07012530320042226, 0.06378163258852536, 0.10818858335318117, 0.16515879221797877, 0.14721004860427858, 0.4455356400356139]
maxi score, test score, baseline:  -0.9973025974025974 -1.0 -0.9973025974025974
probs:  [0.07012530320042226, 0.06378163258852536, 0.10818858335318117, 0.16515879221797877, 0.14721004860427858, 0.4455356400356139]
printing an ep nov before normalisation:  44.3866321165669
maxi score, test score, baseline:  -0.9973025974025974 -1.0 -0.9973025974025974
Printing some Q and Qe and total Qs values:  [[0.351]
 [0.359]
 [0.344]
 [0.35 ]
 [0.352]
 [0.353]
 [0.352]] [[25.721]
 [29.82 ]
 [26.422]
 [26.844]
 [27.065]
 [26.742]
 [25.776]] [[0.351]
 [0.359]
 [0.344]
 [0.35 ]
 [0.352]
 [0.353]
 [0.352]]
printing an ep nov before normalisation:  33.95124009228958
maxi score, test score, baseline:  -0.9973093264248705 -1.0 -0.9973093264248705
probs:  [0.07026972408189489, 0.06391295512936648, 0.10841157857904474, 0.16434086193780617, 0.1466102645155662, 0.4464546157563216]
printing an ep nov before normalisation:  23.265867550796017
actor:  1 policy actor:  1  step number:  67 total reward:  0.15999999999999914  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  52.197357377502165
siam score:  -0.7481424
maxi score, test score, baseline:  -0.9973160206718347 -1.0 -0.9973160206718347
probs:  [0.06257631621277093, 0.05688951585323082, 0.096698212099468, 0.25566156802833806, 0.13087088519520537, 0.39730350261098685]
printing an ep nov before normalisation:  45.570683648448856
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.75023743572714
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  56.180445095088984
printing an ep nov before normalisation:  32.622798883315255
maxi score, test score, baseline:  -0.9973160206718347 -1.0 -0.9973160206718347
maxi score, test score, baseline:  -0.9973160206718347 -1.0 -0.9973160206718347
probs:  [0.06211031474621782, 0.05696626951689672, 0.09682880055978893, 0.25600712861156133, 0.13024687565775733, 0.3978406109077779]
printing an ep nov before normalisation:  39.13458062097138
printing an ep nov before normalisation:  27.945248967258497
maxi score, test score, baseline:  -0.9973160206718347 -1.0 -0.9973160206718347
probs:  [0.06214285916131217, 0.05699611277340771, 0.09635489156482362, 0.25614148901382316, 0.13031519885164183, 0.3980494486349916]
maxi score, test score, baseline:  -0.9973160206718347 -1.0 -0.9973160206718347
probs:  [0.06214285916131217, 0.05699611277340771, 0.09635489156482362, 0.25614148901382316, 0.13031519885164183, 0.3980494486349916]
maxi score, test score, baseline:  -0.9973160206718347 -1.0 -0.9973160206718347
probs:  [0.06206626097850129, 0.05690093961680055, 0.09640176704042075, 0.25676504568505626, 0.1304846394917959, 0.3973813471874254]
maxi score, test score, baseline:  -0.9973160206718347 -1.0 -0.9973160206718347
probs:  [0.06206626097850129, 0.05690093961680055, 0.09640176704042075, 0.25676504568505626, 0.1304846394917959, 0.3973813471874254]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.9973226804123712 -1.0 -0.9973226804123712
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.553]
 [0.553]
 [0.516]
 [0.546]
 [0.553]
 [0.553]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [49.126]
 [49.153]
 [ 0.   ]
 [ 0.   ]] [[0.06 ]
 [0.06 ]
 [0.06 ]
 [1.503]
 [1.534]
 [0.06 ]
 [0.06 ]]
maxi score, test score, baseline:  -0.9973293059125964 -1.0 -0.9973293059125964
probs:  [0.06209403347329346, 0.05690140895694234, 0.09661099962278762, 0.25613716049970986, 0.13087386809482346, 0.3973825293524433]
maxi score, test score, baseline:  -0.9973293059125964 -1.0 -0.9973293059125964
probs:  [0.06209403347329346, 0.05690140895694234, 0.09661099962278762, 0.25613716049970986, 0.13087386809482346, 0.3973825293524433]
printing an ep nov before normalisation:  49.72363964433899
printing an ep nov before normalisation:  23.006508126843315
printing an ep nov before normalisation:  19.46716070175171
maxi score, test score, baseline:  -0.9973293059125964 -1.0 -0.9973293059125964
probs:  [0.06209403347329346, 0.05690140895694234, 0.09661099962278762, 0.25613716049970986, 0.13087386809482346, 0.3973825293524433]
maxi score, test score, baseline:  -0.9973293059125964 -1.0 -0.9973293059125964
probs:  [0.06219741413626458, 0.056996125804939665, 0.09677197131843093, 0.2548971481076356, 0.1310920068650315, 0.39804533376769785]
maxi score, test score, baseline:  -0.9973293059125964 -1.0 -0.9973293059125964
probs:  [0.06219741413626458, 0.056996125804939665, 0.09677197131843093, 0.2548971481076356, 0.1310920068650315, 0.39804533376769785]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9973293059125964 -1.0 -0.9973293059125964
probs:  [0.06144859536237273, 0.05674428637995969, 0.09696982402815814, 0.25688271010130914, 0.1316778701839346, 0.3962767139442657]
printing an ep nov before normalisation:  28.268206119537354
maxi score, test score, baseline:  -0.9973293059125964 -1.0 -0.9973293059125964
probs:  [0.06144859536237273, 0.05674428637995969, 0.09696982402815814, 0.25688271010130914, 0.1316778701839346, 0.3962767139442657]
maxi score, test score, baseline:  -0.9973293059125964 -1.0 -0.9973293059125964
maxi score, test score, baseline:  -0.9973293059125964 -1.0 -0.9973293059125964
probs:  [0.06144859536237273, 0.05674428637995969, 0.09696982402815814, 0.25688271010130914, 0.1316778701839346, 0.3962767139442657]
printing an ep nov before normalisation:  26.140627785317164
maxi score, test score, baseline:  -0.9973293059125964 -1.0 -0.9973293059125964
probs:  [0.06144859536237273, 0.05674428637995969, 0.09696982402815814, 0.25688271010130914, 0.1316778701839346, 0.3962767139442657]
actions average: 
K:  1  action  0 :  tensor([0.3492, 0.0339, 0.1333, 0.1252, 0.1160, 0.1308, 0.1115],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0126, 0.9175, 0.0132, 0.0135, 0.0113, 0.0114, 0.0206],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1191, 0.0440, 0.3050, 0.1218, 0.1066, 0.1696, 0.1339],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1592, 0.0773, 0.1365, 0.1866, 0.1405, 0.1519, 0.1480],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1794, 0.0186, 0.1445, 0.1390, 0.2298, 0.1600, 0.1287],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1806, 0.0141, 0.1796, 0.1387, 0.1381, 0.1953, 0.1537],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1440, 0.1074, 0.1651, 0.1244, 0.1320, 0.1271, 0.2000],
       grad_fn=<DivBackward0>)
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9973293059125964 -1.0 -0.9973293059125964
probs:  [0.061551009327209714, 0.0568388428215423, 0.0971315684666755, 0.25564260719854254, 0.13189758685191277, 0.3969383853341171]
Printing some Q and Qe and total Qs values:  [[0.149]
 [0.149]
 [0.149]
 [0.149]
 [0.149]
 [0.149]
 [0.149]] [[18.425]
 [18.425]
 [18.425]
 [18.425]
 [18.425]
 [18.425]
 [18.425]] [[0.149]
 [0.149]
 [0.149]
 [0.149]
 [0.149]
 [0.149]
 [0.149]]
printing an ep nov before normalisation:  28.63616901196412
printing an ep nov before normalisation:  35.75573297522965
maxi score, test score, baseline:  -0.9973293059125964 -1.0 -0.9973293059125964
probs:  [0.061551009327209714, 0.0568388428215423, 0.0971315684666755, 0.25564260719854254, 0.13189758685191277, 0.3969383853341171]
siam score:  -0.7426685
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.462]
 [0.464]
 [0.466]
 [0.464]
 [0.464]
 [0.464]] [[48.668]
 [54.747]
 [48.668]
 [51.891]
 [48.668]
 [48.668]
 [48.668]] [[1.301]
 [1.496]
 [1.301]
 [1.407]
 [1.301]
 [1.301]
 [1.301]]
Printing some Q and Qe and total Qs values:  [[0.335]
 [0.335]
 [0.493]
 [0.413]
 [0.399]
 [0.412]
 [0.335]] [[49.153]
 [49.153]
 [56.833]
 [50.536]
 [49.656]
 [52.652]
 [49.153]] [[1.091]
 [1.091]
 [1.479]
 [1.21 ]
 [1.17 ]
 [1.273]
 [1.091]]
maxi score, test score, baseline:  -0.9973358974358975 -1.0 -0.9973358974358975
maxi score, test score, baseline:  -0.9973358974358975 -1.0 -0.9973358974358975
probs:  [0.06134500745030147, 0.0565581901786711, 0.09748922189088857, 0.25683562228756196, 0.132805925706383, 0.39496603248619405]
Printing some Q and Qe and total Qs values:  [[0.783]
 [0.783]
 [0.783]
 [0.783]
 [0.783]
 [0.783]
 [0.783]] [[53.231]
 [53.231]
 [53.231]
 [53.231]
 [53.231]
 [53.231]
 [53.231]] [[2.491]
 [2.491]
 [2.491]
 [2.491]
 [2.491]
 [2.491]
 [2.491]]
maxi score, test score, baseline:  -0.9973358974358975 -1.0 -0.9973358974358975
probs:  [0.06134500745030147, 0.0565581901786711, 0.09748922189088857, 0.25683562228756196, 0.132805925706383, 0.39496603248619405]
printing an ep nov before normalisation:  57.8662334700942
maxi score, test score, baseline:  -0.9973358974358975 -1.0 -0.9973358974358975
probs:  [0.06134500745030147, 0.0565581901786711, 0.09748922189088857, 0.25683562228756196, 0.132805925706383, 0.39496603248619405]
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.633]
 [0.52 ]
 [0.528]
 [0.499]
 [0.498]
 [0.591]] [[41.42 ]
 [40.68 ]
 [38.483]
 [38.048]
 [38.299]
 [38.215]
 [38.937]] [[2.032]
 [2.087]
 [1.879]
 [1.868]
 [1.851]
 [1.846]
 [1.97 ]]
maxi score, test score, baseline:  -0.9973358974358975 -1.0 -0.9973358974358975
probs:  [0.06126836185613917, 0.05646488422879379, 0.0975383750133786, 0.2574393751705757, 0.13297799742276528, 0.39431100630834753]
using another actor
printing an ep nov before normalisation:  39.0570190344321
maxi score, test score, baseline:  -0.9973358974358975 -1.0 -0.9973358974358975
probs:  [0.06122511668161543, 0.05640237068191975, 0.09709562070531798, 0.25818303823673144, 0.13322240383452896, 0.3938714498598865]
printing an ep nov before normalisation:  46.9793856603558
maxi score, test score, baseline:  -0.9973358974358975 -1.0 -0.9973358974358975
probs:  [0.06122511668161543, 0.05640237068191975, 0.09709562070531798, 0.25818303823673144, 0.13322240383452896, 0.3938714498598865]
printing an ep nov before normalisation:  40.1546384337867
maxi score, test score, baseline:  -0.9973358974358975 -1.0 -0.9973358974358975
probs:  [0.061181863837641376, 0.05633985400796024, 0.09665316736449739, 0.25892650812610635, 0.13346673476527907, 0.3934318718985156]
maxi score, test score, baseline:  -0.9973358974358975 -1.0 -0.9973358974358975
probs:  [0.06123287395586304, 0.05638681831114601, 0.0967338160704543, 0.25914274679265237, 0.13274324630019213, 0.3937604985696923]
maxi score, test score, baseline:  -0.9973424552429667 -1.0 -0.9973424552429667
probs:  [0.06115677450980267, 0.056294050624916506, 0.09677980715012842, 0.25974711489170765, 0.13291301059070382, 0.3931092422327409]
using explorer policy with actor:  1
printing an ep nov before normalisation:  60.25767717302102
Printing some Q and Qe and total Qs values:  [[0.351]
 [0.554]
 [0.461]
 [0.48 ]
 [0.454]
 [0.442]
 [0.468]] [[69.35 ]
 [46.332]
 [40.743]
 [44.487]
 [49.51 ]
 [37.832]
 [42.919]] [[1.489]
 [1.131]
 [0.903]
 [1.013]
 [1.109]
 [0.813]
 [0.962]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.640900054403346
printing an ep nov before normalisation:  72.38233603582738
printing an ep nov before normalisation:  72.34822455361855
printing an ep nov before normalisation:  50.08866761477009
Printing some Q and Qe and total Qs values:  [[0.57]
 [0.57]
 [0.57]
 [0.57]
 [0.57]
 [0.57]
 [0.57]] [[50.994]
 [50.994]
 [50.994]
 [50.994]
 [50.994]
 [50.994]
 [50.994]] [[2.082]
 [2.082]
 [2.082]
 [2.082]
 [2.082]
 [2.082]
 [2.082]]
printing an ep nov before normalisation:  58.68552801763149
maxi score, test score, baseline:  -0.9973424552429667 -1.0 -0.9973424552429667
probs:  [0.061146679231511146, 0.05626203808027589, 0.09584973104053603, 0.26063210588348695, 0.13322633497917183, 0.39288311078501814]
maxi score, test score, baseline:  -0.9973489795918368 -1.0 -0.9973489795918368
probs:  [0.06117450112497398, 0.05626486537166667, 0.09605511109719143, 0.25998187541820034, 0.13362288684080023, 0.39290076014716735]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9973489795918368 -1.0 -0.9973489795918368
probs:  [0.05419150295745395, 0.04988540380671501, 0.0847842745041908, 0.34826595208028327, 0.11773388122720083, 0.3451389854241562]
using another actor
from probs:  [0.05419150295745395, 0.04988540380671501, 0.0847842745041908, 0.34826595208028327, 0.11773388122720083, 0.3451389854241562]
maxi score, test score, baseline:  -0.9973489795918368 -1.0 -0.9973489795918368
probs:  [0.05414597447143575, 0.049814787779543845, 0.08491698097939734, 0.3477691375786954, 0.11805855369893939, 0.34529456549198817]
printing an ep nov before normalisation:  42.18976524391245
using another actor
maxi score, test score, baseline:  -0.9973619289340102 -1.0 -0.9973619289340102
maxi score, test score, baseline:  -0.9973619289340102 -1.0 -0.9973619289340102
probs:  [0.05414596770070346, 0.04981476650011202, 0.08491704823008597, 0.3477689874304937, 0.11805858583641606, 0.3452946443021888]
actions average: 
K:  1  action  0 :  tensor([0.4046, 0.0043, 0.1313, 0.1007, 0.1472, 0.0886, 0.1233],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0542, 0.7105, 0.0696, 0.0324, 0.0324, 0.0242, 0.0768],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1693, 0.0130, 0.1678, 0.1540, 0.1533, 0.1569, 0.1858],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1196, 0.0167, 0.1470, 0.2642, 0.1440, 0.1281, 0.1805],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1616, 0.0031, 0.1308, 0.1170, 0.3759, 0.0958, 0.1158],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1176, 0.0258, 0.1994, 0.0958, 0.1189, 0.2770, 0.1655],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1680, 0.0285, 0.1865, 0.1466, 0.1510, 0.1547, 0.1647],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  29.51123934421787
maxi score, test score, baseline:  -0.9973619289340102 -1.0 -0.9973619289340102
probs:  [0.05414596770070346, 0.04981476650011202, 0.08491704823008597, 0.3477689874304937, 0.11805858583641606, 0.3452946443021888]
printing an ep nov before normalisation:  36.93071613752459
using explorer policy with actor:  0
printing an ep nov before normalisation:  40.58237179289926
printing an ep nov before normalisation:  36.2677268999596
printing an ep nov before normalisation:  28.584580421447754
maxi score, test score, baseline:  -0.9973619289340102 -1.0 -0.9973619289340102
probs:  [0.05414596770070346, 0.04981476650011202, 0.08491704823008597, 0.3477689874304937, 0.11805858583641606, 0.3452946443021888]
actor:  1 policy actor:  1  step number:  58 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  28.361393982363722
printing an ep nov before normalisation:  39.43367127968305
maxi score, test score, baseline:  -0.9973683544303797 -1.0 -0.9973683544303797
probs:  [0.049653003649292614, 0.04568207969276411, 0.07786447615616134, 0.3188514104501451, 0.19136603441101796, 0.3165829956406189]
maxi score, test score, baseline:  -0.9973683544303797 -1.0 -0.9973683544303797
probs:  [0.049653003649292614, 0.04568207969276411, 0.07786447615616134, 0.3188514104501451, 0.19136603441101796, 0.3165829956406189]
maxi score, test score, baseline:  -0.9973683544303797 -1.0 -0.9973683544303797
probs:  [0.049653003649292614, 0.04568207969276411, 0.07786447615616134, 0.3188514104501451, 0.19136603441101796, 0.3165829956406189]
printing an ep nov before normalisation:  51.31731986999512
Printing some Q and Qe and total Qs values:  [[0.065]
 [0.065]
 [0.065]
 [0.065]
 [0.065]
 [0.065]
 [0.039]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.065]
 [0.065]
 [0.065]
 [0.065]
 [0.065]
 [0.065]
 [0.039]]
maxi score, test score, baseline:  -0.9973747474747475 -1.0 -0.9973747474747475
probs:  [0.04927934529016953, 0.04576528195799879, 0.07800651167686203, 0.3194335996234285, 0.1917153803841012, 0.3157998810674399]
printing an ep nov before normalisation:  29.520805109784224
maxi score, test score, baseline:  -0.9973747474747475 -1.0 -0.9973747474747475
probs:  [0.04927934529016953, 0.04576528195799879, 0.07800651167686203, 0.3194335996234285, 0.1917153803841012, 0.3157998810674399]
printing an ep nov before normalisation:  30.045556985520925
printing an ep nov before normalisation:  34.752566539330935
from probs:  [0.04930072159072125, 0.045785129802668724, 0.07760577666414019, 0.31957248059591997, 0.1917987098070178, 0.3159371815395321]
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.286]
 [0.329]
 [0.328]
 [0.329]
 [0.331]
 [0.334]] [[23.378]
 [32.494]
 [21.643]
 [18.713]
 [18.503]
 [18.254]
 [18.301]] [[0.332]
 [0.286]
 [0.329]
 [0.328]
 [0.329]
 [0.331]
 [0.334]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  54.623868536887116
Printing some Q and Qe and total Qs values:  [[0.671]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]] [[45.278]
 [38.766]
 [38.766]
 [38.766]
 [38.766]
 [38.766]
 [38.766]] [[1.833]
 [1.507]
 [1.507]
 [1.507]
 [1.507]
 [1.507]
 [1.507]]
maxi score, test score, baseline:  -0.9974000000000001 -1.0 -0.9974000000000001
probs:  [0.049421523009016886, 0.04589726952716244, 0.07779626393592305, 0.32035715166156, 0.1911688329310149, 0.31535895893532284]
printing an ep nov before normalisation:  39.4597763256173
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.663]
 [0.53 ]
 [0.53 ]
 [0.53 ]
 [0.53 ]
 [0.53 ]] [[34.075]
 [51.912]
 [34.075]
 [34.075]
 [34.075]
 [34.075]
 [34.075]] [[1.177]
 [1.971]
 [1.177]
 [1.177]
 [1.177]
 [1.177]
 [1.177]]
printing an ep nov before normalisation:  49.335440155832764
siam score:  -0.73764575
maxi score, test score, baseline:  -0.9974000000000001 -1.0 -0.9974000000000001
probs:  [0.049421523009016886, 0.04589726952716244, 0.07779626393592305, 0.32035715166156, 0.1911688329310149, 0.31535895893532284]
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.52457739264332
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9974000000000001 -1.0 -0.9974000000000001
probs:  [0.04935357051302115, 0.0458105797725666, 0.0778791702987456, 0.31974807811453454, 0.19185450273135932, 0.31535409856977276]
maxi score, test score, baseline:  -0.9974000000000001 -1.0 -0.9974000000000001
probs:  [0.04935357051302115, 0.0458105797725666, 0.0778791702987456, 0.31974807811453454, 0.19185450273135932, 0.31535409856977276]
maxi score, test score, baseline:  -0.9974000000000001 -1.0 -0.9974000000000001
probs:  [0.04935357051302115, 0.0458105797725666, 0.0778791702987456, 0.31974807811453454, 0.19185450273135932, 0.31535409856977276]
maxi score, test score, baseline:  -0.9974000000000001 -1.0 -0.9974000000000001
probs:  [0.04935357051302115, 0.0458105797725666, 0.0778791702987456, 0.31974807811453454, 0.19185450273135932, 0.31535409856977276]
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.577]
 [0.516]
 [0.506]
 [0.602]
 [0.533]
 [0.533]] [[34.702]
 [38.489]
 [36.64 ]
 [31.795]
 [48.254]
 [36.506]
 [36.506]] [[0.97 ]
 [1.16 ]
 [1.05 ]
 [0.911]
 [1.446]
 [1.063]
 [1.063]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.65214674912341
maxi score, test score, baseline:  -0.9974000000000001 -1.0 -0.9974000000000001
probs:  [0.04948543793996246, 0.045932955086350605, 0.07808746135783298, 0.3206043660808679, 0.19236814789149925, 0.31352163164348684]
maxi score, test score, baseline:  -0.9974000000000001 -1.0 -0.9974000000000001
probs:  [0.04948543793996246, 0.045932955086350605, 0.07808746135783298, 0.3206043660808679, 0.19236814789149925, 0.31352163164348684]
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.473]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]] [[60.345]
 [59.661]
 [57.688]
 [57.688]
 [57.688]
 [57.688]
 [57.688]] [[2.195]
 [2.177]
 [2.102]
 [2.102]
 [2.102]
 [2.102]
 [2.102]]
maxi score, test score, baseline:  -0.9974000000000001 -1.0 -0.9974000000000001
probs:  [0.04948543793996246, 0.045932955086350605, 0.07808746135783298, 0.3206043660808679, 0.19236814789149925, 0.31352163164348684]
maxi score, test score, baseline:  -0.9974000000000001 -1.0 -0.9974000000000001
probs:  [0.04948543793996246, 0.045932955086350605, 0.07808746135783298, 0.3206043660808679, 0.19236814789149925, 0.31352163164348684]
printing an ep nov before normalisation:  50.69565644843067
printing an ep nov before normalisation:  18.974761962890625
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9974000000000001 -1.0 -0.9974000000000001
probs:  [0.049550860177459154, 0.045993668084456464, 0.07819079898602616, 0.32102918876830805, 0.1926229781891335, 0.3126125057946166]
printing an ep nov before normalisation:  40.29518400691174
actor:  1 policy actor:  1  step number:  50 total reward:  0.24666666666666626  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  2  action  0 :  tensor([0.3479, 0.0181, 0.1117, 0.1187, 0.1202, 0.1236, 0.1598],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0610, 0.7907, 0.0292, 0.0305, 0.0167, 0.0176, 0.0543],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1592, 0.0910, 0.1952, 0.1333, 0.1411, 0.1485, 0.1317],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1118, 0.0457, 0.1054, 0.2935, 0.0904, 0.1253, 0.2280],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1268, 0.0115, 0.0786, 0.1068, 0.4729, 0.0899, 0.1135],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1900, 0.0160, 0.1900, 0.1293, 0.1311, 0.2001, 0.1435],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1564, 0.2236, 0.1261, 0.0928, 0.1032, 0.1146, 0.1832],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9974000000000001 -1.0 -0.9974000000000001
probs:  [0.04535068671761538, 0.04209583253084027, 0.07155641930186823, 0.29375514487740845, 0.26118809203388915, 0.2860538245383784]
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]] [[55.849]
 [55.969]
 [55.969]
 [55.969]
 [55.969]
 [55.969]
 [55.969]] [[1.74 ]
 [1.668]
 [1.668]
 [1.668]
 [1.668]
 [1.668]
 [1.668]]
printing an ep nov before normalisation:  63.42474504509713
printing an ep nov before normalisation:  43.818146290862394
maxi score, test score, baseline:  -0.9974000000000001 -1.0 -0.9974000000000001
probs:  [0.04535068671761538, 0.04209583253084027, 0.07155641930186823, 0.29375514487740845, 0.26118809203388915, 0.2860538245383784]
siam score:  -0.7362712
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.629]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]] [[36.05]
 [49.39]
 [36.05]
 [36.05]
 [36.05]
 [36.05]
 [36.05]] [[1.347]
 [1.95 ]
 [1.347]
 [1.347]
 [1.347]
 [1.347]
 [1.347]]
maxi score, test score, baseline:  -0.9974000000000001 -1.0 -0.9974000000000001
probs:  [0.045479659542929846, 0.04221552160291487, 0.07176013819592235, 0.294592636598165, 0.25908269411756224, 0.2868693499425057]
maxi score, test score, baseline:  -0.9974000000000001 -1.0 -0.9974000000000001
probs:  [0.045479659542929846, 0.04221552160291487, 0.07176013819592235, 0.294592636598165, 0.25908269411756224, 0.2868693499425057]
actor:  1 policy actor:  1  step number:  69 total reward:  0.02666666666666606  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  42.746318986158336
printing an ep nov before normalisation:  40.08465803581431
maxi score, test score, baseline:  -0.9974000000000001 -1.0 -0.9974000000000001
probs:  [0.0475954097966525, 0.044565011879998756, 0.07199398278046645, 0.2788697801059327, 0.2459026513597686, 0.3110731640771809]
maxi score, test score, baseline:  -0.9974000000000001 -1.0 -0.9974000000000001
probs:  [0.0475954097966525, 0.044565011879998756, 0.07199398278046645, 0.2788697801059327, 0.2459026513597686, 0.3110731640771809]
Printing some Q and Qe and total Qs values:  [[-0.016]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]] [[29.378]
 [10.181]
 [10.181]
 [10.181]
 [10.181]
 [10.181]
 [10.181]] [[0.832]
 [0.17 ]
 [0.17 ]
 [0.17 ]
 [0.17 ]
 [0.17 ]
 [0.17 ]]
printing an ep nov before normalisation:  36.25689990633869
maxi score, test score, baseline:  -0.9974000000000001 -1.0 -0.9974000000000001
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.26312106503565
printing an ep nov before normalisation:  46.82790676001469
Printing some Q and Qe and total Qs values:  [[-0.076]
 [-0.082]
 [-0.082]
 [-0.082]
 [-0.082]
 [-0.082]
 [-0.082]] [[16.73]
 [14.6 ]
 [14.6 ]
 [14.6 ]
 [14.6 ]
 [14.6 ]
 [14.6 ]] [[1.492]
 [1.282]
 [1.282]
 [1.282]
 [1.282]
 [1.282]
 [1.282]]
printing an ep nov before normalisation:  26.70993599792908
maxi score, test score, baseline:  -0.9974000000000001 -1.0 -0.9974000000000001
probs:  [0.047566808926446685, 0.04452475612055964, 0.0720592186516385, 0.2797306588494912, 0.2453285146829009, 0.31079004276896305]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[ 0.045]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]] [[19.929]
 [12.276]
 [12.276]
 [12.276]
 [12.276]
 [12.276]
 [12.276]] [[0.808]
 [0.355]
 [0.355]
 [0.355]
 [0.355]
 [0.355]
 [0.355]]
printing an ep nov before normalisation:  16.88010647666185
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9974000000000001 -1.0 -0.9974000000000001
probs:  [0.047646794723604696, 0.04459961219722172, 0.07218050524933448, 0.278517213995825, 0.2457419799888184, 0.31131389384519575]
using explorer policy with actor:  1
printing an ep nov before normalisation:  18.268632396455274
rdn beta is 0 so we're just using the maxi policy
actions average: 
K:  2  action  0 :  tensor([0.3350, 0.1439, 0.0928, 0.1302, 0.1342, 0.0808, 0.0831],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0139, 0.8973, 0.0169, 0.0240, 0.0138, 0.0128, 0.0213],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1814, 0.0067, 0.2392, 0.1185, 0.1775, 0.1508, 0.1258],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2457, 0.0060, 0.1504, 0.1615, 0.1496, 0.1326, 0.1541],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1658, 0.0341, 0.1543, 0.1607, 0.2427, 0.1068, 0.1355],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1149, 0.1219, 0.1738, 0.1209, 0.1221, 0.2105, 0.1360],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1652, 0.1232, 0.1510, 0.1248, 0.1768, 0.1245, 0.1345],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  39.70532272056575
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 40.391962680085236
printing an ep nov before normalisation:  43.893847275224545
maxi score, test score, baseline:  -0.9974124378109452 -1.0 -0.9974124378109452
probs:  [0.04803330942032609, 0.04461293761360297, 0.07265753574204696, 0.27642960177178366, 0.24685874004258312, 0.31140787540965714]
maxi score, test score, baseline:  -0.9974124378109452 -1.0 -0.9974124378109452
probs:  [0.04800690119600492, 0.04457343193223816, 0.07272541982414914, 0.2772777799366139, 0.24628645773423802, 0.31113000937675583]
maxi score, test score, baseline:  -0.9974124378109452 -1.0 -0.9974124378109452
probs:  [0.04800690119600492, 0.04457343193223816, 0.07272541982414914, 0.2772777799366139, 0.24628645773423802, 0.31113000937675583]
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]] [[63.886]
 [63.886]
 [63.886]
 [63.886]
 [63.886]
 [63.886]
 [63.886]] [[1.838]
 [1.838]
 [1.838]
 [1.838]
 [1.838]
 [1.838]
 [1.838]]
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.612]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]] [[56.122]
 [59.071]
 [56.122]
 [56.122]
 [56.122]
 [56.122]
 [56.122]] [[1.778]
 [1.923]
 [1.778]
 [1.778]
 [1.778]
 [1.778]
 [1.778]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9974124378109452 -1.0 -0.9974124378109452
probs:  [0.04806899636107707, 0.044631073671848834, 0.07281957646167725, 0.2776372538432796, 0.24530970790917556, 0.3115333917529417]
line 256 mcts: sample exp_bonus 43.47405916849109
maxi score, test score, baseline:  -0.9974124378109452 -1.0 -0.9974124378109452
probs:  [0.04814795513491663, 0.044704369573008766, 0.07293930386137795, 0.27644849062817584, 0.24571355736198128, 0.31204632344053945]
printing an ep nov before normalisation:  43.07212079595157
printing an ep nov before normalisation:  7.681827582928236e-06
maxi score, test score, baseline:  -0.9974124378109452 -1.0 -0.9974124378109452
probs:  [0.048244496295304876, 0.04479398686321719, 0.0727074889937786, 0.2753732188593325, 0.24620733522731697, 0.31267347376104987]
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.85010693005429
maxi score, test score, baseline:  -0.9974124378109452 -1.0 -0.9974124378109452
probs:  [0.048244496295304876, 0.04479398686321719, 0.0727074889937786, 0.2753732188593325, 0.24620733522731697, 0.31267347376104987]
printing an ep nov before normalisation:  43.76323056147589
maxi score, test score, baseline:  -0.9974124378109452 -1.0 -0.9974124378109452
probs:  [0.048244496295304876, 0.04479398686321719, 0.0727074889937786, 0.2753732188593325, 0.24620733522731697, 0.31267347376104987]
actions average: 
K:  2  action  0 :  tensor([0.4976, 0.0093, 0.0872, 0.0953, 0.1348, 0.0805, 0.0953],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0509, 0.7940, 0.0220, 0.0342, 0.0470, 0.0183, 0.0335],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2136, 0.0140, 0.2012, 0.1293, 0.1459, 0.1193, 0.1767],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2258, 0.0077, 0.1251, 0.1824, 0.1822, 0.1378, 0.1391],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1887, 0.0593, 0.1367, 0.1587, 0.1638, 0.1629, 0.1299],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0954, 0.0356, 0.1695, 0.0938, 0.0909, 0.4060, 0.1088],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1602, 0.0293, 0.1142, 0.1322, 0.1748, 0.1146, 0.2746],
       grad_fn=<DivBackward0>)
actions average: 
K:  4  action  0 :  tensor([0.3726, 0.0312, 0.1140, 0.1266, 0.1310, 0.1007, 0.1239],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0451, 0.7796, 0.0297, 0.0379, 0.0250, 0.0280, 0.0547],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1875, 0.0654, 0.1324, 0.1536, 0.1705, 0.1240, 0.1666],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1623, 0.0329, 0.1332, 0.2429, 0.1449, 0.1137, 0.1701],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2500, 0.0068, 0.1356, 0.1498, 0.1628, 0.1254, 0.1695],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1735, 0.0803, 0.1329, 0.1298, 0.1455, 0.2009, 0.1371],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2358, 0.0314, 0.1071, 0.1212, 0.1039, 0.0878, 0.3128],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  42.450086495165884
printing an ep nov before normalisation:  67.93067612876433
maxi score, test score, baseline:  -0.9974124378109452 -1.0 -0.9974124378109452
probs:  [0.04792290703643468, 0.04486967247689513, 0.072830479009204, 0.27583966205598764, 0.2453341505373409, 0.31320312888413765]
actor:  1 policy actor:  1  step number:  70 total reward:  0.01999999999999935  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.342]
 [0.529]
 [0.515]
 [0.445]
 [0.529]
 [0.529]] [[59.863]
 [66.51 ]
 [59.863]
 [65.349]
 [68.802]
 [59.863]
 [59.863]] [[1.585]
 [1.604]
 [1.585]
 [1.741]
 [1.778]
 [1.585]
 [1.585]]
maxi score, test score, baseline:  -0.9974124378109452 -1.0 -0.9974124378109452
probs:  [0.047628313295626286, 0.044798217197563515, 0.07071557399497538, 0.31272452029386766, 0.23061222070622744, 0.29352115451173966]
printing an ep nov before normalisation:  43.933507406180716
printing an ep nov before normalisation:  40.858780955682
printing an ep nov before normalisation:  35.325255393981934
maxi score, test score, baseline:  -0.9974124378109452 -1.0 -0.9974124378109452
probs:  [0.04750021416406392, 0.044659973978392074, 0.07067022795273332, 0.31175515762668665, 0.23114000205487087, 0.2942744242232532]
maxi score, test score, baseline:  -0.9974124378109452 -1.0 -0.9974124378109452
probs:  [0.04750021416406392, 0.044659973978392074, 0.07067022795273332, 0.31175515762668665, 0.23114000205487087, 0.2942744242232532]
printing an ep nov before normalisation:  25.005809304493336
printing an ep nov before normalisation:  22.67497678361203
maxi score, test score, baseline:  -0.9974124378109452 -1.0 -0.9974124378109452
probs:  [0.04755578538500442, 0.04471221301327841, 0.07075298236656383, 0.31212075426089664, 0.23141102052681659, 0.2934472444474401]
printing an ep nov before normalisation:  57.89263375059477
printing an ep nov before normalisation:  21.02221220269451
printing an ep nov before normalisation:  57.59589732803825
using explorer policy with actor:  1
siam score:  -0.7406772
maxi score, test score, baseline:  -0.9974124378109452 -1.0 -0.9974124378109452
printing an ep nov before normalisation:  51.72521998677861
siam score:  -0.73989326
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9974124378109452 -1.0 -0.9974124378109452
probs:  [0.0474282274255251, 0.044574526276975286, 0.0707080525946067, 0.31115529114905877, 0.23193835310684133, 0.29419554944699283]
printing an ep nov before normalisation:  40.68139037131681
actor:  1 policy actor:  1  step number:  67 total reward:  0.3199999999999993  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  53.84891469247388
maxi score, test score, baseline:  -0.9974124378109452 -1.0 -0.9974124378109452
probs:  [0.04970242924010624, 0.0471304180225511, 0.07068429396971468, 0.28739689371285204, 0.21599948989973966, 0.32908647515503625]
printing an ep nov before normalisation:  29.50457510241119
maxi score, test score, baseline:  -0.9974124378109452 -1.0 -0.9974124378109452
probs:  [0.04970242924010624, 0.0471304180225511, 0.07068429396971468, 0.28739689371285204, 0.21599948989973966, 0.32908647515503625]
maxi score, test score, baseline:  -0.9974186104218362 -1.0 -0.9974186104218362
probs:  [0.04970243297682204, 0.047130417589608196, 0.07068432237723075, 0.2873969231575723, 0.21599943213379988, 0.3290864717649668]
maxi score, test score, baseline:  -0.9974186104218362 -1.0 -0.9974186104218362
probs:  [0.049781292977079346, 0.04720518575242155, 0.0707965626135162, 0.2862642976468877, 0.21634285520446045, 0.32960980580563465]
printing an ep nov before normalisation:  45.532431950700634
printing an ep nov before normalisation:  23.76922417211688
printing an ep nov before normalisation:  51.32797242356848
maxi score, test score, baseline:  -0.9974186104218362 -1.0 -0.9974186104218362
probs:  [0.049461949548915946, 0.04722101649332085, 0.07082032735551669, 0.2863605262072187, 0.21641556852849966, 0.3297206118665282]
printing an ep nov before normalisation:  21.076585918973453
printing an ep nov before normalisation:  37.68525937642003
maxi score, test score, baseline:  -0.9974186104218362 -1.0 -0.9974186104218362
maxi score, test score, baseline:  -0.9974186104218362 -1.0 -0.9974186104218362
probs:  [0.049461949548915946, 0.04722101649332085, 0.07082032735551669, 0.2863605262072187, 0.21641556852849966, 0.3297206118665282]
printing an ep nov before normalisation:  15.912489349674708
printing an ep nov before normalisation:  12.057241945090027
printing an ep nov before normalisation:  8.767755749688604
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.556]
 [0.557]
 [0.557]
 [0.559]
 [0.567]
 [0.57 ]] [[26.183]
 [33.129]
 [23.538]
 [23.856]
 [24.861]
 [22.958]
 [22.506]] [[1.47 ]
 [1.835]
 [1.268]
 [1.286]
 [1.348]
 [1.244]
 [1.219]]
maxi score, test score, baseline:  -0.9974186104218362 -1.0 -0.9974186104218362
probs:  [0.049461949548915946, 0.04722101649332085, 0.07082032735551669, 0.2863605262072187, 0.21641556852849966, 0.3297206118665282]
using another actor
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9974308641975309 -1.0 -0.9974308641975309
probs:  [0.04942915602747155, 0.04718008933856195, 0.07086503700295907, 0.28718685970447233, 0.21590585136865215, 0.3294330065578828]
maxi score, test score, baseline:  -0.9974308641975309 -1.0 -0.9974308641975309
probs:  [0.04942915602747155, 0.04718008933856195, 0.07086503700295907, 0.28718685970447233, 0.21590585136865215, 0.3294330065578828]
siam score:  -0.7419844
maxi score, test score, baseline:  -0.9974369458128078 -1.0 -0.9974369458128078
probs:  [0.04942915923330582, 0.04718008894791365, 0.07086506522946634, 0.2871868887544524, 0.2159057943679427, 0.3294330034669192]
printing an ep nov before normalisation:  42.918195724487305
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.97261335290317
maxi score, test score, baseline:  -0.9974369458128078 -1.0 -0.9974369458128078
maxi score, test score, baseline:  -0.9974369458128078 -1.0 -0.9974369458128078
maxi score, test score, baseline:  -0.9974369458128078 -1.0 -0.9974369458128078
probs:  [0.04950716748621487, 0.04725453816390421, 0.07097699469031511, 0.2860599531487788, 0.21624724324979078, 0.3299541032609962]
printing an ep nov before normalisation:  21.059360514699446
maxi score, test score, baseline:  -0.9974369458128078 -1.0 -0.9974369458128078
probs:  [0.04950716748621487, 0.04725453816390421, 0.07097699469031511, 0.2860599531487788, 0.21624724324979078, 0.3299541032609962]
from probs:  [0.04950716748621487, 0.04725453816390421, 0.07097699469031511, 0.2860599531487788, 0.21624724324979078, 0.3299541032609962]
maxi score, test score, baseline:  -0.9974429975429976 -1.0 -0.9974429975429976
probs:  [0.04956026924979602, 0.047305213778166336, 0.07105321086515404, 0.28636747771937265, 0.21540502662079158, 0.33030880176671934]
printing an ep nov before normalisation:  23.049127286058283
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.36270037336045
printing an ep nov before normalisation:  48.89978031238333
maxi score, test score, baseline:  -0.9974490196078432 -1.0 -0.9974490196078432
Printing some Q and Qe and total Qs values:  [[0.513]
 [1.156]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]] [[28.943]
 [ 0.   ]
 [28.943]
 [28.943]
 [28.943]
 [28.943]
 [28.943]] [[0.795]
 [1.156]
 [0.795]
 [0.795]
 [0.795]
 [0.795]
 [0.795]]
maxi score, test score, baseline:  -0.9974490196078432 -1.0 -0.9974490196078432
probs:  [0.04924267899911536, 0.04732098719988285, 0.0710769537852002, 0.28646321964833354, 0.2154769544773631, 0.33041920589010493]
printing an ep nov before normalisation:  35.445919036865234
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.658]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]] [[40.209]
 [41.678]
 [40.209]
 [40.209]
 [40.209]
 [40.209]
 [40.209]] [[2.337]
 [2.576]
 [2.337]
 [2.337]
 [2.337]
 [2.337]
 [2.337]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9974490196078432 -1.0 -0.9974490196078432
probs:  [0.04924267899911536, 0.04732098719988285, 0.0710769537852002, 0.28646321964833354, 0.2154769544773631, 0.33041920589010493]
maxi score, test score, baseline:  -0.9974490196078432 -1.0 -0.9974490196078432
probs:  [0.04924267899911536, 0.04732098719988285, 0.0710769537852002, 0.28646321964833354, 0.2154769544773631, 0.33041920589010493]
maxi score, test score, baseline:  -0.9974490196078432 -1.0 -0.9974490196078432
probs:  [0.04924267899911536, 0.04732098719988285, 0.0710769537852002, 0.28646321964833354, 0.2154769544773631, 0.33041920589010493]
printing an ep nov before normalisation:  41.31163429612086
maxi score, test score, baseline:  -0.9974490196078432 -1.0 -0.9974490196078432
printing an ep nov before normalisation:  50.418545916644234
Printing some Q and Qe and total Qs values:  [[0.145]
 [0.134]
 [0.09 ]
 [0.118]
 [0.136]
 [0.108]
 [0.142]] [[22.588]
 [17.54 ]
 [15.383]
 [15.664]
 [15.935]
 [16.158]
 [16.254]] [[1.358]
 [0.916]
 [0.687]
 [0.739]
 [0.781]
 [0.771]
 [0.813]]
maxi score, test score, baseline:  -0.9974490196078432 -1.0 -0.9974490196078432
probs:  [0.049233237711289575, 0.0473036359901504, 0.07115738508178954, 0.28585737195999394, 0.21615175409526816, 0.3302966151615083]
printing an ep nov before normalisation:  54.77004528045654
printing an ep nov before normalisation:  40.48401496284583
using explorer policy with actor:  1
using another actor
printing an ep nov before normalisation:  37.68129287310938
maxi score, test score, baseline:  -0.9974490196078432 -1.0 -0.9974490196078432
printing an ep nov before normalisation:  27.420844624124385
maxi score, test score, baseline:  -0.9974490196078432 -1.0 -0.9974490196078432
probs:  [0.04913748215074834, 0.04719509135015358, 0.07120693912563225, 0.28576359501563925, 0.21716230671493678, 0.3295345856428898]
printing an ep nov before normalisation:  47.33778821134081
maxi score, test score, baseline:  -0.9974490196078432 -1.0 -0.9974490196078432
probs:  [0.04913748215074834, 0.04719509135015358, 0.07120693912563225, 0.28576359501563925, 0.21716230671493678, 0.3295345856428898]
maxi score, test score, baseline:  -0.9974490196078432 -1.0 -0.9974490196078432
probs:  [0.04913748215074834, 0.04719509135015358, 0.07120693912563225, 0.28576359501563925, 0.21716230671493678, 0.3295345856428898]
printing an ep nov before normalisation:  16.767494678497314
maxi score, test score, baseline:  -0.9974490196078432 -1.0 -0.9974490196078432
probs:  [0.0492427435301778, 0.047296180464676714, 0.07135960580620403, 0.286377130438751, 0.21548219839299426, 0.33024214136719615]
printing an ep nov before normalisation:  12.636166115500094
using explorer policy with actor:  1
siam score:  -0.73554593
using another actor
maxi score, test score, baseline:  -0.9974550122249389 -1.0 -0.9974550122249389
printing an ep nov before normalisation:  37.634984792501555
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.477]
 [0.431]
 [0.431]
 [0.433]
 [0.413]
 [0.406]] [[46.976]
 [47.628]
 [45.243]
 [45.514]
 [44.019]
 [43.02 ]
 [43.683]] [[0.434]
 [0.477]
 [0.431]
 [0.431]
 [0.433]
 [0.413]
 [0.406]]
maxi score, test score, baseline:  -0.9974550122249389 -1.0 -0.9974550122249389
maxi score, test score, baseline:  -0.9974550122249389 -1.0 -0.9974550122249389
probs:  [0.049346442991366465, 0.0473957665977387, 0.07151003164148242, 0.2869815743610882, 0.21382700753353884, 0.33093917687478547]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  31.599460329328267
printing an ep nov before normalisation:  48.196782602945284
maxi score, test score, baseline:  -0.9974609756097561 -1.0 -0.9974609756097561
probs:  [0.049346445744688744, 0.04739576628319545, 0.07151006001032097, 0.2869816027772183, 0.2138269508682845, 0.33093917431629205]
printing an ep nov before normalisation:  64.93454653432747
maxi score, test score, baseline:  -0.9974669099756691 -1.0 -0.9974669099756691
printing an ep nov before normalisation:  25.373768521937645
Printing some Q and Qe and total Qs values:  [[0.658]
 [0.819]
 [0.661]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [0.662]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.658]
 [0.819]
 [0.661]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [0.662]]
maxi score, test score, baseline:  -0.9974669099756691 -1.0 -0.9974669099756691
probs:  [0.04939772096453638, 0.04744500614404906, 0.07158445176694132, 0.2872804822281167, 0.21300851907041154, 0.33128381982594496]
printing an ep nov before normalisation:  30.106626522535613
maxi score, test score, baseline:  -0.9974669099756691 -1.0 -0.9974669099756691
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9974669099756691 -1.0 -0.9974669099756691
probs:  [0.04907877439845848, 0.047460891348172725, 0.07160844192983166, 0.28737689358328866, 0.21307999318351428, 0.3313950055567342]
siam score:  -0.7435234
printing an ep nov before normalisation:  15.16564130783081
maxi score, test score, baseline:  -0.9974669099756691 -1.0 -0.9974669099756691
probs:  [0.04907877439845848, 0.047460891348172725, 0.07160844192983166, 0.28737689358328866, 0.21307999318351428, 0.3313950055567342]
actor:  1 policy actor:  1  step number:  63 total reward:  0.19999999999999885  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9974669099756691 -1.0 -0.9974669099756691
probs:  [0.04855937651916413, 0.047075653317752715, 0.06922081458199553, 0.32871797110713286, 0.19896110878785864, 0.3074650756860962]
printing an ep nov before normalisation:  25.09574956475376
actor:  1 policy actor:  1  step number:  58 total reward:  0.24666666666666592  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.389]
 [0.468]
 [0.515]
 [0.515]
 [0.454]
 [0.454]
 [0.389]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.389]
 [0.468]
 [0.515]
 [0.515]
 [0.454]
 [0.454]
 [0.389]]
siam score:  -0.745657
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.45767310206674
printing an ep nov before normalisation:  35.959580388085755
printing an ep nov before normalisation:  34.31017498361419
maxi score, test score, baseline:  -0.9974669099756691 -1.0 -0.9974669099756691
probs:  [0.04932487611953194, 0.04793849910210012, 0.06863072969798348, 0.30946943382504066, 0.18985884148510546, 0.3347776197702383]
printing an ep nov before normalisation:  17.717662557520555
maxi score, test score, baseline:  -0.9974669099756691 -1.0 -0.9974669099756691
probs:  [0.04932487611953194, 0.04793849910210012, 0.06863072969798348, 0.30946943382504066, 0.18985884148510546, 0.3347776197702383]
printing an ep nov before normalisation:  34.81208801269531
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.521]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]] [[46.651]
 [54.313]
 [46.651]
 [46.651]
 [46.651]
 [46.651]
 [46.651]] [[1.501]
 [1.762]
 [1.501]
 [1.501]
 [1.501]
 [1.501]
 [1.501]]
maxi score, test score, baseline:  -0.9974669099756691 -1.0 -0.9974669099756691
probs:  [0.04924052617476341, 0.047850713942298735, 0.06859421649430428, 0.3100296794586124, 0.1901227116740974, 0.33416215225592394]
using explorer policy with actor:  0
printing an ep nov before normalisation:  14.972137225004758
printing an ep nov before normalisation:  11.937057647081977
siam score:  -0.7442412
Printing some Q and Qe and total Qs values:  [[0.213]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]] [[32.488]
 [29.384]
 [29.384]
 [29.384]
 [29.384]
 [29.384]
 [29.384]] [[1.41 ]
 [1.088]
 [1.088]
 [1.088]
 [1.088]
 [1.088]
 [1.088]]
maxi score, test score, baseline:  -0.9974728155339806 -1.0 -0.9974728155339806
maxi score, test score, baseline:  -0.9974728155339806 -1.0 -0.9974728155339806
probs:  [0.049240526922678746, 0.04785071238981955, 0.06859424140364019, 0.3100297023100159, 0.19012267591704388, 0.33416214105680164]
printing an ep nov before normalisation:  31.166765392365257
maxi score, test score, baseline:  -0.9974728155339806 -1.0 -0.9974728155339806
printing an ep nov before normalisation:  29.983773362625243
actor:  1 policy actor:  1  step number:  59 total reward:  0.13333333333333286  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.157]
 [0.151]
 [0.152]
 [0.153]
 [0.163]
 [0.153]
 [0.144]] [[37.528]
 [44.489]
 [37.358]
 [37.235]
 [35.866]
 [39.523]
 [32.673]] [[0.157]
 [0.151]
 [0.152]
 [0.153]
 [0.163]
 [0.153]
 [0.144]]
maxi score, test score, baseline:  -0.9974728155339806 -1.0 -0.9974728155339806
probs:  [0.050139912132213825, 0.04859912178555545, 0.06812580493570818, 0.3394215225753836, 0.1810652304855046, 0.3126484080856342]
printing an ep nov before normalisation:  48.42510634749958
maxi score, test score, baseline:  -0.9974728155339806 -1.0 -0.9974728155339806
probs:  [0.050139912132213825, 0.04859912178555545, 0.06812580493570818, 0.3394215225753836, 0.1810652304855046, 0.3126484080856342]
maxi score, test score, baseline:  -0.9974786924939467 -1.0 -0.9974786924939467
probs:  [0.05013991766871518, 0.04859912487274785, 0.06812583178278588, 0.33942154391440976, 0.18106519473273147, 0.31264838702860975]
printing an ep nov before normalisation:  34.15068844476483
printing an ep nov before normalisation:  30.432098301346162
maxi score, test score, baseline:  -0.9974845410628019 -1.0 -0.9974845410628019
probs:  [0.050023634360946964, 0.04847744656392506, 0.06807251789128689, 0.33856849181935633, 0.18140708999166683, 0.3134508193728179]
maxi score, test score, baseline:  -0.9974845410628019 -1.0 -0.9974845410628019
probs:  [0.04990775278626263, 0.0483561913103667, 0.06801936408118163, 0.337718405946583, 0.18174782339045364, 0.31425046248515237]
from probs:  [0.04990775278626263, 0.0483561913103667, 0.06801936408118163, 0.337718405946583, 0.18174782339045364, 0.31425046248515237]
printing an ep nov before normalisation:  38.36075735472627
maxi score, test score, baseline:  -0.9974845410628019 -1.0 -0.9974845410628019
probs:  [0.04984886718609684, 0.048290176573493465, 0.06804369796984663, 0.3372550345938722, 0.18229471841932407, 0.31426750525736685]
printing an ep nov before normalisation:  53.48484952391062
Printing some Q and Qe and total Qs values:  [[0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]] [[17.738]
 [17.738]
 [17.738]
 [17.738]
 [17.738]
 [17.738]
 [17.738]] [[0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]]
printing an ep nov before normalisation:  29.759967327117923
maxi score, test score, baseline:  -0.9974903614457832 -1.0 -0.9974903614457832
probs:  [0.05006468083122374, 0.04825953806460556, 0.06823237798455349, 0.3370419222271507, 0.18231282141469105, 0.3140886594777753]
maxi score, test score, baseline:  -0.9974903614457832 -1.0 -0.9974903614457832
probs:  [0.05006468083122374, 0.04825953806460556, 0.06823237798455349, 0.3370419222271507, 0.18231282141469105, 0.3140886594777753]
line 256 mcts: sample exp_bonus 39.060743944134025
Printing some Q and Qe and total Qs values:  [[0.273]
 [0.248]
 [0.286]
 [0.335]
 [0.396]
 [0.274]
 [0.28 ]] [[36.787]
 [42.613]
 [31.255]
 [31.481]
 [29.197]
 [36.247]
 [31.65 ]] [[0.273]
 [0.248]
 [0.286]
 [0.335]
 [0.396]
 [0.274]
 [0.28 ]]
maxi score, test score, baseline:  -0.9974903614457832 -1.0 -0.9974903614457832
probs:  [0.05006468083122374, 0.04825953806460556, 0.06823237798455349, 0.3370419222271507, 0.18231282141469105, 0.3140886594777753]
maxi score, test score, baseline:  -0.9974903614457832 -1.0 -0.9974903614457832
probs:  [0.05006468083122374, 0.04825953806460556, 0.06823237798455349, 0.3370419222271507, 0.18231282141469105, 0.3140886594777753]
maxi score, test score, baseline:  -0.9974903614457832 -1.0 -0.9974903614457832
probs:  [0.05012114475440581, 0.048313960680471696, 0.0683093864617722, 0.33742290826193494, 0.18251883533573654, 0.3133137645056789]
maxi score, test score, baseline:  -0.9974903614457832 -1.0 -0.9974903614457832
probs:  [0.05013425112272172, 0.048326593221890354, 0.06806528048115293, 0.3375113424870773, 0.18256665515014653, 0.31339587753701115]
maxi score, test score, baseline:  -0.9974961538461539 -1.0 -0.9974961538461539
maxi score, test score, baseline:  -0.9974961538461539 -1.0 -0.9974961538461539
probs:  [0.05013425692700081, 0.0483265961922803, 0.0680653071684106, 0.33751136301047163, 0.18256661992344134, 0.3133958567783954]
maxi score, test score, baseline:  -0.9974961538461539 -1.0 -0.9974961538461539
probs:  [0.05013425692700081, 0.0483265961922803, 0.0680653071684106, 0.33751136301047163, 0.18256661992344134, 0.3133958567783954]
printing an ep nov before normalisation:  57.92190123881902
printing an ep nov before normalisation:  18.856341286592095
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  -0.9974961538461539 -1.0 -0.9974961538461539
printing an ep nov before normalisation:  0.13739315202897728
line 256 mcts: sample exp_bonus 37.19002251331655
printing an ep nov before normalisation:  58.90667071370119
printing an ep nov before normalisation:  51.36115317221886
line 256 mcts: sample exp_bonus 6.3019882269325915
maxi score, test score, baseline:  -0.9974961538461539 -1.0 -0.9974961538461539
maxi score, test score, baseline:  -0.9974961538461539 -1.0 -0.9974961538461539
probs:  [0.04996294867234736, 0.048140845361719066, 0.06803726172255946, 0.3362085635088089, 0.183453399847883, 0.3141969808866823]
printing an ep nov before normalisation:  50.47792188214957
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9974961538461539 -1.0 -0.9974961538461539
probs:  [0.04996294867234736, 0.048140845361719066, 0.06803726172255946, 0.3362085635088089, 0.183453399847883, 0.3141969808866823]
printing an ep nov before normalisation:  73.65787138677149
maxi score, test score, baseline:  -0.9974961538461539 -1.0 -0.9974961538461539
printing an ep nov before normalisation:  50.321613522032955
maxi score, test score, baseline:  -0.9974961538461539 -1.0 -0.9974961538461539
maxi score, test score, baseline:  -0.9975019184652278 -1.0 -0.9975019184652278
probs:  [0.050019006552053984, 0.04819485075746826, 0.06811367191237169, 0.3365866257624179, 0.18365957655545265, 0.31342626846023564]
printing an ep nov before normalisation:  32.303153893502035
maxi score, test score, baseline:  -0.9975019184652278 -1.0 -0.9975019184652278
probs:  [0.050019006552053984, 0.04819485075746826, 0.06811367191237169, 0.3365866257624179, 0.18365957655545265, 0.31342626846023564]
maxi score, test score, baseline:  -0.9975019184652278 -1.0 -0.9975019184652278
probs:  [0.050019006552053984, 0.04819485075746826, 0.06811367191237169, 0.3365866257624179, 0.18365957655545265, 0.31342626846023564]
siam score:  -0.7370346
maxi score, test score, baseline:  -0.9975019184652278 -1.0 -0.9975019184652278
probs:  [0.050019006552053984, 0.04819485075746826, 0.06811367191237169, 0.3365866257624179, 0.18365957655545265, 0.31342626846023564]
maxi score, test score, baseline:  -0.9975019184652278 -1.0 -0.9975019184652278
maxi score, test score, baseline:  -0.9975019184652278 -1.0 -0.9975019184652278
printing an ep nov before normalisation:  49.66317581061367
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.43039321899414
printing an ep nov before normalisation:  53.41642697652181
Printing some Q and Qe and total Qs values:  [[0.926]
 [0.926]
 [0.23 ]
 [0.926]
 [0.926]
 [0.23 ]
 [0.23 ]] [[ 0.   ]
 [ 0.   ]
 [28.725]
 [ 0.   ]
 [ 0.   ]
 [28.725]
 [28.725]] [[0.926]
 [0.926]
 [1.562]
 [0.926]
 [0.926]
 [1.562]
 [1.562]]
from probs:  [0.04964835000888927, 0.048088567619806136, 0.06808039282639712, 0.33584136883142346, 0.1840495822154443, 0.31429173849803976]
maxi score, test score, baseline:  -0.9975076555023924 -1.0 -0.9975076555023924
probs:  [0.04954779315257074, 0.04798234148909519, 0.06778275310170034, 0.33509651482608677, 0.18443752777420008, 0.31515306965634693]
maxi score, test score, baseline:  -0.9975076555023924 -1.0 -0.9975076555023924
probs:  [0.04954779315257074, 0.04798234148909519, 0.06778275310170034, 0.33509651482608677, 0.18443752777420008, 0.31515306965634693]
siam score:  -0.7454784
printing an ep nov before normalisation:  34.26623344421387
printing an ep nov before normalisation:  44.72367355923025
maxi score, test score, baseline:  -0.9975076555023924 -1.0 -0.9975076555023924
probs:  [0.04954779315257074, 0.04798234148909519, 0.06778275310170034, 0.33509651482608677, 0.18443752777420008, 0.31515306965634693]
maxi score, test score, baseline:  -0.9975076555023924 -1.0 -0.9975076555023924
probs:  [0.04954779315257074, 0.04798234148909519, 0.06778275310170034, 0.33509651482608677, 0.18443752777420008, 0.31515306965634693]
UNIT TEST: sample policy line 217 mcts : [0.102 0.265 0.122 0.102 0.143 0.163 0.102]
printing an ep nov before normalisation:  46.08449967966891
maxi score, test score, baseline:  -0.9975076555023924 -1.0 -0.9975076555023924
probs:  [0.04954779315257074, 0.04798234148909519, 0.06778275310170034, 0.33509651482608677, 0.18443752777420008, 0.31515306965634693]
maxi score, test score, baseline:  -0.9975133651551312 -1.0 -0.9975133651551312
probs:  [0.04954779848708169, 0.04798234439747895, 0.06778277948510851, 0.3350965349136344, 0.18443749302861331, 0.3151530496880832]
actor:  1 policy actor:  1  step number:  61 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9975133651551312 -1.0 -0.9975133651551312
probs:  [0.047377078720331034, 0.04588039305301584, 0.06481102001875476, 0.32038201956662415, 0.22023486216630517, 0.301314626474969]
maxi score, test score, baseline:  -0.9975133651551312 -1.0 -0.9975133651551312
probs:  [0.047377078720331034, 0.04588039305301584, 0.06481102001875476, 0.32038201956662415, 0.22023486216630517, 0.301314626474969]
printing an ep nov before normalisation:  19.636033886613404
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.631]
 [0.631]
 [0.631]
 [0.631]
 [0.631]
 [0.631]] [[50.074]
 [42.053]
 [42.053]
 [42.053]
 [42.053]
 [42.053]
 [42.053]] [[1.996]
 [1.728]
 [1.728]
 [1.728]
 [1.728]
 [1.728]
 [1.728]]
printing an ep nov before normalisation:  27.430297737089806
maxi score, test score, baseline:  -0.9975133651551312 -1.0 -0.9975133651551312
probs:  [0.0474249667660301, 0.04592676400498425, 0.06487657972632789, 0.32070663511312186, 0.21944513975467492, 0.30161991463486104]
Starting evaluation
maxi score, test score, baseline:  -0.9975190476190476 -1.0 -0.9975190476190476
probs:  [0.047472508254171104, 0.04597279722632232, 0.0649416831813697, 0.32102888614803843, 0.21866118317565783, 0.3019229420144405]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]] [[6.684]
 [6.684]
 [6.684]
 [6.684]
 [6.684]
 [6.684]
 [6.684]] [[0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]]
printing an ep nov before normalisation:  24.06309435988445
Printing some Q and Qe and total Qs values:  [[0.122]
 [0.115]
 [0.122]
 [0.123]
 [0.123]
 [0.123]
 [0.473]] [[ 8.318]
 [19.152]
 [ 8.318]
 [ 9.247]
 [ 8.884]
 [ 8.716]
 [ 9.479]] [[0.122]
 [0.115]
 [0.122]
 [0.123]
 [0.123]
 [0.123]
 [0.473]]
Printing some Q and Qe and total Qs values:  [[0.567]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]] [[50.082]
 [47.273]
 [47.273]
 [47.273]
 [47.273]
 [47.273]
 [47.273]] [[0.567]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]]
printing an ep nov before normalisation:  9.859462093260003
printing an ep nov before normalisation:  10.273863149005798
printing an ep nov before normalisation:  9.267343605314338
line 256 mcts: sample exp_bonus 10.770615963747943
printing an ep nov before normalisation:  6.018945519854455
printing an ep nov before normalisation:  19.22436789480564
using explorer policy with actor:  0
using explorer policy with actor:  0
printing an ep nov before normalisation:  18.32127350453039
Printing some Q and Qe and total Qs values:  [[0.121]
 [0.113]
 [0.121]
 [0.121]
 [0.121]
 [0.122]
 [0.122]] [[ 9.897]
 [18.082]
 [ 9.897]
 [ 9.897]
 [ 9.897]
 [ 8.272]
 [ 8.414]] [[0.121]
 [0.113]
 [0.121]
 [0.121]
 [0.121]
 [0.122]
 [0.122]]
maxi score, test score, baseline:  -0.9975303317535545 -1.0 -0.9975303317535545
probs:  [0.047472518525934086, 0.04597280289929429, 0.06494173336806965, 0.32102892534556726, 0.21866111455527878, 0.3019229053058559]
printing an ep nov before normalisation:  39.54556959998787
printing an ep nov before normalisation:  28.724614506765764
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9975303317535545 -1.0 -0.9975303317535545
probs:  [0.04735742546023939, 0.04585290105717858, 0.06488265462064588, 0.3201883912552448, 0.21909493067577235, 0.30262369693091895]
printing an ep nov before normalisation:  51.98837934907506
printing an ep nov before normalisation:  41.35758375961842
printing an ep nov before normalisation:  33.70467115308209
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.544]
 [0.486]
 [0.486]
 [0.514]
 [0.486]
 [0.486]] [[41.41 ]
 [48.726]
 [41.41 ]
 [41.41 ]
 [47.786]
 [41.41 ]
 [41.41 ]] [[1.369]
 [1.732]
 [1.369]
 [1.369]
 [1.662]
 [1.369]
 [1.369]]
printing an ep nov before normalisation:  22.631439607962434
Printing some Q and Qe and total Qs values:  [[0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]] [[29.858]
 [29.858]
 [29.858]
 [29.858]
 [29.858]
 [29.858]
 [29.858]] [[0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]]
line 256 mcts: sample exp_bonus 10.93923054906763
printing an ep nov before normalisation:  13.565701528376133
using explorer policy with actor:  0
printing an ep nov before normalisation:  29.623135776328954
printing an ep nov before normalisation:  18.608524123155995
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9975689976689976 -1.0 -0.9975689976689976
probs:  [0.04736935877668969, 0.045864439486388124, 0.0646473908539669, 0.3202691628879269, 0.21914986318437263, 0.3026997848106556]
printing an ep nov before normalisation:  27.747578620910645
printing an ep nov before normalisation:  12.967298030853271
maxi score, test score, baseline:  -0.9975744186046511 -1.0 -0.9975744186046511
probs:  [0.047369363716139985, 0.045864442207401476, 0.06464741470949356, 0.3202691816873141, 0.21914983028835844, 0.3026997673912926]
printing an ep nov before normalisation:  10.903039851688934
printing an ep nov before normalisation:  14.393007658986505
printing an ep nov before normalisation:  10.936661606383765
actions average: 
K:  3  action  0 :  tensor([0.3676, 0.0090, 0.1011, 0.1042, 0.1655, 0.1075, 0.1452],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0201, 0.9289, 0.0042, 0.0111, 0.0045, 0.0026, 0.0286],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1210, 0.0298, 0.2119, 0.1757, 0.1616, 0.1521, 0.1480],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1648, 0.1503, 0.1068, 0.2002, 0.1257, 0.1110, 0.1411],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1771, 0.0207, 0.1354, 0.1235, 0.2705, 0.1097, 0.1631],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1203, 0.0500, 0.1726, 0.1230, 0.1242, 0.2446, 0.1654],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2370, 0.0821, 0.1200, 0.1224, 0.1463, 0.1082, 0.1840],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  11.5365082255434
maxi score, test score, baseline:  -0.9975798143851509 -1.0 -0.9975798143851509
probs:  [0.047420503874902, 0.04591395101061627, 0.06471727798194135, 0.3206157619498822, 0.21938689280673998, 0.3019456123759182]
maxi score, test score, baseline:  -0.9975798143851509 -1.0 -0.9975798143851509
probs:  [0.047420503874902, 0.04591395101061627, 0.06471727798194135, 0.3206157619498822, 0.21938689280673998, 0.3019456123759182]
printing an ep nov before normalisation:  4.842480050366191
printing an ep nov before normalisation:  8.441128959589612
maxi score, test score, baseline:  -0.9975851851851852 -1.0 -0.9975851851851852
probs:  [0.04742050878082881, 0.04591395371606131, 0.0647173016514888, 0.320615780642323, 0.21938686021244796, 0.30194559499685014]
printing an ep nov before normalisation:  10.92402696609497
actor:  1 policy actor:  1  step number:  55 total reward:  0.11999999999999944  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]] [[28.387]
 [22.081]
 [22.081]
 [22.081]
 [22.081]
 [22.081]
 [22.081]] [[0.559]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]]
printing an ep nov before normalisation:  56.827962375925104
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]] [[38.558]
 [38.558]
 [38.558]
 [38.558]
 [38.558]
 [38.558]
 [38.558]] [[1.569]
 [1.569]
 [1.569]
 [1.569]
 [1.569]
 [1.569]
 [1.569]]
Printing some Q and Qe and total Qs values:  [[0.603]
 [1.001]
 [0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.603]] [[41.352]
 [56.809]
 [41.352]
 [41.352]
 [41.352]
 [41.352]
 [41.352]] [[1.239]
 [2.005]
 [1.239]
 [1.239]
 [1.239]
 [1.239]
 [1.239]]
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]] [[15.879]
 [15.879]
 [15.879]
 [15.879]
 [15.879]
 [15.879]
 [15.879]] [[0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  42.01415892671599
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]] [[40.68 ]
 [38.593]
 [38.593]
 [38.593]
 [38.593]
 [38.593]
 [38.593]] [[2.571]
 [2.423]
 [2.423]
 [2.423]
 [2.423]
 [2.423]
 [2.423]]
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]] [[32.493]
 [30.174]
 [30.174]
 [30.174]
 [30.174]
 [30.174]
 [30.174]] [[0.566]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9975851851851852 -1.0 -0.9975851851851852
probs:  [0.05027374890805707, 0.04888677727615974, 0.06619760167351278, 0.341455871361333, 0.2085901998990801, 0.2845958008818572]
printing an ep nov before normalisation:  12.404295315730991
printing an ep nov before normalisation:  10.491955697629578
printing an ep nov before normalisation:  10.36670446395874
using explorer policy with actor:  1
using explorer policy with actor:  0
printing an ep nov before normalisation:  40.83481700619504
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.553]
 [0.57 ]
 [0.574]
 [0.57 ]
 [0.582]
 [0.564]] [[16.985]
 [20.906]
 [11.351]
 [10.367]
 [10.177]
 [15.962]
 [15.407]] [[0.574]
 [0.553]
 [0.57 ]
 [0.574]
 [0.57 ]
 [0.582]
 [0.564]]
maxi score, test score, baseline:  -0.9975958525345622 -1.0 -0.9975958525345622
probs:  [0.050213625572176936, 0.04882066323441903, 0.06620624583065533, 0.3409919514283758, 0.2092133580430052, 0.2845541558913678]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.674]
 [0.579]
 [0.615]
 [0.642]
 [0.71 ]
 [0.695]
 [0.589]] [[11.639]
 [21.086]
 [ 9.051]
 [ 9.475]
 [ 7.236]
 [ 7.969]
 [ 9.645]] [[0.674]
 [0.579]
 [0.615]
 [0.642]
 [0.71 ]
 [0.695]
 [0.589]]
printing an ep nov before normalisation:  11.216532383435931
printing an ep nov before normalisation:  36.30979038013811
printing an ep nov before normalisation:  32.84440957073347
printing an ep nov before normalisation:  22.948536937960036
using explorer policy with actor:  0
printing an ep nov before normalisation:  0.11648898600157054
maxi score, test score, baseline:  -0.9976011494252873 -1.0 -0.9976011494252873
probs:  [0.05021363082467995, 0.04882066651267707, 0.06620626782231928, 0.34099197416038973, 0.20921332541783974, 0.2845541352620943]
maxi score, test score, baseline:  -0.9976324263038548 -1.0 -0.9976324263038548
probs:  [0.050406117392534276, 0.04879258309665762, 0.0663775137347875, 0.34079657124371693, 0.2091934315051526, 0.2844337830271512]
printing an ep nov before normalisation:  37.2668990166371
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.47 ]
 [0.426]
 [0.43 ]
 [0.431]
 [0.428]
 [0.429]] [[32.574]
 [35.159]
 [29.927]
 [29.824]
 [29.271]
 [28.856]
 [28.561]] [[0.448]
 [0.47 ]
 [0.426]
 [0.43 ]
 [0.431]
 [0.428]
 [0.429]]
printing an ep nov before normalisation:  22.311887752607706
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]] [[48.046]
 [48.046]
 [48.046]
 [48.046]
 [48.046]
 [48.046]
 [48.046]] [[1.737]
 [1.737]
 [1.737]
 [1.737]
 [1.737]
 [1.737]
 [1.737]]
maxi score, test score, baseline:  -0.9976324263038548 -1.0 -0.9976324263038548
probs:  [0.05050226766851311, 0.04888564738004934, 0.06650421037773482, 0.34144811274394454, 0.20866855840231793, 0.2839912034274403]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.653]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [34.683]] [[0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [1.752]]
maxi score, test score, baseline:  -0.9976426636568849 -1.0 -0.9976426636568849
probs:  [0.05039374635044433, 0.04877178286300164, 0.0664485666430773, 0.34064989964763454, 0.20908229468173295, 0.28465370981410926]
maxi score, test score, baseline:  -0.9976426636568849 -1.0 -0.9976426636568849
probs:  [0.05039374635044433, 0.04877178286300164, 0.0664485666430773, 0.34064989964763454, 0.20908229468173295, 0.28465370981410926]
maxi score, test score, baseline:  -0.9976426636568849 -1.0 -0.9976426636568849
probs:  [0.05039374635044433, 0.04877178286300164, 0.0664485666430773, 0.34064989964763454, 0.20908229468173295, 0.28465370981410926]
maxi score, test score, baseline:  -0.9976426636568849 -1.0 -0.9976426636568849
probs:  [0.05039374635044433, 0.04877178286300164, 0.0664485666430773, 0.34064989964763454, 0.20908229468173295, 0.28465370981410926]
maxi score, test score, baseline:  -0.9976426636568849 -1.0 -0.9976426636568849
probs:  [0.050443214811913696, 0.0488196549215901, 0.06651383691609553, 0.34098505010066166, 0.20928795091048896, 0.28395029233925007]
maxi score, test score, baseline:  -0.9976426636568849 -1.0 -0.9976426636568849
probs:  [0.050443214811913696, 0.0488196549215901, 0.06651383691609553, 0.34098505010066166, 0.20928795091048896, 0.28395029233925007]
Printing some Q and Qe and total Qs values:  [[0.258]
 [0.46 ]
 [0.35 ]
 [0.365]
 [0.369]
 [0.363]
 [0.385]] [[34.961]
 [23.123]
 [28.327]
 [28.317]
 [28.388]
 [29.198]
 [28.912]] [[1.595]
 [1.342]
 [1.432]
 [1.446]
 [1.453]
 [1.478]
 [1.49 ]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.5507],
        [-0.6021],
        [-0.7380],
        [-0.3582],
        [-0.4428],
        [-0.0000],
        [-0.2660],
        [-0.6015],
        [-0.5283],
        [-0.2817]], dtype=torch.float64)
-0.057834381198 -0.6085644715294215
-0.032346567066 -0.6344576527126818
-0.032346567066 -0.7703567569834638
-0.032346567066 -0.3905347350640462
-0.032346567066 -0.47517507995409913
-0.85602 -0.85602
-0.09703970119800001 -0.36300968094116337
-0.045026434398 -0.6464934272268371
-0.09703970119800001 -0.6253752691753597
-0.032346567066 -0.3140032940353776
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.626]
 [0.626]
 [0.064]
 [0.064]
 [0.064]
 [0.064]] [[ 0.012]
 [ 0.022]
 [ 0.011]
 [21.513]
 [21.513]
 [21.513]
 [21.513]] [[0.627]
 [0.627]
 [0.627]
 [1.232]
 [1.232]
 [1.232]
 [1.232]]
printing an ep nov before normalisation:  42.43921052900223
maxi score, test score, baseline:  -0.9976426636568849 -1.0 -0.9976426636568849
probs:  [0.050541382986110724, 0.04891465509841168, 0.06664336315041168, 0.3416501427087136, 0.20969606743341174, 0.2825543886229407]
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.36 ]
 [0.278]
 [0.278]
 [0.347]
 [0.29 ]
 [0.279]] [[32.561]
 [43.844]
 [33.151]
 [33.939]
 [41.474]
 [34.545]
 [34.34 ]] [[0.694]
 [1.066]
 [0.704]
 [0.725]
 [0.991]
 [0.753]
 [0.736]]
maxi score, test score, baseline:  -0.9976426636568849 -1.0 -0.9976426636568849
probs:  [0.050541382986110724, 0.04891465509841168, 0.06664336315041168, 0.3416501427087136, 0.20969606743341174, 0.2825543886229407]
maxi score, test score, baseline:  -0.9976426636568849 -1.0 -0.9976426636568849
probs:  [0.050541382986110724, 0.04891465509841168, 0.06664336315041168, 0.3416501427087136, 0.20969606743341174, 0.2825543886229407]
maxi score, test score, baseline:  -0.9976426636568849 -1.0 -0.9976426636568849
probs:  [0.050541382986110724, 0.04891465509841168, 0.06664336315041168, 0.3416501427087136, 0.20969606743341174, 0.2825543886229407]
maxi score, test score, baseline:  -0.9976426636568849 -1.0 -0.9976426636568849
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.475]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.552]
 [0.5  ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.499]
 [0.475]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.552]
 [0.5  ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9976426636568849 -1.0 -0.9976426636568849
probs:  [0.05055312347176039, 0.04892601670538678, 0.06642613164890146, 0.34172968488502886, 0.20974487639028558, 0.2826201668986369]
printing an ep nov before normalisation:  19.64000205093747
maxi score, test score, baseline:  -0.9976426636568849 -1.0 -0.9976426636568849
probs:  [0.05044513784940463, 0.04881267645835276, 0.06637038230449625, 0.3409351428153612, 0.2101607728550609, 0.28327588771732415]
maxi score, test score, baseline:  -0.9976426636568849 -1.0 -0.9976426636568849
maxi score, test score, baseline:  -0.9976426636568849 -1.0 -0.9976426636568849
probs:  [0.05010721040415906, 0.04871150162220516, 0.06633088050778595, 0.3402257733310114, 0.21062634788927814, 0.2839982862455603]
Printing some Q and Qe and total Qs values:  [[0.284]
 [0.309]
 [0.281]
 [0.281]
 [0.281]
 [0.281]
 [0.281]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.284]
 [0.309]
 [0.281]
 [0.281]
 [0.281]
 [0.281]
 [0.281]]
maxi score, test score, baseline:  -0.9976426636568849 -1.0 -0.9976426636568849
probs:  [0.05010721040415906, 0.04871150162220516, 0.06633088050778595, 0.3402257733310114, 0.21062634788927814, 0.2839982862455603]
printing an ep nov before normalisation:  32.48321294784546
maxi score, test score, baseline:  -0.9976426636568849 -1.0 -0.9976426636568849
probs:  [0.05010721040415906, 0.04871150162220516, 0.06633088050778595, 0.3402257733310114, 0.21062634788927814, 0.2839982862455603]
printing an ep nov before normalisation:  42.720816625204954
printing an ep nov before normalisation:  47.31892354598069
printing an ep nov before normalisation:  28.00213491894667
maxi score, test score, baseline:  -0.9976477477477478 -1.0 -0.9976477477477478
probs:  [0.05010721543746903, 0.04871150475505062, 0.06633090188815403, 0.34022579505187067, 0.21062631653582423, 0.2839982663316315]
maxi score, test score, baseline:  -0.9976477477477478 -1.0 -0.9976477477477478
probs:  [0.05015369044550257, 0.04875668177453284, 0.06639246466440384, 0.34054207568624917, 0.209892831163278, 0.2842622562660336]
maxi score, test score, baseline:  -0.9976477477477478 -1.0 -0.9976477477477478
probs:  [0.05015369044550257, 0.04875668177453284, 0.06639246466440384, 0.34054207568624917, 0.209892831163278, 0.2842622562660336]
maxi score, test score, baseline:  -0.9976477477477478 -1.0 -0.9976477477477478
probs:  [0.05015369044550257, 0.04875668177453284, 0.06639246466440384, 0.34054207568624917, 0.209892831163278, 0.2842622562660336]
maxi score, test score, baseline:  -0.9976528089887641 -1.0 -0.9976528089887641
maxi score, test score, baseline:  -0.9976528089887641 -1.0 -0.9976528089887641
probs:  [0.05015369546498556, 0.04875668490023228, 0.06639248597382645, 0.34054209735783764, 0.2098927998560172, 0.2842622364471008]
maxi score, test score, baseline:  -0.9976528089887641 -1.0 -0.9976528089887641
probs:  [0.05015369546498556, 0.04875668490023228, 0.06639248597382645, 0.34054209735783764, 0.2098927998560172, 0.2842622364471008]
maxi score, test score, baseline:  -0.9976528089887641 -1.0 -0.9976528089887641
probs:  [0.05015369546498556, 0.04875668490023228, 0.06639248597382645, 0.34054209735783764, 0.2098927998560172, 0.2842622364471008]
printing an ep nov before normalisation:  36.62315846365495
maxi score, test score, baseline:  -0.9976528089887641 -1.0 -0.9976528089887641
probs:  [0.05015369546498556, 0.04875668490023228, 0.06639248597382645, 0.34054209735783764, 0.2098927998560172, 0.2842622364471008]
maxi score, test score, baseline:  -0.9976528089887641 -1.0 -0.9976528089887641
probs:  [0.05015369546498556, 0.04875668490023228, 0.06639248597382645, 0.34054209735783764, 0.2098927998560172, 0.2842622364471008]
using another actor
printing an ep nov before normalisation:  18.56311321258545
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9976578475336323 -1.0 -0.9976578475336323
probs:  [0.050153700461916494, 0.04875668801188751, 0.06639250718751007, 0.3405421189320523, 0.20989276868939544, 0.2842622167172382]
maxi score, test score, baseline:  -0.9976578475336323 -1.0 -0.9976578475336323
probs:  [0.050153700461916494, 0.04875668801188751, 0.06639250718751007, 0.3405421189320523, 0.20989276868939544, 0.2842622167172382]
Printing some Q and Qe and total Qs values:  [[0.781]
 [0.835]
 [0.781]
 [0.781]
 [0.781]
 [0.781]
 [0.781]] [[ 9.843]
 [13.313]
 [ 9.843]
 [ 9.843]
 [ 9.843]
 [ 9.843]
 [ 9.843]] [[0.781]
 [0.835]
 [0.781]
 [0.781]
 [0.781]
 [0.781]
 [0.781]]
maxi score, test score, baseline:  -0.9976578475336323 -1.0 -0.9976578475336323
probs:  [0.050153700461916494, 0.04875668801188751, 0.06639250718751007, 0.3405421189320523, 0.20989276868939544, 0.2842622167172382]
maxi score, test score, baseline:  -0.9976578475336323 -1.0 -0.9976578475336323
probs:  [0.050153700461916494, 0.04875668801188751, 0.06639250718751007, 0.3405421189320523, 0.20989276868939544, 0.2842622167172382]
actor:  1 policy actor:  1  step number:  65 total reward:  0.21333333333333315  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.589]
 [0.626]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]] [[39.042]
 [58.698]
 [39.042]
 [39.042]
 [39.042]
 [39.042]
 [39.042]] [[1.611]
 [2.236]
 [1.611]
 [1.611]
 [1.611]
 [1.611]
 [1.611]]
printing an ep nov before normalisation:  33.96711389104469
maxi score, test score, baseline:  -0.9976628635346756 -1.0 -0.9976628635346756
maxi score, test score, baseline:  -0.9976628635346756 -1.0 -0.9976628635346756
probs:  [0.05302196679305577, 0.05174247847933647, 0.0678946733150181, 0.36147286093910247, 0.19932270333473645, 0.26654531713875085]
printing an ep nov before normalisation:  36.446287047454646
printing an ep nov before normalisation:  46.00975967571043
printing an ep nov before normalisation:  40.65457280095336
printing an ep nov before normalisation:  50.629500519926225
maxi score, test score, baseline:  -0.9976628635346756 -1.0 -0.9976628635346756
probs:  [0.05296414089232926, 0.05167924011122629, 0.06789976165750401, 0.3610291576215633, 0.19903696602140838, 0.2673907336959688]
maxi score, test score, baseline:  -0.9976628635346756 -1.0 -0.9976628635346756
probs:  [0.05296414089232926, 0.05167924011122629, 0.06789976165750401, 0.3610291576215633, 0.19903696602140838, 0.2673907336959688]
printing an ep nov before normalisation:  25.197686216109652
maxi score, test score, baseline:  -0.9976628635346756 -1.0 -0.9976628635346756
probs:  [0.053011125880979885, 0.05172508237786275, 0.06796002958822672, 0.3613501191550115, 0.19921386036602093, 0.2667397826318984]
maxi score, test score, baseline:  -0.9976628635346756 -1.0 -0.9976628635346756
maxi score, test score, baseline:  -0.9976628635346756 -1.0 -0.9976628635346756
probs:  [0.053011125880979885, 0.05172508237786275, 0.06796002958822672, 0.3613501191550115, 0.19921386036602093, 0.2667397826318984]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.418]
 [0.407]
 [0.405]
 [0.414]
 [0.411]
 [0.437]] [[36.397]
 [33.528]
 [23.822]
 [23.977]
 [24.065]
 [24.567]
 [35.07 ]] [[1.805]
 [1.633]
 [1.02 ]
 [1.028]
 [1.043]
 [1.07 ]
 [1.747]]
siam score:  -0.7468115
line 256 mcts: sample exp_bonus 30.367053335446855
maxi score, test score, baseline:  -0.9976678571428571 -1.0 -0.9976678571428571
maxi score, test score, baseline:  -0.9976678571428571 -1.0 -0.9976678571428571
probs:  [0.05305786615997279, 0.05177068419734992, 0.06801999805936222, 0.36166939700968165, 0.19938979661243428, 0.2660922579611992]
printing an ep nov before normalisation:  21.854563804238367
printing an ep nov before normalisation:  43.64951133728027
maxi score, test score, baseline:  -0.9976728285077952 -1.0 -0.9976728285077952
probs:  [0.05305786955108603, 0.05177068582444947, 0.0680200167757432, 0.36166940819831767, 0.1993897727559465, 0.26609224689445726]
printing an ep nov before normalisation:  36.984453201293945
printing an ep nov before normalisation:  48.18228362085377
printing an ep nov before normalisation:  20.874925819494194
actions average: 
K:  0  action  0 :  tensor([0.3770, 0.0060, 0.1409, 0.1398, 0.1202, 0.1173, 0.0989],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0267, 0.9004, 0.0178, 0.0138, 0.0075, 0.0096, 0.0241],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2588, 0.0089, 0.4078, 0.0668, 0.0727, 0.1411, 0.0438],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1692, 0.0048, 0.1746, 0.2097, 0.1367, 0.1483, 0.1568],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2126, 0.0025, 0.1277, 0.1331, 0.2874, 0.1183, 0.1183],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1004, 0.0017, 0.2284, 0.1116, 0.1069, 0.3530, 0.0980],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2432, 0.0243, 0.1283, 0.1299, 0.0904, 0.1069, 0.2770],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9976728285077952 -1.0 -0.9976728285077952
probs:  [0.05305786955108603, 0.05177068582444947, 0.0680200167757432, 0.36166940819831767, 0.1993897727559465, 0.26609224689445726]
actor:  1 policy actor:  1  step number:  69 total reward:  0.03999999999999948  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9976728285077952 -1.0 -0.9976728285077952
probs:  [0.05464869378071142, 0.05364272531869543, 0.06897367076132278, 0.3747924985303293, 0.19291862380466102, 0.25502378780428003]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9976728285077952 -1.0 -0.9976728285077952
probs:  [0.05469199380693213, 0.0536852263338619, 0.06902834869390066, 0.3750900793303483, 0.19227811198213907, 0.25522623985281795]
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.04024400006622
siam score:  -0.7381567
maxi score, test score, baseline:  -0.9976728285077952 -1.0 -0.9976728285077952
probs:  [0.05469199380693213, 0.0536852263338619, 0.06902834869390066, 0.3750900793303483, 0.19227811198213907, 0.25522623985281795]
printing an ep nov before normalisation:  48.51722448890333
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.412]
 [0.369]
 [0.332]
 [0.331]
 [0.322]
 [0.315]] [[35.86 ]
 [34.439]
 [36.241]
 [31.146]
 [31.997]
 [31.946]
 [29.097]] [[1.036]
 [1.025]
 [1.039]
 [0.839]
 [0.865]
 [0.855]
 [0.756]]
printing an ep nov before normalisation:  50.8124309400804
siam score:  -0.7397293
maxi score, test score, baseline:  -0.9976728285077952 -1.0 -0.9976728285077952
probs:  [0.05473496629704914, 0.05372740585691565, 0.06908261302402705, 0.37538540912813295, 0.19164244520857043, 0.2554271604853048]
maxi score, test score, baseline:  -0.9976728285077952 -1.0 -0.9976728285077952
maxi score, test score, baseline:  -0.9976728285077952 -1.0 -0.9976728285077952
probs:  [0.05473496629704914, 0.05372740585691565, 0.06908261302402705, 0.37538540912813295, 0.19164244520857043, 0.2554271604853048]
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.386]
 [0.399]
 [0.409]
 [0.381]
 [0.379]
 [0.381]] [[41.282]
 [49.833]
 [52.25 ]
 [53.435]
 [41.282]
 [48.502]
 [41.282]] [[0.943]
 [1.162]
 [1.237]
 [1.276]
 [0.943]
 [1.123]
 [0.943]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9976728285077952 -1.0 -0.9976728285077952
probs:  [0.05477997162790609, 0.05377158070890244, 0.06913944436250476, 0.3756947096882127, 0.19180029614538313, 0.2548139974670908]
printing an ep nov before normalisation:  44.239174323667555
maxi score, test score, baseline:  -0.9976728285077952 -1.0 -0.9976728285077952
probs:  [0.05477997162790609, 0.05377158070890244, 0.06913944436250476, 0.3756947096882127, 0.19180029614538313, 0.2548139974670908]
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.443]
 [0.443]
 [0.443]
 [0.443]
 [0.443]
 [0.443]] [[42.219]
 [35.915]
 [35.915]
 [35.915]
 [35.915]
 [35.915]
 [35.915]] [[1.025]
 [0.89 ]
 [0.89 ]
 [0.89 ]
 [0.89 ]
 [0.89 ]
 [0.89 ]]
printing an ep nov before normalisation:  53.661927630879156
printing an ep nov before normalisation:  10.34676950772905
maxi score, test score, baseline:  -0.9976728285077952 -1.0 -0.9976728285077952
probs:  [0.05520262528134663, 0.05380783027299129, 0.06935206207950086, 0.3759505733647186, 0.19137164613692334, 0.2543152628645193]
printing an ep nov before normalisation:  10.601029396057129
printing an ep nov before normalisation:  32.86820443377271
maxi score, test score, baseline:  -0.9976777777777778 -1.0 -0.9976777777777778
maxi score, test score, baseline:  -0.9976777777777778 -1.0 -0.9976777777777778
probs:  [0.05520263036181695, 0.0538078335358042, 0.06935208074504767, 0.3759505960359868, 0.19137161723206858, 0.25431524208927575]
maxi score, test score, baseline:  -0.9976777777777778 -1.0 -0.9976777777777778
printing an ep nov before normalisation:  21.299092769622803
printing an ep nov before normalisation:  29.327987584844166
printing an ep nov before normalisation:  25.505739283945402
Printing some Q and Qe and total Qs values:  [[0.244]
 [0.554]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]] [[28.918]
 [28.728]
 [28.918]
 [28.918]
 [28.918]
 [28.918]
 [28.918]] [[1.898]
 [2.19 ]
 [1.898]
 [1.898]
 [1.898]
 [1.898]
 [1.898]]
printing an ep nov before normalisation:  61.80351369798684
maxi score, test score, baseline:  -0.9976876106194691 -1.0 -0.9976876106194691
probs:  [0.05520264045518991, 0.05380784001803437, 0.0693521178279031, 0.37595064107699316, 0.19137155980672826, 0.2543152008151513]
actor:  1 policy actor:  1  step number:  61 total reward:  0.11999999999999944  reward:  1.0 rdn_beta:  2.0
using another actor
printing an ep nov before normalisation:  55.0480793723969
Printing some Q and Qe and total Qs values:  [[-0.003]
 [-0.004]
 [-0.003]
 [ 0.005]
 [-0.001]
 [ 0.   ]
 [-0.009]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.003]
 [-0.004]
 [-0.003]
 [ 0.005]
 [-0.001]
 [ 0.   ]
 [-0.009]]
maxi score, test score, baseline:  -0.9976924944812362 -1.0 -0.9976924944812362
maxi score, test score, baseline:  -0.9976924944812362 -1.0 -0.9976924944812362
probs:  [0.053888907360446284, 0.0525273795837655, 0.06770084735834489, 0.36698514865372767, 0.18680934315510406, 0.27208837388861157]
maxi score, test score, baseline:  -0.9976924944812362 -1.0 -0.9976924944812362
probs:  [0.053888907360446284, 0.0525273795837655, 0.06770084735834489, 0.36698514865372767, 0.18680934315510406, 0.27208837388861157]
siam score:  -0.73370516
printing an ep nov before normalisation:  61.70449761324137
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.569]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]] [[41.558]
 [50.457]
 [41.558]
 [41.558]
 [41.558]
 [41.558]
 [41.558]] [[1.486]
 [1.957]
 [1.486]
 [1.486]
 [1.486]
 [1.486]
 [1.486]]
maxi score, test score, baseline:  -0.9976924944812362 -1.0 -0.9976924944812362
maxi score, test score, baseline:  -0.9976973568281938 -1.0 -0.9976973568281938
probs:  [0.053917972307900984, 0.052548596520545825, 0.06780952131569637, 0.36713285465990814, 0.1860933360561355, 0.27249771913981313]
printing an ep nov before normalisation:  49.85384253963754
printing an ep nov before normalisation:  40.77991008758545
printing an ep nov before normalisation:  45.54820117899226
siam score:  -0.746494
maxi score, test score, baseline:  -0.9976973568281938 -1.0 -0.9976973568281938
maxi score, test score, baseline:  -0.9976973568281938 -1.0 -0.9976973568281938
probs:  [0.053964902505027536, 0.05259433186074824, 0.06786857266588577, 0.367453082350958, 0.18625559664688685, 0.2718635139704935]
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.475]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]] [[41.118]
 [54.633]
 [41.118]
 [41.118]
 [41.118]
 [41.118]
 [41.118]] [[1.296]
 [2.011]
 [1.296]
 [1.296]
 [1.296]
 [1.296]
 [1.296]]
Printing some Q and Qe and total Qs values:  [[0.22]
 [0.22]
 [0.22]
 [0.22]
 [0.22]
 [0.22]
 [0.22]] [[28.963]
 [28.963]
 [28.963]
 [28.963]
 [28.963]
 [28.963]
 [28.963]] [[1.374]
 [1.374]
 [1.374]
 [1.374]
 [1.374]
 [1.374]
 [1.374]]
siam score:  -0.7493997
printing an ep nov before normalisation:  21.3977564670877
maxi score, test score, baseline:  -0.9976973568281938 -1.0 -0.9976973568281938
printing an ep nov before normalisation:  37.1646321513233
printing an ep nov before normalisation:  31.037200732118603
maxi score, test score, baseline:  -0.9976973568281938 -1.0 -0.9976973568281938
probs:  [0.05401159128420607, 0.05263983192957615, 0.06792732024458763, 0.36777166272925826, 0.1864170225379936, 0.27123257127437833]
printing an ep nov before normalisation:  0.10205835764821813
printing an ep nov before normalisation:  42.14169639650544
printing an ep nov before normalisation:  27.31622906382703
printing an ep nov before normalisation:  26.254916226304616
maxi score, test score, baseline:  -0.9976973568281938 -1.0 -0.9976973568281938
probs:  [0.05415022759991571, 0.05277493852301939, 0.06810176361216731, 0.36871764612574043, 0.18689635592809148, 0.2693590682110656]
siam score:  -0.7448349
printing an ep nov before normalisation:  53.925211945061015
maxi score, test score, baseline:  -0.9976973568281938 -1.0 -0.9976973568281938
printing an ep nov before normalisation:  22.361252307891846
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]] [[51.719]
 [46.376]
 [46.376]
 [46.376]
 [46.376]
 [46.376]
 [46.376]] [[1.898]
 [1.668]
 [1.668]
 [1.668]
 [1.668]
 [1.668]
 [1.668]]
maxi score, test score, baseline:  -0.9976973568281938 -1.0 -0.9976973568281938
probs:  [0.05415022759991571, 0.05277493852301939, 0.06810176361216731, 0.36871764612574043, 0.18689635592809148, 0.2693590682110656]
printing an ep nov before normalisation:  34.815801333784016
printing an ep nov before normalisation:  24.994564056396484
printing an ep nov before normalisation:  23.680662935336443
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  17.650017738342285
maxi score, test score, baseline:  -0.9976973568281938 -1.0 -0.9976973568281938
probs:  [0.05415022759991571, 0.05277493852301939, 0.06810176361216731, 0.36871764612574043, 0.18689635592809148, 0.2693590682110656]
maxi score, test score, baseline:  -0.9976973568281938 -1.0 -0.9976973568281938
probs:  [0.05434842554120551, 0.05278623484719563, 0.06829472321242412, 0.3687977343897402, 0.1862964196696091, 0.26947646233982553]
printing an ep nov before normalisation:  64.05646229448676
printing an ep nov before normalisation:  58.31218515853762
UNIT TEST: sample policy line 217 mcts : [0.673 0.061 0.061 0.02  0.061 0.102 0.02 ]
printing an ep nov before normalisation:  78.46927055717977
printing an ep nov before normalisation:  17.03270840141684
maxi score, test score, baseline:  -0.9977021978021978 -1.0 -0.9977021978021978
probs:  [0.05443465932251647, 0.0528699819879735, 0.06840315153589886, 0.36938411284635336, 0.18584969144466387, 0.26905840286259386]
actions average: 
K:  0  action  0 :  tensor([0.4618, 0.0200, 0.1010, 0.1470, 0.1008, 0.0965, 0.0730],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0080, 0.9268, 0.0197, 0.0094, 0.0031, 0.0121, 0.0209],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2272, 0.0059, 0.1660, 0.2081, 0.1364, 0.1357, 0.1207],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1633, 0.0034, 0.1468, 0.2103, 0.1668, 0.1829, 0.1265],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2020, 0.0043, 0.1258, 0.1680, 0.2554, 0.1320, 0.1126],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.2234, 0.0145, 0.1275, 0.1551, 0.1027, 0.2846, 0.0922],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1976, 0.0198, 0.1416, 0.2012, 0.1320, 0.1493, 0.1585],
       grad_fn=<DivBackward0>)
siam score:  -0.7445975
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.463]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]] [[64.305]
 [60.519]
 [64.474]
 [64.474]
 [64.474]
 [64.474]
 [64.474]] [[2.063]
 [1.968]
 [2.063]
 [2.063]
 [2.063]
 [2.063]
 [2.063]]
maxi score, test score, baseline:  -0.9977021978021978 -1.0 -0.9977021978021978
probs:  [0.05443465932251647, 0.0528699819879735, 0.06840315153589886, 0.36938411284635336, 0.18584969144466387, 0.26905840286259386]
printing an ep nov before normalisation:  38.255988563576615
printing an ep nov before normalisation:  54.821603935214874
maxi score, test score, baseline:  -0.9977021978021978 -1.0 -0.9977021978021978
probs:  [0.05443465932251647, 0.0528699819879735, 0.06840315153589886, 0.36938411284635336, 0.18584969144466387, 0.26905840286259386]
printing an ep nov before normalisation:  14.666986509812308
actor:  1 policy actor:  1  step number:  65 total reward:  0.33333333333333337  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  65 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.369]
 [0.331]
 [0.342]
 [0.329]
 [0.451]
 [0.365]] [[33.181]
 [39.013]
 [30.495]
 [32.779]
 [26.893]
 [34.972]
 [35.895]] [[0.352]
 [0.369]
 [0.331]
 [0.342]
 [0.329]
 [0.451]
 [0.365]]
line 256 mcts: sample exp_bonus 68.18210202680473
maxi score, test score, baseline:  -0.9977021978021978 -1.0 -0.9977021978021978
probs:  [0.05064403916812924, 0.04937333165337342, 0.06387659213211068, 0.3449013545000258, 0.2150658346062633, 0.27613884794009763]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977021978021978 -1.0 -0.9977021978021978
probs:  [0.05064403916812924, 0.04937333165337342, 0.06387659213211068, 0.3449013545000258, 0.2150658346062633, 0.27613884794009763]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977070175438597 -1.0 -0.9977070175438597
probs:  [0.050687724948884, 0.049415916925093575, 0.06393173373787563, 0.3451995268287593, 0.21525163157761018, 0.27551346598177734]
printing an ep nov before normalisation:  47.3579085135421
maxi score, test score, baseline:  -0.9977070175438597 -1.0 -0.9977070175438597
probs:  [0.050687724948884, 0.049415916925093575, 0.06393173373787563, 0.3451995268287593, 0.21525163157761018, 0.27551346598177734]
maxi score, test score, baseline:  -0.9977070175438597 -1.0 -0.9977070175438597
probs:  [0.05073118609794283, 0.049458284714707305, 0.06398658061640747, 0.3454961765520814, 0.21543656651695034, 0.2748912055019106]
Printing some Q and Qe and total Qs values:  [[0.699]
 [0.504]
 [0.484]
 [0.488]
 [0.482]
 [0.461]
 [0.46 ]] [[46.808]
 [52.483]
 [37.393]
 [37.598]
 [37.956]
 [38.778]
 [37.931]] [[1.497]
 [1.468]
 [1.008]
 [1.018]
 [1.023]
 [1.025]
 [1.   ]]
maxi score, test score, baseline:  -0.9977070175438597 -1.0 -0.9977070175438597
probs:  [0.05073118609794283, 0.049458284714707305, 0.06398658061640747, 0.3454961765520814, 0.21543656651695034, 0.2748912055019106]
printing an ep nov before normalisation:  38.69177417742964
maxi score, test score, baseline:  -0.9977070175438597 -1.0 -0.9977070175438597
probs:  [0.05067163167340378, 0.04939362403804921, 0.06398020029258049, 0.3450426428624067, 0.2160377287896034, 0.27487417234395634]
printing an ep nov before normalisation:  86.13862889488999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977070175438597 -1.0 -0.9977070175438597
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.501]
 [0.45 ]
 [0.451]
 [0.461]
 [0.494]
 [0.457]] [[13.146]
 [29.891]
 [10.974]
 [11.136]
 [11.01 ]
 [10.302]
 [11.898]] [[0.658]
 [1.139]
 [0.603]
 [0.608]
 [0.615]
 [0.629]
 [0.633]]
printing an ep nov before normalisation:  0.004214597129248432
printing an ep nov before normalisation:  33.526859699606945
maxi score, test score, baseline:  -0.9977070175438597 -1.0 -0.9977070175438597
maxi score, test score, baseline:  -0.9977070175438597 -1.0 -0.9977070175438597
probs:  [0.050544446537101426, 0.04945415518066184, 0.06387217650712519, 0.34546646731785635, 0.21630303209456153, 0.2743597223626937]
maxi score, test score, baseline:  -0.9977070175438597 -1.0 -0.9977070175438597
probs:  [0.05060188378406777, 0.04933423189951008, 0.06377451372034584, 0.34462692747941903, 0.2167369462027545, 0.27492549691390294]
printing an ep nov before normalisation:  51.26094356537589
maxi score, test score, baseline:  -0.9977070175438597 -1.0 -0.9977070175438597
probs:  [0.05062997458098439, 0.049355088633712275, 0.06387777628344977, 0.3447721715842249, 0.21683197489058487, 0.2745330140270438]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.62 ]
 [0.495]
 [0.486]
 [0.491]
 [0.509]
 [0.503]] [[26.018]
 [46.999]
 [25.701]
 [25.941]
 [26.653]
 [26.528]
 [27.863]] [[0.8  ]
 [1.427]
 [0.789]
 [0.786]
 [0.808]
 [0.823]
 [0.848]]
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.595]
 [0.488]
 [0.487]
 [0.488]
 [0.554]
 [0.492]] [[31.96 ]
 [51.595]
 [31.81 ]
 [32.29 ]
 [33.224]
 [53.504]
 [33.598]] [[0.863]
 [1.429]
 [0.857]
 [0.867]
 [0.891]
 [1.433]
 [0.904]]
printing an ep nov before normalisation:  63.81973631917505
printing an ep nov before normalisation:  29.811955815355795
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977070175438597 -1.0 -0.9977070175438597
probs:  [0.050528401057403464, 0.04924955946579776, 0.06381730729306177, 0.34403249436113464, 0.2172460835754101, 0.2751261542471923]
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.724]
 [0.553]
 [0.563]
 [0.61 ]
 [0.598]
 [0.587]] [[26.472]
 [25.298]
 [25.044]
 [25.711]
 [22.899]
 [24.762]
 [21.397]] [[1.394]
 [1.548]
 [1.363]
 [1.412]
 [1.296]
 [1.391]
 [1.186]]
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.453]
 [0.557]
 [0.473]
 [0.404]
 [0.481]
 [0.453]] [[50.121]
 [50.121]
 [60.417]
 [41.185]
 [38.609]
 [38.053]
 [50.121]] [[1.531]
 [1.531]
 [1.978]
 [1.252]
 [1.097]
 [1.156]
 [1.531]]
maxi score, test score, baseline:  -0.9977070175438597 -1.0 -0.9977070175438597
probs:  [0.050528401057403464, 0.04924955946579776, 0.06381730729306177, 0.34403249436113464, 0.2172460835754101, 0.2751261542471923]
printing an ep nov before normalisation:  20.253611513674592
printing an ep nov before normalisation:  27.82085621039043
maxi score, test score, baseline:  -0.9977070175438597 -1.0 -0.9977070175438597
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977070175438597 -1.0 -0.9977070175438597
probs:  [0.05059663339676547, 0.049309526720453294, 0.06397142512161583, 0.3444515782463545, 0.21839180336679456, 0.27327903314801644]
siam score:  -0.73183525
maxi score, test score, baseline:  -0.9977070175438597 -1.0 -0.9977070175438597
probs:  [0.05059663339676547, 0.049309526720453294, 0.06397142512161583, 0.3444515782463545, 0.21839180336679456, 0.27327903314801644]
maxi score, test score, baseline:  -0.9977070175438597 -1.0 -0.9977070175438597
probs:  [0.05059663339676547, 0.049309526720453294, 0.06397142512161583, 0.3444515782463545, 0.21839180336679456, 0.27327903314801644]
maxi score, test score, baseline:  -0.9977070175438597 -1.0 -0.9977070175438597
probs:  [0.05059663339676547, 0.049309526720453294, 0.06397142512161583, 0.3444515782463545, 0.21839180336679456, 0.27327903314801644]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977070175438597 -1.0 -0.9977070175438597
probs:  [0.05059663339676547, 0.049309526720453294, 0.06397142512161583, 0.3444515782463545, 0.21839180336679456, 0.27327903314801644]
maxi score, test score, baseline:  -0.9977070175438597 -1.0 -0.9977070175438597
probs:  [0.0506412043138961, 0.04935296082323026, 0.06402780908850354, 0.3447556913796398, 0.21770205001317855, 0.2735202843815517]
maxi score, test score, baseline:  -0.9977070175438597 -1.0 -0.9977070175438597
probs:  [0.0506412043138961, 0.04935296082323026, 0.06402780908850354, 0.3447556913796398, 0.21770205001317855, 0.2735202843815517]
printing an ep nov before normalisation:  37.962782025460236
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977070175438597 -1.0 -0.9977070175438597
probs:  [0.0506412043138961, 0.04935296082323026, 0.06402780908850354, 0.3447556913796398, 0.21770205001317855, 0.2735202843815517]
maxi score, test score, baseline:  -0.9977070175438597 -1.0 -0.9977070175438597
printing an ep nov before normalisation:  43.9707011869914
maxi score, test score, baseline:  -0.9977070175438597 -1.0 -0.9977070175438597
probs:  [0.050683133396375875, 0.049393820473311804, 0.0640808510304327, 0.34504177893335414, 0.2178826642760315, 0.27291775189049405]
printing an ep nov before normalisation:  38.65982910833772
actor:  1 policy actor:  1  step number:  76 total reward:  0.1133333333333324  reward:  1.0 rdn_beta:  1.333
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[ 0.0297],
        [-0.4359],
        [-0.0000],
        [-0.6063],
        [-0.2648],
        [-0.6333],
        [-0.5568],
        [-0.4687],
        [-0.0000],
        [-0.6063]], dtype=torch.float64)
-0.032346567066 -0.002690417435770892
-0.058614567066 -0.49448983362116505
-0.924 -0.924
-0.032346567066 -0.6386733710740676
-0.09703970119800001 -0.36182358845566176
-0.032346567066 -0.6656791974161694
-0.09703970119800001 -0.6538840282537235
-0.032346567066 -0.5010498181160229
-0.20915399999999976 -0.20915399999999976
-0.032346567066 -0.6386399948507382
printing an ep nov before normalisation:  64.84917520583934
line 256 mcts: sample exp_bonus 48.63266114528465
siam score:  -0.73090124
maxi score, test score, baseline:  -0.9977070175438597 -1.0 -0.9977070175438597
probs:  [0.0524369497097764, 0.05140001580617737, 0.06508583177378893, 0.3591033142815729, 0.20960140993061113, 0.2623724784980734]
printing an ep nov before normalisation:  28.910319805145264
maxi score, test score, baseline:  -0.9977070175438597 -1.0 -0.9977070175438597
probs:  [0.0524369497097764, 0.05140001580617737, 0.06508583177378893, 0.3591033142815729, 0.20960140993061113, 0.2623724784980734]
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.473]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]] [[69.302]
 [78.217]
 [69.302]
 [69.302]
 [69.302]
 [69.302]
 [69.302]] [[1.587]
 [1.725]
 [1.587]
 [1.587]
 [1.587]
 [1.587]
 [1.587]]
printing an ep nov before normalisation:  38.45581887011957
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.39 ]
 [0.46 ]
 [0.464]
 [0.465]
 [0.458]
 [0.467]] [[17.69 ]
 [15.893]
 [10.442]
 [10.232]
 [10.165]
 [10.26 ]
 [10.056]] [[1.45 ]
 [1.098]
 [0.788]
 [0.778]
 [0.774]
 [0.773]
 [0.768]]
printing an ep nov before normalisation:  24.231257438659668
maxi score, test score, baseline:  -0.9977070175438597 -1.0 -0.9977070175438597
probs:  [0.05233916442520206, 0.05129895222065114, 0.06502803634967422, 0.35839493034569814, 0.21000050530328665, 0.26293841135548773]
actor:  1 policy actor:  1  step number:  61 total reward:  0.33333333333333326  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  41.28535397321676
printing an ep nov before normalisation:  24.191279952340246
using explorer policy with actor:  1
printing an ep nov before normalisation:  12.207360881120621
maxi score, test score, baseline:  -0.9977070175438597 -1.0 -0.9977070175438597
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9977070175438597 -1.0 -0.9977070175438597
probs:  [0.05461841033109346, 0.05365461099434158, 0.06637517139526056, 0.3749064248171812, 0.2006981189760326, 0.24974726348609064]
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]] [[56.653]
 [56.653]
 [56.653]
 [56.653]
 [56.653]
 [56.653]
 [56.653]] [[1.553]
 [1.553]
 [1.553]
 [1.553]
 [1.553]
 [1.553]
 [1.553]]
printing an ep nov before normalisation:  70.48981571891065
printing an ep nov before normalisation:  33.52897471833526
maxi score, test score, baseline:  -0.9977070175438597 -1.0 -0.9977070175438597
maxi score, test score, baseline:  -0.9977070175438597 -1.0 -0.9977070175438597
probs:  [0.05461841033109346, 0.05365461099434158, 0.06637517139526056, 0.3749064248171812, 0.2006981189760326, 0.24974726348609064]
printing an ep nov before normalisation:  0.004895705755814106
maxi score, test score, baseline:  -0.9977118161925602 -1.0 -0.9977118161925602
probs:  [0.05456523681819403, 0.05359762629738148, 0.06636848416338906, 0.37450673094883463, 0.20122233778639248, 0.24973958398580834]
printing an ep nov before normalisation:  58.392613940417206
using another actor
siam score:  -0.73689055
maxi score, test score, baseline:  -0.9977118161925602 -1.0 -0.9977118161925602
probs:  [0.05451223860724862, 0.05354082144405338, 0.06636192069104074, 0.3741082974554572, 0.20174629809776742, 0.24973042370443263]
printing an ep nov before normalisation:  22.882094383239746
using explorer policy with actor:  1
printing an ep nov before normalisation:  20.73448753580019
maxi score, test score, baseline:  -0.9977118161925602 -1.0 -0.9977118161925602
probs:  [0.05451223860724862, 0.05354082144405338, 0.06636192069104074, 0.3741082974554572, 0.20174629809776742, 0.24973042370443263]
printing an ep nov before normalisation:  65.04505071159079
maxi score, test score, baseline:  -0.9977165938864629 -1.0 -0.9977165938864629
probs:  [0.054501340122980274, 0.05352536568118107, 0.06640660957509772, 0.37399940220188077, 0.20165489829751598, 0.24991238412134417]
maxi score, test score, baseline:  -0.9977165938864629 -1.0 -0.9977165938864629
probs:  [0.054501340122980274, 0.05352536568118107, 0.06640660957509772, 0.37399940220188077, 0.20165489829751598, 0.24991238412134417]
Printing some Q and Qe and total Qs values:  [[0.072]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]] [[33.253]
 [26.411]
 [26.411]
 [26.411]
 [26.411]
 [26.411]
 [26.411]] [[1.518]
 [1.236]
 [1.236]
 [1.236]
 [1.236]
 [1.236]
 [1.236]]
maxi score, test score, baseline:  -0.9977165938864629 -1.0 -0.9977165938864629
probs:  [0.054501340122980274, 0.05352536568118107, 0.06640660957509772, 0.37399940220188077, 0.20165489829751598, 0.24991238412134417]
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.337181063840404
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]] [[44.353]
 [43.545]
 [43.545]
 [43.545]
 [43.545]
 [43.545]
 [43.545]] [[1.824]
 [1.776]
 [1.776]
 [1.776]
 [1.776]
 [1.776]
 [1.776]]
printing an ep nov before normalisation:  39.67328799068966
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]] [[46.247]
 [46.247]
 [46.247]
 [46.247]
 [46.247]
 [46.247]
 [46.247]] [[2.189]
 [2.189]
 [2.189]
 [2.189]
 [2.189]
 [2.189]
 [2.189]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.556]
 [0.537]
 [0.484]
 [0.553]
 [0.592]
 [0.458]
 [0.521]] [[32.163]
 [30.587]
 [31.727]
 [33.719]
 [45.156]
 [35.417]
 [28.318]] [[1.191]
 [1.102]
 [1.101]
 [1.257]
 [1.801]
 [1.237]
 [0.987]]
siam score:  -0.72807497
maxi score, test score, baseline:  -0.9977213507625272 -1.0 -0.9977213507625272
probs:  [0.05448778656608926, 0.05350730175888271, 0.0664480711042663, 0.37387224380626766, 0.2023211441699698, 0.2493634525945243]
actor:  1 policy actor:  1  step number:  65 total reward:  0.23999999999999988  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  62.014972943902485
printing an ep nov before normalisation:  30.400478351146862
printing an ep nov before normalisation:  41.4556381516458
Printing some Q and Qe and total Qs values:  [[0.215]
 [0.31 ]
 [0.18 ]
 [0.18 ]
 [0.18 ]
 [0.18 ]
 [0.18 ]] [[35.986]
 [33.173]
 [30.769]
 [30.769]
 [30.769]
 [30.769]
 [30.769]] [[1.727]
 [1.638]
 [1.35 ]
 [1.35 ]
 [1.35 ]
 [1.35 ]
 [1.35 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977213507625272 -1.0 -0.9977213507625272
probs:  [0.056249595082945, 0.055328926559306206, 0.06748022040944587, 0.3866406400538297, 0.19506410611805242, 0.23923651177642083]
Printing some Q and Qe and total Qs values:  [[0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.537]] [[34.122]
 [34.122]
 [34.122]
 [34.122]
 [34.122]
 [34.122]
 [34.122]] [[2.204]
 [2.204]
 [2.204]
 [2.204]
 [2.204]
 [2.204]
 [2.204]]
Printing some Q and Qe and total Qs values:  [[-0.024]
 [ 0.023]
 [-0.022]
 [-0.021]
 [-0.024]
 [-0.022]
 [-0.02 ]] [[21.662]
 [26.716]
 [23.639]
 [23.744]
 [23.614]
 [23.328]
 [25.183]] [[1.11 ]
 [1.757]
 [1.347]
 [1.361]
 [1.342]
 [1.31 ]
 [1.533]]
maxi score, test score, baseline:  -0.9977213507625272 -1.0 -0.9977213507625272
probs:  [0.056249595082945, 0.055328926559306206, 0.06748022040944587, 0.3866406400538297, 0.19506410611805242, 0.23923651177642083]
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]] [[29.44 ]
 [20.476]
 [20.476]
 [20.476]
 [20.476]
 [20.476]
 [20.476]] [[1.572]
 [1.031]
 [1.031]
 [1.031]
 [1.031]
 [1.031]
 [1.031]]
maxi score, test score, baseline:  -0.9977213507625272 -1.0 -0.9977213507625272
probs:  [0.056287179099929294, 0.05536589395724586, 0.06752532615361191, 0.38689950394003864, 0.19519466134995278, 0.23872743549922157]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
Printing some Q and Qe and total Qs values:  [[-0.009]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[45.271]
 [45.294]
 [45.294]
 [45.294]
 [45.294]
 [45.294]
 [45.294]] [[1.762]
 [1.771]
 [1.771]
 [1.771]
 [1.771]
 [1.771]
 [1.771]]
printing an ep nov before normalisation:  27.505583346289715
actions average: 
K:  2  action  0 :  tensor([0.4309, 0.0147, 0.1243, 0.1123, 0.1068, 0.0956, 0.1156],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0206, 0.9115, 0.0126, 0.0140, 0.0078, 0.0052, 0.0283],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0752, 0.1145, 0.4083, 0.0907, 0.0904, 0.1105, 0.1105],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2054, 0.0781, 0.1158, 0.1961, 0.1306, 0.0927, 0.1812],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1475, 0.0481, 0.1763, 0.1604, 0.1763, 0.1430, 0.1484],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1159, 0.0401, 0.1562, 0.0877, 0.0930, 0.3939, 0.1132],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1515, 0.0917, 0.1528, 0.1344, 0.1570, 0.1258, 0.1867],
       grad_fn=<DivBackward0>)
actions average: 
K:  0  action  0 :  tensor([0.4607, 0.0029, 0.1254, 0.1055, 0.1090, 0.0919, 0.1046],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0078, 0.9384, 0.0128, 0.0094, 0.0065, 0.0038, 0.0213],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2837, 0.0020, 0.2126, 0.1172, 0.1234, 0.1427, 0.1184],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2106, 0.0116, 0.1336, 0.2361, 0.1556, 0.1314, 0.1210],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2436, 0.0141, 0.0944, 0.1166, 0.3875, 0.0719, 0.0719],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0948, 0.0063, 0.1965, 0.1089, 0.0972, 0.4105, 0.0858],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1801, 0.0801, 0.1362, 0.1242, 0.0937, 0.0861, 0.2996],
       grad_fn=<DivBackward0>)
siam score:  -0.7185223
maxi score, test score, baseline:  -0.9977213507625272 -1.0 -0.9977213507625272
probs:  [0.05636520203336232, 0.05544263681391929, 0.06761896389531427, 0.3874368951340222, 0.19474324363303222, 0.23839305849034967]
printing an ep nov before normalisation:  22.8641939163208
maxi score, test score, baseline:  -0.9977213507625272 -1.0 -0.9977213507625272
probs:  [0.05636520203336232, 0.05544263681391929, 0.06761896389531427, 0.3874368951340222, 0.19474324363303222, 0.23839305849034967]
maxi score, test score, baseline:  -0.9977213507625272 -1.0 -0.9977213507625272
probs:  [0.05627776670693219, 0.05535221406321321, 0.067567970178294, 0.38680308837677646, 0.19510390011845424, 0.23889506055632997]
printing an ep nov before normalisation:  29.351928234100342
maxi score, test score, baseline:  -0.9977213507625272 -1.0 -0.9977213507625272
probs:  [0.05627776670693219, 0.05535221406321321, 0.067567970178294, 0.38680308837677646, 0.19510390011845424, 0.23889506055632997]
maxi score, test score, baseline:  -0.9977213507625272 -1.0 -0.9977213507625272
probs:  [0.056352229272887494, 0.05542544910063532, 0.06765740655152083, 0.38731591527138176, 0.1953624829946617, 0.23788651680891298]
printing an ep nov before normalisation:  24.38697393599133
printing an ep nov before normalisation:  42.9167509100466
maxi score, test score, baseline:  -0.9977213507625272 -1.0 -0.9977213507625272
probs:  [0.056352229272887494, 0.05542544910063532, 0.06765740655152083, 0.38731591527138176, 0.1953624829946617, 0.23788651680891298]
actor:  1 policy actor:  1  step number:  71 total reward:  0.07999999999999918  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9977213507625272 -1.0 -0.9977213507625272
probs:  [0.055153812668691196, 0.05424678860553138, 0.08751788946588238, 0.37906236857058234, 0.1912007943873448, 0.23281834630196782]
actions average: 
K:  4  action  0 :  tensor([0.3907, 0.0609, 0.0993, 0.0977, 0.1778, 0.0876, 0.0859],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0527, 0.7294, 0.0472, 0.0487, 0.0443, 0.0377, 0.0400],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1708, 0.0988, 0.1619, 0.1566, 0.1435, 0.1385, 0.1299],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1771, 0.0880, 0.1434, 0.2282, 0.1302, 0.1126, 0.1204],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2048, 0.0656, 0.1310, 0.1210, 0.2416, 0.1127, 0.1234],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1901, 0.0089, 0.2006, 0.1181, 0.1100, 0.2706, 0.1017],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2061, 0.0886, 0.1478, 0.1401, 0.0974, 0.1432, 0.1767],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9977213507625272 -1.0 -0.9977213507625272
probs:  [0.055153812668691196, 0.05424678860553138, 0.08751788946588238, 0.37906236857058234, 0.1912007943873448, 0.23281834630196782]
printing an ep nov before normalisation:  27.39444834627247
maxi score, test score, baseline:  -0.9977213507625272 -1.0 -0.9977213507625272
probs:  [0.055153812668691196, 0.05424678860553138, 0.08751788946588238, 0.37906236857058234, 0.1912007943873448, 0.23281834630196782]
maxi score, test score, baseline:  -0.9977213507625272 -1.0 -0.9977213507625272
probs:  [0.05519261240837304, 0.0542849487246612, 0.08757951189517849, 0.379329584046671, 0.19063091036496085, 0.23298243256015552]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977213507625272 -1.0 -0.9977213507625272
probs:  [0.05519261240837304, 0.0542849487246612, 0.08757951189517849, 0.379329584046671, 0.19063091036496085, 0.23298243256015552]
actor:  1 policy actor:  1  step number:  63 total reward:  0.13333333333333242  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9977213507625272 -1.0 -0.9977213507625272
probs:  [0.05650973872801658, 0.05564450799633173, 0.08738256226699241, 0.38885889774631877, 0.18561634477829525, 0.2259879484840451]
printing an ep nov before normalisation:  45.17643423041591
printing an ep nov before normalisation:  27.230458686837046
printing an ep nov before normalisation:  28.09931335058948
maxi score, test score, baseline:  -0.9977213507625272 -1.0 -0.9977213507625272
Printing some Q and Qe and total Qs values:  [[0.29 ]
 [0.321]
 [0.293]
 [0.301]
 [0.336]
 [0.294]
 [0.292]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.29 ]
 [0.321]
 [0.293]
 [0.301]
 [0.336]
 [0.294]
 [0.292]]
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.357]
 [0.354]
 [0.53 ]
 [0.344]
 [0.281]
 [0.281]] [[44.315]
 [45.885]
 [41.868]
 [38.668]
 [46.152]
 [37.329]
 [37.329]] [[0.332]
 [0.357]
 [0.354]
 [0.53 ]
 [0.344]
 [0.281]
 [0.281]]
actor:  1 policy actor:  1  step number:  70 total reward:  0.05999999999999972  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  34.63009595870972
printing an ep nov before normalisation:  59.92966727240732
using explorer policy with actor:  1
printing an ep nov before normalisation:  65.35814418405742
printing an ep nov before normalisation:  69.20282752782295
printing an ep nov before normalisation:  35.055324144273555
Printing some Q and Qe and total Qs values:  [[0.618]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]] [[41.868]
 [41.4  ]
 [41.4  ]
 [41.4  ]
 [41.4  ]
 [41.4  ]
 [41.4  ]] [[1.861]
 [1.796]
 [1.796]
 [1.796]
 [1.796]
 [1.796]
 [1.796]]
Printing some Q and Qe and total Qs values:  [[0.232]
 [0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.036]] [[46.309]
 [48.674]
 [48.674]
 [48.674]
 [48.674]
 [48.674]
 [48.674]] [[1.616]
 [1.523]
 [1.523]
 [1.523]
 [1.523]
 [1.523]
 [1.523]]
maxi score, test score, baseline:  -0.9977260869565218 -1.0 -0.9977260869565218
probs:  [0.055248462161504516, 0.05439456691487534, 0.08571679977608931, 0.3801048793610863, 0.20263393839838145, 0.22190135338806316]
maxi score, test score, baseline:  -0.9977260869565218 -1.0 -0.9977260869565218
probs:  [0.05506304401079148, 0.054352795598066904, 0.08580034621736393, 0.3798118068004829, 0.20244224399544142, 0.2225297633778533]
printing an ep nov before normalisation:  36.91291809082031
Printing some Q and Qe and total Qs values:  [[0.294]
 [0.286]
 [0.289]
 [0.29 ]
 [0.292]
 [0.293]
 [0.29 ]] [[39.864]
 [41.484]
 [38.189]
 [39.28 ]
 [40.468]
 [40.379]
 [40.144]] [[0.294]
 [0.286]
 [0.289]
 [0.29 ]
 [0.292]
 [0.293]
 [0.29 ]]
printing an ep nov before normalisation:  52.906700185045324
maxi score, test score, baseline:  -0.9977354978354979 -1.0 -0.9977354978354979
probs:  [0.05506304673804388, 0.05435279640135292, 0.08580039710976335, 0.3798118121448059, 0.20244218935548905, 0.22252975825054477]
printing an ep nov before normalisation:  22.66993326677553
maxi score, test score, baseline:  -0.9977354978354979 -1.0 -0.9977354978354979
probs:  [0.05513681121740331, 0.05442560709556771, 0.08591543834276283, 0.38032167715405796, 0.20197621147260622, 0.22222425471760213]
Printing some Q and Qe and total Qs values:  [[-0.004]
 [-0.006]
 [ 0.011]
 [ 0.012]
 [ 0.009]
 [ 0.006]
 [ 0.007]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.004]
 [-0.006]
 [ 0.011]
 [ 0.012]
 [ 0.009]
 [ 0.006]
 [ 0.007]]
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.018]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.018]]
maxi score, test score, baseline:  -0.9977354978354979 -1.0 -0.9977354978354979
probs:  [0.05513681121740331, 0.05442560709556771, 0.08591543834276283, 0.38032167715405796, 0.20197621147260622, 0.22222425471760213]
printing an ep nov before normalisation:  50.91758281035011
maxi score, test score, baseline:  -0.9977354978354979 -1.0 -0.9977354978354979
probs:  [0.05499925453722344, 0.05443351675760213, 0.08592793564529322, 0.3803770654448833, 0.2020056172526264, 0.22225661036237154]
printing an ep nov before normalisation:  39.252127364648004
maxi score, test score, baseline:  -0.9977354978354979 -1.0 -0.9977354978354979
probs:  [0.05499925453722344, 0.05443351675760213, 0.08592793564529322, 0.3803770654448833, 0.2020056172526264, 0.22225661036237154]
maxi score, test score, baseline:  -0.9977354978354979 -1.0 -0.9977354978354979
probs:  [0.05499925453722344, 0.05443351675760213, 0.08592793564529322, 0.3803770654448833, 0.2020056172526264, 0.22225661036237154]
maxi score, test score, baseline:  -0.9977354978354979 -1.0 -0.9977354978354979
probs:  [0.05499925453722344, 0.05443351675760213, 0.08592793564529322, 0.3803770654448833, 0.2020056172526264, 0.22225661036237154]
maxi score, test score, baseline:  -0.9977354978354979 -1.0 -0.9977354978354979
probs:  [0.05501166679622634, 0.054445801030597464, 0.08572129719445207, 0.3804630874370091, 0.20205128654011778, 0.22230686100159713]
siam score:  -0.72913855
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977401727861771 -1.0 -0.9977401727861771
printing an ep nov before normalisation:  65.39642491456128
UNIT TEST: sample policy line 217 mcts : [0.082 0.143 0.163 0.122 0.163 0.224 0.102]
maxi score, test score, baseline:  -0.9977401727861771 -1.0 -0.9977401727861771
printing an ep nov before normalisation:  25.614284485484053
printing an ep nov before normalisation:  33.110425123354474
maxi score, test score, baseline:  -0.9977401727861771 -1.0 -0.9977401727861771
probs:  [0.05505188135861788, 0.054485600180226176, 0.08578403887042314, 0.38074178528259606, 0.20146703326973947, 0.22246966103839727]
printing an ep nov before normalisation:  60.37592091570353
maxi score, test score, baseline:  -0.9977401727861771 -1.0 -0.9977401727861771
probs:  [0.05505188135861788, 0.054485600180226176, 0.08578403887042314, 0.38074178528259606, 0.20146703326973947, 0.22246966103839727]
printing an ep nov before normalisation:  17.113855345772144
maxi score, test score, baseline:  -0.9977448275862069 -1.0 -0.9977448275862069
probs:  [0.05496411079419129, 0.05439607346349374, 0.08579155754783631, 0.3801142996783334, 0.20183310050398215, 0.2229008580121631]
maxi score, test score, baseline:  -0.9977448275862069 -1.0 -0.9977448275862069
probs:  [0.05496411079419129, 0.05439607346349374, 0.08579155754783631, 0.3801142996783334, 0.20183310050398215, 0.2229008580121631]
actor:  1 policy actor:  1  step number:  78 total reward:  0.05999999999999894  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9977448275862069 -1.0 -0.9977448275862069
probs:  [0.05390164849525584, 0.05334461806803605, 0.08413174941871246, 0.3727513769141812, 0.21728633478548637, 0.21858427231832803]
maxi score, test score, baseline:  -0.9977448275862069 -1.0 -0.9977448275862069
probs:  [0.0539443441749058, 0.05338687142872063, 0.08419844979597052, 0.3730472603464127, 0.21666533737117832, 0.21875773688281205]
maxi score, test score, baseline:  -0.9977448275862069 -1.0 -0.9977448275862069
probs:  [0.0539443441749058, 0.05338687142872063, 0.08419844979597052, 0.3730472603464127, 0.21666533737117832, 0.21875773688281205]
maxi score, test score, baseline:  -0.9977494623655914 -1.0 -0.9977494623655914
probs:  [0.05394434446264737, 0.053386870981273524, 0.08419847299378017, 0.37304725707660874, 0.2166653235477481, 0.21875773093794215]
line 256 mcts: sample exp_bonus 34.83954420288271
maxi score, test score, baseline:  -0.9977494623655914 -1.0 -0.9977494623655914
probs:  [0.05394434446264737, 0.053386870981273524, 0.08419847299378017, 0.37304725707660874, 0.2166653235477481, 0.21875773093794215]
printing an ep nov before normalisation:  33.02758233959319
from probs:  [0.05394434446264737, 0.053386870981273524, 0.08419847299378017, 0.37304725707660874, 0.2166653235477481, 0.21875773093794215]
maxi score, test score, baseline:  -0.9977540772532189 -1.0 -0.9977540772532189
probs:  [0.05398674865711304, 0.05342883514629254, 0.08426474069104029, 0.37334111525080116, 0.21604855609664658, 0.2189300041581064]
printing an ep nov before normalisation:  14.627265047352646
maxi score, test score, baseline:  -0.9977540772532189 -1.0 -0.9977540772532189
probs:  [0.05398674865711304, 0.05342883514629254, 0.08426474069104029, 0.37334111525080116, 0.21604855609664658, 0.2189300041581064]
printing an ep nov before normalisation:  47.081623586827774
maxi score, test score, baseline:  -0.9977540772532189 -1.0 -0.9977540772532189
probs:  [0.05398674865711304, 0.05342883514629254, 0.08426474069104029, 0.37334111525080116, 0.21604855609664658, 0.2189300041581064]
printing an ep nov before normalisation:  37.362260818481445
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9977540772532189 -1.0 -0.9977540772532189
probs:  [0.05398674865711304, 0.05342883514629254, 0.08426474069104029, 0.37334111525080116, 0.21604855609664658, 0.2189300041581064]
Printing some Q and Qe and total Qs values:  [[0.712]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]] [[40.88 ]
 [34.649]
 [34.649]
 [34.649]
 [34.649]
 [34.649]
 [34.649]] [[2.002]
 [1.629]
 [1.629]
 [1.629]
 [1.629]
 [1.629]
 [1.629]]
Printing some Q and Qe and total Qs values:  [[0.204]
 [0.249]
 [0.199]
 [0.213]
 [0.21 ]
 [0.206]
 [0.213]] [[23.006]
 [29.82 ]
 [21.532]
 [21.731]
 [21.891]
 [22.249]
 [23.402]] [[0.822]
 [1.231]
 [0.739]
 [0.763]
 [0.768]
 [0.783]
 [0.852]]
maxi score, test score, baseline:  -0.9977540772532189 -1.0 -0.9977540772532189
printing an ep nov before normalisation:  45.103420967301716
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.774]
 [0.581]
 [0.451]
 [0.56 ]
 [0.535]
 [0.569]] [[35.132]
 [26.758]
 [27.455]
 [37.93 ]
 [33.62 ]
 [34.881]
 [34.786]] [[1.651]
 [1.611]
 [1.439]
 [1.637]
 [1.611]
 [1.626]
 [1.657]]
actor:  1 policy actor:  1  step number:  68 total reward:  0.2199999999999993  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9977586723768737 -1.0 -0.9977586723768737
probs:  [0.05544365164057924, 0.054913403479382304, 0.08422022785054448, 0.3837460306076162, 0.20946905864064003, 0.2122076277812377]
maxi score, test score, baseline:  -0.9977586723768737 -1.0 -0.9977586723768737
probs:  [0.05547482261651802, 0.054944275626434703, 0.08426761627493798, 0.38396222105458644, 0.20958703282348282, 0.2117640316040401]
printing an ep nov before normalisation:  68.55473726660755
using explorer policy with actor:  0
printing an ep nov before normalisation:  52.523403605342445
printing an ep nov before normalisation:  79.10612284559846
maxi score, test score, baseline:  -0.9977586723768737 -1.0 -0.9977586723768737
probs:  [0.055577934031674756, 0.05504639853646976, 0.08442437387889819, 0.38467736398030866, 0.20923340490663128, 0.21104052466601747]
printing an ep nov before normalisation:  57.80381931697477
printing an ep nov before normalisation:  25.478426234376663
printing an ep nov before normalisation:  18.657584968841014
siam score:  -0.72516614
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9977632478632479 -1.0 -0.9977632478632479
probs:  [0.05557793379778703, 0.05504639760033689, 0.08442439576527228, 0.3846773572931679, 0.20923339420944106, 0.21104052133399492]
maxi score, test score, baseline:  -0.9977632478632479 -1.0 -0.9977632478632479
probs:  [0.05549208792390573, 0.05495890968644355, 0.0844276633797739, 0.3840641770451551, 0.20962222610026449, 0.21143493586445722]
maxi score, test score, baseline:  -0.9977632478632479 -1.0 -0.9977632478632479
probs:  [0.05536397751249867, 0.054966351337107644, 0.08443910491076331, 0.3841162891185174, 0.20965066065625507, 0.2114636164648579]
printing an ep nov before normalisation:  73.1349974745261
maxi score, test score, baseline:  -0.9977632478632479 -1.0 -0.9977632478632479
probs:  [0.05527794778030692, 0.05487910001903626, 0.08444239969309444, 0.3835047669728317, 0.2100386299845106, 0.21185715555022014]
Printing some Q and Qe and total Qs values:  [[0.044]
 [0.044]
 [0.044]
 [0.082]
 [0.069]
 [0.044]
 [0.044]] [[20.616]
 [20.616]
 [20.616]
 [25.406]
 [27.704]
 [20.616]
 [20.616]] [[0.044]
 [0.044]
 [0.044]
 [0.082]
 [0.069]
 [0.044]
 [0.044]]
siam score:  -0.72398895
maxi score, test score, baseline:  -0.997767803837953 -1.0 -0.997767803837953
maxi score, test score, baseline:  -0.997767803837953 -1.0 -0.997767803837953
probs:  [0.055349760906216745, 0.054950393210220945, 0.08455221531975649, 0.38400401507403287, 0.2095686746120748, 0.21157494087769826]
siam score:  -0.72291946
maxi score, test score, baseline:  -0.997767803837953 -1.0 -0.997767803837953
maxi score, test score, baseline:  -0.997767803837953 -1.0 -0.997767803837953
probs:  [0.055349760906216745, 0.054950393210220945, 0.08455221531975649, 0.38400401507403287, 0.2095686746120748, 0.21157494087769826]
printing an ep nov before normalisation:  48.05686950683594
maxi score, test score, baseline:  -0.997767803837953 -1.0 -0.997767803837953
maxi score, test score, baseline:  -0.997767803837953 -1.0 -0.997767803837953
probs:  [0.05538041942784062, 0.0549808299862512, 0.08459908826145979, 0.38421715613847673, 0.209684961903651, 0.2111375442823207]
maxi score, test score, baseline:  -0.997767803837953 -1.0 -0.997767803837953
probs:  [0.05538041942784062, 0.0549808299862512, 0.08459908826145979, 0.38421715613847673, 0.209684961903651, 0.2111375442823207]
maxi score, test score, baseline:  -0.997767803837953 -1.0 -0.997767803837953
probs:  [0.05538041942784062, 0.0549808299862512, 0.08459908826145979, 0.38421715613847673, 0.209684961903651, 0.2111375442823207]
siam score:  -0.72307676
printing an ep nov before normalisation:  45.845400113160736
maxi score, test score, baseline:  -0.997767803837953 -1.0 -0.997767803837953
maxi score, test score, baseline:  -0.997767803837953 -1.0 -0.997767803837953
probs:  [0.05533563613164154, 0.05493452673116171, 0.08466544697963141, 0.383892382934683, 0.20948677706208715, 0.21168523016079524]
printing an ep nov before normalisation:  24.53821703146439
actor:  1 policy actor:  1  step number:  45 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.997767803837953 -1.0 -0.997767803837953
probs:  [0.05396320890691963, 0.05370331271354121, 0.08276627205997855, 0.37527051351440466, 0.204783256782857, 0.22951343602229898]
printing an ep nov before normalisation:  48.99085372849849
maxi score, test score, baseline:  -0.997767803837953 -1.0 -0.997767803837953
probs:  [0.05396320890691963, 0.05370331271354121, 0.08276627205997855, 0.37527051351440466, 0.204783256782857, 0.22951343602229898]
maxi score, test score, baseline:  -0.997767803837953 -1.0 -0.997767803837953
probs:  [0.05396320890691963, 0.05370331271354121, 0.08276627205997855, 0.37527051351440466, 0.204783256782857, 0.22951343602229898]
maxi score, test score, baseline:  -0.997767803837953 -1.0 -0.997767803837953
probs:  [0.05396320890691963, 0.05370331271354121, 0.08276627205997855, 0.37527051351440466, 0.204783256782857, 0.22951343602229898]
printing an ep nov before normalisation:  38.37611662470832
maxi score, test score, baseline:  -0.997767803837953 -1.0 -0.997767803837953
probs:  [0.05402866791297457, 0.053768455677386674, 0.08286675652600685, 0.3757266926350835, 0.20503211786396172, 0.22857730938458665]
Printing some Q and Qe and total Qs values:  [[0.788]
 [0.966]
 [0.718]
 [0.719]
 [0.689]
 [0.689]
 [0.694]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.788]
 [0.966]
 [0.718]
 [0.719]
 [0.689]
 [0.689]
 [0.694]]
maxi score, test score, baseline:  -0.997772340425532 -1.0 -0.997772340425532
probs:  [0.05402866881373529, 0.053768456229900584, 0.08286678031787596, 0.37572669637480305, 0.20503211296150287, 0.22857728530218235]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.997772340425532 -1.0 -0.997772340425532
probs:  [0.05402866881373529, 0.053768456229900584, 0.08286678031787596, 0.37572669637480305, 0.20503211296150287, 0.22857728530218235]
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.82360205283448
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.634]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]] [[41.951]
 [53.509]
 [41.951]
 [41.951]
 [41.951]
 [41.951]
 [41.951]] [[1.569]
 [2.168]
 [1.569]
 [1.569]
 [1.569]
 [1.569]
 [1.569]]
printing an ep nov before normalisation:  36.151195656545205
siam score:  -0.72031265
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]] [[41.78 ]
 [40.945]
 [40.945]
 [40.945]
 [40.945]
 [40.945]
 [40.945]] [[2.628]
 [2.544]
 [2.544]
 [2.544]
 [2.544]
 [2.544]
 [2.544]]
Printing some Q and Qe and total Qs values:  [[0.71 ]
 [0.675]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]] [[34.275]
 [30.746]
 [32.177]
 [32.177]
 [32.177]
 [32.177]
 [32.177]] [[2.056]
 [1.786]
 [1.797]
 [1.797]
 [1.797]
 [1.797]
 [1.797]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.997772340425532 -1.0 -0.997772340425532
probs:  [0.05402866881373529, 0.053768456229900584, 0.08286678031787596, 0.37572669637480305, 0.20503211296150287, 0.22857728530218235]
printing an ep nov before normalisation:  25.947885513305664
printing an ep nov before normalisation:  25.137431621551514
siam score:  -0.720914
maxi score, test score, baseline:  -0.997772340425532 -1.0 -0.997772340425532
probs:  [0.05402866881373529, 0.053768456229900584, 0.08286678031787596, 0.37572669637480305, 0.20503211296150287, 0.22857728530218235]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.7190993
maxi score, test score, baseline:  -0.997772340425532 -1.0 -0.997772340425532
probs:  [0.054006128455284796, 0.05374482542839324, 0.08296508852869436, 0.37556070391706237, 0.20564236543268372, 0.22808088823788153]
printing an ep nov before normalisation:  42.256851218241394
printing an ep nov before normalisation:  22.56798058575699
printing an ep nov before normalisation:  40.87980272724053
maxi score, test score, baseline:  -0.997772340425532 -1.0 -0.997772340425532
probs:  [0.054006128455284796, 0.05374482542839324, 0.08296508852869436, 0.37556070391706237, 0.20564236543268372, 0.22808088823788153]
maxi score, test score, baseline:  -0.997772340425532 -1.0 -0.997772340425532
probs:  [0.054006128455284796, 0.05374482542839324, 0.08296508852869436, 0.37556070391706237, 0.20564236543268372, 0.22808088823788153]
maxi score, test score, baseline:  -0.997772340425532 -1.0 -0.997772340425532
probs:  [0.054006128455284796, 0.05374482542839324, 0.08296508852869436, 0.37556070391706237, 0.20564236543268372, 0.22808088823788153]
maxi score, test score, baseline:  -0.997772340425532 -1.0 -0.997772340425532
siam score:  -0.7148599
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.46802782698407
actions average: 
K:  4  action  0 :  tensor([0.4721, 0.0448, 0.0961, 0.0978, 0.1240, 0.0805, 0.0847],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0222, 0.8329, 0.0393, 0.0228, 0.0109, 0.0344, 0.0374],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.3161, 0.0389, 0.2017, 0.1012, 0.0928, 0.1163, 0.1330],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1820, 0.1240, 0.0909, 0.2857, 0.1135, 0.1160, 0.0880],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2318, 0.0940, 0.0709, 0.1195, 0.3232, 0.0892, 0.0714],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1741, 0.0192, 0.1102, 0.1704, 0.1278, 0.2897, 0.1086],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2049, 0.0010, 0.1445, 0.1724, 0.1768, 0.1591, 0.1413],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9977768577494692 -1.0 -0.9977768577494692
probs:  [0.0540450640846218, 0.05378357186165301, 0.0830249761375251, 0.37583203454519676, 0.20506874810419104, 0.22824560526681215]
Printing some Q and Qe and total Qs values:  [[ 0.033]
 [-0.007]
 [ 0.029]
 [ 0.032]
 [ 0.019]
 [ 0.027]
 [ 0.024]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.033]
 [-0.007]
 [ 0.029]
 [ 0.032]
 [ 0.019]
 [ 0.027]
 [ 0.024]]
printing an ep nov before normalisation:  46.313737005117446
maxi score, test score, baseline:  -0.9977768577494692 -1.0 -0.9977768577494692
probs:  [0.053920649192670186, 0.053790633091908534, 0.0830358859970516, 0.3758814824244813, 0.20509572084594782, 0.22827562844794055]
printing an ep nov before normalisation:  58.515257884146735
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.56 ]] [[37.939]
 [37.738]
 [37.738]
 [37.738]
 [37.738]
 [37.738]
 [37.738]] [[2.228]
 [2.217]
 [2.217]
 [2.217]
 [2.217]
 [2.217]
 [2.217]]
Printing some Q and Qe and total Qs values:  [[0.116]
 [0.04 ]
 [0.04 ]
 [0.04 ]
 [0.04 ]
 [0.04 ]
 [0.04 ]] [[38.087]
 [20.869]
 [20.869]
 [20.869]
 [20.869]
 [20.869]
 [20.869]] [[0.116]
 [0.04 ]
 [0.04 ]
 [0.04 ]
 [0.04 ]
 [0.04 ]
 [0.04 ]]
maxi score, test score, baseline:  -0.9977768577494692 -1.0 -0.9977768577494692
probs:  [0.053920649192670186, 0.053790633091908534, 0.0830358859970516, 0.3758814824244813, 0.20509572084594782, 0.22827562844794055]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977768577494692 -1.0 -0.9977768577494692
probs:  [0.053920649192670186, 0.053790633091908534, 0.0830358859970516, 0.3758814824244813, 0.20509572084594782, 0.22827562844794055]
printing an ep nov before normalisation:  49.983475338811644
maxi score, test score, baseline:  -0.9977768577494692 -1.0 -0.9977768577494692
probs:  [0.053920649192670186, 0.053790633091908534, 0.0830358859970516, 0.3758814824244813, 0.20509572084594782, 0.22827562844794055]
printing an ep nov before normalisation:  58.00787327232195
actions average: 
K:  1  action  0 :  tensor([0.4161, 0.0317, 0.1107, 0.1072, 0.1228, 0.0844, 0.1272],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0229, 0.8801, 0.0244, 0.0159, 0.0125, 0.0138, 0.0303],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1887, 0.0105, 0.2720, 0.1212, 0.1280, 0.1586, 0.1211],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2069, 0.0063, 0.1406, 0.2322, 0.1565, 0.1297, 0.1278],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1935, 0.0124, 0.1388, 0.2209, 0.1708, 0.1449, 0.1187],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1841, 0.0060, 0.1297, 0.1298, 0.1220, 0.2914, 0.1370],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2044, 0.0229, 0.1105, 0.1523, 0.0710, 0.1147, 0.3242],
       grad_fn=<DivBackward0>)
siam score:  -0.71349597
maxi score, test score, baseline:  -0.9977768577494692 -1.0 -0.9977768577494692
probs:  [0.053871831764551494, 0.053741335689281806, 0.08309455196044806, 0.3755357536723756, 0.20488605757043005, 0.22887046934291305]
printing an ep nov before normalisation:  34.622821837132676
printing an ep nov before normalisation:  28.588969167092415
maxi score, test score, baseline:  -0.9977768577494692 -1.0 -0.9977768577494692
probs:  [0.053871831764551494, 0.053741335689281806, 0.08309455196044806, 0.3755357536723756, 0.20488605757043005, 0.22887046934291305]
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.613]
 [0.268]
 [0.508]
 [0.482]
 [0.179]
 [0.501]] [[24.313]
 [44.928]
 [21.175]
 [28.418]
 [29.78 ]
 [20.001]
 [30.025]] [[0.749]
 [1.513]
 [0.521]
 [0.958]
 [0.969]
 [0.401]
 [0.995]]
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.621]
 [0.382]
 [0.476]
 [0.476]
 [0.476]
 [0.476]] [[30.43 ]
 [47.153]
 [27.484]
 [30.43 ]
 [30.43 ]
 [30.43 ]
 [30.43 ]] [[0.975]
 [1.569]
 [0.801]
 [0.975]
 [0.975]
 [0.975]
 [0.975]]
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.547]
 [0.497]
 [0.486]
 [0.494]
 [0.49 ]
 [0.491]] [[35.054]
 [54.116]
 [34.754]
 [39.264]
 [36.674]
 [36.169]
 [26.59 ]] [[0.947]
 [1.534]
 [0.945]
 [1.059]
 [0.995]
 [0.977]
 [0.712]]
printing an ep nov before normalisation:  20.764739513397217
maxi score, test score, baseline:  -0.9977813559322034 -1.0 -0.9977813559322034
maxi score, test score, baseline:  -0.9977813559322034 -1.0 -0.9977813559322034
probs:  [0.05382308875805134, 0.05369211309215956, 0.08315319061539807, 0.3751905492635692, 0.20467644642153227, 0.22946461184928973]
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.487]
 [0.487]
 [0.572]
 [0.493]
 [0.487]
 [0.487]] [[23.875]
 [23.875]
 [23.875]
 [30.076]
 [34.734]
 [23.875]
 [23.875]] [[1.044]
 [1.044]
 [1.044]
 [1.333]
 [1.407]
 [1.044]
 [1.044]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977858350951374 -1.0 -0.9977858350951374
probs:  [0.053823089485714314, 0.053692113646578686, 0.08315321444096366, 0.37519055301652915, 0.2046764414251082, 0.22946458798510597]
line 256 mcts: sample exp_bonus 49.24795722462818
printing an ep nov before normalisation:  49.788889157879765
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.532]
 [0.498]
 [0.483]
 [0.508]
 [0.488]
 [0.506]] [[40.657]
 [38.464]
 [41.222]
 [41.848]
 [44.85 ]
 [41.079]
 [37.81 ]] [[1.193]
 [1.149]
 [1.205]
 [1.211]
 [1.334]
 [1.19 ]
 [1.101]]
maxi score, test score, baseline:  -0.9977858350951374 -1.0 -0.9977858350951374
maxi score, test score, baseline:  -0.9977858350951374 -1.0 -0.9977858350951374
probs:  [0.05376861594916271, 0.05363717547026764, 0.08320279018780286, 0.37480532488195284, 0.2051571235754261, 0.22942896993538797]
Printing some Q and Qe and total Qs values:  [[0.452]
 [0.471]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]] [[42.829]
 [35.064]
 [41.105]
 [41.105]
 [41.105]
 [41.105]
 [41.105]] [[1.297]
 [1.034]
 [1.25 ]
 [1.25 ]
 [1.25 ]
 [1.25 ]
 [1.25 ]]
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]] [[24.523]
 [24.523]
 [24.523]
 [24.523]
 [24.523]
 [24.523]
 [24.523]] [[0.915]
 [0.915]
 [0.915]
 [0.915]
 [0.915]
 [0.915]
 [0.915]]
maxi score, test score, baseline:  -0.9977858350951374 -1.0 -0.9977858350951374
probs:  [0.053806880148505314, 0.05367534589822606, 0.08326205311815887, 0.3750726209352776, 0.2045905473344511, 0.22959255256538097]
maxi score, test score, baseline:  -0.9977858350951374 -1.0 -0.9977858350951374
probs:  [0.053806880148505314, 0.05367534589822606, 0.08326205311815887, 0.3750726209352776, 0.2045905473344511, 0.22959255256538097]
printing an ep nov before normalisation:  51.83380948465079
printing an ep nov before normalisation:  38.83335033408847
actor:  1 policy actor:  1  step number:  62 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9977858350951374 -1.0 -0.9977858350951374
printing an ep nov before normalisation:  54.56369888230574
maxi score, test score, baseline:  -0.9977858350951374 -1.0 -0.9977858350951374
probs:  [0.05237038818344939, 0.052241791952770185, 0.10633875808850085, 0.3650333738232823, 0.19978607046342117, 0.224229617488576]
maxi score, test score, baseline:  -0.9977858350951374 -1.0 -0.9977858350951374
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9977858350951374 -1.0 -0.9977858350951374
probs:  [0.052406551264111374, 0.05227786600800016, 0.10641228274204675, 0.3652859892619947, 0.1992325543335355, 0.22438475639031155]
from probs:  [0.052406551264111374, 0.05227786600800016, 0.10641228274204675, 0.3652859892619947, 0.1992325543335355, 0.22438475639031155]
printing an ep nov before normalisation:  9.699543884822301
maxi score, test score, baseline:  -0.9977858350951374 -1.0 -0.9977858350951374
probs:  [0.052284582487457994, 0.052284582487457994, 0.10642597199383827, 0.36533302268792983, 0.19925819928118205, 0.2244136410621338]
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.46 ]
 [0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.354]] [[47.62 ]
 [51.794]
 [47.62 ]
 [47.62 ]
 [47.62 ]
 [47.62 ]
 [47.62 ]] [[1.69 ]
 [1.978]
 [1.69 ]
 [1.69 ]
 [1.69 ]
 [1.69 ]
 [1.69 ]]
siam score:  -0.7245851
printing an ep nov before normalisation:  45.37934116344893
using explorer policy with actor:  1
printing an ep nov before normalisation:  59.373453651295485
Printing some Q and Qe and total Qs values:  [[0.45 ]
 [0.45 ]
 [0.385]
 [0.432]
 [0.431]
 [0.368]
 [0.422]] [[55.863]
 [43.006]
 [38.613]
 [43.903]
 [50.917]
 [43.898]
 [36.959]] [[0.45 ]
 [0.45 ]
 [0.385]
 [0.432]
 [0.431]
 [0.368]
 [0.422]]
Printing some Q and Qe and total Qs values:  [[0.308]
 [0.296]
 [0.296]
 [0.296]
 [0.296]
 [0.296]
 [0.296]] [[31.406]
 [14.137]
 [14.137]
 [14.137]
 [14.137]
 [14.137]
 [14.137]] [[1.248]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]]
siam score:  -0.72076064
printing an ep nov before normalisation:  14.718897429720817
maxi score, test score, baseline:  -0.9977902953586498 -1.0 -0.9977902953586498
probs:  [0.05228458374759897, 0.05228458374759897, 0.10642598312046536, 0.3653330313855291, 0.19925819706581516, 0.22441362093299247]
Printing some Q and Qe and total Qs values:  [[0.318]
 [0.318]
 [0.318]
 [0.318]
 [0.318]
 [0.318]
 [0.318]] [[25.331]
 [25.331]
 [25.331]
 [25.331]
 [25.331]
 [25.331]
 [25.331]] [[1.4]
 [1.4]
 [1.4]
 [1.4]
 [1.4]
 [1.4]
 [1.4]]
maxi score, test score, baseline:  -0.9977902953586498 -1.0 -0.9977902953586498
maxi score, test score, baseline:  -0.9977902953586498 -1.0 -0.9977902953586498
probs:  [0.05232042432201379, 0.05232042432201379, 0.10649903190891204, 0.36558401183030137, 0.19870835189954428, 0.22456775571721488]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.4446],
        [-0.5803],
        [-0.0000],
        [-0.0000],
        [-0.0000],
        [-0.5921],
        [-0.6118],
        [-0.3667],
        [-0.0000]], dtype=torch.float64)
-0.7722000000000001 -0.7722000000000001
-0.084359833866 -0.5289640948968608
-0.032346567066 -0.6126272572079702
-0.09069976753199967 -0.09069976753199967
-0.957 -0.957
-0.6204000000000002 -0.6204000000000002
-0.032346567066 -0.6243987769841807
-0.084359833866 -0.6962077583290524
-0.084359833866 -0.4510246411849561
-0.34979999999999983 -0.34979999999999983
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.799]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]] [[18.632]
 [48.708]
 [18.632]
 [18.632]
 [18.632]
 [18.632]
 [18.632]] [[0.747]
 [1.806]
 [0.747]
 [0.747]
 [0.747]
 [0.747]
 [0.747]]
maxi score, test score, baseline:  -0.9977947368421053 -1.0 -0.9977947368421053
probs:  [0.052320425583288754, 0.052320425583288754, 0.10649904300761164, 0.36558402053628947, 0.19870834960627648, 0.2245677356832449]
maxi score, test score, baseline:  -0.9977947368421053 -1.0 -0.9977947368421053
maxi score, test score, baseline:  -0.9977947368421053 -1.0 -0.9977947368421053
probs:  [0.05214414367429662, 0.05214414367429662, 0.10663038977999643, 0.3643485758866828, 0.19936326506217839, 0.2253694819225493]
printing an ep nov before normalisation:  27.707254886627197
maxi score, test score, baseline:  -0.9977947368421053 -1.0 -0.9977947368421053
probs:  [0.05214414367429662, 0.05214414367429662, 0.10663038977999643, 0.3643485758866828, 0.19936326506217839, 0.2253694819225493]
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]] [[47.442]
 [47.442]
 [47.442]
 [47.442]
 [47.442]
 [47.442]
 [47.442]] [[2.071]
 [2.071]
 [2.071]
 [2.071]
 [2.071]
 [2.071]
 [2.071]]
maxi score, test score, baseline:  -0.9977947368421053 -1.0 -0.9977947368421053
probs:  [0.05208722724334711, 0.05208722724334711, 0.10675912149289178, 0.36394951041481705, 0.19980796066321047, 0.22530895294238648]
maxi score, test score, baseline:  -0.9977947368421053 -1.0 -0.9977947368421053
probs:  [0.05208722724334711, 0.05208722724334711, 0.10675912149289178, 0.36394951041481705, 0.19980796066321047, 0.22530895294238648]
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.562]
 [0.494]
 [0.5  ]
 [0.505]
 [0.504]
 [0.515]] [[10.447]
 [32.729]
 [10.42 ]
 [10.575]
 [10.614]
 [10.636]
 [10.968]] [[0.649]
 [1.376]
 [0.645]
 [0.655]
 [0.662]
 [0.662]
 [0.682]]
printing an ep nov before normalisation:  47.40116081615611
maxi score, test score, baseline:  -0.9977947368421053 -1.0 -0.9977947368421053
probs:  [0.05215851679523156, 0.05215851679523156, 0.10690543000009936, 0.36444872702153197, 0.1987108778016508, 0.22561793158625482]
printing an ep nov before normalisation:  32.04542696475983
maxi score, test score, baseline:  -0.9977947368421053 -1.0 -0.9977947368421053
probs:  [0.05215851679523156, 0.05215851679523156, 0.10690543000009936, 0.36444872702153197, 0.1987108778016508, 0.22561793158625482]
printing an ep nov before normalisation:  36.49211044299481
using another actor
maxi score, test score, baseline:  -0.9978035639412998 -1.0 -0.9978035639412998
probs:  [0.05222886403170759, 0.05222886403170759, 0.1070498218594927, 0.36494134465802364, 0.19762833012190062, 0.22592277529716795]
maxi score, test score, baseline:  -0.9978035639412998 -1.0 -0.9978035639412998
probs:  [0.05222886403170759, 0.05222886403170759, 0.1070498218594927, 0.36494134465802364, 0.19762833012190062, 0.22592277529716795]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9978035639412998 -1.0 -0.9978035639412998
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.392]
 [0.403]
 [0.371]] [[30.793]
 [30.793]
 [30.793]
 [30.793]
 [32.193]
 [32.054]
 [30.793]] [[0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.392]
 [0.403]
 [0.371]]
printing an ep nov before normalisation:  53.6788869992843
from probs:  [0.052054423828207165, 0.052054423828207165, 0.1071824997672138, 0.3637188018233489, 0.19826844718203865, 0.2267214035709843]
maxi score, test score, baseline:  -0.997807949790795 -1.0 -0.997807949790795
probs:  [0.052054425069987476, 0.052054425069987476, 0.1071825110037684, 0.3637188103919423, 0.19826844472386598, 0.2267213837404484]
siam score:  -0.71952623
printing an ep nov before normalisation:  42.9686473468684
maxi score, test score, baseline:  -0.9978123173277662 -1.0 -0.9978123173277662
probs:  [0.05205442630657281, 0.05205442630657281, 0.10718252219332494, 0.36371881892468894, 0.1982684422759719, 0.22672136399286863]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.9978123173277662 -1.0 -0.9978123173277662
probs:  [0.05205442630657281, 0.05205442630657281, 0.10718252219332494, 0.36371881892468894, 0.1982684422759719, 0.22672136399286863]
printing an ep nov before normalisation:  42.882358200010245
Printing some Q and Qe and total Qs values:  [[0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.586]
 [0.589]] [[30.884]
 [30.884]
 [30.884]
 [30.884]
 [30.884]
 [31.886]
 [31.793]] [[0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.586]
 [0.589]]
Printing some Q and Qe and total Qs values:  [[0.426]
 [0.465]
 [0.418]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]] [[30.245]
 [35.78 ]
 [28.59 ]
 [26.357]
 [26.357]
 [26.357]
 [26.357]] [[0.426]
 [0.465]
 [0.418]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]]
printing an ep nov before normalisation:  30.20522135201124
printing an ep nov before normalisation:  39.08339023590088
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.523]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]] [[26.54 ]
 [41.933]
 [17.572]
 [17.572]
 [17.572]
 [17.572]
 [17.572]] [[0.755]
 [1.013]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]]
maxi score, test score, baseline:  -0.997820997920998 -1.0 -0.997820997920998
probs:  [0.05198388035721096, 0.05198388035721096, 0.10696785328735543, 0.36322431227872876, 0.19864963851921852, 0.22719043520027535]
Printing some Q and Qe and total Qs values:  [[0.344]
 [0.356]
 [0.354]
 [0.351]
 [0.366]
 [0.361]
 [0.368]] [[32.331]
 [39.957]
 [30.593]
 [37.39 ]
 [31.424]
 [31.747]
 [30.406]] [[0.936]
 [1.208]
 [0.887]
 [1.116]
 [0.927]
 [0.933]
 [0.894]]
maxi score, test score, baseline:  -0.997820997920998 -1.0 -0.997820997920998
probs:  [0.05198388035721096, 0.05198388035721096, 0.10696785328735543, 0.36322431227872876, 0.19864963851921852, 0.22719043520027535]
maxi score, test score, baseline:  -0.997820997920998 -1.0 -0.997820997920998
probs:  [0.05198388035721096, 0.05198388035721096, 0.10696785328735543, 0.36322431227872876, 0.19864963851921852, 0.22719043520027535]
printing an ep nov before normalisation:  70.79257500014181
printing an ep nov before normalisation:  43.08389592147844
maxi score, test score, baseline:  -0.997820997920998 -1.0 -0.997820997920998
probs:  [0.05209487390818935, 0.05196500701427919, 0.10703600896413637, 0.3630928191966522, 0.19864636514908524, 0.22716492576765762]
printing an ep nov before normalisation:  14.079632759094238
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.997820997920998 -1.0 -0.997820997920998
probs:  [0.05209487390818935, 0.05196500701427919, 0.10703600896413637, 0.3630928191966522, 0.19864636514908524, 0.22716492576765762]
actor:  1 policy actor:  1  step number:  78 total reward:  0.24666666666666515  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  33.965699760380666
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.997820997920998 -1.0 -0.997820997920998
probs:  [0.05355747100361854, 0.053433678050663604, 0.10592898111286037, 0.37338578855813903, 0.19325467415079417, 0.22043940712392415]
actor:  1 policy actor:  1  step number:  67 total reward:  0.19999999999999984  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.997820997920998 -1.0 -0.997820997920998
probs:  [0.05478004138295118, 0.054661325576888485, 0.105003627531689, 0.3819895793738453, 0.18874781321853543, 0.21481761291609075]
line 256 mcts: sample exp_bonus 37.89789834456644
maxi score, test score, baseline:  -0.997820997920998 -1.0 -0.997820997920998
probs:  [0.05478004138295118, 0.054661325576888485, 0.105003627531689, 0.3819895793738453, 0.18874781321853543, 0.21481761291609075]
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  72 total reward:  0.08666666666666611  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.997820997920998 -1.0 -0.997820997920998
probs:  [0.05401640467787983, 0.053898838586820465, 0.10375359552315398, 0.3766494990518609, 0.1866867523342479, 0.2249949098260369]
maxi score, test score, baseline:  -0.997820997920998 -1.0 -0.997820997920998
maxi score, test score, baseline:  -0.997820997920998 -1.0 -0.997820997920998
probs:  [0.05401640467787983, 0.053898838586820465, 0.10375359552315398, 0.3766494990518609, 0.1866867523342479, 0.2249949098260369]
printing an ep nov before normalisation:  42.642187265046616
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]] [[35.204]
 [35.204]
 [35.204]
 [35.204]
 [35.204]
 [35.204]
 [35.204]] [[1.245]
 [1.245]
 [1.245]
 [1.245]
 [1.245]
 [1.245]
 [1.245]]
printing an ep nov before normalisation:  26.110302426547722
maxi score, test score, baseline:  -0.997820997920998 -1.0 -0.997820997920998
probs:  [0.05393419007627358, 0.0538162946643471, 0.10381070251223061, 0.37607101222759864, 0.18697616796663366, 0.2253916325529164]
maxi score, test score, baseline:  -0.997820997920998 -1.0 -0.997820997920998
probs:  [0.05393419007627358, 0.0538162946643471, 0.10381070251223061, 0.37607101222759864, 0.18697616796663366, 0.2253916325529164]
maxi score, test score, baseline:  -0.997820997920998 -1.0 -0.997820997920998
probs:  [0.05393419007627358, 0.0538162946643471, 0.10381070251223061, 0.37607101222759864, 0.18697616796663366, 0.2253916325529164]
maxi score, test score, baseline:  -0.9978253112033195 -1.0 -0.9978253112033195
probs:  [0.0539341900891028, 0.05381629452285558, 0.10381071227656173, 0.37607101111909846, 0.1869761672076771, 0.2253916247847043]
maxi score, test score, baseline:  -0.9978253112033195 -1.0 -0.9978253112033195
probs:  [0.0539341900891028, 0.05381629452285558, 0.10381071227656173, 0.37607101111909846, 0.1869761672076771, 0.2253916247847043]
maxi score, test score, baseline:  -0.9978253112033195 -1.0 -0.9978253112033195
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.671]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]] [[53.557]
 [55.915]
 [52.982]
 [52.982]
 [52.982]
 [52.982]
 [52.982]] [[2.327]
 [2.463]
 [2.303]
 [2.303]
 [2.303]
 [2.303]
 [2.303]]
maxi score, test score, baseline:  -0.9978253112033195 -1.0 -0.9978253112033195
probs:  [0.05385218490618063, 0.05373396085748892, 0.10386767382879943, 0.37549399782475096, 0.18726484563276374, 0.2257873369500162]
maxi score, test score, baseline:  -0.9978253112033195 -1.0 -0.9978253112033195
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.485]
 [0.358]
 [0.382]
 [0.06 ]
 [0.691]
 [0.382]] [[19.91 ]
 [17.618]
 [16.932]
 [17.438]
 [21.089]
 [20.668]
 [17.438]] [[1.191]
 [1.289]
 [1.13 ]
 [1.177]
 [1.022]
 [1.634]
 [1.177]]
maxi score, test score, baseline:  -0.9978253112033195 -1.0 -0.9978253112033195
probs:  [0.05377038834898464, 0.05365183665352596, 0.10392449046769411, 0.3749184524841749, 0.18755278964395222, 0.2261820424016681]
printing an ep nov before normalisation:  24.816279411315918
line 256 mcts: sample exp_bonus 25.237310423884615
maxi score, test score, baseline:  -0.9978253112033195 -1.0 -0.9978253112033195
maxi score, test score, baseline:  -0.9978253112033195 -1.0 -0.9978253112033195
probs:  [0.053801582076514635, 0.05368296143502718, 0.1039848522822261, 0.3751364160725838, 0.1876617871114649, 0.2257324010221834]
Printing some Q and Qe and total Qs values:  [[0.209]
 [0.199]
 [0.208]
 [0.208]
 [0.218]
 [0.211]
 [0.207]] [[23.843]
 [31.799]
 [23.988]
 [26.432]
 [26.299]
 [25.999]
 [25.135]] [[0.209]
 [0.199]
 [0.208]
 [0.208]
 [0.218]
 [0.211]
 [0.207]]
maxi score, test score, baseline:  -0.9978253112033195 -1.0 -0.9978253112033195
probs:  [0.053801582076514635, 0.05368296143502718, 0.1039848522822261, 0.3751364160725838, 0.1876617871114649, 0.2257324010221834]
maxi score, test score, baseline:  -0.9978296066252588 -1.0 -0.9978296066252588
maxi score, test score, baseline:  -0.9978296066252588 -1.0 -0.9978296066252588
probs:  [0.05368931923245734, 0.05368931923245734, 0.10399719236333496, 0.37518093894483606, 0.18768405163231627, 0.22575917859459807]
printing an ep nov before normalisation:  9.417357241556871
maxi score, test score, baseline:  -0.9978296066252588 -1.0 -0.9978296066252588
probs:  [0.05367108462364566, 0.05378830403809561, 0.10406079953003702, 0.37505384354204235, 0.18768880829895407, 0.22573715996722535]
Printing some Q and Qe and total Qs values:  [[0.393]
 [0.554]
 [0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]] [[35.721]
 [62.301]
 [35.721]
 [35.721]
 [35.721]
 [35.721]
 [35.721]] [[1.041]
 [1.991]
 [1.041]
 [1.041]
 [1.041]
 [1.041]
 [1.041]]
printing an ep nov before normalisation:  29.221774189353702
Printing some Q and Qe and total Qs values:  [[0.243]
 [0.269]
 [0.199]
 [0.243]
 [0.281]
 [0.243]
 [0.243]] [[33.248]
 [44.004]
 [39.464]
 [33.248]
 [34.465]
 [33.248]
 [33.248]] [[0.243]
 [0.269]
 [0.199]
 [0.243]
 [0.281]
 [0.243]
 [0.243]]
printing an ep nov before normalisation:  22.583397660118067
maxi score, test score, baseline:  -0.9978296066252588 -1.0 -0.9978296066252588
probs:  [0.05367108462364566, 0.05378830403809561, 0.10406079953003702, 0.37505384354204235, 0.18768880829895407, 0.22573715996722535]
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.552]
 [0.521]
 [0.529]
 [0.597]
 [0.569]
 [0.55 ]] [[19.062]
 [17.136]
 [17.401]
 [17.632]
 [17.487]
 [18.863]
 [18.602]] [[0.525]
 [0.552]
 [0.521]
 [0.529]
 [0.597]
 [0.569]
 [0.55 ]]
printing an ep nov before normalisation:  50.39597880604466
maxi score, test score, baseline:  -0.9978338842975206 -1.0 -0.9978338842975206
probs:  [0.05370203378504655, 0.053819321114273365, 0.10412088815323878, 0.37527057751116305, 0.18779723033160603, 0.22528994910467223]
maxi score, test score, baseline:  -0.9978381443298969 -1.0 -0.9978381443298969
maxi score, test score, baseline:  -0.9978381443298969 -1.0 -0.9978381443298969
probs:  [0.05370203362283888, 0.05381932110366566, 0.1041208979749994, 0.3752705762590396, 0.1877972296779649, 0.22528994136149152]
maxi score, test score, baseline:  -0.9978381443298969 -1.0 -0.9978381443298969
Printing some Q and Qe and total Qs values:  [[1.108]
 [0.964]
 [0.964]
 [0.964]
 [0.964]
 [0.964]
 [0.964]] [[82.333]
 [79.471]
 [79.471]
 [79.471]
 [79.471]
 [79.471]
 [79.471]] [[3.042]
 [2.82 ]
 [2.82 ]
 [2.82 ]
 [2.82 ]
 [2.82 ]
 [2.82 ]]
maxi score, test score, baseline:  -0.9978381443298969 -1.0 -0.9978381443298969
probs:  [0.05363298497559484, 0.05386686446939765, 0.10430106252104943, 0.3747871950328197, 0.18819800865730618, 0.225213884343832]
maxi score, test score, baseline:  -0.9978381443298969 -1.0 -0.9978381443298969
probs:  [0.05366364277812359, 0.05389765629554771, 0.10436075544387544, 0.37500188885121316, 0.18830577835828619, 0.22477027827295393]
maxi score, test score, baseline:  -0.9978381443298969 -1.0 -0.9978381443298969
maxi score, test score, baseline:  -0.9978381443298969 -1.0 -0.9978381443298969
probs:  [0.05366106545782711, 0.054009994517863445, 0.10416718136096488, 0.3749844281928089, 0.1883637852388299, 0.22481354523170566]
printing an ep nov before normalisation:  23.694691329575917
printing an ep nov before normalisation:  29.684747409077215
actor:  1 policy actor:  1  step number:  59 total reward:  0.2799999999999997  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  0.000961588211794151
actor:  1 policy actor:  1  step number:  58 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.16]
 [0.16]
 [0.16]
 [0.16]
 [0.16]
 [0.16]
 [0.16]] [[25.71]
 [25.71]
 [25.71]
 [25.71]
 [25.71]
 [25.71]
 [25.71]] [[0.16]
 [0.16]
 [0.16]
 [0.16]
 [0.16]
 [0.16]
 [0.16]]
maxi score, test score, baseline:  -0.997846611909651 -1.0 -0.997846611909651
probs:  [0.054085478599056325, 0.05441513768988022, 0.1018022383658803, 0.37796291330511345, 0.1813487736583699, 0.2303854583816998]
printing an ep nov before normalisation:  28.022979470909693
printing an ep nov before normalisation:  36.341326564537994
maxi score, test score, baseline:  -0.997846611909651 -1.0 -0.997846611909651
probs:  [0.054085478599056325, 0.05441513768988022, 0.1018022383658803, 0.37796291330511345, 0.1813487736583699, 0.2303854583816998]
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  -0.997846611909651 -1.0 -0.997846611909651
probs:  [0.05409994206550398, 0.05453703199211747, 0.10192119481556185, 0.37806474865362866, 0.18088063940070628, 0.23049644307248168]
maxi score, test score, baseline:  -0.997846611909651 -1.0 -0.997846611909651
probs:  [0.05409994206550398, 0.05453703199211747, 0.10192119481556185, 0.37806474865362866, 0.18088063940070628, 0.23049644307248168]
printing an ep nov before normalisation:  54.917953870808795
printing an ep nov before normalisation:  52.75916520665113
maxi score, test score, baseline:  -0.997846611909651 -1.0 -0.997846611909651
probs:  [0.05409994206550398, 0.05453703199211747, 0.10192119481556185, 0.37806474865362866, 0.18088063940070628, 0.23049644307248168]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9978508196721312 -1.0 -0.9978508196721312
probs:  [0.054003172466593974, 0.0545476706143813, 0.1020309985859119, 0.3773872102229022, 0.18115566386807913, 0.23087528424213155]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9978508196721312 -1.0 -0.9978508196721312
probs:  [0.054003172466593974, 0.0545476706143813, 0.1020309985859119, 0.3773872102229022, 0.18115566386807913, 0.23087528424213155]
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.719]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]] [[23.226]
 [30.351]
 [25.694]
 [25.694]
 [25.694]
 [25.694]
 [25.694]] [[0.71 ]
 [0.936]
 [0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]]
maxi score, test score, baseline:  -0.9978508196721312 -1.0 -0.9978508196721312
probs:  [0.05384379723988735, 0.0543912507428699, 0.1021323024561196, 0.3762703072819347, 0.18168643015530653, 0.23167591212388192]
printing an ep nov before normalisation:  41.57609939575195
maxi score, test score, baseline:  -0.9978508196721312 -1.0 -0.9978508196721312
probs:  [0.05384379723988735, 0.0543912507428699, 0.1021323024561196, 0.3762703072819347, 0.18168643015530653, 0.23167591212388192]
printing an ep nov before normalisation:  0.0
Printing some Q and Qe and total Qs values:  [[0.1  ]
 [0.198]
 [0.1  ]
 [0.1  ]
 [0.1  ]
 [0.1  ]
 [0.1  ]] [[32.574]
 [42.93 ]
 [32.574]
 [32.574]
 [32.574]
 [32.574]
 [32.574]] [[0.258]
 [0.453]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]]
line 256 mcts: sample exp_bonus 54.41888982829573
maxi score, test score, baseline:  -0.9978508196721312 -1.0 -0.9978508196721312
printing an ep nov before normalisation:  39.002896821103214
maxi score, test score, baseline:  -0.9978508196721312 -1.0 -0.9978508196721312
probs:  [0.05382723572294326, 0.054480175241736664, 0.10219091535189753, 0.3761548653109754, 0.1816945327867237, 0.23165227558572346]
Printing some Q and Qe and total Qs values:  [[0.55 ]
 [0.494]
 [0.55 ]
 [0.509]
 [0.55 ]
 [0.515]
 [0.55 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.55 ]
 [0.494]
 [0.55 ]
 [0.509]
 [0.55 ]
 [0.515]
 [0.55 ]]
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]] [[50.365]
 [29.849]
 [29.849]
 [29.849]
 [29.849]
 [29.849]
 [29.849]] [[1.052]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]]
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.283]
 [0.462]
 [0.441]
 [0.504]
 [0.481]
 [0.507]] [[44.196]
 [47.276]
 [38.966]
 [40.677]
 [37.512]
 [37.633]
 [31.472]] [[0.971]
 [0.828]
 [0.903]
 [0.904]
 [0.927]
 [0.906]
 [0.856]]
printing an ep nov before normalisation:  60.33446106918294
printing an ep nov before normalisation:  77.16474197909753
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.506]
 [0.503]
 [0.48 ]
 [0.498]
 [0.477]
 [0.506]] [[18.634]
 [25.314]
 [15.529]
 [15.781]
 [16.096]
 [16.373]
 [25.314]] [[1.808]
 [2.772]
 [1.403]
 [1.415]
 [1.477]
 [1.494]
 [2.772]]
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.461]
 [0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]] [[36.27]
 [46.3 ]
 [36.27]
 [36.27]
 [36.27]
 [36.27]
 [36.27]] [[1.021]
 [1.361]
 [1.021]
 [1.021]
 [1.021]
 [1.021]
 [1.021]]
printing an ep nov before normalisation:  72.42232645076278
maxi score, test score, baseline:  -0.9978550102249489 -1.0 -0.9978550102249489
probs:  [0.05380019804879172, 0.05444809888800049, 0.10243049767663637, 0.3759656613026251, 0.18161623628871246, 0.2317393077952339]
printing an ep nov before normalisation:  37.76222573109899
maxi score, test score, baseline:  -0.9978550102249489 -1.0 -0.9978550102249489
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]] [[44.127]
 [44.857]
 [44.857]
 [44.857]
 [44.857]
 [44.857]
 [44.857]] [[2.473]
 [2.477]
 [2.477]
 [2.477]
 [2.477]
 [2.477]
 [2.477]]
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  -0.9978550102249489 -1.0 -0.9978550102249489
siam score:  -0.7156772
actions average: 
K:  2  action  0 :  tensor([0.4795, 0.0257, 0.1080, 0.0967, 0.0845, 0.1044, 0.1012],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0162, 0.8999, 0.0146, 0.0146, 0.0037, 0.0030, 0.0481],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.3182, 0.0089, 0.1754, 0.1308, 0.1174, 0.1203, 0.1290],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2260, 0.0253, 0.1317, 0.2359, 0.1327, 0.1298, 0.1186],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1895, 0.0430, 0.1102, 0.1572, 0.2688, 0.1216, 0.1095],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1550, 0.0151, 0.2672, 0.1326, 0.1126, 0.2020, 0.1154],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2005, 0.0698, 0.1751, 0.1227, 0.0839, 0.0959, 0.2521],
       grad_fn=<DivBackward0>)
siam score:  -0.7159153
Printing some Q and Qe and total Qs values:  [[0.147]
 [0.205]
 [0.147]
 [0.147]
 [0.147]
 [0.147]
 [0.147]] [[24.817]
 [30.955]
 [24.817]
 [24.817]
 [24.817]
 [24.817]
 [24.817]] [[0.371]
 [0.538]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]]
printing an ep nov before normalisation:  35.48986353055105
maxi score, test score, baseline:  -0.9978550102249489 -1.0 -0.9978550102249489
probs:  [0.053673526814466216, 0.05432527137781625, 0.10259232962885458, 0.3750777971795874, 0.18224784404106414, 0.23208323095821146]
maxi score, test score, baseline:  -0.9978550102249489 -1.0 -0.9978550102249489
printing an ep nov before normalisation:  26.982402153362504
actor:  1 policy actor:  1  step number:  70 total reward:  0.24666666666666648  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9978550102249489 -1.0 -0.9978550102249489
probs:  [0.054823074687636114, 0.055449587432570585, 0.10184802150516713, 0.3831338095284432, 0.17841973040756665, 0.22632577643861637]
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.52704710583426
maxi score, test score, baseline:  -0.9978550102249489 -1.0 -0.9978550102249489
probs:  [0.054669862060507196, 0.05529973772166022, 0.10194722346594243, 0.38206009798935997, 0.17892994434508525, 0.22709313441744497]
maxi score, test score, baseline:  -0.9978550102249489 -1.0 -0.9978550102249489
printing an ep nov before normalisation:  27.262065410614014
siam score:  -0.72086227
printing an ep nov before normalisation:  4.733666628453648e-05
printing an ep nov before normalisation:  31.377854522182265
maxi score, test score, baseline:  -0.9978550102249489 -1.0 -0.9978550102249489
probs:  [0.05457723339791284, 0.05530961535242271, 0.1020527856102217, 0.3814115511790585, 0.17919341541154948, 0.22745539904883472]
maxi score, test score, baseline:  -0.9978550102249489 -1.0 -0.9978550102249489
probs:  [0.054607940280227085, 0.05534073530438175, 0.10211026910734436, 0.38162659539344357, 0.17929440687928058, 0.2270200530353226]
siam score:  -0.7222961
printing an ep nov before normalisation:  38.02629470825195
maxi score, test score, baseline:  -0.9978550102249489 -1.0 -0.9978550102249489
probs:  [0.054607940280227085, 0.05534073530438175, 0.10211026910734436, 0.38162659539344357, 0.17929440687928058, 0.2270200530353226]
printing an ep nov before normalisation:  66.32355732257501
maxi score, test score, baseline:  -0.9978550102249489 -1.0 -0.9978550102249489
printing an ep nov before normalisation:  32.86672422219932
printing an ep nov before normalisation:  32.065370997759146
printing an ep nov before normalisation:  39.30719536026324
maxi score, test score, baseline:  -0.9978550102249489 -1.0 -0.9978550102249489
probs:  [0.054607940280227085, 0.05534073530438175, 0.10211026910734436, 0.38162659539344357, 0.17929440687928058, 0.2270200530353226]
printing an ep nov before normalisation:  27.616812199378842
printing an ep nov before normalisation:  25.75527224643764
maxi score, test score, baseline:  -0.9978550102249489 -1.0 -0.9978550102249489
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9978550102249489 -1.0 -0.9978550102249489
probs:  [0.054607940280227085, 0.05534073530438175, 0.10211026910734436, 0.38162659539344357, 0.17929440687928058, 0.2270200530353226]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.109]
 [0.233]
 [0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.132]] [[29.198]
 [25.697]
 [32.268]
 [32.268]
 [32.268]
 [32.268]
 [32.268]] [[1.892]
 [1.768]
 [2.132]
 [2.132]
 [2.132]
 [2.132]
 [2.132]]
printing an ep nov before normalisation:  25.70261648707401
printing an ep nov before normalisation:  43.16667263385285
maxi score, test score, baseline:  -0.9978550102249489 -1.0 -0.9978550102249489
probs:  [0.054638501416535264, 0.05537170754975629, 0.10216747976693066, 0.3818406189299539, 0.17939491900485896, 0.2265867733319649]
printing an ep nov before normalisation:  50.84039651492043
Starting evaluation
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9978550102249489 -1.0 -0.9978550102249489
probs:  [0.054683716765757624, 0.05541753113683397, 0.10198826561494984, 0.3821572677890651, 0.17897858875022413, 0.22677462994316933]
maxi score, test score, baseline:  -0.9978550102249489 -1.0 -0.9978550102249489
probs:  [0.054683716765757624, 0.05541753113683397, 0.10198826561494984, 0.3821572677890651, 0.17897858875022413, 0.22677462994316933]
printing an ep nov before normalisation:  64.060019942193
maxi score, test score, baseline:  -0.9978550102249489 -1.0 -0.9978550102249489
probs:  [0.054683716765757624, 0.05541753113683397, 0.10198826561494984, 0.3821572677890651, 0.17897858875022413, 0.22677462994316933]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.757]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]] [[30.238]
 [33.624]
 [30.238]
 [30.238]
 [30.238]
 [30.238]
 [30.238]] [[0.675]
 [0.757]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]]
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.609]
 [0.543]
 [0.584]
 [0.542]
 [0.513]
 [0.586]] [[44.577]
 [26.106]
 [26.618]
 [27.038]
 [27.48 ]
 [27.825]
 [26.76 ]] [[1.355]
 [0.898]
 [0.844]
 [0.895]
 [0.864]
 [0.844]
 [0.891]]
maxi score, test score, baseline:  -0.9978550102249489 -1.0 -0.9978550102249489
probs:  [0.054683716765757624, 0.05541753113683397, 0.10198826561494984, 0.3821572677890651, 0.17897858875022413, 0.22677462994316933]
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.477]
 [0.422]
 [0.42 ]
 [0.421]
 [0.424]
 [0.43 ]] [[60.075]
 [53.123]
 [51.913]
 [54.269]
 [54.678]
 [54.471]
 [54.961]] [[0.42 ]
 [0.477]
 [0.422]
 [0.42 ]
 [0.421]
 [0.424]
 [0.43 ]]
maxi score, test score, baseline:  -0.9978550102249489 -1.0 -0.9978550102249489
probs:  [0.05460769831752556, 0.05534345751840124, 0.10203761851384929, 0.3816245319720833, 0.17923198926742623, 0.22715470441071428]
maxi score, test score, baseline:  -0.9978550102249489 -1.0 -0.9978550102249489
probs:  [0.05460769831752556, 0.05534345751840124, 0.10203761851384929, 0.3816245319720833, 0.17923198926742623, 0.22715470441071428]
line 256 mcts: sample exp_bonus 16.99049503950974
printing an ep nov before normalisation:  38.35490285511412
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]] [[39.709]
 [27.419]
 [27.419]
 [27.419]
 [27.419]
 [27.419]
 [27.419]] [[1.95 ]
 [1.174]
 [1.174]
 [1.174]
 [1.174]
 [1.174]
 [1.174]]
printing an ep nov before normalisation:  55.658306331462825
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.627]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]] [[22.688]
 [28.151]
 [13.749]
 [13.749]
 [13.749]
 [13.749]
 [13.749]] [[0.625]
 [0.627]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]]
maxi score, test score, baseline:  -0.9978633401221996 -1.0 -0.9978633401221996
probs:  [0.054607695628559584, 0.055343456749362994, 0.10203763932872915, 0.3816245129255765, 0.17923199720660332, 0.22715469816116843]
printing an ep nov before normalisation:  6.952576666662722
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.579]
 [0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]] [[12.448]
 [ 9.389]
 [12.448]
 [12.448]
 [12.448]
 [12.448]
 [12.448]] [[0.574]
 [0.579]
 [0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.595]
 [0.454]
 [0.45 ]
 [0.452]
 [0.434]
 [0.476]] [[30.572]
 [28.257]
 [26.45 ]
 [24.868]
 [25.872]
 [26.044]
 [26.474]] [[2.023]
 [1.964]
 [1.676]
 [1.544]
 [1.627]
 [1.624]
 [1.7  ]]
maxi score, test score, baseline:  -0.9978674796747967 -1.0 -0.9978674796747967
probs:  [0.05460769429228723, 0.0553434563671918, 0.10203764967260458, 0.3816245034604807, 0.17923200115194424, 0.22715469505549152]
printing an ep nov before normalisation:  13.26469508025582
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.499]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]] [[26.184]
 [23.904]
 [16.594]
 [16.594]
 [16.594]
 [16.594]
 [16.594]] [[0.602]
 [0.499]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]]
printing an ep nov before normalisation:  20.81694652333306
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]] [[13.44]
 [13.44]
 [13.44]
 [13.44]
 [13.44]
 [13.44]
 [13.44]] [[0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]]
maxi score, test score, baseline:  -0.9978674796747967 -1.0 -0.9978674796747967
probs:  [0.05460769429228723, 0.0553434563671918, 0.10203764967260458, 0.3816245034604807, 0.17923200115194424, 0.22715469505549152]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.69509694912489
printing an ep nov before normalisation:  10.055094957351685
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
line 256 mcts: sample exp_bonus 12.180112481117249
Printing some Q and Qe and total Qs values:  [[0.58]
 [0.58]
 [0.58]
 [0.58]
 [0.58]
 [0.58]
 [0.58]] [[19.993]
 [19.993]
 [19.993]
 [19.993]
 [19.993]
 [19.993]
 [19.993]] [[0.58]
 [0.58]
 [0.58]
 [0.58]
 [0.58]
 [0.58]
 [0.58]]
maxi score, test score, baseline:  -0.9978674796747967 -1.0 -0.9978674796747967
siam score:  -0.71011186
printing an ep nov before normalisation:  15.241846761329612
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.402]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]] [[40.65]
 [36.84]
 [35.63]
 [35.63]
 [35.63]
 [35.63]
 [35.63]] [[0.502]
 [0.402]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]]
maxi score, test score, baseline:  -0.9978838709677419 -1.0 -0.9978838709677419
actor:  1 policy actor:  1  step number:  75 total reward:  0.03999999999999959  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 29.46755347518291
printing an ep nov before normalisation:  29.62708970872086
Printing some Q and Qe and total Qs values:  [[0.12 ]
 [0.12 ]
 [0.12 ]
 [0.12 ]
 [0.12 ]
 [0.12 ]
 [0.119]] [[40.369]
 [40.369]
 [40.369]
 [40.369]
 [40.369]
 [40.369]
 [38.522]] [[0.12 ]
 [0.12 ]
 [0.12 ]
 [0.12 ]
 [0.12 ]
 [0.12 ]
 [0.119]]
printing an ep nov before normalisation:  52.96882507053027
printing an ep nov before normalisation:  12.865571975708008
siam score:  -0.71336883
printing an ep nov before normalisation:  25.509903468955745
printing an ep nov before normalisation:  29.59864220938144
actions average: 
K:  2  action  0 :  tensor([0.5521, 0.0044, 0.0912, 0.0994, 0.1150, 0.0650, 0.0729],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0276, 0.8453, 0.0242, 0.0280, 0.0179, 0.0146, 0.0425],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1202, 0.0484, 0.3169, 0.1214, 0.1039, 0.1830, 0.1062],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1442, 0.0187, 0.1129, 0.2900, 0.1882, 0.1391, 0.1068],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2989, 0.0152, 0.1009, 0.1288, 0.1802, 0.0993, 0.1768],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2415, 0.0016, 0.1336, 0.1417, 0.1364, 0.2114, 0.1339],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2068, 0.1737, 0.1135, 0.1064, 0.1230, 0.0964, 0.1801],
       grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 22.112617724592777
Printing some Q and Qe and total Qs values:  [[0.564]
 [0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]] [[20.741]
 [18.299]
 [18.299]
 [18.299]
 [18.299]
 [18.299]
 [18.299]] [[0.564]
 [0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]]
Printing some Q and Qe and total Qs values:  [[0.828]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]] [[23.06 ]
 [20.571]
 [20.571]
 [20.571]
 [20.571]
 [20.571]
 [20.571]] [[0.828]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]]
printing an ep nov before normalisation:  21.88568070914073
printing an ep nov before normalisation:  38.58679817262136
printing an ep nov before normalisation:  15.004048033799343
using explorer policy with actor:  0
using explorer policy with actor:  0
siam score:  -0.7135254
maxi score, test score, baseline:  -0.997895991983968 -1.0 -0.997895991983968
probs:  [0.054114418883111894, 0.054735875776375204, 0.10111497574999599, 0.37817003545185474, 0.1770532848026401, 0.23481140933602207]
printing an ep nov before normalisation:  20.570926658036612
Printing some Q and Qe and total Qs values:  [[0.819]
 [0.629]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]] [[18.418]
 [17.449]
 [10.101]
 [10.101]
 [10.101]
 [10.101]
 [10.101]] [[0.819]
 [0.629]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]]
Printing some Q and Qe and total Qs values:  [[0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]] [[11.428]
 [10.583]
 [10.583]
 [10.583]
 [10.583]
 [10.583]
 [10.583]] [[0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]]
printing an ep nov before normalisation:  10.16640338497183
using explorer policy with actor:  0
printing an ep nov before normalisation:  23.62497934889161
printing an ep nov before normalisation:  38.64236878226665
printing an ep nov before normalisation:  23.970827850086287
from probs:  [0.054114418883111894, 0.054735875776375204, 0.10111497574999599, 0.37817003545185474, 0.1770532848026401, 0.23481140933602207]
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.997895991983968 -1.0 -0.997895991983968
probs:  [0.054144292103715935, 0.05476609291155643, 0.10117085910391431, 0.3783792412240299, 0.1765982342889722, 0.23494128036781128]
printing an ep nov before normalisation:  21.816301927429187
maxi score, test score, baseline:  -0.997895991983968 -1.0 -0.997895991983968
maxi score, test score, baseline:  -0.997895991983968 -1.0 -0.997895991983968
probs:  [0.054144292103715935, 0.05476609291155643, 0.10117085910391431, 0.3783792412240299, 0.1765982342889722, 0.23494128036781128]
actor:  0 policy actor:  1  step number:  106 total reward:  0.13999999999999913  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  28.359416051477336
maxi score, test score, baseline:  -0.9976200000000001 -1.0 -0.9976200000000001
probs:  [0.05414441677065144, 0.054766164229015486, 0.10117024314529721, 0.3783801213572208, 0.17659810735413825, 0.23494094714367686]
printing an ep nov before normalisation:  20.47419751151181
maxi score, test score, baseline:  -0.9976200000000001 -1.0 -0.9976200000000001
probs:  [0.05414441677065144, 0.054766164229015486, 0.10117024314529721, 0.3783801213572208, 0.17659810735413825, 0.23494094714367686]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.218]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]] [[18.595]
 [11.829]
 [11.829]
 [11.829]
 [11.829]
 [11.829]
 [11.829]] [[0.218]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]]
maxi score, test score, baseline:  -0.9976200000000001 -1.0 -0.9976200000000001
probs:  [0.05414441677065144, 0.054766164229015486, 0.10117024314529721, 0.3783801213572208, 0.17659810735413825, 0.23494094714367686]
printing an ep nov before normalisation:  57.462183150495335
printing an ep nov before normalisation:  14.464953321567242
printing an ep nov before normalisation:  15.346523504750404
printing an ep nov before normalisation:  15.259916566368329
printing an ep nov before normalisation:  10.43447140274175
printing an ep nov before normalisation:  35.05085154935296
printing an ep nov before normalisation:  40.189970155018116
printing an ep nov before normalisation:  35.943776110954246
printing an ep nov before normalisation:  13.910024121570856
Printing some Q and Qe and total Qs values:  [[0.217]
 [0.217]
 [0.217]
 [0.217]
 [0.217]
 [0.217]
 [0.217]] [[19.828]
 [16.269]
 [16.269]
 [16.269]
 [16.269]
 [16.269]
 [16.269]] [[0.217]
 [0.217]
 [0.217]
 [0.217]
 [0.217]
 [0.217]
 [0.217]]
printing an ep nov before normalisation:  35.51541328430176
printing an ep nov before normalisation:  34.782094754100115
printing an ep nov before normalisation:  11.564548785039177
using explorer policy with actor:  0
printing an ep nov before normalisation:  50.52176599752558
maxi score, test score, baseline:  -0.9976200000000001 -1.0 -0.9976200000000001
probs:  [0.0540680126851359, 0.05469138575644259, 0.10121679218321578, 0.3778446889040817, 0.17684186911990032, 0.2353372513512238]
printing an ep nov before normalisation:  38.78231764270481
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
probs:  [0.05409388097205223, 0.05470619264569641, 0.10108883865240885, 0.37802731023728353, 0.1768151874350269, 0.235268590057532]
using explorer policy with actor:  1
printing an ep nov before normalisation:  29.624290466308594
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
probs:  [0.05409388097205223, 0.05470619264569641, 0.10108883865240885, 0.37802731023728353, 0.1768151874350269, 0.235268590057532]
printing an ep nov before normalisation:  20.06439442746258
printing an ep nov before normalisation:  40.07120431973906
printing an ep nov before normalisation:  41.761238887546774
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  69 total reward:  0.173333333333333  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  61 total reward:  0.10666666666666602  reward:  1.0 rdn_beta:  1.333
using another actor
actions average: 
K:  1  action  0 :  tensor([0.6709, 0.0025, 0.0597, 0.0645, 0.0598, 0.0683, 0.0744],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0487, 0.8777, 0.0167, 0.0184, 0.0105, 0.0077, 0.0202],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2893, 0.0425, 0.2393, 0.1027, 0.0906, 0.1081, 0.1275],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.3337, 0.0113, 0.1045, 0.2368, 0.0940, 0.0902, 0.1295],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.3030, 0.0182, 0.0790, 0.0965, 0.2951, 0.1016, 0.1066],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.3616, 0.0181, 0.1021, 0.0917, 0.0892, 0.2334, 0.1037],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2500, 0.0187, 0.1335, 0.1513, 0.0948, 0.0790, 0.2727],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
probs:  [0.055735103600799564, 0.05631308508658024, 0.10009521730341268, 0.3895287648645764, 0.1715758528458253, 0.2267519762988058]
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
probs:  [0.05576557372753115, 0.05634387195026603, 0.100149997010377, 0.3897421557079693, 0.17166980432445028, 0.2263285972794063]
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
probs:  [0.05576557372753115, 0.05634387195026603, 0.100149997010377, 0.3897421557079693, 0.17166980432445028, 0.2263285972794063]
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
probs:  [0.05576557372753115, 0.05634387195026603, 0.100149997010377, 0.3897421557079693, 0.17166980432445028, 0.2263285972794063]
printing an ep nov before normalisation:  17.702569870221474
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
probs:  [0.05576557372753115, 0.05634387195026603, 0.100149997010377, 0.3897421557079693, 0.17166980432445028, 0.2263285972794063]
printing an ep nov before normalisation:  23.900645121634877
printing an ep nov before normalisation:  25.757114159082125
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
probs:  [0.05576557372753115, 0.05634387195026603, 0.100149997010377, 0.3897421557079693, 0.17166980432445028, 0.2263285972794063]
Printing some Q and Qe and total Qs values:  [[0.647]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]] [[16.715]
 [13.283]
 [13.283]
 [13.283]
 [13.283]
 [13.283]
 [13.283]] [[0.647]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]]
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
probs:  [0.05572163703228404, 0.056301762951368434, 0.10024633611958533, 0.3894341207740161, 0.1714724225808484, 0.22682372054189756]
actions average: 
K:  1  action  0 :  tensor([0.5552, 0.0208, 0.0810, 0.0886, 0.0959, 0.0873, 0.0712],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0212, 0.9054, 0.0117, 0.0176, 0.0097, 0.0082, 0.0261],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1824, 0.0057, 0.1828, 0.1438, 0.1268, 0.2279, 0.1305],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1209, 0.1252, 0.1238, 0.2908, 0.0858, 0.1573, 0.0963],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2011, 0.0084, 0.1305, 0.1516, 0.1616, 0.2066, 0.1403],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2021, 0.0543, 0.1464, 0.1194, 0.1019, 0.2691, 0.1068],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2041, 0.0206, 0.1527, 0.1694, 0.1631, 0.1595, 0.1307],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  46.095039435819956
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
probs:  [0.05572163703228404, 0.056301762951368434, 0.10024633611958533, 0.3894341207740161, 0.1714724225808484, 0.22682372054189756]
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
probs:  [0.05564900023713409, 0.05623065042688913, 0.10029068718983852, 0.3889250917347693, 0.17170391894319625, 0.22720065146817273]
Printing some Q and Qe and total Qs values:  [[0.225]
 [0.288]
 [0.258]
 [0.281]
 [0.259]
 [0.215]
 [0.225]] [[33.855]
 [52.864]
 [47.93 ]
 [59.379]
 [54.786]
 [55.415]
 [33.855]] [[0.323]
 [0.512]
 [0.449]
 [0.548]
 [0.495]
 [0.456]
 [0.323]]
printing an ep nov before normalisation:  47.919241512242436
printing an ep nov before normalisation:  0.0
printing an ep nov before normalisation:  74.52768781211091
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
probs:  [0.0555609241718938, 0.05623725301497007, 0.10038772149460887, 0.38830841072902783, 0.1719475264607068, 0.2275581641287926]
printing an ep nov before normalisation:  41.23373334809305
printing an ep nov before normalisation:  29.877752421671815
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
probs:  [0.0555609241718938, 0.05623725301497007, 0.10038772149460887, 0.38830841072902783, 0.1719475264607068, 0.2275581641287926]
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
probs:  [0.0555609241718938, 0.05623725301497007, 0.10038772149460887, 0.38830841072902783, 0.1719475264607068, 0.2275581641287926]
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
probs:  [0.0555609241718938, 0.05623725301497007, 0.10038772149460887, 0.38830841072902783, 0.1719475264607068, 0.2275581641287926]
siam score:  -0.7357795
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  25.701713218653374
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.96927576145652
printing an ep nov before normalisation:  49.5657177811357
printing an ep nov before normalisation:  61.71625235335654
actor:  1 policy actor:  1  step number:  76 total reward:  0.059999999999999276  reward:  1.0 rdn_beta:  1.333
from probs:  [0.05619514672801645, 0.05685762731184523, 0.09986262079747796, 0.3927527482184485, 0.1701986599944883, 0.22413319694972342]
siam score:  -0.72669005
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
probs:  [0.05622514237078725, 0.0568879774114549, 0.09991598049830189, 0.39296281727986804, 0.1702896526323676, 0.22371842980722023]
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
probs:  [0.05622514237078725, 0.0568879774114549, 0.09991598049830189, 0.39296281727986804, 0.1702896526323676, 0.22371842980722023]
printing an ep nov before normalisation:  41.03676248676642
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
printing an ep nov before normalisation:  23.751675928495953
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.172684615392726
printing an ep nov before normalisation:  27.599865095424825
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
Printing some Q and Qe and total Qs values:  [[0.362]
 [0.393]
 [0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]] [[43.332]
 [47.367]
 [43.332]
 [43.332]
 [43.332]
 [43.332]
 [43.332]] [[0.362]
 [0.393]
 [0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]]
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
probs:  [0.05615434458958666, 0.05681890861469578, 0.0999591488998198, 0.3924666763227085, 0.17051638855331824, 0.22408453301987089]
actor:  1 policy actor:  1  step number:  66 total reward:  0.086666666666666  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
probs:  [0.0554948509795602, 0.05624407895472779, 0.09902956127677102, 0.387848172916328, 0.16900658296081342, 0.23237675291179943]
printing an ep nov before normalisation:  49.898225717024374
Printing some Q and Qe and total Qs values:  [[0.297]
 [0.369]
 [0.304]
 [0.292]
 [0.232]
 [0.28 ]
 [0.29 ]] [[31.416]
 [30.272]
 [31.191]
 [32.495]
 [43.169]
 [33.249]
 [28.756]] [[0.945]
 [0.968]
 [0.943]
 [0.987]
 [1.383]
 [1.007]
 [0.825]]
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.352]
 [0.27 ]
 [0.27 ]
 [0.27 ]
 [0.27 ]
 [0.27 ]] [[46.906]
 [51.514]
 [52.49 ]
 [52.49 ]
 [52.49 ]
 [52.49 ]
 [52.49 ]] [[1.97 ]
 [2.065]
 [2.032]
 [2.032]
 [2.032]
 [2.032]
 [2.032]]
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
probs:  [0.055525698982813176, 0.056275344434659556, 0.09908466721868822, 0.38806421122964285, 0.16910068073909126, 0.23194939739510484]
printing an ep nov before normalisation:  65.34935481423382
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
probs:  [0.05555640430820486, 0.05630646530582128, 0.0991395182851805, 0.38827925032481553, 0.1691943432973114, 0.23152401847866633]
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.552]
 [0.508]
 [0.503]
 [0.507]
 [0.522]
 [0.487]] [[65.903]
 [66.587]
 [64.727]
 [63.116]
 [62.365]
 [62.064]
 [61.434]] [[1.908]
 [1.952]
 [1.846]
 [1.788]
 [1.767]
 [1.773]
 [1.717]]
Printing some Q and Qe and total Qs values:  [[-0.037]
 [-0.088]
 [-0.088]
 [-0.088]
 [-0.088]
 [-0.088]
 [-0.088]] [[35.787]
 [37.613]
 [37.613]
 [37.613]
 [37.613]
 [37.613]
 [37.613]] [[1.774]
 [1.816]
 [1.816]
 [1.816]
 [1.816]
 [1.816]
 [1.816]]
printing an ep nov before normalisation:  35.3729155225446
Printing some Q and Qe and total Qs values:  [[0.611]
 [0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]] [[64.743]
 [49.533]
 [49.533]
 [49.533]
 [49.533]
 [49.533]
 [49.533]] [[2.23]
 [1.76]
 [1.76]
 [1.76]
 [1.76]
 [1.76]
 [1.76]]
printing an ep nov before normalisation:  59.31873975172917
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
probs:  [0.05561505225619395, 0.056365906956739646, 0.09924428520928076, 0.38868998043460684, 0.16886720021093019, 0.23121757493224862]
Printing some Q and Qe and total Qs values:  [[0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]] [[33.058]
 [33.058]
 [33.058]
 [33.058]
 [33.058]
 [33.058]
 [33.058]] [[2.304]
 [2.304]
 [2.304]
 [2.304]
 [2.304]
 [2.304]
 [2.304]]
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
probs:  [0.05561505225619395, 0.056365906956739646, 0.09924428520928076, 0.38868998043460684, 0.16886720021093019, 0.23121757493224862]
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
probs:  [0.05561505225619395, 0.056365906956739646, 0.09924428520928076, 0.38868998043460684, 0.16886720021093019, 0.23121757493224862]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
siam score:  -0.7119762
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
probs:  [0.05561505225619395, 0.056365906956739646, 0.09924428520928076, 0.38868998043460684, 0.16886720021093019, 0.23121757493224862]
Printing some Q and Qe and total Qs values:  [[0.161]
 [0.14 ]
 [0.14 ]
 [0.14 ]
 [0.14 ]
 [0.14 ]
 [0.14 ]] [[45.997]
 [43.231]
 [43.231]
 [43.231]
 [43.231]
 [43.231]
 [43.231]] [[1.75 ]
 [1.601]
 [1.601]
 [1.601]
 [1.601]
 [1.601]
 [1.601]]
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
probs:  [0.05561505225619395, 0.056365906956739646, 0.09924428520928076, 0.38868998043460684, 0.16886720021093019, 0.23121757493224862]
from probs:  [0.05561505225619395, 0.056365906956739646, 0.09924428520928076, 0.38868998043460684, 0.16886720021093019, 0.23121757493224862]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.3662],
        [ 0.5369],
        [-0.5890],
        [-0.3673],
        [-0.5968],
        [-0.5560],
        [-0.4484],
        [-0.5001],
        [-0.5894],
        [-0.3974]], dtype=torch.float64)
-0.032346567066 -0.39850568718612983
-0.07129443439800001 0.4656012042163499
-0.032346567066 -0.6213836882584731
-0.032346567066 -0.3996504635597182
-0.032346567066 -0.6291609967930959
-0.032346567066 -0.5883798186923053
-0.09703970119800001 -0.545470621505698
-0.032346567066 -0.5324490894928969
-0.032346567066 -0.6217466363933793
-0.07129183386599999 -0.46870665391744504
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
printing an ep nov before normalisation:  27.17324884622589
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
Printing some Q and Qe and total Qs values:  [[0.25 ]
 [0.311]
 [0.264]
 [0.268]
 [0.248]
 [0.249]
 [0.251]] [[17.017]
 [31.782]
 [21.154]
 [27.178]
 [16.608]
 [16.731]
 [16.643]] [[0.25 ]
 [0.311]
 [0.264]
 [0.268]
 [0.248]
 [0.249]
 [0.251]]
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
probs:  [0.055673432228012416, 0.056425077004873356, 0.09934857342883825, 0.38909883382260735, 0.16854208651335012, 0.2309119970023185]
printing an ep nov before normalisation:  24.455918841969734
printing an ep nov before normalisation:  31.498146057128906
actor:  1 policy actor:  1  step number:  77 total reward:  0.05333333333333257  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.128]
 [0.139]
 [0.134]
 [0.135]
 [0.146]
 [0.153]
 [0.173]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.128]
 [0.139]
 [0.134]
 [0.135]
 [0.146]
 [0.153]
 [0.173]]
printing an ep nov before normalisation:  25.214405059814453
printing an ep nov before normalisation:  31.543136972099646
printing an ep nov before normalisation:  27.818945324902703
line 256 mcts: sample exp_bonus 31.980192843172887
maxi score, test score, baseline:  -0.9976200000000001 -0.943 -0.943
probs:  [0.05515426879993517, 0.05589888755887439, 0.0984211555509761, 0.38546296830428606, 0.16696788058085726, 0.23809483920507105]
actor:  0 policy actor:  1  step number:  55 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  36.54691017251005
printing an ep nov before normalisation:  54.70050111527412
siam score:  -0.7042624
printing an ep nov before normalisation:  39.40723795146503
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05515426879993517, 0.05589888755887439, 0.0984211555509761, 0.38546296830428606, 0.16696788058085726, 0.23809483920507105]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05515426879993517, 0.05589888755887439, 0.0984211555509761, 0.38546296830428606, 0.16696788058085726, 0.23809483920507105]
printing an ep nov before normalisation:  14.757474660873413
printing an ep nov before normalisation:  38.83146114874826
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.385]
 [0.5  ]] [[36.653]
 [36.653]
 [36.653]
 [36.653]
 [36.653]
 [34.76 ]
 [36.653]] [[0.868]
 [0.868]
 [0.868]
 [0.868]
 [0.868]
 [0.718]
 [0.868]]
printing an ep nov before normalisation:  26.68408525079237
printing an ep nov before normalisation:  35.98488361035285
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05508260728131703, 0.05582911742632409, 0.09845939508684706, 0.38496078629208685, 0.16718023380801147, 0.23848786010541337]
printing an ep nov before normalisation:  53.91688981104035
Printing some Q and Qe and total Qs values:  [[0.511]
 [0.469]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]] [[58.136]
 [46.686]
 [57.408]
 [57.408]
 [57.408]
 [57.408]
 [57.408]] [[1.844]
 [1.442]
 [1.772]
 [1.772]
 [1.772]
 [1.772]
 [1.772]]
printing an ep nov before normalisation:  32.33867600933128
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.055011105937931876, 0.05575950324144908, 0.09849754915109134, 0.38445972673894746, 0.16739211239091747, 0.23888000253966274]
printing an ep nov before normalisation:  28.37061882019043
Printing some Q and Qe and total Qs values:  [[0.416]
 [0.534]
 [0.512]
 [0.515]
 [0.518]
 [0.524]
 [0.416]] [[69.571]
 [57.067]
 [63.739]
 [64.615]
 [64.154]
 [63.883]
 [69.571]] [[2.225]
 [1.931]
 [2.129]
 [2.16 ]
 [2.148]
 [2.145]
 [2.225]]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05493976423335236, 0.05569004448198014, 0.09853561802995368, 0.3839597858857489, 0.1676035179191602, 0.23927126944980473]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05493976423335236, 0.05569004448198014, 0.09853561802995368, 0.3839597858857489, 0.1676035179191602, 0.23927126944980473]
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.525]
 [0.429]
 [0.431]
 [0.434]
 [0.44 ]
 [0.44 ]] [[14.43 ]
 [41.824]
 [11.888]
 [11.597]
 [11.49 ]
 [11.414]
 [11.675]] [[0.682]
 [1.568]
 [0.61 ]
 [0.604]
 [0.603]
 [0.607]
 [0.615]]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05493976423335236, 0.05569004448198014, 0.09853561802995368, 0.3839597858857489, 0.1676035179191602, 0.23927126944980473]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05493976423335236, 0.05569004448198014, 0.09853561802995368, 0.3839597858857489, 0.1676035179191602, 0.23927126944980473]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 37.18850945512971
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05493976423335236, 0.05569004448198014, 0.09853561802995368, 0.3839597858857489, 0.1676035179191602, 0.23927126944980473]
line 256 mcts: sample exp_bonus 18.6014083208049
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.054967087278612575, 0.05571774156913632, 0.09858467518835407, 0.38415113733630973, 0.16718887019653808, 0.23939048843104924]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05498378708952242, 0.055823810212356155, 0.09869222342534671, 0.3842685421765964, 0.16729878637383228, 0.23893285072234605]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.054912676744106645, 0.0557548044582772, 0.09873062006129574, 0.38377022326300664, 0.16750906968064228, 0.2393226057926716]
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]] [[36.336]
 [36.336]
 [36.336]
 [36.336]
 [36.336]
 [36.336]
 [36.336]] [[1.48]
 [1.48]
 [1.48]
 [1.48]
 [1.48]
 [1.48]
 [1.48]]
Printing some Q and Qe and total Qs values:  [[0.193]
 [0.302]
 [0.193]
 [0.193]
 [0.193]
 [0.193]
 [0.193]] [[32.43 ]
 [39.003]
 [32.43 ]
 [32.43 ]
 [32.43 ]
 [32.43 ]
 [32.43 ]] [[0.193]
 [0.302]
 [0.193]
 [0.193]
 [0.193]
 [0.193]
 [0.193]]
printing an ep nov before normalisation:  31.125788688659668
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.054854867879616645, 0.05569929832094541, 0.09855257808743867, 0.3833650592477415, 0.1677591461494218, 0.23976905031483606]
printing an ep nov before normalisation:  29.57066774368286
printing an ep nov before normalisation:  38.15201806818059
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.054882096069922594, 0.055726946679804115, 0.09860154922157734, 0.3835557463150782, 0.16734537429854796, 0.23988828741506976]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.054882096069922594, 0.055726946679804115, 0.09860154922157734, 0.3835557463150782, 0.16734537429854796, 0.23988828741506976]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.054882096069922594, 0.055726946679804115, 0.09860154922157734, 0.3835557463150782, 0.16734537429854796, 0.23988828741506976]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.054882096069922594, 0.055726946679804115, 0.09860154922157734, 0.3835557463150782, 0.16734537429854796, 0.23988828741506976]
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.278]
 [0.294]
 [0.297]
 [0.294]
 [0.294]
 [0.294]] [[34.802]
 [43.021]
 [33.762]
 [32.159]
 [33.762]
 [33.762]
 [33.762]] [[0.915]
 [1.208]
 [0.89 ]
 [0.836]
 [0.89 ]
 [0.89 ]
 [0.89 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.054882096069922594, 0.055726946679804115, 0.09860154922157734, 0.3835557463150782, 0.16734537429854796, 0.23988828741506976]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.054811349541064446, 0.0556582984896173, 0.09863938782974184, 0.38305997647325235, 0.16755395057901476, 0.24027703708730927]
printing an ep nov before normalisation:  41.90901346167837
actor:  1 policy actor:  1  step number:  65 total reward:  0.2799999999999998  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  24.554471969604492
siam score:  -0.7112149
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.055792889092449185, 0.056610725624727264, 0.09811441379805072, 0.38993830274579033, 0.16466015674823323, 0.23488351199074922]
Printing some Q and Qe and total Qs values:  [[0.437]
 [0.783]
 [0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]] [[41.625]
 [35.229]
 [41.625]
 [41.625]
 [41.625]
 [41.625]
 [41.625]] [[2.104]
 [2.017]
 [2.104]
 [2.104]
 [2.104]
 [2.104]
 [2.104]]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.055805740731642815, 0.05662376609977779, 0.09790632508981877, 0.3900283074993562, 0.16469814549363762, 0.2349377150857668]
using explorer policy with actor:  1
printing an ep nov before normalisation:  25.85414552476182
printing an ep nov before normalisation:  49.3475321011271
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05579126424979887, 0.056694905575787075, 0.09795615614738493, 0.38992735678394375, 0.1647135012644832, 0.23491681597860214]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05579126424979887, 0.056694905575787075, 0.09795615614738493, 0.38992735678394375, 0.1647135012644832, 0.23491681597860214]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.055817873327064255, 0.05672194666724024, 0.0980029234788186, 0.3901137101407974, 0.16431448472029125, 0.2350290616657882]
printing an ep nov before normalisation:  25.7843017578125
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05574895312204187, 0.05665528822916123, 0.09803953979135213, 0.38963073942735094, 0.1645169960885838, 0.23540848334151007]
printing an ep nov before normalisation:  1.6113402009113997
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05574895312204187, 0.05665528822916123, 0.09803953979135213, 0.38963073942735094, 0.1645169960885838, 0.23540848334151007]
printing an ep nov before normalisation:  54.91591283829893
printing an ep nov before normalisation:  35.67440748214722
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.055680179680583905, 0.05658877173827996, 0.09807607813051718, 0.38914879718593803, 0.16471907621483003, 0.23578709704985104]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05570667806076192, 0.05661570355749472, 0.0981228012498732, 0.3893343750068091, 0.16432092764245565, 0.2358995144826055]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05570667806076192, 0.05661570355749472, 0.0981228012498732, 0.3893343750068091, 0.16432092764245565, 0.2358995144826055]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05570667806076192, 0.05661570355749472, 0.0981228012498732, 0.3893343750068091, 0.16432092764245565, 0.2358995144826055]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05570667806076192, 0.05661570355749472, 0.0981228012498732, 0.3893343750068091, 0.16432092764245565, 0.2358995144826055]
printing an ep nov before normalisation:  58.73443844511517
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.434]] [[30.865]
 [28.999]
 [28.999]
 [28.999]
 [28.999]
 [28.999]
 [28.999]] [[1.696]
 [1.449]
 [1.449]
 [1.449]
 [1.449]
 [1.449]
 [1.449]]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05573300070273101, 0.056642456763917284, 0.09816921449982376, 0.3895187220689078, 0.16392541960571122, 0.23601118635890894]
printing an ep nov before normalisation:  40.35283978189824
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05573300070273101, 0.056642456763917284, 0.09816921449982376, 0.3895187220689078, 0.16392541960571122, 0.23601118635890894]
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]] [[23.377]
 [21.566]
 [21.566]
 [21.566]
 [21.566]
 [21.566]
 [21.566]] [[1.287]
 [0.928]
 [0.928]
 [0.928]
 [0.928]
 [0.928]
 [0.928]]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05573300070273101, 0.056642456763917284, 0.09816921449982376, 0.3895187220689078, 0.16392541960571122, 0.23601118635890894]
siam score:  -0.7068908
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]] [[27.17]
 [27.17]
 [27.17]
 [27.17]
 [27.17]
 [27.17]
 [27.17]] [[0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]]
printing an ep nov before normalisation:  35.45389907152341
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.055664439188900364, 0.056576151690273496, 0.09820594095126867, 0.38903826438292344, 0.16412529296448738, 0.23638991082214655]
printing an ep nov before normalisation:  12.788799055275065
printing an ep nov before normalisation:  14.457452578864352
printing an ep nov before normalisation:  37.17172861099243
from probs:  [0.055664439188900364, 0.056576151690273496, 0.09820594095126867, 0.38903826438292344, 0.16412529296448738, 0.23638991082214655]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05566975994401928, 0.056485820816599766, 0.09821533784489306, 0.3890755275432065, 0.1641410059588102, 0.23641254789247115]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.689]
 [0.487]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.613]] [[ 0.   ]
 [25.606]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [33.887]] [[0.533]
 [1.033]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [1.386]]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05546496533395625, 0.056287060272011386, 0.09832513104480442, 0.3876403906766048, 0.1647382625468409, 0.23754419012578215]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05546496533395625, 0.056287060272011386, 0.09832513104480442, 0.3876403906766048, 0.1647382625468409, 0.23754419012578215]
printing an ep nov before normalisation:  54.1864266258741
printing an ep nov before normalisation:  25.1328182220459
line 256 mcts: sample exp_bonus 37.51159214761871
printing an ep nov before normalisation:  36.47536545803724
Printing some Q and Qe and total Qs values:  [[0.179]
 [0.572]
 [0.472]
 [0.507]
 [0.518]
 [0.503]
 [0.52 ]] [[53.558]
 [36.757]
 [30.703]
 [30.762]
 [32.297]
 [33.467]
 [29.116]] [[1.487]
 [1.308]
 [1.002]
 [1.04 ]
 [1.103]
 [1.127]
 [0.996]]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.055396988958792415, 0.056221086751797406, 0.09836157411205758, 0.38716403340321004, 0.16493650671881674, 0.23791981005532586]
printing an ep nov before normalisation:  25.49432477722915
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05542809841290716, 0.05625266011266504, 0.09841686948361641, 0.3873819034258017, 0.1650292789057138, 0.23749118965929597]
siam score:  -0.7144046
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.055360302736534917, 0.05618686531709333, 0.09845339032679555, 0.38690681210484856, 0.16522744137150214, 0.2378651881432255]
printing an ep nov before normalisation:  41.621163589906885
using explorer policy with actor:  0
from probs:  [0.055360302736534917, 0.05618686531709333, 0.09845339032679555, 0.38690681210484856, 0.16522744137150214, 0.2378651881432255]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05529264982956126, 0.05612120907731008, 0.09848983426154784, 0.38643272126808265, 0.16542518653138574, 0.23823839903211236]
printing an ep nov before normalisation:  31.664173733738988
printing an ep nov before normalisation:  58.78484667206863
siam score:  -0.71213496
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  32.56208273524842
printing an ep nov before normalisation:  51.8482320389941
printing an ep nov before normalisation:  57.32066123070076
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05529795712281665, 0.056030455659408944, 0.09849929787321259, 0.3864698899799257, 0.16544109048495087, 0.23826130887968516]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05529795712281665, 0.056030455659408944, 0.09849929787321259, 0.3864698899799257, 0.16544109048495087, 0.23826130887968516]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]] [[21.087]
 [16.599]
 [16.599]
 [16.599]
 [16.599]
 [16.599]
 [16.599]] [[1.525]
 [0.983]
 [0.983]
 [0.983]
 [0.983]
 [0.983]
 [0.983]]
line 256 mcts: sample exp_bonus 29.50350124459523
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.651]
 [0.529]
 [0.533]
 [0.515]
 [0.525]
 [0.513]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.515]
 [0.651]
 [0.529]
 [0.533]
 [0.515]
 [0.525]
 [0.513]]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.055310901575842474, 0.056043571993867614, 0.09828791624085674, 0.38656054422142544, 0.1654798801221431, 0.23831718584586467]
Printing some Q and Qe and total Qs values:  [[0.518]
 [0.518]
 [0.521]
 [0.518]
 [0.518]
 [0.518]
 [0.518]] [[ 0.   ]
 [ 0.   ]
 [53.103]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.03 ]
 [0.03 ]
 [1.742]
 [0.03 ]
 [0.03 ]
 [0.03 ]
 [0.03 ]]
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.595]
 [0.71 ]
 [0.595]
 [0.595]
 [0.595]
 [0.595]] [[44.096]
 [44.096]
 [40.487]
 [44.096]
 [44.096]
 [44.096]
 [44.096]] [[2.136]
 [2.136]
 [2.063]
 [2.136]
 [2.136]
 [2.136]
 [2.136]]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.055341965063090685, 0.05607504795406363, 0.09834317458501535, 0.3867780919902809, 0.16557296566217156, 0.237888754745378]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.055341965063090685, 0.05607504795406363, 0.09834317458501535, 0.3867780919902809, 0.16557296566217156, 0.237888754745378]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
siam score:  -0.7140493
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05537288873735255, 0.05610638224478263, 0.09839818421807286, 0.38699466060302967, 0.16566563223548095, 0.23746225196128137]
actor:  1 policy actor:  1  step number:  68 total reward:  0.21999999999999942  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  37.29398250579834
line 256 mcts: sample exp_bonus 51.72975078009392
actions average: 
K:  0  action  0 :  tensor([0.4199, 0.0055, 0.1111, 0.1329, 0.1309, 0.1018, 0.0980],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0070, 0.9439, 0.0046, 0.0160, 0.0025, 0.0022, 0.0238],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2879, 0.0039, 0.2056, 0.1278, 0.1207, 0.1629, 0.0913],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1715, 0.0391, 0.0988, 0.3306, 0.1298, 0.1354, 0.0949],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2665, 0.0167, 0.0699, 0.0995, 0.4191, 0.0724, 0.0560],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1763, 0.0021, 0.1304, 0.1129, 0.1085, 0.3993, 0.0704],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1914, 0.0713, 0.1102, 0.1410, 0.1129, 0.1652, 0.2079],
       grad_fn=<DivBackward0>)
siam score:  -0.7109238
printing an ep nov before normalisation:  53.82800366783178
actions average: 
K:  1  action  0 :  tensor([0.5323, 0.0090, 0.0767, 0.0831, 0.1586, 0.0631, 0.0771],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0300, 0.8595, 0.0194, 0.0260, 0.0146, 0.0113, 0.0391],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2300, 0.0248, 0.1832, 0.1454, 0.1496, 0.1276, 0.1393],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1986, 0.0116, 0.1320, 0.1920, 0.2072, 0.1418, 0.1168],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2601, 0.0055, 0.1369, 0.1296, 0.2317, 0.1200, 0.1162],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2331, 0.0155, 0.1168, 0.1283, 0.1530, 0.2458, 0.1075],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2192, 0.1026, 0.1056, 0.1091, 0.1120, 0.0783, 0.2733],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05442080071351773, 0.05514165202947007, 0.09670452995221507, 0.3803268765360234, 0.1800343939715147, 0.233371746797259]
printing an ep nov before normalisation:  38.07880215543909
siam score:  -0.714474
printing an ep nov before normalisation:  53.67003545178237
actor:  1 policy actor:  1  step number:  55 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  49.95800020229622
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]] [[57.06 ]
 [46.319]
 [46.319]
 [46.319]
 [46.319]
 [46.319]
 [46.319]] [[1.069]
 [0.988]
 [0.988]
 [0.988]
 [0.988]
 [0.988]
 [0.988]]
siam score:  -0.71245694
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.0553783628802954, 0.056076283417803116, 0.09587554582735822, 0.3870369512125967, 0.17699610094195903, 0.22863675571998746]
using explorer policy with actor:  1
printing an ep nov before normalisation:  54.386883741180725
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05538337229666566, 0.055990752449687015, 0.09588422737245188, 0.3870720341535913, 0.1770121381868524, 0.2286574755407517]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.374]
 [0.434]
 [0.395]
 [0.381]
 [0.383]
 [0.406]
 [0.37 ]] [[16.746]
 [22.468]
 [14.716]
 [14.52 ]
 [14.412]
 [14.645]
 [28.412]] [[1.574]
 [2.434]
 [1.311]
 [1.27 ]
 [1.257]
 [1.312]
 [3.201]]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05538337229666566, 0.055990752449687015, 0.09588422737245188, 0.3870720341535913, 0.1770121381868524, 0.2286574755407517]
line 256 mcts: sample exp_bonus 29.464748560778332
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05538337229666566, 0.055990752449687015, 0.09588422737245188, 0.3870720341535913, 0.1770121381868524, 0.2286574755407517]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05544128273826391, 0.05604929951680965, 0.09598458878643075, 0.38747760407539333, 0.17668037334414885, 0.22836685153895367]
printing an ep nov before normalisation:  53.51816579792591
Printing some Q and Qe and total Qs values:  [[-0.049]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.033]] [[33.954]
 [24.983]
 [24.983]
 [24.983]
 [24.983]
 [24.983]
 [24.983]] [[1.214]
 [0.81 ]
 [0.81 ]
 [0.81 ]
 [0.81 ]
 [0.81 ]
 [0.81 ]]
Printing some Q and Qe and total Qs values:  [[0.808]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]] [[17.975]
 [18.082]
 [18.082]
 [18.082]
 [18.082]
 [18.082]
 [18.082]] [[1.637]
 [1.377]
 [1.377]
 [1.377]
 [1.377]
 [1.377]
 [1.377]]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05547047932072877, 0.05607881706539234, 0.09603518778410121, 0.3876820793883138, 0.17677357085819073, 0.22795986558327322]
printing an ep nov before normalisation:  33.36605114402104
printing an ep nov before normalisation:  37.407025880890075
Printing some Q and Qe and total Qs values:  [[0.044]
 [0.069]
 [0.044]
 [0.044]
 [0.044]
 [0.044]
 [0.044]] [[30.924]
 [37.35 ]
 [30.924]
 [30.924]
 [30.924]
 [30.924]
 [30.924]] [[0.245]
 [0.35 ]
 [0.245]
 [0.245]
 [0.245]
 [0.245]
 [0.245]]
line 256 mcts: sample exp_bonus 29.469479323777698
printing an ep nov before normalisation:  42.551677271208376
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
Printing some Q and Qe and total Qs values:  [[0.44 ]
 [0.285]
 [0.285]
 [0.285]
 [0.285]
 [0.285]
 [0.285]] [[30.247]
 [25.616]
 [25.616]
 [25.616]
 [25.616]
 [25.616]
 [25.616]] [[1.362]
 [1.032]
 [1.032]
 [1.032]
 [1.032]
 [1.032]
 [1.032]]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05540379602814595, 0.056013587949421344, 0.09606547077518371, 0.3872147909220061, 0.17699685168538268, 0.22830550263986027]
line 256 mcts: sample exp_bonus 21.94412598139159
printing an ep nov before normalisation:  44.21355931060322
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]] [[25.835]
 [19.261]
 [19.261]
 [19.261]
 [19.261]
 [19.261]
 [19.261]] [[1.11 ]
 [0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
printing an ep nov before normalisation:  23.615866587108957
printing an ep nov before normalisation:  39.20663006193088
using explorer policy with actor:  0
from probs:  [0.05533725161663784, 0.055948494685916214, 0.09609569069598001, 0.38674847567585446, 0.17721966748632825, 0.22865041983928333]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05532342003486275, 0.05601795837613531, 0.09614501879519112, 0.3866520285731755, 0.17722830833014092, 0.22863326589049449]
actions average: 
K:  2  action  0 :  tensor([0.5413, 0.0415, 0.0822, 0.0751, 0.1008, 0.0704, 0.0889],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0151, 0.9303, 0.0126, 0.0105, 0.0068, 0.0057, 0.0190],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1476, 0.0472, 0.3700, 0.0762, 0.0841, 0.0879, 0.1871],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.3084, 0.0255, 0.1037, 0.2287, 0.1427, 0.1051, 0.0859],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.3007, 0.0069, 0.1399, 0.1481, 0.1518, 0.1171, 0.1356],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2476, 0.0582, 0.1365, 0.0891, 0.0754, 0.3025, 0.0907],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2977, 0.0159, 0.1126, 0.1416, 0.1566, 0.0670, 0.2085],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05532342003486275, 0.05601795837613531, 0.09614501879519112, 0.3866520285731755, 0.17722830833014092, 0.22863326589049449]
printing an ep nov before normalisation:  39.37927164001364
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05525701470272786, 0.05595319768492799, 0.09617527748872648, 0.3861866890705637, 0.17745056923375765, 0.22897725181929623]
printing an ep nov before normalisation:  18.916412591934204
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05525701470272786, 0.05595319768492799, 0.09617527748872648, 0.3861866890705637, 0.17745056923375765, 0.22897725181929623]
printing an ep nov before normalisation:  20.291600374372752
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05525701470272786, 0.05595319768492799, 0.09617527748872648, 0.3861866890705637, 0.17745056923375765, 0.22897725181929623]
maxi score, test score, baseline:  -0.9952466666666667 -0.943 -0.943
probs:  [0.05525701470272786, 0.05595319768492799, 0.09617527748872648, 0.3861866890705637, 0.17745056923375765, 0.22897725181929623]
actor:  0 policy actor:  1  step number:  59 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05525701470272786, 0.05595319768492799, 0.09617527748872648, 0.3861866890705637, 0.17745056923375765, 0.22897725181929623]
printing an ep nov before normalisation:  68.66599565513187
printing an ep nov before normalisation:  33.16054869663456
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05528615973194867, 0.055982710799763254, 0.09622605680704273, 0.3863908031174814, 0.17754432039491883, 0.22856994914884524]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.0552199261969245, 0.055918120221597055, 0.09625638831623703, 0.38592666720710045, 0.1777664575089178, 0.22891244054922313]
printing an ep nov before normalisation:  58.83450634978483
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.055248524470026254, 0.055947080962292554, 0.09630629067839552, 0.3861269519633513, 0.17733994059352606, 0.2290312113324085]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.489]
 [0.133]
 [0.46 ]
 [0.426]
 [0.195]
 [0.494]] [[33.796]
 [38.044]
 [25.976]
 [28.674]
 [29.867]
 [20.405]
 [34.287]] [[0.701]
 [0.881]
 [0.346]
 [0.713]
 [0.696]
 [0.326]
 [0.83 ]]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.055276938700353155, 0.055975855327570906, 0.09635587189636566, 0.3863259477969399, 0.1769161685066888, 0.22914921777208158]
printing an ep nov before normalisation:  46.300558126173144
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.27272669359284
printing an ep nov before normalisation:  59.681259348676186
printing an ep nov before normalisation:  37.86095721521949
printing an ep nov before normalisation:  22.445855964753232
printing an ep nov before normalisation:  42.39604815460485
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.055215920227880536, 0.05582561566146278, 0.09639516479151562, 0.385898335247603, 0.17715218300340765, 0.22951278106813047]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.055179211542318814, 0.055790658123454906, 0.09647672980363095, 0.3856409738078527, 0.1774656957177819, 0.22944673100496066]
printing an ep nov before normalisation:  30.228855441091568
printing an ep nov before normalisation:  27.969371878331525
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.055179211542318814, 0.055790658123454906, 0.09647672980363095, 0.3856409738078527, 0.1774656957177819, 0.22944673100496066]
siam score:  -0.7143303
printing an ep nov before normalisation:  29.87300320912011
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.055179211542318814, 0.055790658123454906, 0.09647672980363095, 0.3856409738078527, 0.1774656957177819, 0.22944673100496066]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.055208260141272766, 0.05582002939202018, 0.09652757172775356, 0.3858444120782463, 0.17755927674448382, 0.22904044991622347]
printing an ep nov before normalisation:  30.936083793640137
printing an ep nov before normalisation:  29.14501704569623
printing an ep nov before normalisation:  30.137981851566096
printing an ep nov before normalisation:  8.706667163021109
printing an ep nov before normalisation:  54.140095197655924
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.055220507144996954, 0.0558324124345233, 0.09632681520759034, 0.38593018245426686, 0.17759873087390152, 0.22909135188472102]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.055220507144996954, 0.0558324124345233, 0.09632681520759034, 0.38593018245426686, 0.17759873087390152, 0.22909135188472102]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.055220507144996954, 0.0558324124345233, 0.09632681520759034, 0.38593018245426686, 0.17759873087390152, 0.22909135188472102]
printing an ep nov before normalisation:  29.666479216202447
actor:  1 policy actor:  1  step number:  60 total reward:  0.09999999999999931  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05449646214356867, 0.05510032479669933, 0.0950624854676975, 0.3808594228279179, 0.18839928615662258, 0.22608201860749402]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05449646214356867, 0.05510032479669933, 0.0950624854676975, 0.3808594228279179, 0.18839928615662258, 0.22608201860749402]
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.761]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]] [[16.973]
 [18.237]
 [16.973]
 [16.973]
 [16.973]
 [16.973]
 [16.973]] [[0.782]
 [1.021]
 [0.782]
 [0.782]
 [0.782]
 [0.782]
 [0.782]]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.0544418411599669, 0.055047226342946926, 0.09489201768693223, 0.3804766186240758, 0.18868227678953245, 0.22646001939654578]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.0544418411599669, 0.055047226342946926, 0.09489201768693223, 0.3804766186240758, 0.18868227678953245, 0.22646001939654578]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.0544700595309288, 0.05507575926775731, 0.09494125368279865, 0.3806742423536027, 0.18878024553899275, 0.22605843962591987]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.0544700595309288, 0.05507575926775731, 0.09494125368279865, 0.3806742423536027, 0.18878024553899275, 0.22605843962591987]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.0544700595309288, 0.05507575926775731, 0.09494125368279865, 0.3806742423536027, 0.18878024553899275, 0.22605843962591987]
printing an ep nov before normalisation:  48.12780772341242
Printing some Q and Qe and total Qs values:  [[0.039]
 [0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]] [[39.847]
 [34.052]
 [34.052]
 [34.052]
 [34.052]
 [34.052]
 [34.052]] [[1.109]
 [0.922]
 [0.922]
 [0.922]
 [0.922]
 [0.922]
 [0.922]]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.054403769258583805, 0.055010858501759706, 0.09496780641126906, 0.38020971416517996, 0.18902206974091482, 0.22638578192229258]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05439049357487333, 0.05508033342135218, 0.09501737269852667, 0.38011715807910645, 0.18902477329600387, 0.22636986893013755]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05439535448901023, 0.05499573764984974, 0.09502587337895911, 0.3801512008787486, 0.18904169610916238, 0.22639013749426998]
actions average: 
K:  0  action  0 :  tensor([0.5296, 0.0454, 0.1019, 0.0957, 0.0862, 0.0700, 0.0713],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0118, 0.9408, 0.0040, 0.0178, 0.0071, 0.0034, 0.0150],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0808, 0.0314, 0.3036, 0.1233, 0.0674, 0.2369, 0.1565],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1904, 0.0172, 0.1010, 0.3461, 0.1034, 0.1228, 0.1192],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2246, 0.0084, 0.1226, 0.1223, 0.3052, 0.1123, 0.1046],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1763, 0.0045, 0.1919, 0.1085, 0.0890, 0.3464, 0.0835],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1731, 0.1029, 0.1191, 0.1252, 0.0976, 0.0829, 0.2992],
       grad_fn=<DivBackward0>)
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05439535448901023, 0.05499573764984974, 0.09502587337895911, 0.3801512008787486, 0.18904169610916238, 0.22639013749426998]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05439535448901023, 0.05499573764984974, 0.09502587337895911, 0.3801512008787486, 0.18904169610916238, 0.22639013749426998]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05439535448901023, 0.05499573764984974, 0.09502587337895911, 0.3801512008787486, 0.18904169610916238, 0.22639013749426998]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05439535448901023, 0.05499573764984974, 0.09502587337895911, 0.3801512008787486, 0.18904169610916238, 0.22639013749426998]
actor:  1 policy actor:  1  step number:  58 total reward:  0.08666666666666611  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.725]
 [0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.704]] [[43.4  ]
 [43.003]
 [43.003]
 [43.003]
 [43.003]
 [43.003]
 [43.003]] [[0.725]
 [0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.704]]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05392026152716227, 0.05451538802325194, 0.09419503923323469, 0.3768239471861146, 0.18683544275882544, 0.2337099212714111]
printing an ep nov before normalisation:  21.807312328775197
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05392026152716227, 0.05451538802325194, 0.09419503923323469, 0.3768239471861146, 0.18683544275882544, 0.2337099212714111]
printing an ep nov before normalisation:  35.470569133758545
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05392026152716227, 0.05451538802325194, 0.09419503923323469, 0.3768239471861146, 0.18683544275882544, 0.2337099212714111]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05392026152716227, 0.05451538802325194, 0.09419503923323469, 0.3768239471861146, 0.18683544275882544, 0.2337099212714111]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05392026152716227, 0.05451538802325194, 0.09419503923323469, 0.3768239471861146, 0.18683544275882544, 0.2337099212714111]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05392026152716227, 0.05451538802325194, 0.09419503923323469, 0.3768239471861146, 0.18683544275882544, 0.2337099212714111]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05392026152716227, 0.05451538802325194, 0.09419503923323469, 0.3768239471861146, 0.18683544275882544, 0.2337099212714111]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05392026152716227, 0.05451538802325194, 0.09419503923323469, 0.3768239471861146, 0.18683544275882544, 0.2337099212714111]
printing an ep nov before normalisation:  20.80172218504502
printing an ep nov before normalisation:  33.32657572500622
printing an ep nov before normalisation:  43.61870824067593
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.87862201022784
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.053907480570147846, 0.05458375216355665, 0.09424401788274676, 0.37673484750263286, 0.1868391619457463, 0.23369073993516945]
printing an ep nov before normalisation:  10.244864698411078
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
using another actor
printing an ep nov before normalisation:  20.475186304872075
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.12 ]
 [0.016]
 [0.016]
 [0.016]
 [0.026]
 [0.014]] [[ 9.859]
 [26.669]
 [ 9.859]
 [ 9.859]
 [ 9.859]
 [10.213]
 [10.408]] [[0.016]
 [0.12 ]
 [0.016]
 [0.016]
 [0.016]
 [0.026]
 [0.014]]
Printing some Q and Qe and total Qs values:  [[0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]] [[39.058]
 [39.058]
 [39.058]
 [39.058]
 [39.058]
 [39.058]
 [39.058]] [[0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]]
printing an ep nov before normalisation:  41.65062803205582
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.195364010293915
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05391788662919148, 0.054674708652815915, 0.09390454024649121, 0.3768081308694502, 0.1869228629387974, 0.23377187066325367]
using another actor
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05391788662919148, 0.054674708652815915, 0.09390454024649121, 0.3768081308694502, 0.1869228629387974, 0.23377187066325367]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05391788662919148, 0.054674708652815915, 0.09390454024649121, 0.3768081308694502, 0.1869228629387974, 0.23377187066325367]
printing an ep nov before normalisation:  63.61889573665372
printing an ep nov before normalisation:  25.829253318447726
actor:  1 policy actor:  1  step number:  52 total reward:  0.3266666666666661  reward:  1.0 rdn_beta:  1.667
from probs:  [0.05289205109668321, 0.0536344382437947, 0.09211603873821333, 0.3696238054181096, 0.20241798879976414, 0.2293156777034351]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05289205109668321, 0.0536344382437947, 0.09211603873821333, 0.3696238054181096, 0.20241798879976414, 0.2293156777034351]
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.351]
 [0.33 ]
 [0.358]
 [0.396]
 [0.34 ]
 [0.374]] [[41.615]
 [41.006]
 [42.949]
 [38.685]
 [38.04 ]
 [39.932]
 [36.827]] [[0.421]
 [0.351]
 [0.33 ]
 [0.358]
 [0.396]
 [0.34 ]
 [0.374]]
printing an ep nov before normalisation:  29.45895008379011
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05289205109668321, 0.0536344382437947, 0.09211603873821333, 0.3696238054181096, 0.20241798879976414, 0.2293156777034351]
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.419]
 [0.422]
 [0.422]
 [0.411]
 [0.422]
 [0.424]] [[18.186]
 [24.425]
 [26.525]
 [26.525]
 [16.987]
 [26.525]
 [17.382]] [[1.504]
 [2.247]
 [2.496]
 [2.496]
 [1.365]
 [2.496]
 [1.425]]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05292404298992629, 0.05366688030571857, 0.0921718152711026, 0.36984785709655055, 0.20193475499022717, 0.22945464934647472]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05292404298992629, 0.05366688030571857, 0.0921718152711026, 0.36984785709655055, 0.20193475499022717, 0.22945464934647472]
printing an ep nov before normalisation:  40.36429676351097
actor:  1 policy actor:  1  step number:  79 total reward:  0.2666666666666666  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05226025713607458, 0.05299375409805014, 0.09101453229233451, 0.3651991064411639, 0.19939732501137739, 0.23913502502099943]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05226025713607458, 0.05299375409805014, 0.09101453229233451, 0.3651991064411639, 0.19939732501137739, 0.23913502502099943]
printing an ep nov before normalisation:  50.2105145045158
printing an ep nov before normalisation:  51.06071164749573
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.398]
 [0.329]
 [0.341]
 [0.333]
 [0.334]
 [0.338]] [[43.16 ]
 [46.668]
 [30.34 ]
 [32.805]
 [31.416]
 [31.55 ]
 [30.595]] [[1.069]
 [1.195]
 [0.661]
 [0.743]
 [0.696]
 [0.701]
 [0.678]]
printing an ep nov before normalisation:  0.0021837730912466213
Printing some Q and Qe and total Qs values:  [[0.387]
 [0.391]
 [0.396]
 [0.396]
 [0.396]
 [0.396]
 [0.396]] [[42.579]
 [43.092]
 [36.243]
 [36.243]
 [36.243]
 [36.243]
 [36.243]] [[1.342]
 [1.366]
 [1.109]
 [1.109]
 [1.109]
 [1.109]
 [1.109]]
printing an ep nov before normalisation:  74.73907135064857
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05229569949677243, 0.0529446133982028, 0.09107632457146336, 0.36544732308598465, 0.19893851163523732, 0.23929752781233943]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05229569949677243, 0.0529446133982028, 0.09107632457146336, 0.36544732308598465, 0.19893851163523732, 0.23929752781233943]
Printing some Q and Qe and total Qs values:  [[ 0.084]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]] [[33.601]
 [26.887]
 [26.887]
 [26.887]
 [26.887]
 [26.887]
 [26.887]] [[1.029]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]]
printing an ep nov before normalisation:  21.92128783536094
printing an ep nov before normalisation:  28.65907907485962
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05229569949677243, 0.0529446133982028, 0.09107632457146336, 0.36544732308598465, 0.19893851163523732, 0.23929752781233943]
printing an ep nov before normalisation:  44.13475224944584
Printing some Q and Qe and total Qs values:  [[ 0.059]
 [-0.038]
 [-0.038]
 [-0.038]
 [-0.038]
 [-0.038]
 [-0.038]] [[34.044]
 [31.327]
 [31.327]
 [31.327]
 [31.327]
 [31.327]
 [31.327]] [[1.27 ]
 [1.076]
 [1.076]
 [1.076]
 [1.076]
 [1.076]
 [1.076]]
printing an ep nov before normalisation:  59.267371099991955
printing an ep nov before normalisation:  43.26833248138428
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.052227548655567564, 0.05287788588080308, 0.09109323491746053, 0.3649697786556247, 0.19919200624639735, 0.2396395456441468]
printing an ep nov before normalisation:  41.93492306330108
Printing some Q and Qe and total Qs values:  [[0.791]
 [0.64 ]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]] [[34.326]
 [33.864]
 [24.788]
 [24.788]
 [24.788]
 [24.788]
 [24.788]] [[0.791]
 [0.64 ]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05219042490222299, 0.05284256892282287, 0.09116408951603247, 0.36470952928113964, 0.198970289268172, 0.24012309810961002]
Printing some Q and Qe and total Qs values:  [[0.406]
 [0.405]
 [0.408]
 [0.404]
 [0.405]
 [0.404]
 [0.4  ]] [[46.452]
 [58.599]
 [40.428]
 [40.745]
 [41.164]
 [41.237]
 [40.135]] [[1.201]
 [1.572]
 [1.018]
 [1.024]
 [1.037]
 [1.039]
 [1.001]]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05219042490222299, 0.05284256892282287, 0.09116408951603247, 0.36470952928113964, 0.198970289268172, 0.24012309810961002]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05222110753990177, 0.05287363593575444, 0.09121774332528504, 0.36492441142989607, 0.19849855305269917, 0.2402645487164634]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05215332207752926, 0.05280727064239454, 0.0912348305122719, 0.36444942676296327, 0.1987491272152573, 0.2406060227895837]
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]]
actions average: 
K:  2  action  0 :  tensor([0.5552, 0.0213, 0.0748, 0.0964, 0.1212, 0.0522, 0.0789],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0112, 0.9152, 0.0076, 0.0284, 0.0072, 0.0066, 0.0240],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1608, 0.0100, 0.4203, 0.1059, 0.0877, 0.1332, 0.0821],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1555, 0.0097, 0.1541, 0.2181, 0.1516, 0.2099, 0.1011],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2193, 0.0295, 0.1189, 0.1261, 0.2085, 0.1487, 0.1490],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1518, 0.1106, 0.1858, 0.1304, 0.0735, 0.2309, 0.1170],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1610, 0.1158, 0.1203, 0.1683, 0.1000, 0.0862, 0.2484],
       grad_fn=<DivBackward0>)
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  28.895613925773535
printing an ep nov before normalisation:  8.729616761770794
Printing some Q and Qe and total Qs values:  [[0.248]
 [0.168]
 [0.353]
 [0.323]
 [0.334]
 [0.379]
 [0.283]] [[18.014]
 [27.829]
 [20.462]
 [17.983]
 [17.844]
 [18.035]
 [26.258]] [[0.762]
 [1.091]
 [0.969]
 [0.835]
 [0.841]
 [0.894]
 [1.141]]
printing an ep nov before normalisation:  36.53985522736408
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]] [[40.409]
 [30.173]
 [30.173]
 [30.173]
 [30.173]
 [30.173]
 [30.173]] [[1.426]
 [0.945]
 [0.945]
 [0.945]
 [0.945]
 [0.945]
 [0.945]]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05221418261922325, 0.05286889626657537, 0.09134141418766392, 0.3648756559180962, 0.1978124887761302, 0.24088736223231094]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05221418261922325, 0.05286889626657537, 0.09134141418766392, 0.3648756559180962, 0.1978124887761302, 0.24088736223231094]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05221418261922325, 0.05286889626657537, 0.09134141418766392, 0.3648756559180962, 0.1978124887761302, 0.24088736223231094]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05221418261922325, 0.05286889626657537, 0.09134141418766392, 0.3648756559180962, 0.1978124887761302, 0.24088736223231094]
printing an ep nov before normalisation:  25.38045883178711
printing an ep nov before normalisation:  36.61671893010554
actor:  1 policy actor:  1  step number:  69 total reward:  0.3866666666666667  reward:  1.0 rdn_beta:  1.0
from probs:  [0.05221418261922325, 0.05286889626657537, 0.09134141418766392, 0.3648756559180962, 0.1978124887761302, 0.24088736223231094]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05109382317801584, 0.05173445270183599, 0.11087404179139754, 0.3570293593722627, 0.19356003557415635, 0.23570828738233163]
line 256 mcts: sample exp_bonus 51.22051170336215
printing an ep nov before normalisation:  4.80671872082894e-06
actor:  1 policy actor:  1  step number:  65 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.052065375618265824, 0.052686719409348395, 0.11004594985638204, 0.36383707708354035, 0.1902427358865438, 0.23112214214591953]
siam score:  -0.72247046
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.052065375618265824, 0.052686719409348395, 0.11004594985638204, 0.36383707708354035, 0.1902427358865438, 0.23112214214591953]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.052065375618265824, 0.052686719409348395, 0.11004594985638204, 0.36383707708354035, 0.1902427358865438, 0.23112214214591953]
printing an ep nov before normalisation:  22.278495781515826
printing an ep nov before normalisation:  22.166614532470703
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.052065375618265824, 0.052686719409348395, 0.11004594985638204, 0.36383707708354035, 0.1902427358865438, 0.23112214214591953]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05205458048508414, 0.05274968024317762, 0.11008343560156202, 0.3637618459841233, 0.19024460364630835, 0.2311058540397446]
actor:  1 policy actor:  1  step number:  62 total reward:  0.1933333333333327  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.517]
 [0.466]
 [0.489]
 [0.469]
 [0.461]
 [0.452]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.46 ]
 [0.517]
 [0.466]
 [0.489]
 [0.469]
 [0.461]
 [0.452]]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05279876518230961, 0.05347734471175399, 0.10944846580533917, 0.3689763765073956, 0.1877044672758515, 0.22759458051735007]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05279876518230961, 0.05347734471175399, 0.10944846580533917, 0.3689763765073956, 0.1877044672758515, 0.22759458051735007]
printing an ep nov before normalisation:  37.954470654878406
printing an ep nov before normalisation:  51.62580595888868
printing an ep nov before normalisation:  45.2924108505249
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05279876518230961, 0.05347734471175399, 0.10944846580533917, 0.3689763765073956, 0.1877044672758515, 0.22759458051735007]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05279876518230961, 0.05347734471175399, 0.10944846580533917, 0.3689763765073956, 0.1877044672758515, 0.22759458051735007]
printing an ep nov before normalisation:  48.61253193806312
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05279876518230961, 0.05347734471175399, 0.10944846580533917, 0.3689763765073956, 0.1877044672758515, 0.22759458051735007]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05279876518230961, 0.05347734471175399, 0.10944846580533917, 0.3689763765073956, 0.1877044672758515, 0.22759458051735007]
printing an ep nov before normalisation:  23.259079456329346
printing an ep nov before normalisation:  27.28411383578475
line 256 mcts: sample exp_bonus 40.2656074946378
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
using another actor
printing an ep nov before normalisation:  38.17987488800085
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.052732422840806974, 0.053412475110358515, 0.10950507189146527, 0.36851151319316644, 0.18793091454785318, 0.22790760241634947]
printing an ep nov before normalisation:  21.847768007660196
actor:  1 policy actor:  1  step number:  67 total reward:  0.18666666666666598  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 21.092667917452935
printing an ep nov before normalisation:  71.15406055664783
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.0534296739960784, 0.05409424791925454, 0.10891014777433482, 0.37339717860002747, 0.18555097711370386, 0.22461777459660082]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.0534296739960784, 0.05409424791925454, 0.10891014777433482, 0.37339717860002747, 0.18555097711370386, 0.22461777459660082]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.053299345241227855, 0.05396681234513952, 0.10902134976924008, 0.3724839600498987, 0.18599583013412543, 0.22523270246036844]
printing an ep nov before normalisation:  41.02345199895607
from probs:  [0.053299345241227855, 0.05396681234513952, 0.10902134976924008, 0.3724839600498987, 0.18599583013412543, 0.22523270246036844]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05327653837871986, 0.053945978278577854, 0.10889841834028645, 0.3723240036437042, 0.18583699175770724, 0.22571806960100438]
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]] [[45.782]
 [45.782]
 [45.782]
 [45.782]
 [45.782]
 [45.782]
 [45.782]] [[2.502]
 [2.502]
 [2.502]
 [2.502]
 [2.502]
 [2.502]
 [2.502]]
printing an ep nov before normalisation:  28.776852378064117
siam score:  -0.7159705
printing an ep nov before normalisation:  36.735472962156635
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 11.395660384124309
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
printing an ep nov before normalisation:  70.18794286492451
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05323893697789121, 0.053910161367801517, 0.1090090852029767, 0.3720604334403298, 0.18615274965087159, 0.22562863336012914]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.0000],
        [-0.5806],
        [-0.0000],
        [-0.6224],
        [-0.4642],
        [-0.0000],
        [-0.0000],
        [-0.0000],
        [-0.0000]], dtype=torch.float64)
-0.7392000000000002 -0.7392000000000002
0.8987154164999999 0.8987154164999999
-0.032346567066 -0.6129510811702202
-0.6470626800000002 -0.6470626800000002
-0.032346567066 -0.6547453941974342
-0.084359833866 -0.5485491821977174
-0.5214 -0.5214
-0.8298840000000001 -0.8298840000000001
-0.8945775456419999 -0.8945775456419999
-0.0648199001999996 -0.0648199001999996
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.053266899491463536, 0.05393847731080795, 0.10906641317499106, 0.3722562696229304, 0.1857245736993093, 0.22574736670049783]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.053202317329369885, 0.053875336088989347, 0.10912155469089657, 0.3718037392194587, 0.18594419329612918, 0.22605285937515632]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.053202317329369885, 0.053875336088989347, 0.10912155469089657, 0.3718037392194587, 0.18594419329612918, 0.22605285937515632]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
siam score:  -0.7083038
Printing some Q and Qe and total Qs values:  [[0.295]
 [0.295]
 [0.295]
 [0.295]
 [0.295]
 [0.295]
 [0.295]] [[34.903]
 [34.903]
 [34.903]
 [34.903]
 [34.903]
 [34.903]
 [34.903]] [[1.628]
 [1.628]
 [1.628]
 [1.628]
 [1.628]
 [1.628]
 [1.628]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.464]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]] [[62.554]
 [49.555]
 [51.093]
 [51.093]
 [51.093]
 [51.093]
 [51.093]] [[1.376]
 [0.993]
 [1.036]
 [1.036]
 [1.036]
 [1.036]
 [1.036]]
line 256 mcts: sample exp_bonus 40.06566623011315
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05313787409263985, 0.053812330692868046, 0.10917657758960349, 0.37135218227287997, 0.1861633404601386, 0.22635769489187005]
printing an ep nov before normalisation:  31.749745364749238
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
printing an ep nov before normalisation:  11.13551767770006
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.053179781850627474, 0.053854771705303985, 0.10899780028849863, 0.3716456839737477, 0.18578540447815486, 0.22653655770366746]
printing an ep nov before normalisation:  36.73258721828461
siam score:  -0.706622
actor:  1 policy actor:  1  step number:  52 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05425444699688419, 0.05490632715497133, 0.10816141823696149, 0.3791758282912448, 0.18232003737987545, 0.22118194194006277]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05425444699688419, 0.05490632715497133, 0.10816141823696149, 0.3791758282912448, 0.18232003737987545, 0.22118194194006277]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05425444699688419, 0.05490632715497133, 0.10816141823696149, 0.3791758282912448, 0.18232003737987545, 0.22118194194006277]
printing an ep nov before normalisation:  69.53676853745185
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05425444699688419, 0.05490632715497133, 0.10816141823696149, 0.3791758282912448, 0.18232003737987545, 0.22118194194006277]
printing an ep nov before normalisation:  22.9360032081604
printing an ep nov before normalisation:  23.712220191955566
printing an ep nov before normalisation:  47.96122178812861
Printing some Q and Qe and total Qs values:  [[0.816]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]] [[55.042]
 [56.096]
 [56.096]
 [56.096]
 [56.096]
 [56.096]
 [56.096]] [[2.701]
 [2.726]
 [2.726]
 [2.726]
 [2.726]
 [2.726]
 [2.726]]
printing an ep nov before normalisation:  14.433471548649264
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05425444699688419, 0.05490632715497133, 0.10816141823696149, 0.3791758282912448, 0.18232003737987545, 0.22118194194006277]
printing an ep nov before normalisation:  45.94868444690285
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05425444699688419, 0.05490632715497133, 0.10816141823696149, 0.3791758282912448, 0.18232003737987545, 0.22118194194006277]
printing an ep nov before normalisation:  20.621120970372342
actions average: 
K:  0  action  0 :  tensor([0.4666, 0.0035, 0.1222, 0.1029, 0.0961, 0.0976, 0.1111],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0072, 0.9755, 0.0039, 0.0027, 0.0015, 0.0020, 0.0071],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1678, 0.0586, 0.3592, 0.0875, 0.0676, 0.0806, 0.1788],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2750, 0.0252, 0.1114, 0.2539, 0.0984, 0.0933, 0.1428],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2587, 0.0046, 0.1150, 0.1042, 0.2796, 0.1058, 0.1321],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1656, 0.0046, 0.1455, 0.1311, 0.1463, 0.2696, 0.1373],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2534, 0.0340, 0.1234, 0.1088, 0.1065, 0.1307, 0.2432],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05425444699688419, 0.05490632715497133, 0.10816141823696149, 0.3791758282912448, 0.18232003737987545, 0.22118194194006277]
actor:  1 policy actor:  1  step number:  64 total reward:  0.21999999999999986  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  36.337690310899305
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05363974482388044, 0.05428636479835325, 0.1071117272653296, 0.37487049283726376, 0.1806719420106892, 0.22941972826448376]
printing an ep nov before normalisation:  31.292519569396973
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05364375794207901, 0.05421548395124885, 0.10711975091147098, 0.37489859903808564, 0.18068548285075617, 0.22943692530635948]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05358046346305855, 0.05415340695455673, 0.10717033279173305, 0.374455095036231, 0.1808927219010122, 0.22974797985340847]
printing an ep nov before normalisation:  21.6184663772583
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.664]
 [0.528]
 [0.524]
 [0.5  ]
 [0.574]
 [0.5  ]] [[28.066]
 [45.46 ]
 [34.834]
 [39.846]
 [28.066]
 [44.803]
 [28.066]] [[0.943]
 [1.642]
 [1.179]
 [1.329]
 [0.943]
 [1.532]
 [0.943]]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05358046346305855, 0.05415340695455673, 0.10717033279173305, 0.374455095036231, 0.1808927219010122, 0.22974797985340847]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05358046346305855, 0.05415340695455673, 0.10717033279173305, 0.374455095036231, 0.1808927219010122, 0.22974797985340847]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05360802042552951, 0.054181259322052616, 0.10722552025689763, 0.37464809236052893, 0.1809859200330506, 0.2293511876019408]
printing an ep nov before normalisation:  19.56533363887242
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05360802042552951, 0.054181259322052616, 0.10722552025689763, 0.37464809236052893, 0.1809859200330506, 0.2293511876019408]
printing an ep nov before normalisation:  18.08863878250122
actor:  1 policy actor:  1  step number:  56 total reward:  0.12666666666666615  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  26.167268771221636
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.0541613834246125, 0.05472396292035275, 0.10678186364837214, 0.3785255050379016, 0.17917068576661202, 0.22663659920214882]
printing an ep nov before normalisation:  51.2886987423811
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.0541613834246125, 0.05472396292035275, 0.10678186364837214, 0.3785255050379016, 0.17917068576661202, 0.22663659920214882]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.0541613834246125, 0.05472396292035275, 0.10678186364837214, 0.3785255050379016, 0.17917068576661202, 0.22663659920214882]
Printing some Q and Qe and total Qs values:  [[0.698]
 [0.485]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]] [[52.31 ]
 [63.306]
 [60.745]
 [60.745]
 [60.745]
 [60.745]
 [60.745]] [[1.764]
 [1.818]
 [1.816]
 [1.816]
 [1.816]
 [1.816]
 [1.816]]
printing an ep nov before normalisation:  18.846883845345406
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.0541613834246125, 0.05472396292035275, 0.10678186364837214, 0.3785255050379016, 0.17917068576661202, 0.22663659920214882]
printing an ep nov before normalisation:  55.48979950838131
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.453]
 [0.433]
 [0.435]
 [0.488]
 [0.401]
 [0.425]] [[34.587]
 [44.533]
 [38.28 ]
 [36.882]
 [66.169]
 [40.566]
 [31.947]] [[0.663]
 [0.792]
 [0.691]
 [0.674]
 [1.109]
 [0.688]
 [0.6  ]]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
printing an ep nov before normalisation:  40.97629322551004
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05409904821329824, 0.05466282846875929, 0.10683184065892265, 0.37808872242143254, 0.17937516819816401, 0.22694239203942343]
siam score:  -0.71591586
from probs:  [0.05409904821329824, 0.05466282846875929, 0.10683184065892265, 0.37808872242143254, 0.17937516819816401, 0.22694239203942343]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.054050276092628136, 0.054615395462118584, 0.1066592760832457, 0.37774693076511795, 0.17962395706547513, 0.22730416453141447]
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.457]
 [0.484]
 [0.416]
 [0.408]
 [0.415]
 [0.468]] [[27.737]
 [39.865]
 [30.352]
 [33.823]
 [28.604]
 [28.027]
 [25.175]] [[0.747]
 [1.071]
 [0.882]
 [0.892]
 [0.766]
 [0.761]
 [0.748]]
printing an ep nov before normalisation:  63.98503629894234
printing an ep nov before normalisation:  31.49989604949951
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05406361689454749, 0.054628876092638744, 0.10643839984778122, 0.3778403644690924, 0.1796683688110872, 0.22736037388485292]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
printing an ep nov before normalisation:  38.933761309586586
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.054090901080175315, 0.05465644625129306, 0.10649218121424779, 0.37803145212173456, 0.17975919830684473, 0.22696982102570454]
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.597]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]] [[31.029]
 [42.759]
 [31.029]
 [31.029]
 [31.029]
 [31.029]
 [31.029]] [[0.875]
 [1.336]
 [0.875]
 [0.875]
 [0.875]
 [0.875]
 [0.875]]
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
maxi score, test score, baseline:  -0.9926866666666666 -0.943 -0.943
probs:  [0.05410416188963821, 0.054669846050933904, 0.1062727536052365, 0.37812432559015924, 0.17980334375627396, 0.22702556910775823]
printing an ep nov before normalisation:  7.3144607767602565
actor:  0 policy actor:  1  step number:  61 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.054131339347354944, 0.05469730836303265, 0.10632620093750474, 0.3783146657626941, 0.1798938179530873, 0.22663666763632628]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.054131339347354944, 0.05469730836303265, 0.10632620093750474, 0.3783146657626941, 0.1798938179530873, 0.22663666763632628]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.544]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]] [[37.564]
 [40.97 ]
 [37.564]
 [37.564]
 [37.564]
 [37.564]
 [37.564]] [[1.729]
 [1.951]
 [1.729]
 [1.729]
 [1.729]
 [1.729]
 [1.729]]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.054131339347354944, 0.05469730836303265, 0.10632620093750474, 0.3783146657626941, 0.1798938179530873, 0.22663666763632628]
printing an ep nov before normalisation:  36.7854421287836
Printing some Q and Qe and total Qs values:  [[0.715]
 [1.089]
 [0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.626]] [[32.302]
 [36.174]
 [32.237]
 [32.237]
 [32.237]
 [32.237]
 [32.237]] [[1.722]
 [2.269]
 [1.63 ]
 [1.63 ]
 [1.63 ]
 [1.63 ]
 [1.63 ]]
Printing some Q and Qe and total Qs values:  [[0.193]
 [0.274]
 [0.226]
 [0.232]
 [0.232]
 [0.235]
 [0.243]] [[28.796]
 [22.377]
 [19.738]
 [20.785]
 [20.036]
 [19.548]
 [18.145]] [[1.309]
 [0.978]
 [0.76 ]
 [0.834]
 [0.785]
 [0.757]
 [0.675]]
printing an ep nov before normalisation:  57.06380217200709
Printing some Q and Qe and total Qs values:  [[0.253]
 [0.271]
 [0.246]
 [0.224]
 [0.218]
 [0.216]
 [0.222]] [[46.183]
 [33.311]
 [33.133]
 [26.751]
 [27.363]
 [27.504]
 [27.021]] [[1.178]
 [0.932]
 [0.903]
 [0.75 ]
 [0.756]
 [0.758]
 [0.753]]
Printing some Q and Qe and total Qs values:  [[0.141]
 [0.175]
 [0.133]
 [0.135]
 [0.127]
 [0.144]
 [0.133]] [[23.109]
 [23.627]
 [22.283]
 [20.211]
 [23.   ]
 [21.037]
 [23.194]] [[1.401]
 [1.484]
 [1.313]
 [1.118]
 [1.376]
 [1.206]
 [1.401]]
Printing some Q and Qe and total Qs values:  [[0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.134]
 [0.176]] [[24.927]
 [24.927]
 [24.927]
 [24.927]
 [24.927]
 [27.388]
 [24.927]] [[1.609]
 [1.609]
 [1.609]
 [1.609]
 [1.609]
 [1.801]
 [1.609]]
line 256 mcts: sample exp_bonus 63.40499545075176
actor:  1 policy actor:  1  step number:  68 total reward:  0.13999999999999913  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.0546418643847198, 0.05519873453026698, 0.10599760889613631, 0.38189182913120434, 0.17789607340136285, 0.22437388965630975]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  49.4128936703426
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.385]
 [0.385]
 [0.2  ]
 [0.385]
 [0.385]
 [0.385]] [[37.578]
 [37.578]
 [37.578]
 [24.761]
 [37.578]
 [37.578]
 [37.578]] [[4.406]
 [4.406]
 [4.406]
 [2.2  ]
 [4.406]
 [4.406]
 [4.406]]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
printing an ep nov before normalisation:  32.847185134887695
siam score:  -0.69838685
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05466873898529742, 0.05522588368664687, 0.10604980361442462, 0.3820800490244464, 0.17798371649259973, 0.2239918081965849]
printing an ep nov before normalisation:  0.0
Printing some Q and Qe and total Qs values:  [[0.134]
 [0.215]
 [0.157]
 [0.153]
 [0.168]
 [0.189]
 [0.157]] [[34.956]
 [26.638]
 [30.037]
 [25.138]
 [27.503]
 [27.144]
 [30.037]] [[1.134]
 [0.815]
 [0.921]
 [0.682]
 [0.811]
 [0.814]
 [0.921]]
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.642]
 [0.439]
 [0.438]
 [0.443]
 [0.454]
 [0.435]] [[ 9.739]
 [27.407]
 [ 7.343]
 [ 7.345]
 [12.812]
 [ 7.21 ]
 [ 7.277]] [[0.551]
 [1.144]
 [0.518]
 [0.517]
 [0.638]
 [0.531]
 [0.513]]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.054607726865389214, 0.055166056063246184, 0.10609802827171085, 0.3816525362532289, 0.17818487353598023, 0.22429077901044464]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.054607726865389214, 0.055166056063246184, 0.10609802827171085, 0.3816525362532289, 0.17818487353598023, 0.22429077901044464]
printing an ep nov before normalisation:  40.987824537986434
actor:  1 policy actor:  1  step number:  75 total reward:  0.0933333333333326  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  32.86170830848607
printing an ep nov before normalisation:  43.169766209453066
printing an ep nov before normalisation:  24.76033852394988
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.054204866930882924, 0.054759067069021804, 0.10507559164324758, 0.3788310538400179, 0.1768681129486823, 0.23026130756814756]
printing an ep nov before normalisation:  45.63374872842557
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.054204866930882924, 0.054759067069021804, 0.10507559164324758, 0.3788310538400179, 0.1768681129486823, 0.23026130756814756]
printing an ep nov before normalisation:  30.39783000946045
printing an ep nov before normalisation:  30.897571897114865
printing an ep nov before normalisation:  26.851700199923556
Printing some Q and Qe and total Qs values:  [[0.822]
 [0.692]
 [0.822]
 [0.822]
 [0.822]
 [0.896]
 [0.924]] [[16.695]
 [26.458]
 [16.695]
 [16.695]
 [16.695]
 [24.199]
 [20.339]] [[0.822]
 [0.692]
 [0.822]
 [0.822]
 [0.822]
 [0.896]
 [0.924]]
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.527]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]] [[44.527]
 [36.947]
 [37.849]
 [37.849]
 [37.849]
 [37.849]
 [37.849]] [[1.535]
 [1.196]
 [1.222]
 [1.222]
 [1.222]
 [1.222]
 [1.222]]
line 256 mcts: sample exp_bonus 49.66681070904686
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05425959904012267, 0.05481436014779015, 0.10518181584802666, 0.3792143773488378, 0.17704700659913905, 0.22948284101608366]
printing an ep nov before normalisation:  73.5836715272631
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05419836590661786, 0.05475429407710869, 0.10522770887260635, 0.3787853184096235, 0.17724408396461686, 0.2297902287694268]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05419836590661786, 0.05475429407710869, 0.10522770887260635, 0.3787853184096235, 0.17724408396461686, 0.2297902287694268]
printing an ep nov before normalisation:  28.604727294490587
printing an ep nov before normalisation:  41.34689900126318
siam score:  -0.70014465
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05421123322278708, 0.05476729370226016, 0.10501491469884545, 0.3788754362981127, 0.1772862357362092, 0.22984488634178554]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05421123322278708, 0.05476729370226016, 0.10501491469884545, 0.3788754362981127, 0.1772862357362092, 0.22984488634178554]
line 256 mcts: sample exp_bonus 50.06664610325282
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.054224011283803074, 0.054780203154486715, 0.10480359658864984, 0.3789649290767624, 0.17732809511871414, 0.22989916477758382]
printing an ep nov before normalisation:  18.02208440574747
actor:  1 policy actor:  1  step number:  80 total reward:  0.13999999999999957  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05362171633927737, 0.054171715074572094, 0.1036381035173274, 0.3747466796815836, 0.18648103909248476, 0.22734074629475487]
printing an ep nov before normalisation:  27.84443622604524
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.323]
 [0.313]
 [0.312]
 [0.311]
 [0.313]
 [0.3  ]] [[24.842]
 [33.464]
 [24.338]
 [24.102]
 [22.758]
 [25.5  ]
 [20.814]] [[0.305]
 [0.323]
 [0.313]
 [0.312]
 [0.311]
 [0.313]
 [0.3  ]]
Printing some Q and Qe and total Qs values:  [[0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.249]] [[47.422]
 [47.422]
 [47.422]
 [47.422]
 [47.422]
 [47.422]
 [47.422]] [[1.582]
 [1.582]
 [1.582]
 [1.582]
 [1.582]
 [1.582]
 [1.582]]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.0536492805696114, 0.0541995627358263, 0.10369144269900682, 0.37493972928127206, 0.18606215153876932, 0.22745783317551407]
printing an ep nov before normalisation:  22.725582994741345
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05361507752747453, 0.05416678373747265, 0.10378674090959795, 0.37469998142132943, 0.1858579970462317, 0.22787341935789365]
printing an ep nov before normalisation:  68.70588300717203
UNIT TEST: sample policy line 217 mcts : [0.673 0.061 0.02  0.061 0.02  0.143 0.02 ]
printing an ep nov before normalisation:  40.94554283967727
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05361507752747453, 0.05416678373747265, 0.10378674090959795, 0.37469998142132943, 0.1858579970462317, 0.22787341935789365]
Printing some Q and Qe and total Qs values:  [[0.295]
 [0.199]
 [0.288]
 [0.293]
 [0.299]
 [0.309]
 [0.309]] [[13.302]
 [26.563]
 [13.205]
 [13.019]
 [11.128]
 [11.071]
 [10.852]] [[0.295]
 [0.199]
 [0.288]
 [0.293]
 [0.299]
 [0.309]
 [0.309]]
printing an ep nov before normalisation:  33.160974249223884
printing an ep nov before normalisation:  27.22956657409668
printing an ep nov before normalisation:  35.99110776443384
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05361507752747453, 0.05416678373747265, 0.10378674090959795, 0.37469998142132943, 0.1858579970462317, 0.22787341935789365]
printing an ep nov before normalisation:  62.72115406450553
printing an ep nov before normalisation:  17.648154084243394
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]] [[24.778]
 [24.778]
 [24.778]
 [24.778]
 [24.778]
 [24.778]
 [24.778]] [[0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05361507752747453, 0.05416678373747265, 0.10378674090959795, 0.37469998142132943, 0.1858579970462317, 0.22787341935789365]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05361507752747453, 0.05416678373747265, 0.10378674090959795, 0.37469998142132943, 0.1858579970462317, 0.22787341935789365]
actor:  1 policy actor:  1  step number:  59 total reward:  0.21333333333333304  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.054228279380916795, 0.05476861452026394, 0.10336586789011777, 0.37899663446076626, 0.18374557509177408, 0.22489502865616118]
printing an ep nov before normalisation:  61.47241745355359
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05424440370414006, 0.054714808818639715, 0.10316889430062373, 0.3791095637536771, 0.18380030514024498, 0.2249620242826745]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05424440370414006, 0.054714808818639715, 0.10316889430062373, 0.3791095637536771, 0.18380030514024498, 0.2249620242826745]
Printing some Q and Qe and total Qs values:  [[0.227]
 [0.311]
 [0.225]
 [0.225]
 [0.395]
 [0.281]
 [0.249]] [[22.414]
 [29.372]
 [23.8  ]
 [23.863]
 [25.316]
 [23.473]
 [24.149]] [[0.375]
 [0.542]
 [0.389]
 [0.39 ]
 [0.577]
 [0.441]
 [0.417]]
printing an ep nov before normalisation:  22.924013326244054
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05424440370414006, 0.054714808818639715, 0.10316889430062373, 0.3791095637536771, 0.18380030514024498, 0.2249620242826745]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05424440370414006, 0.054714808818639715, 0.10316889430062373, 0.3791095637536771, 0.18380030514024498, 0.2249620242826745]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05424440370414006, 0.054714808818639715, 0.10316889430062373, 0.3791095637536771, 0.18380030514024498, 0.2249620242826745]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05424440370414006, 0.054714808818639715, 0.10316889430062373, 0.3791095637536771, 0.18380030514024498, 0.2249620242826745]
printing an ep nov before normalisation:  56.71419057190222
siam score:  -0.7056858
Printing some Q and Qe and total Qs values:  [[0.768]
 [0.67 ]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]] [[18.666]
 [22.202]
 [10.854]
 [10.854]
 [10.854]
 [10.854]
 [10.854]] [[0.768]
 [0.67 ]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.054234168209235205, 0.054768810402713496, 0.10320417023644361, 0.37903819918924353, 0.1838044201239474, 0.22495023183841667]
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.826]
 [0.786]
 [0.786]
 [0.786]
 [0.786]
 [0.786]] [[71.872]
 [69.292]
 [68.273]
 [68.273]
 [68.273]
 [68.273]
 [68.273]] [[1.703]
 [1.919]
 [1.859]
 [1.859]
 [1.859]
 [1.859]
 [1.859]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  39.311590794476416
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.564]
 [0.533]
 [0.603]
 [0.488]
 [0.346]
 [0.518]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.427]
 [0.564]
 [0.533]
 [0.603]
 [0.488]
 [0.346]
 [0.518]]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05426063433137064, 0.054795538071232615, 0.10325459243068637, 0.37922355909256733, 0.18389427187339213, 0.22457140420075092]
printing an ep nov before normalisation:  54.583531715223955
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  65 total reward:  0.1599999999999998  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.299]] [[37.124]
 [37.124]
 [37.124]
 [37.124]
 [37.124]
 [37.124]
 [37.124]] [[0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.0547881413170886, 0.055313350654586806, 0.10289415059905199, 0.38291975159008745, 0.1820723455899877, 0.2220122602491976]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.0547881413170886, 0.055313350654586806, 0.10289415059905199, 0.38291975159008745, 0.1820723455899877, 0.2220122602491976]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05481799526314122, 0.055275358662583256, 0.1029502813075392, 0.3831288396172687, 0.18217172550269764, 0.22165579964677]
printing an ep nov before normalisation:  41.154173149010354
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05481799526314122, 0.055275358662583256, 0.1029502813075392, 0.3831288396172687, 0.18217172550269764, 0.22165579964677]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05481799526314122, 0.055275358662583256, 0.1029502813075392, 0.3831288396172687, 0.18217172550269764, 0.22165579964677]
printing an ep nov before normalisation:  48.315211181398986
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
actions average: 
K:  4  action  0 :  tensor([0.5286, 0.0855, 0.0625, 0.0800, 0.0624, 0.1259, 0.0552],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0285, 0.8964, 0.0170, 0.0184, 0.0072, 0.0079, 0.0246],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.3968, 0.0185, 0.1241, 0.1150, 0.1273, 0.1447, 0.0736],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2953, 0.1223, 0.1006, 0.1270, 0.1768, 0.0788, 0.0991],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.3354, 0.1635, 0.0734, 0.0869, 0.1403, 0.1016, 0.0989],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2060, 0.0800, 0.1730, 0.1253, 0.0651, 0.2540, 0.0967],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2187, 0.1873, 0.1254, 0.1059, 0.0890, 0.0889, 0.1848],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.054758192952586236, 0.05521651441415427, 0.10299130411843593, 0.38270981017604133, 0.18237869745893914, 0.22194548087984314]
printing an ep nov before normalisation:  45.508618511473905
printing an ep nov before normalisation:  28.192914986438495
Printing some Q and Qe and total Qs values:  [[0.241]
 [0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]] [[23.439]
 [12.017]
 [12.017]
 [12.017]
 [12.017]
 [12.017]
 [12.017]] [[0.882]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.054758192952586236, 0.05521651441415427, 0.10299130411843593, 0.38270981017604133, 0.18237869745893914, 0.22194548087984314]
printing an ep nov before normalisation:  55.11880356508802
Printing some Q and Qe and total Qs values:  [[0.824]
 [1.154]
 [0.828]
 [0.827]
 [0.827]
 [0.826]
 [0.825]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.824]
 [1.154]
 [0.828]
 [0.827]
 [0.827]
 [0.826]
 [0.825]]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05478504744845646, 0.05524359422809154, 0.10304187077331882, 0.38289789086504106, 0.1819770693988757, 0.22205452728621636]
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.8895892463083
Printing some Q and Qe and total Qs values:  [[0.656]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]] [[54.232]
 [44.516]
 [44.516]
 [44.516]
 [44.516]
 [44.516]
 [44.516]] [[2.285]
 [1.904]
 [1.904]
 [1.904]
 [1.904]
 [1.904]
 [1.904]]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05478504744845646, 0.05524359422809154, 0.10304187077331882, 0.38289789086504106, 0.1819770693988757, 0.22205452728621636]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05478504744845646, 0.05524359422809154, 0.10304187077331882, 0.38289789086504106, 0.1819770693988757, 0.22205452728621636]
printing an ep nov before normalisation:  39.82108032923961
printing an ep nov before normalisation:  68.57504780500302
line 256 mcts: sample exp_bonus 55.25446060473026
from probs:  [0.054725388785559034, 0.05518489269433004, 0.10308293904530032, 0.3824798677698385, 0.1821828998757394, 0.2223440118292326]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05466584660304535, 0.055126305772208256, 0.10312392713365305, 0.3820626608426186, 0.18238832847948416, 0.22263293116899063]
actor:  1 policy actor:  1  step number:  62 total reward:  0.0066666666666660435  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 38.90333642619339
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05433986109052408, 0.05479756772333967, 0.10250826853627551, 0.3797795601469445, 0.18129884253678635, 0.22727589996612982]
printing an ep nov before normalisation:  36.802171558254415
from probs:  [0.05433986109052408, 0.05479756772333967, 0.10250826853627551, 0.3797795601469445, 0.18129884253678635, 0.22727589996612982]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.054366457886924874, 0.054824389096128086, 0.10255849943837975, 0.37996583580041055, 0.18138773248645115, 0.22689708529170563]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.054366457886924874, 0.054824389096128086, 0.10255849943837975, 0.37996583580041055, 0.18138773248645115, 0.22689708529170563]
from probs:  [0.054366457886924874, 0.054824389096128086, 0.10255849943837975, 0.37996583580041055, 0.18138773248645115, 0.22689708529170563]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.054378531287901925, 0.054836564441743, 0.10235885932173633, 0.3800503941292659, 0.1814280833613821, 0.22694756745797073]
printing an ep nov before normalisation:  48.722554827223334
printing an ep nov before normalisation:  55.886033589248896
actions average: 
K:  0  action  0 :  tensor([0.4196, 0.0348, 0.1125, 0.1131, 0.1151, 0.0967, 0.1081],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0160, 0.9407, 0.0092, 0.0135, 0.0040, 0.0050, 0.0116],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2211, 0.0117, 0.2460, 0.1463, 0.1320, 0.1062, 0.1366],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2445, 0.0596, 0.1568, 0.1927, 0.1152, 0.1033, 0.1280],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2693, 0.0030, 0.1170, 0.1325, 0.2585, 0.0861, 0.1336],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.2698, 0.0025, 0.1999, 0.1706, 0.0944, 0.1685, 0.0942],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2036, 0.0096, 0.1326, 0.1501, 0.1206, 0.1331, 0.2504],
       grad_fn=<DivBackward0>)
siam score:  -0.7057594
siam score:  -0.7034497
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.054378531287901925, 0.054836564441743, 0.10235885932173633, 0.3800503941292659, 0.1814280833613821, 0.22694756745797073]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.054378531287901925, 0.054836564441743, 0.10235885932173633, 0.3800503941292659, 0.1814280833613821, 0.22694756745797073]
using explorer policy with actor:  1
siam score:  -0.70220405
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.475]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]] [[65.874]
 [62.068]
 [47.902]
 [47.902]
 [47.902]
 [47.902]
 [47.902]] [[1.808]
 [1.711]
 [1.207]
 [1.207]
 [1.207]
 [1.207]
 [1.207]]
printing an ep nov before normalisation:  71.70762767571358
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]] [[43.459]
 [42.153]
 [42.153]
 [42.153]
 [42.153]
 [42.153]
 [42.153]] [[1.83 ]
 [1.768]
 [1.768]
 [1.768]
 [1.768]
 [1.768]
 [1.768]]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
printing an ep nov before normalisation:  9.961555111997882
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05431882723124167, 0.05477780375207014, 0.10239797574811713, 0.37963205487287544, 0.18163005109023359, 0.22724328730546212]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05431882723124167, 0.05477780375207014, 0.10239797574811713, 0.37963205487287544, 0.18163005109023359, 0.22724328730546212]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05431882723124167, 0.05477780375207014, 0.10239797574811713, 0.37963205487287544, 0.18163005109023359, 0.22724328730546212]
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.482]
 [0.466]
 [0.467]
 [0.467]
 [0.498]
 [0.468]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.464]
 [0.482]
 [0.466]
 [0.467]
 [0.467]
 [0.498]
 [0.468]]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
printing an ep nov before normalisation:  17.05029825847099
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.463]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]] [[67.302]
 [68.679]
 [67.302]
 [67.302]
 [67.302]
 [67.302]
 [67.302]] [[1.22 ]
 [1.284]
 [1.22 ]
 [1.22 ]
 [1.22 ]
 [1.22 ]
 [1.22 ]]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05425924081523302, 0.054719158844242076, 0.10243701509963661, 0.3792145399106028, 0.18183162086262192, 0.22753842446766367]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05425924081523302, 0.054719158844242076, 0.10243701509963661, 0.3792145399106028, 0.18183162086262192, 0.22753842446766367]
printing an ep nov before normalisation:  27.53850424681275
printing an ep nov before normalisation:  23.485151261706942
printing an ep nov before normalisation:  63.19140354979062
printing an ep nov before normalisation:  20.378381769642324
actor:  1 policy actor:  1  step number:  98 total reward:  0.1266666666666667  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.212]
 [0.119]
 [0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.132]] [[39.459]
 [38.797]
 [36.002]
 [36.002]
 [36.002]
 [36.002]
 [36.002]] [[1.008]
 [0.9  ]
 [0.847]
 [0.847]
 [0.847]
 [0.847]
 [0.847]]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05476351345341568, 0.055216097289735075, 0.10217300935412202, 0.38274783510187305, 0.17982008558057314, 0.22527945922028117]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05476351345341568, 0.055216097289735075, 0.10217300935412202, 0.38274783510187305, 0.17982008558057314, 0.22527945922028117]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
actor:  1 policy actor:  1  step number:  71 total reward:  0.23999999999999932  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.596]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]] [[34.357]
 [55.996]
 [34.357]
 [34.357]
 [34.357]
 [34.357]
 [34.357]] [[1.1  ]
 [1.696]
 [1.1  ]
 [1.1  ]
 [1.1  ]
 [1.1  ]
 [1.1  ]]
printing an ep nov before normalisation:  26.868832111358643
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]] [[37.48 ]
 [29.595]
 [29.595]
 [29.595]
 [29.595]
 [29.595]
 [29.595]] [[1.488]
 [1.178]
 [1.178]
 [1.178]
 [1.178]
 [1.178]
 [1.178]]
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05406360128601151, 0.054510386709894575, 0.1141392168200309, 0.37784585424143885, 0.17704600964750072, 0.2223949312951234]
Starting evaluation
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.219]
 [0.22 ]
 [0.219]
 [0.219]
 [0.219]
 [0.219]
 [0.219]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.219]
 [0.22 ]
 [0.219]
 [0.219]
 [0.219]
 [0.219]
 [0.219]]
printing an ep nov before normalisation:  27.79705197122052
using explorer policy with actor:  0
printing an ep nov before normalisation:  45.55936665327663
printing an ep nov before normalisation:  45.15337785302655
Printing some Q and Qe and total Qs values:  [[0.236]
 [0.223]
 [0.236]
 [0.236]
 [0.186]
 [0.236]
 [0.236]] [[40.378]
 [40.404]
 [40.378]
 [40.378]
 [31.152]
 [40.378]
 [40.378]] [[0.236]
 [0.223]
 [0.236]
 [0.236]
 [0.186]
 [0.236]
 [0.236]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
printing an ep nov before normalisation:  54.88544627794134
printing an ep nov before normalisation:  13.071235764497025
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]] [[40.396]
 [40.396]
 [40.396]
 [40.396]
 [40.396]
 [40.396]
 [40.396]] [[0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]]
printing an ep nov before normalisation:  19.82228836252915
printing an ep nov before normalisation:  19.090652895050013
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.397]
 [0.371]
 [0.395]
 [0.341]
 [0.383]
 [0.377]] [[58.638]
 [46.335]
 [45.003]
 [56.631]
 [47.215]
 [45.02 ]
 [41.692]] [[0.377]
 [0.397]
 [0.371]
 [0.395]
 [0.341]
 [0.383]
 [0.377]]
printing an ep nov before normalisation:  0.0
Printing some Q and Qe and total Qs values:  [[0.888]
 [0.865]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]] [[16.186]
 [15.293]
 [13.789]
 [13.789]
 [13.789]
 [13.789]
 [13.789]] [[0.888]
 [0.865]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]]
printing an ep nov before normalisation:  14.89779856367079
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
probs:  [0.05400418368506095, 0.05445187505273373, 0.11420161413939128, 0.3774295264801806, 0.17723596264626898, 0.22267683799636454]
Printing some Q and Qe and total Qs values:  [[0.863]
 [0.853]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]] [[13.785]
 [14.27 ]
 [12.901]
 [12.901]
 [12.901]
 [12.901]
 [12.901]] [[0.863]
 [0.853]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]]
actions average: 
K:  0  action  0 :  tensor([0.5675, 0.0029, 0.0811, 0.0748, 0.1179, 0.0813, 0.0746],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0095, 0.9607, 0.0035, 0.0076, 0.0025, 0.0026, 0.0136],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1478, 0.0122, 0.2400, 0.1149, 0.1260, 0.2300, 0.1292],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1645, 0.0445, 0.1113, 0.2680, 0.1448, 0.1279, 0.1390],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2193, 0.0094, 0.1128, 0.0959, 0.3733, 0.0959, 0.0934],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1894, 0.0145, 0.1378, 0.1230, 0.1360, 0.2894, 0.1099],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1960, 0.0697, 0.1125, 0.0942, 0.1231, 0.1019, 0.3027],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.99026 -0.943 -0.943
printing an ep nov before normalisation:  30.869583832414932
printing an ep nov before normalisation:  30.736689501099526
printing an ep nov before normalisation:  29.78928890260621
Printing some Q and Qe and total Qs values:  [[0.816]
 [0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.677]] [[31.682]
 [21.205]
 [21.205]
 [21.205]
 [21.205]
 [21.205]
 [21.205]] [[0.816]
 [0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.677]]
printing an ep nov before normalisation:  2.077918264338905
printing an ep nov before normalisation:  32.573567042299494
actor:  1 policy actor:  1  step number:  75 total reward:  0.03999999999999948  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  35 total reward:  0.5466666666666667  reward:  1.0 rdn_beta:  2
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9871666666666666 -0.943 -0.943
printing an ep nov before normalisation:  56.006160642315876
actor:  0 policy actor:  1  step number:  38 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  23.17700960699778
printing an ep nov before normalisation:  30.6348786191509
line 256 mcts: sample exp_bonus 37.02746583524938
actor:  0 policy actor:  1  step number:  43 total reward:  0.5466666666666669  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  30.88094942884254
printing an ep nov before normalisation:  22.77860313526867
maxi score, test score, baseline:  -0.9809666666666667 -0.943 -0.943
probs:  [0.05438936163827686, 0.05483118017417809, 0.11379711999333231, 0.38012839470075827, 0.1760045816059328, 0.22084936188752172]
printing an ep nov before normalisation:  42.8025493246851
Printing some Q and Qe and total Qs values:  [[0.614]
 [0.693]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]] [[26.737]
 [27.535]
 [26.737]
 [26.737]
 [26.737]
 [26.737]
 [26.737]] [[0.614]
 [0.693]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9809666666666667 -0.943 -0.943
probs:  [0.05438936163827686, 0.05483118017417809, 0.11379711999333231, 0.38012839470075827, 0.1760045816059328, 0.22084936188752172]
printing an ep nov before normalisation:  28.216635093190234
Printing some Q and Qe and total Qs values:  [[0.702]
 [0.673]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]] [[28.485]
 [27.792]
 [13.45 ]
 [13.45 ]
 [13.45 ]
 [13.45 ]
 [13.45 ]] [[0.702]
 [0.673]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]]
printing an ep nov before normalisation:  11.366159915924072
maxi score, test score, baseline:  -0.9809666666666667 -0.943 -0.943
maxi score, test score, baseline:  -0.9809666666666667 -0.943 -0.943
probs:  [0.05433063933245587, 0.05477335321093953, 0.1138587871493815, 0.37971693873943935, 0.17619231180551737, 0.22112796976226645]
maxi score, test score, baseline:  -0.9809666666666667 -0.943 -0.943
probs:  [0.05433063933245587, 0.05477335321093953, 0.1138587871493815, 0.37971693873943935, 0.17619231180551737, 0.22112796976226645]
maxi score, test score, baseline:  -0.9809666666666667 -0.943 -0.943
probs:  [0.05433063933245587, 0.05477335321093953, 0.1138587871493815, 0.37971693873943935, 0.17619231180551737, 0.22112796976226645]
actor:  1 policy actor:  1  step number:  85 total reward:  0.13333333333333264  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]] [[20.698]
 [20.698]
 [20.698]
 [20.698]
 [20.698]
 [20.698]
 [20.698]] [[0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]]
printing an ep nov before normalisation:  48.413456532751766
Printing some Q and Qe and total Qs values:  [[0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]] [[18.709]
 [13.319]
 [13.319]
 [13.319]
 [13.319]
 [13.319]
 [13.319]] [[0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]]
printing an ep nov before normalisation:  60.67431838946252
printing an ep nov before normalisation:  34.96429095317623
printing an ep nov before normalisation:  32.0381056777253
printing an ep nov before normalisation:  20.55504876007533
printing an ep nov before normalisation:  30.585287259433347
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.417]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]] [[25.049]
 [25.   ]
 [16.709]
 [16.709]
 [16.709]
 [16.709]
 [16.709]] [[0.434]
 [0.417]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]]
maxi score, test score, baseline:  -0.9809666666666667 -0.943 -0.943
Printing some Q and Qe and total Qs values:  [[0.392]
 [0.454]
 [0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.392]] [[22.686]
 [19.365]
 [22.686]
 [22.686]
 [22.686]
 [22.686]
 [22.686]] [[0.392]
 [0.454]
 [0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.392]]
printing an ep nov before normalisation:  38.33280366213427
printing an ep nov before normalisation:  13.332632343362551
printing an ep nov before normalisation:  19.287293291389496
using explorer policy with actor:  0
printing an ep nov before normalisation:  36.42798011207664
maxi score, test score, baseline:  -0.9809666666666667 -0.943 -0.943
probs:  [0.0534122661268826, 0.0696010435154786, 0.11216219728200079, 0.37328502626367543, 0.17361820229914288, 0.21792126451281982]
maxi score, test score, baseline:  -0.9809666666666667 -0.943 -0.943
probs:  [0.0534122661268826, 0.0696010435154786, 0.11216219728200079, 0.37328502626367543, 0.17361820229914288, 0.21792126451281982]
maxi score, test score, baseline:  -0.9809666666666667 -0.943 -0.943
maxi score, test score, baseline:  -0.9809666666666667 -0.943 -0.943
probs:  [0.05343706334883581, 0.06963337534599018, 0.11221433799774967, 0.3734586992146761, 0.17369894597422897, 0.21755757811851922]
printing an ep nov before normalisation:  53.685175231701066
maxi score, test score, baseline:  -0.9809666666666667 -0.943 -0.943
probs:  [0.05343706334883581, 0.06963337534599018, 0.11221433799774967, 0.3734586992146761, 0.17369894597422897, 0.21755757811851922]
printing an ep nov before normalisation:  35.550194496178946
maxi score, test score, baseline:  -0.9809666666666667 -0.943 -0.943
probs:  [0.05343706334883581, 0.06963337534599018, 0.11221433799774967, 0.3734586992146761, 0.17369894597422897, 0.21755757811851922]
line 256 mcts: sample exp_bonus 16.145403472394754
maxi score, test score, baseline:  -0.9809666666666667 -0.943 -0.943
probs:  [0.05343706334883581, 0.06963337534599018, 0.11221433799774967, 0.3734586992146761, 0.17369894597422897, 0.21755757811851922]
maxi score, test score, baseline:  -0.9809666666666667 -0.943 -0.943
probs:  [0.05343706334883581, 0.06963337534599018, 0.11221433799774967, 0.3734586992146761, 0.17369894597422897, 0.21755757811851922]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9809666666666667 -0.943 -0.943
probs:  [0.05343706334883581, 0.06963337534599018, 0.11221433799774967, 0.3734586992146761, 0.17369894597422897, 0.21755757811851922]
printing an ep nov before normalisation:  42.43250846862793
maxi score, test score, baseline:  -0.9809666666666667 -0.7676666666666666 -0.7676666666666666
printing an ep nov before normalisation:  50.038518190697445
printing an ep nov before normalisation:  21.62558710231309
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]] [[46.564]
 [46.564]
 [46.564]
 [46.564]
 [46.564]
 [46.564]
 [46.564]] [[93.552]
 [93.552]
 [93.552]
 [93.552]
 [93.552]
 [93.552]
 [93.552]]
maxi score, test score, baseline:  -0.9809666666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05349182549180355, 0.06938010968379775, 0.11307499338703389, 0.37384577256378093, 0.17393575422058363, 0.21627154465300025]
Printing some Q and Qe and total Qs values:  [[0.51 ]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]] [[56.864]
 [34.92 ]
 [34.92 ]
 [34.92 ]
 [34.92 ]
 [34.92 ]
 [34.92 ]] [[2.092]
 [1.214]
 [1.214]
 [1.214]
 [1.214]
 [1.214]
 [1.214]]
printing an ep nov before normalisation:  49.164429750581526
printing an ep nov before normalisation:  30.17387866973877
maxi score, test score, baseline:  -0.9809666666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.053516235496690635, 0.06941178811596616, 0.11312666095590988, 0.37401673516934153, 0.1740152638150087, 0.21591331644708298]
Printing some Q and Qe and total Qs values:  [[0.219]
 [0.295]
 [0.267]
 [0.273]
 [0.287]
 [0.279]
 [0.286]] [[29.854]
 [28.493]
 [26.622]
 [26.96 ]
 [37.064]
 [29.71 ]
 [26.47 ]] [[0.957]
 [0.98 ]
 [0.876]
 [0.896]
 [1.313]
 [1.012]
 [0.89 ]]
Printing some Q and Qe and total Qs values:  [[0.119]
 [0.3  ]
 [0.139]
 [0.166]
 [0.183]
 [0.146]
 [0.162]] [[22.287]
 [39.308]
 [24.731]
 [20.461]
 [20.907]
 [21.09 ]
 [25.171]] [[0.348]
 [0.968]
 [0.431]
 [0.348]
 [0.376]
 [0.344]
 [0.465]]
maxi score, test score, baseline:  -0.9809666666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.053516235496690635, 0.06941178811596616, 0.11312666095590988, 0.37401673516934153, 0.1740152638150087, 0.21591331644708298]
printing an ep nov before normalisation:  47.70866521632218
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.219]
 [0.131]
 [0.237]
 [0.204]
 [0.209]
 [0.207]] [[54.327]
 [30.207]
 [29.928]
 [31.104]
 [33.362]
 [33.506]
 [28.692]] [[1.251]
 [0.618]
 [0.523]
 [0.655]
 [0.673]
 [0.681]
 [0.571]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.342]
 [0.332]
 [0.317]
 [0.319]
 [0.321]
 [0.318]] [[28.754]
 [28.385]
 [23.576]
 [22.055]
 [22.484]
 [22.29 ]
 [21.154]] [[1.325]
 [1.22 ]
 [0.949]
 [0.851]
 [0.877]
 [0.868]
 [0.803]]
maxi score, test score, baseline:  -0.9809666666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.053516235496690635, 0.06941178811596616, 0.11312666095590988, 0.37401673516934153, 0.1740152638150087, 0.21591331644708298]
Printing some Q and Qe and total Qs values:  [[0.368]
 [0.534]
 [0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]] [[29.954]
 [32.47 ]
 [25.939]
 [25.939]
 [25.939]
 [25.939]
 [25.939]] [[1.16 ]
 [1.44 ]
 [1.011]
 [1.011]
 [1.011]
 [1.011]
 [1.011]]
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.67 ]
 [0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.469]] [[31.074]
 [30.097]
 [31.074]
 [31.074]
 [31.074]
 [31.074]
 [31.074]] [[1.319]
 [1.459]
 [1.319]
 [1.319]
 [1.319]
 [1.319]
 [1.319]]
actor:  1 policy actor:  1  step number:  67 total reward:  0.02666666666666595  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  68 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  55 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 58.81243439090383
Printing some Q and Qe and total Qs values:  [[0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.314]
 [0.267]
 [0.267]] [[50.763]
 [50.763]
 [50.763]
 [50.763]
 [54.206]
 [50.763]
 [50.763]] [[1.074]
 [1.074]
 [1.074]
 [1.074]
 [1.176]
 [1.074]
 [1.074]]
Printing some Q and Qe and total Qs values:  [[0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]] [[45.078]
 [45.078]
 [45.078]
 [45.078]
 [45.078]
 [45.078]
 [45.078]] [[90.363]
 [90.363]
 [90.363]
 [90.363]
 [90.363]
 [90.363]
 [90.363]]
line 256 mcts: sample exp_bonus 53.33153865034096
Printing some Q and Qe and total Qs values:  [[0.168]
 [0.198]
 [0.172]
 [0.15 ]
 [0.153]
 [0.264]
 [0.156]] [[37.074]
 [41.145]
 [41.341]
 [42.381]
 [42.115]
 [58.57 ]
 [38.054]] [[0.524]
 [0.65 ]
 [0.629]
 [0.631]
 [0.628]
 [1.126]
 [0.535]]
printing an ep nov before normalisation:  44.894179007908164
using explorer policy with actor:  1
actions average: 
K:  4  action  0 :  tensor([0.5477, 0.0368, 0.0682, 0.1115, 0.1053, 0.0542, 0.0764],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0255, 0.8949, 0.0270, 0.0142, 0.0074, 0.0081, 0.0230],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2636, 0.2184, 0.1038, 0.0952, 0.1074, 0.0802, 0.1314],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0805, 0.1927, 0.1881, 0.1203, 0.1157, 0.1644, 0.1383],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2205, 0.1025, 0.0764, 0.1081, 0.3101, 0.0625, 0.1200],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1265, 0.0804, 0.1114, 0.0936, 0.0586, 0.4493, 0.0803],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2474, 0.1165, 0.1337, 0.1072, 0.1205, 0.0966, 0.1781],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  49.62608835311073
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
Printing some Q and Qe and total Qs values:  [[0.325]
 [0.327]
 [0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]] [[51.916]
 [34.947]
 [41.918]
 [41.918]
 [41.918]
 [41.918]
 [41.918]] [[1.13 ]
 [0.763]
 [0.859]
 [0.859]
 [0.859]
 [0.859]
 [0.859]]
maxi score, test score, baseline:  -0.9809666666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05325535150001527, 0.0685349983201694, 0.11055604872217374, 0.3721913792013049, 0.1816942432238251, 0.2137679790325115]
maxi score, test score, baseline:  -0.9809666666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05325535150001527, 0.0685349983201694, 0.11055604872217374, 0.3721913792013049, 0.1816942432238251, 0.2137679790325115]
maxi score, test score, baseline:  -0.9809666666666667 -0.7676666666666666 -0.7676666666666666
printing an ep nov before normalisation:  11.875308884514702
printing an ep nov before normalisation:  37.162703427439254
maxi score, test score, baseline:  -0.9809666666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05313817406596795, 0.06847728127800846, 0.11066185562489123, 0.37137037349475116, 0.18207688276364112, 0.21427543277274014]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.61707688538496
maxi score, test score, baseline:  -0.9809666666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.053079762146978106, 0.0684485098407565, 0.11071459943414376, 0.3709611093799384, 0.18226762520465592, 0.21452839399352727]
Printing some Q and Qe and total Qs values:  [[0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.353]] [[32.932]
 [32.932]
 [32.932]
 [32.932]
 [32.932]
 [32.932]
 [32.932]] [[0.907]
 [0.907]
 [0.907]
 [0.907]
 [0.907]
 [0.907]
 [0.907]]
printing an ep nov before normalisation:  61.49553874654016
siam score:  -0.70870775
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.700875934880813
printing an ep nov before normalisation:  27.85405158996582
printing an ep nov before normalisation:  39.63289499282837
actor:  1 policy actor:  1  step number:  41 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  0.667
using another actor
actions average: 
K:  3  action  0 :  tensor([0.6458, 0.0518, 0.0671, 0.0478, 0.0558, 0.0540, 0.0776],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0356, 0.8434, 0.0357, 0.0180, 0.0105, 0.0118, 0.0450],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.2526, 0.2216, 0.2551, 0.0618, 0.0458, 0.0518, 0.1114],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1708, 0.2987, 0.0941, 0.1558, 0.0829, 0.0805, 0.1173],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2992, 0.0401, 0.1240, 0.1098, 0.2177, 0.0989, 0.1102],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.2363, 0.0337, 0.2115, 0.0591, 0.0524, 0.3050, 0.1020],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2069, 0.2043, 0.1215, 0.1330, 0.0877, 0.0959, 0.1506],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  33.62566090968837
siam score:  -0.7063237
printing an ep nov before normalisation:  30.406630685770608
actor:  1 policy actor:  1  step number:  63 total reward:  0.10666666666666602  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9809666666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05112055934003894, 0.0934344673121927, 0.10679083146570514, 0.35723905834161307, 0.18434909792234752, 0.2070659856181027]
Printing some Q and Qe and total Qs values:  [[0.165]
 [0.199]
 [0.17 ]
 [0.184]
 [0.248]
 [0.171]
 [0.17 ]] [[15.521]
 [26.622]
 [18.212]
 [18.453]
 [25.736]
 [18.125]
 [18.395]] [[0.25 ]
 [0.661]
 [0.346]
 [0.368]
 [0.68 ]
 [0.345]
 [0.352]]
maxi score, test score, baseline:  -0.9809666666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05112055934003894, 0.0934344673121927, 0.10679083146570514, 0.35723905834161307, 0.18434909792234752, 0.2070659856181027]
printing an ep nov before normalisation:  14.15427783222218
maxi score, test score, baseline:  -0.9809666666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05112055934003894, 0.0934344673121927, 0.10679083146570514, 0.35723905834161307, 0.18434909792234752, 0.2070659856181027]
actions average: 
K:  0  action  0 :  tensor([0.5800, 0.0050, 0.0707, 0.0893, 0.0990, 0.0853, 0.0707],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0058, 0.9512, 0.0076, 0.0070, 0.0020, 0.0036, 0.0228],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2168, 0.0087, 0.4064, 0.0921, 0.0856, 0.1222, 0.0682],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.3242, 0.0471, 0.1125, 0.1486, 0.1431, 0.1361, 0.0884],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.3022, 0.0164, 0.0705, 0.0987, 0.3420, 0.0894, 0.0807],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0832, 0.0133, 0.0965, 0.0872, 0.0706, 0.5931, 0.0560],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2496, 0.0708, 0.1513, 0.0977, 0.0799, 0.0963, 0.2545],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  26.908406446304937
actor:  0 policy actor:  1  step number:  53 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9784866666666666 -0.7676666666666666 -0.7676666666666666
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  -0.9784866666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.05113457396546355, 0.09318546741155216, 0.10682014789201882, 0.3573372141817529, 0.18439973238978627, 0.20712286415942627]
printing an ep nov before normalisation:  14.029595851898193
using explorer policy with actor:  1
siam score:  -0.7095828
printing an ep nov before normalisation:  34.85982037819904
maxi score, test score, baseline:  -0.9784866666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.05117769536850969, 0.09330857480102643, 0.10695250307791206, 0.35763948393453665, 0.1835984926081924, 0.20732325020982278]
Printing some Q and Qe and total Qs values:  [[0.235]
 [0.259]
 [0.259]
 [0.259]
 [0.259]
 [0.259]
 [0.259]] [[60.217]
 [48.457]
 [48.457]
 [48.457]
 [48.457]
 [48.457]
 [48.457]] [[2.075]
 [1.663]
 [1.663]
 [1.663]
 [1.663]
 [1.663]
 [1.663]]
printing an ep nov before normalisation:  28.034472465515137
maxi score, test score, baseline:  -0.9784866666666666 -0.7676666666666666 -0.7676666666666666
using explorer policy with actor:  1
printing an ep nov before normalisation:  16.652336011392073
actions average: 
K:  1  action  0 :  tensor([0.5658, 0.0043, 0.0768, 0.0793, 0.1209, 0.0741, 0.0788],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0360, 0.8797, 0.0335, 0.0111, 0.0058, 0.0087, 0.0252],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0950, 0.0671, 0.4492, 0.0736, 0.0683, 0.1662, 0.0805],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1944, 0.0279, 0.1067, 0.2515, 0.1427, 0.1764, 0.1004],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.3149, 0.0072, 0.1097, 0.0816, 0.3008, 0.0797, 0.1062],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1049, 0.0179, 0.0967, 0.0838, 0.0737, 0.5482, 0.0749],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2707, 0.0268, 0.2003, 0.0873, 0.1373, 0.0963, 0.1813],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  28.663510937421847
maxi score, test score, baseline:  -0.9784866666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.05117769536850969, 0.09330857480102643, 0.10695250307791206, 0.35763948393453665, 0.1835984926081924, 0.20732325020982278]
Printing some Q and Qe and total Qs values:  [[0.841]
 [0.828]
 [0.778]
 [0.874]
 [0.886]
 [0.824]
 [1.024]] [[47.345]
 [43.944]
 [45.437]
 [46.367]
 [46.61 ]
 [46.921]
 [44.067]] [[2.653]
 [2.437]
 [2.476]
 [2.628]
 [2.655]
 [2.611]
 [2.64 ]]
printing an ep nov before normalisation:  49.1264121365904
maxi score, test score, baseline:  -0.9784866666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.05117769536850969, 0.09330857480102643, 0.10695250307791206, 0.35763948393453665, 0.1835984926081924, 0.20732325020982278]
maxi score, test score, baseline:  -0.9784866666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.05117769536850969, 0.09330857480102643, 0.10695250307791206, 0.35763948393453665, 0.1835984926081924, 0.20732325020982278]
printing an ep nov before normalisation:  46.89805377595821
printing an ep nov before normalisation:  12.560839653015137
Printing some Q and Qe and total Qs values:  [[0.188]
 [0.167]
 [0.018]
 [0.203]
 [0.2  ]
 [0.206]
 [0.19 ]] [[13.088]
 [15.829]
 [31.118]
 [12.153]
 [12.138]
 [11.777]
 [11.75 ]] [[0.533]
 [0.649]
 [1.257]
 [0.502]
 [0.498]
 [0.487]
 [0.469]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9784866666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.05119981965466967, 0.09334895994204492, 0.10699880192871587, 0.35779443844679354, 0.18367801225355712, 0.20697996777421884]
printing an ep nov before normalisation:  29.00663771037456
maxi score, test score, baseline:  -0.9784866666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.05119981965466967, 0.09334895994204492, 0.10699880192871587, 0.35779443844679354, 0.18367801225355712, 0.20697996777421884]
maxi score, test score, baseline:  -0.9784866666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.05119981965466967, 0.09334895994204492, 0.10699880192871587, 0.35779443844679354, 0.18367801225355712, 0.20697996777421884]
Printing some Q and Qe and total Qs values:  [[-0.17 ]
 [-0.111]
 [-0.12 ]
 [-0.11 ]
 [-0.126]
 [-0.115]
 [-0.131]] [[36.363]
 [40.48 ]
 [30.44 ]
 [38.757]
 [35.864]
 [31.881]
 [25.453]] [[0.321]
 [0.446]
 [0.278]
 [0.42 ]
 [0.358]
 [0.306]
 [0.188]]
Printing some Q and Qe and total Qs values:  [[0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]] [[7.911]
 [7.911]
 [7.911]
 [7.911]
 [7.911]
 [7.911]
 [7.911]] [[0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]]
maxi score, test score, baseline:  -0.9784866666666666 -0.7676666666666666 -0.7676666666666666
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05122750184120017, 0.09285784350477401, 0.10705673163613469, 0.3579883194914075, 0.1837775082390957, 0.20709209528738798]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05122750184120017, 0.09285784350477401, 0.10705673163613469, 0.3579883194914075, 0.1837775082390957, 0.20709209528738798]
printing an ep nov before normalisation:  24.0963908856574
Printing some Q and Qe and total Qs values:  [[0.247]
 [0.257]
 [0.262]
 [0.257]
 [0.255]
 [0.265]
 [0.262]] [[38.009]
 [39.917]
 [30.714]
 [30.592]
 [30.691]
 [30.488]
 [36.053]] [[0.868]
 [0.93 ]
 [0.685]
 [0.676]
 [0.677]
 [0.681]
 [0.83 ]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05122750184120017, 0.09285784350477401, 0.10705673163613469, 0.3579883194914075, 0.1837775082390957, 0.20709209528738798]
using another actor
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  63 total reward:  0.13333333333333264  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  16.624947448870554
printing an ep nov before normalisation:  48.54934186489278
printing an ep nov before normalisation:  66.4892638041831
printing an ep nov before normalisation:  66.36826723736763
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05167546010128585, 0.09272555447924426, 0.10672653728560043, 0.3611268756063534, 0.1823779731002025, 0.20536759942731356]
printing an ep nov before normalisation:  32.401814141728735
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05167546010128585, 0.09272555447924426, 0.10672653728560043, 0.3611268756063534, 0.1823779731002025, 0.20536759942731356]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05167546010128585, 0.09272555447924426, 0.10672653728560043, 0.3611268756063534, 0.1823779731002025, 0.20536759942731356]
printing an ep nov before normalisation:  0.002591025830156468
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.051697395533146145, 0.09276496008050883, 0.1067719014490841, 0.36128050790111216, 0.18245553313118384, 0.205029701904965]
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.051697395533146145, 0.09276496008050883, 0.1067719014490841, 0.36128050790111216, 0.18245553313118384, 0.205029701904965]
printing an ep nov before normalisation:  25.111119747161865
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
Printing some Q and Qe and total Qs values:  [[0.392]
 [0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]] [[47.096]
 [38.51 ]
 [38.51 ]
 [38.51 ]
 [38.51 ]
 [38.51 ]
 [38.51 ]] [[1.143]
 [0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]]
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05171923584354508, 0.09280419480214801, 0.10681706889405544, 0.36143347398020276, 0.18253275682853204, 0.20469326965151668]
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05171923584354508, 0.09280419480214801, 0.10681706889405544, 0.36143347398020276, 0.18253275682853204, 0.20469326965151668]
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05166028286814243, 0.09282177510391652, 0.10686075245122506, 0.3610204281010235, 0.1827174839665589, 0.20491927750913352]
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05166028286814243, 0.09282177510391652, 0.10686075245122506, 0.3610204281010235, 0.1827174839665589, 0.20491927750913352]
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05166028286814243, 0.09282177510391652, 0.10686075245122506, 0.3610204281010235, 0.1827174839665589, 0.20491927750913352]
printing an ep nov before normalisation:  49.703085281235246
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05166028286814243, 0.09282177510391652, 0.10686075245122506, 0.3610204281010235, 0.1827174839665589, 0.20491927750913352]
printing an ep nov before normalisation:  29.73952087470345
printing an ep nov before normalisation:  26.802018444607743
printing an ep nov before normalisation:  39.81143348713122
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.051686158852791944, 0.09260455675266882, 0.10667633300869926, 0.36120165932955034, 0.18280917458102505, 0.20502211747526464]
printing an ep nov before normalisation:  47.056050300598145
actor:  1 policy actor:  1  step number:  71 total reward:  0.013333333333332531  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  21.844399412861435
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05146232272882744, 0.09220305826865027, 0.10621373670296451, 0.3596339471715689, 0.1815348599304707, 0.20895207519751832]
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05148641706308088, 0.09224627666657843, 0.10579449816934117, 0.3598027000076999, 0.1816200114707568, 0.2090500966225427]
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05148641706308088, 0.09224627666657843, 0.10579449816934117, 0.3598027000076999, 0.1816200114707568, 0.2090500966225427]
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05148641706308088, 0.09224627666657843, 0.10579449816934117, 0.3598027000076999, 0.1816200114707568, 0.2090500966225427]
printing an ep nov before normalisation:  36.95902579030985
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05147926641108734, 0.09227707723483726, 0.10582121571139788, 0.3597528671225697, 0.18162387748521366, 0.20904569603489406]
printing an ep nov before normalisation:  52.62298795868264
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05147926641108734, 0.09227707723483726, 0.10582121571139788, 0.3597528671225697, 0.18162387748521366, 0.20904569603489406]
line 256 mcts: sample exp_bonus 58.59420914360425
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05151482905758575, 0.09208074690079351, 0.10589441599355096, 0.36000194234296223, 0.18174957925106824, 0.20875848645403927]
actor:  1 policy actor:  1  step number:  67 total reward:  0.306666666666666  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
printing an ep nov before normalisation:  13.084635734558105
Printing some Q and Qe and total Qs values:  [[0.246]
 [0.193]
 [0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]] [[49.509]
 [38.75 ]
 [31.053]
 [31.053]
 [31.053]
 [31.053]
 [31.053]] [[1.218]
 [0.898]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]]
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.050461664980841966, 0.10913885730085829, 0.10404626839957586, 0.3526254568899298, 0.17832119918276385, 0.2054065532460303]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
printing an ep nov before normalisation:  54.875887191678856
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.050461664980841966, 0.10913885730085829, 0.10404626839957586, 0.3526254568899298, 0.17832119918276385, 0.2054065532460303]
printing an ep nov before normalisation:  30.886802673339844
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.050461664980841966, 0.10913885730085829, 0.10404626839957586, 0.3526254568899298, 0.17832119918276385, 0.2054065532460303]
Printing some Q and Qe and total Qs values:  [[0.15 ]
 [0.148]
 [0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.152]] [[64.4  ]
 [64.252]
 [68.195]
 [68.195]
 [68.195]
 [68.195]
 [68.195]] [[1.953]
 [1.945]
 [2.112]
 [2.112]
 [2.112]
 [2.112]
 [2.112]]
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.050461664980841966, 0.10913885730085829, 0.10404626839957586, 0.3526254568899298, 0.17832119918276385, 0.2054065532460303]
printing an ep nov before normalisation:  47.34874047929338
actor:  1 policy actor:  1  step number:  62 total reward:  0.5133333333333336  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05139595938131066, 0.10840708904773594, 0.1034590975929749, 0.35917137695806034, 0.1756250882005486, 0.20194138881936957]
Printing some Q and Qe and total Qs values:  [[0.096]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]] [[22.982]
 [15.884]
 [15.884]
 [15.884]
 [15.884]
 [15.884]
 [15.884]] [[1.229]
 [0.876]
 [0.876]
 [0.876]
 [0.876]
 [0.876]
 [0.876]]
printing an ep nov before normalisation:  37.086082827141404
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05139595938131066, 0.10840708904773594, 0.1034590975929749, 0.35917137695806034, 0.1756250882005486, 0.20194138881936957]
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05133753687014778, 0.10845284736429574, 0.10349581406369972, 0.35876205299330166, 0.17579367914348457, 0.20215806956507057]
from probs:  [0.05133753687014778, 0.10845284736429574, 0.10349581406369972, 0.35876205299330166, 0.17579367914348457, 0.20215806956507057]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.051279237085159725, 0.10849850955792197, 0.10353245340538787, 0.35835358888142266, 0.17596191593318192, 0.2023742951369258]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
Printing some Q and Qe and total Qs values:  [[0.116]
 [0.431]
 [0.157]
 [0.157]
 [0.157]
 [0.157]
 [0.157]] [[35.95 ]
 [32.355]
 [26.144]
 [26.144]
 [26.144]
 [26.144]
 [26.144]] [[1.483]
 [1.662]
 [1.151]
 [1.151]
 [1.151]
 [1.151]
 [1.151]]
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.051279237085159725, 0.10849850955792197, 0.10353245340538787, 0.35835358888142266, 0.17596191593318192, 0.2023742951369258]
Printing some Q and Qe and total Qs values:  [[0.146]
 [0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]] [[72.785]
 [70.067]
 [70.067]
 [70.067]
 [70.067]
 [70.067]
 [70.067]] [[1.581]
 [1.526]
 [1.526]
 [1.526]
 [1.526]
 [1.526]
 [1.526]]
printing an ep nov before normalisation:  36.51952981372915
printing an ep nov before normalisation:  34.080313552655355
Printing some Q and Qe and total Qs values:  [[0.307]
 [0.302]
 [0.858]
 [0.505]
 [0.326]
 [0.307]
 [0.312]] [[12.558]
 [43.021]
 [12.288]
 [14.178]
 [14.012]
 [12.558]
 [13.107]] [[0.307]
 [0.302]
 [0.858]
 [0.505]
 [0.326]
 [0.307]
 [0.312]]
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
Printing some Q and Qe and total Qs values:  [[0.343]
 [0.385]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]] [[33.733]
 [35.299]
 [33.733]
 [33.733]
 [33.733]
 [33.733]
 [33.733]] [[0.343]
 [0.385]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]]
actions average: 
K:  4  action  0 :  tensor([0.5090, 0.0899, 0.0898, 0.0712, 0.0938, 0.0606, 0.0857],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0604, 0.8443, 0.0235, 0.0149, 0.0121, 0.0078, 0.0370],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2006, 0.0903, 0.2447, 0.1030, 0.0986, 0.1517, 0.1111],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1703, 0.0536, 0.0890, 0.3546, 0.1044, 0.1348, 0.0933],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2982, 0.0833, 0.1270, 0.1096, 0.2198, 0.0812, 0.0808],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.4637, 0.0053, 0.1070, 0.1248, 0.0967, 0.1126, 0.0897],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.3219, 0.1920, 0.1276, 0.0838, 0.0778, 0.0729, 0.1241],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05130037996613318, 0.1085433059019788, 0.10357519686724613, 0.35850167101782826, 0.17603460048908096, 0.2020448457577327]
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
actor:  1 policy actor:  1  step number:  55 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  9.91156375177373
Printing some Q and Qe and total Qs values:  [[0.348]
 [0.348]
 [0.352]
 [0.348]
 [0.348]
 [0.348]
 [0.355]] [[11.242]
 [11.242]
 [11.655]
 [11.242]
 [11.242]
 [11.242]
 [10.869]] [[0.348]
 [0.348]
 [0.352]
 [0.348]
 [0.348]
 [0.348]
 [0.355]]
printing an ep nov before normalisation:  25.395811050421003
printing an ep nov before normalisation:  61.207644490328356
printing an ep nov before normalisation:  29.48852979136987
printing an ep nov before normalisation:  23.53655421527413
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05064510121323417, 0.10732186432790147, 0.10240289251983989, 0.35391204154483485, 0.18581947590555856, 0.19989862448863113]
actions average: 
K:  3  action  0 :  tensor([0.5011, 0.0624, 0.0786, 0.0768, 0.0895, 0.0847, 0.1069],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0687, 0.7423, 0.0351, 0.0300, 0.0275, 0.0397, 0.0568],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1190, 0.1234, 0.3511, 0.0867, 0.0697, 0.1398, 0.1104],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2193, 0.1616, 0.0876, 0.1658, 0.1154, 0.1527, 0.0975],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2512, 0.0706, 0.0917, 0.1223, 0.2053, 0.1336, 0.1252],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0986, 0.0491, 0.1431, 0.0747, 0.0675, 0.4736, 0.0933],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.5030, 0.1089, 0.0602, 0.0614, 0.0570, 0.0761, 0.1333],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.093]
 [0.129]
 [0.094]
 [0.091]
 [0.093]
 [0.096]
 [0.095]] [[31.877]
 [39.846]
 [29.128]
 [30.966]
 [31.197]
 [31.337]
 [31.109]] [[0.993]
 [1.462]
 [0.845]
 [0.943]
 [0.957]
 [0.967]
 [0.954]]
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05064510121323417, 0.10732186432790147, 0.10240289251983989, 0.35391204154483485, 0.18581947590555856, 0.19989862448863113]
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05064510121323417, 0.10732186432790147, 0.10240289251983989, 0.35391204154483485, 0.18581947590555856, 0.19989862448863113]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.117]
 [0.27 ]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]] [[27.604]
 [45.834]
 [27.604]
 [27.604]
 [27.604]
 [27.604]
 [27.604]] [[0.223]
 [0.513]
 [0.223]
 [0.223]
 [0.223]
 [0.223]
 [0.223]]
Printing some Q and Qe and total Qs values:  [[0.084]
 [0.068]
 [0.068]
 [0.068]
 [0.068]
 [0.068]
 [0.068]] [[27.286]
 [26.499]
 [26.499]
 [26.499]
 [26.499]
 [26.499]
 [26.499]] [[1.207]
 [1.158]
 [1.158]
 [1.158]
 [1.158]
 [1.158]
 [1.158]]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  37.193763267861456
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.050661983773835025, 0.1070237275662074, 0.10243707406325658, 0.35403028487561083, 0.18588153773532665, 0.19996539198576355]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05060356611341097, 0.10706598425148323, 0.102471137986753, 0.3536209972902804, 0.18606465163877775, 0.2001736627192946]
Printing some Q and Qe and total Qs values:  [[0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]] [[24.05]
 [24.05]
 [24.05]
 [24.05]
 [24.05]
 [24.05]
 [24.05]] [[0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]]
printing an ep nov before normalisation:  39.771889999305785
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05059706289838089, 0.10709085981225311, 0.10249732158765082, 0.35357568468104156, 0.18606703821492432, 0.20017203280574916]
printing an ep nov before normalisation:  31.338368031214024
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05059706289838089, 0.10709085981225311, 0.10249732158765082, 0.35357568468104156, 0.18606703821492432, 0.20017203280574916]
printing an ep nov before normalisation:  40.43245463416253
printing an ep nov before normalisation:  47.67763874358806
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05061765765826671, 0.10713451021497795, 0.10253909732479362, 0.3537199278284191, 0.18614291952903253, 0.19984588744451004]
printing an ep nov before normalisation:  73.59685691419324
printing an ep nov before normalisation:  55.167593518791534
using another actor
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.050642491193896695, 0.10718714466361152, 0.10258947126760995, 0.3538938588360948, 0.18574290697097082, 0.19994412706781628]
printing an ep nov before normalisation:  37.98303434180822
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05058423153453254, 0.10722955289103502, 0.10262369413998192, 0.3534856784980006, 0.18592517002287753, 0.2001516729135724]
UNIT TEST: sample policy line 217 mcts : [0.102 0.163 0.163 0.102 0.224 0.102 0.143]
printing an ep nov before normalisation:  33.31131964657177
printing an ep nov before normalisation:  24.71287488937378
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05058423153453254, 0.10722955289103502, 0.10262369413998192, 0.3534856784980006, 0.18592517002287753, 0.2001516729135724]
printing an ep nov before normalisation:  36.03741645812988
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05058423153453254, 0.10722955289103502, 0.10262369413998192, 0.3534856784980006, 0.18592517002287753, 0.2001516729135724]
printing an ep nov before normalisation:  33.3874026063962
printing an ep nov before normalisation:  55.2009047962986
printing an ep nov before normalisation:  21.191663401467462
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05060477066162662, 0.10727315291571952, 0.10266541907181796, 0.3536295319444621, 0.1860008078491675, 0.1998263175572063]
actor:  1 policy actor:  1  step number:  66 total reward:  0.3266666666666669  reward:  1.0 rdn_beta:  0.667
from probs:  [0.05060477066162662, 0.10727315291571952, 0.10266541907181796, 0.3536295319444621, 0.1860008078491675, 0.1998263175572063]
actor:  1 policy actor:  1  step number:  54 total reward:  0.3266666666666661  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  32.72802874841241
siam score:  -0.7154585
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.05031504986479805, 0.12299319250004895, 0.10043764571208755, 0.35160183664105094, 0.18067072269093964, 0.19398155259107494]
actions average: 
K:  2  action  0 :  tensor([0.5685, 0.0259, 0.0838, 0.0774, 0.1114, 0.0606, 0.0723],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0310, 0.9016, 0.0203, 0.0099, 0.0078, 0.0087, 0.0207],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1971, 0.0773, 0.3231, 0.0917, 0.0794, 0.1254, 0.1060],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1758, 0.0154, 0.1153, 0.3033, 0.1391, 0.1367, 0.1145],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2330, 0.0419, 0.1016, 0.1395, 0.2743, 0.1093, 0.1005],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1765, 0.0401, 0.1179, 0.0951, 0.1006, 0.3640, 0.1058],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1954, 0.0303, 0.1590, 0.1463, 0.1228, 0.1149, 0.2314],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  52 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
maxi score, test score, baseline:  -0.9784866666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.04936394506175714, 0.13960262938758203, 0.09853655865064972, 0.3449403910767516, 0.17724896419622796, 0.19030751162703158]
actor:  0 policy actor:  1  step number:  52 total reward:  0.33999999999999986  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  39.661050036683726
maxi score, test score, baseline:  -0.9758066666666666 -0.7676666666666666 -0.7676666666666666
using explorer policy with actor:  1
printing an ep nov before normalisation:  59.63770412616509
maxi score, test score, baseline:  -0.9758066666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.04936394506175714, 0.13960262938758203, 0.09853655865064972, 0.3449403910767516, 0.17724896419622796, 0.19030751162703158]
printing an ep nov before normalisation:  45.66898479723528
maxi score, test score, baseline:  -0.9758066666666666 -0.7676666666666666 -0.7676666666666666
maxi score, test score, baseline:  -0.9758066666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.04936394506175714, 0.13960262938758203, 0.09853655865064972, 0.3449403910767516, 0.17724896419622796, 0.19030751162703158]
actor:  1 policy actor:  1  step number:  67 total reward:  0.1733333333333329  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 24.18631693955093
printing an ep nov before normalisation:  49.06968454581598
Printing some Q and Qe and total Qs values:  [[-0.021]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]] [[28.952]
 [18.424]
 [18.424]
 [18.424]
 [18.424]
 [18.424]
 [18.424]] [[0.825]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]]
siam score:  -0.72084135
Printing some Q and Qe and total Qs values:  [[-0.062]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]] [[19.789]
 [17.638]
 [17.638]
 [17.638]
 [17.638]
 [17.638]
 [17.638]] [[0.698]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]]
UNIT TEST: sample policy line 217 mcts : [0.041 0.51  0.02  0.388 0.02  0.02  0.   ]
maxi score, test score, baseline:  -0.9758066666666666 -0.7676666666666666 -0.7676666666666666
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.766755302906766
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9758066666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.04867103031737683, 0.15025266306577337, 0.0974345022222323, 0.3400870294034059, 0.17549197967249358, 0.18806279531871786]
maxi score, test score, baseline:  -0.9758066666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.04867103031737683, 0.15025266306577337, 0.0974345022222323, 0.3400870294034059, 0.17549197967249358, 0.18806279531871786]
printing an ep nov before normalisation:  24.166731181287034
actor:  1 policy actor:  1  step number:  62 total reward:  0.13999999999999968  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.197]
 [0.21 ]
 [0.197]
 [0.197]
 [0.197]
 [1.092]
 [0.232]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.197]
 [0.21 ]
 [0.197]
 [0.197]
 [0.197]
 [1.092]
 [0.232]]
printing an ep nov before normalisation:  43.314404160756254
using another actor
maxi score, test score, baseline:  -0.9758066666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.04825244956631552, 0.14925031365343966, 0.0967356878704432, 0.3371552066895541, 0.18213816467630486, 0.1864681775439426]
maxi score, test score, baseline:  -0.9758066666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.048193769217670684, 0.14936041371956482, 0.0967580292458153, 0.3367440898023215, 0.18230322456941947, 0.18664047344520812]
printing an ep nov before normalisation:  34.093425160300086
printing an ep nov before normalisation:  36.26804126933564
maxi score, test score, baseline:  -0.9758066666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.048193769217670684, 0.14936041371956482, 0.0967580292458153, 0.3367440898023215, 0.18230322456941947, 0.18664047344520812]
maxi score, test score, baseline:  -0.9758066666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.048193769217670684, 0.14936041371956482, 0.0967580292458153, 0.3367440898023215, 0.18230322456941947, 0.18664047344520812]
printing an ep nov before normalisation:  64.29447865261719
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.196]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]] [[34.069]
 [36.768]
 [36.718]
 [36.718]
 [36.718]
 [36.718]
 [36.718]] [[1.518]
 [1.374]
 [1.344]
 [1.344]
 [1.344]
 [1.344]
 [1.344]]
Printing some Q and Qe and total Qs values:  [[0.343]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]] [[45.098]
 [44.927]
 [44.927]
 [44.927]
 [44.927]
 [44.927]
 [44.927]] [[1.875]
 [1.754]
 [1.754]
 [1.754]
 [1.754]
 [1.754]
 [1.754]]
maxi score, test score, baseline:  -0.9758066666666666 -0.7676666666666666 -0.7676666666666666
actor:  1 policy actor:  1  step number:  48 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  52.94053541147549
maxi score, test score, baseline:  -0.9758066666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.04758965913952458, 0.14748466194991292, 0.09554347747443144, 0.33251296788089996, 0.1929421177477949, 0.1839271158074363]
printing an ep nov before normalisation:  46.90994231340029
maxi score, test score, baseline:  -0.9758066666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.04758965913952458, 0.14748466194991292, 0.09554347747443144, 0.33251296788089996, 0.1929421177477949, 0.1839271158074363]
maxi score, test score, baseline:  -0.9758066666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.04758965913952458, 0.14748466194991292, 0.09554347747443144, 0.33251296788089996, 0.1929421177477949, 0.1839271158074363]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9758066666666666 -0.7676666666666666 -0.7676666666666666
maxi score, test score, baseline:  -0.9758066666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.04758965913952458, 0.14748466194991292, 0.09554347747443144, 0.33251296788089996, 0.1929421177477949, 0.1839271158074363]
maxi score, test score, baseline:  -0.9758066666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.04758965913952458, 0.14748466194991292, 0.09554347747443144, 0.33251296788089996, 0.1929421177477949, 0.1839271158074363]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9758066666666666 -0.7676666666666666 -0.7676666666666666
maxi score, test score, baseline:  -0.9758066666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.04758965913952458, 0.14748466194991292, 0.09554347747443144, 0.33251296788089996, 0.1929421177477949, 0.1839271158074363]
Printing some Q and Qe and total Qs values:  [[0.331]
 [0.639]
 [0.3  ]
 [0.3  ]
 [0.338]
 [0.3  ]
 [0.3  ]] [[33.314]
 [38.125]
 [32.174]
 [32.174]
 [35.007]
 [32.174]
 [32.174]] [[0.963]
 [1.449]
 [0.889]
 [0.889]
 [1.033]
 [0.889]
 [0.889]]
printing an ep nov before normalisation:  31.597740114772108
printing an ep nov before normalisation:  48.3768606926203
maxi score, test score, baseline:  -0.9758066666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.04758965913952458, 0.14748466194991292, 0.09554347747443144, 0.33251296788089996, 0.1929421177477949, 0.1839271158074363]
printing an ep nov before normalisation:  37.47280887102647
maxi score, test score, baseline:  -0.9758066666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.04758965913952458, 0.14748466194991292, 0.09554347747443144, 0.33251296788089996, 0.1929421177477949, 0.1839271158074363]
printing an ep nov before normalisation:  38.72304523616348
maxi score, test score, baseline:  -0.9758066666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.04758965913952458, 0.14748466194991292, 0.09554347747443144, 0.33251296788089996, 0.1929421177477949, 0.1839271158074363]
maxi score, test score, baseline:  -0.9758066666666666 -0.7676666666666666 -0.7676666666666666
actor:  1 policy actor:  1  step number:  63 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  59 total reward:  0.23999999999999944  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  72 total reward:  0.13999999999999868  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
Printing some Q and Qe and total Qs values:  [[0.103]
 [0.083]
 [0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]] [[45.449]
 [47.192]
 [45.449]
 [45.449]
 [45.449]
 [45.449]
 [45.449]] [[1.325]
 [1.396]
 [1.325]
 [1.325]
 [1.325]
 [1.325]
 [1.325]]
printing an ep nov before normalisation:  67.39500952623112
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.047700517923171655, 0.14564520113100393, 0.09471810051770316, 0.33329035250026606, 0.1976296320915356, 0.1810161958363196]
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.359]
 [0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]] [[29.792]
 [37.422]
 [29.792]
 [29.792]
 [29.792]
 [29.792]
 [29.792]] [[0.305]
 [0.359]
 [0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]]
printing an ep nov before normalisation:  48.91239627717316
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.04772676189777559, 0.14600502032392143, 0.09490447467870287, 0.33347404245014584, 0.1971101494579422, 0.18077955119151193]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.04772676189777559, 0.14600502032392143, 0.09490447467870287, 0.33347404245014584, 0.1971101494579422, 0.18077955119151193]
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.04775168280304173, 0.14608140164624747, 0.09495409875651722, 0.3336485862167288, 0.1966900894395612, 0.18087414113790362]
Printing some Q and Qe and total Qs values:  [[0.281]
 [0.319]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]] [[61.458]
 [55.713]
 [54.164]
 [54.164]
 [54.164]
 [54.164]
 [54.164]] [[1.708]
 [1.56 ]
 [1.433]
 [1.433]
 [1.433]
 [1.433]
 [1.433]]
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.04775168280304173, 0.14608140164624747, 0.09495409875651722, 0.3336485862167288, 0.1966900894395612, 0.18087414113790362]
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.04775168280304173, 0.14608140164624747, 0.09495409875651722, 0.3336485862167288, 0.1966900894395612, 0.18087414113790362]
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.04775168280304173, 0.14608140164624747, 0.09495409875651722, 0.3336485862167288, 0.1966900894395612, 0.18087414113790362]
Printing some Q and Qe and total Qs values:  [[0.049]
 [0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]] [[36.45 ]
 [28.472]
 [28.472]
 [28.472]
 [28.472]
 [28.472]
 [28.472]] [[1.222]
 [0.801]
 [0.801]
 [0.801]
 [0.801]
 [0.801]
 [0.801]]
printing an ep nov before normalisation:  34.26060806068512
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.04777646506764909, 0.14615735804192126, 0.0950034467643106, 0.3338221589566766, 0.1962723663105126, 0.18096820485892984]
actor:  1 policy actor:  1  step number:  56 total reward:  0.4066666666666662  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  61.72456027797659
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.0472628030664501, 0.14485986714686153, 0.09411351380536215, 0.3302243973713062, 0.19457559471866892, 0.18896382389135113]
printing an ep nov before normalisation:  54.38699988563293
printing an ep nov before normalisation:  21.487793922424316
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.04720487276981733, 0.14495853585089036, 0.09413075763974582, 0.3298185397012449, 0.19475403460565563, 0.1891332594326459]
printing an ep nov before normalisation:  48.02688050941704
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.04720487276981733, 0.14495853585089036, 0.09413075763974582, 0.3298185397012449, 0.19475403460565563, 0.1891332594326459]
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
printing an ep nov before normalisation:  48.96624037201742
from probs:  [0.04720487276981733, 0.14495853585089036, 0.09413075763974582, 0.3298185397012449, 0.19475403460565563, 0.1891332594326459]
printing an ep nov before normalisation:  32.232552985143236
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
printing an ep nov before normalisation:  29.75660066991176
printing an ep nov before normalisation:  34.60467755794525
printing an ep nov before normalisation:  31.371771708333508
printing an ep nov before normalisation:  43.41730923749737
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.047204260886598466, 0.14498355934212645, 0.09439512107879709, 0.3298140155568577, 0.19404810165261974, 0.18955494148300053]
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.047204260886598466, 0.14498355934212645, 0.09439512107879709, 0.3298140155568577, 0.19404810165261974, 0.18955494148300053]
printing an ep nov before normalisation:  66.23213161776874
printing an ep nov before normalisation:  58.47978466813572
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.047204260886598466, 0.14498355934212645, 0.09439512107879709, 0.3298140155568577, 0.19404810165261974, 0.18955494148300053]
printing an ep nov before normalisation:  62.38318233460589
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.047204260886598466, 0.14498355934212645, 0.09439512107879709, 0.3298140155568577, 0.19404810165261974, 0.18955494148300053]
printing an ep nov before normalisation:  25.169472694396973
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.04726965554671944, 0.14518479643925597, 0.09452607687051028, 0.3302720331791311, 0.19330709570960855, 0.18944034225477466]
actor:  1 policy actor:  1  step number:  51 total reward:  0.35999999999999943  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  74.21592391495192
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.04645897675428379, 0.159874932161692, 0.09290265558669103, 0.3245941204527401, 0.18998478295022425, 0.18618453209436875]
printing an ep nov before normalisation:  36.807589530944824
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.04645897675428379, 0.159874932161692, 0.09290265558669103, 0.3245941204527401, 0.18998478295022425, 0.18618453209436875]
printing an ep nov before normalisation:  38.703178443279974
actions average: 
K:  2  action  0 :  tensor([0.6018, 0.0372, 0.0633, 0.0603, 0.1290, 0.0511, 0.0572],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0367, 0.8589, 0.0299, 0.0198, 0.0100, 0.0149, 0.0298],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2869, 0.0958, 0.2250, 0.0839, 0.0895, 0.1137, 0.1051],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1965, 0.0706, 0.0748, 0.2897, 0.1127, 0.1857, 0.0700],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2095, 0.0699, 0.1240, 0.1178, 0.2380, 0.1116, 0.1292],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1796, 0.0033, 0.0804, 0.1168, 0.1227, 0.4390, 0.0583],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1892, 0.1277, 0.0643, 0.1416, 0.0814, 0.0596, 0.3363],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.04645897675428379, 0.159874932161692, 0.09290265558669103, 0.3245941204527401, 0.18998478295022425, 0.18618453209436875]
printing an ep nov before normalisation:  31.276608539738834
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.0464681593167281, 0.1599065957221268, 0.09272298529116693, 0.32465843419654333, 0.19002241480226878, 0.18622141067116602]
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.252]
 [0.242]
 [0.213]
 [0.215]
 [0.242]
 [0.242]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.242]
 [0.252]
 [0.242]
 [0.213]
 [0.215]
 [0.242]
 [0.242]]
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.04641041608892523, 0.1600263746845538, 0.092737627211431, 0.32425388913311676, 0.19018932264211239, 0.18638237023986085]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.04710970953645923, 0.15857580382516956, 0.09256030773537194, 0.3291530906992675, 0.18816800233866293, 0.18443308586506885]
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.04705254725759822, 0.1586943777032234, 0.0925748023447147, 0.328752615723979, 0.18833323093512785, 0.18459242603535667]
Printing some Q and Qe and total Qs values:  [[0.11 ]
 [0.109]
 [0.101]
 [0.098]
 [0.092]
 [0.092]
 [0.099]] [[28.195]
 [40.396]
 [26.142]
 [25.886]
 [27.22 ]
 [27.403]
 [26.818]] [[0.9  ]
 [1.567]
 [0.779]
 [0.762]
 [0.829]
 [0.839]
 [0.814]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.04708003151578436, 0.15820195043668556, 0.09262895249231581, 0.3289451133994385, 0.18844347416648452, 0.18470047798929115]
printing an ep nov before normalisation:  35.28950358406142
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.139]
 [0.079]
 [0.079]
 [0.079]
 [0.079]
 [0.079]
 [0.079]] [[37.872]
 [34.065]
 [34.065]
 [34.065]
 [34.065]
 [34.065]
 [34.065]] [[1.636]
 [1.398]
 [1.398]
 [1.398]
 [1.398]
 [1.398]
 [1.398]]
printing an ep nov before normalisation:  47.800921303070076
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.04708003151578436, 0.15820195043668556, 0.09262895249231581, 0.3289451133994385, 0.18844347416648452, 0.18470047798929115]
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.04708003151578436, 0.15820195043668556, 0.09262895249231581, 0.3289451133994385, 0.18844347416648452, 0.18470047798929115]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.49651765823364
actor:  1 policy actor:  1  step number:  71 total reward:  0.11999999999999911  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.04688972023726289, 0.15698390063356804, 0.09225399662658719, 0.32761218753034665, 0.18719778557249173, 0.1890624093997435]
Printing some Q and Qe and total Qs values:  [[0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]] [[71.417]
 [71.417]
 [71.417]
 [71.417]
 [71.417]
 [71.417]
 [71.417]] [[1.256]
 [1.256]
 [1.256]
 [1.256]
 [1.256]
 [1.256]
 [1.256]]
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.04688972023726289, 0.15698390063356804, 0.09225399662658719, 0.32761218753034665, 0.18719778557249173, 0.1890624093997435]
actor:  1 policy actor:  1  step number:  64 total reward:  0.12666666666666604  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  35.908284144630926
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.04825406736527296, 0.1543120239530174, 0.0849657101621963, 0.3371091855780088, 0.18668070390020852, 0.18867830904129598]
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
printing an ep nov before normalisation:  37.64467468885914
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.04825406736527296, 0.1543120239530174, 0.0849657101621963, 0.3371091855780088, 0.18668070390020852, 0.18867830904129598]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3266666666666661  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  45.43912437696912
printing an ep nov before normalisation:  67.36988861680864
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.04887288465100712, 0.1531127884045028, 0.0847521538605083, 0.34144560663022483, 0.18492660219914644, 0.1868899642546106]
printing an ep nov before normalisation:  57.245096880931634
Printing some Q and Qe and total Qs values:  [[0.153]
 [0.169]
 [0.153]
 [0.153]
 [0.153]
 [0.153]
 [0.153]] [[64.766]
 [76.811]
 [64.766]
 [64.766]
 [64.766]
 [64.766]
 [64.766]] [[1.419]
 [1.81 ]
 [1.419]
 [1.419]
 [1.419]
 [1.419]
 [1.419]]
printing an ep nov before normalisation:  31.60292387008667
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.04887288465100712, 0.1531127884045028, 0.0847521538605083, 0.34144560663022483, 0.18492660219914644, 0.1868899642546106]
actor:  1 policy actor:  1  step number:  66 total reward:  0.3399999999999992  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.093]
 [0.041]
 [0.092]
 [0.094]
 [0.095]
 [0.095]
 [0.093]] [[38.689]
 [50.872]
 [34.862]
 [35.043]
 [35.417]
 [35.767]
 [35.162]] [[0.689]
 [1.114]
 [0.538]
 [0.547]
 [0.563]
 [0.576]
 [0.551]]
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.04833730663281092, 0.15143176531188965, 0.0838223148087746, 0.33769507952600164, 0.19387575464053147, 0.1848377790799918]
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.04833730663281092, 0.15143176531188965, 0.0838223148087746, 0.33769507952600164, 0.19387575464053147, 0.1848377790799918]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.04833730663281092, 0.15143176531188965, 0.0838223148087746, 0.33769507952600164, 0.19387575464053147, 0.1848377790799918]
printing an ep nov before normalisation:  57.643660903642775
Printing some Q and Qe and total Qs values:  [[0.064]
 [0.052]
 [0.023]
 [0.023]
 [0.023]
 [0.023]
 [0.023]] [[56.182]
 [58.858]
 [50.68 ]
 [50.68 ]
 [50.68 ]
 [50.68 ]
 [50.68 ]] [[1.151]
 [1.22 ]
 [0.945]
 [0.945]
 [0.945]
 [0.945]
 [0.945]]
Printing some Q and Qe and total Qs values:  [[0.182]
 [0.185]
 [0.184]
 [0.181]
 [0.185]
 [0.179]
 [0.181]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.182]
 [0.185]
 [0.184]
 [0.181]
 [0.185]
 [0.179]
 [0.181]]
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.04835611239924412, 0.15149079112432864, 0.08385496427427408, 0.337826771892617, 0.1939513390461804, 0.18452002126335584]
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.048297554203212134, 0.15160473471125765, 0.08385578101708137, 0.33741641931966404, 0.19413630161830983, 0.18468920913047504]
printing an ep nov before normalisation:  40.28796191992041
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.04823911620188151, 0.15171844442118343, 0.08385659608346821, 0.33700690902342556, 0.1943208845421128, 0.18485804972792846]
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.433]
 [0.381]
 [0.381]
 [0.381]
 [0.381]
 [0.381]] [[25.138]
 [32.194]
 [28.76 ]
 [28.76 ]
 [28.76 ]
 [28.76 ]
 [28.76 ]] [[0.378]
 [0.433]
 [0.381]
 [0.381]
 [0.381]
 [0.381]
 [0.381]]
printing an ep nov before normalisation:  41.122390687113004
printing an ep nov before normalisation:  36.417148480910654
printing an ep nov before normalisation:  64.56504233575416
actor:  1 policy actor:  1  step number:  68 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.048800788576413665, 0.15071959113866218, 0.08388113710866453, 0.3409427963855325, 0.19267956283613125, 0.18297612395459584]
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.048800788576413665, 0.15071959113866218, 0.08388113710866453, 0.3409427963855325, 0.19267956283613125, 0.18297612395459584]
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
Printing some Q and Qe and total Qs values:  [[0.229]
 [0.245]
 [0.193]
 [0.184]
 [0.229]
 [0.229]
 [0.229]] [[39.799]
 [43.676]
 [37.598]
 [39.626]
 [39.799]
 [39.799]
 [39.799]] [[1.754]
 [2.058]
 [1.554]
 [1.695]
 [1.754]
 [1.754]
 [1.754]]
maxi score, test score, baseline:  -0.9733266666666668 -0.7676666666666666 -0.7676666666666666
probs:  [0.048800788576413665, 0.15071959113866218, 0.08388113710866453, 0.3409427963855325, 0.19267956283613125, 0.18297612395459584]
printing an ep nov before normalisation:  41.891754454631055
line 256 mcts: sample exp_bonus 0.00047126593642587977
actor:  0 policy actor:  0  step number:  85 total reward:  0.013333333333331865  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9712999999999999 -0.7676666666666666 -0.7676666666666666
probs:  [0.048819372878042624, 0.15077709437720668, 0.08391311726892978, 0.34107293867338634, 0.19275308900126448, 0.1826643878011701]
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.18441318295595
maxi score, test score, baseline:  -0.9712999999999999 -0.7676666666666666 -0.7676666666666666
probs:  [0.04880685754963094, 0.15080304663615526, 0.08397880457154644, 0.340985804424323, 0.19275409631898263, 0.18267139049936182]
printing an ep nov before normalisation:  28.803548991215102
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  60 total reward:  0.17999999999999938  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.149]
 [0.206]
 [0.158]
 [0.16 ]
 [0.147]
 [0.153]
 [0.158]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.149]
 [0.206]
 [0.158]
 [0.16 ]
 [0.147]
 [0.153]
 [0.158]]
maxi score, test score, baseline:  -0.9712999999999999 -0.7676666666666666 -0.7676666666666666
probs:  [0.04843217852600819, 0.14964322313257752, 0.0833333795494619, 0.33836199541503853, 0.19896297196099205, 0.1812662514159218]
maxi score, test score, baseline:  -0.9712999999999999 -0.7676666666666666 -0.7676666666666666
probs:  [0.04843217852600819, 0.14964322313257752, 0.0833333795494619, 0.33836199541503853, 0.19896297196099205, 0.1812662514159218]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9712999999999999 -0.7676666666666666 -0.7676666666666666
probs:  [0.04843217852600819, 0.14964322313257752, 0.0833333795494619, 0.33836199541503853, 0.19896297196099205, 0.1812662514159218]
printing an ep nov before normalisation:  60.25896902040831
maxi score, test score, baseline:  -0.9712999999999999 -0.7676666666666666 -0.7676666666666666
probs:  [0.04843217852600819, 0.14964322313257752, 0.0833333795494619, 0.33836199541503853, 0.19896297196099205, 0.1812662514159218]
maxi score, test score, baseline:  -0.9712999999999999 -0.7676666666666666 -0.7676666666666666
maxi score, test score, baseline:  -0.9712999999999999 -0.7676666666666666 -0.7676666666666666
probs:  [0.04845038575556909, 0.14969958383453408, 0.08336474346577573, 0.3384894973298618, 0.1990379247018194, 0.1809578649124399]
printing an ep nov before normalisation:  38.99105163046266
maxi score, test score, baseline:  -0.9712999999999999 -0.7676666666666666 -0.7676666666666666
probs:  [0.04845038575556909, 0.14969958383453408, 0.08336474346577573, 0.3384894973298618, 0.1990379247018194, 0.1809578649124399]
printing an ep nov before normalisation:  35.48551129085061
printing an ep nov before normalisation:  43.989906311035156
printing an ep nov before normalisation:  36.55430093450457
maxi score, test score, baseline:  -0.9712999999999999 -0.7676666666666666 -0.7676666666666666
probs:  [0.04845038575556909, 0.14969958383453408, 0.08336474346577573, 0.3384894973298618, 0.1990379247018194, 0.1809578649124399]
printing an ep nov before normalisation:  42.552463066112445
maxi score, test score, baseline:  -0.9712999999999999 -0.7676666666666666 -0.7676666666666666
probs:  [0.048478648529404214, 0.1492025963861538, 0.08341342914145174, 0.3386874163910864, 0.19915427257095797, 0.1810636369809459]
maxi score, test score, baseline:  -0.9712999999999999 -0.7676666666666666 -0.7676666666666666
probs:  [0.048478648529404214, 0.1492025963861538, 0.08341342914145174, 0.3386874163910864, 0.19915427257095797, 0.1810636369809459]
printing an ep nov before normalisation:  43.47782790574438
printing an ep nov before normalisation:  32.30196952312263
actor:  0 policy actor:  0  step number:  76 total reward:  0.1133333333333324  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  42.41887221083876
actor:  1 policy actor:  1  step number:  42 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9690733333333335 -0.7676666666666666 -0.7676666666666666
probs:  [0.04780125380758521, 0.14711390214246925, 0.09624690223747207, 0.3339437444428537, 0.1963656774773161, 0.17852851989230376]
printing an ep nov before normalisation:  32.14416441208147
actor:  1 policy actor:  1  step number:  66 total reward:  0.20666666666666644  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  30.390436504822286
Printing some Q and Qe and total Qs values:  [[0.518]
 [0.516]
 [0.529]
 [0.533]
 [0.539]
 [0.528]
 [0.513]] [[19.35 ]
 [22.671]
 [19.098]
 [19.151]
 [19.327]
 [19.984]
 [17.977]] [[0.518]
 [0.516]
 [0.529]
 [0.533]
 [0.539]
 [0.528]
 [0.513]]
printing an ep nov before normalisation:  23.87386441130179
printing an ep nov before normalisation:  45.41278296236635
actions average: 
K:  3  action  0 :  tensor([0.5683, 0.0691, 0.0698, 0.0805, 0.0906, 0.0524, 0.0693],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0268, 0.8705, 0.0219, 0.0242, 0.0102, 0.0112, 0.0353],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1417, 0.1043, 0.4180, 0.0696, 0.0738, 0.0953, 0.0972],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1933, 0.1260, 0.0973, 0.2473, 0.1326, 0.1103, 0.0931],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1853, 0.1072, 0.0574, 0.1378, 0.3974, 0.0584, 0.0565],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1909, 0.0749, 0.1282, 0.0910, 0.1386, 0.2860, 0.0905],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2199, 0.0830, 0.0938, 0.1069, 0.1470, 0.1093, 0.2401],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.076]
 [0.182]
 [0.076]
 [0.076]
 [0.076]
 [0.076]
 [0.076]] [[39.937]
 [42.999]
 [39.937]
 [39.937]
 [39.937]
 [39.937]
 [39.937]] [[1.271]
 [1.556]
 [1.271]
 [1.271]
 [1.271]
 [1.271]
 [1.271]]
maxi score, test score, baseline:  -0.9690733333333335 -0.7676666666666666 -0.7676666666666666
probs:  [0.04724151252083688, 0.15748652073011507, 0.09511828845348688, 0.33002397808700484, 0.19406142006925164, 0.17606828013930464]
Printing some Q and Qe and total Qs values:  [[0.3  ]
 [0.574]
 [0.312]
 [0.312]
 [0.315]
 [0.312]
 [0.312]] [[29.414]
 [37.405]
 [24.77 ]
 [24.77 ]
 [28.442]
 [24.77 ]
 [24.77 ]] [[0.951]
 [1.499]
 [0.804]
 [0.804]
 [0.934]
 [0.804]
 [0.804]]
printing an ep nov before normalisation:  25.530056065886395
Printing some Q and Qe and total Qs values:  [[0.712]
 [0.74 ]
 [0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.712]] [[35.319]
 [30.05 ]
 [35.319]
 [35.319]
 [35.319]
 [35.319]
 [35.319]] [[1.888]
 [1.636]
 [1.888]
 [1.888]
 [1.888]
 [1.888]
 [1.888]]
maxi score, test score, baseline:  -0.9690733333333335 -0.7676666666666666 -0.7676666666666666
probs:  [0.04724151252083688, 0.15748652073011507, 0.09511828845348688, 0.33002397808700484, 0.19406142006925164, 0.17606828013930464]
printing an ep nov before normalisation:  29.088315963745117
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  60 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  1.333
from probs:  [0.04724151252083688, 0.15748652073011507, 0.09511828845348688, 0.33002397808700484, 0.19406142006925164, 0.17606828013930464]
line 256 mcts: sample exp_bonus 23.856283986815562
actor:  1 policy actor:  1  step number:  61 total reward:  0.346666666666666  reward:  1.0 rdn_beta:  1.667
siam score:  -0.745272
printing an ep nov before normalisation:  41.857383824243854
printing an ep nov before normalisation:  62.30293921905028
printing an ep nov before normalisation:  24.400103092193604
line 256 mcts: sample exp_bonus 0.0
actor:  1 policy actor:  1  step number:  60 total reward:  0.13999999999999924  reward:  1.0 rdn_beta:  1.667
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9690733333333335 -0.7676666666666666 -0.7676666666666666
printing an ep nov before normalisation:  62.75780060095201
maxi score, test score, baseline:  -0.9690733333333335 -0.7676666666666666 -0.7676666666666666
probs:  [0.04686246602974204, 0.154002914566852, 0.09339100579090225, 0.3273714625431448, 0.20631074959187976, 0.17206140147747914]
printing an ep nov before normalisation:  45.78160497890418
maxi score, test score, baseline:  -0.9690733333333335 -0.7676666666666666 -0.7676666666666666
maxi score, test score, baseline:  -0.9690733333333335 -0.7676666666666666 -0.7676666666666666
probs:  [0.04686246602974204, 0.154002914566852, 0.09339100579090225, 0.3273714625431448, 0.20631074959187976, 0.17206140147747914]
printing an ep nov before normalisation:  46.742689419542096
printing an ep nov before normalisation:  6.580097000602219
printing an ep nov before normalisation:  40.7767329792605
maxi score, test score, baseline:  -0.9690733333333335 -0.7676666666666666 -0.7676666666666666
probs:  [0.046889004428185076, 0.15409030000574733, 0.09344396860682429, 0.3275573071839187, 0.20586037711055108, 0.17215904266477347]
actor:  1 policy actor:  1  step number:  51 total reward:  0.3866666666666658  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9690733333333335 -0.7676666666666666 -0.7676666666666666
printing an ep nov before normalisation:  36.47470504914701
maxi score, test score, baseline:  -0.9690733333333335 -0.7676666666666666 -0.7676666666666666
probs:  [0.046125029369811284, 0.1679013204816639, 0.09191929972100017, 0.3222072981926651, 0.202498858609189, 0.16934819362567047]
Printing some Q and Qe and total Qs values:  [[0.643]
 [0.632]
 [0.63 ]
 [0.617]
 [0.63 ]
 [0.613]
 [0.621]] [[18.293]
 [24.707]
 [15.878]
 [16.332]
 [15.878]
 [16.35 ]
 [17.058]] [[0.643]
 [0.632]
 [0.63 ]
 [0.617]
 [0.63 ]
 [0.613]
 [0.621]]
maxi score, test score, baseline:  -0.9690733333333335 -0.7676666666666666 -0.7676666666666666
probs:  [0.046125029369811284, 0.1679013204816639, 0.09191929972100017, 0.3222072981926651, 0.202498858609189, 0.16934819362567047]
printing an ep nov before normalisation:  39.74717132205209
maxi score, test score, baseline:  -0.9690733333333335 -0.7676666666666666 -0.7676666666666666
probs:  [0.04615576119076448, 0.16734575547493424, 0.09198063137142316, 0.3224225087765906, 0.20263407975755293, 0.16946126342873455]
maxi score, test score, baseline:  -0.9690733333333335 -0.7676666666666666 -0.7676666666666666
maxi score, test score, baseline:  -0.9690733333333335 -0.7676666666666666 -0.7676666666666666
probs:  [0.04615576119076448, 0.16734575547493424, 0.09198063137142316, 0.3224225087765906, 0.20263407975755293, 0.16946126342873455]
maxi score, test score, baseline:  -0.9690733333333335 -0.7676666666666666 -0.7676666666666666
probs:  [0.04615576119076448, 0.16734575547493424, 0.09198063137142316, 0.3224225087765906, 0.20263407975755293, 0.16946126342873455]
printing an ep nov before normalisation:  52.85983607134659
maxi score, test score, baseline:  -0.9690733333333335 -0.7676666666666666 -0.7676666666666666
probs:  [0.04615576119076448, 0.16734575547493424, 0.09198063137142316, 0.3224225087765906, 0.20263407975755293, 0.16946126342873455]
siam score:  -0.7529747
printing an ep nov before normalisation:  29.9567538362475
printing an ep nov before normalisation:  2.067991405922669
maxi score, test score, baseline:  -0.9690733333333335 -0.7676666666666666 -0.7676666666666666
probs:  [0.04620797237108542, 0.16753545305668474, 0.09185536414293574, 0.3227881362680896, 0.2023081623796914, 0.16930491178151305]
maxi score, test score, baseline:  -0.9690733333333335 -0.7676666666666666 -0.7676666666666666
probs:  [0.04620797237108542, 0.16753545305668474, 0.09185536414293574, 0.3227881362680896, 0.2023081623796914, 0.16930491178151305]
printing an ep nov before normalisation:  48.029546389466255
siam score:  -0.7539296
printing an ep nov before normalisation:  37.06359218828798
maxi score, test score, baseline:  -0.9690733333333335 -0.7676666666666666 -0.7676666666666666
probs:  [0.046174884519604525, 0.16776089214266793, 0.09191954263483652, 0.32255617184462326, 0.20205438759397013, 0.16953412126429762]
printing an ep nov before normalisation:  28.976084713518194
Printing some Q and Qe and total Qs values:  [[0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]] [[40.122]
 [40.122]
 [40.122]
 [40.122]
 [40.122]
 [40.122]
 [40.122]] [[1.896]
 [1.896]
 [1.896]
 [1.896]
 [1.896]
 [1.896]
 [1.896]]
maxi score, test score, baseline:  -0.9690733333333335 -0.7676666666666666 -0.7676666666666666
probs:  [0.0462054779347389, 0.1672083347751755, 0.09198053211700567, 0.3227704130228589, 0.20218855865117283, 0.1696466834990482]
actor:  1 policy actor:  1  step number:  54 total reward:  0.23333333333333273  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.129]
 [0.628]
 [0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]] [[32.078]
 [30.803]
 [34.774]
 [34.774]
 [34.774]
 [34.774]
 [34.774]] [[1.384]
 [1.785]
 [1.689]
 [1.689]
 [1.689]
 [1.689]
 [1.689]]
maxi score, test score, baseline:  -0.9690733333333335 -0.7676666666666666 -0.7676666666666666
probs:  [0.045921384570872636, 0.1661781066391916, 0.09141417798250329, 0.3207809490569596, 0.20094263344776553, 0.1747627483027075]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9690733333333335 -0.7676666666666666 -0.7676666666666666
probs:  [0.045921384570872636, 0.1661781066391916, 0.09141417798250329, 0.3207809490569596, 0.20094263344776553, 0.1747627483027075]
actions average: 
K:  2  action  0 :  tensor([0.5696, 0.0150, 0.0740, 0.0823, 0.0862, 0.0761, 0.0969],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0269, 0.8283, 0.0268, 0.0350, 0.0129, 0.0111, 0.0590],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.3216, 0.0673, 0.1848, 0.1151, 0.1172, 0.1049, 0.0890],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2993, 0.1219, 0.0837, 0.2392, 0.0869, 0.0719, 0.0971],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.3301, 0.0182, 0.0815, 0.1087, 0.2769, 0.0891, 0.0954],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1538, 0.0435, 0.1117, 0.1230, 0.2724, 0.2053, 0.0904],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1966, 0.1345, 0.1178, 0.0921, 0.1126, 0.1044, 0.2419],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  70 total reward:  0.08666666666666556  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.502]
 [0.312]
 [0.338]
 [0.339]
 [0.316]
 [0.404]] [[37.331]
 [34.22 ]
 [32.086]
 [35.239]
 [39.851]
 [36.062]
 [33.02 ]] [[0.572]
 [0.726]
 [0.509]
 [0.574]
 [0.633]
 [0.563]
 [0.613]]
maxi score, test score, baseline:  -0.9690733333333335 -0.7676666666666666 -0.7676666666666666
probs:  [0.045746926039723926, 0.165545455158334, 0.09106638629504311, 0.31955924166985067, 0.19963281806919897, 0.1784491727678494]
actor:  1 policy actor:  1  step number:  39 total reward:  0.4266666666666663  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9690733333333335 -0.7676666666666666 -0.7676666666666666
probs:  [0.04799908209925067, 0.16192667746337336, 0.0781224156627582, 0.33522384514690723, 0.2002819674673102, 0.1764460121604003]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9690733333333335 -0.7676666666666666 -0.7676666666666666
probs:  [0.04799908209925067, 0.16192667746337336, 0.0781224156627582, 0.33522384514690723, 0.2002819674673102, 0.1764460121604003]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9690733333333335 -0.7676666666666666 -0.7676666666666666
probs:  [0.04799908209925067, 0.16192667746337336, 0.0781224156627582, 0.33522384514690723, 0.2002819674673102, 0.1764460121604003]
maxi score, test score, baseline:  -0.9690733333333335 -0.7676666666666666 -0.7676666666666666
probs:  [0.04799908209925067, 0.16192667746337336, 0.0781224156627582, 0.33522384514690723, 0.2002819674673102, 0.1764460121604003]
actor:  0 policy actor:  1  step number:  42 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  1.333
from probs:  [0.04799908209925067, 0.16192667746337336, 0.0781224156627582, 0.33522384514690723, 0.2002819674673102, 0.1764460121604003]
Printing some Q and Qe and total Qs values:  [[0.211]
 [0.266]
 [0.152]
 [0.211]
 [0.211]
 [0.211]
 [0.211]] [[32.275]
 [24.953]
 [28.445]
 [32.275]
 [32.275]
 [32.275]
 [32.275]] [[0.861]
 [0.675]
 [0.676]
 [0.861]
 [0.861]
 [0.861]
 [0.861]]
maxi score, test score, baseline:  -0.96618 -0.7676666666666666 -0.7676666666666666
actor:  1 policy actor:  1  step number:  79 total reward:  0.053333333333332344  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.96618 -0.7676666666666666 -0.7676666666666666
probs:  [0.04818825809384622, 0.16150647132033236, 0.07815046632734919, 0.3365500555941276, 0.19965660445654485, 0.1759481442077998]
maxi score, test score, baseline:  -0.96618 -0.7676666666666666 -0.7676666666666666
probs:  [0.04818825809384622, 0.16150647132033236, 0.07815046632734919, 0.3365500555941276, 0.19965660445654485, 0.1759481442077998]
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]] [[37.26 ]
 [20.042]
 [20.042]
 [20.042]
 [20.042]
 [20.042]
 [20.042]] [[1.442]
 [0.715]
 [0.715]
 [0.715]
 [0.715]
 [0.715]
 [0.715]]
maxi score, test score, baseline:  -0.96618 -0.7676666666666666 -0.7676666666666666
probs:  [0.04818825809384622, 0.16150647132033236, 0.07815046632734919, 0.3365500555941276, 0.19965660445654485, 0.1759481442077998]
actor:  1 policy actor:  1  step number:  43 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  0
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  57 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  33.0144980373862
maxi score, test score, baseline:  -0.9635133333333333 -0.7676666666666666 -0.7676666666666666
probs:  [0.048881603343121804, 0.1600748698774868, 0.07828195930714735, 0.34141053307626784, 0.1975096105193811, 0.17384142387659515]
maxi score, test score, baseline:  -0.9635133333333333 -0.7676666666666666 -0.7676666666666666
probs:  [0.048881603343121804, 0.1600748698774868, 0.07828195930714735, 0.34141053307626784, 0.1975096105193811, 0.17384142387659515]
printing an ep nov before normalisation:  30.075221018433616
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.9635133333333333 -0.7676666666666666 -0.7676666666666666
probs:  [0.048881603343121804, 0.1600748698774868, 0.07828195930714735, 0.34141053307626784, 0.1975096105193811, 0.17384142387659515]
maxi score, test score, baseline:  -0.9635133333333333 -0.7676666666666666 -0.7676666666666666
probs:  [0.048881603343121804, 0.1600748698774868, 0.07828195930714735, 0.34141053307626784, 0.1975096105193811, 0.17384142387659515]
maxi score, test score, baseline:  -0.9635133333333333 -0.7676666666666666 -0.7676666666666666
probs:  [0.048881603343121804, 0.1600748698774868, 0.07828195930714735, 0.34141053307626784, 0.1975096105193811, 0.17384142387659515]
maxi score, test score, baseline:  -0.9635133333333333 -0.7676666666666666 -0.7676666666666666
Printing some Q and Qe and total Qs values:  [[0.366]
 [0.43 ]
 [0.392]
 [0.388]
 [0.381]
 [0.37 ]
 [0.406]] [[21.886]
 [31.217]
 [20.778]
 [20.804]
 [22.006]
 [21.6  ]
 [26.927]] [[0.366]
 [0.43 ]
 [0.392]
 [0.388]
 [0.381]
 [0.37 ]
 [0.406]]
actor:  0 policy actor:  1  step number:  59 total reward:  0.4  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.529475277516696
siam score:  -0.7479689
maxi score, test score, baseline:  -0.9607133333333333 -0.7676666666666666 -0.7676666666666666
probs:  [0.048901240399325, 0.16013929834167193, 0.0783134395565568, 0.34154800804700114, 0.19758911862636344, 0.1735088950290816]
Printing some Q and Qe and total Qs values:  [[0.175]
 [0.175]
 [0.175]
 [0.175]
 [0.175]
 [0.175]
 [0.175]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.175]
 [0.175]
 [0.175]
 [0.175]
 [0.175]
 [0.175]
 [0.175]]
maxi score, test score, baseline:  -0.9607133333333333 -0.7676666666666666 -0.7676666666666666
maxi score, test score, baseline:  -0.9607133333333333 -0.7676666666666666 -0.7676666666666666
probs:  [0.048936180064662764, 0.15953806045719215, 0.0783694514846041, 0.3417926134206394, 0.19773058517923436, 0.17363310939366722]
maxi score, test score, baseline:  -0.9607133333333333 -0.7676666666666666 -0.7676666666666666
probs:  [0.048936180064662764, 0.15953806045719215, 0.0783694514846041, 0.3417926134206394, 0.19773058517923436, 0.17363310939366722]
printing an ep nov before normalisation:  52.33747471646344
printing an ep nov before normalisation:  27.46349977908618
maxi score, test score, baseline:  -0.9607133333333333 -0.7676666666666666 -0.7676666666666666
probs:  [0.04887565067591828, 0.15967197115531123, 0.07836066631423332, 0.3413682733943153, 0.19793163901993058, 0.17379179944029122]
printing an ep nov before normalisation:  26.895015239715576
maxi score, test score, baseline:  -0.9607133333333333 -0.7676666666666666 -0.7676666666666666
probs:  [0.04887565067591828, 0.15967197115531123, 0.07836066631423332, 0.3413682733943153, 0.19793163901993058, 0.17379179944029122]
Printing some Q and Qe and total Qs values:  [[0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]] [[36.511]
 [36.511]
 [36.511]
 [36.511]
 [36.511]
 [36.511]
 [36.511]] [[1.078]
 [1.078]
 [1.078]
 [1.078]
 [1.078]
 [1.078]
 [1.078]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]] [[23.568]
 [23.568]
 [23.568]
 [23.568]
 [23.568]
 [23.568]
 [23.568]] [[1.518]
 [1.518]
 [1.518]
 [1.518]
 [1.518]
 [1.518]
 [1.518]]
printing an ep nov before normalisation:  0.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  51.71551112187706
printing an ep nov before normalisation:  0.0
printing an ep nov before normalisation:  37.10305719557283
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9607133333333333 -0.7676666666666666 -0.7676666666666666
probs:  [0.048663749326907625, 0.16030149820379047, 0.07837268512205789, 0.3398824592054397, 0.19825105043420593, 0.17452855770759845]
Printing some Q and Qe and total Qs values:  [[0.605]
 [0.312]
 [0.605]
 [0.605]
 [0.188]
 [0.605]
 [0.187]] [[ 0.   ]
 [47.175]
 [ 0.   ]
 [ 0.   ]
 [ 8.577]
 [ 0.   ]
 [ 7.901]] [[0.434]
 [1.16 ]
 [0.434]
 [0.434]
 [0.203]
 [0.434]
 [0.187]]
printing an ep nov before normalisation:  57.59797671303794
Printing some Q and Qe and total Qs values:  [[0.193]
 [0.192]
 [0.192]
 [0.192]
 [0.192]
 [0.192]
 [0.192]] [[55.637]
 [49.297]
 [49.297]
 [49.297]
 [49.297]
 [49.297]
 [49.297]] [[1.443]
 [1.252]
 [1.252]
 [1.252]
 [1.252]
 [1.252]
 [1.252]]
maxi score, test score, baseline:  -0.9607133333333333 -0.7676666666666666 -0.7676666666666666
probs:  [0.048663749326907625, 0.16030149820379047, 0.07837268512205789, 0.3398824592054397, 0.19825105043420593, 0.17452855770759845]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9607133333333333 -0.7676666666666666 -0.7676666666666666
probs:  [0.04869853987566027, 0.15970001289194088, 0.0784287734051915, 0.3401260185492957, 0.1983930770895534, 0.17465357818835828]
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.601]
 [0.333]
 [0.333]
 [0.351]
 [0.286]
 [0.333]] [[52.596]
 [47.253]
 [47.352]
 [47.352]
 [53.744]
 [56.263]
 [47.352]] [[1.7  ]
 [1.825]
 [1.561]
 [1.561]
 [1.811]
 [1.839]
 [1.561]]
maxi score, test score, baseline:  -0.9607133333333333 -0.7676666666666666 -0.7676666666666666
probs:  [0.04869853987566027, 0.15970001289194088, 0.0784287734051915, 0.3401260185492957, 0.1983930770895534, 0.17465357818835828]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9607133333333333 -0.7676666666666666 -0.7676666666666666
printing an ep nov before normalisation:  0.0
actor:  1 policy actor:  1  step number:  54 total reward:  0.3399999999999995  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  0.11389149357697192
printing an ep nov before normalisation:  20.98916475819893
printing an ep nov before normalisation:  24.829974598115648
printing an ep nov before normalisation:  2.6743353487290733e-06
actor:  1 policy actor:  1  step number:  75 total reward:  0.05333333333333279  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9607133333333333 -0.7676666666666666 -0.7676666666666666
probs:  [0.04847992047886533, 0.1573875933564812, 0.0776493581433044, 0.33859835906039143, 0.20582567662880585, 0.17205909233215183]
maxi score, test score, baseline:  -0.9607133333333333 -0.7676666666666666 -0.7676666666666666
probs:  [0.04853951256696951, 0.15758142493723812, 0.07774490445216804, 0.3390155508563525, 0.20484759919886222, 0.17227100798840972]
printing an ep nov before normalisation:  28.754265308380127
maxi score, test score, baseline:  -0.9607133333333333 -0.7676666666666666 -0.7676666666666666
probs:  [0.04847972711477495, 0.15770882453367996, 0.07773525396717683, 0.33859643264787487, 0.20505613751467988, 0.1724236242218135]
maxi score, test score, baseline:  -0.9607133333333333 -0.7676666666666666 -0.7676666666666666
probs:  [0.04847972711477495, 0.15770882453367996, 0.07773525396717683, 0.33859643264787487, 0.20505613751467988, 0.1724236242218135]
printing an ep nov before normalisation:  58.347840088848564
printing an ep nov before normalisation:  42.216527669
printing an ep nov before normalisation:  75.2215676011979
printing an ep nov before normalisation:  44.74417857392712
actor:  1 policy actor:  1  step number:  60 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  1.667
using another actor
from probs:  [0.0484200466118527, 0.1578360004888437, 0.077725620422952, 0.33817805017274677, 0.20526430975574705, 0.17257597254785775]
printing an ep nov before normalisation:  39.38761686952879
maxi score, test score, baseline:  -0.9607133333333333 -0.7676666666666666 -0.7676666666666666
probs:  [0.04791203294934751, 0.15649538413680578, 0.07699460546393704, 0.334620997584484, 0.2128537877548705, 0.17112319211055516]
actor:  1 policy actor:  1  step number:  65 total reward:  0.1333333333333323  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  30.742096469974083
maxi score, test score, baseline:  -0.9607133333333333 -0.7676666666666666 -0.7676666666666666
probs:  [0.047530758290863236, 0.15524761482439442, 0.08435486861505345, 0.33195178667820047, 0.21115627863516026, 0.16975869295632817]
printing an ep nov before normalisation:  75.45312510043694
maxi score, test score, baseline:  -0.9607133333333333 -0.7676666666666666 -0.7676666666666666
probs:  [0.047530758290863236, 0.15524761482439442, 0.08435486861505345, 0.33195178667820047, 0.21115627863516026, 0.16975869295632817]
actions average: 
K:  0  action  0 :  tensor([0.5604, 0.0193, 0.0833, 0.0734, 0.1161, 0.0752, 0.0723],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0127, 0.8875, 0.0236, 0.0164, 0.0067, 0.0060, 0.0472],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.3052, 0.0104, 0.1260, 0.1464, 0.1295, 0.1340, 0.1486],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2507, 0.0057, 0.0865, 0.3713, 0.0915, 0.0957, 0.0985],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.3528, 0.0088, 0.0701, 0.0867, 0.3453, 0.0729, 0.0634],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.2306, 0.0022, 0.1478, 0.1026, 0.0781, 0.2928, 0.1459],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.3888, 0.0067, 0.1011, 0.1790, 0.0767, 0.0925, 0.1552],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.246]
 [0.429]
 [0.413]
 [0.257]
 [0.254]
 [0.253]
 [0.278]] [[24.647]
 [31.022]
 [27.443]
 [19.899]
 [19.615]
 [19.278]
 [19.361]] [[0.565]
 [0.83 ]
 [0.768]
 [0.515]
 [0.508]
 [0.502]
 [0.529]]
actor:  0 policy actor:  1  step number:  42 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.047530758290863236, 0.15524761482439442, 0.08435486861505345, 0.33195178667820047, 0.21115627863516026, 0.16975869295632817]
printing an ep nov before normalisation:  31.794384734776134
printing an ep nov before normalisation:  35.686998882486996
actor:  1 policy actor:  1  step number:  45 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.0467922074952746, 0.16840021058809201, 0.08304252262183019, 0.3267813726615269, 0.207868106492105, 0.16711558014117137]
actor:  1 policy actor:  1  step number:  62 total reward:  0.15333333333333243  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.046542413624285926, 0.16749937519515384, 0.08259865879055175, 0.32503262668555155, 0.20675597547417512, 0.17157095023028182]
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.046542413624285926, 0.16749937519515384, 0.08259865879055175, 0.32503262668555155, 0.20675597547417512, 0.17157095023028182]
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.046542413624285926, 0.16749937519515384, 0.08259865879055175, 0.32503262668555155, 0.20675597547417512, 0.17157095023028182]
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
printing an ep nov before normalisation:  30.47697943625087
Printing some Q and Qe and total Qs values:  [[0.115]
 [0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]] [[51.016]
 [50.038]
 [50.038]
 [50.038]
 [50.038]
 [50.038]
 [50.038]] [[2.115]
 [2.064]
 [2.064]
 [2.064]
 [2.064]
 [2.064]
 [2.064]]
printing an ep nov before normalisation:  30.127973507471292
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.046520889666188815, 0.16753025628056195, 0.08272682913773886, 0.32488298757758327, 0.20674187171348485, 0.1715971656244423]
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.046520889666188815, 0.16753025628056195, 0.08272682913773886, 0.32488298757758327, 0.20674187171348485, 0.1715971656244423]
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.046520889666188815, 0.16753025628056195, 0.08272682913773886, 0.32488298757758327, 0.20674187171348485, 0.1715971656244423]
printing an ep nov before normalisation:  33.07183369750672
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.046520889666188815, 0.16753025628056195, 0.08272682913773886, 0.32488298757758327, 0.20674187171348485, 0.1715971656244423]
printing an ep nov before normalisation:  0.00445640120005919
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.04653202488095053, 0.1671808702455446, 0.08285289633078097, 0.3249603953770433, 0.20726161172341254, 0.17121220144226798]
printing an ep nov before normalisation:  23.835885524749756
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.04653202488095053, 0.1671808702455446, 0.08285289633078097, 0.3249603953770433, 0.20726161172341254, 0.17121220144226798]
printing an ep nov before normalisation:  71.4513340372192
printing an ep nov before normalisation:  52.20564367742956
printing an ep nov before normalisation:  77.17389211068328
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.04653202488095053, 0.1671808702455446, 0.08285289633078097, 0.3249603953770433, 0.20726161172341254, 0.17121220144226798]
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
actor:  1 policy actor:  1  step number:  52 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  57.79982629425068
actor:  1 policy actor:  1  step number:  47 total reward:  0.3999999999999999  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.285]
 [0.356]
 [0.285]
 [0.285]
 [0.285]
 [0.285]
 [0.285]] [[ 0.   ]
 [38.334]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.131]
 [ 1.257]
 [-0.131]
 [-0.131]
 [-0.131]
 [-0.131]
 [-0.131]]
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.04765488099803978, 0.1647301393610569, 0.08289993780822881, 0.33283132072162724, 0.20362369984000162, 0.16826002127104572]
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.04765488099803978, 0.1647301393610569, 0.08289993780822881, 0.33283132072162724, 0.20362369984000162, 0.16826002127104572]
Printing some Q and Qe and total Qs values:  [[0.248]
 [0.251]
 [0.117]
 [0.22 ]
 [0.281]
 [0.186]
 [0.21 ]] [[28.687]
 [28.759]
 [27.789]
 [23.965]
 [51.972]
 [26.223]
 [24.115]] [[0.602]
 [0.607]
 [0.45 ]
 [0.46 ]
 [1.198]
 [0.481]
 [0.454]]
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.04768339402481218, 0.16482889762716374, 0.08294959789447054, 0.33303093981761567, 0.20314627316709702, 0.1683608974688408]
Printing some Q and Qe and total Qs values:  [[0.213]
 [0.238]
 [0.222]
 [0.221]
 [0.221]
 [0.22 ]
 [0.219]] [[44.301]
 [41.044]
 [31.292]
 [31.185]
 [30.994]
 [30.488]
 [30.038]] [[1.439]
 [1.31 ]
 [0.835]
 [0.828]
 [0.82 ]
 [0.795]
 [0.773]]
printing an ep nov before normalisation:  49.36554413992109
printing an ep nov before normalisation:  32.77652654452843
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.04770150507824143, 0.16489162741899965, 0.0829811412477036, 0.33315773491873935, 0.20322359739835288, 0.16804439393796308]
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.04770150507824143, 0.16489162741899965, 0.0829811412477036, 0.33315773491873935, 0.20322359739835288, 0.16804439393796308]
printing an ep nov before normalisation:  34.081692695617676
actor:  1 policy actor:  1  step number:  64 total reward:  0.13999999999999957  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.01]
 [0.01]
 [0.01]
 [0.01]
 [0.01]
 [0.01]
 [0.01]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.01]
 [0.01]
 [0.01]
 [0.01]
 [0.01]
 [0.01]
 [0.01]]
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.04740675378020718, 0.16420771252671812, 0.08256923374451135, 0.3310936646388406, 0.20241239014632476, 0.1723102451633979]
printing an ep nov before normalisation:  33.734775122949884
actor:  1 policy actor:  1  step number:  71 total reward:  0.25333333333333274  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  19.020224610109157
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.04702668482738604, 0.16288858907828593, 0.08190646604296699, 0.3284328141595086, 0.208819466790269, 0.17092597910158341]
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.04702668482738604, 0.16288858907828593, 0.08190646604296699, 0.3284328141595086, 0.208819466790269, 0.17092597910158341]
printing an ep nov before normalisation:  40.79513663239874
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.04702668482738604, 0.16288858907828593, 0.08190646604296699, 0.3284328141595086, 0.208819466790269, 0.17092597910158341]
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.04702668482738604, 0.16288858907828593, 0.08190646604296699, 0.3284328141595086, 0.208819466790269, 0.17092597910158341]
printing an ep nov before normalisation:  12.742652417160143
printing an ep nov before normalisation:  12.714610802219365
actor:  1 policy actor:  1  step number:  44 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]] [[18.268]
 [18.268]
 [18.268]
 [18.268]
 [18.268]
 [18.268]
 [18.268]] [[0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]]
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.04649343415863922, 0.16103781053977403, 0.09309911484879473, 0.3246995434279557, 0.20644638360071957, 0.1682237134241167]
Printing some Q and Qe and total Qs values:  [[0.213]
 [0.232]
 [0.21 ]
 [0.22 ]
 [0.221]
 [0.224]
 [0.206]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.213]
 [0.232]
 [0.21 ]
 [0.22 ]
 [0.221]
 [0.224]
 [0.206]]
Printing some Q and Qe and total Qs values:  [[0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]] [[36.267]
 [36.267]
 [36.267]
 [36.267]
 [36.267]
 [36.267]
 [36.267]] [[2.264]
 [2.264]
 [2.264]
 [2.264]
 [2.264]
 [2.264]
 [2.264]]
actions average: 
K:  3  action  0 :  tensor([0.5593, 0.0515, 0.0764, 0.0652, 0.1192, 0.0732, 0.0552],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0293, 0.8431, 0.0186, 0.0271, 0.0124, 0.0133, 0.0563],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.2220, 0.1034, 0.2573, 0.1274, 0.1010, 0.1027, 0.0863],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2599, 0.0274, 0.1025, 0.3021, 0.0938, 0.0910, 0.1232],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1751, 0.0653, 0.1199, 0.2456, 0.1945, 0.0946, 0.1049],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1600, 0.1123, 0.1496, 0.1736, 0.1343, 0.1400, 0.1301],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.3061, 0.1623, 0.1105, 0.1024, 0.1007, 0.1023, 0.1156],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  58.026090443548505
printing an ep nov before normalisation:  65.1536147841697
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.783]
 [0.521]
 [0.517]
 [0.432]
 [0.405]
 [0.463]] [[26.107]
 [33.427]
 [25.368]
 [25.482]
 [31.483]
 [26.145]
 [23.511]] [[1.188]
 [1.955]
 [1.232]
 [1.234]
 [1.492]
 [1.16 ]
 [1.067]]
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.568]
 [0.569]
 [0.577]
 [0.553]
 [0.544]
 [0.545]] [[38.35 ]
 [37.815]
 [36.717]
 [36.8  ]
 [38.701]
 [37.759]
 [37.154]] [[1.165]
 [1.162]
 [1.146]
 [1.155]
 [1.162]
 [1.137]
 [1.129]]
printing an ep nov before normalisation:  76.29758091915247
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.0464667458477729, 0.160573646633601, 0.09318007521355226, 0.32451218678359856, 0.20678915074924334, 0.1684781947722319]
printing an ep nov before normalisation:  64.85426197113904
actor:  1 policy actor:  1  step number:  46 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.245]
 [0.179]
 [0.364]
 [0.281]
 [0.245]
 [0.268]
 [0.245]] [[52.996]
 [57.861]
 [54.133]
 [57.635]
 [52.996]
 [51.959]
 [52.996]] [[1.296]
 [1.357]
 [1.445]
 [1.453]
 [1.296]
 [1.292]
 [1.296]]
actor:  1 policy actor:  1  step number:  50 total reward:  0.32666666666666655  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.04525084691449285, 0.17364661839637752, 0.09073831024518166, 0.3159997310241913, 0.21030406079970543, 0.16406043262005118]
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.04525084691449285, 0.17364661839637752, 0.09073831024518166, 0.3159997310241913, 0.21030406079970543, 0.16406043262005118]
printing an ep nov before normalisation:  25.256480111016167
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.04525084691449285, 0.17364661839637752, 0.09073831024518166, 0.3159997310241913, 0.21030406079970543, 0.16406043262005118]
siam score:  -0.7675479
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  11.799691784192174
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]] [[11.809]
 [11.809]
 [11.809]
 [11.809]
 [11.809]
 [11.809]
 [11.809]] [[0.1]
 [0.1]
 [0.1]
 [0.1]
 [0.1]
 [0.1]
 [0.1]]
printing an ep nov before normalisation:  17.66170386536343
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.045232416881383304, 0.17366853595152396, 0.09084758423009745, 0.315871658066167, 0.21028735420047892, 0.16409245067034947]
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.045232416881383304, 0.17366853595152396, 0.09084758423009745, 0.315871658066167, 0.21028735420047892, 0.16409245067034947]
printing an ep nov before normalisation:  52.60747630633502
printing an ep nov before normalisation:  51.73752746593653
printing an ep nov before normalisation:  20.691260504129968
actor:  1 policy actor:  1  step number:  61 total reward:  0.2933333333333328  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.045729672049099064, 0.1724910355750838, 0.09075003588795987, 0.3193570787751199, 0.20863235910271827, 0.16303981861001904]
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
printing an ep nov before normalisation:  46.78527581778698
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
printing an ep nov before normalisation:  62.08221428767576
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.04578019709352219, 0.1719383119279525, 0.0908504476257004, 0.31971080808886965, 0.2088633952691464, 0.16285683999480888]
printing an ep nov before normalisation:  16.178912989133277
actor:  1 policy actor:  1  step number:  54 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.047301624373213697, 0.1700379072286965, 0.08210415771057437, 0.33028617341371264, 0.2100804099804943, 0.16018972729330846]
printing an ep nov before normalisation:  11.428786516189575
actor:  1 policy actor:  1  step number:  53 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.047830676325667454, 0.16876649285839157, 0.08212267775051789, 0.3339957768103199, 0.2082215965460636, 0.1590627797090395]
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.047849135890331344, 0.1688317562363708, 0.08215440878825662, 0.33412498617867664, 0.20830212958823557, 0.15873758331812898]
Printing some Q and Qe and total Qs values:  [[0.138]
 [0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]] [[52.224]
 [49.318]
 [49.318]
 [49.318]
 [49.318]
 [49.318]
 [49.318]] [[1.471]
 [1.373]
 [1.373]
 [1.373]
 [1.373]
 [1.373]
 [1.373]]
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.047849135890331344, 0.1688317562363708, 0.08215440878825662, 0.33412498617867664, 0.20830212958823557, 0.15873758331812898]
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]] [[52.153]
 [52.153]
 [52.153]
 [52.153]
 [52.153]
 [52.153]
 [52.153]] [[0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]]
printing an ep nov before normalisation:  21.169441120584533
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.047849135890331344, 0.1688317562363708, 0.08215440878825662, 0.33412498617867664, 0.20830212958823557, 0.15873758331812898]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.047849135890331344, 0.1688317562363708, 0.08215440878825662, 0.33412498617867664, 0.20830212958823557, 0.15873758331812898]
Printing some Q and Qe and total Qs values:  [[0.128]
 [0.134]
 [0.065]
 [0.065]
 [0.065]
 [0.065]
 [0.065]] [[48.212]
 [43.915]
 [43.667]
 [43.667]
 [43.667]
 [43.667]
 [43.667]] [[1.075]
 [0.976]
 [0.901]
 [0.901]
 [0.901]
 [0.901]
 [0.901]]
Printing some Q and Qe and total Qs values:  [[0.816]
 [0.588]
 [0.588]
 [0.588]
 [0.588]
 [0.588]
 [0.588]] [[26.694]
 [43.371]
 [43.371]
 [43.371]
 [43.371]
 [43.371]
 [43.371]] [[0.816]
 [0.588]
 [0.588]
 [0.588]
 [0.588]
 [0.588]
 [0.588]]
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.047849135890331344, 0.1688317562363708, 0.08215440878825662, 0.33412498617867664, 0.20830212958823557, 0.15873758331812898]
Printing some Q and Qe and total Qs values:  [[0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]] [[26.191]
 [26.191]
 [26.191]
 [26.191]
 [26.191]
 [26.191]
 [26.191]] [[0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]]
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.047849135890331344, 0.1688317562363708, 0.08215440878825662, 0.33412498617867664, 0.20830212958823557, 0.15873758331812898]
Printing some Q and Qe and total Qs values:  [[-0.022]
 [-0.033]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]] [[37.404]
 [32.31 ]
 [30.377]
 [30.377]
 [30.377]
 [30.377]
 [30.377]] [[0.827]
 [0.674]
 [0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.603]]
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.047849135890331344, 0.1688317562363708, 0.08215440878825662, 0.33412498617867664, 0.20830212958823557, 0.15873758331812898]
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.047849135890331344, 0.1688317562363708, 0.08215440878825662, 0.33412498617867664, 0.20830212958823557, 0.15873758331812898]
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.047849135890331344, 0.1688317562363708, 0.08215440878825662, 0.33412498617867664, 0.20830212958823557, 0.15873758331812898]
actor:  1 policy actor:  1  step number:  66 total reward:  0.17999999999999972  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.04822421150854839, 0.1679292191614761, 0.08216721045365556, 0.3367549415915731, 0.20698277355329553, 0.1579416437314514]
Printing some Q and Qe and total Qs values:  [[0.112]
 [0.151]
 [0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.132]] [[64.907]
 [58.883]
 [58.47 ]
 [58.47 ]
 [58.47 ]
 [58.47 ]
 [58.47 ]] [[1.783]
 [1.667]
 [1.637]
 [1.637]
 [1.637]
 [1.637]
 [1.637]]
printing an ep nov before normalisation:  22.64042042538735
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.04816397684730838, 0.16807416064080025, 0.08216515459094104, 0.3363325881628827, 0.20719465339821136, 0.15806946635985608]
Printing some Q and Qe and total Qs values:  [[0.103]
 [0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]] [[52.819]
 [53.278]
 [53.278]
 [53.278]
 [53.278]
 [53.278]
 [53.278]] [[1.727]
 [1.742]
 [1.742]
 [1.742]
 [1.742]
 [1.742]
 [1.742]]
printing an ep nov before normalisation:  37.78096810950707
line 256 mcts: sample exp_bonus 41.52356084824017
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.048103839590353345, 0.16821886773811148, 0.08216310205272168, 0.3359109177136078, 0.20740619061640098, 0.15819708228880464]
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
printing an ep nov before normalisation:  76.45704381831848
printing an ep nov before normalisation:  85.4689456091539
maxi score, test score, baseline:  -0.95782 -0.7676666666666666 -0.7676666666666666
probs:  [0.04814047038567422, 0.1683472194590303, 0.08222574080996374, 0.33616732060577775, 0.2075644661327143, 0.15755478260683972]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.95782 -0.7676666666666666 -0.7676666666666666
probs:  [0.04814047038567422, 0.1683472194590303, 0.08222574080996374, 0.33616732060577775, 0.2075644661327143, 0.15755478260683972]
maxi score, test score, baseline:  -0.95782 -0.7676666666666666 -0.7676666666666666
probs:  [0.04814047038567422, 0.1683472194590303, 0.08222574080996374, 0.33616732060577775, 0.2075644661327143, 0.15755478260683972]
actions average: 
K:  3  action  0 :  tensor([0.5022, 0.0562, 0.0802, 0.0872, 0.1120, 0.0691, 0.0931],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0477, 0.8013, 0.0235, 0.0407, 0.0173, 0.0173, 0.0522],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.2114, 0.0213, 0.2557, 0.0999, 0.1167, 0.1254, 0.1695],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2426, 0.1513, 0.1121, 0.1654, 0.1086, 0.0951, 0.1248],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2786, 0.0954, 0.0806, 0.0958, 0.2768, 0.0823, 0.0905],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1236, 0.1462, 0.0985, 0.1319, 0.0867, 0.3276, 0.0854],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2464, 0.1969, 0.0966, 0.0892, 0.1098, 0.0845, 0.1766],
       grad_fn=<DivBackward0>)
UNIT TEST: sample policy line 217 mcts : [0.041 0.796 0.02  0.02  0.082 0.02  0.02 ]
using explorer policy with actor:  1
from probs:  [0.04814047038567422, 0.1683472194590303, 0.08222574080996374, 0.33616732060577775, 0.2075644661327143, 0.15755478260683972]
printing an ep nov before normalisation:  24.092446177047215
maxi score, test score, baseline:  -0.95782 -0.7676666666666666 -0.7676666666666666
probs:  [0.048080446788635336, 0.16849202195618854, 0.08222379675391968, 0.33574644635901113, 0.20777609279343454, 0.15768119534881078]
actor:  1 policy actor:  1  step number:  42 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.95782 -0.7676666666666666 -0.7676666666666666
maxi score, test score, baseline:  -0.95782 -0.7676666666666666 -0.7676666666666666
probs:  [0.04874595427636703, 0.166886534267095, 0.08224535134107748, 0.3404128605100177, 0.20542969680484152, 0.15627960280060121]
Starting evaluation
using explorer policy with actor:  0
printing an ep nov before normalisation:  7.052135467529298
printing an ep nov before normalisation:  53.25461470541344
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.04874595427636703, 0.166886534267095, 0.08224535134107748, 0.3404128605100177, 0.20542969680484152, 0.15627960280060121]
printing an ep nov before normalisation:  57.621154893044604
Printing some Q and Qe and total Qs values:  [[-0.105]
 [-0.107]
 [-0.107]
 [-0.107]
 [-0.107]
 [-0.107]
 [-0.107]] [[48.88 ]
 [37.451]
 [37.451]
 [37.451]
 [37.451]
 [37.451]
 [37.451]] [[1.139]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]]
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.478]
 [0.447]
 [0.447]
 [0.447]
 [0.447]
 [0.447]] [[53.156]
 [50.376]
 [37.693]
 [37.693]
 [37.693]
 [37.693]
 [37.693]] [[0.455]
 [0.478]
 [0.447]
 [0.447]
 [0.447]
 [0.447]
 [0.447]]
actions average: 
K:  3  action  0 :  tensor([0.5332, 0.0411, 0.1044, 0.0858, 0.0807, 0.0809, 0.0739],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0195, 0.9213, 0.0165, 0.0093, 0.0068, 0.0123, 0.0143],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.3998, 0.0614, 0.1966, 0.0794, 0.0944, 0.0935, 0.0750],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2449, 0.1114, 0.0869, 0.2577, 0.1091, 0.1187, 0.0713],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2319, 0.0110, 0.0537, 0.0279, 0.5343, 0.1134, 0.0278],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1523, 0.1999, 0.1578, 0.0624, 0.0757, 0.2905, 0.0614],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1438, 0.1343, 0.1338, 0.1520, 0.1416, 0.1719, 0.1225],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  41.246132672515756
maxi score, test score, baseline:  -0.9578200000000001 -0.7676666666666666 -0.7676666666666666
probs:  [0.04877686630302655, 0.1669925699279182, 0.08229756507837692, 0.34062923868186856, 0.204924866319073, 0.15637889368973673]
printing an ep nov before normalisation:  29.817148785754437
actor:  0 policy actor:  1  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  30.019284035313788
Printing some Q and Qe and total Qs values:  [[0.932]
 [0.909]
 [0.929]
 [0.823]
 [0.741]
 [0.698]
 [0.706]] [[39.564]
 [38.055]
 [34.566]
 [32.269]
 [33.382]
 [33.85 ]
 [30.226]] [[0.932]
 [0.909]
 [0.929]
 [0.823]
 [0.741]
 [0.698]
 [0.706]]
printing an ep nov before normalisation:  49.33702098054194
Printing some Q and Qe and total Qs values:  [[0.35 ]
 [0.673]
 [0.323]
 [0.323]
 [0.323]
 [0.323]
 [0.323]] [[45.394]
 [51.453]
 [28.963]
 [28.963]
 [28.963]
 [28.963]
 [28.963]] [[1.1  ]
 [1.556]
 [0.714]
 [0.714]
 [0.714]
 [0.714]
 [0.714]]
actor:  0 policy actor:  1  step number:  29 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  48.23966912627331
actor:  0 policy actor:  1  step number:  30 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  0.0
actor:  0 policy actor:  1  step number:  31 total reward:  0.5866666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  31 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  33 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  33 total reward:  0.6266666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  33 total reward:  0.6133333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  33 total reward:  0.5733333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  34 total reward:  0.6066666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  34 total reward:  0.6066666666666667  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[0.575]
 [0.717]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]] [[26.645]
 [39.589]
 [20.322]
 [20.322]
 [20.322]
 [20.322]
 [20.322]] [[0.575]
 [0.717]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]]
Printing some Q and Qe and total Qs values:  [[0.579]
 [0.74 ]
 [0.611]
 [0.578]
 [0.602]
 [0.602]
 [0.576]] [[28.319]
 [27.97 ]
 [25.885]
 [26.545]
 [26.841]
 [26.62 ]
 [24.282]] [[0.579]
 [0.74 ]
 [0.611]
 [0.578]
 [0.602]
 [0.602]
 [0.576]]
printing an ep nov before normalisation:  25.221011899336233
printing an ep nov before normalisation:  0.0
actor:  0 policy actor:  1  step number:  39 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  45 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.9163133333333333 -0.7676666666666666 -0.7676666666666666
printing an ep nov before normalisation:  10.39372444152832
maxi score, test score, baseline:  -0.9163133333333333 -0.7676666666666666 -0.7676666666666666
probs:  [0.0487484830291247, 0.16724139091331167, 0.08234778456358538, 0.34042984982054814, 0.20462966503068303, 0.15660282664274713]
actor:  0 policy actor:  1  step number:  48 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  24.79902218848469
maxi score, test score, baseline:  -0.9136333333333334 -0.7676666666666666 -0.7676666666666666
probs:  [0.0487484830291247, 0.16724139091331167, 0.08234778456358538, 0.34042984982054814, 0.20462966503068303, 0.15660282664274713]
maxi score, test score, baseline:  -0.9136333333333334 -0.7676666666666666 -0.7676666666666666
probs:  [0.0487484830291247, 0.16724139091331167, 0.08234778456358538, 0.34042984982054814, 0.20462966503068303, 0.15660282664274713]
actor:  0 policy actor:  1  step number:  61 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9112066666666667 -0.7676666666666666 -0.7676666666666666
probs:  [0.0487484830291247, 0.16724139091331167, 0.08234778456358538, 0.34042984982054814, 0.20462966503068303, 0.15660282664274713]
printing an ep nov before normalisation:  36.33807178070954
printing an ep nov before normalisation:  28.136612431165048
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.39 ]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]] [[21.927]
 [19.569]
 [12.409]
 [12.409]
 [12.409]
 [12.409]
 [12.409]] [[0.455]
 [0.39 ]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]]
from probs:  [0.04863047130753369, 0.16752731596627296, 0.08234431128763961, 0.33960237115088715, 0.20504304495940623, 0.15685248532826035]
printing an ep nov before normalisation:  19.87919807434082
printing an ep nov before normalisation:  12.476110458374023
maxi score, test score, baseline:  -0.9112066666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.048661129287325626, 0.16763313568142274, 0.08239628178187006, 0.3398169696969353, 0.2045409267015361, 0.15695155685091014]
maxi score, test score, baseline:  -0.9112066666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.04860227705425415, 0.16777603139599592, 0.08239463626477768, 0.3394043069466289, 0.20474640916500997, 0.15707633917333347]
printing an ep nov before normalisation:  27.27106057991293
maxi score, test score, baseline:  -0.9112066666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.04860227705425415, 0.16777603139599592, 0.08239463626477768, 0.3394043069466289, 0.20474640916500997, 0.15707633917333347]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.29385117043112
printing an ep nov before normalisation:  13.311724177559842
maxi score, test score, baseline:  -0.9112066666666666 -0.7676666666666666 -0.7676666666666666
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.387]] [[28.565]
 [22.897]
 [22.897]
 [22.897]
 [22.897]
 [22.897]
 [22.897]] [[0.43 ]
 [0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.387]]
maxi score, test score, baseline:  -0.9112066666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.04863865233347099, 0.1679018447509015, 0.08245637219154202, 0.3396589246912578, 0.20489996822167186, 0.1564442378111558]
using explorer policy with actor:  0
printing an ep nov before normalisation:  23.321826457977295
maxi score, test score, baseline:  -0.9112066666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.04863865233347099, 0.1679018447509015, 0.08245637219154202, 0.3396589246912578, 0.20489996822167186, 0.1564442378111558]
Printing some Q and Qe and total Qs values:  [[0.424]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]] [[48.546]
 [49.703]
 [49.703]
 [49.703]
 [49.703]
 [49.703]
 [49.703]] [[0.424]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]]
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]] [[42.894]
 [42.279]
 [42.279]
 [42.279]
 [42.279]
 [42.279]
 [42.279]] [[0.46 ]
 [0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]]
printing an ep nov before normalisation:  30.30039293792548
maxi score, test score, baseline:  -0.9112066666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.0486763772950697, 0.16725520077846787, 0.08252039879193074, 0.3399229898708325, 0.20505922499276552, 0.15656580827093358]
maxi score, test score, baseline:  -0.9112066666666666 -0.7676666666666666 -0.7676666666666666
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.392]] [[45.367]
 [44.36 ]
 [44.36 ]
 [44.36 ]
 [44.36 ]
 [44.36 ]
 [44.36 ]] [[0.409]
 [0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.392]]
maxi score, test score, baseline:  -0.9112066666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.0486763772950697, 0.16725520077846787, 0.08252039879193074, 0.3399229898708325, 0.20505922499276552, 0.15656580827093358]
actor:  1 policy actor:  1  step number:  49 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.097]
 [0.102]
 [0.072]
 [0.072]
 [0.072]
 [0.072]
 [0.072]] [[68.983]
 [69.381]
 [57.792]
 [57.792]
 [57.792]
 [57.792]
 [57.792]] [[1.182]
 [1.196]
 [0.896]
 [0.896]
 [0.896]
 [0.896]
 [0.896]]
maxi score, test score, baseline:  -0.9112066666666666 -0.7676666666666666 -0.7676666666666666
printing an ep nov before normalisation:  48.39586768035975
maxi score, test score, baseline:  -0.9112066666666666 -0.7676666666666666 -0.7676666666666666
probs:  [0.04919642336714769, 0.16599761754293604, 0.08253308509492974, 0.3435694858852311, 0.2032349171143776, 0.15546847099537792]
actions average: 
K:  4  action  0 :  tensor([0.5798, 0.0914, 0.0574, 0.0672, 0.0812, 0.0487, 0.0743],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0270, 0.8889, 0.0098, 0.0211, 0.0143, 0.0106, 0.0283],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1837, 0.1108, 0.3951, 0.0479, 0.0338, 0.1072, 0.1216],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1261, 0.1071, 0.0989, 0.2216, 0.0981, 0.2153, 0.1328],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.3190, 0.0752, 0.1008, 0.1414, 0.1428, 0.1153, 0.1055],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.3553, 0.0993, 0.1766, 0.0780, 0.0659, 0.1575, 0.0674],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1210, 0.2071, 0.0830, 0.1269, 0.0545, 0.0790, 0.3284],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  36.035051345825195
Printing some Q and Qe and total Qs values:  [[-0.149]
 [-0.146]
 [-0.149]
 [-0.149]
 [-0.151]
 [-0.161]
 [-0.149]] [[36.83 ]
 [47.953]
 [36.83 ]
 [36.83 ]
 [44.902]
 [43.679]
 [36.83 ]] [[0.532]
 [0.936]
 [0.532]
 [0.532]
 [0.821]
 [0.767]
 [0.532]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[49.655]
 [45.154]
 [45.154]
 [45.154]
 [45.154]
 [45.154]
 [45.154]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
printing an ep nov before normalisation:  18.46283244701543
printing an ep nov before normalisation:  21.2395419445067
maxi score, test score, baseline:  -0.9112066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04968772626092204, 0.2790696753110166, 0.09230692241236367, 0.34690808439711834, 0.16620062685582795, 0.0658269647627513]
printing an ep nov before normalisation:  39.667441355090816
maxi score, test score, baseline:  -0.9112066666666667 0.10466666666666669 0.10466666666666669
printing an ep nov before normalisation:  45.509611578064906
Printing some Q and Qe and total Qs values:  [[0.288]
 [0.449]
 [0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]] [[24.891]
 [32.096]
 [24.891]
 [24.891]
 [24.891]
 [24.891]
 [24.891]] [[0.868]
 [1.356]
 [0.868]
 [0.868]
 [0.868]
 [0.868]
 [0.868]]
maxi score, test score, baseline:  -0.9112066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.049706046431515556, 0.2791727973962559, 0.09197159689180565, 0.34703628616714033, 0.16626202154511088, 0.06585125156817179]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9112066666666667 0.10466666666666669 0.10466666666666669
maxi score, test score, baseline:  -0.9112066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.05086268320044552, 0.27264587661767814, 0.09135847848494615, 0.35514983506541264, 0.16351585087668452, 0.06646727575483322]
printing an ep nov before normalisation:  56.97396348579913
maxi score, test score, baseline:  -0.9112066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.05088931457460932, 0.27278893721503167, 0.09140636888200872, 0.35533620765428425, 0.16307707262321366, 0.06650209905085235]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9112066666666667 0.10466666666666669 0.10466666666666669
maxi score, test score, baseline:  -0.9112066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.05089651360925829, 0.27219343493947185, 0.09153809189477943, 0.35538561121079837, 0.16342906636635215, 0.06655728197933992]
Printing some Q and Qe and total Qs values:  [[-0.133]
 [-0.112]
 [-0.138]
 [-0.138]
 [-0.138]
 [-0.138]
 [-0.138]] [[52.944]
 [46.318]
 [44.555]
 [44.555]
 [44.555]
 [44.555]
 [44.555]] [[1.178]
 [0.977]
 [0.893]
 [0.893]
 [0.893]
 [0.893]
 [0.893]]
siam score:  -0.76601744
maxi score, test score, baseline:  -0.9112066666666667 0.10466666666666669 0.10466666666666669
Printing some Q and Qe and total Qs values:  [[-0.143]
 [-0.135]
 [-0.146]
 [-0.146]
 [-0.146]
 [-0.146]
 [-0.146]] [[62.235]
 [59.184]
 [53.618]
 [53.618]
 [53.618]
 [53.618]
 [53.618]] [[0.994]
 [0.931]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]]
printing an ep nov before normalisation:  16.68272540660312
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9112066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.05089651360925829, 0.27219343493947185, 0.09153809189477943, 0.35538561121079837, 0.16342906636635215, 0.06655728197933992]
printing an ep nov before normalisation:  41.542347664599184
actor:  1 policy actor:  1  step number:  58 total reward:  0.24666666666666637  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  54.72056650509358
maxi score, test score, baseline:  -0.9112066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.05062667321187403, 0.27074725524476245, 0.09105221466617677, 0.35349721015309077, 0.1678724524695288, 0.06620419425456722]
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.036971332233804
Printing some Q and Qe and total Qs values:  [[-0.005]
 [-0.007]
 [ 0.027]
 [-0.001]
 [ 0.01 ]
 [ 0.033]
 [ 0.109]] [[50.256]
 [32.155]
 [23.665]
 [23.62 ]
 [23.588]
 [23.287]
 [22.751]] [[1.201]
 [0.596]
 [0.347]
 [0.318]
 [0.327]
 [0.341]
 [0.399]]
printing an ep nov before normalisation:  55.03456531394888
actor:  1 policy actor:  1  step number:  49 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9112066666666667 0.10466666666666669 0.10466666666666669
actions average: 
K:  2  action  0 :  tensor([0.5216, 0.1606, 0.0505, 0.0471, 0.1231, 0.0383, 0.0588],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0750, 0.7841, 0.0236, 0.0401, 0.0177, 0.0164, 0.0431],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2090, 0.2542, 0.2139, 0.0665, 0.0710, 0.0743, 0.1112],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.4350, 0.0548, 0.0936, 0.1334, 0.0908, 0.0795, 0.1130],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.3917, 0.0117, 0.1107, 0.0625, 0.1928, 0.1457, 0.0850],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1749, 0.1147, 0.1351, 0.0967, 0.1013, 0.2821, 0.0952],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.5143, 0.0176, 0.1290, 0.0346, 0.0876, 0.0957, 0.1212],
       grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 44.05549403387668
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.083]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]] [[37.17]
 [37.17]
 [37.17]
 [37.17]
 [37.17]
 [37.17]
 [37.17]] [[1.25]
 [1.25]
 [1.25]
 [1.25]
 [1.25]
 [1.25]
 [1.25]]
siam score:  -0.77271223
printing an ep nov before normalisation:  42.22389226656726
maxi score, test score, baseline:  -0.9112066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04891849360631875, 0.2634606869818305, 0.088319553933803, 0.34154018026996097, 0.19365984266939773, 0.06410124253868914]
maxi score, test score, baseline:  -0.9112066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04891849360631875, 0.2634606869818305, 0.088319553933803, 0.34154018026996097, 0.19365984266939773, 0.06410124253868914]
maxi score, test score, baseline:  -0.9112066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04891849360631875, 0.2634606869818305, 0.088319553933803, 0.34154018026996097, 0.19365984266939773, 0.06410124253868914]
actions average: 
K:  3  action  0 :  tensor([0.6498, 0.0252, 0.0600, 0.0603, 0.0658, 0.0630, 0.0760],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0205, 0.9219, 0.0134, 0.0107, 0.0085, 0.0090, 0.0160],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.3076, 0.1073, 0.2140, 0.0837, 0.0703, 0.0935, 0.1236],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2508, 0.0656, 0.0753, 0.2969, 0.0868, 0.0819, 0.1426],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2781, 0.0257, 0.0240, 0.2080, 0.3546, 0.0225, 0.0871],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0544, 0.0062, 0.1277, 0.0835, 0.0709, 0.5133, 0.1439],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1557, 0.1780, 0.1252, 0.1485, 0.0937, 0.1245, 0.1744],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9112066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04891849360631875, 0.2634606869818305, 0.088319553933803, 0.34154018026996097, 0.19365984266939773, 0.06410124253868914]
printing an ep nov before normalisation:  0.0
siam score:  -0.77452993
printing an ep nov before normalisation:  46.04643420674847
printing an ep nov before normalisation:  30.610720307676626
from probs:  [0.048859844154533365, 0.263767304706117, 0.0883279864569598, 0.3411288008237872, 0.19384762149822835, 0.0640684423603743]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9112066666666667 0.10466666666666669 0.10466666666666669
from probs:  [0.04886919015313996, 0.264441777974998, 0.08845948427702963, 0.3411932629450236, 0.19308455016176962, 0.06395173448803919]
printing an ep nov before normalisation:  23.43955721173968
maxi score, test score, baseline:  -0.9112066666666667 0.10466666666666669 0.10466666666666669
maxi score, test score, baseline:  -0.9112066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.048929752351733115, 0.2635285574805621, 0.08856924372315343, 0.3416170843738538, 0.19332432291994742, 0.06403103915075029]
using explorer policy with actor:  0
printing an ep nov before normalisation:  50.17608953559953
maxi score, test score, baseline:  -0.9112066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.048929752351733115, 0.2635285574805621, 0.08856924372315343, 0.3416170843738538, 0.19332432291994742, 0.06403103915075029]
printing an ep nov before normalisation:  28.102320547448606
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.12 ]
 [-0.126]
 [-0.126]
 [-0.126]
 [-0.112]
 [-0.126]
 [-0.126]] [[37.922]
 [31.71 ]
 [31.71 ]
 [31.71 ]
 [30.649]
 [31.71 ]
 [31.71 ]] [[1.065]
 [0.731]
 [0.731]
 [0.731]
 [0.689]
 [0.731]
 [0.731]]
maxi score, test score, baseline:  -0.9112066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.048929752351733115, 0.2635285574805621, 0.08856924372315343, 0.3416170843738538, 0.19332432291994742, 0.06403103915075029]
printing an ep nov before normalisation:  51.497408961650656
Printing some Q and Qe and total Qs values:  [[-0.079]
 [-0.102]
 [-0.102]
 [-0.102]
 [-0.102]
 [-0.102]
 [-0.102]] [[34.72 ]
 [36.209]
 [36.209]
 [36.209]
 [36.209]
 [36.209]
 [36.209]] [[1.152]
 [1.21 ]
 [1.21 ]
 [1.21 ]
 [1.21 ]
 [1.21 ]
 [1.21 ]]
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.526]
 [0.443]
 [0.434]
 [0.425]
 [0.455]
 [0.455]] [[31.005]
 [23.478]
 [18.848]
 [17.175]
 [17.23 ]
 [23.572]
 [23.572]] [[1.124]
 [1.06 ]
 [0.871]
 [0.824]
 [0.816]
 [0.99 ]
 [0.99 ]]
actor:  1 policy actor:  1  step number:  65 total reward:  0.2133333333333327  reward:  1.0 rdn_beta:  1.0
from probs:  [0.04887133547957977, 0.2638341742642893, 0.0885780691124494, 0.3412073327830893, 0.19351084908641766, 0.06399823927417456]
printing an ep nov before normalisation:  56.54286759041276
printing an ep nov before normalisation:  57.505130947279966
maxi score, test score, baseline:  -0.9112066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.048699329914319175, 0.26290352611809054, 0.09179229038177988, 0.34000362082267094, 0.19282838462263918, 0.06377284814050031]
printing an ep nov before normalisation:  31.011158647163537
actor:  1 policy actor:  1  step number:  44 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  1.333
using another actor
actor:  0 policy actor:  1  step number:  35 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9079266666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04983896589217686, 0.25700152217429006, 0.0915153102701869, 0.3479972414458316, 0.1892299952793706, 0.06441696493814385]
actor:  1 policy actor:  1  step number:  59 total reward:  0.4533333333333335  reward:  1.0 rdn_beta:  1.333
siam score:  -0.7850628
printing an ep nov before normalisation:  19.98470600181864
printing an ep nov before normalisation:  34.95762606903977
actor:  1 policy actor:  1  step number:  71 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  24.20350713265313
maxi score, test score, baseline:  -0.9079266666666667 0.10466666666666669 0.10466666666666669
probs:  [0.052008082467778924, 0.24522723005872726, 0.0911035012384847, 0.3632109608234268, 0.18276692775766176, 0.0656832976539205]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9079266666666667 0.10466666666666669 0.10466666666666669
maxi score, test score, baseline:  -0.9079266666666667 0.10466666666666669 0.10466666666666669
probs:  [0.052122483477237774, 0.2435641784853147, 0.09130412042264237, 0.3640116658801759, 0.18316969475272984, 0.06582785698189943]
maxi score, test score, baseline:  -0.9079266666666667 0.10466666666666669 0.10466666666666669
probs:  [0.052122483477237774, 0.2435641784853147, 0.09130412042264237, 0.3640116658801759, 0.18316969475272984, 0.06582785698189943]
maxi score, test score, baseline:  -0.9079266666666667 0.10466666666666669 0.10466666666666669
probs:  [0.052122483477237774, 0.2435641784853147, 0.09130412042264237, 0.3640116658801759, 0.18316969475272984, 0.06582785698189943]
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.559]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]] [[20.871]
 [33.448]
 [20.871]
 [20.871]
 [20.871]
 [20.871]
 [20.871]] [[1.106]
 [1.873]
 [1.106]
 [1.106]
 [1.106]
 [1.106]
 [1.106]]
printing an ep nov before normalisation:  43.70867726010591
printing an ep nov before normalisation:  33.06098631607524
maxi score, test score, baseline:  -0.9079266666666667 0.10466666666666669 0.10466666666666669
probs:  [0.052076338714946845, 0.24388084527021595, 0.09133223090720916, 0.36368787026103644, 0.1833719046734461, 0.06565081017314565]
printing an ep nov before normalisation:  26.478154081614882
maxi score, test score, baseline:  -0.9079266666666667 0.10466666666666669 0.10466666666666669
probs:  [0.052022115320890766, 0.24415900240559038, 0.09134603456040273, 0.3633075324785302, 0.18354520507697394, 0.06562011015761203]
maxi score, test score, baseline:  -0.9079266666666667 0.10466666666666669 0.10466666666666669
probs:  [0.052022115320890766, 0.24415900240559038, 0.09134603456040273, 0.3633075324785302, 0.18354520507697394, 0.06562011015761203]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.426]
 [0.572]
 [0.57 ]
 [0.442]
 [0.57 ]
 [0.57 ]
 [0.57 ]] [[28.805]
 [29.255]
 [35.35 ]
 [28.585]
 [35.35 ]
 [35.35 ]
 [35.35 ]] [[0.426]
 [0.572]
 [0.57 ]
 [0.442]
 [0.57 ]
 [0.57 ]
 [0.57 ]]
printing an ep nov before normalisation:  24.178546554287337
printing an ep nov before normalisation:  25.290954756755774
printing an ep nov before normalisation:  30.901165756963117
actor:  1 policy actor:  1  step number:  64 total reward:  0.08666666666666623  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  77 total reward:  0.1866666666666661  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  22.28914737701416
actor:  0 policy actor:  1  step number:  52 total reward:  0.33999999999999986  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  38.236700261345334
actions average: 
K:  0  action  0 :  tensor([0.5709, 0.0246, 0.0872, 0.0854, 0.0909, 0.0763, 0.0646],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0107, 0.9399, 0.0168, 0.0049, 0.0012, 0.0063, 0.0201],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.3239, 0.0148, 0.2172, 0.1083, 0.0930, 0.1153, 0.1274],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2280, 0.0034, 0.1137, 0.1898, 0.1117, 0.1461, 0.2074],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2574, 0.0339, 0.1026, 0.1062, 0.2819, 0.1024, 0.1156],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.2288, 0.0093, 0.1162, 0.0837, 0.0979, 0.3826, 0.0815],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.4654, 0.0036, 0.1203, 0.1055, 0.0860, 0.1199, 0.0995],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9028733333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05198447933687736, 0.24408189208097053, 0.0915059641710193, 0.36304560171458344, 0.18356211747042445, 0.06581994522612479]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9028733333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05215311942161138, 0.2416255240943127, 0.09180314322480379, 0.3642259314195305, 0.18415869830755577, 0.06603358353218577]
printing an ep nov before normalisation:  20.571351051330566
maxi score, test score, baseline:  -0.9028733333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05215311942161138, 0.2416255240943127, 0.09180314322480379, 0.3642259314195305, 0.18415869830755577, 0.06603358353218577]
printing an ep nov before normalisation:  14.32012876626008
Printing some Q and Qe and total Qs values:  [[-0.169]
 [-0.161]
 [-0.169]
 [-0.168]
 [-0.168]
 [-0.168]
 [-0.168]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.169]
 [-0.161]
 [-0.169]
 [-0.168]
 [-0.168]
 [-0.168]
 [-0.168]]
maxi score, test score, baseline:  -0.9028733333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05217155556159552, 0.24116891954733466, 0.0916199897186704, 0.36435414645257336, 0.18458996017959148, 0.0660954285402346]
maxi score, test score, baseline:  -0.9028733333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05220045074530854, 0.2413027587920612, 0.09167078932895192, 0.36455638642890625, 0.18413755950910118, 0.06613205519567082]
maxi score, test score, baseline:  -0.9028733333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05220045074530854, 0.2413027587920612, 0.09167078932895192, 0.36455638642890625, 0.18413755950910118, 0.06613205519567082]
Printing some Q and Qe and total Qs values:  [[-0.163]
 [-0.171]
 [-0.156]
 [-0.168]
 [-0.156]
 [-0.167]
 [-0.17 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.163]
 [-0.171]
 [-0.156]
 [-0.168]
 [-0.156]
 [-0.167]
 [-0.17 ]]
maxi score, test score, baseline:  -0.9028733333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05227258310827003, 0.24057655269851144, 0.09147365535166843, 0.36506124729163897, 0.18439247356633257, 0.06622348798357866]
maxi score, test score, baseline:  -0.9028733333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05227258310827003, 0.24057655269851144, 0.09147365535166843, 0.36506124729163897, 0.18439247356633257, 0.06622348798357866]
line 256 mcts: sample exp_bonus 39.23446865548106
Printing some Q and Qe and total Qs values:  [[-0.028]
 [-0.049]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]] [[53.811]
 [48.176]
 [45.835]
 [45.835]
 [45.835]
 [45.835]
 [45.835]] [[1.061]
 [0.894]
 [0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.867]]
maxi score, test score, baseline:  -0.9028733333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05227258310827003, 0.24057655269851144, 0.09147365535166843, 0.36506124729163897, 0.18439247356633257, 0.06622348798357866]
Printing some Q and Qe and total Qs values:  [[-0.114]
 [-0.104]
 [-0.114]
 [-0.114]
 [-0.114]
 [-0.114]
 [-0.114]] [[71.382]
 [76.895]
 [73.453]
 [73.453]
 [73.453]
 [73.453]
 [73.453]] [[0.989]
 [1.108]
 [1.03 ]
 [1.03 ]
 [1.03 ]
 [1.03 ]
 [1.03 ]]
printing an ep nov before normalisation:  56.34431476722695
maxi score, test score, baseline:  -0.9028733333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05227258310827003, 0.24057655269851144, 0.09147365535166843, 0.36506124729163897, 0.18439247356633257, 0.06622348798357866]
siam score:  -0.8019606
siam score:  -0.8034608
maxi score, test score, baseline:  -0.9028733333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05221875262586995, 0.24084871664031463, 0.09148769030514817, 0.36468366007197617, 0.18456737085695496, 0.06619380949973622]
printing an ep nov before normalisation:  60.193310832446365
Printing some Q and Qe and total Qs values:  [[-0.073]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]] [[24.18 ]
 [23.885]
 [23.885]
 [23.885]
 [23.885]
 [23.885]
 [23.885]] [[1.117]
 [1.09 ]
 [1.09 ]
 [1.09 ]
 [1.09 ]
 [1.09 ]
 [1.09 ]]
Printing some Q and Qe and total Qs values:  [[-0.077]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]] [[25.237]
 [25.046]
 [25.046]
 [25.046]
 [25.046]
 [25.046]
 [25.046]] [[1.185]
 [1.169]
 [1.169]
 [1.169]
 [1.169]
 [1.169]
 [1.169]]
maxi score, test score, baseline:  -0.9028733333333334 0.10466666666666669 0.10466666666666669
line 256 mcts: sample exp_bonus 28.932043591609208
printing an ep nov before normalisation:  35.740688003675096
Printing some Q and Qe and total Qs values:  [[-0.08]
 [-0.08]
 [-0.08]
 [-0.08]
 [-0.08]
 [-0.08]
 [-0.08]] [[14.381]
 [14.381]
 [14.381]
 [14.381]
 [14.381]
 [14.381]
 [14.381]] [[0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]]
maxi score, test score, baseline:  -0.9028733333333334 0.10466666666666669 0.10466666666666669
probs:  [0.052193864954772004, 0.24125427236537858, 0.09155241223020666, 0.36450864630407837, 0.18428999199147442, 0.06620081215408989]
maxi score, test score, baseline:  -0.9028733333333334 0.10466666666666669 0.10466666666666669
probs:  [0.052193864954772004, 0.24125427236537858, 0.09155241223020666, 0.36450864630407837, 0.18428999199147442, 0.06620081215408989]
printing an ep nov before normalisation:  22.760325693840052
maxi score, test score, baseline:  -0.9028733333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05224890753463983, 0.24045277130752674, 0.091649067745618, 0.36489389250590987, 0.1844846969318826, 0.066270663974423]
actor:  1 policy actor:  1  step number:  50 total reward:  0.3399999999999995  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9028733333333334 0.10466666666666669 0.10466666666666669
printing an ep nov before normalisation:  40.80030890673184
actor:  0 policy actor:  1  step number:  46 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.90022 0.10466666666666669 0.10466666666666669
printing an ep nov before normalisation:  77.30025619509188
printing an ep nov before normalisation:  39.27783327990137
maxi score, test score, baseline:  -0.90022 0.10466666666666669 0.10466666666666669
probs:  [0.051467185874319525, 0.2358164631680023, 0.10629568116206156, 0.3594225773667752, 0.18171947281531736, 0.06527861961352403]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.90022 0.10466666666666669 0.10466666666666669
maxi score, test score, baseline:  -0.90022 0.10466666666666669 0.10466666666666669
probs:  [0.051467185874319525, 0.2358164631680023, 0.10629568116206156, 0.3594225773667752, 0.18171947281531736, 0.06527861961352403]
maxi score, test score, baseline:  -0.90022 0.10466666666666669 0.10466666666666669
probs:  [0.051538979701639585, 0.2351219664172573, 0.10607082494492583, 0.35992506651892653, 0.18197343280236009, 0.0653697296148908]
maxi score, test score, baseline:  -0.90022 0.10466666666666669 0.10466666666666669
probs:  [0.051538979701639585, 0.2351219664172573, 0.10607082494492583, 0.35992506651892653, 0.18197343280236009, 0.0653697296148908]
actor:  0 policy actor:  1  step number:  56 total reward:  0.31333333333333313  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.172]
 [-0.168]
 [-0.169]
 [-0.165]
 [-0.171]
 [-0.167]
 [-0.174]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.172]
 [-0.168]
 [-0.169]
 [-0.165]
 [-0.171]
 [-0.167]
 [-0.174]]
Printing some Q and Qe and total Qs values:  [[-0.098]
 [-0.085]
 [-0.096]
 [-0.095]
 [-0.093]
 [-0.092]
 [-0.096]] [[31.26 ]
 [36.061]
 [51.706]
 [30.888]
 [30.894]
 [31.656]
 [51.706]] [[0.672]
 [0.915]
 [1.655]
 [0.657]
 [0.659]
 [0.697]
 [1.655]]
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.051538979701639585, 0.2351219664172573, 0.10607082494492583, 0.35992506651892653, 0.18197343280236009, 0.0653697296148908]
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05156610002473164, 0.23524594029521897, 0.10575576272749487, 0.360114883229712, 0.18206936691199246, 0.06524794681085001]
printing an ep nov before normalisation:  24.94197198306807
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05156610002473164, 0.23524594029521897, 0.10575576272749487, 0.360114883229712, 0.18206936691199246, 0.06524794681085001]
actor:  1 policy actor:  1  step number:  60 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.050412793424879074, 0.23046869945790127, 0.12491049672217089, 0.3520420279923338, 0.17834127988834733, 0.06382470251436753]
line 256 mcts: sample exp_bonus 27.842794032644917
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.050412793424879074, 0.23046869945790127, 0.12491049672217089, 0.3520420279923338, 0.17834127988834733, 0.06382470251436753]
actor:  1 policy actor:  1  step number:  57 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
printing an ep nov before normalisation:  40.76508180578868
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.04937973481823321, 0.2263209455827742, 0.1227331776846113, 0.3448122458388789, 0.19396623572898625, 0.0627876603465163]
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.04937973481823321, 0.2263209455827742, 0.1227331776846113, 0.3448122458388789, 0.19396623572898625, 0.0627876603465163]
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.049365830244388065, 0.22527156333916504, 0.12310098452436816, 0.3447133748378885, 0.1947047200750579, 0.06284352697913237]
printing an ep nov before normalisation:  77.8562219586787
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.049365830244388065, 0.22527156333916504, 0.12310098452436816, 0.3447133748378885, 0.1947047200750579, 0.06284352697913237]
printing an ep nov before normalisation:  0.0
printing an ep nov before normalisation:  1.1631786723263815
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.0493395993558508, 0.22563194910653933, 0.1232368133837954, 0.3445290076533975, 0.19441571228239438, 0.06284691821802268]
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.0492848063086706, 0.22586103591102882, 0.12330101546139925, 0.34414473435817483, 0.19459453223927614, 0.06281387572145045]
actor:  1 policy actor:  1  step number:  48 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.048049992457954466, 0.22019017390099513, 0.12020672443693765, 0.3355022822085433, 0.21481165007764563, 0.06123917691792388]
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.047984929954096575, 0.21987866221888, 0.1204360751835704, 0.3350453968441146, 0.21542700699316114, 0.061227928806177195]
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
siam score:  -0.8050434
printing an ep nov before normalisation:  45.58326195239674
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.04803002602751192, 0.2191440933999599, 0.12054945016757106, 0.3353610224707796, 0.2156299026785793, 0.061285505255598165]
Printing some Q and Qe and total Qs values:  [[-0.11 ]
 [-0.106]
 [-0.108]
 [-0.112]
 [-0.109]
 [-0.108]
 [-0.11 ]] [[28.944]
 [46.032]
 [27.852]
 [28.433]
 [34.274]
 [34.541]
 [29.439]] [[0.12 ]
 [0.509]
 [0.098]
 [0.108]
 [0.241]
 [0.249]
 [0.131]]
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.04803002602751192, 0.2191440933999599, 0.12054945016757106, 0.3353610224707796, 0.2156299026785793, 0.061285505255598165]
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.04803002602751192, 0.2191440933999599, 0.12054945016757106, 0.3353610224707796, 0.2156299026785793, 0.061285505255598165]
printing an ep nov before normalisation:  23.743261584620537
Printing some Q and Qe and total Qs values:  [[0.139]
 [0.247]
 [0.157]
 [0.157]
 [0.182]
 [0.157]
 [0.157]] [[23.607]
 [20.981]
 [23.763]
 [23.763]
 [25.402]
 [23.763]
 [23.763]] [[1.055]
 [0.978]
 [1.085]
 [1.085]
 [1.226]
 [1.085]
 [1.085]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.04803002602751192, 0.2191440933999599, 0.12054945016757106, 0.3353610224707796, 0.2156299026785793, 0.061285505255598165]
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.048074825547206566, 0.2184143551372252, 0.12066207959279472, 0.33567457252973215, 0.21583146411323292, 0.0613427030798084]
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.048074825547206566, 0.2184143551372252, 0.12066207959279472, 0.33567457252973215, 0.21583146411323292, 0.0613427030798084]
actor:  1 policy actor:  1  step number:  55 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.04758051008131054, 0.2161637055889102, 0.11941933263518147, 0.3322148782176209, 0.22390998808136311, 0.060711585395613686]
Printing some Q and Qe and total Qs values:  [[-0.167]
 [-0.146]
 [-0.167]
 [-0.167]
 [-0.167]
 [-0.167]
 [-0.167]] [[32.45 ]
 [36.056]
 [32.45 ]
 [32.45 ]
 [32.45 ]
 [32.45 ]
 [32.45 ]] [[0.144]
 [0.233]
 [0.144]
 [0.144]
 [0.144]
 [0.144]
 [0.144]]
actor:  1 policy actor:  1  step number:  59 total reward:  0.19999999999999907  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.04764548008368255, 0.2159226115783697, 0.11935387873469919, 0.33267048312880526, 0.22365483063883196, 0.06075271583561146]
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.04764548008368255, 0.2159226115783697, 0.11935387873469919, 0.33267048312880526, 0.22365483063883196, 0.06075271583561146]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
printing an ep nov before normalisation:  20.522913932800293
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.04766555572590337, 0.2155505761232512, 0.119598212935129, 0.3328102440332211, 0.22356162855373604, 0.06081378262875921]
printing an ep nov before normalisation:  62.125725830137945
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.04766555572590337, 0.2155505761232512, 0.119598212935129, 0.3328102440332211, 0.22356162855373604, 0.06081378262875921]
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.04766555572590337, 0.2155505761232512, 0.119598212935129, 0.3328102440332211, 0.22356162855373604, 0.06081378262875921]
actions average: 
K:  0  action  0 :  tensor([0.5357, 0.0039, 0.0837, 0.0933, 0.1306, 0.0823, 0.0705],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0305, 0.8999, 0.0121, 0.0201, 0.0046, 0.0182, 0.0145],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.3709, 0.0016, 0.2651, 0.0912, 0.0907, 0.1052, 0.0752],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2467, 0.0018, 0.1207, 0.2678, 0.0967, 0.1278, 0.1386],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.3406, 0.0047, 0.0584, 0.1139, 0.3498, 0.0737, 0.0589],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.2238, 0.0056, 0.1490, 0.1351, 0.1091, 0.2975, 0.0799],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2895, 0.0424, 0.1220, 0.1357, 0.1200, 0.1219, 0.1684],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  52.439390752947006
printing an ep nov before normalisation:  42.032092719671766
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.04758716410632599, 0.21610026267277857, 0.11978893008732815, 0.33226008847712857, 0.22347897453997037, 0.060784580116468326]
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.04758716410632599, 0.21610026267277857, 0.11978893008732815, 0.33226008847712857, 0.22347897453997037, 0.060784580116468326]
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.04758716410632599, 0.21610026267277857, 0.11978893008732815, 0.33226008847712857, 0.22347897453997037, 0.060784580116468326]
printing an ep nov before normalisation:  36.76406214118208
actor:  1 policy actor:  1  step number:  42 total reward:  0.5666666666666667  reward:  1.0 rdn_beta:  1.333
using another actor
printing an ep nov before normalisation:  35.2977196375529
printing an ep nov before normalisation:  50.33497773601017
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.050189577145194175, 0.2059297422342408, 0.11728254834905891, 0.3505091757015981, 0.21363577561526012, 0.0624531809546479]
printing an ep nov before normalisation:  59.067409296936624
actor:  1 policy actor:  1  step number:  51 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  38.49430615358353
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
printing an ep nov before normalisation:  35.482896155441544
Printing some Q and Qe and total Qs values:  [[-0.171]
 [-0.17 ]
 [-0.172]
 [-0.163]
 [-0.163]
 [-0.172]
 [-0.171]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.171]
 [-0.17 ]
 [-0.172]
 [-0.163]
 [-0.163]
 [-0.172]
 [-0.171]]
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05093879363536804, 0.20203770690584855, 0.11686454197204477, 0.3557636814449722, 0.21122761085188224, 0.06316766518988415]
printing an ep nov before normalisation:  32.2571560704245
printing an ep nov before normalisation:  39.28882645459778
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05093879363536804, 0.20203770690584855, 0.11686454197204477, 0.3557636814449722, 0.21122761085188224, 0.06316766518988415]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05088720818127841, 0.20222648594466178, 0.11691782960224936, 0.3554019341758344, 0.21143100896739164, 0.0631355331285844]
printing an ep nov before normalisation:  38.46388838363967
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05088720818127841, 0.20222648594466178, 0.11691782960224936, 0.3554019341758344, 0.21143100896739164, 0.0631355331285844]
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05088720818127841, 0.20222648594466178, 0.11691782960224936, 0.3554019341758344, 0.21143100896739164, 0.0631355331285844]
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05088720818127841, 0.20222648594466178, 0.11691782960224936, 0.3554019341758344, 0.21143100896739164, 0.0631355331285844]
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05088720818127841, 0.20222648594466178, 0.11691782960224936, 0.3554019341758344, 0.21143100896739164, 0.0631355331285844]
actor:  1 policy actor:  1  step number:  55 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05093387757300687, 0.20205569743328824, 0.11686962025075312, 0.35572920715051365, 0.21124699456842858, 0.06316460302400947]
Printing some Q and Qe and total Qs values:  [[-0.165]
 [-0.155]
 [-0.167]
 [-0.166]
 [-0.166]
 [-0.167]
 [-0.168]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.165]
 [-0.155]
 [-0.167]
 [-0.166]
 [-0.166]
 [-0.167]
 [-0.168]]
from probs:  [0.05093387757300687, 0.20205569743328824, 0.11686962025075312, 0.35572920715051365, 0.21124699456842858, 0.06316460302400947]
printing an ep nov before normalisation:  35.84325212209893
printing an ep nov before normalisation:  28.257912585023448
actor:  1 policy actor:  1  step number:  42 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.164]
 [-0.168]
 [-0.164]
 [-0.164]
 [-0.164]
 [-0.164]
 [-0.164]] [[24.795]
 [31.39 ]
 [24.795]
 [24.795]
 [24.795]
 [24.795]
 [24.795]] [[0.46 ]
 [0.882]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]]
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05201573351381586, 0.19798082541704903, 0.11581926413892497, 0.3633173560724377, 0.2068457872485008, 0.06402103360927153]
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.051965650117601764, 0.19816392266025662, 0.11587110749707752, 0.36296614457596615, 0.2070430463535711, 0.06399012879552676]
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.051965650117601764, 0.19816392266025662, 0.11587110749707752, 0.36296614457596615, 0.2070430463535711, 0.06399012879552676]
printing an ep nov before normalisation:  48.322019681492634
siam score:  -0.8049583
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.051915632114758185, 0.19834678083529456, 0.11592288316389553, 0.3626153916526996, 0.20724004789947567, 0.06395926433387647]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.051915632114758185, 0.19834678083529456, 0.11592288316389553, 0.3626153916526996, 0.20724004789947567, 0.06395926433387647]
siam score:  -0.8055328
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.0518656793772935, 0.19852940041008055, 0.1159745912718683, 0.3622650964050925, 0.20743679239032392, 0.06392844014534123]
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.0518656793772935, 0.19852940041008055, 0.1159745912718683, 0.3622650964050925, 0.20743679239032392, 0.06392844014534123]
printing an ep nov before normalisation:  54.06376043197796
Printing some Q and Qe and total Qs values:  [[-0.122]
 [-0.098]
 [-0.122]
 [-0.124]
 [-0.124]
 [-0.125]
 [-0.124]] [[26.659]
 [49.081]
 [24.378]
 [24.678]
 [25.311]
 [25.268]
 [25.346]] [[0.313]
 [1.149]
 [0.23 ]
 [0.239]
 [0.262]
 [0.259]
 [0.264]]
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05189576712058907, 0.1986447881955211, 0.11604196494278092, 0.36247571334243694, 0.2069762227736646, 0.0639655436250074]
printing an ep nov before normalisation:  58.22693135026244
actions average: 
K:  4  action  0 :  tensor([0.5544, 0.0800, 0.0723, 0.0760, 0.0660, 0.0543, 0.0969],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0280, 0.8981, 0.0181, 0.0247, 0.0058, 0.0072, 0.0181],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2311, 0.1325, 0.2683, 0.0949, 0.0699, 0.1137, 0.0896],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1223, 0.2318, 0.0601, 0.2963, 0.0807, 0.0785, 0.1303],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2022, 0.0386, 0.0682, 0.0986, 0.4601, 0.0739, 0.0585],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1307, 0.1579, 0.1780, 0.0903, 0.1144, 0.2257, 0.1031],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1665, 0.2064, 0.1278, 0.1217, 0.1051, 0.1086, 0.1638],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05191490172253822, 0.19871817021432223, 0.11571544621290673, 0.3626096572949709, 0.2070526846004346, 0.0639891399548274]
Printing some Q and Qe and total Qs values:  [[-0.192]
 [-0.192]
 [-0.192]
 [-0.192]
 [-0.192]
 [-0.192]
 [-0.192]] [[52.46]
 [52.46]
 [52.46]
 [52.46]
 [52.46]
 [52.46]
 [52.46]] [[0.145]
 [0.145]
 [0.145]
 [0.145]
 [0.145]
 [0.145]
 [0.145]]
printing an ep nov before normalisation:  62.627382477046126
printing an ep nov before normalisation:  38.64283044294706
printing an ep nov before normalisation:  49.40465534082864
actor:  1 policy actor:  1  step number:  45 total reward:  0.3999999999999996  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05118233552383378, 0.19590874743619155, 0.11408028035169397, 0.3574816273308262, 0.20412535170206647, 0.07722165765538806]
printing an ep nov before normalisation:  31.19115283900862
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05118233552383378, 0.19590874743619155, 0.11408028035169397, 0.3574816273308262, 0.20412535170206647, 0.07722165765538806]
Printing some Q and Qe and total Qs values:  [[-0.181]
 [-0.181]
 [-0.181]
 [-0.181]
 [-0.181]
 [-0.181]
 [-0.181]] [[25.95]
 [25.95]
 [25.95]
 [25.95]
 [25.95]
 [25.95]
 [25.95]] [[0.32]
 [0.32]
 [0.32]
 [0.32]
 [0.32]
 [0.32]
 [0.32]]
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05118233552383378, 0.19590874743619155, 0.11408028035169397, 0.3574816273308262, 0.20412535170206647, 0.07722165765538806]
using explorer policy with actor:  1
printing an ep nov before normalisation:  66.4975299626609
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05122201240459151, 0.19528430161522736, 0.11416884337986699, 0.35775936910183515, 0.20428390042920153, 0.07728157306927753]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05121117395453404, 0.1948382337168201, 0.114305161532861, 0.35768288549292865, 0.20463088884904945, 0.0773316564538067]
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05125034508765261, 0.19422127459373903, 0.11439271877234772, 0.3579570865140365, 0.20478771592576558, 0.07739085910645839]
actions average: 
K:  1  action  0 :  tensor([0.5146, 0.0501, 0.0690, 0.1016, 0.1275, 0.0712, 0.0660],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0158, 0.9305, 0.0109, 0.0139, 0.0048, 0.0059, 0.0182],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2729, 0.0848, 0.2529, 0.1341, 0.0936, 0.0833, 0.0783],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2784, 0.0262, 0.0722, 0.3818, 0.0859, 0.0843, 0.0714],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2654, 0.0571, 0.1188, 0.1844, 0.1156, 0.1274, 0.1313],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2489, 0.0513, 0.1348, 0.1520, 0.1527, 0.1523, 0.1079],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2174, 0.1236, 0.0989, 0.1559, 0.0688, 0.0734, 0.2620],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05125034508765261, 0.19422127459373903, 0.11439271877234772, 0.3579570865140365, 0.20478771592576558, 0.07739085910645839]
printing an ep nov before normalisation:  30.575594902038574
printing an ep nov before normalisation:  31.894252347335375
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.05125034508765261, 0.19422127459373903, 0.11439271877234772, 0.3579570865140365, 0.20478771592576558, 0.07739085910645839]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  54 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.665]
 [0.746]
 [0.691]
 [0.614]
 [0.608]
 [0.609]
 [0.619]] [[47.79 ]
 [37.519]
 [38.728]
 [25.874]
 [22.655]
 [22.268]
 [21.866]] [[0.665]
 [0.746]
 [0.691]
 [0.614]
 [0.608]
 [0.609]
 [0.619]]
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.051334299197708905, 0.19414336817471528, 0.11440518801448521, 0.3585454414800075, 0.20412648416656126, 0.07744521896652187]
siam score:  -0.80646455
actions average: 
K:  4  action  0 :  tensor([0.6448, 0.0798, 0.0558, 0.0516, 0.0826, 0.0390, 0.0464],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0509, 0.7362, 0.0416, 0.0341, 0.0382, 0.0258, 0.0732],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2156, 0.0639, 0.3816, 0.0617, 0.0811, 0.1162, 0.0799],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.3159, 0.0344, 0.1029, 0.3386, 0.0610, 0.0563, 0.0909],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1899, 0.1236, 0.0893, 0.1136, 0.3198, 0.1078, 0.0560],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1742, 0.0610, 0.0857, 0.1586, 0.1695, 0.2653, 0.0857],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2380, 0.0118, 0.1736, 0.1503, 0.1441, 0.1578, 0.1244],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.8975933333333334 0.10466666666666669 0.10466666666666669
probs:  [0.051334299197708905, 0.19414336817471528, 0.11440518801448521, 0.3585454414800075, 0.20412648416656126, 0.07744521896652187]
printing an ep nov before normalisation:  0.0
actor:  0 policy actor:  0  step number:  59 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  31.982502234780235
Printing some Q and Qe and total Qs values:  [[0.036]
 [0.919]
 [0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.036]] [[32.478]
 [ 0.   ]
 [32.478]
 [32.478]
 [32.478]
 [32.478]
 [32.478]] [[0.036]
 [0.919]
 [0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.036]]
printing an ep nov before normalisation:  32.70052467437702
printing an ep nov before normalisation:  29.25202802906197
printing an ep nov before normalisation:  24.661424160003662
printing an ep nov before normalisation:  52.88806519661912
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.05139241296261008, 0.19436357176047075, 0.11453488791283481, 0.3589522431775007, 0.203223915281926, 0.07753296890465762]
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.05139241296261008, 0.19436357176047075, 0.11453488791283481, 0.3589522431775007, 0.203223915281926, 0.07753296890465762]
actor:  1 policy actor:  1  step number:  57 total reward:  0.1999999999999994  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
Printing some Q and Qe and total Qs values:  [[-0.107]
 [-0.091]
 [-0.111]
 [-0.111]
 [-0.111]
 [-0.111]
 [-0.111]] [[75.505]
 [66.432]
 [63.459]
 [63.459]
 [63.459]
 [63.459]
 [63.459]] [[1.512]
 [1.292]
 [1.195]
 [1.195]
 [1.195]
 [1.195]
 [1.195]]
printing an ep nov before normalisation:  85.52923042089748
printing an ep nov before normalisation:  28.573820786424083
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.05135559013718306, 0.19422404340242228, 0.11445270571859992, 0.3586944800138908, 0.20379581311940562, 0.07747736760849827]
printing an ep nov before normalisation:  42.68056430017175
actor:  1 policy actor:  1  step number:  56 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.050300354551605836, 0.21081016866506416, 0.11209760192491183, 0.35130773389275943, 0.1996001455558208, 0.07588399540983776]
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
printing an ep nov before normalisation:  24.80245344041549
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.05033587289887682, 0.21095932802724132, 0.11217687274893133, 0.35155636558933867, 0.19919194515927238, 0.07577961557633953]
Printing some Q and Qe and total Qs values:  [[-0.14 ]
 [-0.134]
 [-0.156]
 [-0.156]
 [-0.156]
 [-0.156]
 [-0.156]] [[44.379]
 [48.713]
 [44.519]
 [44.519]
 [44.519]
 [44.519]
 [44.519]] [[0.726]
 [0.879]
 [0.715]
 [0.715]
 [0.715]
 [0.715]
 [0.715]]
printing an ep nov before normalisation:  85.93450007948708
actor:  1 policy actor:  1  step number:  59 total reward:  0.0799999999999994  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  34.049069215518806
printing an ep nov before normalisation:  34.481459997995024
siam score:  -0.7949841
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.05039066988736317, 0.21118944807729587, 0.11229917017448329, 0.35193994954697205, 0.19831857749836077, 0.07586218481552491]
Printing some Q and Qe and total Qs values:  [[0.121]
 [0.121]
 [0.121]
 [0.121]
 [0.121]
 [0.121]
 [0.121]] [[28.501]
 [28.501]
 [28.501]
 [28.501]
 [28.501]
 [28.501]
 [28.501]] [[0.77]
 [0.77]
 [0.77]
 [0.77]
 [0.77]
 [0.77]
 [0.77]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.05039066988736317, 0.21118944807729587, 0.11229917017448329, 0.35193994954697205, 0.19831857749836077, 0.07586218481552491]
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.242]
 [0.034]
 [0.034]
 [0.034]
 [0.034]
 [0.034]] [[35.774]
 [47.117]
 [35.774]
 [35.774]
 [35.774]
 [35.774]
 [35.774]] [[0.713]
 [1.29 ]
 [0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]]
printing an ep nov before normalisation:  0.01075748206829985
Printing some Q and Qe and total Qs values:  [[0.199]
 [0.368]
 [0.199]
 [0.199]
 [0.199]
 [0.199]
 [0.199]] [[27.992]
 [35.12 ]
 [27.992]
 [27.992]
 [27.992]
 [27.992]
 [27.992]] [[0.992]
 [1.516]
 [0.992]
 [0.992]
 [0.992]
 [0.992]
 [0.992]]
actor:  1 policy actor:  1  step number:  62 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.050673168087366216, 0.21009441752312458, 0.11205131148848695, 0.3539208199674427, 0.1973338089406631, 0.07592647399291662]
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.050700295857479934, 0.21020711608916043, 0.1121113845315906, 0.3541107186543868, 0.19690332816425665, 0.07596715670312552]
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.387]
 [0.427]
 [0.411]
 [0.305]
 [0.411]
 [0.313]] [[20.01 ]
 [31.436]
 [23.785]
 [20.01 ]
 [29.409]
 [20.01 ]
 [20.86 ]] [[0.411]
 [0.387]
 [0.427]
 [0.411]
 [0.305]
 [0.411]
 [0.313]]
actor:  1 policy actor:  1  step number:  66 total reward:  0.39333333333333287  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  76.62489700745124
printing an ep nov before normalisation:  29.668357717921225
printing an ep nov before normalisation:  14.183731079101562
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.05145936568141412, 0.20725971775179738, 0.11144344173881923, 0.35943330501294635, 0.19426507062912823, 0.07613909918589469]
printing an ep nov before normalisation:  18.217661380767822
siam score:  -0.79907364
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.05145936568141412, 0.20725971775179738, 0.11144344173881923, 0.35943330501294635, 0.19426507062912823, 0.07613909918589469]
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.05145936568141412, 0.20725971775179738, 0.11144344173881923, 0.35943330501294635, 0.19426507062912823, 0.07613909918589469]
printing an ep nov before normalisation:  31.73776388168335
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
printing an ep nov before normalisation:  26.334949084212163
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.05145936568141412, 0.20725971775179738, 0.11144344173881923, 0.35943330501294635, 0.19426507062912823, 0.07613909918589469]
line 256 mcts: sample exp_bonus 29.25718391726151
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.05145936568141412, 0.20725971775179738, 0.11144344173881923, 0.35943330501294635, 0.19426507062912823, 0.07613909918589469]
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.81363349929907
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.05145936568141412, 0.20725971775179738, 0.11144344173881923, 0.35943330501294635, 0.19426507062912823, 0.07613909918589469]
printing an ep nov before normalisation:  52.8074228374
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.05145936568141412, 0.20725971775179738, 0.11144344173881923, 0.35943330501294635, 0.19426507062912823, 0.07613909918589469]
using explorer policy with actor:  1
siam score:  -0.8063904
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.05145936568141412, 0.20725971775179738, 0.11144344173881923, 0.35943330501294635, 0.19426507062912823, 0.07613909918589469]
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.05145936568141412, 0.20725971775179738, 0.11144344173881923, 0.35943330501294635, 0.19426507062912823, 0.07613909918589469]
printing an ep nov before normalisation:  22.405136458268682
actor:  1 policy actor:  1  step number:  44 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  68 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.05212018684583525, 0.2046938099263297, 0.1108619527635387, 0.3640669732267156, 0.19196829059462436, 0.07628878664295641]
actor:  1 policy actor:  1  step number:  55 total reward:  0.34666666666666657  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.05254480113372519, 0.20304507168554314, 0.11048831381180976, 0.3670443623220414, 0.1904924819018973, 0.07638496914498324]
Printing some Q and Qe and total Qs values:  [[-0.111]
 [-0.111]
 [-0.111]
 [-0.111]
 [-0.111]
 [-0.111]
 [-0.111]] [[64.275]
 [64.275]
 [64.275]
 [64.275]
 [64.275]
 [64.275]
 [64.275]] [[0.889]
 [0.889]
 [0.889]
 [0.889]
 [0.889]
 [0.889]
 [0.889]]
Printing some Q and Qe and total Qs values:  [[-0.077]
 [-0.059]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]] [[41.13 ]
 [59.887]
 [41.13 ]
 [41.13 ]
 [41.13 ]
 [41.13 ]
 [41.13 ]] [[0.33 ]
 [0.693]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]]
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.05258538266605135, 0.20242851666237624, 0.11057376017715369, 0.3673284563658588, 0.1906398743501794, 0.0764440097783805]
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.05258538266605135, 0.20242851666237624, 0.11057376017715369, 0.3673284563658588, 0.1906398743501794, 0.0764440097783805]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
actor:  1 policy actor:  1  step number:  49 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[-0.17]
 [-0.17]
 [-0.17]
 [-0.17]
 [-0.17]
 [-0.17]
 [-0.16]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [56.222]] [[-0.406]
 [-0.406]
 [-0.406]
 [-0.406]
 [-0.406]
 [-0.406]
 [ 0.601]]
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.052157381630127675, 0.20926183887966907, 0.10934187887907826, 0.36433220319147436, 0.1890853711824565, 0.07582132623719409]
using explorer policy with actor:  1
printing an ep nov before normalisation:  61.535015567928774
siam score:  -0.81508166
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.262]
 [0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]] [[43.147]
 [54.823]
 [43.147]
 [43.147]
 [43.147]
 [43.147]
 [43.147]] [[0.519]
 [0.988]
 [0.519]
 [0.519]
 [0.519]
 [0.519]
 [0.519]]
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.052157381630127675, 0.20926183887966907, 0.10934187887907826, 0.36433220319147436, 0.1890853711824565, 0.07582132623719409]
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.05210916604037571, 0.20945633590845214, 0.10938200832618833, 0.36399412302010287, 0.18924869733682062, 0.07580966936806025]
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.05210916604037571, 0.20945633590845214, 0.10938200832618833, 0.36399412302010287, 0.18924869733682062, 0.07580966936806025]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.05210916604037571, 0.20945633590845214, 0.10938200832618833, 0.36399412302010287, 0.18924869733682062, 0.07580966936806025]
actor:  1 policy actor:  1  step number:  57 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.051716831313997486, 0.20747242041597258, 0.11610995865098762, 0.3612470190720734, 0.18815725579723358, 0.0752965147497353]
Printing some Q and Qe and total Qs values:  [[-0.141]
 [-0.141]
 [-0.141]
 [-0.141]
 [-0.141]
 [-0.141]
 [-0.141]] [[50.674]
 [50.674]
 [50.674]
 [50.674]
 [50.674]
 [50.674]
 [50.674]] [[0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]]
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.05175729143515036, 0.2068513158943646, 0.11620092617183819, 0.36153026232177043, 0.18830473434316713, 0.07535546983370932]
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.05175729143515036, 0.2068513158943646, 0.11620092617183819, 0.36153026232177043, 0.18830473434316713, 0.07535546983370932]
Printing some Q and Qe and total Qs values:  [[0.19 ]
 [0.27 ]
 [0.418]
 [0.217]
 [0.171]
 [0.179]
 [0.197]] [[22.842]
 [20.872]
 [27.272]
 [24.655]
 [26.268]
 [24.991]
 [22.01 ]] [[0.758]
 [0.737]
 [1.212]
 [0.877]
 [0.913]
 [0.856]
 [0.722]]
Printing some Q and Qe and total Qs values:  [[-0.167]
 [-0.164]
 [-0.167]
 [-0.167]
 [-0.167]
 [-0.167]
 [-0.167]] [[40.285]
 [42.941]
 [40.285]
 [40.285]
 [40.285]
 [40.285]
 [40.285]] [[0.263]
 [0.305]
 [0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]]
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.05174924215049724, 0.20642200046742284, 0.11634162719586454, 0.3614733727006849, 0.18861186706779923, 0.07540189041773136]
printing an ep nov before normalisation:  51.9516861119429
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  66 total reward:  0.17999999999999894  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
actor:  1 policy actor:  1  step number:  75 total reward:  0.0533333333333329  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.05179124784150722, 0.20580016113306787, 0.11642946609922025, 0.3617674582579237, 0.18875098720574637, 0.07546067946253468]
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.05179124784150722, 0.20580016113306787, 0.11642946609922025, 0.3617674582579237, 0.18875098720574637, 0.07546067946253468]
printing an ep nov before normalisation:  57.66755530355547
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.051830955389534465, 0.2051902020784584, 0.1165188585771331, 0.3620454326811196, 0.18889597047502465, 0.07551858079872967]
printing an ep nov before normalisation:  54.530226739538286
printing an ep nov before normalisation:  17.47898544058615
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.0519096123273463, 0.2039819302558506, 0.11669593671278655, 0.3625960740076403, 0.18918316876758978, 0.07563327792878637]
actions average: 
K:  0  action  0 :  tensor([0.5896, 0.0139, 0.0680, 0.0857, 0.0699, 0.0808, 0.0921],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0090, 0.9655, 0.0067, 0.0068, 0.0019, 0.0021, 0.0080],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2182, 0.0340, 0.2487, 0.1324, 0.0705, 0.1375, 0.1586],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1569, 0.0159, 0.0755, 0.4271, 0.0852, 0.0706, 0.1688],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2261, 0.0072, 0.0996, 0.1300, 0.3188, 0.1114, 0.1070],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.2485, 0.0159, 0.1325, 0.0649, 0.0604, 0.4183, 0.0596],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2532, 0.0789, 0.0884, 0.2056, 0.0633, 0.0777, 0.2329],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  68 total reward:  0.1799999999999996  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.051738165526365835, 0.20510650722804438, 0.11678568740335388, 0.36139423496165746, 0.1895651645169957, 0.0754102403635828]
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.051738165526365835, 0.20510650722804438, 0.11678568740335388, 0.36139423496165746, 0.1895651645169957, 0.0754102403635828]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.068]
 [-0.137]
 [-0.11 ]
 [-0.098]
 [-0.095]
 [-0.123]
 [-0.11 ]] [[42.347]
 [40.797]
 [37.931]
 [34.626]
 [35.189]
 [40.292]
 [37.931]] [[0.44 ]
 [0.338]
 [0.305]
 [0.246]
 [0.261]
 [0.341]
 [0.305]]
Printing some Q and Qe and total Qs values:  [[-0.063]
 [-0.078]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]] [[45.814]
 [54.927]
 [45.814]
 [45.814]
 [45.814]
 [45.814]
 [45.814]] [[0.953]
 [1.276]
 [0.953]
 [0.953]
 [0.953]
 [0.953]
 [0.953]]
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.051753273037497126, 0.20516651381249063, 0.11681983778488012, 0.3614999952790311, 0.18962062132357999, 0.07513975876252106]
printing an ep nov before normalisation:  47.73315531351296
Printing some Q and Qe and total Qs values:  [[0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]] [[52.303]
 [52.303]
 [52.303]
 [52.303]
 [52.303]
 [52.303]
 [52.303]] [[2.223]
 [2.223]
 [2.223]
 [2.223]
 [2.223]
 [2.223]
 [2.223]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.051753273037497126, 0.20516651381249063, 0.11681983778488012, 0.3614999952790311, 0.18962062132357999, 0.07513975876252106]
actor:  1 policy actor:  1  step number:  53 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  48 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04951399194271737, 0.22269279077373605, 0.1117579647161404, 0.34582388045015566, 0.19832540178000774, 0.07188597033724273]
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04951399194271737, 0.22269279077373605, 0.1117579647161404, 0.34582388045015566, 0.19832540178000774, 0.07188597033724273]
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04953077964780369, 0.22276845311534824, 0.11145622045561833, 0.34594140298596415, 0.1983927800754216, 0.07191036371984402]
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
printing an ep nov before normalisation:  59.33459008337696
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04953077964780369, 0.22276845311534824, 0.11145622045561833, 0.34594140298596415, 0.1983927800754216, 0.07191036371984402]
actor:  1 policy actor:  1  step number:  66 total reward:  0.15333333333333277  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04956935938015818, 0.22269006500749494, 0.11154316426255234, 0.3462114808765275, 0.19801950845557115, 0.071966422017696]
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
Printing some Q and Qe and total Qs values:  [[-0.165]
 [-0.165]
 [-0.165]
 [-0.165]
 [-0.165]
 [-0.148]
 [-0.165]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [36.794]
 [ 0.   ]] [[-0.547]
 [-0.547]
 [-0.547]
 [-0.547]
 [-0.547]
 [ 0.549]
 [-0.547]]
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.049595354603708376, 0.22280709306886418, 0.11160174745013864, 0.3463934607585195, 0.19759814972953074, 0.0720041943892385]
Printing some Q and Qe and total Qs values:  [[-0.162]
 [-0.162]
 [-0.162]
 [-0.162]
 [-0.162]
 [-0.162]
 [-0.162]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.162]
 [-0.162]
 [-0.162]
 [-0.162]
 [-0.162]
 [-0.162]
 [-0.162]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.4199999999999995  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04882255694500515, 0.2193280303997489, 0.10952760004101736, 0.34098348124966754, 0.21045705096893988, 0.07088128039562121]
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04882255694500515, 0.2193280303997489, 0.10952760004101736, 0.34098348124966754, 0.21045705096893988, 0.07088128039562121]
using explorer policy with actor:  1
printing an ep nov before normalisation:  57.057881355285645
Printing some Q and Qe and total Qs values:  [[-0.087]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]] [[33.463]
 [33.463]
 [33.463]
 [33.463]
 [33.463]
 [33.463]
 [33.463]] [[0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]]
actions average: 
K:  0  action  0 :  tensor([0.5905, 0.0142, 0.0845, 0.0747, 0.0685, 0.0819, 0.0858],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0105, 0.9110, 0.0107, 0.0158, 0.0028, 0.0057, 0.0433],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.4790, 0.0175, 0.2237, 0.0714, 0.0567, 0.0685, 0.0832],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1995, 0.0223, 0.1254, 0.2143, 0.1417, 0.1610, 0.1358],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2965, 0.0087, 0.0828, 0.0973, 0.2766, 0.1525, 0.0857],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.2226, 0.0035, 0.2219, 0.0903, 0.0786, 0.2818, 0.1014],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2099, 0.0506, 0.0919, 0.1118, 0.0716, 0.2251, 0.2391],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04878939841287061, 0.21959459134881207, 0.10927024926730265, 0.34075084538945893, 0.21070801825230512, 0.07088689732925058]
printing an ep nov before normalisation:  43.93023553393786
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04882873943326036, 0.21896413199154993, 0.10935849217013466, 0.34102625227459793, 0.21087827881500845, 0.0709441053154487]
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04882873943326036, 0.21896413199154993, 0.10935849217013466, 0.34102625227459793, 0.21087827881500845, 0.0709441053154487]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04882873943326036, 0.21896413199154993, 0.10935849217013466, 0.34102625227459793, 0.21087827881500845, 0.0709441053154487]
printing an ep nov before normalisation:  45.10123718052569
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
printing an ep nov before normalisation:  30.294393781945068
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04882873943326036, 0.21896413199154993, 0.10935849217013466, 0.34102625227459793, 0.21087827881500845, 0.0709441053154487]
actions average: 
K:  3  action  0 :  tensor([0.5941, 0.0479, 0.0802, 0.0801, 0.0785, 0.0610, 0.0582],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0289, 0.8727, 0.0274, 0.0248, 0.0100, 0.0170, 0.0191],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.2170, 0.2315, 0.2383, 0.0579, 0.0687, 0.1252, 0.0613],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2067, 0.0315, 0.1000, 0.3273, 0.1192, 0.1330, 0.0823],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2797, 0.0382, 0.1288, 0.0940, 0.2482, 0.1066, 0.1044],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.3500, 0.0057, 0.1111, 0.1177, 0.1629, 0.1559, 0.0966],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1302, 0.4692, 0.0858, 0.0528, 0.0947, 0.0903, 0.0769],
       grad_fn=<DivBackward0>)
using another actor
siam score:  -0.82237536
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04887197293229419, 0.21915841736219918, 0.10912622767841718, 0.34132890846359876, 0.21050749999013918, 0.07100697357335153]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8950066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04805227357275401, 0.23227958616305033, 0.10729315850747081, 0.3355906015990366, 0.2069693774377259, 0.06981500271996224]
Printing some Q and Qe and total Qs values:  [[-0.16 ]
 [-0.161]
 [-0.161]
 [-0.161]
 [-0.161]
 [-0.161]
 [-0.161]] [[63.995]
 [49.2  ]
 [49.2  ]
 [49.2  ]
 [49.2  ]
 [49.2  ]
 [49.2  ]] [[0.448]
 [0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]]
actions average: 
K:  4  action  0 :  tensor([0.5826, 0.0319, 0.0824, 0.0720, 0.0675, 0.0975, 0.0661],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0266, 0.8618, 0.0305, 0.0187, 0.0120, 0.0199, 0.0304],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1824, 0.1256, 0.2996, 0.0751, 0.0569, 0.1340, 0.1265],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1462, 0.0099, 0.1123, 0.4344, 0.0627, 0.1659, 0.0685],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2656, 0.0833, 0.0825, 0.1067, 0.2703, 0.0925, 0.0991],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2344, 0.0326, 0.1337, 0.1175, 0.1298, 0.2478, 0.1043],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1940, 0.2408, 0.0858, 0.0971, 0.0903, 0.1217, 0.1704],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  1  step number:  45 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.08 ]
 [-0.053]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]] [[34.64 ]
 [49.238]
 [34.64 ]
 [34.64 ]
 [34.64 ]
 [34.64 ]
 [34.64 ]] [[0.288]
 [0.618]
 [0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]]
maxi score, test score, baseline:  -0.8920733333333334 0.10466666666666669 0.10466666666666669
probs:  [0.04800279977795354, 0.23248820621897454, 0.10732667842244206, 0.3352437590148318, 0.2071425390746963, 0.0697960174911019]
actor:  0 policy actor:  1  step number:  54 total reward:  0.38  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8893133333333334 0.10466666666666669 0.10466666666666669
probs:  [0.04800279977795354, 0.23248820621897454, 0.10732667842244206, 0.3352437590148318, 0.2071425390746963, 0.0697960174911019]
line 256 mcts: sample exp_bonus 46.030783077724955
Printing some Q and Qe and total Qs values:  [[-0.036]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]] [[41.356]
 [41.356]
 [41.356]
 [41.356]
 [41.356]
 [41.356]
 [41.356]] [[1.297]
 [1.297]
 [1.297]
 [1.297]
 [1.297]
 [1.297]
 [1.297]]
maxi score, test score, baseline:  -0.8893133333333334 0.10466666666666669 0.10466666666666669
probs:  [0.04800279977795354, 0.23248820621897454, 0.10732667842244206, 0.3352437590148318, 0.2071425390746963, 0.0697960174911019]
maxi score, test score, baseline:  -0.8893133333333334 0.10466666666666669 0.10466666666666669
probs:  [0.0480182304301197, 0.2325631053384065, 0.10703914185641804, 0.33535178116196385, 0.20720926807375953, 0.06981847313933248]
Printing some Q and Qe and total Qs values:  [[-0.042]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.046]] [[61.512]
 [54.422]
 [54.422]
 [54.422]
 [54.422]
 [54.422]
 [54.422]] [[1.257]
 [1.049]
 [1.049]
 [1.049]
 [1.049]
 [1.049]
 [1.049]]
Printing some Q and Qe and total Qs values:  [[0.061]
 [0.061]
 [0.061]
 [0.061]
 [0.061]
 [0.061]
 [0.061]] [[59.144]
 [59.144]
 [59.144]
 [59.144]
 [59.144]
 [59.144]
 [59.144]] [[1.394]
 [1.394]
 [1.394]
 [1.394]
 [1.394]
 [1.394]
 [1.394]]
maxi score, test score, baseline:  -0.8893133333333334 0.10466666666666669 0.10466666666666669
probs:  [0.0480182304301197, 0.2325631053384065, 0.10703914185641804, 0.33535178116196385, 0.20720926807375953, 0.06981847313933248]
actions average: 
K:  0  action  0 :  tensor([0.5733, 0.0091, 0.0699, 0.0723, 0.1391, 0.0683, 0.0680],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0059, 0.9354, 0.0148, 0.0157, 0.0011, 0.0038, 0.0233],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1496, 0.0643, 0.4292, 0.0702, 0.0910, 0.1137, 0.0819],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2156, 0.0525, 0.0896, 0.3219, 0.0758, 0.1225, 0.1221],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1268, 0.0171, 0.0522, 0.0670, 0.5475, 0.1289, 0.0606],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.2113, 0.0035, 0.1242, 0.1087, 0.1139, 0.3211, 0.1173],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2477, 0.0817, 0.1281, 0.1233, 0.1283, 0.1311, 0.1598],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]] [[13.79 ]
 [11.562]
 [11.562]
 [11.562]
 [11.562]
 [11.562]
 [11.562]] [[0.444]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]]
printing an ep nov before normalisation:  24.92680585248446
actor:  1 policy actor:  1  step number:  63 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  37 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8893133333333334 0.10466666666666669 0.10466666666666669
probs:  [0.045357888369995845, 0.2752989068314683, 0.10106621063121615, 0.31672819691461945, 0.19561421975898294, 0.06593457749371735]
actor:  0 policy actor:  0  step number:  64 total reward:  0.2333333333333325  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.8868466666666667 0.10466666666666669 0.10466666666666669
probs:  [0.045357888369995845, 0.2752989068314683, 0.10106621063121615, 0.31672819691461945, 0.19561421975898294, 0.06593457749371735]
printing an ep nov before normalisation:  28.687234071312883
printing an ep nov before normalisation:  47.82001206655403
Printing some Q and Qe and total Qs values:  [[-0.049]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]] [[63.057]
 [61.526]
 [61.526]
 [61.526]
 [61.526]
 [61.526]
 [61.526]] [[0.943]
 [0.91 ]
 [0.91 ]
 [0.91 ]
 [0.91 ]
 [0.91 ]
 [0.91 ]]
printing an ep nov before normalisation:  37.08611011505127
maxi score, test score, baseline:  -0.8868466666666667 0.10466666666666669 0.10466666666666669
maxi score, test score, baseline:  -0.8868466666666667 0.10466666666666669 0.10466666666666669
probs:  [0.045357888369995845, 0.2752989068314683, 0.10106621063121615, 0.31672819691461945, 0.19561421975898294, 0.06593457749371735]
printing an ep nov before normalisation:  29.24939298659701
maxi score, test score, baseline:  -0.8868466666666667 0.10466666666666669 0.10466666666666669
probs:  [0.045357888369995845, 0.2752989068314683, 0.10106621063121615, 0.31672819691461945, 0.19561421975898294, 0.06593457749371735]
printing an ep nov before normalisation:  33.7963010288631
maxi score, test score, baseline:  -0.8868466666666667 0.10466666666666669 0.10466666666666669
actor:  1 policy actor:  1  step number:  43 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.8868466666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04476416626496728, 0.27168645422346055, 0.09974113396924533, 0.31257184890888073, 0.20616567781944822, 0.06507071881399798]
actor:  1 policy actor:  1  step number:  53 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8868466666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04488395656662506, 0.2711020647846905, 0.09969032107808415, 0.3134115523913839, 0.20578461086493433, 0.06512749431428212]
printing an ep nov before normalisation:  35.27300061117412
printing an ep nov before normalisation:  56.271276524202776
maxi score, test score, baseline:  -0.8868466666666667 0.10466666666666669 0.10466666666666669
probs:  [0.044833959559962513, 0.27134597203143224, 0.09971152890895836, 0.31306108444841796, 0.2059436571883849, 0.06510379786284412]
printing an ep nov before normalisation:  74.70467485390914
siam score:  -0.81977725
printing an ep nov before normalisation:  84.1746220179869
maxi score, test score, baseline:  -0.8868466666666667 0.10466666666666669 0.10466666666666669
probs:  [0.044833959559962513, 0.27134597203143224, 0.09971152890895836, 0.31306108444841796, 0.2059436571883849, 0.06510379786284412]
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.765]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]] [[46.817]
 [49.849]
 [36.747]
 [36.747]
 [36.747]
 [36.747]
 [36.747]] [[0.578]
 [0.765]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]]
printing an ep nov before normalisation:  62.19873270209973
printing an ep nov before normalisation:  30.33165209151534
printing an ep nov before normalisation:  29.49514670773824
using explorer policy with actor:  1
printing an ep nov before normalisation:  67.9397931373096
printing an ep nov before normalisation:  51.91403220121447
actions average: 
K:  4  action  0 :  tensor([0.5512, 0.0218, 0.0844, 0.0807, 0.1195, 0.0797, 0.0628],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0703, 0.7810, 0.0333, 0.0395, 0.0174, 0.0196, 0.0390],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2215, 0.0316, 0.3758, 0.0615, 0.0736, 0.1797, 0.0563],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1500, 0.0294, 0.0991, 0.3669, 0.1184, 0.1522, 0.0841],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1314, 0.0025, 0.0613, 0.0738, 0.5919, 0.0956, 0.0435],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.3213, 0.0151, 0.1607, 0.1130, 0.1173, 0.1369, 0.1357],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.3273, 0.3033, 0.0639, 0.1118, 0.0642, 0.0792, 0.0503],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[-0.081]
 [-0.085]
 [-0.101]
 [-0.101]
 [-0.101]
 [-0.101]
 [-0.101]] [[52.301]
 [48.117]
 [49.147]
 [49.147]
 [49.147]
 [49.147]
 [49.147]] [[1.306]
 [1.154]
 [1.174]
 [1.174]
 [1.174]
 [1.174]
 [1.174]]
maxi score, test score, baseline:  -0.8868466666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04484727184638302, 0.27142674166358915, 0.09944361316188831, 0.3131542772219794, 0.20600494941395225, 0.06512314669220777]
actor:  0 policy actor:  0  step number:  71 total reward:  0.1599999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8845266666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04485290786650719, 0.2714609370871159, 0.0994561308246308, 0.3131937322284771, 0.2060308986916226, 0.06500539330164641]
printing an ep nov before normalisation:  34.827122999711435
maxi score, test score, baseline:  -0.8845266666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04487723321679303, 0.27160852627898086, 0.09951015769541248, 0.3133640220486765, 0.2055993801278167, 0.06504068063232035]
maxi score, test score, baseline:  -0.8845266666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04487723321679303, 0.27160852627898086, 0.09951015769541248, 0.3133640220486765, 0.2055993801278167, 0.06504068063232035]
maxi score, test score, baseline:  -0.8845266666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04487723321679303, 0.27160852627898086, 0.09951015769541248, 0.3133640220486765, 0.2055993801278167, 0.06504068063232035]
printing an ep nov before normalisation:  55.57947807829644
Printing some Q and Qe and total Qs values:  [[-0.143]
 [-0.131]
 [-0.145]
 [-0.145]
 [-0.145]
 [-0.145]
 [-0.145]] [[70.463]
 [63.812]
 [65.093]
 [65.093]
 [65.093]
 [65.093]
 [65.093]] [[0.353]
 [0.297]
 [0.296]
 [0.296]
 [0.296]
 [0.296]
 [0.296]]
maxi score, test score, baseline:  -0.8845266666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04487723321679303, 0.27160852627898086, 0.09951015769541248, 0.3133640220486765, 0.2055993801278167, 0.06504068063232035]
printing an ep nov before normalisation:  19.609957933425903
printing an ep nov before normalisation:  16.971327558679924
maxi score, test score, baseline:  -0.8845266666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04487723321679303, 0.27160852627898086, 0.09951015769541248, 0.3133640220486765, 0.2055993801278167, 0.06504068063232035]
printing an ep nov before normalisation:  32.559370921010526
maxi score, test score, baseline:  -0.8845266666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04487723321679303, 0.27160852627898086, 0.09951015769541248, 0.3133640220486765, 0.2055993801278167, 0.06504068063232035]
printing an ep nov before normalisation:  21.84391927014414
maxi score, test score, baseline:  -0.8845266666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04492208052665141, 0.2708791207922666, 0.09960976406433027, 0.31367797602025244, 0.20580532065424448, 0.06510573794225478]
printing an ep nov before normalisation:  27.4169848171679
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  29.1633566773899
maxi score, test score, baseline:  -0.8845266666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04492208052665141, 0.2708791207922666, 0.09960976406433027, 0.31367797602025244, 0.20580532065424448, 0.06510573794225478]
actor:  1 policy actor:  1  step number:  56 total reward:  0.32666666666666655  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8845266666666667 0.10466666666666669 0.10466666666666669
probs:  [0.045322426445637214, 0.26892644332408183, 0.09944061501116085, 0.3164843207472372, 0.20453029489116628, 0.06529589958071662]
printing an ep nov before normalisation:  38.08552637947004
printing an ep nov before normalisation:  32.829477005593425
actor:  1 policy actor:  1  step number:  57 total reward:  0.2933333333333328  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  29.165892601013184
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.8845266666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04504056044478804, 0.26724985534456214, 0.10505366666143098, 0.3145110922250879, 0.2032553755614989, 0.06488944976263207]
maxi score, test score, baseline:  -0.8845266666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04504056044478804, 0.26724985534456214, 0.10505366666143098, 0.3145110922250879, 0.2032553755614989, 0.06488944976263207]
Printing some Q and Qe and total Qs values:  [[0.62]
 [0.62]
 [0.62]
 [0.62]
 [0.62]
 [0.62]
 [0.62]] [[32.074]
 [32.074]
 [32.074]
 [32.074]
 [32.074]
 [32.074]
 [32.074]] [[0.62]
 [0.62]
 [0.62]
 [0.62]
 [0.62]
 [0.62]
 [0.62]]
siam score:  -0.83507246
actor:  0 policy actor:  1  step number:  53 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  48 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.167]
 [-0.167]
 [-0.167]
 [-0.167]
 [-0.167]
 [-0.167]
 [-0.167]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.167]
 [-0.167]
 [-0.167]
 [-0.167]
 [-0.167]
 [-0.167]
 [-0.167]]
Printing some Q and Qe and total Qs values:  [[-0.147]
 [-0.147]
 [-0.234]
 [-0.156]
 [-0.163]
 [-0.268]
 [-0.147]] [[42.482]
 [54.513]
 [38.822]
 [41.267]
 [61.418]
 [33.999]
 [42.482]] [[0.317]
 [0.57 ]
 [0.152]
 [0.283]
 [0.7  ]
 [0.017]
 [0.317]]
printing an ep nov before normalisation:  85.60901558028469
actor:  1 policy actor:  1  step number:  51 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.147]
 [-0.139]
 [-0.138]
 [-0.145]
 [-0.138]
 [-0.142]
 [-0.142]] [[48.379]
 [47.073]
 [46.65 ]
 [47.408]
 [47.544]
 [49.45 ]
 [49.555]] [[1.493]
 [1.416]
 [1.391]
 [1.432]
 [1.448]
 [1.567]
 [1.574]]
maxi score, test score, baseline:  -0.87926 0.10466666666666669 0.10466666666666669
probs:  [0.04459416160297395, 0.27452709555680976, 0.10401071088991802, 0.3113860366161524, 0.20123625118872857, 0.06424574414541727]
maxi score, test score, baseline:  -0.87926 0.10466666666666669 0.10466666666666669
probs:  [0.04459416160297395, 0.27452709555680976, 0.10401071088991802, 0.3113860366161524, 0.20123625118872857, 0.06424574414541727]
maxi score, test score, baseline:  -0.87926 0.10466666666666669 0.10466666666666669
probs:  [0.04459416160297395, 0.27452709555680976, 0.10401071088991802, 0.3113860366161524, 0.20123625118872857, 0.06424574414541727]
printing an ep nov before normalisation:  46.25880282646431
printing an ep nov before normalisation:  57.24705268375459
maxi score, test score, baseline:  -0.87926 0.10466666666666669 0.10466666666666669
probs:  [0.04462306014556745, 0.27470544499844024, 0.10407822876622727, 0.3115883434865172, 0.20083983630663207, 0.06416508629661578]
actor:  1 policy actor:  1  step number:  75 total reward:  0.19999999999999918  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  44.22547516742703
maxi score, test score, baseline:  -0.87926 0.10466666666666669 0.10466666666666669
probs:  [0.044655460533517245, 0.27454543270818305, 0.10406090814561371, 0.3118154583321924, 0.20074159614901715, 0.06418114413147652]
siam score:  -0.8232686
maxi score, test score, baseline:  -0.87926 0.10466666666666669 0.10466666666666669
probs:  [0.044655460533517245, 0.27454543270818305, 0.10406090814561371, 0.3118154583321924, 0.20074159614901715, 0.06418114413147652]
maxi score, test score, baseline:  -0.87926 0.10466666666666669 0.10466666666666669
probs:  [0.04467881112209547, 0.2746893537556661, 0.10411541512614883, 0.31197892640058444, 0.20032275823463622, 0.06421473536086898]
printing an ep nov before normalisation:  36.74353594167759
printing an ep nov before normalisation:  0.22944081346821577
actor:  1 policy actor:  1  step number:  69 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.023]
 [0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.023]
 [0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]]
printing an ep nov before normalisation:  57.92561685447445
Printing some Q and Qe and total Qs values:  [[-0.148]
 [-0.143]
 [-0.148]
 [-0.148]
 [-0.148]
 [-0.148]
 [-0.148]] [[37.959]
 [54.753]
 [37.959]
 [37.959]
 [37.959]
 [37.959]
 [37.959]] [[0.068]
 [0.229]
 [0.068]
 [0.068]
 [0.068]
 [0.068]
 [0.068]]
maxi score, test score, baseline:  -0.87926 0.10466666666666669 0.10466666666666669
maxi score, test score, baseline:  -0.87926 0.10466666666666669 0.10466666666666669
printing an ep nov before normalisation:  0.0
Printing some Q and Qe and total Qs values:  [[0.104]
 [0.09 ]
 [0.085]
 [0.12 ]
 [0.099]
 [0.098]
 [0.105]] [[ 3.861]
 [ 5.465]
 [ 6.077]
 [ 4.068]
 [22.022]
 [10.886]
 [10.809]] [[0.104]
 [0.09 ]
 [0.085]
 [0.12 ]
 [0.099]
 [0.098]
 [0.105]]
using another actor
printing an ep nov before normalisation:  43.63552966200006
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.472]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]] [[44.045]
 [43.424]
 [37.216]
 [37.216]
 [37.216]
 [37.216]
 [37.216]] [[0.455]
 [0.472]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]]
maxi score, test score, baseline:  -0.87926 0.10466666666666669 0.10466666666666669
probs:  [0.04500609418019687, 0.2727194984196114, 0.10380045857101763, 0.3142725374436713, 0.19976986930436016, 0.06443154208114274]
printing an ep nov before normalisation:  35.89389085769653
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.87926 0.10466666666666669 0.10466666666666669
maxi score, test score, baseline:  -0.87926 0.10466666666666669 0.10466666666666669
probs:  [0.04500609418019687, 0.2727194984196114, 0.10380045857101763, 0.3142725374436713, 0.19976986930436016, 0.06443154208114274]
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.579]
 [0.566]
 [0.519]
 [0.566]
 [0.53 ]
 [0.566]] [[34.652]
 [35.508]
 [40.518]
 [35.33 ]
 [40.518]
 [35.887]
 [40.518]] [[0.538]
 [0.579]
 [0.566]
 [0.519]
 [0.566]
 [0.53 ]
 [0.566]]
line 256 mcts: sample exp_bonus 50.84516356687275
maxi score, test score, baseline:  -0.87926 0.10466666666666669 0.10466666666666669
probs:  [0.04500609418019687, 0.2727194984196114, 0.10380045857101763, 0.3142725374436713, 0.19976986930436016, 0.06443154208114274]
printing an ep nov before normalisation:  32.66341466490328
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  81 total reward:  0.14666666666666606  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  10.143583023319135
siam score:  -0.82439697
printing an ep nov before normalisation:  31.096968215113296
printing an ep nov before normalisation:  16.722052483265735
siam score:  -0.82585114
printing an ep nov before normalisation:  45.18277909041609
maxi score, test score, baseline:  -0.87926 0.10466666666666669 0.10466666666666669
probs:  [0.04496813960784681, 0.2730327460739717, 0.10385318250642186, 0.3140063861546068, 0.1997159982934257, 0.0644235473637272]
actor:  0 policy actor:  1  step number:  60 total reward:  0.28666666666666607  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8766866666666666 0.10466666666666669 0.10466666666666669
probs:  [0.04496813960784681, 0.2730327460739717, 0.10385318250642186, 0.3140063861546068, 0.1997159982934257, 0.0644235473637272]
printing an ep nov before normalisation:  34.87049341201782
maxi score, test score, baseline:  -0.8766866666666666 0.10466666666666669 0.10466666666666669
probs:  [0.04501262359514859, 0.27231212975626884, 0.10395609063985688, 0.3143178026099506, 0.1999140189186344, 0.06448733448014056]
maxi score, test score, baseline:  -0.8766866666666666 0.10466666666666669 0.10466666666666669
probs:  [0.04501262359514859, 0.27231212975626884, 0.10395609063985688, 0.3143178026099506, 0.1999140189186344, 0.06448733448014056]
siam score:  -0.82808185
actor:  0 policy actor:  0  step number:  64 total reward:  0.16666666666666619  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8743533333333334 0.10466666666666669 0.10466666666666669
probs:  [0.04501262359514859, 0.27231212975626884, 0.10395609063985688, 0.3143178026099506, 0.1999140189186344, 0.06448733448014056]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8743533333333334 0.10466666666666669 0.10466666666666669
probs:  [0.04501262359514859, 0.27231212975626884, 0.10395609063985688, 0.3143178026099506, 0.1999140189186344, 0.06448733448014056]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8743533333333334 0.10466666666666669 0.10466666666666669
probs:  [0.04567351354670403, 0.26905635278599216, 0.10360130778362159, 0.3189504203028523, 0.19790575581545736, 0.06481264976537258]
line 256 mcts: sample exp_bonus 22.81669843196869
printing an ep nov before normalisation:  48.09809084577395
maxi score, test score, baseline:  -0.8743533333333334 0.10466666666666669 0.10466666666666669
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.018]
 [0.007]
 [0.007]
 [0.007]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.007]
 [0.007]
 [0.018]
 [0.007]
 [0.007]
 [0.007]]
printing an ep nov before normalisation:  27.411932945251465
Printing some Q and Qe and total Qs values:  [[-0.157]
 [-0.153]
 [-0.157]
 [-0.157]
 [-0.157]
 [-0.157]
 [-0.157]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.157]
 [-0.153]
 [-0.157]
 [-0.157]
 [-0.157]
 [-0.157]
 [-0.157]]
Printing some Q and Qe and total Qs values:  [[-0.112]
 [-0.111]
 [-0.117]
 [-0.114]
 [-0.116]
 [-0.119]
 [-0.114]] [[45.804]
 [43.822]
 [40.017]
 [38.751]
 [42.031]
 [42.173]
 [39.107]] [[1.06 ]
 [0.973]
 [0.799]
 [0.747]
 [0.89 ]
 [0.893]
 [0.763]]
printing an ep nov before normalisation:  77.86456584790984
printing an ep nov before normalisation:  61.23068513642295
printing an ep nov before normalisation:  56.308364547120846
maxi score, test score, baseline:  -0.8743533333333334 0.10466666666666669 0.10466666666666669
probs:  [0.0457007691701044, 0.2682795965972161, 0.10378671483786873, 0.31914208620873635, 0.19809390371011018, 0.06499692947596418]
printing an ep nov before normalisation:  31.20899649855149
printing an ep nov before normalisation:  35.966790813809844
printing an ep nov before normalisation:  22.323183050643536
actor:  1 policy actor:  1  step number:  58 total reward:  0.3133333333333328  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8743533333333334 0.10466666666666669 0.10466666666666669
probs:  [0.0460199252030511, 0.26671276542588895, 0.10361368861096673, 0.32137925814371165, 0.19712178013176615, 0.06515258248461542]
maxi score, test score, baseline:  -0.8743533333333334 0.10466666666666669 0.10466666666666669
probs:  [0.0460199252030511, 0.26671276542588895, 0.10361368861096673, 0.32137925814371165, 0.19712178013176615, 0.06515258248461542]
printing an ep nov before normalisation:  87.41205658836144
actions average: 
K:  1  action  0 :  tensor([0.5745, 0.0628, 0.0770, 0.0559, 0.0807, 0.0717, 0.0774],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0094, 0.9558, 0.0053, 0.0078, 0.0026, 0.0028, 0.0164],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1115, 0.1967, 0.3446, 0.0651, 0.0432, 0.1648, 0.0742],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2965, 0.0213, 0.0935, 0.2973, 0.0907, 0.1254, 0.0754],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2048, 0.0623, 0.1019, 0.1044, 0.3155, 0.1239, 0.0873],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2189, 0.0086, 0.1712, 0.1172, 0.1178, 0.2214, 0.1449],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2601, 0.1059, 0.1624, 0.1107, 0.0882, 0.1202, 0.1525],
       grad_fn=<DivBackward0>)
siam score:  -0.83665377
maxi score, test score, baseline:  -0.8743533333333334 0.10466666666666669 0.10466666666666669
probs:  [0.0460199252030511, 0.26671276542588895, 0.10361368861096673, 0.32137925814371165, 0.19712178013176615, 0.06515258248461542]
maxi score, test score, baseline:  -0.8743533333333334 0.10466666666666669 0.10466666666666669
probs:  [0.0460199252030511, 0.26671276542588895, 0.10361368861096673, 0.32137925814371165, 0.19712178013176615, 0.06515258248461542]
actor:  1 policy actor:  1  step number:  48 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8743533333333334 0.10466666666666669 0.10466666666666669
maxi score, test score, baseline:  -0.8743533333333334 0.10466666666666669 0.10466666666666669
probs:  [0.046595436800892114, 0.2638874091177781, 0.10330168260126239, 0.32541339274256376, 0.1953688181441569, 0.06543326059334666]
actor:  1 policy actor:  1  step number:  66 total reward:  0.1799999999999996  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  24.32041645050049
maxi score, test score, baseline:  -0.8743533333333334 0.10466666666666669 0.10466666666666669
probs:  [0.046587162537957735, 0.26401838173733233, 0.10328330975741075, 0.32535546494235995, 0.19533404944469937, 0.0654216315802399]
printing an ep nov before normalisation:  58.7365417986048
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8743533333333334 0.10466666666666669 0.10466666666666669
probs:  [0.046587162537957735, 0.26401838173733233, 0.10328330975741075, 0.32535546494235995, 0.19533404944469937, 0.0654216315802399]
maxi score, test score, baseline:  -0.8743533333333334 0.10466666666666669 0.10466666666666669
Printing some Q and Qe and total Qs values:  [[0.232]
 [0.395]
 [0.266]
 [0.204]
 [0.258]
 [0.254]
 [0.241]] [[29.998]
 [24.108]
 [26.39 ]
 [30.672]
 [29.11 ]
 [29.262]
 [27.034]] [[1.122]
 [0.963]
 [0.959]
 [1.13 ]
 [1.099]
 [1.104]
 [0.969]]
printing an ep nov before normalisation:  40.45450269439584
printing an ep nov before normalisation:  24.093004244954358
printing an ep nov before normalisation:  23.517509006593382
printing an ep nov before normalisation:  29.834865731074846
maxi score, test score, baseline:  -0.8743533333333334 0.10466666666666669 0.10466666666666669
probs:  [0.046587162537957735, 0.26401838173733233, 0.10328330975741075, 0.32535546494235995, 0.19533404944469937, 0.0654216315802399]
actor:  1 policy actor:  1  step number:  77 total reward:  0.1466666666666665  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  44.297367534178456
actor:  0 policy actor:  0  step number:  67 total reward:  0.173333333333333  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  55 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8720066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04674349800611851, 0.2632505007276062, 0.1031986517119176, 0.326451321477059, 0.19485811911211728, 0.06549790896518144]
maxi score, test score, baseline:  -0.8720066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04675708653820154, 0.26332720865963094, 0.10293739680423969, 0.32654645466938187, 0.19491488826140566, 0.06551696506714033]
maxi score, test score, baseline:  -0.8720066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04676249737791268, 0.2633577531159579, 0.10294932753859627, 0.32658433591010183, 0.19493749326137622, 0.06540859279605514]
maxi score, test score, baseline:  -0.8720066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04676249737791268, 0.2633577531159579, 0.10294932753859627, 0.32658433591010183, 0.19493749326137622, 0.06540859279605514]
printing an ep nov before normalisation:  42.61246314775377
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]] [[57.536]
 [56.552]
 [56.552]
 [56.552]
 [56.552]
 [56.552]
 [56.552]] [[0.457]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]]
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]] [[29.989]
 [29.989]
 [29.989]
 [29.989]
 [29.989]
 [29.989]
 [29.989]] [[0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]]
maxi score, test score, baseline:  -0.8720066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04676249737791268, 0.2633577531159579, 0.10294932753859627, 0.32658433591010183, 0.19493749326137622, 0.06540859279605514]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.77 ]
 [0.753]
 [0.365]
 [0.659]
 [0.611]
 [0.632]
 [0.619]] [[41.062]
 [32.962]
 [29.809]
 [26.05 ]
 [32.926]
 [35.439]
 [24.58 ]] [[0.77 ]
 [0.753]
 [0.365]
 [0.659]
 [0.611]
 [0.632]
 [0.619]]
Printing some Q and Qe and total Qs values:  [[0.114]
 [0.274]
 [0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]] [[18.027]
 [30.58 ]
 [18.027]
 [18.027]
 [18.027]
 [18.027]
 [18.027]] [[0.481]
 [1.141]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]]
printing an ep nov before normalisation:  46.65983759486782
line 256 mcts: sample exp_bonus 36.404851903205554
actor:  1 policy actor:  1  step number:  61 total reward:  0.1733333333333329  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.8720066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.046878614340928645, 0.2637788167738661, 0.10239023525237237, 0.32739173782131425, 0.19494048334145242, 0.06462011247006619]
siam score:  -0.83530015
maxi score, test score, baseline:  -0.8720066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04690189796472658, 0.2639101372110401, 0.10244116891319813, 0.32755474350913544, 0.19453981936412829, 0.06465223303777146]
maxi score, test score, baseline:  -0.8720066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04690189796472658, 0.2639101372110401, 0.10244116891319813, 0.32755474350913544, 0.19453981936412829, 0.06465223303777146]
printing an ep nov before normalisation:  23.197638988494873
Printing some Q and Qe and total Qs values:  [[-0.137]
 [-0.148]
 [-0.148]
 [-0.148]
 [-0.148]
 [-0.148]
 [-0.148]] [[49.617]
 [24.822]
 [24.822]
 [24.822]
 [24.822]
 [24.822]
 [24.822]] [[0.323]
 [0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]]
Printing some Q and Qe and total Qs values:  [[-0.132]
 [-0.132]
 [-0.132]
 [-0.132]
 [-0.132]
 [-0.132]
 [-0.132]] [[61.808]
 [61.808]
 [61.808]
 [61.808]
 [61.808]
 [61.808]
 [61.808]] [[0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]]
printing an ep nov before normalisation:  43.650856970517594
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  78 total reward:  0.046666666666665524  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.8720066666666667 0.10466666666666669 0.10466666666666669
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[-0.02 ]
 [ 0.04 ]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]] [[63.332]
 [67.083]
 [60.29 ]
 [60.29 ]
 [60.29 ]
 [60.29 ]
 [60.29 ]] [[0.792]
 [0.919]
 [0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]]
maxi score, test score, baseline:  -0.8720066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.046948152218639974, 0.2641710127902789, 0.10254235155130542, 0.32787856365582907, 0.19374387739642024, 0.06471604238752626]
Printing some Q and Qe and total Qs values:  [[-0.12 ]
 [-0.129]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]] [[23.691]
 [27.22 ]
 [22.502]
 [22.502]
 [22.502]
 [22.502]
 [22.502]] [[0.43 ]
 [0.538]
 [0.464]
 [0.464]
 [0.464]
 [0.464]
 [0.464]]
printing an ep nov before normalisation:  33.44562128077176
printing an ep nov before normalisation:  13.754395810744997
maxi score, test score, baseline:  -0.8720066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.046948152218639974, 0.2641710127902789, 0.10254235155130542, 0.32787856365582907, 0.19374387739642024, 0.06471604238752626]
Printing some Q and Qe and total Qs values:  [[-0.073]
 [-0.082]
 [-0.082]
 [-0.082]
 [-0.082]
 [-0.082]
 [-0.082]] [[26.251]
 [21.334]
 [21.334]
 [21.334]
 [21.334]
 [21.334]
 [21.334]] [[0.474]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]]
maxi score, test score, baseline:  -0.8720066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.04696181933765356, 0.2642480958104887, 0.1022805428174575, 0.3279742454171682, 0.19380039996434653, 0.06473489665288555]
maxi score, test score, baseline:  -0.8720066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.047005804907454006, 0.26355763802221804, 0.10237648864142189, 0.3282821827950331, 0.1939823093597612, 0.06479557627411167]
printing an ep nov before normalisation:  52.5998022977518
Printing some Q and Qe and total Qs values:  [[-0.111]
 [-0.109]
 [-0.109]
 [-0.111]
 [-0.111]
 [-0.112]
 [-0.111]] [[33.08 ]
 [32.801]
 [32.02 ]
 [32.354]
 [32.9  ]
 [33.037]
 [32.789]] [[1.222]
 [1.203]
 [1.141]
 [1.165]
 [1.208]
 [1.218]
 [1.199]]
maxi score, test score, baseline:  -0.8720066666666667 0.10466666666666669 0.10466666666666669
probs:  [0.047005804907454006, 0.26355763802221804, 0.10237648864142189, 0.3282821827950331, 0.1939823093597612, 0.06479557627411167]
actor:  0 policy actor:  0  step number:  60 total reward:  0.13999999999999935  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
line 256 mcts: sample exp_bonus 35.674189423668416
printing an ep nov before normalisation:  0.00015292455657345272
maxi score, test score, baseline:  -0.8697266666666668 0.10466666666666669 0.10466666666666669
probs:  [0.04700174657599599, 0.2631072972546484, 0.102497041514335, 0.32825334072582635, 0.19430902027294636, 0.06483155365624782]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  40 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  62.97032327894031
maxi score, test score, baseline:  -0.8697266666666668 0.10466666666666669 0.10466666666666669
probs:  [0.04601476229876807, 0.25756943785821096, 0.12138314776574481, 0.32134359843504906, 0.19021995460705998, 0.06346909903516704]
Printing some Q and Qe and total Qs values:  [[-0.023]
 [ 0.04 ]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.024]] [[40.859]
 [40.12 ]
 [38.26 ]
 [38.26 ]
 [38.26 ]
 [38.26 ]
 [38.26 ]] [[0.499]
 [0.547]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]]
Printing some Q and Qe and total Qs values:  [[-0.164]
 [-0.162]
 [-0.164]
 [-0.158]
 [-0.162]
 [-0.164]
 [-0.164]] [[26.552]
 [42.232]
 [26.552]
 [15.962]
 [23.019]
 [26.552]
 [26.552]] [[ 0.34 ]
 [ 0.922]
 [ 0.34 ]
 [-0.046]
 [ 0.211]
 [ 0.34 ]
 [ 0.34 ]]
maxi score, test score, baseline:  -0.8697266666666668 0.10466666666666669 0.10466666666666669
probs:  [0.04600848079151059, 0.25711850945432874, 0.12154235564714667, 0.3212992011636466, 0.190530310227807, 0.06350114271556039]
maxi score, test score, baseline:  -0.8697266666666668 0.10466666666666669 0.10466666666666669
probs:  [0.046049950635432, 0.25644756919151457, 0.12165210583232439, 0.3215895255032181, 0.19070242344004168, 0.06355842539746921]
maxi score, test score, baseline:  -0.8697266666666668 0.10466666666666669 0.10466666666666669
probs:  [0.046049950635432, 0.25644756919151457, 0.12165210583232439, 0.3215895255032181, 0.19070242344004168, 0.06355842539746921]
printing an ep nov before normalisation:  16.205017566680908
actor:  1 policy actor:  1  step number:  78 total reward:  0.2599999999999991  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8697266666666668 0.10466666666666669 0.10466666666666669
maxi score, test score, baseline:  -0.8697266666666668 0.10466666666666669 0.10466666666666669
probs:  [0.04625853360871596, 0.2552320634660493, 0.12167032838092216, 0.3230510636833801, 0.1900650874859394, 0.06372292337499309]
printing an ep nov before normalisation:  38.85073422471242
printing an ep nov before normalisation:  0.003386994876564131
actor:  1 policy actor:  1  step number:  54 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  45.55929490202615
maxi score, test score, baseline:  -0.8697266666666668 0.10466666666666669 0.10466666666666669
maxi score, test score, baseline:  -0.8697266666666668 0.10466666666666669 0.10466666666666669
probs:  [0.046718700401487526, 0.2533229996525449, 0.12127551640020749, 0.3262764888559264, 0.18842120647371546, 0.06398508821611822]
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([0.6019, 0.0413, 0.0771, 0.0709, 0.0689, 0.0688, 0.0710],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0148, 0.9416, 0.0080, 0.0094, 0.0038, 0.0036, 0.0188],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2897, 0.0189, 0.2587, 0.1185, 0.1107, 0.1283, 0.0749],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.3642, 0.0095, 0.0440, 0.3737, 0.1105, 0.0481, 0.0499],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.3082, 0.0101, 0.0773, 0.0857, 0.3529, 0.0903, 0.0756],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2875, 0.0411, 0.1535, 0.0917, 0.0838, 0.2307, 0.1116],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.3682, 0.1109, 0.0864, 0.0765, 0.0661, 0.0718, 0.2200],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.8697266666666668 0.10466666666666669 0.10466666666666669
maxi score, test score, baseline:  -0.8697266666666668 0.10466666666666669 0.10466666666666669
probs:  [0.04667132195489165, 0.2535427190920253, 0.12132452495048451, 0.3259443813368905, 0.18855702092272147, 0.06396003174298649]
maxi score, test score, baseline:  -0.8697266666666668 0.10466666666666669 0.10466666666666669
probs:  [0.04667132195489165, 0.2535427190920253, 0.12132452495048451, 0.3259443813368905, 0.18855702092272147, 0.06396003174298649]
printing an ep nov before normalisation:  36.37483562422145
Printing some Q and Qe and total Qs values:  [[-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]] [[55.102]
 [55.102]
 [55.102]
 [55.102]
 [55.102]
 [55.102]
 [55.102]] [[0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]]
maxi score, test score, baseline:  -0.8697266666666668 0.10466666666666669 0.10466666666666669
probs:  [0.04667132195489165, 0.2535427190920253, 0.12132452495048451, 0.3259443813368905, 0.18855702092272147, 0.06396003174298649]
printing an ep nov before normalisation:  52.355356423422464
actor:  1 policy actor:  1  step number:  45 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  1.0
from probs:  [0.04671228920189048, 0.25288620296257225, 0.12143120902996296, 0.32623119122858557, 0.18872288942427032, 0.06401621815271837]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[-0.113]
 [-0.166]
 [-0.134]
 [-0.134]
 [-0.118]
 [-0.15 ]
 [-0.134]] [[24.605]
 [25.416]
 [29.051]
 [29.051]
 [25.417]
 [24.232]
 [29.051]] [[1.142]
 [1.164]
 [1.533]
 [1.533]
 [1.212]
 [1.07 ]
 [1.533]]
printing an ep nov before normalisation:  27.09779681035295
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.013]
 [0.005]
 [0.002]
 [0.003]
 [0.003]
 [0.082]] [[14.418]
 [33.037]
 [10.578]
 [10.534]
 [10.454]
 [10.39 ]
 [23.654]] [[0.005]
 [0.013]
 [0.005]
 [0.002]
 [0.003]
 [0.003]
 [0.082]]
printing an ep nov before normalisation:  41.33643485182873
printing an ep nov before normalisation:  51.923394799496485
printing an ep nov before normalisation:  31.950148446567525
maxi score, test score, baseline:  -0.8697266666666668 0.10466666666666669 0.10466666666666669
probs:  [0.045792561180532626, 0.24750402078851563, 0.13879727765147343, 0.3197918107829638, 0.18532036688161932, 0.06279396271489524]
printing an ep nov before normalisation:  14.83492923703239
line 256 mcts: sample exp_bonus 36.49040030284365
maxi score, test score, baseline:  -0.8697266666666668 0.10466666666666669 0.10466666666666669
probs:  [0.045792561180532626, 0.24750402078851563, 0.13879727765147343, 0.3197918107829638, 0.18532036688161932, 0.06279396271489524]
Printing some Q and Qe and total Qs values:  [[0.04 ]
 [0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.018]] [[26.978]
 [16.598]
 [16.598]
 [16.598]
 [16.598]
 [16.598]
 [16.598]] [[0.04 ]
 [0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.018]]
printing an ep nov before normalisation:  13.51138517791844
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  41.09763260776433
Printing some Q and Qe and total Qs values:  [[0.991]
 [0.951]
 [0.991]
 [0.991]
 [0.991]
 [0.991]
 [0.991]] [[42.106]
 [49.438]
 [42.106]
 [42.106]
 [42.106]
 [42.106]
 [42.106]] [[0.991]
 [0.951]
 [0.991]
 [0.991]
 [0.991]
 [0.991]
 [0.991]]
printing an ep nov before normalisation:  30.72730541229248
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  12.688384056091309
actor:  0 policy actor:  0  step number:  26 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  34.467270374298096
actor:  0 policy actor:  0  step number:  28 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  28 total reward:  0.6333333333333334  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.689]
 [0.655]
 [0.669]
 [0.589]
 [0.61 ]
 [0.587]] [[28.799]
 [37.542]
 [34.308]
 [36.117]
 [28.69 ]
 [29.037]
 [28.638]] [[0.586]
 [0.689]
 [0.655]
 [0.669]
 [0.589]
 [0.61 ]
 [0.587]]
actor:  0 policy actor:  0  step number:  29 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  29 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[0.046]
 [0.047]
 [0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.05 ]] [[12.1  ]
 [34.192]
 [12.1  ]
 [12.1  ]
 [12.1  ]
 [14.054]
 [14.289]] [[0.046]
 [0.047]
 [0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.05 ]]
actor:  0 policy actor:  0  step number:  30 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  31 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  32 total reward:  0.6066666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  32 total reward:  0.5933333333333334  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.8303000000000001 0.10466666666666669 0.10466666666666669
printing an ep nov before normalisation:  36.10200628091597
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8303000000000001 0.10466666666666669 0.10466666666666669
probs:  [0.045784083823812015, 0.24706926763498588, 0.13898574867267852, 0.3197320560596496, 0.18560735599209602, 0.06282148781677792]
printing an ep nov before normalisation:  43.43323449004044
Printing some Q and Qe and total Qs values:  [[0.728]
 [0.762]
 [0.73 ]
 [0.884]
 [0.675]
 [0.626]
 [0.774]] [[40.757]
 [28.498]
 [35.751]
 [34.185]
 [44.32 ]
 [41.942]
 [23.816]] [[0.728]
 [0.762]
 [0.73 ]
 [0.884]
 [0.675]
 [0.626]
 [0.774]]
actor:  0 policy actor:  0  step number:  53 total reward:  0.48  reward:  1.0 rdn_beta:  2
siam score:  -0.8418106
Printing some Q and Qe and total Qs values:  [[0.682]
 [0.719]
 [0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.682]] [[26.219]
 [39.532]
 [26.219]
 [26.219]
 [26.219]
 [26.219]
 [26.219]] [[0.682]
 [0.719]
 [0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.682]]
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.655]
 [0.574]
 [0.496]
 [0.574]
 [0.524]
 [0.496]] [[34.388]
 [33.496]
 [35.463]
 [36.035]
 [35.463]
 [28.04 ]
 [23.92 ]] [[0.466]
 [0.655]
 [0.574]
 [0.496]
 [0.574]
 [0.524]
 [0.496]]
Printing some Q and Qe and total Qs values:  [[-0.158]
 [-0.14 ]
 [-0.155]
 [-0.158]
 [-0.154]
 [-0.154]
 [-0.153]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.158]
 [-0.14 ]
 [-0.155]
 [-0.158]
 [-0.154]
 [-0.154]
 [-0.153]]
actor:  0 policy actor:  0  step number:  55 total reward:  0.48  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  74.92598920569816
actor:  0 policy actor:  0  step number:  55 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.8214466666666668 0.10466666666666669 0.10466666666666669
probs:  [0.04582282378139836, 0.24643088534573782, 0.13910358087025362, 0.32000327210872687, 0.18576475193953282, 0.06287468595435045]
actor:  1 policy actor:  1  step number:  67 total reward:  0.29333333333333234  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  58 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  61 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  62 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  76.57699290000124
actor:  0 policy actor:  0  step number:  70 total reward:  0.33999999999999986  reward:  1.0 rdn_beta:  2
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 45.18880768231163
printing an ep nov before normalisation:  31.041488183912808
Printing some Q and Qe and total Qs values:  [[-0.1  ]
 [-0.101]
 [-0.101]
 [-0.101]
 [-0.101]
 [-0.101]
 [-0.101]] [[50.325]
 [42.501]
 [42.501]
 [42.501]
 [42.501]
 [42.501]
 [42.501]] [[1.375]
 [1.14 ]
 [1.14 ]
 [1.14 ]
 [1.14 ]
 [1.14 ]
 [1.14 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.81026 0.4866666666666667 0.4866666666666667
maxi score, test score, baseline:  -0.81026 0.4866666666666667 0.4866666666666667
maxi score, test score, baseline:  -0.81026 0.4866666666666667 0.4866666666666667
probs:  [0.0744262610453579, 0.5169007706186326, 0.0744262610453579, 0.18539418519993578, 0.0744262610453579, 0.0744262610453579]
actor:  1 policy actor:  1  step number:  48 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 42.001274406763685
printing an ep nov before normalisation:  56.03871318298971
maxi score, test score, baseline:  -0.81026 0.4866666666666667 0.4866666666666667
probs:  [0.07444320687915273, 0.5170186424620996, 0.07444320687915273, 0.18520853002128948, 0.07444320687915273, 0.07444320687915273]
printing an ep nov before normalisation:  38.80117034420789
actor:  1 policy actor:  1  step number:  48 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.667
from probs:  [0.07444320687915273, 0.5170186424620996, 0.07444320687915273, 0.18520853002128948, 0.07444320687915273, 0.07444320687915273]
printing an ep nov before normalisation:  27.9508376121521
maxi score, test score, baseline:  -0.81026 0.4866666666666667 0.4866666666666667
maxi score, test score, baseline:  -0.81026 0.4866666666666667 0.4866666666666667
probs:  [0.07441448517968707, 0.5168057330672727, 0.07441448517968707, 0.18553632621397897, 0.07441448517968707, 0.07441448517968707]
maxi score, test score, baseline:  -0.81026 0.4866666666666667 0.4866666666666667
probs:  [0.07441448517968707, 0.5168057330672727, 0.07441448517968707, 0.18553632621397897, 0.07441448517968707, 0.07441448517968707]
maxi score, test score, baseline:  -0.81026 0.4866666666666667 0.4866666666666667
probs:  [0.07441448517968707, 0.5168057330672727, 0.07441448517968707, 0.18553632621397897, 0.07441448517968707, 0.07441448517968707]
maxi score, test score, baseline:  -0.81026 0.4866666666666667 0.4866666666666667
probs:  [0.07441448517968707, 0.5168057330672727, 0.07441448517968707, 0.18553632621397897, 0.07441448517968707, 0.07441448517968707]
siam score:  -0.83832216
line 256 mcts: sample exp_bonus 25.207600593566895
printing an ep nov before normalisation:  34.8181187539422
actor:  1 policy actor:  1  step number:  44 total reward:  0.5666666666666668  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.81026 0.4866666666666667 0.4866666666666667
maxi score, test score, baseline:  -0.81026 0.4866666666666667 0.4866666666666667
probs:  [0.07670663561341366, 0.5337970800549566, 0.07670663561341366, 0.15937637749138875, 0.07670663561341366, 0.07670663561341366]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.093]
 [-0.104]
 [-0.104]
 [-0.104]
 [-0.104]
 [-0.104]
 [-0.104]] [[38.306]
 [33.217]
 [33.217]
 [33.217]
 [33.217]
 [33.217]
 [33.217]] [[1.229]
 [0.937]
 [0.937]
 [0.937]
 [0.937]
 [0.937]
 [0.937]]
actor:  0 policy actor:  1  step number:  55 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  0.667
UNIT TEST: sample policy line 217 mcts : [0.694 0.163 0.02  0.02  0.02  0.061 0.02 ]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  0.4266666666666661  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  53.34508011751108
siam score:  -0.84360474
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
probs:  [0.07667590556982319, 0.5335634862188983, 0.07667590556982319, 0.15973289150180883, 0.07667590556982319, 0.07667590556982319]
actor:  1 policy actor:  1  step number:  75 total reward:  0.38666666666666616  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 0.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
probs:  [0.07667590556982319, 0.5335634862188983, 0.07667590556982319, 0.15973289150180883, 0.07667590556982319, 0.07667590556982319]
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
probs:  [0.07667590556982319, 0.5335634862188983, 0.07667590556982319, 0.15973289150180883, 0.07667590556982319, 0.07667590556982319]
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]] [[25.293]
 [19.735]
 [19.735]
 [19.735]
 [19.735]
 [19.735]
 [19.735]] [[2.057]
 [1.778]
 [1.778]
 [1.778]
 [1.778]
 [1.778]
 [1.778]]
printing an ep nov before normalisation:  20.506819723423405
printing an ep nov before normalisation:  11.731121313850963
printing an ep nov before normalisation:  38.495035886062034
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.664]
 [0.555]
 [0.435]
 [0.554]
 [0.56 ]
 [0.475]] [[24.931]
 [12.378]
 [16.421]
 [27.158]
 [11.003]
 [11.132]
 [15.718]] [[1.371]
 [1.097]
 [1.137]
 [1.415]
 [0.935]
 [0.946]
 [1.031]]
printing an ep nov before normalisation:  84.71365805002661
actor:  1 policy actor:  1  step number:  86 total reward:  0.17999999999999838  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
probs:  [0.07663224459871133, 0.5332397965487781, 0.07663224459871133, 0.16023122505637646, 0.07663224459871133, 0.07663224459871133]
actions average: 
K:  3  action  0 :  tensor([0.5856, 0.0881, 0.0558, 0.0661, 0.0850, 0.0587, 0.0606],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0132, 0.9085, 0.0176, 0.0264, 0.0051, 0.0111, 0.0180],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1586, 0.0163, 0.5333, 0.0351, 0.0351, 0.1683, 0.0533],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1355, 0.1570, 0.0823, 0.2730, 0.0974, 0.1385, 0.1164],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1937, 0.0720, 0.0918, 0.0751, 0.3866, 0.1084, 0.0724],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1654, 0.0319, 0.1081, 0.1030, 0.1701, 0.3479, 0.0736],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2216, 0.3221, 0.0841, 0.0961, 0.0800, 0.0692, 0.1268],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  54.360856479530185
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  62 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.212]
 [0.346]] [[27.096]
 [27.096]
 [27.096]
 [27.096]
 [27.096]
 [35.038]
 [27.096]] [[0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.879]
 [0.718]]
printing an ep nov before normalisation:  37.975094668164814
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
probs:  [0.07663224459871133, 0.5332397965487781, 0.07663224459871133, 0.16023122505637646, 0.07663224459871133, 0.07663224459871133]
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
probs:  [0.07661043417702859, 0.5330781004614056, 0.07661043417702859, 0.16048016283048, 0.07661043417702859, 0.07661043417702859]
actor:  1 policy actor:  1  step number:  42 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
probs:  [0.076588637114875, 0.5329165034176749, 0.076588637114875, 0.160728948122825, 0.076588637114875, 0.076588637114875]
actor:  1 policy actor:  1  step number:  57 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
probs:  [0.07656685339997966, 0.5327550053266132, 0.07656685339997966, 0.16097758107346796, 0.07656685339997966, 0.07656685339997966]
printing an ep nov before normalisation:  36.533223580179495
printing an ep nov before normalisation:  42.45052455826818
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
probs:  [0.07656685339997966, 0.5327550053266132, 0.07656685339997966, 0.16097758107346796, 0.07656685339997966, 0.07656685339997966]
actor:  1 policy actor:  1  step number:  45 total reward:  0.38666666666666616  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
printing an ep nov before normalisation:  39.08534288406372
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  54 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
probs:  [0.07652332596295525, 0.5324323056391621, 0.07652332596295525, 0.1614743905090168, 0.07652332596295525, 0.07652332596295525]
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
siam score:  -0.84902656
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
probs:  [0.07652332596295525, 0.5324323056391621, 0.07652332596295525, 0.1614743905090168, 0.07652332596295525, 0.07652332596295525]
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
printing an ep nov before normalisation:  24.178868534339696
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
probs:  [0.07652332596295525, 0.5324323056391621, 0.07652332596295525, 0.1614743905090168, 0.07652332596295525, 0.07652332596295525]
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
probs:  [0.07652332596295525, 0.5324323056391621, 0.07652332596295525, 0.1614743905090168, 0.07652332596295525, 0.07652332596295525]
printing an ep nov before normalisation:  28.981526992900335
actor:  1 policy actor:  1  step number:  52 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
probs:  [0.07653660968719707, 0.5325248688672655, 0.07653660968719707, 0.16132869238394618, 0.07653660968719707, 0.07653660968719707]
actor:  1 policy actor:  1  step number:  53 total reward:  0.42666666666666675  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  44.713979663270266
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
probs:  [0.07652817675209285, 0.532456415692466, 0.07652817675209285, 0.1614308772991625, 0.07652817675209285, 0.07652817675209285]
printing an ep nov before normalisation:  42.534564582556264
from probs:  [0.07652817675209285, 0.532456415692466, 0.07652817675209285, 0.1614308772991625, 0.07652817675209285, 0.07652817675209285]
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
probs:  [0.07650652292323416, 0.5322958430086108, 0.07650652292323416, 0.16167806529845255, 0.07650652292323416, 0.07650652292323416]
actor:  1 policy actor:  1  step number:  43 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.142]
 [-0.142]
 [-0.142]
 [-0.142]
 [-0.142]
 [-0.142]
 [-0.142]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.142]
 [-0.142]
 [-0.142]
 [-0.142]
 [-0.142]
 [-0.142]
 [-0.142]]
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
probs:  [0.07650652292323416, 0.5322958430086108, 0.07650652292323416, 0.16167806529845255, 0.07650652292323416, 0.07650652292323416]
printing an ep nov before normalisation:  36.84621334075928
printing an ep nov before normalisation:  51.4790153503418
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
actor:  1 policy actor:  1  step number:  37 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.089121947602735
printing an ep nov before normalisation:  44.24165328010518
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.092]
 [-0.092]
 [-0.092]
 [-0.092]
 [-0.092]
 [-0.092]
 [-0.092]] [[30.616]
 [30.616]
 [30.616]
 [30.616]
 [30.616]
 [30.616]
 [30.616]] [[1.019]
 [1.019]
 [1.019]
 [1.019]
 [1.019]
 [1.019]
 [1.019]]
printing an ep nov before normalisation:  51.386980618700164
printing an ep nov before normalisation:  28.35730205978831
printing an ep nov before normalisation:  44.63409317452927
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
probs:  [0.06527963606479345, 0.4540595891363289, 0.06527963606479345, 0.13818078709538234, 0.06527963606479345, 0.21192071557390849]
printing an ep nov before normalisation:  38.83961612894887
siam score:  -0.8482936
actor:  1 policy actor:  1  step number:  63 total reward:  0.2799999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
probs:  [0.06527963606479345, 0.4540595891363289, 0.06527963606479345, 0.13818078709538234, 0.06527963606479345, 0.21192071557390849]
siam score:  -0.84670806
actions average: 
K:  0  action  0 :  tensor([0.6040, 0.0147, 0.0614, 0.0832, 0.1033, 0.0689, 0.0646],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0088,     0.9483,     0.0024,     0.0211,     0.0009,     0.0006,
            0.0180], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2044, 0.0293, 0.2785, 0.1115, 0.1082, 0.1646, 0.1035],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1985, 0.0087, 0.0623, 0.4342, 0.1145, 0.1056, 0.0762],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1917, 0.0108, 0.1021, 0.1491, 0.3296, 0.1439, 0.0727],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.2556, 0.0042, 0.1347, 0.1153, 0.1180, 0.2897, 0.0825],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2019, 0.0744, 0.1242, 0.1208, 0.1159, 0.1211, 0.2417],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  66.92706441256925
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
probs:  [0.06527963606479345, 0.4540595891363289, 0.06527963606479345, 0.13818078709538234, 0.06527963606479345, 0.21192071557390849]
printing an ep nov before normalisation:  72.20084763740196
Printing some Q and Qe and total Qs values:  [[-0.144]
 [-0.138]
 [-0.143]
 [-0.138]
 [-0.143]
 [-0.143]
 [-0.138]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.144]
 [-0.138]
 [-0.143]
 [-0.138]
 [-0.143]
 [-0.143]
 [-0.138]]
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
probs:  [0.06523115814550194, 0.4537135483480509, 0.06523115814550194, 0.13832770325696153, 0.06523115814550194, 0.21226527395848185]
printing an ep nov before normalisation:  27.524529772062763
printing an ep nov before normalisation:  52.85641761759721
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
probs:  [0.06523115814550194, 0.4537135483480509, 0.06523115814550194, 0.13832770325696153, 0.06523115814550194, 0.21226527395848185]
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
probs:  [0.06523115814550194, 0.4537135483480509, 0.06523115814550194, 0.13832770325696153, 0.06523115814550194, 0.21226527395848185]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
probs:  [0.06523115814550194, 0.4537135483480509, 0.06523115814550194, 0.13832770325696153, 0.06523115814550194, 0.21226527395848185]
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
siam score:  -0.8530843
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
probs:  [0.06525288710589092, 0.4538649483792068, 0.06525288710589092, 0.13837383102541234, 0.06525288710589092, 0.2120025592777082]
maxi score, test score, baseline:  -0.80786 0.4866666666666667 0.4866666666666667
probs:  [0.06525288710589092, 0.4538649483792068, 0.06525288710589092, 0.13837383102541234, 0.06525288710589092, 0.2120025592777082]
actor:  1 policy actor:  1  step number:  47 total reward:  0.37333333333333274  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  52 total reward:  0.25999999999999945  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  38.57144539912752
printing an ep nov before normalisation:  38.18394660617996
actor:  0 policy actor:  0  step number:  42 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8049666666666667 0.4866666666666667 0.4866666666666667
probs:  [0.06520452515191426, 0.453519725475731, 0.06520452515191426, 0.13852069471800318, 0.06520452515191426, 0.2123460043505232]
actions average: 
K:  2  action  0 :  tensor([0.5866, 0.0250, 0.0710, 0.0810, 0.0923, 0.0676, 0.0765],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0256, 0.9064, 0.0104, 0.0137, 0.0061, 0.0050, 0.0328],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2244, 0.1419, 0.3359, 0.0704, 0.0388, 0.1288, 0.0599],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1865, 0.0995, 0.0806, 0.3924, 0.0815, 0.0746, 0.0849],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2600, 0.0540, 0.0624, 0.0888, 0.4009, 0.0588, 0.0751],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2539, 0.0990, 0.0992, 0.0857, 0.1103, 0.2277, 0.1243],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1248, 0.2166, 0.0853, 0.1851, 0.0729, 0.0895, 0.2258],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  39.77094493530839
using explorer policy with actor:  1
printing an ep nov before normalisation:  52.734004422408624
maxi score, test score, baseline:  -0.8049666666666667 0.4866666666666667 0.4866666666666667
probs:  [0.06515623702898006, 0.453175029601557, 0.06515623702898006, 0.13866733420336835, 0.06515623702898006, 0.21268892510813434]
Printing some Q and Qe and total Qs values:  [[0.238]
 [0.238]
 [0.238]
 [0.238]
 [0.238]
 [0.238]
 [0.238]] [[35.944]
 [35.944]
 [35.944]
 [35.944]
 [35.944]
 [35.944]
 [35.944]] [[1.472]
 [1.472]
 [1.472]
 [1.472]
 [1.472]
 [1.472]
 [1.472]]
Printing some Q and Qe and total Qs values:  [[0.59 ]
 [0.514]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]] [[35.496]
 [36.448]
 [35.496]
 [35.496]
 [35.496]
 [35.496]
 [35.496]] [[1.104]
 [1.053]
 [1.104]
 [1.104]
 [1.104]
 [1.104]
 [1.104]]
actions average: 
K:  0  action  0 :  tensor([0.6068, 0.0166, 0.0738, 0.0773, 0.0951, 0.0723, 0.0581],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0035,     0.9591,     0.0041,     0.0117,     0.0007,     0.0007,
            0.0202], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1697, 0.0094, 0.3198, 0.1223, 0.1281, 0.1245, 0.1262],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2654, 0.0173, 0.0653, 0.3066, 0.1443, 0.0903, 0.1109],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2715, 0.0016, 0.0519, 0.0489, 0.4500, 0.0597, 0.1164],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1553, 0.0153, 0.1524, 0.1056, 0.1005, 0.3748, 0.0962],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2879, 0.0320, 0.1101, 0.1207, 0.1232, 0.0913, 0.2349],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  39.5971206210292
maxi score, test score, baseline:  -0.8049666666666667 0.4866666666666667 0.4866666666666667
actor:  1 policy actor:  1  step number:  44 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  0  action  0 :  tensor([0.6979, 0.0061, 0.0667, 0.0603, 0.0614, 0.0440, 0.0636],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0139, 0.9456, 0.0095, 0.0079, 0.0040, 0.0045, 0.0147],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1224, 0.0323, 0.5658, 0.0535, 0.0259, 0.0594, 0.1407],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2506, 0.0220, 0.0781, 0.3648, 0.0874, 0.1005, 0.0966],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2599, 0.0087, 0.0815, 0.0997, 0.4112, 0.0684, 0.0706],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1922, 0.0094, 0.1623, 0.1155, 0.1114, 0.2574, 0.1517],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2394, 0.2097, 0.0786, 0.1430, 0.0783, 0.0655, 0.1857],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  30.51480088229555
actor:  0 policy actor:  0  step number:  63 total reward:  0.21333333333333304  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  74.85699764239895
Printing some Q and Qe and total Qs values:  [[-0.039]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.028]] [[59.595]
 [53.474]
 [53.474]
 [53.474]
 [53.474]
 [53.474]
 [53.474]] [[1.577]
 [1.363]
 [1.363]
 [1.363]
 [1.363]
 [1.363]
 [1.363]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.3714],
        [-0.4683],
        [-0.3328],
        [-0.2787],
        [-0.2798],
        [-0.3891],
        [-0.3871],
        [ 0.5325],
        [-0.2656],
        [-0.2892]], dtype=torch.float64)
-0.09703970119800001 -0.4684070958179746
-0.032346567066 -0.5006833076009214
-0.09703970119800001 -0.42985475198595535
-0.045026434398 -0.3237259965391659
-0.084359833866 -0.36412132004475056
-0.057834381198 -0.4469640716825416
-0.083839701198 -0.4709046795990996
-0.07103438119800001 0.4614594627782699
-0.032346567066 -0.2979059480694239
-0.084359833866 -0.3736039727190979
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.80254 0.4866666666666667 0.4866666666666667
probs:  [0.06506966295667822, 0.4525553623300222, 0.06506966295667822, 0.13883032521564376, 0.06506966295667822, 0.21340532358429948]
maxi score, test score, baseline:  -0.80254 0.4866666666666667 0.4866666666666667
maxi score, test score, baseline:  -0.80254 0.4866666666666667 0.4866666666666667
probs:  [0.06506966295667822, 0.4525553623300222, 0.06506966295667822, 0.13883032521564376, 0.06506966295667822, 0.21340532358429948]
printing an ep nov before normalisation:  68.15219888811316
maxi score, test score, baseline:  -0.80254 0.4866666666666667 0.4866666666666667
probs:  [0.06506966295667822, 0.4525553623300222, 0.06506966295667822, 0.13883032521564376, 0.06506966295667822, 0.21340532358429948]
printing an ep nov before normalisation:  0.10558238627709216
actor:  1 policy actor:  1  step number:  52 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.80254 0.4866666666666667 0.4866666666666667
probs:  [0.06509148045436271, 0.4527073681910193, 0.06509148045436271, 0.13887692499538892, 0.06509148045436271, 0.2131412654505036]
actor:  1 policy actor:  1  step number:  42 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.80254 0.4866666666666667 0.4866666666666667
Printing some Q and Qe and total Qs values:  [[-0.148]
 [-0.151]
 [-0.151]
 [-0.165]
 [-0.151]
 [-0.155]
 [-0.151]] [[58.345]
 [54.396]
 [54.396]
 [59.311]
 [54.396]
 [54.162]
 [54.396]] [[1.523]
 [1.301]
 [1.301]
 [1.56 ]
 [1.301]
 [1.285]
 [1.301]]
actions average: 
K:  2  action  0 :  tensor([0.5914, 0.0394, 0.0781, 0.0688, 0.0872, 0.0771, 0.0580],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0305, 0.8758, 0.0256, 0.0253, 0.0062, 0.0075, 0.0290],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2317, 0.0212, 0.2451, 0.0999, 0.0884, 0.2448, 0.0690],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0956, 0.0853, 0.0532, 0.5760, 0.0777, 0.0521, 0.0599],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2342, 0.0052, 0.1000, 0.1358, 0.2426, 0.2041, 0.0781],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2563, 0.0675, 0.1937, 0.1200, 0.0961, 0.1527, 0.1138],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.3192, 0.0593, 0.1057, 0.1273, 0.1595, 0.1106, 0.1185],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.80254 0.4866666666666667 0.4866666666666667
maxi score, test score, baseline:  -0.80254 0.4866666666666667 0.4866666666666667
probs:  [0.06494767486061542, 0.45168079588009646, 0.06494767486061542, 0.13931373003275885, 0.06494767486061542, 0.21416244950529836]
maxi score, test score, baseline:  -0.80254 0.4866666666666667 0.4866666666666667
probs:  [0.06494767486061542, 0.45168079588009646, 0.06494767486061542, 0.13931373003275885, 0.06494767486061542, 0.21416244950529836]
printing an ep nov before normalisation:  31.786591622643815
Printing some Q and Qe and total Qs values:  [[0.94 ]
 [0.946]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [0.94 ]] [[32.786]
 [33.812]
 [32.786]
 [32.786]
 [32.786]
 [32.786]
 [32.786]] [[0.94 ]
 [0.946]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [0.94 ]]
printing an ep nov before normalisation:  38.08292304407965
printing an ep nov before normalisation:  25.869741439819336
maxi score, test score, baseline:  -0.80254 0.4866666666666667 0.4866666666666667
probs:  [0.06494767486061542, 0.45168079588009646, 0.06494767486061542, 0.13931373003275885, 0.06494767486061542, 0.21416244950529836]
printing an ep nov before normalisation:  35.670299193902075
line 256 mcts: sample exp_bonus 14.971338183545246
actor:  0 policy actor:  0  step number:  50 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  53 total reward:  0.42666666666666664  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.7998066666666668 0.4866666666666667 0.4866666666666667
probs:  [0.06496953100995238, 0.45183306271861157, 0.06496953100995238, 0.13936066323906185, 0.06496953100995238, 0.21389768101246942]
actor:  1 policy actor:  1  step number:  45 total reward:  0.4933333333333333  reward:  1.0 rdn_beta:  1.333
from probs:  [0.06496953100995238, 0.45183306271861157, 0.06496953100995238, 0.13936066323906185, 0.06496953100995238, 0.21389768101246942]
maxi score, test score, baseline:  -0.7998066666666668 0.4866666666666667 0.4866666666666667
probs:  [0.06494924046592365, 0.4516917030974188, 0.06494924046592365, 0.1396297921528505, 0.06494924046592365, 0.2138307833519598]
printing an ep nov before normalisation:  23.788490295410156
printing an ep nov before normalisation:  48.162615956830926
printing an ep nov before normalisation:  35.94315326403125
maxi score, test score, baseline:  -0.7998066666666668 0.4866666666666667 0.4866666666666667
probs:  [0.06494924046592365, 0.4516917030974188, 0.06494924046592365, 0.1396297921528505, 0.06494924046592365, 0.2138307833519598]
actor:  0 policy actor:  1  step number:  45 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.028]
 [-0.028]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [31.036]
 [31.036]] [[0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [1.198]
 [1.198]]
printing an ep nov before normalisation:  26.61173958762035
siam score:  -0.8555333
from probs:  [0.06495906826022607, 0.4517601711127555, 0.06495906826022607, 0.13949943864425052, 0.06495906826022607, 0.21386318546231578]
printing an ep nov before normalisation:  41.93798047677934
maxi score, test score, baseline:  -0.7969 0.4866666666666667 0.4866666666666667
probs:  [0.06495906826022607, 0.4517601711127555, 0.06495906826022607, 0.13949943864425052, 0.06495906826022607, 0.21386318546231578]
maxi score, test score, baseline:  -0.7969 0.4866666666666667 0.4866666666666667
probs:  [0.06493310539193094, 0.45157107736406665, 0.06493310539193094, 0.13969206883163773, 0.06493310539193094, 0.21393753762850287]
Sims:  50 1 epoch:  102783 pick best:  False frame count:  102783
maxi score, test score, baseline:  -0.7969 0.4866666666666667 0.4866666666666667
probs:  [0.06488544943585985, 0.45123086522384975, 0.06488544943585985, 0.1398376860255809, 0.06488544943585985, 0.21427510044298984]
maxi score, test score, baseline:  -0.7969 0.4866666666666667 0.4866666666666667
probs:  [0.06488544943585985, 0.45123086522384975, 0.06488544943585985, 0.1398376860255809, 0.06488544943585985, 0.21427510044298984]
maxi score, test score, baseline:  -0.7969 0.4866666666666667 0.4866666666666667
maxi score, test score, baseline:  -0.7969 0.4866666666666667 0.4866666666666667
probs:  [0.06488544943585985, 0.45123086522384975, 0.06488544943585985, 0.1398376860255809, 0.06488544943585985, 0.21427510044298984]
printing an ep nov before normalisation:  46.87010212329223
maxi score, test score, baseline:  -0.7969 0.4866666666666667 0.4866666666666667
probs:  [0.06488544943585985, 0.45123086522384975, 0.06488544943585985, 0.1398376860255809, 0.06488544943585985, 0.21427510044298984]
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.9229891650309
actor:  1 policy actor:  1  step number:  43 total reward:  0.56  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.7969 0.4866666666666667 0.4866666666666667
probs:  [0.06144525251081805, 0.4272646653863593, 0.06144525251081805, 0.18550154115983702, 0.06144525251081805, 0.20289803592134967]
Printing some Q and Qe and total Qs values:  [[0.249]
 [0.206]
 [0.246]
 [0.25 ]
 [0.25 ]
 [0.251]
 [0.25 ]] [[35.848]
 [35.587]
 [34.032]
 [33.299]
 [34.111]
 [34.134]
 [34.327]] [[0.249]
 [0.206]
 [0.246]
 [0.25 ]
 [0.25 ]
 [0.251]
 [0.25 ]]
printing an ep nov before normalisation:  31.868937015533447
using explorer policy with actor:  0
actions average: 
K:  0  action  0 :  tensor([0.5791, 0.0191, 0.0734, 0.0877, 0.0944, 0.0714, 0.0748],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0168, 0.9194, 0.0060, 0.0272, 0.0097, 0.0063, 0.0146],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0498, 0.0099, 0.4369, 0.0550, 0.0535, 0.3214, 0.0735],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2242, 0.0022, 0.0951, 0.3260, 0.0884, 0.1431, 0.1211],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2107, 0.0033, 0.0725, 0.1256, 0.3557, 0.1333, 0.0989],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1895, 0.0065, 0.1619, 0.1180, 0.1001, 0.3114, 0.1126],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0922, 0.1713, 0.1388, 0.1049, 0.0615, 0.1505, 0.2807],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  57 total reward:  0.39999999999999947  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.7969 0.4866666666666667 0.4866666666666667
probs:  [0.061464763650185196, 0.4274005901241923, 0.061464763650185196, 0.185560530354402, 0.061464763650185196, 0.20264458857084994]
printing an ep nov before normalisation:  39.037152108571185
printing an ep nov before normalisation:  33.62585974289165
actor:  0 policy actor:  1  step number:  58 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  0.667
from probs:  [0.061464763650185196, 0.4274005901241923, 0.061464763650185196, 0.185560530354402, 0.061464763650185196, 0.20264458857084994]
maxi score, test score, baseline:  -0.7943533333333334 0.4866666666666667 0.4866666666666667
actor:  1 policy actor:  1  step number:  40 total reward:  0.48666666666666647  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.7943533333333334 0.4866666666666667 0.4866666666666667
probs:  [0.06148419963409979, 0.42753599129008035, 0.06148419963409979, 0.18561929232699242, 0.06148419963409979, 0.2023921174806278]
maxi score, test score, baseline:  -0.7943533333333334 0.4866666666666667 0.4866666666666667
probs:  [0.06148419963409979, 0.42753599129008035, 0.06148419963409979, 0.18561929232699242, 0.06148419963409979, 0.2023921174806278]
maxi score, test score, baseline:  -0.7943533333333334 0.4866666666666667 0.4866666666666667
probs:  [0.06148419963409979, 0.42753599129008035, 0.06148419963409979, 0.18561929232699242, 0.06148419963409979, 0.2023921174806278]
maxi score, test score, baseline:  -0.7943533333333334 0.4866666666666667 0.4866666666666667
probs:  [0.061450216758541835, 0.4272914748614403, 0.061450216758541835, 0.18592745919366502, 0.061450216758541835, 0.20243041566926911]
Printing some Q and Qe and total Qs values:  [[0.368]
 [0.368]
 [0.368]
 [0.347]
 [0.368]
 [0.368]
 [0.368]] [[27.275]
 [27.275]
 [27.275]
 [13.682]
 [27.275]
 [27.275]
 [27.275]] [[0.368]
 [0.368]
 [0.368]
 [0.347]
 [0.368]
 [0.368]
 [0.368]]
Printing some Q and Qe and total Qs values:  [[-0.116]
 [-0.116]
 [-0.118]
 [-0.118]
 [-0.118]
 [-0.118]
 [-0.118]] [[32.674]
 [35.007]
 [32.013]
 [32.013]
 [32.013]
 [32.013]
 [32.013]] [[0.578]
 [0.674]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]]
maxi score, test score, baseline:  -0.7943533333333334 0.4866666666666667 0.4866666666666667
probs:  [0.061450216758541835, 0.4272914748614403, 0.061450216758541835, 0.18592745919366502, 0.061450216758541835, 0.20243041566926911]
maxi score, test score, baseline:  -0.7943533333333334 0.4866666666666667 0.4866666666666667
probs:  [0.061450216758541835, 0.4272914748614403, 0.061450216758541835, 0.18592745919366502, 0.061450216758541835, 0.20243041566926911]
actor:  1 policy actor:  1  step number:  46 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  68.82208903383015
printing an ep nov before normalisation:  60.77188335956677
maxi score, test score, baseline:  -0.7943533333333334 0.4866666666666667 0.4866666666666667
probs:  [0.061412427165295824, 0.42702044877128614, 0.061412427165295824, 0.1859714492757168, 0.061412427165295824, 0.20277082045710962]
using another actor
printing an ep nov before normalisation:  31.983506679534916
using explorer policy with actor:  0
printing an ep nov before normalisation:  42.8867619124637
maxi score, test score, baseline:  -0.7943533333333334 0.4866666666666667 0.4866666666666667
probs:  [0.06144719908640875, 0.42726267922971317, 0.06144719908640875, 0.18582563070140526, 0.06144719908640875, 0.20257009280965516]
printing an ep nov before normalisation:  24.44481265471208
printing an ep nov before normalisation:  71.39242360378687
printing an ep nov before normalisation:  39.72053432046392
maxi score, test score, baseline:  -0.7943533333333334 0.4866666666666667 0.4866666666666667
probs:  [0.0613941224719815, 0.4268851671224016, 0.0613941224719815, 0.18607372883256817, 0.0613941224719815, 0.20285873662908577]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[34.95]
 [34.95]
 [34.95]
 [34.95]
 [34.95]
 [34.95]
 [34.95]] [[0.992]
 [0.992]
 [0.992]
 [0.992]
 [0.992]
 [0.992]
 [0.992]]
maxi score, test score, baseline:  -0.7943533333333334 0.4866666666666667 0.4866666666666667
probs:  [0.0613941224719815, 0.4268851671224016, 0.0613941224719815, 0.18607372883256817, 0.0613941224719815, 0.20285873662908577]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.274]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]] [[22.304]
 [20.598]
 [20.598]
 [20.598]
 [20.598]
 [20.598]
 [20.598]] [[0.274]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.7943533333333334 0.4866666666666667 0.4866666666666667
probs:  [0.0613941224719815, 0.4268851671224016, 0.0613941224719815, 0.18607372883256817, 0.0613941224719815, 0.20285873662908577]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  41 total reward:  0.5466666666666666  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.7943533333333334 0.4866666666666667 0.4866666666666667
siam score:  -0.8446524
maxi score, test score, baseline:  -0.7943533333333334 0.4866666666666667 0.4866666666666667
probs:  [0.05929290684381982, 0.4122478061123295, 0.05929290684381982, 0.21396811219730325, 0.05929290684381982, 0.1959053611589079]
printing an ep nov before normalisation:  63.429291180522966
printing an ep nov before normalisation:  27.6281372173929
maxi score, test score, baseline:  -0.7943533333333334 0.4866666666666667 0.4866666666666667
probs:  [0.059217434521731956, 0.4117070756784812, 0.059217434521731956, 0.21439613196763202, 0.059217434521731956, 0.19624448878869097]
maxi score, test score, baseline:  -0.7943533333333334 0.4866666666666667 0.4866666666666667
printing an ep nov before normalisation:  55.5768032366203
printing an ep nov before normalisation:  31.48416757583618
printing an ep nov before normalisation:  48.80096152365296
siam score:  -0.8468277
maxi score, test score, baseline:  -0.7943533333333334 0.4866666666666667 0.4866666666666667
probs:  [0.05923544581475685, 0.41183254029033284, 0.05923544581475685, 0.21446144810058648, 0.05923544581475685, 0.19599967416481007]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7943533333333334 0.4866666666666667 0.4866666666666667
probs:  [0.05923544581475685, 0.41183254029033284, 0.05923544581475685, 0.21446144810058648, 0.05923544581475685, 0.19599967416481007]
printing an ep nov before normalisation:  32.53158441723007
maxi score, test score, baseline:  -0.7943533333333334 0.4866666666666667 0.4866666666666667
printing an ep nov before normalisation:  29.256176377156535
maxi score, test score, baseline:  -0.7943533333333334 0.4866666666666667 0.4866666666666667
maxi score, test score, baseline:  -0.7943533333333334 0.4866666666666667 0.4866666666666667
probs:  [0.05923544581475685, 0.41183254029033284, 0.05923544581475685, 0.21446144810058648, 0.05923544581475685, 0.19599967416481007]
actor:  1 policy actor:  1  step number:  55 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7943533333333334 0.4866666666666667 0.4866666666666667
probs:  [0.05917968328752584, 0.4114366202936144, 0.05917968328752584, 0.21476438267422995, 0.05917968328752584, 0.1962599471695781]
printing an ep nov before normalisation:  28.582144531134965
Printing some Q and Qe and total Qs values:  [[0.734]
 [0.893]
 [0.843]
 [0.734]
 [0.734]
 [0.734]
 [0.734]] [[26.393]
 [30.289]
 [22.151]
 [26.393]
 [26.393]
 [26.393]
 [26.393]] [[0.734]
 [0.893]
 [0.843]
 [0.734]
 [0.734]
 [0.734]
 [0.734]]
maxi score, test score, baseline:  -0.7943533333333334 0.4866666666666667 0.4866666666666667
probs:  [0.05917968328752584, 0.4114366202936144, 0.05917968328752584, 0.21476438267422995, 0.05917968328752584, 0.1962599471695781]
printing an ep nov before normalisation:  44.966315050963324
actor:  0 policy actor:  1  step number:  51 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.79174 0.4866666666666667 0.4866666666666667
probs:  [0.05917968328752584, 0.4114366202936144, 0.05917968328752584, 0.21476438267422995, 0.05917968328752584, 0.1962599471695781]
actor:  1 policy actor:  1  step number:  48 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.088]
 [-0.079]
 [-0.088]
 [-0.088]
 [-0.088]
 [-0.088]
 [-0.088]] [[40.641]
 [50.72 ]
 [40.641]
 [40.641]
 [40.641]
 [40.641]
 [40.641]] [[1.105]
 [1.566]
 [1.105]
 [1.105]
 [1.105]
 [1.105]
 [1.105]]
siam score:  -0.8497516
printing an ep nov before normalisation:  14.577693524002548
Printing some Q and Qe and total Qs values:  [[0.292]
 [0.256]
 [0.28 ]
 [0.272]
 [0.263]
 [0.27 ]
 [0.265]] [[23.977]
 [54.827]
 [24.006]
 [24.109]
 [24.206]
 [20.633]
 [22.663]] [[0.292]
 [0.256]
 [0.28 ]
 [0.272]
 [0.263]
 [0.27 ]
 [0.265]]
Printing some Q and Qe and total Qs values:  [[0.27]
 [0.27]
 [0.27]
 [0.27]
 [0.27]
 [0.27]
 [0.27]] [[19.671]
 [19.671]
 [19.671]
 [19.671]
 [19.671]
 [19.671]
 [19.671]] [[0.27]
 [0.27]
 [0.27]
 [0.27]
 [0.27]
 [0.27]
 [0.27]]
printing an ep nov before normalisation:  24.261044496750152
maxi score, test score, baseline:  -0.79174 0.4866666666666667 0.4866666666666667
probs:  [0.059177796189751226, 0.4114159911869348, 0.059177796189751226, 0.2152626372915089, 0.059177796189751226, 0.1957879829523026]
Printing some Q and Qe and total Qs values:  [[0.076]
 [0.084]
 [0.086]
 [0.113]
 [0.088]
 [0.086]
 [0.079]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.076]
 [0.084]
 [0.086]
 [0.113]
 [0.088]
 [0.086]
 [0.079]]
maxi score, test score, baseline:  -0.79174 0.4866666666666667 0.4866666666666667
probs:  [0.059177796189751226, 0.4114159911869348, 0.059177796189751226, 0.2152626372915089, 0.059177796189751226, 0.1957879829523026]
Printing some Q and Qe and total Qs values:  [[0.287]
 [0.42 ]
 [0.333]
 [0.32 ]
 [0.333]
 [0.324]
 [0.33 ]] [[39.543]
 [35.372]
 [32.95 ]
 [26.423]
 [32.95 ]
 [28.751]
 [34.964]] [[0.287]
 [0.42 ]
 [0.333]
 [0.32 ]
 [0.333]
 [0.324]
 [0.33 ]]
maxi score, test score, baseline:  -0.79174 0.4866666666666667 0.4866666666666667
probs:  [0.059177796189751226, 0.4114159911869348, 0.059177796189751226, 0.2152626372915089, 0.059177796189751226, 0.1957879829523026]
printing an ep nov before normalisation:  45.789975426150065
printing an ep nov before normalisation:  12.955156491637174
maxi score, test score, baseline:  -0.79174 0.4866666666666667 0.4866666666666667
probs:  [0.059177796189751226, 0.4114159911869348, 0.059177796189751226, 0.2152626372915089, 0.059177796189751226, 0.1957879829523026]
maxi score, test score, baseline:  -0.79174 0.4866666666666667 0.4866666666666667
probs:  [0.059177796189751226, 0.4114159911869348, 0.059177796189751226, 0.2152626372915089, 0.059177796189751226, 0.1957879829523026]
actor:  1 policy actor:  1  step number:  64 total reward:  0.1666666666666663  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  2  action  0 :  tensor([0.5917, 0.0369, 0.0616, 0.0838, 0.0890, 0.0720, 0.0652],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0281, 0.9067, 0.0109, 0.0198, 0.0071, 0.0102, 0.0172],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1918, 0.0097, 0.2795, 0.1349, 0.1192, 0.1570, 0.1078],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1577, 0.0297, 0.0835, 0.4045, 0.1318, 0.1211, 0.0717],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2892, 0.0122, 0.0801, 0.1210, 0.3170, 0.0989, 0.0816],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1847, 0.0446, 0.1419, 0.1649, 0.1001, 0.2763, 0.0875],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2238, 0.1137, 0.1033, 0.1406, 0.1090, 0.1148, 0.1948],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  25.84733981891056
maxi score, test score, baseline:  -0.79174 0.4866666666666667 0.4866666666666667
probs:  [0.05903025354100281, 0.4103658692291182, 0.05903025354100281, 0.21592144913370714, 0.05903025354100281, 0.1966219210141661]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.79174 0.4866666666666667 0.4866666666666667
probs:  [0.05903025354100281, 0.4103658692291182, 0.05903025354100281, 0.21592144913370714, 0.05903025354100281, 0.1966219210141661]
printing an ep nov before normalisation:  34.97530859668678
maxi score, test score, baseline:  -0.79174 0.4866666666666667 0.4866666666666667
probs:  [0.05899301451623892, 0.4100990306831499, 0.05899301451623892, 0.21628745084684461, 0.05899301451623892, 0.19663447492128863]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.112]
 [-0.1  ]
 [-0.112]
 [-0.112]
 [-0.112]
 [-0.112]
 [-0.112]] [[31.674]
 [31.84 ]
 [31.674]
 [31.674]
 [31.674]
 [31.674]
 [31.674]] [[0.299]
 [0.315]
 [0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.299]]
printing an ep nov before normalisation:  20.82085711916207
maxi score, test score, baseline:  -0.79174 0.4866666666666667 0.4866666666666667
probs:  [0.05901155180011273, 0.410228145190264, 0.05901155180011273, 0.2160408384440504, 0.05901155180011273, 0.19669636096534748]
siam score:  -0.85513425
Printing some Q and Qe and total Qs values:  [[0.649]
 [0.756]
 [0.628]
 [0.62 ]
 [0.58 ]
 [0.608]
 [0.581]] [[33.742]
 [34.598]
 [26.626]
 [28.213]
 [28.198]
 [27.384]
 [27.718]] [[0.649]
 [0.756]
 [0.628]
 [0.62 ]
 [0.58 ]
 [0.608]
 [0.581]]
Printing some Q and Qe and total Qs values:  [[0.843]
 [0.88 ]
 [0.771]
 [0.755]
 [0.761]
 [0.755]
 [0.761]] [[25.469]
 [23.734]
 [22.309]
 [22.747]
 [20.062]
 [23.239]
 [20.78 ]] [[0.843]
 [0.88 ]
 [0.771]
 [0.755]
 [0.761]
 [0.755]
 [0.761]]
printing an ep nov before normalisation:  25.080330345962114
maxi score, test score, baseline:  -0.79174 0.4866666666666667 0.4866666666666667
probs:  [0.0588469305606535, 0.40905921700160547, 0.0588469305606535, 0.2169376110878318, 0.0588469305606535, 0.19746238022860213]
maxi score, test score, baseline:  -0.79174 0.4866666666666667 0.4866666666666667
probs:  [0.0588469305606535, 0.40905921700160547, 0.0588469305606535, 0.2169376110878318, 0.0588469305606535, 0.19746238022860213]
actor:  0 policy actor:  0  step number:  62 total reward:  0.16666666666666596  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7894066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.0588469305606535, 0.40905921700160547, 0.0588469305606535, 0.2169376110878318, 0.0588469305606535, 0.19746238022860213]
printing an ep nov before normalisation:  31.50459778962304
maxi score, test score, baseline:  -0.7894066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.0588469305606535, 0.40905921700160547, 0.0588469305606535, 0.2169376110878318, 0.0588469305606535, 0.19746238022860213]
Printing some Q and Qe and total Qs values:  [[-0.059]
 [-0.059]
 [-0.059]
 [-0.059]
 [-0.059]
 [-0.059]
 [-0.059]] [[34.289]
 [33.814]
 [33.814]
 [33.814]
 [33.814]
 [33.814]
 [33.814]] [[1.274]
 [1.255]
 [1.255]
 [1.255]
 [1.255]
 [1.255]
 [1.255]]
printing an ep nov before normalisation:  26.07680143049725
maxi score, test score, baseline:  -0.7894066666666667 0.4866666666666667 0.4866666666666667
printing an ep nov before normalisation:  28.71420244858177
siam score:  -0.84926623
printing an ep nov before normalisation:  28.841678640115255
maxi score, test score, baseline:  -0.7894066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.05879226583088869, 0.40867105843982215, 0.05879226583088869, 0.21723539668581318, 0.05879226583088869, 0.19771674738169878]
actor:  1 policy actor:  1  step number:  60 total reward:  0.36666666666666603  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.7894066666666667 0.4866666666666667 0.4866666666666667
maxi score, test score, baseline:  -0.7894066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.05881019591628241, 0.4087959346451563, 0.05881019591628241, 0.21730175748045275, 0.05881019591628241, 0.19747172012554362]
printing an ep nov before normalisation:  33.93997751446007
maxi score, test score, baseline:  -0.7894066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.05882805825349962, 0.4089203390102837, 0.05882805825349962, 0.21736786753323978, 0.05882805825349962, 0.1972276186959775]
maxi score, test score, baseline:  -0.7894066666666667 0.4866666666666667 0.4866666666666667
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.46 ]
 [0.522]] [[33.653]
 [33.653]
 [33.653]
 [33.653]
 [33.653]
 [37.237]
 [33.653]] [[1.03 ]
 [1.03 ]
 [1.03 ]
 [1.03 ]
 [1.03 ]
 [1.054]
 [1.03 ]]
actor:  1 policy actor:  1  step number:  57 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  44.402922668670804
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.7894066666666667 0.4866666666666667 0.4866666666666667
printing an ep nov before normalisation:  39.2281534965957
printing an ep nov before normalisation:  52.37216721397573
Printing some Q and Qe and total Qs values:  [[-0.016]
 [ 0.193]
 [-0.016]
 [ 0.08 ]
 [-0.018]
 [-0.017]
 [-0.008]] [[28.374]
 [31.564]
 [27.344]
 [28.874]
 [27.867]
 [27.51 ]
 [27.149]] [[0.421]
 [0.723]
 [0.391]
 [0.532]
 [0.404]
 [0.394]
 [0.394]]
from probs:  [0.0586648246931756, 0.4077612319295437, 0.0586648246931756, 0.21825925904138485, 0.0586648246931756, 0.1979850349495446]
siam score:  -0.847578
printing an ep nov before normalisation:  37.11714073140602
maxi score, test score, baseline:  -0.7894066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.05862850587826533, 0.40750088840718496, 0.05862850587826533, 0.21862207134242856, 0.05862850587826533, 0.19799152261559042]
maxi score, test score, baseline:  -0.7894066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.05862850587826533, 0.40750088840718496, 0.05862850587826533, 0.21862207134242856, 0.05862850587826533, 0.19799152261559042]
printing an ep nov before normalisation:  65.22307203927807
printing an ep nov before normalisation:  62.479017050064876
from probs:  [0.05859226740340317, 0.4072411138886401, 0.05859226740340317, 0.2189845446617202, 0.05859226740340317, 0.19799753923943014]
maxi score, test score, baseline:  -0.7894066666666667 0.4866666666666667 0.4866666666666667
maxi score, test score, baseline:  -0.7894066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.05862774987493708, 0.40748821329115475, 0.05862774987493708, 0.21911737930237382, 0.05862774987493708, 0.1975111577816602]
printing an ep nov before normalisation:  85.62824616156239
using explorer policy with actor:  0
siam score:  -0.8453274
printing an ep nov before normalisation:  31.06810782026643
Printing some Q and Qe and total Qs values:  [[0.917]
 [0.961]
 [0.94 ]
 [0.926]
 [0.895]
 [0.931]
 [0.887]] [[27.408]
 [28.935]
 [27.162]
 [26.964]
 [23.405]
 [27.257]
 [21.001]] [[0.917]
 [0.961]
 [0.94 ]
 [0.926]
 [0.895]
 [0.931]
 [0.887]]
maxi score, test score, baseline:  -0.7894066666666668 0.4866666666666667 0.4866666666666667
maxi score, test score, baseline:  -0.7894066666666668 0.4866666666666667 0.4866666666666667
probs:  [0.05862774987493708, 0.40748821329115475, 0.05862774987493708, 0.21911737930237382, 0.05862774987493708, 0.1975111577816602]
printing an ep nov before normalisation:  44.69522883249245
actor:  0 policy actor:  0  step number:  48 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7868066666666668 0.4866666666666667 0.4866666666666667
probs:  [0.05862774987493708, 0.40748821329115475, 0.05862774987493708, 0.21911737930237382, 0.05862774987493708, 0.1975111577816602]
Printing some Q and Qe and total Qs values:  [[-0.137]
 [-0.117]
 [-0.134]
 [-0.136]
 [-0.136]
 [-0.138]
 [-0.136]] [[35.332]
 [53.411]
 [33.808]
 [32.447]
 [32.772]
 [33.195]
 [32.   ]] [[0.117]
 [0.373]
 [0.1  ]
 [0.081]
 [0.085]
 [0.089]
 [0.074]]
maxi score, test score, baseline:  -0.7868066666666668 0.4866666666666667 0.4866666666666667
probs:  [0.05857383901894926, 0.40710537581387546, 0.05857383901894926, 0.21941320639302891, 0.05857383901894926, 0.19775990073624794]
UNIT TEST: sample policy line 217 mcts : [0.286 0.204 0.163 0.061 0.082 0.102 0.102]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7868066666666668 0.4866666666666667 0.4866666666666667
probs:  [0.05857383901894926, 0.40710537581387546, 0.05857383901894926, 0.21941320639302891, 0.05857383901894926, 0.19775990073624794]
maxi score, test score, baseline:  -0.7868066666666668 0.4866666666666667 0.4866666666666667
probs:  [0.058520029727992987, 0.4067232595809493, 0.058520029727992987, 0.21970847616202308, 0.058520029727992987, 0.19800817507304852]
actor:  1 policy actor:  1  step number:  57 total reward:  0.4266666666666662  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  44.58719253540039
maxi score, test score, baseline:  -0.7868066666666668 0.4866666666666667 0.4866666666666667
probs:  [0.058520029727992987, 0.4067232595809493, 0.058520029727992987, 0.21970847616202308, 0.058520029727992987, 0.19800817507304852]
maxi score, test score, baseline:  -0.7868066666666668 0.4866666666666667 0.4866666666666667
probs:  [0.058520029727992987, 0.4067232595809493, 0.058520029727992987, 0.21970847616202308, 0.058520029727992987, 0.19800817507304852]
printing an ep nov before normalisation:  0.0
printing an ep nov before normalisation:  44.159832196308344
Printing some Q and Qe and total Qs values:  [[-0.139]
 [-0.12 ]
 [-0.122]
 [-0.138]
 [-0.137]
 [-0.136]
 [-0.121]] [[40.55 ]
 [51.881]
 [50.752]
 [41.423]
 [42.587]
 [42.062]
 [47.676]] [[0.277]
 [0.536]
 [0.51 ]
 [0.297]
 [0.322]
 [0.312]
 [0.446]]
printing an ep nov before normalisation:  34.17507717443424
maxi score, test score, baseline:  -0.7868066666666668 0.4866666666666667 0.4866666666666667
probs:  [0.058359208383259156, 0.4055812180239672, 0.058359208383259156, 0.22059095724410058, 0.058359208383259156, 0.1987501995821548]
Printing some Q and Qe and total Qs values:  [[-0.107]
 [-0.108]
 [-0.108]
 [-0.108]
 [-0.108]
 [-0.108]
 [-0.108]] [[31.118]
 [32.198]
 [32.198]
 [32.198]
 [32.198]
 [32.198]
 [32.198]] [[1.226]
 [1.316]
 [1.316]
 [1.316]
 [1.316]
 [1.316]
 [1.316]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.7868066666666668 0.4866666666666667 0.4866666666666667
probs:  [0.05830580249574637, 0.4052019664822763, 0.05830580249574637, 0.22088401340203503, 0.05830580249574637, 0.19899661262844937]
Printing some Q and Qe and total Qs values:  [[1.009]
 [1.009]
 [1.009]
 [1.009]
 [1.009]
 [1.009]
 [1.009]] [[29.943]
 [29.943]
 [29.943]
 [29.943]
 [29.943]
 [29.943]
 [29.943]] [[1.009]
 [1.009]
 [1.009]
 [1.009]
 [1.009]
 [1.009]
 [1.009]]
printing an ep nov before normalisation:  25.441040641228106
maxi score, test score, baseline:  -0.7868066666666668 0.4866666666666667 0.4866666666666667
printing an ep nov before normalisation:  62.71193235922204
actor:  1 policy actor:  1  step number:  61 total reward:  0.1599999999999996  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  38 total reward:  0.54  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.087]
 [-0.131]
 [-0.087]
 [-0.11 ]
 [-0.112]
 [-0.113]
 [-0.087]] [[37.994]
 [33.157]
 [37.994]
 [19.654]
 [19.767]
 [20.009]
 [37.994]] [[0.718]
 [0.536]
 [0.718]
 [0.172]
 [0.173]
 [0.179]
 [0.718]]
Printing some Q and Qe and total Qs values:  [[-0.091]
 [-0.066]
 [-0.091]
 [-0.091]
 [-0.091]
 [-0.091]
 [-0.091]] [[29.192]
 [42.799]
 [29.192]
 [29.192]
 [29.192]
 [29.192]
 [29.192]] [[0.212]
 [0.538]
 [0.212]
 [0.212]
 [0.212]
 [0.212]
 [0.212]]
printing an ep nov before normalisation:  28.454229248068028
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.227]
 [0.45 ]
 [0.226]
 [0.224]
 [0.225]
 [0.282]
 [0.243]] [[21.723]
 [28.645]
 [22.526]
 [22.169]
 [22.316]
 [26.473]
 [21.789]] [[0.497]
 [0.89 ]
 [0.515]
 [0.505]
 [0.509]
 [0.669]
 [0.514]]
siam score:  -0.8519029
actor:  1 policy actor:  1  step number:  44 total reward:  0.54  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7837266666666667 0.4866666666666667 0.4866666666666667
probs:  [0.06024751046787089, 0.4189883630892685, 0.06024751046787089, 0.21038150300394828, 0.06024751046787089, 0.1898876025031708]
Printing some Q and Qe and total Qs values:  [[-0.116]
 [-0.125]
 [-0.108]
 [-0.108]
 [-0.108]
 [-0.108]
 [-0.108]] [[45.486]
 [43.32 ]
 [43.117]
 [43.117]
 [43.117]
 [43.117]
 [43.117]] [[1.55 ]
 [1.393]
 [1.396]
 [1.396]
 [1.396]
 [1.396]
 [1.396]]
printing an ep nov before normalisation:  20.89313507080078
maxi score, test score, baseline:  -0.7837266666666667 0.4866666666666667 0.4866666666666667
probs:  [0.06024751046787089, 0.4189883630892685, 0.06024751046787089, 0.21038150300394828, 0.06024751046787089, 0.1898876025031708]
printing an ep nov before normalisation:  40.52125743796975
maxi score, test score, baseline:  -0.7837266666666667 0.4866666666666667 0.4866666666666667
maxi score, test score, baseline:  -0.7837266666666667 0.4866666666666667 0.4866666666666667
probs:  [0.060196961898703154, 0.4186293972855117, 0.060196961898703154, 0.2106592130795128, 0.060196961898703154, 0.19012050393886612]
siam score:  -0.85108674
maxi score, test score, baseline:  -0.7837266666666667 0.4866666666666667 0.4866666666666667
probs:  [0.060196961898703154, 0.4186293972855117, 0.060196961898703154, 0.2106592130795128, 0.060196961898703154, 0.19012050393886612]
line 256 mcts: sample exp_bonus 57.258476404352116
actor:  1 policy actor:  1  step number:  48 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.7837266666666667 0.4866666666666667 0.4866666666666667
probs:  [0.06021387128151149, 0.4187472140002746, 0.06021387128151149, 0.2107184811967089, 0.06021387128151149, 0.18989269095848205]
UNIT TEST: sample policy line 217 mcts : [0.    0.041 0.    0.878 0.    0.061 0.02 ]
maxi score, test score, baseline:  -0.7837266666666667 0.4866666666666667 0.4866666666666667
probs:  [0.06023071708347484, 0.41886458771328133, 0.06023071708347484, 0.21077752646029674, 0.06023071708347484, 0.1896657345759974]
printing an ep nov before normalisation:  54.61736225163883
printing an ep nov before normalisation:  41.065354850209985
actor:  1 policy actor:  1  step number:  53 total reward:  0.22666666666666613  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.516]
 [0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.342]] [[25.394]
 [29.682]
 [25.394]
 [25.394]
 [25.394]
 [25.394]
 [25.394]] [[0.678]
 [0.909]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]]
maxi score, test score, baseline:  -0.7837266666666667 0.4866666666666667 0.4866666666666667
probs:  [0.060129970316617765, 0.4181491245578837, 0.060129970316617765, 0.21133234444509144, 0.060129970316617765, 0.19012862004717168]
printing an ep nov before normalisation:  60.00674736595557
Printing some Q and Qe and total Qs values:  [[0.527]
 [0.683]
 [0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.521]] [[30.695]
 [33.698]
 [31.634]
 [31.634]
 [31.634]
 [31.634]
 [31.634]] [[0.979]
 [1.222]
 [1.001]
 [1.001]
 [1.001]
 [1.001]
 [1.001]]
using explorer policy with actor:  0
using explorer policy with actor:  0
printing an ep nov before normalisation:  29.466209411621094
printing an ep nov before normalisation:  22.322402871140042
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.27718273106482
printing an ep nov before normalisation:  39.97368084430805
maxi score, test score, baseline:  -0.7837266666666667 0.4866666666666667 0.4866666666666667
probs:  [0.060079726336035096, 0.417792311947327, 0.060079726336035096, 0.21160904080890114, 0.060079726336035096, 0.19035946823566652]
actor:  0 policy actor:  0  step number:  43 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.7810866666666667 0.4866666666666667 0.4866666666666667
probs:  [0.060079726336035096, 0.417792311947327, 0.060079726336035096, 0.21160904080890114, 0.060079726336035096, 0.19035946823566652]
maxi score, test score, baseline:  -0.7810866666666667 0.4866666666666667 0.4866666666666667
probs:  [0.060079726336035096, 0.417792311947327, 0.060079726336035096, 0.21160904080890114, 0.060079726336035096, 0.19035946823566652]
siam score:  -0.84288484
maxi score, test score, baseline:  -0.7810866666666667 0.4866666666666667 0.4866666666666667
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 29.204565624631837
printing an ep nov before normalisation:  34.86971481052113
actor:  1 policy actor:  1  step number:  48 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.7810866666666667 0.4866666666666667 0.4866666666666667
probs:  [0.06009657603274948, 0.4179097071074177, 0.06009657603274948, 0.21166848220004175, 0.06009657603274948, 0.1901320825942921]
actor:  1 policy actor:  1  step number:  62 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.73 ]
 [0.857]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]] [[55.399]
 [50.522]
 [55.399]
 [55.399]
 [55.399]
 [55.399]
 [55.399]] [[0.73 ]
 [0.857]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]]
Printing some Q and Qe and total Qs values:  [[0.699]
 [0.769]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]] [[33.986]
 [41.426]
 [33.986]
 [33.986]
 [33.986]
 [33.986]
 [33.986]] [[0.699]
 [0.769]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.22666666666666635  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.076]
 [-0.052]
 [-0.088]
 [-0.088]
 [-0.088]
 [-0.088]
 [-0.088]] [[47.786]
 [45.033]
 [35.557]
 [35.557]
 [35.557]
 [35.557]
 [35.557]] [[0.843]
 [0.784]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]]
printing an ep nov before normalisation:  44.72226491911985
printing an ep nov before normalisation:  28.230692594301424
printing an ep nov before normalisation:  54.154014750013665
actor:  0 policy actor:  1  step number:  54 total reward:  0.3399999999999993  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7784066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.06004644019976479, 0.41755365763855895, 0.06004644019976479, 0.21194491097932172, 0.06004644019976479, 0.19036211078282497]
printing an ep nov before normalisation:  64.56450874061129
maxi score, test score, baseline:  -0.7784066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.06004644019976479, 0.41755365763855895, 0.06004644019976479, 0.21194491097932172, 0.06004644019976479, 0.19036211078282497]
maxi score, test score, baseline:  -0.7784066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.06004644019976479, 0.41755365763855895, 0.06004644019976479, 0.21194491097932172, 0.06004644019976479, 0.19036211078282497]
maxi score, test score, baseline:  -0.7784066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.06004644019976479, 0.41755365763855895, 0.06004644019976479, 0.21194491097932172, 0.06004644019976479, 0.19036211078282497]
actor:  1 policy actor:  1  step number:  64 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  29.130680561065674
printing an ep nov before normalisation:  54.640120955231495
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.588]
 [0.594]
 [0.594]
 [0.56 ]
 [0.594]
 [0.594]] [[61.444]
 [59.588]
 [47.792]
 [47.792]
 [61.273]
 [47.792]
 [47.792]] [[0.512]
 [0.588]
 [0.594]
 [0.594]
 [0.56 ]
 [0.594]
 [0.594]]
maxi score, test score, baseline:  -0.7784066666666667 0.4866666666666667 0.4866666666666667
maxi score, test score, baseline:  -0.7784066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.06004644019976479, 0.41755365763855895, 0.06004644019976479, 0.21194491097932172, 0.06004644019976479, 0.19036211078282497]
Printing some Q and Qe and total Qs values:  [[-0.131]
 [-0.131]
 [-0.131]
 [-0.131]
 [-0.131]
 [-0.131]
 [-0.131]] [[28.949]
 [28.949]
 [28.949]
 [28.949]
 [28.949]
 [28.949]
 [28.949]] [[1.809]
 [1.809]
 [1.809]
 [1.809]
 [1.809]
 [1.809]
 [1.809]]
maxi score, test score, baseline:  -0.7784066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.060063248950060766, 0.4176707656250521, 0.060063248950060766, 0.21200433509472238, 0.060063248950060766, 0.19013515243004314]
actor:  1 policy actor:  1  step number:  59 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.15 ]
 [-0.113]
 [-0.127]
 [-0.127]
 [-0.127]
 [-0.127]
 [-0.127]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.15 ]
 [-0.113]
 [-0.127]
 [-0.127]
 [-0.127]
 [-0.127]
 [-0.127]]
printing an ep nov before normalisation:  51.80820958489309
maxi score, test score, baseline:  -0.7784066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.060063248950060766, 0.4176707656250521, 0.060063248950060766, 0.21200433509472238, 0.060063248950060766, 0.19013515243004314]
Printing some Q and Qe and total Qs values:  [[0.909]
 [1.009]
 [0.992]
 [0.992]
 [0.992]
 [0.992]
 [0.992]] [[57.115]
 [56.939]
 [51.655]
 [51.655]
 [51.655]
 [51.655]
 [51.655]] [[0.909]
 [1.009]
 [0.992]
 [0.992]
 [0.992]
 [0.992]
 [0.992]]
Printing some Q and Qe and total Qs values:  [[0.45 ]
 [0.45 ]
 [0.45 ]
 [0.517]
 [0.45 ]
 [0.45 ]
 [0.45 ]] [[54.973]
 [54.973]
 [54.973]
 [60.616]
 [54.973]
 [54.973]
 [54.973]] [[0.45 ]
 [0.45 ]
 [0.45 ]
 [0.517]
 [0.45 ]
 [0.45 ]
 [0.45 ]]
printing an ep nov before normalisation:  34.16006447349581
printing an ep nov before normalisation:  27.873969669641912
actor:  0 policy actor:  1  step number:  41 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.7755000000000001 0.4866666666666667 0.4866666666666667
actions average: 
K:  4  action  0 :  tensor([0.5529, 0.0173, 0.0896, 0.0976, 0.0898, 0.0870, 0.0658],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0253, 0.9171, 0.0139, 0.0133, 0.0050, 0.0089, 0.0165],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1942, 0.0202, 0.3223, 0.1151, 0.0812, 0.1833, 0.0837],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.3485, 0.0088, 0.1437, 0.1410, 0.1122, 0.1217, 0.1241],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1693, 0.0863, 0.0928, 0.1288, 0.3397, 0.0986, 0.0844],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.3893, 0.0707, 0.1202, 0.0949, 0.1071, 0.1322, 0.0855],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1490, 0.2465, 0.1431, 0.1029, 0.0595, 0.1450, 0.1539],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.7755000000000001 0.4866666666666667 0.4866666666666667
probs:  [0.060063248950060766, 0.4176707656250521, 0.060063248950060766, 0.21200433509472238, 0.060063248950060766, 0.19013515243004314]
actor:  0 policy actor:  1  step number:  49 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  9.844689763092807
actor:  1 policy actor:  1  step number:  42 total reward:  0.5266666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7729133333333333 0.4866666666666667 0.4866666666666667
Printing some Q and Qe and total Qs values:  [[-0.134]
 [-0.132]
 [-0.135]
 [-0.135]
 [-0.135]
 [-0.135]
 [-0.135]] [[77.531]
 [77.315]
 [75.358]
 [75.358]
 [75.358]
 [75.358]
 [75.358]] [[1.443]
 [1.44 ]
 [1.382]
 [1.382]
 [1.382]
 [1.382]
 [1.382]]
printing an ep nov before normalisation:  29.392907020704907
actions average: 
K:  2  action  0 :  tensor([0.6015, 0.0163, 0.0699, 0.0700, 0.1035, 0.0649, 0.0739],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0070, 0.9489, 0.0107, 0.0054, 0.0033, 0.0037, 0.0209],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1388, 0.0079, 0.2805, 0.1111, 0.1564, 0.2091, 0.0962],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1751, 0.0211, 0.0948, 0.3714, 0.1126, 0.1261, 0.0988],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2281, 0.1079, 0.1234, 0.1169, 0.1831, 0.1397, 0.1008],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0847, 0.0085, 0.1651, 0.1171, 0.0801, 0.4716, 0.0729],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1460, 0.1265, 0.1336, 0.1134, 0.1562, 0.1165, 0.2077],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.523]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]] [[35.781]
 [37.555]
 [35.781]
 [35.781]
 [35.781]
 [35.781]
 [35.781]] [[0.833]
 [1.072]
 [0.833]
 [0.833]
 [0.833]
 [0.833]
 [0.833]]
printing an ep nov before normalisation:  38.06147683357352
printing an ep nov before normalisation:  41.30250191706504
actor:  1 policy actor:  1  step number:  53 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.199]
 [0.289]
 [0.199]
 [0.199]
 [0.199]
 [0.199]
 [0.199]] [[34.807]
 [37.093]
 [34.807]
 [34.807]
 [34.807]
 [34.807]
 [34.807]] [[0.872]
 [1.068]
 [0.872]
 [0.872]
 [0.872]
 [0.872]
 [0.872]]
printing an ep nov before normalisation:  41.428818702697754
maxi score, test score, baseline:  -0.7729133333333333 0.4866666666666667 0.4866666666666667
probs:  [0.060926658572029405, 0.42380252253535167, 0.060926658572029405, 0.20723820692204076, 0.060926658572029405, 0.18617929482651951]
from probs:  [0.060926658572029405, 0.42380252253535167, 0.060926658572029405, 0.20723820692204076, 0.060926658572029405, 0.18617929482651951]
actions average: 
K:  2  action  0 :  tensor([0.5247, 0.0228, 0.0815, 0.1064, 0.0882, 0.0967, 0.0797],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0260, 0.9367, 0.0088, 0.0070, 0.0044, 0.0039, 0.0132],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2428, 0.0110, 0.2754, 0.0979, 0.1245, 0.1518, 0.0965],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1677, 0.0993, 0.0530, 0.4546, 0.0588, 0.0670, 0.0996],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1788, 0.1206, 0.0999, 0.1069, 0.2301, 0.1437, 0.1199],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1906, 0.0126, 0.1781, 0.0550, 0.0894, 0.4190, 0.0554],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2459, 0.1523, 0.0758, 0.1596, 0.0815, 0.0837, 0.2012],
       grad_fn=<DivBackward0>)
actions average: 
K:  2  action  0 :  tensor([0.7488, 0.0083, 0.0363, 0.0529, 0.0628, 0.0397, 0.0511],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0184, 0.8881, 0.0261, 0.0169, 0.0091, 0.0166, 0.0247],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2239, 0.1021, 0.1972, 0.1140, 0.1116, 0.1607, 0.0904],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.3060, 0.0022, 0.1082, 0.1208, 0.1625, 0.2052, 0.0951],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2228, 0.0426, 0.0784, 0.1031, 0.3675, 0.1152, 0.0704],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1365, 0.0225, 0.0784, 0.1038, 0.0905, 0.4993, 0.0689],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2952, 0.0864, 0.0784, 0.1846, 0.0761, 0.0914, 0.1880],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  36.36237485903166
printing an ep nov before normalisation:  27.292045515623062
actor:  1 policy actor:  1  step number:  60 total reward:  0.32666666666666555  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.7729133333333333 0.4866666666666667 0.4866666666666667
probs:  [0.060878208851388435, 0.42345844262034565, 0.060878208851388435, 0.20750565538755464, 0.060878208851388435, 0.18640127543793444]
from probs:  [0.060878208851388435, 0.42345844262034565, 0.060878208851388435, 0.20750565538755464, 0.060878208851388435, 0.18640127543793444]
maxi score, test score, baseline:  -0.7729133333333333 0.4866666666666667 0.4866666666666667
printing an ep nov before normalisation:  41.459223357364735
maxi score, test score, baseline:  -0.7729133333333333 0.4866666666666667 0.4866666666666667
probs:  [0.06082983800918679, 0.4231149228837337, 0.06082983800918679, 0.20777266843430275, 0.06082983800918679, 0.18662289465440313]
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.695]
 [0.556]
 [0.561]
 [0.491]
 [0.541]
 [0.543]] [[48.44 ]
 [39.041]
 [25.076]
 [30.838]
 [44.121]
 [31.136]
 [29.589]] [[0.501]
 [0.695]
 [0.556]
 [0.561]
 [0.491]
 [0.541]
 [0.543]]
actions average: 
K:  4  action  0 :  tensor([0.5631, 0.1281, 0.0489, 0.0631, 0.0732, 0.0727, 0.0509],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0216, 0.9073, 0.0122, 0.0234, 0.0077, 0.0075, 0.0202],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1255, 0.1423, 0.4366, 0.0510, 0.0434, 0.1277, 0.0735],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1656, 0.1319, 0.1192, 0.2456, 0.0978, 0.1246, 0.1153],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2364, 0.0280, 0.1258, 0.0825, 0.2907, 0.1540, 0.0825],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2290, 0.0056, 0.1475, 0.1261, 0.1265, 0.2441, 0.1213],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1771, 0.0508, 0.1112, 0.1651, 0.1303, 0.1276, 0.2379],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  33.8964613099972
printing an ep nov before normalisation:  52.35787889477358
printing an ep nov before normalisation:  62.53317914269959
maxi score, test score, baseline:  -0.7729133333333333 0.4866666666666667 0.4866666666666667
using explorer policy with actor:  1
printing an ep nov before normalisation:  53.83347128730474
maxi score, test score, baseline:  -0.7729133333333333 0.4866666666666667 0.4866666666666667
actor:  1 policy actor:  1  step number:  52 total reward:  0.3399999999999995  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  48.04196102153469
maxi score, test score, baseline:  -0.7729133333333333 0.4866666666666667 0.4866666666666667
probs:  [0.06073333219084681, 0.4224295584826039, 0.06073333219084681, 0.2083053925178835, 0.06073333219084681, 0.1870650524269724]
siam score:  -0.85161775
maxi score, test score, baseline:  -0.7729133333333333 0.4866666666666667 0.4866666666666667
probs:  [0.06073333219084681, 0.4224295584826039, 0.06073333219084681, 0.2083053925178835, 0.06073333219084681, 0.1870650524269724]
Printing some Q and Qe and total Qs values:  [[-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]] [[76.283]
 [76.283]
 [76.283]
 [76.283]
 [76.283]
 [76.283]
 [76.283]] [[1.886]
 [1.886]
 [1.886]
 [1.886]
 [1.886]
 [1.886]
 [1.886]]
Printing some Q and Qe and total Qs values:  [[-0.028]
 [ 0.005]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.028]] [[51.858]
 [59.305]
 [51.858]
 [51.858]
 [51.858]
 [51.858]
 [51.858]] [[1.396]
 [1.777]
 [1.396]
 [1.396]
 [1.396]
 [1.396]
 [1.396]]
printing an ep nov before normalisation:  31.255223203102894
actions average: 
K:  1  action  0 :  tensor([0.6069, 0.0053, 0.0661, 0.0743, 0.0952, 0.0811, 0.0711],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0113, 0.9581, 0.0046, 0.0040, 0.0034, 0.0022, 0.0164],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2851, 0.0975, 0.1974, 0.0784, 0.0953, 0.1239, 0.1224],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.3053, 0.0580, 0.0620, 0.3055, 0.0908, 0.0917, 0.0867],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1848, 0.0130, 0.1056, 0.1122, 0.3437, 0.1252, 0.1155],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1883, 0.0375, 0.1311, 0.0812, 0.1045, 0.3690, 0.0882],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2118, 0.1088, 0.0981, 0.1091, 0.1047, 0.1085, 0.2590],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.7729133333333333 0.4866666666666667 0.4866666666666667
probs:  [0.06073333219084681, 0.4224295584826039, 0.06073333219084681, 0.2083053925178835, 0.06073333219084681, 0.1870650524269724]
actor:  1 policy actor:  1  step number:  51 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  28.378026495173042
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7729133333333333 0.4866666666666667 0.4866666666666667
probs:  [0.06063713958474176, 0.4217464184501133, 0.06063713958474176, 0.20883638763109005, 0.06063713958474176, 0.1875057751645714]
using another actor
actor:  1 policy actor:  1  step number:  68 total reward:  0.13999999999999968  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.7729133333333333 0.4866666666666667 0.4866666666666667
probs:  [0.06065362946334579, 0.42186132590028386, 0.06065362946334579, 0.20889326808106495, 0.06065362946334579, 0.18728451762861392]
maxi score, test score, baseline:  -0.7729133333333333 0.4866666666666667 0.4866666666666667
probs:  [0.06065362946334579, 0.42186132590028386, 0.06065362946334579, 0.20889326808106495, 0.06065362946334579, 0.18728451762861392]
actor:  1 policy actor:  1  step number:  49 total reward:  0.4933333333333333  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7729133333333333 0.4866666666666667 0.4866666666666667
probs:  [0.060632806628465194, 0.4217134444080523, 0.060632806628465194, 0.2090083483093185, 0.060632806628465194, 0.18737978739723354]
maxi score, test score, baseline:  -0.7729133333333333 0.4866666666666667 0.4866666666666667
probs:  [0.060632806628465194, 0.4217134444080523, 0.060632806628465194, 0.2090083483093185, 0.060632806628465194, 0.18737978739723354]
printing an ep nov before normalisation:  15.783050986665087
printing an ep nov before normalisation:  35.88353763408949
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  27.48965199292615
maxi score, test score, baseline:  -0.7729133333333333 0.4866666666666667 0.4866666666666667
maxi score, test score, baseline:  -0.7729133333333333 0.4866666666666667 0.4866666666666667
probs:  [0.060632806628465194, 0.4217134444080523, 0.060632806628465194, 0.2090083483093185, 0.060632806628465194, 0.18737978739723354]
actor:  0 policy actor:  1  step number:  77 total reward:  0.239999999999999  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.7704333333333334 0.4866666666666667 0.4866666666666667
probs:  [0.060632806628465194, 0.4217134444080523, 0.060632806628465194, 0.2090083483093185, 0.060632806628465194, 0.18737978739723354]
actor:  1 policy actor:  1  step number:  59 total reward:  0.15999999999999925  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  69.73667567902955
printing an ep nov before normalisation:  71.31503277764708
maxi score, test score, baseline:  -0.7704333333333334 0.4866666666666667 0.4866666666666667
maxi score, test score, baseline:  -0.7704333333333334 0.4866666666666667 0.4866666666666667
probs:  [0.06058496175557211, 0.4213736553620543, 0.06058496175557211, 0.20927276950484136, 0.06058496175557211, 0.18759868986638817]
Printing some Q and Qe and total Qs values:  [[-0.051]
 [ 0.071]
 [-0.054]
 [-0.056]
 [-0.054]
 [-0.054]
 [-0.056]] [[36.307]
 [52.596]
 [28.389]
 [30.002]
 [30.051]
 [29.963]
 [28.888]] [[0.737]
 [1.476]
 [0.435]
 [0.494]
 [0.497]
 [0.494]
 [0.452]]
maxi score, test score, baseline:  -0.7704333333333334 0.4866666666666667 0.4866666666666667
probs:  [0.06058496175557211, 0.4213736553620543, 0.06058496175557211, 0.20927276950484136, 0.06058496175557211, 0.18759868986638817]
Printing some Q and Qe and total Qs values:  [[0.318]
 [0.419]
 [0.318]
 [0.318]
 [0.318]
 [0.318]
 [0.318]] [[48.506]
 [48.219]
 [48.506]
 [48.506]
 [48.506]
 [48.506]
 [48.506]] [[1.991]
 [2.076]
 [1.991]
 [1.991]
 [1.991]
 [1.991]
 [1.991]]
actor:  0 policy actor:  0  step number:  69 total reward:  0.09333333333333216  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  58.689473628990434
maxi score, test score, baseline:  -0.7682466666666667 0.4866666666666667 0.4866666666666667
probs:  [0.06058496175557211, 0.4213736553620543, 0.06058496175557211, 0.20927276950484136, 0.06058496175557211, 0.18759868986638817]
maxi score, test score, baseline:  -0.7682466666666667 0.4866666666666667 0.4866666666666667
probs:  [0.06058496175557211, 0.4213736553620543, 0.06058496175557211, 0.20927276950484136, 0.06058496175557211, 0.18759868986638817]
printing an ep nov before normalisation:  50.58242453408418
maxi score, test score, baseline:  -0.7682466666666667 0.4866666666666667 0.4866666666666667
probs:  [0.06058496175557211, 0.4213736553620543, 0.06058496175557211, 0.20927276950484136, 0.06058496175557211, 0.18759868986638817]
line 256 mcts: sample exp_bonus 30.95145238576509
Printing some Q and Qe and total Qs values:  [[0.19]
 [0.26]
 [0.19]
 [0.19]
 [0.19]
 [0.19]
 [0.19]] [[61.043]
 [64.315]
 [61.043]
 [61.043]
 [61.043]
 [61.043]
 [61.043]] [[0.78 ]
 [0.894]
 [0.78 ]
 [0.78 ]
 [0.78 ]
 [0.78 ]
 [0.78 ]]
maxi score, test score, baseline:  -0.7682466666666667 0.4866666666666667 0.4866666666666667
probs:  [0.06058496175557211, 0.4213736553620543, 0.06058496175557211, 0.20927276950484136, 0.06058496175557211, 0.18759868986638817]
printing an ep nov before normalisation:  40.321207858778564
actions average: 
K:  3  action  0 :  tensor([0.6266, 0.0044, 0.0659, 0.0617, 0.1212, 0.0688, 0.0515],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0297, 0.9002, 0.0198, 0.0146, 0.0042, 0.0066, 0.0250],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.3157, 0.1921, 0.2297, 0.0735, 0.0534, 0.0738, 0.0618],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1541, 0.0315, 0.1138, 0.3041, 0.1692, 0.1344, 0.0930],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2809, 0.0051, 0.0872, 0.0938, 0.3626, 0.1085, 0.0619],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.2254, 0.0041, 0.1460, 0.1001, 0.1975, 0.2517, 0.0751],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2727, 0.1779, 0.0752, 0.0711, 0.0575, 0.0529, 0.2927],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.7682466666666667 0.4866666666666667 0.4866666666666667
probs:  [0.06058496175557211, 0.4213736553620543, 0.06058496175557211, 0.20927276950484136, 0.06058496175557211, 0.18759868986638817]
printing an ep nov before normalisation:  50.18600728890234
actions average: 
K:  4  action  0 :  tensor([0.6332, 0.0754, 0.0590, 0.0615, 0.0702, 0.0480, 0.0527],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0270, 0.9094, 0.0127, 0.0137, 0.0077, 0.0059, 0.0236],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.3056, 0.0422, 0.1138, 0.1700, 0.1230, 0.1584, 0.0870],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2847, 0.1384, 0.0490, 0.2179, 0.1524, 0.0842, 0.0734],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2103, 0.0756, 0.0787, 0.1069, 0.2892, 0.0901, 0.1492],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2798, 0.0194, 0.1297, 0.1092, 0.1103, 0.2814, 0.0703],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.3216, 0.1402, 0.0856, 0.1223, 0.0833, 0.0755, 0.1714],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  58 total reward:  0.13999999999999946  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.289]
 [0.522]
 [0.319]
 [0.389]
 [0.278]
 [0.288]
 [0.327]] [[33.184]
 [32.919]
 [24.45 ]
 [28.104]
 [30.709]
 [28.5  ]
 [24.06 ]] [[0.694]
 [0.923]
 [0.617]
 [0.731]
 [0.652]
 [0.636]
 [0.62 ]]
actor:  1 policy actor:  1  step number:  61 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  42.2432024390385
printing an ep nov before normalisation:  26.730027198791504
Printing some Q and Qe and total Qs values:  [[-0.135]
 [-0.117]
 [-0.13 ]
 [-0.135]
 [-0.135]
 [-0.135]
 [-0.13 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.135]
 [-0.117]
 [-0.13 ]
 [-0.135]
 [-0.135]
 [-0.135]
 [-0.13 ]]
printing an ep nov before normalisation:  35.40187563214983
maxi score, test score, baseline:  -0.7682466666666667 0.4866666666666667 0.4866666666666667
probs:  [0.060506005799995384, 0.42081070933511816, 0.060506005799995384, 0.20985765647525703, 0.060506005799995384, 0.1878136167896387]
printing an ep nov before normalisation:  30.915343761444092
maxi score, test score, baseline:  -0.7682466666666667 0.4866666666666667 0.4866666666666667
probs:  [0.060506005799995384, 0.42081070933511816, 0.060506005799995384, 0.20985765647525703, 0.060506005799995384, 0.1878136167896387]
Printing some Q and Qe and total Qs values:  [[0.249]
 [0.228]
 [0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.249]] [[42.76 ]
 [46.429]
 [42.76 ]
 [42.76 ]
 [42.76 ]
 [42.76 ]
 [42.76 ]] [[1.277]
 [1.387]
 [1.277]
 [1.277]
 [1.277]
 [1.277]
 [1.277]]
Printing some Q and Qe and total Qs values:  [[-0.117]
 [-0.082]
 [-0.114]
 [-0.104]
 [-0.116]
 [-0.103]
 [-0.115]] [[27.976]
 [32.481]
 [27.188]
 [28.92 ]
 [27.891]
 [28.749]
 [27.62 ]] [[0.054]
 [0.15 ]
 [0.047]
 [0.08 ]
 [0.054]
 [0.079]
 [0.051]]
line 256 mcts: sample exp_bonus 24.008742339971658
printing an ep nov before normalisation:  42.9405457236539
maxi score, test score, baseline:  -0.7682466666666667 0.4866666666666667 0.4866666666666667
probs:  [0.06045841359187381, 0.4204727100868652, 0.06045841359187381, 0.21012099049222216, 0.06045841359187381, 0.18803105864529102]
Printing some Q and Qe and total Qs values:  [[-0.048]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]] [[34.444]
 [34.444]
 [34.444]
 [34.444]
 [34.444]
 [34.444]
 [34.444]] [[0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]]
printing an ep nov before normalisation:  38.00874233031688
maxi score, test score, baseline:  -0.7682466666666667 0.4866666666666667 0.4866666666666667
maxi score, test score, baseline:  -0.7682466666666667 0.4866666666666667 0.4866666666666667
probs:  [0.06045841359187381, 0.4204727100868652, 0.06045841359187381, 0.21012099049222216, 0.06045841359187381, 0.18803105864529102]
printing an ep nov before normalisation:  36.00561076196896
siam score:  -0.8472454
printing an ep nov before normalisation:  34.62761741333234
actor:  1 policy actor:  1  step number:  46 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.7682466666666667 0.4866666666666667 0.4866666666666667
probs:  [0.06045841359187381, 0.4204727100868652, 0.06045841359187381, 0.21012099049222216, 0.06045841359187381, 0.18803105864529102]
actor:  1 policy actor:  1  step number:  66 total reward:  0.0066666666666659324  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.7682466666666667 0.4866666666666667 0.4866666666666667
probs:  [0.06045841359187381, 0.4204727100868652, 0.06045841359187381, 0.21012099049222216, 0.06045841359187381, 0.18803105864529102]
printing an ep nov before normalisation:  41.52479223459826
printing an ep nov before normalisation:  60.31421009872135
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.528]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]] [[35.052]
 [39.218]
 [35.052]
 [35.052]
 [35.052]
 [35.052]
 [35.052]] [[1.22 ]
 [1.322]
 [1.22 ]
 [1.22 ]
 [1.22 ]
 [1.22 ]
 [1.22 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7682466666666667 0.4866666666666667 0.4866666666666667
probs:  [0.06045841359187381, 0.4204727100868652, 0.06045841359187381, 0.21012099049222216, 0.06045841359187381, 0.18803105864529102]
printing an ep nov before normalisation:  35.1195163266491
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.13091046879765145
maxi score, test score, baseline:  -0.7682466666666667 0.4866666666666667 0.4866666666666667
probs:  [0.06045841359187381, 0.4204727100868652, 0.06045841359187381, 0.21012099049222216, 0.06045841359187381, 0.18803105864529102]
printing an ep nov before normalisation:  46.45901268832498
actor:  1 policy actor:  1  step number:  47 total reward:  0.5466666666666667  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  46 total reward:  0.31333333333333313  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.7682466666666667 0.4866666666666667 0.4866666666666667
probs:  [0.05850740279535227, 0.4068782744431491, 0.05850740279535227, 0.23564536798687932, 0.05850740279535227, 0.1819541491839148]
siam score:  -0.85041106
maxi score, test score, baseline:  -0.7682466666666667 0.4866666666666667 0.4866666666666667
probs:  [0.05850740279535227, 0.4068782744431491, 0.05850740279535227, 0.23564536798687932, 0.05850740279535227, 0.1819541491839148]
line 256 mcts: sample exp_bonus 27.99529635412102
printing an ep nov before normalisation:  39.009809339586134
maxi score, test score, baseline:  -0.7682466666666667 0.4866666666666667 0.4866666666666667
probs:  [0.05850740279535227, 0.4068782744431491, 0.05850740279535227, 0.23564536798687932, 0.05850740279535227, 0.1819541491839148]
actor:  1 policy actor:  1  step number:  62 total reward:  0.3266666666666661  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.7682466666666667 0.4866666666666667 0.4866666666666667
probs:  [0.058522816836640415, 0.4069856778427996, 0.058522816836640415, 0.23570755633993046, 0.058522816836640415, 0.1817383153073487]
maxi score, test score, baseline:  -0.7682466666666667 0.4866666666666667 0.4866666666666667
maxi score, test score, baseline:  -0.7682466666666667 0.4866666666666667 0.4866666666666667
probs:  [0.058522816836640415, 0.4069856778427996, 0.058522816836640415, 0.23570755633993046, 0.058522816836640415, 0.1817383153073487]
printing an ep nov before normalisation:  40.54203290380242
actor:  0 policy actor:  1  step number:  49 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.7656333333333334 0.4866666666666667 0.4866666666666667
probs:  [0.058522816836640415, 0.4069856778427996, 0.058522816836640415, 0.23570755633993046, 0.058522816836640415, 0.1817383153073487]
actor:  0 policy actor:  0  step number:  54 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.7629 0.4866666666666667 0.4866666666666667
probs:  [0.058522816836640415, 0.4069856778427996, 0.058522816836640415, 0.23570755633993046, 0.058522816836640415, 0.1817383153073487]
maxi score, test score, baseline:  -0.7629 0.4866666666666667 0.4866666666666667
printing an ep nov before normalisation:  50.47727675004186
actions average: 
K:  0  action  0 :  tensor([0.6063, 0.0054, 0.0793, 0.0715, 0.0901, 0.0707, 0.0768],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0118, 0.9499, 0.0167, 0.0050, 0.0016, 0.0027, 0.0122],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2582, 0.0096, 0.2137, 0.1186, 0.1343, 0.1450, 0.1205],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2900, 0.0474, 0.0938, 0.2707, 0.0965, 0.0950, 0.1066],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.3437, 0.0134, 0.0771, 0.1184, 0.3093, 0.0712, 0.0669],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.2416, 0.0163, 0.1425, 0.0829, 0.1007, 0.3387, 0.0773],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1145, 0.0879, 0.1116, 0.1466, 0.0900, 0.0554, 0.3941],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.7629 0.4866666666666667 0.4866666666666667
probs:  [0.058522816836640415, 0.4069856778427996, 0.058522816836640415, 0.23570755633993046, 0.058522816836640415, 0.1817383153073487]
actor:  1 policy actor:  1  step number:  65 total reward:  0.13333333333333253  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  55 total reward:  0.2933333333333331  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  53 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  1.333
siam score:  -0.853672
actions average: 
K:  2  action  0 :  tensor([0.5972, 0.0425, 0.0724, 0.0767, 0.0861, 0.0715, 0.0535],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0149, 0.9269, 0.0158, 0.0124, 0.0037, 0.0059, 0.0205],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1843, 0.0085, 0.2517, 0.1392, 0.1330, 0.1553, 0.1281],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.3311, 0.0633, 0.0975, 0.1976, 0.0928, 0.0973, 0.1203],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2577, 0.0137, 0.1297, 0.1550, 0.1648, 0.1456, 0.1335],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1303, 0.0039, 0.1531, 0.1075, 0.0769, 0.4657, 0.0626],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.3023, 0.0127, 0.0946, 0.1419, 0.0947, 0.0901, 0.2639],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  81.81991453656656
printing an ep nov before normalisation:  64.69625682564292
Printing some Q and Qe and total Qs values:  [[-0.121]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.116]] [[62.382]
 [33.614]
 [33.614]
 [33.614]
 [33.614]
 [33.614]
 [33.614]] [[0.409]
 [0.083]
 [0.083]
 [0.083]
 [0.083]
 [0.083]
 [0.083]]
maxi score, test score, baseline:  -0.7601533333333333 0.4866666666666667 0.4866666666666667
maxi score, test score, baseline:  -0.7601533333333333 0.4866666666666667 0.4866666666666667
maxi score, test score, baseline:  -0.7601533333333333 0.4866666666666667 0.4866666666666667
probs:  [0.05848831734777702, 0.40673912077093394, 0.05848831734777702, 0.2360755389270468, 0.05848831734777702, 0.18172038825868822]
printing an ep nov before normalisation:  38.86359922723426
printing an ep nov before normalisation:  66.2519146321263
actor:  1 policy actor:  1  step number:  55 total reward:  0.4800000000000001  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  36.87257812981409
Printing some Q and Qe and total Qs values:  [[ 0.014]
 [ 0.421]
 [-0.007]
 [ 0.156]
 [ 0.057]
 [ 0.095]
 [ 0.271]] [[33.882]
 [41.18 ]
 [31.566]
 [28.815]
 [31.626]
 [30.188]
 [30.702]] [[0.602]
 [1.253]
 [0.503]
 [0.575]
 [0.57 ]
 [0.56 ]
 [0.752]]
actions average: 
K:  2  action  0 :  tensor([0.4089, 0.0711, 0.0971, 0.0924, 0.1222, 0.1156, 0.0927],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0237, 0.9315, 0.0083, 0.0096, 0.0064, 0.0036, 0.0169],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2910, 0.0222, 0.2657, 0.0935, 0.1137, 0.1281, 0.0858],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2526, 0.0329, 0.1179, 0.2943, 0.0963, 0.0897, 0.1163],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2271, 0.0201, 0.0699, 0.1120, 0.4072, 0.0839, 0.0800],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1577, 0.0268, 0.2305, 0.0738, 0.0894, 0.3541, 0.0677],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2292, 0.1355, 0.0970, 0.1647, 0.1131, 0.1062, 0.1543],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.7601533333333333 0.4866666666666667 0.4866666666666667
probs:  [0.05846915973632161, 0.40659946853694096, 0.05846915973632161, 0.23650506775780247, 0.05846915973632161, 0.1814879844962918]
maxi score, test score, baseline:  -0.7601533333333333 0.4866666666666667 0.4866666666666667
probs:  [0.05846915973632161, 0.40659946853694096, 0.05846915973632161, 0.23650506775780247, 0.05846915973632161, 0.1814879844962918]
Printing some Q and Qe and total Qs values:  [[0.32 ]
 [0.4  ]
 [0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]] [[53.364]
 [55.095]
 [46.705]
 [46.705]
 [46.705]
 [46.705]
 [46.705]] [[1.798]
 [1.941]
 [1.645]
 [1.645]
 [1.645]
 [1.645]
 [1.645]]
actions average: 
K:  2  action  0 :  tensor([0.4866, 0.0746, 0.0864, 0.0910, 0.0987, 0.0960, 0.0668],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0100, 0.9590, 0.0050, 0.0079, 0.0049, 0.0029, 0.0102],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2295, 0.0277, 0.3176, 0.0837, 0.0608, 0.1857, 0.0950],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2186, 0.0113, 0.1286, 0.1421, 0.1808, 0.1883, 0.1303],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2893, 0.0050, 0.0670, 0.0759, 0.3739, 0.1242, 0.0646],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1763, 0.0687, 0.0964, 0.0838, 0.1169, 0.3699, 0.0880],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.3010, 0.0623, 0.0896, 0.0887, 0.1056, 0.0891, 0.2637],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  39 total reward:  0.5333333333333332  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7601533333333333 0.4866666666666667 0.4866666666666667
probs:  [0.057344173662337065, 0.39876092309574035, 0.057344173662337065, 0.25121592830676925, 0.057344173662337065, 0.17799062761047932]
maxi score, test score, baseline:  -0.7601533333333333 0.4866666666666667 0.4866666666666667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.521]
 [0.602]
 [0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.521]] [[39.425]
 [42.076]
 [39.425]
 [39.425]
 [39.425]
 [39.425]
 [39.425]] [[2.238]
 [2.435]
 [2.238]
 [2.238]
 [2.238]
 [2.238]
 [2.238]]
maxi score, test score, baseline:  -0.7601533333333333 0.4866666666666667 0.4866666666666667
probs:  [0.057344173662337065, 0.39876092309574035, 0.057344173662337065, 0.25121592830676925, 0.057344173662337065, 0.17799062761047932]
printing an ep nov before normalisation:  34.29940596108697
Printing some Q and Qe and total Qs values:  [[0.644]
 [0.72 ]
 [0.659]
 [0.64 ]
 [0.597]
 [0.637]
 [0.612]] [[36.301]
 [26.453]
 [34.354]
 [32.478]
 [34.291]
 [34.217]
 [33.017]] [[1.212]
 [1.134]
 [1.196]
 [1.148]
 [1.134]
 [1.173]
 [1.128]]
actor:  1 policy actor:  1  step number:  50 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  71 total reward:  0.22666666666666613  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  80.81947157507983
maxi score, test score, baseline:  -0.7601533333333333 0.4866666666666667 0.4866666666666667
probs:  [0.05730793994760272, 0.39850241497991484, 0.05730793994760272, 0.2516086220163332, 0.05730793994760272, 0.1779651431609437]
from probs:  [0.05730793994760272, 0.39850241497991484, 0.05730793994760272, 0.2516086220163332, 0.05730793994760272, 0.1779651431609437]
actor:  1 policy actor:  1  step number:  55 total reward:  0.31999999999999984  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.114]
 [-0.097]
 [-0.125]
 [-0.127]
 [-0.126]
 [-0.127]
 [-0.108]] [[53.636]
 [52.204]
 [49.207]
 [49.551]
 [49.413]
 [49.049]
 [50.298]] [[0.474]
 [0.463]
 [0.376]
 [0.381]
 [0.379]
 [0.371]
 [0.415]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.148]
 [-0.148]
 [-0.148]
 [-0.148]
 [-0.148]
 [-0.148]
 [-0.148]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.148]
 [-0.148]
 [-0.148]
 [-0.148]
 [-0.148]
 [-0.148]
 [-0.148]]
Printing some Q and Qe and total Qs values:  [[-0.143]
 [-0.143]
 [-0.143]
 [-0.143]
 [-0.143]
 [-0.143]
 [-0.143]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.143]
 [-0.143]
 [-0.143]
 [-0.143]
 [-0.143]
 [-0.143]
 [-0.143]]
maxi score, test score, baseline:  -0.7601533333333333 0.4866666666666667 0.4866666666666667
probs:  [0.05730793994760272, 0.39850241497991484, 0.05730793994760272, 0.2516086220163332, 0.05730793994760272, 0.1779651431609437]
printing an ep nov before normalisation:  60.083010672091575
maxi score, test score, baseline:  -0.7601533333333333 0.4866666666666667 0.4866666666666667
maxi score, test score, baseline:  -0.7601533333333333 0.4866666666666667 0.4866666666666667
probs:  [0.05730793994760272, 0.39850241497991484, 0.05730793994760272, 0.2516086220163332, 0.05730793994760272, 0.1779651431609437]
line 256 mcts: sample exp_bonus 48.58512878417969
maxi score, test score, baseline:  -0.7601533333333333 0.4866666666666667 0.4866666666666667
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  -0.7601533333333333 0.4866666666666667 0.4866666666666667
maxi score, test score, baseline:  -0.7601533333333333 0.4866666666666667 0.4866666666666667
probs:  [0.05725715944373454, 0.39814256040880147, 0.05725715944373454, 0.25193647442458583, 0.05725715944373454, 0.17814948683540893]
maxi score, test score, baseline:  -0.7601533333333333 0.4866666666666667 0.4866666666666667
probs:  [0.05725715944373454, 0.39814256040880147, 0.05725715944373454, 0.25193647442458583, 0.05725715944373454, 0.17814948683540893]
maxi score, test score, baseline:  -0.7601533333333333 0.4866666666666667 0.4866666666666667
probs:  [0.05725715944373454, 0.39814256040880147, 0.05725715944373454, 0.25193647442458583, 0.05725715944373454, 0.17814948683540893]
maxi score, test score, baseline:  -0.7601533333333333 0.4866666666666667 0.4866666666666667
probs:  [0.05725715944373454, 0.39814256040880147, 0.05725715944373454, 0.25193647442458583, 0.05725715944373454, 0.17814948683540893]
printing an ep nov before normalisation:  28.637661888049813
maxi score, test score, baseline:  -0.7601533333333333 0.4866666666666667 0.4866666666666667
probs:  [0.05725715944373454, 0.39814256040880147, 0.05725715944373454, 0.25193647442458583, 0.05725715944373454, 0.17814948683540893]
actor:  1 policy actor:  1  step number:  58 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7601533333333333 0.4866666666666667 0.4866666666666667
probs:  [0.05720647085655015, 0.39778335720260777, 0.05720647085655015, 0.2522637333943368, 0.05720647085655015, 0.178333496833405]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.133]
 [-0.091]
 [-0.099]
 [-0.138]
 [-0.138]
 [-0.101]
 [-0.136]] [[38.093]
 [45.549]
 [44.231]
 [38.997]
 [38.782]
 [45.727]
 [38.541]] [[0.229]
 [0.409]
 [0.377]
 [0.241]
 [0.237]
 [0.403]
 [0.234]]
printing an ep nov before normalisation:  44.691630794008844
maxi score, test score, baseline:  -0.7601533333333333 0.4866666666666667 0.4866666666666667
probs:  [0.05720647085655015, 0.39778335720260777, 0.05720647085655015, 0.2522637333943368, 0.05720647085655015, 0.178333496833405]
from probs:  [0.05720647085655015, 0.39778335720260777, 0.05720647085655015, 0.2522637333943368, 0.05720647085655015, 0.178333496833405]
maxi score, test score, baseline:  -0.7601533333333333 0.4866666666666667 0.4866666666666667
probs:  [0.05710536843577807, 0.3970668978236322, 0.05710536843577807, 0.2529164774517086, 0.05710536843577807, 0.17870051941732482]
actor:  0 policy actor:  0  step number:  58 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.146]
 [-0.146]
 [-0.146]
 [-0.146]
 [-0.146]
 [-0.146]
 [-0.146]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.146]
 [-0.146]
 [-0.146]
 [-0.146]
 [-0.146]
 [-0.146]
 [-0.146]]
siam score:  -0.85013616
printing an ep nov before normalisation:  83.43838942988569
printing an ep nov before normalisation:  68.78629975716416
maxi score, test score, baseline:  -0.7577133333333332 0.4866666666666667 0.4866666666666667
probs:  [0.05710536843577807, 0.3970668978236322, 0.05710536843577807, 0.2529164774517086, 0.05710536843577807, 0.17870051941732482]
line 256 mcts: sample exp_bonus 50.59182899421667
printing an ep nov before normalisation:  64.38260967640744
Printing some Q and Qe and total Qs values:  [[-0.026]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]] [[66.292]
 [64.71 ]
 [64.71 ]
 [64.71 ]
 [64.71 ]
 [64.71 ]
 [64.71 ]] [[1.546]
 [1.504]
 [1.504]
 [1.504]
 [1.504]
 [1.504]
 [1.504]]
Printing some Q and Qe and total Qs values:  [[-0.075]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]] [[47.039]
 [37.329]
 [37.329]
 [37.329]
 [37.329]
 [37.329]
 [37.329]] [[1.65 ]
 [1.068]
 [1.068]
 [1.068]
 [1.068]
 [1.068]
 [1.068]]
maxi score, test score, baseline:  -0.7577133333333332 0.4866666666666667 0.4866666666666667
probs:  [0.05710536843577807, 0.3970668978236322, 0.05710536843577807, 0.2529164774517086, 0.05710536843577807, 0.17870051941732482]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]] [[68.619]
 [68.619]
 [68.619]
 [68.619]
 [68.619]
 [68.619]
 [68.619]] [[2.318]
 [2.318]
 [2.318]
 [2.318]
 [2.318]
 [2.318]
 [2.318]]
printing an ep nov before normalisation:  34.0472412109375
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
actor:  1 policy actor:  1  step number:  58 total reward:  0.2599999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  68 total reward:  0.12666666666666637  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  90.42630304178564
maxi score, test score, baseline:  -0.7577133333333332 0.4866666666666667 0.4866666666666667
siam score:  -0.84238005
printing an ep nov before normalisation:  43.46147487047695
actor:  1 policy actor:  1  step number:  58 total reward:  0.3533333333333325  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  26.85552193136912
maxi score, test score, baseline:  -0.7577133333333332 0.4866666666666667 0.4866666666666667
actor:  1 policy actor:  1  step number:  41 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  0.667
from probs:  [0.057134636958800555, 0.39727081612381887, 0.057134636958800555, 0.25304634078796623, 0.057134636958800555, 0.17827893221181307]
Printing some Q and Qe and total Qs values:  [[0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]] [[32.709]
 [32.709]
 [32.709]
 [32.709]
 [32.709]
 [32.709]
 [32.709]] [[2.745]
 [2.745]
 [2.745]
 [2.745]
 [2.745]
 [2.745]
 [2.745]]
using another actor
maxi score, test score, baseline:  -0.7577133333333332 0.4866666666666667 0.4866666666666667
Printing some Q and Qe and total Qs values:  [[0.073]
 [0.117]
 [0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.073]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.073]
 [0.117]
 [0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.073]]
maxi score, test score, baseline:  -0.7577133333333332 0.4866666666666667 0.4866666666666667
probs:  [0.05703395960470477, 0.3965573555644717, 0.05703395960470477, 0.2536975641529116, 0.05703395960470477, 0.17864320146850224]
using explorer policy with actor:  0
printing an ep nov before normalisation:  0.002025314674369838
actor:  1 policy actor:  1  step number:  55 total reward:  0.33333333333333315  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.7577133333333332 0.4866666666666667 0.4866666666666667
probs:  [0.057048542690103544, 0.3966589550805899, 0.057048542690103544, 0.25376255014168564, 0.057048542690103544, 0.17843286670741387]
maxi score, test score, baseline:  -0.7577133333333332 0.4866666666666667 0.4866666666666667
probs:  [0.057048542690103544, 0.3966589550805899, 0.057048542690103544, 0.25376255014168564, 0.057048542690103544, 0.17843286670741387]
printing an ep nov before normalisation:  30.11383096361463
actor:  0 policy actor:  1  step number:  48 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7550866666666666 0.4866666666666667 0.4866666666666667
probs:  [0.057048542690103544, 0.3966589550805899, 0.057048542690103544, 0.25376255014168564, 0.057048542690103544, 0.17843286670741387]
printing an ep nov before normalisation:  24.190707441025534
printing an ep nov before normalisation:  40.212526840280134
siam score:  -0.8528508
siam score:  -0.8533237
maxi score, test score, baseline:  -0.7550866666666666 0.4866666666666667 0.4866666666666667
actor:  1 policy actor:  1  step number:  54 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  64.1341098250884
maxi score, test score, baseline:  -0.7550866666666666 0.4866666666666667 0.4866666666666667
maxi score, test score, baseline:  -0.7550866666666666 0.4866666666666667 0.4866666666666667
probs:  [0.05694825720192864, 0.39594826487043067, 0.05694825720192864, 0.254411845827158, 0.05694825720192864, 0.17879511769662557]
printing an ep nov before normalisation:  37.26746580702475
maxi score, test score, baseline:  -0.7550866666666666 0.4866666666666667 0.4866666666666667
probs:  [0.05694825720192864, 0.39594826487043067, 0.05694825720192864, 0.254411845827158, 0.05694825720192864, 0.17879511769662557]
Printing some Q and Qe and total Qs values:  [[0.092]
 [0.092]
 [0.092]
 [0.092]
 [0.092]
 [0.092]
 [0.092]] [[58.104]
 [58.104]
 [58.104]
 [58.104]
 [58.104]
 [58.104]
 [58.104]] [[1.756]
 [1.756]
 [1.756]
 [1.756]
 [1.756]
 [1.756]
 [1.756]]
maxi score, test score, baseline:  -0.7550866666666667 0.4866666666666667 0.4866666666666667
Printing some Q and Qe and total Qs values:  [[-0.124]
 [-0.129]
 [-0.136]
 [-0.172]
 [-0.121]
 [-0.129]
 [-0.129]] [[76.274]
 [54.451]
 [58.409]
 [63.039]
 [71.024]
 [54.451]
 [54.451]] [[1.014]
 [0.653]
 [0.71 ]
 [0.75 ]
 [0.931]
 [0.653]
 [0.653]]
maxi score, test score, baseline:  -0.7550866666666667 0.4866666666666667 0.4866666666666667
probs:  [0.05694825720192864, 0.39594826487043067, 0.05694825720192864, 0.254411845827158, 0.05694825720192864, 0.17879511769662557]
actor:  1 policy actor:  1  step number:  43 total reward:  0.5866666666666667  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.7550866666666667 0.4866666666666667 0.4866666666666667
probs:  [0.05105957820014734, 0.35492342188264, 0.05105957820014734, 0.22805675716680662, 0.15462322637068446, 0.1602774381795742]
actor:  0 policy actor:  1  step number:  46 total reward:  0.3399999999999995  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.7524066666666667 0.4866666666666667 0.4866666666666667
maxi score, test score, baseline:  -0.7524066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.05105957820014734, 0.35492342188264, 0.05105957820014734, 0.22805675716680662, 0.15462322637068446, 0.1602774381795742]
maxi score, test score, baseline:  -0.7524066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.05105957820014734, 0.35492342188264, 0.05105957820014734, 0.22805675716680662, 0.15462322637068446, 0.1602774381795742]
printing an ep nov before normalisation:  24.15123134933874
Printing some Q and Qe and total Qs values:  [[-0.067]
 [-0.081]
 [-0.06 ]
 [-0.06 ]
 [-0.06 ]
 [-0.06 ]
 [-0.06 ]] [[40.422]
 [35.263]
 [24.503]
 [24.503]
 [24.503]
 [24.503]
 [24.503]] [[0.973]
 [0.826]
 [0.569]
 [0.569]
 [0.569]
 [0.569]
 [0.569]]
Printing some Q and Qe and total Qs values:  [[-0.062]
 [-0.149]
 [-0.062]
 [-0.062]
 [-0.062]
 [-0.093]
 [-0.062]] [[21.17 ]
 [30.958]
 [21.17 ]
 [21.17 ]
 [21.17 ]
 [19.42 ]
 [21.17 ]] [[0.848]
 [1.184]
 [0.848]
 [0.848]
 [0.848]
 [0.742]
 [0.848]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.5  reward:  1.0 rdn_beta:  1.333
from probs:  [0.05105957820014734, 0.35492342188264, 0.05105957820014734, 0.22805675716680662, 0.15462322637068446, 0.1602774381795742]
actions average: 
K:  2  action  0 :  tensor([0.5981, 0.0221, 0.0668, 0.0611, 0.1317, 0.0564, 0.0638],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0252, 0.8885, 0.0221, 0.0173, 0.0086, 0.0145, 0.0238],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2040, 0.0305, 0.4037, 0.0777, 0.0814, 0.1628, 0.0399],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1836, 0.0312, 0.0755, 0.3037, 0.2019, 0.1502, 0.0540],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2049, 0.0592, 0.0855, 0.0893, 0.4318, 0.0638, 0.0656],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0729, 0.1348, 0.1409, 0.0757, 0.0716, 0.4261, 0.0780],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1717, 0.0859, 0.1761, 0.1188, 0.1037, 0.1261, 0.2178],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.7524066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.050986490290249736, 0.35441423806960687, 0.050986490290249736, 0.2291636699978068, 0.15440150714574255, 0.16004760420634428]
printing an ep nov before normalisation:  59.99024376320101
printing an ep nov before normalisation:  60.888062242047766
maxi score, test score, baseline:  -0.7524066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.050931640041184216, 0.35402674368548803, 0.050931640041184216, 0.22941064227724858, 0.15452183635525965, 0.1601774975996353]
actor:  1 policy actor:  1  step number:  65 total reward:  0.1733333333333329  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.7524066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.05094331069628102, 0.3541080487678654, 0.05094331069628102, 0.22946331749808363, 0.1545573062862454, 0.15998470605524354]
Printing some Q and Qe and total Qs values:  [[-0.029]
 [-0.029]
 [-0.029]
 [-0.022]
 [-0.029]
 [-0.029]
 [-0.029]] [[47.514]
 [47.514]
 [47.514]
 [67.96 ]
 [47.514]
 [47.514]
 [47.514]] [[0.75]
 [0.75]
 [0.75]
 [1.41]
 [0.75]
 [0.75]
 [0.75]]
Printing some Q and Qe and total Qs values:  [[-0.13 ]
 [-0.121]
 [-0.121]
 [-0.121]
 [-0.121]
 [-0.121]
 [-0.121]] [[89.815]
 [74.287]
 [74.287]
 [74.287]
 [74.287]
 [74.287]
 [74.287]] [[0.179]
 [0.121]
 [0.121]
 [0.121]
 [0.121]
 [0.121]
 [0.121]]
maxi score, test score, baseline:  -0.7524066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.05094331069628102, 0.3541080487678654, 0.05094331069628102, 0.22946331749808363, 0.1545573062862454, 0.15998470605524354]
maxi score, test score, baseline:  -0.7524066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.05094331069628102, 0.3541080487678654, 0.05094331069628102, 0.22946331749808363, 0.1545573062862454, 0.15998470605524354]
maxi score, test score, baseline:  -0.7524066666666667 0.4866666666666667 0.4866666666666667
Printing some Q and Qe and total Qs values:  [[0.21 ]
 [0.314]
 [0.21 ]
 [0.21 ]
 [0.21 ]
 [0.21 ]
 [0.21 ]] [[34.865]
 [47.856]
 [34.865]
 [34.865]
 [34.865]
 [34.865]
 [34.865]] [[0.832]
 [1.369]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]]
actor:  1 policy actor:  1  step number:  38 total reward:  0.5666666666666667  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.112]
 [-0.099]
 [-0.112]
 [-0.112]
 [-0.112]
 [-0.112]
 [-0.112]] [[43.79 ]
 [36.893]
 [38.678]
 [38.678]
 [38.678]
 [38.678]
 [38.678]] [[0.555]
 [0.404]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.202216123011645
printing an ep nov before normalisation:  32.916226387023926
maxi score, test score, baseline:  -0.7524066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.048338640384370496, 0.33596228638182923, 0.048338640384370496, 0.2177072007467615, 0.14664108603658618, 0.203012146066082]
actor:  1 policy actor:  1  step number:  36 total reward:  0.54  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.7524066666666667 0.4866666666666667 0.4866666666666667
maxi score, test score, baseline:  -0.7524066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.04687176682272478, 0.3257283133711992, 0.04687176682272478, 0.21244997380530606, 0.16999439470759398, 0.19808378447045116]
maxi score, test score, baseline:  -0.7524066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.0467743564898268, 0.32503986814945013, 0.0467743564898268, 0.21291447920705128, 0.17031482041217888, 0.19818211925166612]
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.058]
 [0.026]
 [0.026]
 [0.026]
 [0.026]
 [0.026]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.026]
 [0.058]
 [0.026]
 [0.026]
 [0.026]
 [0.026]
 [0.026]]
maxi score, test score, baseline:  -0.7524066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.0467743564898268, 0.32503986814945013, 0.0467743564898268, 0.21291447920705128, 0.17031482041217888, 0.19818211925166612]
maxi score, test score, baseline:  -0.7524066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.0467743564898268, 0.32503986814945013, 0.0467743564898268, 0.21291447920705128, 0.17031482041217888, 0.19818211925166612]
printing an ep nov before normalisation:  33.2540959960507
maxi score, test score, baseline:  -0.7524066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.0467743564898268, 0.32503986814945013, 0.0467743564898268, 0.21291447920705128, 0.17031482041217888, 0.19818211925166612]
printing an ep nov before normalisation:  44.05998012329036
maxi score, test score, baseline:  -0.7524066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.0467743564898268, 0.32503986814945013, 0.0467743564898268, 0.21291447920705128, 0.17031482041217888, 0.19818211925166612]
printing an ep nov before normalisation:  91.8503816205507
printing an ep nov before normalisation:  52.032437636752896
printing an ep nov before normalisation:  67.58479682484047
maxi score, test score, baseline:  -0.7524066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.0467743564898268, 0.32503986814945013, 0.0467743564898268, 0.21291447920705128, 0.17031482041217888, 0.19818211925166612]
maxi score, test score, baseline:  -0.7524066666666667 0.4866666666666667 0.4866666666666667
actor:  1 policy actor:  1  step number:  49 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7524066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.04671845507166775, 0.3246455322080128, 0.04671845507166775, 0.2131123871770096, 0.1704476496211738, 0.19835752085046823]
actor:  1 policy actor:  1  step number:  48 total reward:  0.47333333333333305  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  0  action  0 :  tensor([0.7322, 0.0022, 0.0644, 0.0503, 0.0508, 0.0497, 0.0504],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0096, 0.9539, 0.0050, 0.0063, 0.0012, 0.0014, 0.0227],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.3011, 0.0044, 0.4156, 0.0702, 0.0683, 0.0873, 0.0531],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2330, 0.0090, 0.1126, 0.3073, 0.1079, 0.0977, 0.1325],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1979, 0.0007, 0.0952, 0.0831, 0.4356, 0.1083, 0.0792],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1617, 0.0072, 0.1467, 0.0988, 0.1150, 0.3929, 0.0777],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1614, 0.0893, 0.1125, 0.1172, 0.1094, 0.1204, 0.2898],
       grad_fn=<DivBackward0>)
from probs:  [0.04671845507166775, 0.3246455322080128, 0.04671845507166775, 0.2131123871770096, 0.1704476496211738, 0.19835752085046823]
maxi score, test score, baseline:  -0.7524066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.046677458398099274, 0.32435503300137597, 0.046677458398099274, 0.21337747877923843, 0.17063425773671195, 0.19827831368647522]
maxi score, test score, baseline:  -0.7524066666666667 0.4866666666666667 0.4866666666666667
using another actor
from probs:  [0.046692192881421374, 0.32445767173077134, 0.046692192881421374, 0.21344498540173282, 0.17037191673056207, 0.1983410403740911]
Printing some Q and Qe and total Qs values:  [[-0.108]
 [-0.091]
 [-0.113]
 [-0.113]
 [-0.113]
 [-0.113]
 [-0.113]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.108]
 [-0.091]
 [-0.113]
 [-0.113]
 [-0.113]
 [-0.113]
 [-0.113]]
maxi score, test score, baseline:  -0.7524066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.046692192881421374, 0.32445767173077134, 0.046692192881421374, 0.21344498540173282, 0.17037191673056207, 0.1983410403740911]
maxi score, test score, baseline:  -0.7524066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.046692192881421374, 0.32445767173077134, 0.046692192881421374, 0.21344498540173282, 0.17037191673056207, 0.1983410403740911]
maxi score, test score, baseline:  -0.7524066666666667 0.4866666666666667 0.4866666666666667
Printing some Q and Qe and total Qs values:  [[ 0.049]
 [ 0.297]
 [-0.02 ]
 [ 0.06 ]
 [ 0.06 ]
 [ 0.06 ]
 [ 0.06 ]] [[51.105]
 [44.818]
 [37.064]
 [37.266]
 [37.266]
 [37.266]
 [37.266]] [[1.255]
 [1.354]
 [0.854]
 [0.938]
 [0.938]
 [0.938]
 [0.938]]
actor:  1 policy actor:  1  step number:  42 total reward:  0.48666666666666647  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7524066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.046636572011844415, 0.3240653108673016, 0.046636572011844415, 0.2136422635580373, 0.17050386982127683, 0.19851541172969545]
printing an ep nov before normalisation:  65.542150963341
printing an ep nov before normalisation:  68.86011313920591
printing an ep nov before normalisation:  60.4062557220459
maxi score, test score, baseline:  -0.7524066666666667 0.4866666666666667 0.4866666666666667
Starting evaluation
printing an ep nov before normalisation:  39.65706900044014
maxi score, test score, baseline:  -0.7524066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.046636572011844415, 0.3240653108673016, 0.046636572011844415, 0.2136422635580373, 0.17050386982127683, 0.19851541172969545]
printing an ep nov before normalisation:  39.123593653746596
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.755]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]] [[57.415]
 [53.822]
 [57.415]
 [57.415]
 [57.415]
 [57.415]
 [57.415]] [[0.675]
 [0.755]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]]
printing an ep nov before normalisation:  77.00690403233965
printing an ep nov before normalisation:  37.68039518866564
printing an ep nov before normalisation:  53.58209639214197
Printing some Q and Qe and total Qs values:  [[0.719]
 [0.645]
 [0.554]
 [0.571]
 [0.668]
 [0.535]
 [0.577]] [[48.504]
 [35.812]
 [32.148]
 [38.31 ]
 [45.532]
 [37.243]
 [31.648]] [[0.719]
 [0.645]
 [0.554]
 [0.571]
 [0.668]
 [0.535]
 [0.577]]
Printing some Q and Qe and total Qs values:  [[0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]] [[63.718]
 [63.718]
 [63.718]
 [63.718]
 [63.718]
 [63.718]
 [63.718]] [[1.97]
 [1.97]
 [1.97]
 [1.97]
 [1.97]
 [1.97]
 [1.97]]
Printing some Q and Qe and total Qs values:  [[0.54 ]
 [0.777]
 [0.634]
 [0.601]
 [0.634]
 [0.634]
 [0.634]] [[58.277]
 [45.36 ]
 [32.621]
 [45.804]
 [32.621]
 [32.621]
 [32.621]] [[0.54 ]
 [0.777]
 [0.634]
 [0.601]
 [0.634]
 [0.634]
 [0.634]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.815]
 [0.869]
 [0.851]
 [0.822]
 [0.818]
 [0.848]
 [0.835]] [[37.705]
 [37.614]
 [38.902]
 [38.144]
 [38.096]
 [36.411]
 [39.476]] [[0.815]
 [0.869]
 [0.851]
 [0.822]
 [0.818]
 [0.848]
 [0.835]]
Printing some Q and Qe and total Qs values:  [[0.895]
 [0.905]
 [0.875]
 [0.859]
 [0.865]
 [0.865]
 [0.883]] [[37.08 ]
 [39.945]
 [37.269]
 [39.083]
 [39.425]
 [38.549]
 [38.758]] [[0.895]
 [0.905]
 [0.875]
 [0.859]
 [0.865]
 [0.865]
 [0.883]]
maxi score, test score, baseline:  -0.7524066666666667 0.4866666666666667 0.4866666666666667
probs:  [0.046636572011844415, 0.3240653108673016, 0.046636572011844415, 0.2136422635580373, 0.17050386982127683, 0.19851541172969545]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.043]
 [0.013]
 [0.012]
 [0.001]
 [0.001]
 [0.001]] [[27.14 ]
 [34.645]
 [25.405]
 [24.863]
 [20.192]
 [22.256]
 [21.291]] [[0.003]
 [0.043]
 [0.013]
 [0.012]
 [0.001]
 [0.001]
 [0.001]]
Printing some Q and Qe and total Qs values:  [[0.274]
 [0.379]
 [0.267]
 [0.267]
 [0.267]
 [0.271]
 [0.284]] [[27.509]
 [45.982]
 [26.842]
 [27.683]
 [27.859]
 [28.507]
 [28.133]] [[0.718]
 [1.637]
 [0.682]
 [0.718]
 [0.726]
 [0.759]
 [0.756]]
printing an ep nov before normalisation:  54.91901174292173
Printing some Q and Qe and total Qs values:  [[0.791]
 [0.839]
 [0.782]
 [0.796]
 [0.794]
 [0.796]
 [0.805]] [[43.849]
 [43.933]
 [38.266]
 [41.236]
 [35.882]
 [37.446]
 [35.636]] [[0.791]
 [0.839]
 [0.782]
 [0.796]
 [0.794]
 [0.796]
 [0.805]]
UNIT TEST: sample policy line 217 mcts : [0.122 0.286 0.061 0.245 0.102 0.041 0.143]
printing an ep nov before normalisation:  47.876751299279796
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.336]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]] [[27.823]
 [34.476]
 [27.823]
 [27.823]
 [27.823]
 [27.823]
 [27.823]] [[1.379]
 [1.754]
 [1.379]
 [1.379]
 [1.379]
 [1.379]
 [1.379]]
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  28 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  28 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  28 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  28 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  43.01202304233605
actor:  0 policy actor:  0  step number:  29 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  29 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  0.0
actor:  0 policy actor:  0  step number:  31 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  31 total reward:  0.6133333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  32 total reward:  0.6333333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  32 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.6960333333333335 0.4866666666666667 0.4866666666666667
probs:  [0.046636572011844415, 0.3240653108673016, 0.046636572011844415, 0.2136422635580373, 0.17050386982127683, 0.19851541172969545]
printing an ep nov before normalisation:  25.069867732936864
actor:  1 policy actor:  1  step number:  53 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  34 total reward:  0.5666666666666667  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.84 ]
 [0.695]
 [0.678]
 [0.499]
 [0.685]
 [0.664]] [[65.429]
 [56.142]
 [42.176]
 [50.249]
 [54.615]
 [48.129]
 [47.776]] [[0.469]
 [0.84 ]
 [0.695]
 [0.678]
 [0.499]
 [0.685]
 [0.664]]
printing an ep nov before normalisation:  67.79850996508806
actor:  1 policy actor:  1  step number:  54 total reward:  0.39333333333333276  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  40 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  54.52180905653814
printing an ep nov before normalisation:  50.70850525242178
actor:  0 policy actor:  0  step number:  53 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.6869933333333333 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.18312433958549
printing an ep nov before normalisation:  42.74850281506348
maxi score, test score, baseline:  -0.6869933333333333 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.945886398061294
line 256 mcts: sample exp_bonus 44.598526758534675
Printing some Q and Qe and total Qs values:  [[0.59 ]
 [0.701]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]] [[32.75 ]
 [38.691]
 [32.75 ]
 [32.75 ]
 [32.75 ]
 [32.75 ]
 [32.75 ]] [[1.363]
 [1.726]
 [1.363]
 [1.363]
 [1.363]
 [1.363]
 [1.363]]
Printing some Q and Qe and total Qs values:  [[-0.09 ]
 [-0.098]
 [-0.087]
 [-0.095]
 [-0.09 ]
 [-0.088]
 [-0.076]] [[19.247]
 [19.97 ]
 [13.597]
 [21.316]
 [19.668]
 [16.229]
 [16.597]] [[0.091]
 [0.094]
 [0.012]
 [0.117]
 [0.098]
 [0.049]
 [0.067]]
maxi score, test score, baseline:  -0.6869933333333333 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.297758933997855
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6869933333333333 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6869933333333333 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.353723808876545
printing an ep nov before normalisation:  67.33040668297109
maxi score, test score, baseline:  -0.6869933333333333 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6869933333333333 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.141]
 [-0.129]
 [-0.129]
 [-0.129]
 [-0.129]
 [-0.129]
 [-0.129]] [[43.054]
 [30.559]
 [30.559]
 [30.559]
 [30.559]
 [30.559]
 [30.559]] [[0.622]
 [0.319]
 [0.319]
 [0.319]
 [0.319]
 [0.319]
 [0.319]]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.079]
 [-0.057]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]] [[30.986]
 [50.602]
 [30.986]
 [30.986]
 [30.986]
 [30.986]
 [30.986]] [[0.514]
 [1.271]
 [0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]]
Printing some Q and Qe and total Qs values:  [[-0.112]
 [-0.111]
 [-0.111]
 [-0.111]
 [-0.111]
 [-0.111]
 [-0.111]] [[42.976]
 [32.748]
 [32.748]
 [32.748]
 [32.748]
 [32.748]
 [32.748]] [[0.985]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]]
Printing some Q and Qe and total Qs values:  [[-0.111]
 [-0.111]
 [-0.111]
 [-0.111]
 [-0.111]
 [-0.111]
 [-0.111]] [[49.635]
 [44.826]
 [44.826]
 [44.826]
 [44.826]
 [44.826]
 [44.826]] [[1.142]
 [0.963]
 [0.963]
 [0.963]
 [0.963]
 [0.963]
 [0.963]]
maxi score, test score, baseline:  -0.6869933333333333 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.23987317063474
maxi score, test score, baseline:  -0.6869933333333333 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6869933333333333 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6869933333333333 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6869933333333333 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.565]
 [0.429]
 [0.429]
 [0.429]
 [0.423]
 [0.429]] [[26.452]
 [35.254]
 [26.452]
 [26.452]
 [26.452]
 [37.03 ]
 [26.452]] [[0.429]
 [0.565]
 [0.429]
 [0.429]
 [0.429]
 [0.423]
 [0.429]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.6869933333333333 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  41 total reward:  0.5466666666666669  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  72.9632658718673
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.6869933333333333 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.54 ]
 [0.587]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]] [[23.268]
 [37.092]
 [23.268]
 [23.268]
 [23.268]
 [23.268]
 [23.268]] [[0.54 ]
 [0.587]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]]
printing an ep nov before normalisation:  32.79678683740091
printing an ep nov before normalisation:  45.3389817489272
maxi score, test score, baseline:  -0.6869933333333333 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.848532784110983
printing an ep nov before normalisation:  20.69631688297935
actor:  0 policy actor:  0  step number:  54 total reward:  0.28666666666666607  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.228]] [[63.669]
 [63.669]
 [63.669]
 [63.669]
 [63.669]
 [63.669]
 [63.669]] [[2.228]
 [2.228]
 [2.228]
 [2.228]
 [2.228]
 [2.228]
 [2.228]]
maxi score, test score, baseline:  -0.68442 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.4266666666666663  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  4  action  0 :  tensor([0.6717, 0.0169, 0.0667, 0.0595, 0.0703, 0.0665, 0.0484],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0520, 0.8862, 0.0108, 0.0143, 0.0065, 0.0085, 0.0218],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1203, 0.0779, 0.4507, 0.0821, 0.0644, 0.1206, 0.0839],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2640, 0.0101, 0.1169, 0.2304, 0.1267, 0.1584, 0.0935],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.3181, 0.0435, 0.0858, 0.0885, 0.3270, 0.0807, 0.0565],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2250, 0.0159, 0.1179, 0.0858, 0.1494, 0.3471, 0.0589],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.3560, 0.3114, 0.0555, 0.0715, 0.0826, 0.0654, 0.0576],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.68442 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.08 ]
 [-0.069]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.074]] [[46.987]
 [49.215]
 [43.716]
 [43.716]
 [43.716]
 [43.716]
 [43.716]] [[1.363]
 [1.464]
 [1.237]
 [1.237]
 [1.237]
 [1.237]
 [1.237]]
printing an ep nov before normalisation:  39.15913222600126
maxi score, test score, baseline:  -0.68442 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.879038978489405
printing an ep nov before normalisation:  57.034302600373124
printing an ep nov before normalisation:  42.37116208732485
actions average: 
K:  2  action  0 :  tensor([0.5647, 0.0686, 0.0769, 0.0679, 0.0849, 0.0674, 0.0696],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0221, 0.8902, 0.0265, 0.0110, 0.0026, 0.0077, 0.0399],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2858, 0.0322, 0.2139, 0.1139, 0.0979, 0.1506, 0.1059],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1637, 0.1013, 0.0907, 0.4036, 0.0531, 0.0798, 0.1078],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2605, 0.0208, 0.0907, 0.0987, 0.3469, 0.1121, 0.0703],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1723, 0.0577, 0.0933, 0.0911, 0.0813, 0.4451, 0.0593],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1180, 0.1131, 0.1382, 0.1028, 0.0977, 0.1310, 0.2992],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.576]
 [0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]] [[38.625]
 [45.89 ]
 [24.583]
 [24.583]
 [24.583]
 [24.583]
 [24.583]] [[1.596]
 [1.962]
 [1.169]
 [1.169]
 [1.169]
 [1.169]
 [1.169]]
maxi score, test score, baseline:  -0.68442 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.68442 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.1866666666666661  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.68442 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.693846407584466
maxi score, test score, baseline:  -0.68442 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.265772776048706
actor:  1 policy actor:  1  step number:  56 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  1.333
siam score:  -0.8557491
maxi score, test score, baseline:  -0.68442 0.6353333333333333 0.6353333333333333
actions average: 
K:  1  action  0 :  tensor([0.6046, 0.0226, 0.0892, 0.0755, 0.0753, 0.0752, 0.0577],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0093, 0.9523, 0.0036, 0.0212, 0.0011, 0.0029, 0.0095],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2394, 0.0224, 0.3134, 0.0820, 0.0778, 0.1584, 0.1066],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2759, 0.0051, 0.1322, 0.2394, 0.1327, 0.1157, 0.0991],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2075, 0.0052, 0.1029, 0.1097, 0.3622, 0.1312, 0.0814],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2440, 0.0030, 0.1276, 0.1364, 0.1136, 0.2747, 0.1007],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.3056, 0.1135, 0.1169, 0.1266, 0.1277, 0.1045, 0.1052],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.68442 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.68442 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.871105017044144
maxi score, test score, baseline:  -0.68442 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.68442 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  0.56  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.634578704833984
maxi score, test score, baseline:  -0.68442 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  41 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6815133333333333 0.6353333333333333 0.6353333333333333
printing an ep nov before normalisation:  44.746178974614025
maxi score, test score, baseline:  -0.6815133333333333 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  1.333
siam score:  -0.8582571
maxi score, test score, baseline:  -0.6815133333333333 0.6353333333333333 0.6353333333333333
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.857032
using explorer policy with actor:  1
siam score:  -0.8583937
siam score:  -0.85766965
maxi score, test score, baseline:  -0.6815133333333333 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6815133333333333 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.861105534690154
printing an ep nov before normalisation:  65.17305562449371
actor:  0 policy actor:  0  step number:  61 total reward:  0.3599999999999992  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6787933333333335 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.047]
 [-0.047]
 [-0.047]
 [-0.069]
 [-0.047]
 [-0.047]
 [-0.047]] [[67.769]
 [67.769]
 [67.769]
 [68.613]
 [67.769]
 [67.769]
 [67.769]] [[1.679]
 [1.679]
 [1.679]
 [1.685]
 [1.679]
 [1.679]
 [1.679]]
maxi score, test score, baseline:  -0.6787933333333335 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.02  0.061 0.041 0.755 0.061 0.02  0.041]
printing an ep nov before normalisation:  35.62171679823582
maxi score, test score, baseline:  -0.6787933333333335 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  0.03333333333333233  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.683]
 [0.735]
 [0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.684]] [[23.81 ]
 [31.425]
 [22.642]
 [22.642]
 [22.642]
 [22.642]
 [22.642]] [[0.683]
 [0.735]
 [0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.684]]
printing an ep nov before normalisation:  33.330869946577415
printing an ep nov before normalisation:  0.015241221424844298
actor:  0 policy actor:  0  step number:  70 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  39 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6763266666666667 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6763266666666667 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6763266666666667 0.6353333333333333 0.6353333333333333
maxi score, test score, baseline:  -0.6763266666666667 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.169]
 [0.317]
 [0.142]
 [0.145]
 [0.142]
 [0.142]
 [0.142]] [[33.89 ]
 [44.593]
 [32.987]
 [36.814]
 [32.987]
 [32.987]
 [32.987]] [[0.4  ]
 [0.688]
 [0.362]
 [0.414]
 [0.362]
 [0.362]
 [0.362]]
siam score:  -0.8704572
printing an ep nov before normalisation:  45.5956058342678
actor:  1 policy actor:  1  step number:  39 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6763266666666667 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  0.06396444775873533
actor:  1 policy actor:  1  step number:  43 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  30.22374558498668
maxi score, test score, baseline:  -0.6763266666666667 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.02549841249514
maxi score, test score, baseline:  -0.6763266666666667 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  42 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8647916
maxi score, test score, baseline:  -0.6763266666666667 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6763266666666667 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6763266666666667 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  57 total reward:  0.25333333333333297  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  23.83355757327623
maxi score, test score, baseline:  -0.6763266666666667 0.6353333333333333 0.6353333333333333
printing an ep nov before normalisation:  70.88986306315664
actor:  1 policy actor:  1  step number:  50 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  54.38393955743719
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.891746807367205
maxi score, test score, baseline:  -0.6763266666666667 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.14103231873331
maxi score, test score, baseline:  -0.6763266666666667 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6763266666666667 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.075258338498756
printing an ep nov before normalisation:  44.56506292014952
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]] [[11.812]
 [10.828]
 [10.828]
 [10.828]
 [10.828]
 [10.828]
 [10.828]] [[0.459]
 [0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]]
printing an ep nov before normalisation:  18.628578682706767
Printing some Q and Qe and total Qs values:  [[0.981]
 [0.981]
 [0.981]
 [0.981]
 [0.981]
 [0.981]
 [0.981]] [[35.764]
 [35.764]
 [35.764]
 [35.764]
 [35.764]
 [35.764]
 [35.764]] [[0.981]
 [0.981]
 [0.981]
 [0.981]
 [0.981]
 [0.981]
 [0.981]]
printing an ep nov before normalisation:  45.83766680649711
using explorer policy with actor:  1
siam score:  -0.8618485
printing an ep nov before normalisation:  47.174347655218625
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.8530431772129
maxi score, test score, baseline:  -0.6763266666666667 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.66 ]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]] [[43.207]
 [52.132]
 [35.213]
 [35.213]
 [35.213]
 [35.213]
 [35.213]] [[1.765]
 [2.099]
 [1.62 ]
 [1.62 ]
 [1.62 ]
 [1.62 ]
 [1.62 ]]
printing an ep nov before normalisation:  43.825243477169806
printing an ep nov before normalisation:  42.742819772061935
maxi score, test score, baseline:  -0.6763266666666667 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  15.141468048095703
maxi score, test score, baseline:  -0.6763266666666667 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.36666666666666603  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6763266666666667 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6763266666666667 0.6353333333333333 0.6353333333333333
actor:  1 policy actor:  1  step number:  53 total reward:  0.5466666666666671  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6763266666666667 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6763266666666667 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6763266666666667 0.6353333333333333 0.6353333333333333
printing an ep nov before normalisation:  59.83175277709961
printing an ep nov before normalisation:  44.819041337507834
maxi score, test score, baseline:  -0.6763266666666667 0.6353333333333333 0.6353333333333333
maxi score, test score, baseline:  -0.6763266666666667 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.42286104727776
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6763266666666667 0.6353333333333333 0.6353333333333333
maxi score, test score, baseline:  -0.6763266666666667 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.057674421854465
printing an ep nov before normalisation:  55.98223829148733
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.6763266666666667 0.6353333333333333 0.6353333333333333
actor:  1 policy actor:  1  step number:  67 total reward:  0.31999999999999984  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6763266666666667 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6763266666666667 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.61865730427192
printing an ep nov before normalisation:  43.89575479553565
Printing some Q and Qe and total Qs values:  [[-0.085]
 [-0.088]
 [-0.085]
 [-0.085]
 [-0.085]
 [-0.085]
 [-0.085]] [[40.459]
 [63.998]
 [40.459]
 [40.459]
 [40.459]
 [40.459]
 [40.459]] [[0.23 ]
 [0.474]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]]
printing an ep nov before normalisation:  0.0
actor:  0 policy actor:  0  step number:  63 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  71.68644267601306
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.12362813949585
actor:  1 policy actor:  1  step number:  48 total reward:  0.4066666666666663  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
siam score:  -0.8598364
printing an ep nov before normalisation:  57.13177636409199
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
printing an ep nov before normalisation:  66.18969101998964
actor:  1 policy actor:  1  step number:  46 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  65.99985020836667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.064]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]] [[67.068]
 [43.668]
 [43.668]
 [43.668]
 [43.668]
 [43.668]
 [43.668]] [[1.073]
 [0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]]
line 256 mcts: sample exp_bonus 57.607952255417985
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.45999999999999963  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.777]
 [0.849]
 [0.721]
 [0.753]
 [0.83 ]
 [0.758]
 [0.838]] [[40.482]
 [42.483]
 [41.092]
 [43.377]
 [42.963]
 [42.18 ]
 [38.889]] [[0.777]
 [0.849]
 [0.721]
 [0.753]
 [0.83 ]
 [0.758]
 [0.838]]
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  14.779961109161377
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8570358
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.17948271041847
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.6628, 0.0213, 0.0620, 0.0737, 0.0678, 0.0514, 0.0611],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0077, 0.9576, 0.0063, 0.0092, 0.0030, 0.0034, 0.0127],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1730, 0.0879, 0.4456, 0.0627, 0.0459, 0.0867, 0.0983],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2208, 0.2194, 0.1110, 0.1817, 0.0688, 0.0758, 0.1226],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.4668, 0.0017, 0.0528, 0.0555, 0.3322, 0.0467, 0.0443],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1985, 0.0098, 0.1743, 0.0907, 0.0865, 0.3457, 0.0945],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.3449, 0.0950, 0.1220, 0.1148, 0.0953, 0.0956, 0.1324],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  55 total reward:  0.3599999999999992  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  37.441714671681915
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.903056144714355
actor:  1 policy actor:  1  step number:  59 total reward:  0.13333333333333264  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  39.353560754729386
printing an ep nov before normalisation:  39.3414574401632
actor:  1 policy actor:  1  step number:  39 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  4  action  0 :  tensor([0.5431, 0.0566, 0.0680, 0.1079, 0.0883, 0.0703, 0.0656],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0168, 0.8766, 0.0110, 0.0294, 0.0070, 0.0360, 0.0233],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2364, 0.0307, 0.2729, 0.1128, 0.0991, 0.1285, 0.1197],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2054, 0.1000, 0.1336, 0.1745, 0.1270, 0.1287, 0.1308],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2870, 0.0844, 0.0621, 0.0975, 0.3273, 0.0845, 0.0573],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1746, 0.0736, 0.1285, 0.1409, 0.1121, 0.2328, 0.1375],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.3044, 0.0207, 0.0896, 0.1662, 0.1219, 0.1101, 0.1871],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.60838681298739
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.28322370506558
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.52172776480838
Printing some Q and Qe and total Qs values:  [[-0.003]
 [ 0.041]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[51.979]
 [50.827]
 [50.809]
 [50.809]
 [50.809]
 [50.809]
 [50.809]] [[1.093]
 [1.099]
 [1.054]
 [1.054]
 [1.054]
 [1.054]
 [1.054]]
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.499]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]] [[25.942]
 [30.924]
 [25.942]
 [25.942]
 [25.942]
 [25.942]
 [25.942]] [[0.9  ]
 [0.999]
 [0.9  ]
 [0.9  ]
 [0.9  ]
 [0.9  ]
 [0.9  ]]
printing an ep nov before normalisation:  30.577397346496582
printing an ep nov before normalisation:  41.360986660712264
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.712287135827793
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.25999999999999945  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  54 total reward:  0.13999999999999946  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.659274986565286
printing an ep nov before normalisation:  40.72744332471707
printing an ep nov before normalisation:  82.41046812342344
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.975093364715576
printing an ep nov before normalisation:  34.28164964140225
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.01383938134722
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  3.112802005489357e-05
Printing some Q and Qe and total Qs values:  [[-0.062]
 [-0.051]
 [-0.039]
 [-0.051]
 [-0.051]
 [-0.051]
 [-0.051]] [[57.628]
 [39.097]
 [36.592]
 [39.097]
 [39.097]
 [39.097]
 [39.097]] [[1.562]
 [0.817]
 [0.727]
 [0.817]
 [0.817]
 [0.817]
 [0.817]]
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.85407536896655
actor:  1 policy actor:  1  step number:  51 total reward:  0.2799999999999998  reward:  1.0 rdn_beta:  1.0
siam score:  -0.85690534
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8589157
printing an ep nov before normalisation:  19.651201094993453
printing an ep nov before normalisation:  50.86435815681114
actor:  1 policy actor:  1  step number:  38 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.87418129343857
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.92337777795337
UNIT TEST: sample policy line 217 mcts : [0.041 0.837 0.02  0.02  0.082 0.    0.   ]
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.156]
 [ 0.047]
 [-0.003]
 [ 0.156]
 [-0.04 ]
 [ 0.156]
 [ 0.156]] [[52.09 ]
 [41.561]
 [42.839]
 [52.09 ]
 [45.194]
 [52.09 ]
 [52.09 ]] [[1.007]
 [0.617]
 [0.601]
 [1.007]
 [0.627]
 [1.007]
 [1.007]]
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.312763823249504
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.514]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]] [[22.863]
 [49.855]
 [22.863]
 [22.863]
 [22.863]
 [22.863]
 [22.863]] [[0.806]
 [1.338]
 [0.806]
 [0.806]
 [0.806]
 [0.806]
 [0.806]]
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.15999999999999936  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  58 total reward:  0.24666666666666626  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.1599999999999996  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  27.126474380493164
printing an ep nov before normalisation:  54.57664937656371
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.623014993674815
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.41333333333333333  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  46 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.086]
 [-0.077]
 [-0.083]
 [-0.084]
 [-0.084]
 [-0.083]
 [-0.083]] [[37.244]
 [37.112]
 [34.386]
 [35.513]
 [35.591]
 [34.386]
 [34.386]] [[0.689]
 [0.693]
 [0.578]
 [0.621]
 [0.625]
 [0.578]
 [0.578]]
line 256 mcts: sample exp_bonus 30.51373304051787
siam score:  -0.8534282
printing an ep nov before normalisation:  33.37444013891081
actor:  1 policy actor:  1  step number:  44 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  48 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  55 total reward:  0.4933333333333335  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.32731192455498
actor:  1 policy actor:  1  step number:  46 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
printing an ep nov before normalisation:  21.211395263671875
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  0.35333333333333317  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  76.56686910671259
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 43.76017453005469
printing an ep nov before normalisation:  54.51751707936497
Printing some Q and Qe and total Qs values:  [[0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.574]
 [0.601]] [[25.599]
 [25.599]
 [25.599]
 [25.599]
 [25.599]
 [33.626]
 [25.599]] [[1.669]
 [1.669]
 [1.669]
 [1.669]
 [1.669]
 [2.241]
 [1.669]]
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  39 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  33.07825717933494
actor:  1 policy actor:  1  step number:  68 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6739 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  50 total reward:  0.33999999999999986  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.67122 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8629398
actor:  1 policy actor:  1  step number:  40 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.67122 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.66134282647879
printing an ep nov before normalisation:  28.48405122756958
maxi score, test score, baseline:  -0.67122 0.6353333333333333 0.6353333333333333
maxi score, test score, baseline:  -0.67122 0.6353333333333333 0.6353333333333333
actor:  1 policy actor:  1  step number:  50 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  63 total reward:  0.013333333333332753  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.67122 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.918]
 [0.916]
 [0.918]
 [0.918]
 [0.918]
 [0.918]
 [0.765]] [[35.094]
 [45.622]
 [35.094]
 [35.094]
 [35.094]
 [35.094]
 [34.779]] [[0.918]
 [0.916]
 [0.918]
 [0.918]
 [0.918]
 [0.918]
 [0.765]]
maxi score, test score, baseline:  -0.67122 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  54 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6687800000000002 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.80354690551758
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.6687800000000002 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6687800000000002 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  59.088670918424626
siam score:  -0.8594612
maxi score, test score, baseline:  -0.6687800000000002 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.33070634250837
maxi score, test score, baseline:  -0.6687800000000002 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.379]
 [0.563]
 [0.388]
 [0.381]
 [0.382]
 [0.389]
 [0.382]] [[28.828]
 [47.35 ]
 [27.811]
 [28.37 ]
 [28.09 ]
 [27.825]
 [27.31 ]] [[0.571]
 [0.958]
 [0.568]
 [0.567]
 [0.565]
 [0.569]
 [0.557]]
actions average: 
K:  1  action  0 :  tensor([0.6231, 0.0194, 0.0703, 0.0734, 0.0816, 0.0602, 0.0719],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0051, 0.9735, 0.0032, 0.0027, 0.0017, 0.0026, 0.0112],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2604, 0.0048, 0.2893, 0.1220, 0.1138, 0.1269, 0.0828],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2121, 0.0902, 0.0842, 0.2586, 0.1377, 0.1107, 0.1065],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2657, 0.0069, 0.0779, 0.1032, 0.3746, 0.1079, 0.0637],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.4012, 0.0333, 0.1183, 0.0967, 0.1050, 0.1360, 0.1095],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2882, 0.1411, 0.0808, 0.1242, 0.1159, 0.1352, 0.1146],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.6687800000000002 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.6687800000000002 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.45886668308869
siam score:  -0.8580873
actor:  0 policy actor:  0  step number:  55 total reward:  0.17333333333333278  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6664333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6664333333333334 0.6353333333333333 0.6353333333333333
siam score:  -0.85661393
Printing some Q and Qe and total Qs values:  [[-0.041]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]] [[31.397]
 [31.397]
 [31.397]
 [31.397]
 [31.397]
 [31.397]
 [31.397]] [[0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.053]]
printing an ep nov before normalisation:  35.04532232436209
siam score:  -0.85714555
actor:  1 policy actor:  1  step number:  47 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6664333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8589609
printing an ep nov before normalisation:  18.052324813335535
maxi score, test score, baseline:  -0.6664333333333334 0.6353333333333333 0.6353333333333333
actor:  1 policy actor:  1  step number:  50 total reward:  0.4733333333333334  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  66 total reward:  0.13999999999999924  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6664333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6664333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.42340740680975
printing an ep nov before normalisation:  59.94506109573343
maxi score, test score, baseline:  -0.6664333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6664333333333334 0.6353333333333333 0.6353333333333333
Printing some Q and Qe and total Qs values:  [[-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]] [[38.662]
 [38.662]
 [38.662]
 [38.662]
 [38.662]
 [38.662]
 [38.662]] [[0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]]
actor:  1 policy actor:  1  step number:  37 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  49.11846748227903
maxi score, test score, baseline:  -0.6664333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  60 total reward:  0.13999999999999935  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  65.81155760865694
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6664333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.056]
 [0.333]
 [0.124]
 [0.124]
 [0.124]
 [0.124]
 [0.124]] [[55.424]
 [56.849]
 [54.647]
 [54.647]
 [54.647]
 [54.647]
 [54.647]] [[1.721]
 [2.053]
 [1.76 ]
 [1.76 ]
 [1.76 ]
 [1.76 ]
 [1.76 ]]
maxi score, test score, baseline:  -0.6664333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6664333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.17999999999999938  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.344]
 [0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]] [[29.752]
 [36.001]
 [29.752]
 [29.752]
 [29.752]
 [29.752]
 [29.752]] [[1.03 ]
 [1.411]
 [1.03 ]
 [1.03 ]
 [1.03 ]
 [1.03 ]
 [1.03 ]]
UNIT TEST: sample policy line 217 mcts : [0.408 0.469 0.02  0.02  0.02  0.041 0.02 ]
maxi score, test score, baseline:  -0.6664333333333334 0.6353333333333333 0.6353333333333333
printing an ep nov before normalisation:  43.34095220018295
siam score:  -0.8676224
maxi score, test score, baseline:  -0.6664333333333334 0.6353333333333333 0.6353333333333333
maxi score, test score, baseline:  -0.6664333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.526]
 [0.57 ]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]] [[45.865]
 [56.036]
 [45.865]
 [45.865]
 [45.865]
 [45.865]
 [45.865]] [[0.714]
 [0.834]
 [0.714]
 [0.714]
 [0.714]
 [0.714]
 [0.714]]
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.44 ]
 [0.323]
 [0.323]
 [0.323]
 [0.323]
 [0.323]] [[44.212]
 [45.47 ]
 [42.813]
 [42.813]
 [42.813]
 [42.813]
 [42.813]] [[0.505]
 [0.623]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]]
maxi score, test score, baseline:  -0.6664333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.16666666666666619  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.054]
 [-0.054]
 [-0.054]
 [-0.054]
 [-0.054]
 [-0.054]
 [-0.054]] [[25.304]
 [25.304]
 [25.304]
 [25.304]
 [25.304]
 [25.304]
 [25.304]] [[0.04]
 [0.04]
 [0.04]
 [0.04]
 [0.04]
 [0.04]
 [0.04]]
maxi score, test score, baseline:  -0.6664333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.45245880485249
printing an ep nov before normalisation:  47.193644976465485
printing an ep nov before normalisation:  26.11515998840332
printing an ep nov before normalisation:  64.7551332643921
actor:  1 policy actor:  1  step number:  54 total reward:  0.36666666666666614  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.557]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]] [[25.634]
 [41.499]
 [25.634]
 [25.634]
 [25.634]
 [25.634]
 [25.634]] [[0.64]
 [0.92]
 [0.64]
 [0.64]
 [0.64]
 [0.64]
 [0.64]]
Printing some Q and Qe and total Qs values:  [[-0.038]
 [-0.006]
 [-0.038]
 [-0.038]
 [-0.038]
 [-0.038]
 [-0.038]] [[37.784]
 [48.691]
 [37.784]
 [37.784]
 [37.784]
 [37.784]
 [37.784]] [[1.027]
 [1.577]
 [1.027]
 [1.027]
 [1.027]
 [1.027]
 [1.027]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6664333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.56807470321655
maxi score, test score, baseline:  -0.6664333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.148]
 [0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]] [[61.53 ]
 [51.971]
 [51.971]
 [51.971]
 [51.971]
 [51.971]
 [51.971]] [[1.685]
 [1.288]
 [1.288]
 [1.288]
 [1.288]
 [1.288]
 [1.288]]
maxi score, test score, baseline:  -0.6664333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.24763867016791
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.97545549014888
printing an ep nov before normalisation:  0.0
printing an ep nov before normalisation:  20.914732501305938
actor:  1 policy actor:  1  step number:  65 total reward:  0.013333333333332753  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6664333333333334 0.6353333333333333 0.6353333333333333
printing an ep nov before normalisation:  35.915123082185545
maxi score, test score, baseline:  -0.6664333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.945906747633316
maxi score, test score, baseline:  -0.6664333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6664333333333334 0.6353333333333333 0.6353333333333333
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  58 total reward:  0.1799999999999996  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6664333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  40 total reward:  0.5933333333333334  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  27.761082649230957
maxi score, test score, baseline:  -0.6664333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.814695835113525
actions average: 
K:  1  action  0 :  tensor([0.5176, 0.0420, 0.0805, 0.0893, 0.1066, 0.0821, 0.0819],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0217,     0.9435,     0.0038,     0.0049,     0.0037,     0.0009,
            0.0215], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2556, 0.0088, 0.2811, 0.0806, 0.0872, 0.1753, 0.1115],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1923, 0.1055, 0.0803, 0.3087, 0.0768, 0.1015, 0.1348],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2594, 0.0094, 0.0798, 0.1241, 0.3380, 0.0977, 0.0916],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1951, 0.0021, 0.1274, 0.0766, 0.0882, 0.4548, 0.0559],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.3587, 0.0027, 0.1279, 0.1271, 0.1291, 0.1381, 0.1164],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.6664333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6664333333333333 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6664333333333333 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6664333333333333 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.28159381936777
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6664333333333333 0.6353333333333333 0.6353333333333333
Printing some Q and Qe and total Qs values:  [[-0.099]
 [-0.081]
 [-0.103]
 [-0.092]
 [-0.099]
 [-0.103]
 [-0.089]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.099]
 [-0.081]
 [-0.103]
 [-0.092]
 [-0.099]
 [-0.103]
 [-0.089]]
maxi score, test score, baseline:  -0.6664333333333333 0.6353333333333333 0.6353333333333333
actor:  1 policy actor:  1  step number:  49 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  60 total reward:  0.15333333333333266  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  53.32179900540257
maxi score, test score, baseline:  -0.6664333333333333 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6664333333333333 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6664333333333333 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6664333333333333 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86732703
actor:  0 policy actor:  0  step number:  46 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 46.295324860329856
actor:  1 policy actor:  1  step number:  46 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  61 total reward:  0.29333333333333245  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.096]
 [-0.079]
 [-0.096]
 [-0.086]
 [-0.099]
 [-0.096]
 [-0.091]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.096]
 [-0.079]
 [-0.096]
 [-0.086]
 [-0.099]
 [-0.096]
 [-0.091]]
printing an ep nov before normalisation:  43.28411815836315
Printing some Q and Qe and total Qs values:  [[-0.04 ]
 [ 0.006]
 [-0.035]
 [-0.038]
 [-0.038]
 [-0.037]
 [-0.037]] [[29.9  ]
 [51.671]
 [24.45 ]
 [26.697]
 [30.167]
 [27.533]
 [23.053]] [[0.326]
 [0.86 ]
 [0.209]
 [0.256]
 [0.333]
 [0.276]
 [0.176]]
Printing some Q and Qe and total Qs values:  [[-0.021]
 [ 0.274]
 [ 0.225]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]] [[32.673]
 [51.15 ]
 [54.485]
 [32.673]
 [32.673]
 [32.673]
 [32.673]] [[0.425]
 [1.2  ]
 [1.237]
 [0.425]
 [0.425]
 [0.425]
 [0.425]]
maxi score, test score, baseline:  -0.6638333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.47169495242932
siam score:  -0.86593354
printing an ep nov before normalisation:  0.0517438504643053
actor:  1 policy actor:  1  step number:  53 total reward:  0.2933333333333328  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  0  action  0 :  tensor([0.6196, 0.0124, 0.0751, 0.0776, 0.0741, 0.0657, 0.0755],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0124, 0.9498, 0.0097, 0.0064, 0.0036, 0.0037, 0.0143],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2595, 0.0068, 0.3181, 0.0990, 0.0997, 0.1094, 0.1075],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1917, 0.0075, 0.0704, 0.5151, 0.0843, 0.0684, 0.0627],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1750, 0.0055, 0.0680, 0.0709, 0.5453, 0.0823, 0.0531],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1692, 0.0263, 0.0979, 0.1147, 0.1322, 0.3681, 0.0917],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1864, 0.1868, 0.2145, 0.0991, 0.0782, 0.0852, 0.1498],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  55.72139561201087
actor:  1 policy actor:  1  step number:  61 total reward:  0.21333333333333238  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  35.711705557316975
Printing some Q and Qe and total Qs values:  [[-0.069]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.074]] [[71.661]
 [70.049]
 [70.049]
 [70.049]
 [70.049]
 [70.049]
 [70.049]] [[1.463]
 [1.412]
 [1.412]
 [1.412]
 [1.412]
 [1.412]
 [1.412]]
maxi score, test score, baseline:  -0.6638333333333334 0.6353333333333333 0.6353333333333333
printing an ep nov before normalisation:  30.07313118045719
printing an ep nov before normalisation:  30.691120624542236
printing an ep nov before normalisation:  0.655840238156884
maxi score, test score, baseline:  -0.6638333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.32959814086978
line 256 mcts: sample exp_bonus 58.08654171266237
printing an ep nov before normalisation:  26.546982123186577
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.58636194656286
actor:  1 policy actor:  1  step number:  45 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  63 total reward:  0.0666666666666661  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  45.25725819389193
maxi score, test score, baseline:  -0.6638333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.75054733800523
printing an ep nov before normalisation:  61.33751382122237
printing an ep nov before normalisation:  46.137857118305085
maxi score, test score, baseline:  -0.6638333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.85880095
printing an ep nov before normalisation:  23.821024777615033
maxi score, test score, baseline:  -0.6638333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6638333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.92895326072972
maxi score, test score, baseline:  -0.6638333333333334 0.6353333333333333 0.6353333333333333
printing an ep nov before normalisation:  24.681605912171467
Printing some Q and Qe and total Qs values:  [[0.304]
 [0.465]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]] [[73.101]
 [72.914]
 [64.656]
 [64.656]
 [64.656]
 [64.656]
 [64.656]] [[1.471]
 [1.629]
 [1.311]
 [1.311]
 [1.311]
 [1.311]
 [1.311]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.1808],
        [ 0.0639],
        [ 0.1109],
        [-0.0000],
        [-0.0677],
        [ 0.2794],
        [ 0.0527],
        [ 0.4649],
        [ 0.0989],
        [-0.0664]], dtype=torch.float64)
-0.032346567066 -0.2131590518791398
-0.071551887066 -0.007692089361702167
-0.058483887065999995 0.05237472204541464
-0.90580908 -0.90580908
-0.09703970119800001 -0.1647396667383407
-0.09703970119800001 0.1823239689765995
-0.09703970119800001 -0.04430815417746228
-0.045546567066 0.41932047589704635
-0.058614567066 0.040314048183349896
-0.058614567066 -0.12504643192577633
printing an ep nov before normalisation:  55.22683125927547
actor:  1 policy actor:  1  step number:  49 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6638333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.131]
 [0.297]
 [0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]] [[26.616]
 [37.276]
 [26.616]
 [26.616]
 [26.616]
 [26.616]
 [26.616]] [[0.23 ]
 [0.463]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]]
maxi score, test score, baseline:  -0.6638333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.113]
 [-0.089]
 [-0.113]
 [-0.102]
 [-0.113]
 [-0.113]
 [-0.113]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.113]
 [-0.089]
 [-0.113]
 [-0.102]
 [-0.113]
 [-0.113]
 [-0.113]]
maxi score, test score, baseline:  -0.6638333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.714]
 [0.778]
 [0.72 ]
 [0.702]
 [0.702]
 [0.695]
 [0.699]] [[34.482]
 [32.178]
 [30.237]
 [29.157]
 [29.139]
 [29.704]
 [28.484]] [[0.714]
 [0.778]
 [0.72 ]
 [0.702]
 [0.702]
 [0.695]
 [0.699]]
maxi score, test score, baseline:  -0.6638333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.02 ]
 [ 0.032]
 [-0.006]
 [ 0.001]
 [ 0.   ]
 [-0.003]
 [ 0.005]] [[10.088]
 [12.936]
 [23.11 ]
 [48.181]
 [47.175]
 [45.69 ]
 [41.512]] [[ 0.02 ]
 [ 0.032]
 [-0.006]
 [ 0.001]
 [ 0.   ]
 [-0.003]
 [ 0.005]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.876175082581724
maxi score, test score, baseline:  -0.6638333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.009]
 [ 0.04 ]
 [-0.015]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]] [[48.988]
 [57.595]
 [58.834]
 [48.988]
 [48.988]
 [54.392]
 [48.988]] [[1.089]
 [1.387]
 [1.368]
 [1.089]
 [1.089]
 [1.245]
 [1.089]]
maxi score, test score, baseline:  -0.6638333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.09]
 [-0.09]
 [-0.09]
 [-0.09]
 [-0.09]
 [-0.09]
 [-0.09]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.09]
 [-0.09]
 [-0.09]
 [-0.09]
 [-0.09]
 [-0.09]
 [-0.09]]
actor:  1 policy actor:  1  step number:  39 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.6638333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.199]
 [0.778]
 [0.635]
 [0.623]
 [0.489]
 [0.489]
 [0.75 ]] [[73.963]
 [58.996]
 [45.58 ]
 [54.868]
 [44.413]
 [44.413]
 [57.24 ]] [[0.199]
 [0.778]
 [0.635]
 [0.623]
 [0.489]
 [0.489]
 [0.75 ]]
maxi score, test score, baseline:  -0.6638333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6638333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8678334
maxi score, test score, baseline:  -0.6638333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  61 total reward:  0.15999999999999936  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  17.419515337262837
maxi score, test score, baseline:  -0.6615133333333335 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  53 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  1.667
UNIT TEST: sample policy line 217 mcts : [0.082 0.102 0.102 0.122 0.245 0.082 0.265]
siam score:  -0.87167925
printing an ep nov before normalisation:  44.09006625810942
printing an ep nov before normalisation:  42.8266868122131
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  34.265009751250986
line 256 mcts: sample exp_bonus 37.09151092669076
maxi score, test score, baseline:  -0.6588200000000001 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.5282, 0.0148, 0.0877, 0.1001, 0.1164, 0.0774, 0.0753],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0177, 0.9440, 0.0059, 0.0075, 0.0033, 0.0030, 0.0186],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2112, 0.0184, 0.2877, 0.1266, 0.0734, 0.1985, 0.0842],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1533, 0.0097, 0.1174, 0.2900, 0.0969, 0.1497, 0.1830],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1806, 0.0017, 0.0954, 0.0874, 0.4515, 0.1237, 0.0597],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1315, 0.0154, 0.1954, 0.0822, 0.0546, 0.4484, 0.0726],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.3723, 0.0784, 0.0894, 0.0863, 0.0868, 0.1189, 0.1679],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.6588200000000001 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6588200000000001 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6588200000000001 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.17163837132475
maxi score, test score, baseline:  -0.6588200000000001 0.6353333333333333 0.6353333333333333
printing an ep nov before normalisation:  29.157540320380935
Printing some Q and Qe and total Qs values:  [[-0.074]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.074]] [[70.23 ]
 [64.274]
 [64.274]
 [64.274]
 [64.274]
 [64.274]
 [64.274]] [[0.465]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]]
Printing some Q and Qe and total Qs values:  [[-0.071]
 [-0.066]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.073]] [[74.226]
 [74.531]
 [71.548]
 [71.548]
 [71.548]
 [71.548]
 [71.548]] [[0.523]
 [0.531]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]]
actor:  0 policy actor:  0  step number:  65 total reward:  0.31999999999999995  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.65618 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.65618 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 37.35431986346625
printing an ep nov before normalisation:  60.563271606981196
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 40.926797430367685
printing an ep nov before normalisation:  48.61076939862377
printing an ep nov before normalisation:  40.030793895629074
maxi score, test score, baseline:  -0.65618 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.43934392929077
printing an ep nov before normalisation:  47.613227016641595
actions average: 
K:  0  action  0 :  tensor([0.5508, 0.0099, 0.1105, 0.0849, 0.0910, 0.0748, 0.0780],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0121, 0.9459, 0.0080, 0.0140, 0.0022, 0.0043, 0.0135],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2929, 0.0112, 0.3893, 0.0822, 0.0691, 0.1010, 0.0543],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1857, 0.0153, 0.1203, 0.4146, 0.1118, 0.0759, 0.0765],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2035, 0.0033, 0.1257, 0.0923, 0.3542, 0.1346, 0.0863],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.2264, 0.0142, 0.1943, 0.1017, 0.1043, 0.2826, 0.0765],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1485, 0.0814, 0.1479, 0.2153, 0.0956, 0.1129, 0.1984],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  66 total reward:  0.23333333333333306  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.65618 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.94013585792011
printing an ep nov before normalisation:  37.87807039766854
maxi score, test score, baseline:  -0.65618 0.6353333333333333 0.6353333333333333
maxi score, test score, baseline:  -0.65618 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.09202294461046
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.41502313911402
maxi score, test score, baseline:  -0.65618 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  0.39333333333333276  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.061]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]] [[39.752]
 [31.991]
 [31.991]
 [31.991]
 [31.991]
 [31.991]
 [31.991]] [[0.569]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]]
actor:  1 policy actor:  1  step number:  84 total reward:  0.21999999999999975  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.06]
 [-0.06]
 [-0.06]
 [-0.06]
 [-0.06]
 [-0.06]
 [-0.06]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.06]
 [-0.06]
 [-0.06]
 [-0.06]
 [-0.06]
 [-0.06]
 [-0.06]]
maxi score, test score, baseline:  -0.65618 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.063]
 [0.23 ]
 [0.176]
 [0.063]
 [0.063]
 [0.016]
 [0.063]] [[31.427]
 [46.203]
 [48.054]
 [31.427]
 [31.427]
 [38.934]
 [31.427]] [[0.762]
 [1.265]
 [1.253]
 [0.762]
 [0.762]
 [0.886]
 [0.762]]
printing an ep nov before normalisation:  52.115251701739346
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  15.31742156129972
actor:  1 policy actor:  1  step number:  56 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.65618 0.6353333333333333 0.6353333333333333
maxi score, test score, baseline:  -0.65618 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.65618 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.65618 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.65618 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  38 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.483]
 [0.517]
 [0.517]
 [0.388]
 [0.517]
 [0.517]] [[32.505]
 [47.645]
 [32.505]
 [32.505]
 [45.494]
 [32.505]
 [32.505]] [[0.921]
 [1.211]
 [0.921]
 [0.921]
 [1.069]
 [0.921]
 [0.921]]
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.684]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]] [[66.345]
 [68.217]
 [64.506]
 [64.506]
 [64.506]
 [64.506]
 [64.506]] [[1.411]
 [1.619]
 [1.51 ]
 [1.51 ]
 [1.51 ]
 [1.51 ]
 [1.51 ]]
maxi score, test score, baseline:  -0.65618 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.84401755098049
printing an ep nov before normalisation:  30.164063984094213
maxi score, test score, baseline:  -0.65618 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.18777185755028
printing an ep nov before normalisation:  42.635184708738734
maxi score, test score, baseline:  -0.65618 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.574664149340805
maxi score, test score, baseline:  -0.65618 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  52 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.095]
 [-0.086]
 [-0.099]
 [-0.095]
 [-0.095]
 [-0.098]
 [-0.092]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.095]
 [-0.086]
 [-0.099]
 [-0.095]
 [-0.095]
 [-0.098]
 [-0.092]]
actor:  1 policy actor:  1  step number:  66 total reward:  0.31333333333333213  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  34.552395138881906
printing an ep nov before normalisation:  74.4491310629129
Printing some Q and Qe and total Qs values:  [[-0.162]
 [-0.097]
 [-0.101]
 [-0.102]
 [-0.121]
 [-0.156]
 [-0.21 ]] [[29.938]
 [37.319]
 [31.434]
 [30.519]
 [21.45 ]
 [27.69 ]
 [23.873]] [[ 0.108]
 [ 0.281]
 [ 0.19 ]
 [ 0.177]
 [ 0.025]
 [ 0.081]
 [-0.028]]
maxi score, test score, baseline:  -0.6536333333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  66 total reward:  0.2866666666666665  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6510600000000001 0.6353333333333333 0.6353333333333333
maxi score, test score, baseline:  -0.6510600000000001 0.6353333333333333 0.6353333333333333
maxi score, test score, baseline:  -0.6510600000000001 0.6353333333333333 0.6353333333333333
maxi score, test score, baseline:  -0.6510600000000001 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  41 total reward:  0.56  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.1  ]
 [-0.107]
 [-0.1  ]
 [-0.1  ]
 [-0.099]
 [-0.099]
 [-0.099]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.1  ]
 [-0.107]
 [-0.1  ]
 [-0.1  ]
 [-0.099]
 [-0.099]
 [-0.099]]
Printing some Q and Qe and total Qs values:  [[0.437]
 [0.452]
 [0.42 ]
 [0.418]
 [0.438]
 [0.418]
 [0.425]] [[20.27 ]
 [17.525]
 [13.369]
 [17.797]
 [15.244]
 [13.378]
 [13.424]] [[0.682]
 [0.665]
 [0.582]
 [0.633]
 [0.622]
 [0.58 ]
 [0.588]]
printing an ep nov before normalisation:  26.05522632598877
actor:  1 policy actor:  1  step number:  51 total reward:  0.48  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  30.747562646865845
printing an ep nov before normalisation:  29.29544771043615
actor:  1 policy actor:  1  step number:  52 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6510600000000001 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6510600000000001 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6510600000000001 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.85653446500196
printing an ep nov before normalisation:  33.770026776749226
maxi score, test score, baseline:  -0.6510600000000001 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6510600000000001 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.39096939759406
actor:  0 policy actor:  0  step number:  44 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6483533333333333 0.6353333333333333 0.6353333333333333
printing an ep nov before normalisation:  47.307456554684734
printing an ep nov before normalisation:  42.86682424611007
maxi score, test score, baseline:  -0.6483533333333333 0.6353333333333333 0.6353333333333333
printing an ep nov before normalisation:  38.13800451226811
maxi score, test score, baseline:  -0.6483533333333333 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  51 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.048]
 [-0.03 ]
 [-0.062]
 [-0.055]
 [-0.049]
 [-0.056]
 [-0.052]] [[41.548]
 [36.943]
 [40.472]
 [43.425]
 [46.547]
 [43.928]
 [40.256]] [[0.295]
 [0.233]
 [0.262]
 [0.32 ]
 [0.379]
 [0.327]
 [0.268]]
maxi score, test score, baseline:  -0.6456866666666666 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  75.92156610068129
maxi score, test score, baseline:  -0.6456866666666666 0.6353333333333333 0.6353333333333333
printing an ep nov before normalisation:  37.312357013291766
printing an ep nov before normalisation:  66.63528986467209
maxi score, test score, baseline:  -0.6456866666666666 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]] [[73.799]
 [73.799]
 [73.799]
 [73.799]
 [73.799]
 [73.799]
 [73.799]] [[1.167]
 [1.167]
 [1.167]
 [1.167]
 [1.167]
 [1.167]
 [1.167]]
Printing some Q and Qe and total Qs values:  [[0.831]
 [0.946]
 [0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]] [[33.191]
 [36.407]
 [33.191]
 [33.191]
 [33.191]
 [33.191]
 [33.191]] [[0.831]
 [0.946]
 [0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]]
maxi score, test score, baseline:  -0.6456866666666666 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]] [[28.097]
 [28.097]
 [28.097]
 [28.097]
 [28.097]
 [28.097]
 [28.097]] [[0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]]
siam score:  -0.87102234
Printing some Q and Qe and total Qs values:  [[0.495]
 [0.507]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.469]] [[45.67 ]
 [54.891]
 [45.67 ]
 [45.67 ]
 [45.67 ]
 [45.67 ]
 [48.755]] [[1.078]
 [1.266]
 [1.078]
 [1.078]
 [1.078]
 [1.078]
 [1.111]]
maxi score, test score, baseline:  -0.6456866666666666 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  41 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.525]
 [0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]] [[29.575]
 [39.697]
 [29.575]
 [29.575]
 [29.575]
 [29.575]
 [29.575]] [[1.011]
 [1.312]
 [1.011]
 [1.011]
 [1.011]
 [1.011]
 [1.011]]
Printing some Q and Qe and total Qs values:  [[0.778]
 [0.778]
 [0.778]
 [0.778]
 [0.778]
 [0.778]
 [0.778]] [[55.082]
 [55.082]
 [55.082]
 [55.082]
 [55.082]
 [55.082]
 [55.082]] [[0.778]
 [0.778]
 [0.778]
 [0.778]
 [0.778]
 [0.778]
 [0.778]]
printing an ep nov before normalisation:  31.899314020950733
actor:  0 policy actor:  0  step number:  50 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[ 0.046]
 [ 0.326]
 [ 0.046]
 [-0.017]
 [ 0.046]
 [ 0.046]
 [ 0.046]] [[42.007]
 [67.33 ]
 [42.007]
 [68.267]
 [42.007]
 [42.007]
 [42.007]] [[0.475]
 [1.127]
 [0.475]
 [0.798]
 [0.475]
 [0.475]
 [0.475]]
printing an ep nov before normalisation:  0.0
siam score:  -0.8696736
maxi score, test score, baseline:  -0.6428733333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.083]
 [-0.114]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]] [[75.882]
 [81.273]
 [75.882]
 [75.882]
 [75.882]
 [75.882]
 [75.882]] [[1.151]
 [1.219]
 [1.151]
 [1.151]
 [1.151]
 [1.151]
 [1.151]]
printing an ep nov before normalisation:  83.53194626367684
printing an ep nov before normalisation:  61.03504036691566
maxi score, test score, baseline:  -0.6428733333333334 0.6353333333333333 0.6353333333333333
printing an ep nov before normalisation:  33.454356857210364
printing an ep nov before normalisation:  41.6759600521482
maxi score, test score, baseline:  -0.6428733333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.217218542594
maxi score, test score, baseline:  -0.6428733333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.5184, 0.0938, 0.0761, 0.0692, 0.0904, 0.0831, 0.0690],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0081, 0.9356, 0.0051, 0.0194, 0.0023, 0.0027, 0.0269],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2157, 0.0064, 0.4135, 0.0885, 0.0936, 0.1014, 0.0809],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2480, 0.0396, 0.1028, 0.2479, 0.0947, 0.1239, 0.1431],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1696, 0.0070, 0.0543, 0.0626, 0.5486, 0.1003, 0.0575],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2761, 0.0110, 0.1044, 0.0987, 0.1763, 0.2454, 0.0881],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1618, 0.1764, 0.1404, 0.0879, 0.1260, 0.1782, 0.1295],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  34.023520946502686
Printing some Q and Qe and total Qs values:  [[0.747]
 [0.852]
 [0.747]
 [0.747]
 [0.747]
 [0.747]
 [0.747]] [[29.568]
 [34.295]
 [29.568]
 [29.568]
 [29.568]
 [29.568]
 [29.568]] [[0.747]
 [0.852]
 [0.747]
 [0.747]
 [0.747]
 [0.747]
 [0.747]]
actor:  1 policy actor:  1  step number:  65 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  4  action  0 :  tensor([0.4949, 0.0645, 0.0789, 0.1149, 0.0837, 0.0918, 0.0714],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0230, 0.9219, 0.0167, 0.0111, 0.0020, 0.0078, 0.0176],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2448, 0.0197, 0.2077, 0.1313, 0.1062, 0.1857, 0.1048],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2490, 0.0669, 0.1183, 0.1427, 0.1274, 0.1758, 0.1198],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.3386, 0.0206, 0.0748, 0.1155, 0.2569, 0.1115, 0.0820],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2308, 0.0323, 0.1052, 0.1589, 0.0827, 0.3120, 0.0781],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2481, 0.2277, 0.1315, 0.1008, 0.0650, 0.0860, 0.1409],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.6428733333333334 0.6353333333333333 0.6353333333333333
maxi score, test score, baseline:  -0.6428733333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
using explorer policy with actor:  1
printing an ep nov before normalisation:  61.83927798417557
using explorer policy with actor:  1
printing an ep nov before normalisation:  6.688375416184726
maxi score, test score, baseline:  -0.6428733333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6428733333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0
actor:  1 policy actor:  1  step number:  44 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6428733333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  57 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6428733333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.119]
 [-0.109]
 [-0.111]
 [-0.117]
 [-0.111]
 [-0.118]
 [-0.119]] [[17.147]
 [31.189]
 [ 0.   ]
 [17.398]
 [ 0.   ]
 [19.016]
 [18.861]] [[ 0.003]
 [ 0.221]
 [-0.243]
 [ 0.008]
 [-0.243]
 [ 0.031]
 [ 0.028]]
printing an ep nov before normalisation:  26.371539161259847
maxi score, test score, baseline:  -0.6428733333333334 0.6353333333333333 0.6353333333333333
actor:  1 policy actor:  1  step number:  43 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  77.89289362932594
printing an ep nov before normalisation:  68.80745372829452
actor:  1 policy actor:  1  step number:  54 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  41 total reward:  0.5733333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6428733333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6428733333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8622185
maxi score, test score, baseline:  -0.6428733333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.6883, 0.0173, 0.0572, 0.0577, 0.0704, 0.0614, 0.0479],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0296, 0.9093, 0.0136, 0.0109, 0.0051, 0.0090, 0.0225],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2309, 0.0193, 0.3003, 0.1027, 0.1041, 0.1450, 0.0977],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2525, 0.0723, 0.1258, 0.2629, 0.0702, 0.0798, 0.1366],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2796, 0.0273, 0.0492, 0.0442, 0.4644, 0.0796, 0.0556],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1890, 0.0592, 0.1271, 0.0838, 0.0708, 0.3969, 0.0733],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([    0.0792,     0.1797,     0.1010,     0.0541,     0.0005,     0.0015,
            0.5841], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.6428733333333334 0.6353333333333333 0.6353333333333333
maxi score, test score, baseline:  -0.6428733333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.769512649807936
printing an ep nov before normalisation:  71.38051081750743
actor:  0 policy actor:  0  step number:  59 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  59.77434747348508
maxi score, test score, baseline:  -0.6405533333333334 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.545]] [[42.422]
 [25.808]
 [25.808]
 [25.808]
 [25.808]
 [25.808]
 [25.808]] [[1.771]
 [1.306]
 [1.306]
 [1.306]
 [1.306]
 [1.306]
 [1.306]]
actor:  1 policy actor:  1  step number:  61 total reward:  0.10666666666666624  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]] [[48.301]
 [37.12 ]
 [37.12 ]
 [37.12 ]
 [37.12 ]
 [37.12 ]
 [37.12 ]] [[1.913]
 [1.64 ]
 [1.64 ]
 [1.64 ]
 [1.64 ]
 [1.64 ]
 [1.64 ]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.82092809677124
actor:  1 policy actor:  1  step number:  73 total reward:  0.2799999999999986  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  21.86863373003884
actions average: 
K:  4  action  0 :  tensor([0.5882, 0.0496, 0.0608, 0.0858, 0.0865, 0.0644, 0.0648],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0375, 0.8595, 0.0202, 0.0281, 0.0115, 0.0109, 0.0323],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2213, 0.1333, 0.3111, 0.0694, 0.0767, 0.1014, 0.0867],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1830, 0.2134, 0.0866, 0.1570, 0.1246, 0.0915, 0.1440],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2035, 0.1019, 0.0785, 0.1387, 0.3063, 0.1044, 0.0668],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1917, 0.0173, 0.1241, 0.0914, 0.0713, 0.4228, 0.0816],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2221, 0.0260, 0.1577, 0.2041, 0.1216, 0.1127, 0.1557],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  58 total reward:  0.08666666666666589  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.043]
 [-0.02 ]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]] [[38.648]
 [40.431]
 [40.182]
 [40.182]
 [40.182]
 [40.182]
 [40.182]] [[1.118]
 [1.243]
 [1.214]
 [1.214]
 [1.214]
 [1.214]
 [1.214]]
Printing some Q and Qe and total Qs values:  [[-0.073]
 [-0.001]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.073]] [[35.142]
 [50.62 ]
 [35.142]
 [35.142]
 [35.142]
 [35.142]
 [35.142]] [[0.932]
 [1.447]
 [0.932]
 [0.932]
 [0.932]
 [0.932]
 [0.932]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6383800000000001 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.445278999548854
maxi score, test score, baseline:  -0.6383800000000001 0.6353333333333333 0.6353333333333333
maxi score, test score, baseline:  -0.6383800000000001 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6383800000000001 0.6353333333333333 0.6353333333333333
Printing some Q and Qe and total Qs values:  [[0.418]
 [0.531]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]] [[56.456]
 [55.437]
 [56.456]
 [56.456]
 [56.456]
 [56.456]
 [56.456]] [[1.367]
 [1.457]
 [1.367]
 [1.367]
 [1.367]
 [1.367]
 [1.367]]
printing an ep nov before normalisation:  56.270514617986905
actor:  1 policy actor:  1  step number:  58 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6383800000000001 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  0.5733333333333333  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6383800000000001 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.436]
 [0.391]
 [0.39 ]
 [0.392]
 [0.396]
 [0.393]] [[34.2  ]
 [34.212]
 [29.98 ]
 [30.801]
 [30.83 ]
 [31.525]
 [30.911]] [[1.052]
 [1.103]
 [0.897]
 [0.927]
 [0.931]
 [0.961]
 [0.935]]
Printing some Q and Qe and total Qs values:  [[0.69 ]
 [0.775]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]] [[49.915]
 [56.459]
 [49.915]
 [49.915]
 [49.915]
 [49.915]
 [49.915]] [[1.239]
 [1.422]
 [1.239]
 [1.239]
 [1.239]
 [1.239]
 [1.239]]
siam score:  -0.8669382
maxi score, test score, baseline:  -0.6383800000000001 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6383800000000001 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.80962024503124
printing an ep nov before normalisation:  35.19913774088649
actor:  1 policy actor:  1  step number:  47 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  45 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.485]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]] [[17.965]
 [24.694]
 [17.965]
 [17.965]
 [17.965]
 [17.965]
 [17.965]] [[0.478]
 [0.485]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]]
printing an ep nov before normalisation:  59.851740104349055
maxi score, test score, baseline:  -0.6383800000000001 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6383800000000001 0.6353333333333333 0.6353333333333333
actor:  0 policy actor:  0  step number:  45 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  37.57488094729601
actor:  1 policy actor:  1  step number:  34 total reward:  0.5133333333333333  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.63558 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.72017192840576
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.63558 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.63558 0.6353333333333333 0.6353333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  75.81862119491001
maxi score, test score, baseline:  -0.63558 0.6353333333333333 0.6353333333333333
Starting evaluation
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]] [[44.719]
 [44.719]
 [44.719]
 [44.719]
 [44.719]
 [44.719]
 [44.719]] [[0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]]
printing an ep nov before normalisation:  12.040891222290771
printing an ep nov before normalisation:  45.90803096359595
printing an ep nov before normalisation:  42.86068453924547
printing an ep nov before normalisation:  20.531317017639626
printing an ep nov before normalisation:  51.697572251691035
maxi score, test score, baseline:  -0.63558 0.6353333333333333 0.6353333333333333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.08980369567871
Printing some Q and Qe and total Qs values:  [[0.752]
 [0.807]
 [0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]] [[29.513]
 [29.965]
 [29.513]
 [29.513]
 [29.513]
 [29.513]
 [29.513]] [[0.752]
 [0.807]
 [0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]]
printing an ep nov before normalisation:  24.1723537995016
printing an ep nov before normalisation:  26.76425852908981
siam score:  -0.86069113
printing an ep nov before normalisation:  49.94338629979176
printing an ep nov before normalisation:  28.0094051361084
printing an ep nov before normalisation:  37.3437970647465
printing an ep nov before normalisation:  48.00876879385935
printing an ep nov before normalisation:  39.09540212615725
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  35.965906770612484
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333333  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  39.966613998849304
actor:  0 policy actor:  0  step number:  28 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  29 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  29 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  29 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  2
siam score:  -0.8629224
actor:  0 policy actor:  0  step number:  31 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  33 total reward:  0.6133333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  34 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  59 total reward:  0.07999999999999952  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  69 total reward:  0.15999999999999936  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5667266666666667 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.5724, 0.0205, 0.0792, 0.0740, 0.0915, 0.0792, 0.0832],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0042,     0.9721,     0.0054,     0.0034,     0.0008,     0.0013,
            0.0128], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2959, 0.0038, 0.4923, 0.0440, 0.0465, 0.0832, 0.0344],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2250, 0.0520, 0.0912, 0.3317, 0.0822, 0.1274, 0.0906],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2179, 0.0074, 0.1178, 0.1250, 0.2860, 0.1377, 0.1082],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.2862, 0.0072, 0.1027, 0.0834, 0.1159, 0.3395, 0.0651],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1931, 0.0457, 0.1308, 0.1220, 0.0980, 0.1069, 0.3035],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  70.8609806709183
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5667266666666667 0.6673333333333336 0.6673333333333336
maxi score, test score, baseline:  -0.5667266666666668 0.6673333333333336 0.6673333333333336
printing an ep nov before normalisation:  78.87737129702833
actor:  1 policy actor:  1  step number:  49 total reward:  0.4399999999999995  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  61.1141527036706
maxi score, test score, baseline:  -0.5667266666666668 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.61536502838135
Printing some Q and Qe and total Qs values:  [[-0.074]
 [-0.076]
 [-0.076]
 [-0.076]
 [-0.076]
 [-0.076]
 [-0.076]] [[75.031]
 [68.099]
 [68.099]
 [68.099]
 [68.099]
 [68.099]
 [68.099]] [[1.926]
 [1.663]
 [1.663]
 [1.663]
 [1.663]
 [1.663]
 [1.663]]
printing an ep nov before normalisation:  32.604435473240436
maxi score, test score, baseline:  -0.5667266666666668 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.5200000000000002  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  55 total reward:  0.10666666666666613  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  51.21779056982672
printing an ep nov before normalisation:  36.418909114232356
printing an ep nov before normalisation:  33.51195543629332
printing an ep nov before normalisation:  66.86765271721696
maxi score, test score, baseline:  -0.5645133333333333 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  0.5333333333333333  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  56 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8727956
printing an ep nov before normalisation:  39.413923424335465
maxi score, test score, baseline:  -0.5645133333333333 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5645133333333333 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  57 total reward:  0.14666666666666595  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  72.91929038024328
maxi score, test score, baseline:  -0.56222 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.56222 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.56222 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.56222 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  6.4428406480919875
actor:  1 policy actor:  1  step number:  51 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  43 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.56222 0.6673333333333336 0.6673333333333336
printing an ep nov before normalisation:  79.151472326742
printing an ep nov before normalisation:  68.367545848133
printing an ep nov before normalisation:  33.332294106691585
printing an ep nov before normalisation:  33.07309872600466
using explorer policy with actor:  1
printing an ep nov before normalisation:  66.68793116405335
maxi score, test score, baseline:  -0.56222 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.032723903656006
printing an ep nov before normalisation:  14.241951500717008
printing an ep nov before normalisation:  25.354955196380615
actor:  1 policy actor:  1  step number:  57 total reward:  0.10666666666666613  reward:  1.0 rdn_beta:  0.333
siam score:  -0.87460905
maxi score, test score, baseline:  -0.56222 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.56222 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.56222 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.426146499507322
actor:  1 policy actor:  1  step number:  48 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.56222 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.56222 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  81.51611001178033
Printing some Q and Qe and total Qs values:  [[0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]] [[78.686]
 [78.686]
 [78.686]
 [78.686]
 [78.686]
 [78.686]
 [78.686]] [[2.098]
 [2.098]
 [2.098]
 [2.098]
 [2.098]
 [2.098]
 [2.098]]
printing an ep nov before normalisation:  51.4655175874642
printing an ep nov before normalisation:  30.742835998535156
printing an ep nov before normalisation:  33.13319683074951
Printing some Q and Qe and total Qs values:  [[-0.077]
 [-0.109]
 [-0.064]
 [-0.065]
 [-0.065]
 [-0.065]
 [-0.084]] [[62.48 ]
 [57.349]
 [38.038]
 [38.217]
 [38.648]
 [38.99 ]
 [54.076]] [[0.644]
 [0.525]
 [0.245]
 [0.248]
 [0.255]
 [0.26 ]
 [0.496]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.615]] [[20.78]
 [20.78]
 [20.78]
 [20.78]
 [20.78]
 [20.78]
 [20.78]] [[1.666]
 [1.666]
 [1.666]
 [1.666]
 [1.666]
 [1.666]
 [1.666]]
Printing some Q and Qe and total Qs values:  [[0.394]
 [0.601]
 [0.469]
 [0.457]
 [0.457]
 [0.457]
 [0.457]] [[31.188]
 [26.638]
 [25.826]
 [26.547]
 [26.547]
 [26.547]
 [26.547]] [[1.984]
 [1.959]
 [1.785]
 [1.81 ]
 [1.81 ]
 [1.81 ]
 [1.81 ]]
actor:  1 policy actor:  1  step number:  63 total reward:  0.03999999999999959  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.56222 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.835]
 [0.899]
 [0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]] [[43.415]
 [45.127]
 [43.415]
 [43.415]
 [43.415]
 [43.415]
 [43.415]] [[0.835]
 [0.899]
 [0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]]
maxi score, test score, baseline:  -0.56222 0.6673333333333336 0.6673333333333336
maxi score, test score, baseline:  -0.56222 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.8694737
maxi score, test score, baseline:  -0.56222 0.6673333333333336 0.6673333333333336
printing an ep nov before normalisation:  30.031042098999023
maxi score, test score, baseline:  -0.56222 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  57 total reward:  0.19999999999999962  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  41 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  0  action  0 :  tensor([0.6053, 0.0022, 0.0710, 0.0751, 0.1019, 0.0702, 0.0743],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0163, 0.9561, 0.0031, 0.0047, 0.0028, 0.0030, 0.0140],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1064, 0.0213, 0.4579, 0.0575, 0.0520, 0.2006, 0.1042],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1957, 0.0028, 0.1253, 0.2292, 0.1605, 0.1635, 0.1231],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2203, 0.0043, 0.0709, 0.0791, 0.4677, 0.0904, 0.0674],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.3058, 0.0072, 0.1100, 0.1365, 0.1407, 0.1702, 0.1296],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1471, 0.0214, 0.1013, 0.2182, 0.1088, 0.1001, 0.3031],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[-0.094]
 [-0.091]
 [-0.077]
 [-0.086]
 [-0.089]
 [-0.086]
 [-0.08 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.094]
 [-0.091]
 [-0.077]
 [-0.086]
 [-0.089]
 [-0.086]
 [-0.08 ]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.5569133333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.39019873974698
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5569133333333334 0.6673333333333336 0.6673333333333336
siam score:  -0.87205744
line 256 mcts: sample exp_bonus 81.34412631160541
maxi score, test score, baseline:  -0.5569133333333334 0.6673333333333336 0.6673333333333336
printing an ep nov before normalisation:  70.68737892470757
maxi score, test score, baseline:  -0.5569133333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5569133333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.32085876644593
actor:  0 policy actor:  0  step number:  55 total reward:  0.26666666666666616  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  56.73420397304178
actor:  1 policy actor:  1  step number:  42 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.55438 0.6673333333333336 0.6673333333333336
printing an ep nov before normalisation:  43.78037358722647
printing an ep nov before normalisation:  64.210654278677
maxi score, test score, baseline:  -0.55438 0.6673333333333336 0.6673333333333336
maxi score, test score, baseline:  -0.55438 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  0.17999999999999905  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  0  action  0 :  tensor([0.5784, 0.0062, 0.0809, 0.0903, 0.0936, 0.0758, 0.0747],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0023,     0.9748,     0.0030,     0.0053,     0.0005,     0.0006,
            0.0134], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2465, 0.0062, 0.3015, 0.0967, 0.0813, 0.1786, 0.0893],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1548, 0.0261, 0.1052, 0.4105, 0.1210, 0.1158, 0.0667],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1611, 0.0044, 0.0973, 0.0930, 0.4572, 0.1086, 0.0784],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1737, 0.0031, 0.1177, 0.1274, 0.1728, 0.3049, 0.1005],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.3533, 0.0057, 0.1106, 0.1034, 0.1050, 0.1304, 0.1915],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.55438 0.6673333333333336 0.6673333333333336
Printing some Q and Qe and total Qs values:  [[-0.118]
 [-0.074]
 [-0.076]
 [-0.084]
 [-0.099]
 [-0.105]
 [-0.078]] [[38.961]
 [50.68 ]
 [42.075]
 [33.953]
 [28.832]
 [30.128]
 [36.935]] [[ 0.158]
 [ 0.409]
 [ 0.256]
 [ 0.105]
 [-0.001]
 [ 0.016]
 [ 0.163]]
printing an ep nov before normalisation:  47.09349100451789
printing an ep nov before normalisation:  41.677839175486646
maxi score, test score, baseline:  -0.55438 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.4266666666666662  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.55438 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.022]
 [ 0.242]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.021]
 [-0.022]] [[29.774]
 [65.218]
 [29.774]
 [29.774]
 [29.774]
 [35.364]
 [29.774]] [[0.458]
 [1.838]
 [0.458]
 [0.458]
 [0.458]
 [0.635]
 [0.458]]
Printing some Q and Qe and total Qs values:  [[0.32]
 [0.41]
 [0.32]
 [0.32]
 [0.32]
 [0.32]
 [0.32]] [[52.608]
 [61.066]
 [52.608]
 [52.608]
 [52.608]
 [52.608]
 [52.608]] [[1.527]
 [1.897]
 [1.527]
 [1.527]
 [1.527]
 [1.527]
 [1.527]]
maxi score, test score, baseline:  -0.55438 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.55438 0.6673333333333336 0.6673333333333336
Printing some Q and Qe and total Qs values:  [[0.757]
 [0.757]
 [0.769]
 [0.757]
 [0.719]
 [0.726]
 [0.757]] [[58.104]
 [58.104]
 [63.278]
 [58.104]
 [62.105]
 [60.914]
 [58.104]] [[2.512]
 [2.512]
 [2.682]
 [2.512]
 [2.597]
 [2.567]
 [2.512]]
printing an ep nov before normalisation:  59.75873456071654
printing an ep nov before normalisation:  83.1590755057461
maxi score, test score, baseline:  -0.55438 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.966333389282227
maxi score, test score, baseline:  -0.55438 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.811063612499755
siam score:  -0.8665863
maxi score, test score, baseline:  -0.55438 0.6673333333333336 0.6673333333333336
actor:  1 policy actor:  1  step number:  59 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.60482131014618
maxi score, test score, baseline:  -0.55438 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.55438 0.6673333333333336 0.6673333333333336
maxi score, test score, baseline:  -0.55438 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.077]
 [-0.09 ]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]] [[45.666]
 [44.483]
 [37.745]
 [37.745]
 [37.745]
 [37.745]
 [37.745]] [[0.871]
 [0.828]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]]
maxi score, test score, baseline:  -0.55438 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8619911
actor:  1 policy actor:  1  step number:  63 total reward:  0.45333333333333337  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  45 total reward:  0.3999999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.5515800000000001 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.601845841006714
printing an ep nov before normalisation:  34.16762295027933
printing an ep nov before normalisation:  35.658886432647705
maxi score, test score, baseline:  -0.5515800000000001 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.41274055411983
printing an ep nov before normalisation:  65.05299465468403
maxi score, test score, baseline:  -0.5515800000000001 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5515800000000001 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.3533333333333323  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  54 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  0.667
siam score:  -0.86092985
maxi score, test score, baseline:  -0.5515800000000001 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.79737032803291
Printing some Q and Qe and total Qs values:  [[-0.102]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]] [[47.062]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[ 1.467]
 [-1.226]
 [-1.226]
 [-1.226]
 [-1.226]
 [-1.226]
 [-1.226]]
actor:  0 policy actor:  0  step number:  55 total reward:  0.22666666666666624  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  28.04810506123265
printing an ep nov before normalisation:  34.50710189494279
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5491266666666667 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.1  ]
 [-0.082]
 [-0.097]
 [-0.099]
 [-0.1  ]
 [-0.104]
 [-0.079]] [[15.663]
 [29.045]
 [14.579]
 [14.642]
 [15.187]
 [16.679]
 [20.899]] [[ 0.008]
 [ 0.575]
 [-0.034]
 [-0.033]
 [-0.011]
 [ 0.046]
 [ 0.244]]
printing an ep nov before normalisation:  32.85592490577996
maxi score, test score, baseline:  -0.5491266666666667 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.453135283285974
printing an ep nov before normalisation:  28.473361625469174
Printing some Q and Qe and total Qs values:  [[0.431]
 [0.43 ]
 [0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]] [[28.89 ]
 [29.262]
 [28.89 ]
 [28.89 ]
 [28.89 ]
 [28.89 ]
 [28.89 ]] [[1.314]
 [1.334]
 [1.314]
 [1.314]
 [1.314]
 [1.314]
 [1.314]]
maxi score, test score, baseline:  -0.5491266666666667 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.334490399744205
maxi score, test score, baseline:  -0.5491266666666667 0.6673333333333336 0.6673333333333336
printing an ep nov before normalisation:  27.165167331695557
actor:  1 policy actor:  1  step number:  60 total reward:  0.2199999999999993  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.5491266666666667 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  42 total reward:  0.45999999999999963  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.019]
 [-0.01 ]
 [-0.033]
 [-0.035]
 [-0.035]
 [-0.038]
 [-0.037]] [[35.725]
 [34.362]
 [33.981]
 [33.335]
 [34.279]
 [34.328]
 [33.839]] [[1.177]
 [1.102]
 [1.056]
 [1.015]
 [1.072]
 [1.073]
 [1.044]]
maxi score, test score, baseline:  -0.5462066666666667 0.6673333333333336 0.6673333333333336
maxi score, test score, baseline:  -0.5462066666666667 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.75505262789811
printing an ep nov before normalisation:  71.57209647401005
maxi score, test score, baseline:  -0.5462066666666667 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.16644160471641
actor:  0 policy actor:  0  step number:  41 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.061]
 [0.282]
 [0.104]
 [0.061]
 [0.061]
 [0.061]
 [0.061]] [[44.15 ]
 [50.166]
 [41.961]
 [44.15 ]
 [44.15 ]
 [44.15 ]
 [44.15 ]] [[0.265]
 [0.533]
 [0.291]
 [0.265]
 [0.265]
 [0.265]
 [0.265]]
maxi score, test score, baseline:  -0.54322 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.52 ]
 [0.553]
 [0.52 ]
 [0.474]
 [0.52 ]
 [0.52 ]
 [0.465]] [[35.096]
 [38.386]
 [35.096]
 [32.797]
 [35.096]
 [35.096]
 [33.67 ]] [[0.707]
 [0.769]
 [0.707]
 [0.641]
 [0.707]
 [0.707]
 [0.639]]
maxi score, test score, baseline:  -0.54322 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  57 total reward:  0.23999999999999966  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  46.6174173157359
maxi score, test score, baseline:  -0.54322 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.31743092589008
actor:  1 policy actor:  1  step number:  57 total reward:  0.2933333333333328  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  59.2225681938775
siam score:  -0.8759434
maxi score, test score, baseline:  -0.54322 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8768201
actor:  1 policy actor:  1  step number:  34 total reward:  0.6066666666666668  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.54322 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.54322 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8737014
maxi score, test score, baseline:  -0.54322 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.21458514432717
printing an ep nov before normalisation:  30.472169909623492
using explorer policy with actor:  1
printing an ep nov before normalisation:  24.606050311165205
siam score:  -0.86941695
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.54322 0.6673333333333336 0.6673333333333336
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.54322 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.15333333333333288  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.54322 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.54322 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.662]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]] [[33.917]
 [18.932]
 [26.28 ]
 [26.28 ]
 [26.28 ]
 [26.28 ]
 [26.28 ]] [[1.54 ]
 [1.266]
 [1.342]
 [1.342]
 [1.342]
 [1.342]
 [1.342]]
printing an ep nov before normalisation:  54.708309417921406
printing an ep nov before normalisation:  38.82003625567159
Printing some Q and Qe and total Qs values:  [[-0.056]
 [ 0.179]
 [-0.054]
 [-0.056]
 [-0.057]
 [-0.056]
 [-0.062]] [[39.382]
 [48.271]
 [38.453]
 [39.376]
 [39.451]
 [39.98 ]
 [39.695]] [[0.364]
 [0.783]
 [0.346]
 [0.363]
 [0.364]
 [0.376]
 [0.363]]
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.662]
 [0.507]
 [0.507]
 [0.512]
 [0.463]
 [0.448]] [[26.524]
 [15.675]
 [18.339]
 [18.339]
 [15.355]
 [17.486]
 [26.896]] [[1.413]
 [1.221]
 [1.162]
 [1.162]
 [1.06 ]
 [1.087]
 [1.408]]
printing an ep nov before normalisation:  39.482712829415235
maxi score, test score, baseline:  -0.54322 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  79 total reward:  0.15999999999999825  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.54322 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.54322 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.54322 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.54322 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.54322 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.8056250615512
maxi score, test score, baseline:  -0.54322 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.772223811610345
maxi score, test score, baseline:  -0.54322 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.33803905294137
Printing some Q and Qe and total Qs values:  [[0.319]
 [0.477]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.323]
 [0.45 ]] [[27.03 ]
 [36.122]
 [32.884]
 [32.884]
 [32.884]
 [25.911]
 [32.884]] [[0.673]
 [1.055]
 [0.948]
 [0.948]
 [0.948]
 [0.649]
 [0.948]]
maxi score, test score, baseline:  -0.54322 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.319]
 [0.499]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.465]
 [0.45 ]] [[27.03 ]
 [45.489]
 [32.884]
 [32.884]
 [32.884]
 [43.514]
 [32.884]] [[0.512]
 [0.939]
 [0.721]
 [0.721]
 [0.721]
 [0.878]
 [0.721]]
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]] [[39.782]
 [39.782]
 [39.782]
 [39.782]
 [39.782]
 [39.782]
 [39.782]] [[1.401]
 [1.401]
 [1.401]
 [1.401]
 [1.401]
 [1.401]
 [1.401]]
maxi score, test score, baseline:  -0.54322 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.066]
 [-0.081]
 [-0.053]
 [-0.053]
 [-0.067]
 [-0.067]
 [-0.07 ]] [[16.165]
 [21.994]
 [21.101]
 [21.101]
 [16.695]
 [16.725]
 [16.392]] [[0.756]
 [1.319]
 [1.259]
 [1.259]
 [0.808]
 [0.811]
 [0.775]]
maxi score, test score, baseline:  -0.54322 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.27999999999999925  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.54322 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.049]
 [-0.033]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]] [[36.705]
 [34.361]
 [37.297]
 [37.297]
 [37.297]
 [37.297]
 [37.297]] [[0.605]
 [0.529]
 [0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]]
maxi score, test score, baseline:  -0.54322 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.015]
 [1.015]
 [1.015]
 [1.015]
 [1.015]
 [1.015]
 [1.015]] [[44.703]
 [44.703]
 [44.703]
 [44.703]
 [44.703]
 [44.703]
 [44.703]] [[1.015]
 [1.015]
 [1.015]
 [1.015]
 [1.015]
 [1.015]
 [1.015]]
printing an ep nov before normalisation:  57.20154166362897
maxi score, test score, baseline:  -0.54322 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.54322 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  46 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.54046 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.54046 0.6673333333333336 0.6673333333333336
maxi score, test score, baseline:  -0.54046 0.6673333333333336 0.6673333333333336
actor:  1 policy actor:  1  step number:  51 total reward:  0.3999999999999996  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.489]
 [0.355]
 [0.355]
 [0.355]
 [0.355]
 [0.355]] [[49.387]
 [54.745]
 [49.387]
 [49.387]
 [49.387]
 [49.387]
 [49.387]] [[1.154]
 [1.43 ]
 [1.154]
 [1.154]
 [1.154]
 [1.154]
 [1.154]]
printing an ep nov before normalisation:  23.579983711242676
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.067]
 [ 0.135]
 [-0.067]
 [-0.061]
 [-0.067]
 [-0.067]
 [-0.067]] [[40.972]
 [46.432]
 [40.972]
 [40.298]
 [40.972]
 [40.972]
 [40.972]] [[0.843]
 [1.282]
 [0.843]
 [0.819]
 [0.843]
 [0.843]
 [0.843]]
maxi score, test score, baseline:  -0.54046 0.6673333333333336 0.6673333333333336
actor:  1 policy actor:  1  step number:  51 total reward:  0.3999999999999999  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  48.662566309403246
Printing some Q and Qe and total Qs values:  [[-0.046]
 [-0.018]
 [-0.047]
 [-0.049]
 [-0.049]
 [-0.05 ]
 [-0.049]] [[30.126]
 [49.841]
 [27.541]
 [28.62 ]
 [28.862]
 [30.488]
 [29.405]] [[0.319]
 [0.942]
 [0.24 ]
 [0.27 ]
 [0.277]
 [0.325]
 [0.294]]
maxi score, test score, baseline:  -0.54046 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.66793727874756
maxi score, test score, baseline:  -0.54046 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.027]
 [ 0.158]
 [-0.027]
 [-0.027]
 [-0.029]
 [-0.027]
 [-0.027]] [[32.446]
 [37.78 ]
 [32.446]
 [32.446]
 [30.209]
 [32.446]
 [32.446]] [[0.364]
 [0.64 ]
 [0.364]
 [0.364]
 [0.324]
 [0.364]
 [0.364]]
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]] [[76.843]
 [76.843]
 [76.843]
 [76.843]
 [76.843]
 [76.843]
 [76.843]] [[0.942]
 [0.942]
 [0.942]
 [0.942]
 [0.942]
 [0.942]
 [0.942]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  56 total reward:  0.19333333333333302  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.54046 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.54046 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.073703934508195
printing an ep nov before normalisation:  21.873525632535802
actor:  1 policy actor:  1  step number:  51 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  30.196140099493963
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.07]
 [-0.07]
 [-0.07]
 [-0.07]
 [-0.07]
 [-0.07]
 [-0.07]] [[24.653]
 [24.653]
 [24.653]
 [24.653]
 [24.653]
 [24.653]
 [24.653]] [[0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]]
maxi score, test score, baseline:  -0.54046 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.38139067587625
maxi score, test score, baseline:  -0.54046 0.6673333333333336 0.6673333333333336
maxi score, test score, baseline:  -0.54046 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.097]
 [-0.046]
 [ 0.057]
 [-0.078]
 [-0.071]
 [ 0.164]
 [-0.08 ]] [[34.655]
 [38.731]
 [37.397]
 [34.235]
 [35.413]
 [34.519]
 [34.313]] [[0.755]
 [1.044]
 [1.068]
 [0.75 ]
 [0.826]
 [1.008]
 [0.752]]
maxi score, test score, baseline:  -0.54046 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 33.76729357243042
printing an ep nov before normalisation:  64.96125015730354
maxi score, test score, baseline:  -0.54046 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  47 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.5378466666666667 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5378466666666667 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.12803204963114
Printing some Q and Qe and total Qs values:  [[0.715]
 [0.847]
 [0.66 ]
 [0.715]
 [0.715]
 [0.715]
 [0.715]] [[39.549]
 [70.058]
 [50.923]
 [39.549]
 [39.549]
 [39.549]
 [39.549]] [[0.715]
 [0.847]
 [0.66 ]
 [0.715]
 [0.715]
 [0.715]
 [0.715]]
maxi score, test score, baseline:  -0.5378466666666667 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.40855739627563
maxi score, test score, baseline:  -0.5378466666666667 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5378466666666667 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.15142103208153
maxi score, test score, baseline:  -0.5378466666666667 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.33985835206258
printing an ep nov before normalisation:  40.527623306247435
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  58.13355953705077
actor:  0 policy actor:  0  step number:  46 total reward:  0.35333333333333317  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  47 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  15.834224224090576
actor:  1 policy actor:  1  step number:  53 total reward:  0.33333333333333315  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.5351400000000001 0.6673333333333336 0.6673333333333336
Printing some Q and Qe and total Qs values:  [[0.722]
 [0.653]
 [0.722]
 [0.722]
 [0.722]
 [0.722]
 [0.722]] [[49.531]
 [67.844]
 [49.531]
 [49.531]
 [49.531]
 [49.531]
 [49.531]] [[1.714]
 [2.159]
 [1.714]
 [1.714]
 [1.714]
 [1.714]
 [1.714]]
printing an ep nov before normalisation:  70.82648174580909
maxi score, test score, baseline:  -0.5351400000000001 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  54.147008787251934
printing an ep nov before normalisation:  41.452891717592266
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  0.19999999999999973  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  55 total reward:  0.4266666666666662  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  61.1221194270156
printing an ep nov before normalisation:  70.00881017071373
printing an ep nov before normalisation:  59.03954560441941
printing an ep nov before normalisation:  76.24075723152889
Printing some Q and Qe and total Qs values:  [[0.688]
 [0.638]
 [0.688]
 [0.688]
 [0.688]
 [0.688]
 [0.688]] [[35.356]
 [49.809]
 [35.356]
 [35.356]
 [35.356]
 [35.356]
 [35.356]] [[1.459]
 [1.929]
 [1.459]
 [1.459]
 [1.459]
 [1.459]
 [1.459]]
maxi score, test score, baseline:  -0.5351400000000001 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.182715621690576
maxi score, test score, baseline:  -0.5351400000000001 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.4866666666666668  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  49.000281986764584
maxi score, test score, baseline:  -0.5351400000000001 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  47 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.5323666666666667 0.6673333333333336 0.6673333333333336
maxi score, test score, baseline:  -0.5323666666666667 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5323666666666667 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.346666666666666  reward:  1.0 rdn_beta:  1.667
siam score:  -0.87015456
maxi score, test score, baseline:  -0.5323666666666667 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.009]
 [ 0.471]
 [ 0.356]
 [ 0.356]
 [ 0.356]
 [ 0.356]
 [ 0.356]] [[67.619]
 [67.888]
 [61.589]
 [61.589]
 [61.589]
 [61.589]
 [61.589]] [[1.112]
 [1.599]
 [1.331]
 [1.331]
 [1.331]
 [1.331]
 [1.331]]
Printing some Q and Qe and total Qs values:  [[0.645]
 [0.499]
 [0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]] [[49.464]
 [55.148]
 [49.464]
 [49.464]
 [49.464]
 [49.464]
 [49.464]] [[1.694]
 [1.724]
 [1.694]
 [1.694]
 [1.694]
 [1.694]
 [1.694]]
maxi score, test score, baseline:  -0.5323666666666667 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.5323666666666667 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.86830324
maxi score, test score, baseline:  -0.5323666666666667 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5323666666666667 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  55 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5299933333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.739002582189116
printing an ep nov before normalisation:  40.77119460749922
maxi score, test score, baseline:  -0.5299933333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.59145145483375
actor:  1 policy actor:  1  step number:  47 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.20527976263568
printing an ep nov before normalisation:  65.81854436927301
printing an ep nov before normalisation:  0.28314970006931617
actor:  0 policy actor:  0  step number:  45 total reward:  0.38666666666666616  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.52722 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.52722 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  53 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.5245533333333333 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.636279243713496
maxi score, test score, baseline:  -0.5245533333333333 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.461613860197566
Printing some Q and Qe and total Qs values:  [[0.367]
 [0.455]
 [0.388]
 [0.388]
 [0.385]
 [0.388]
 [0.388]] [[59.288]
 [56.604]
 [51.16 ]
 [51.16 ]
 [55.157]
 [51.16 ]
 [51.16 ]] [[1.507]
 [1.523]
 [1.308]
 [1.308]
 [1.414]
 [1.308]
 [1.308]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.5245533333333333 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.319]
 [0.439]
 [0.315]
 [0.315]
 [0.307]
 [0.315]
 [0.315]] [[31.535]
 [37.697]
 [28.314]
 [28.314]
 [31.77 ]
 [28.314]
 [28.314]] [[0.972]
 [1.296]
 [0.862]
 [0.862]
 [0.968]
 [0.862]
 [0.862]]
maxi score, test score, baseline:  -0.5245533333333333 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  39 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  49 total reward:  0.3999999999999996  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.094]
 [-0.083]
 [-0.094]
 [-0.094]
 [-0.094]
 [-0.094]
 [-0.094]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.094]
 [-0.083]
 [-0.094]
 [-0.094]
 [-0.094]
 [-0.094]
 [-0.094]]
printing an ep nov before normalisation:  72.55412993214865
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]] [[38.694]
 [30.814]
 [30.814]
 [30.814]
 [30.814]
 [30.814]
 [30.814]] [[1.454]
 [1.057]
 [1.057]
 [1.057]
 [1.057]
 [1.057]
 [1.057]]
maxi score, test score, baseline:  -0.5245533333333333 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5245533333333333 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5245533333333333 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.162901383475926
maxi score, test score, baseline:  -0.5245533333333333 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  27.92850391281761
maxi score, test score, baseline:  -0.5245533333333333 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.85006666183472
actor:  0 policy actor:  0  step number:  34 total reward:  0.54  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.5214733333333333 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5214733333333333 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5214733333333333 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5214733333333333 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5214733333333333 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.45999999999999963  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  43.278729052900204
actor:  1 policy actor:  1  step number:  45 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.78296661376953
actions average: 
K:  1  action  0 :  tensor([0.6114, 0.0069, 0.0738, 0.0776, 0.0939, 0.0768, 0.0594],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0079, 0.9638, 0.0061, 0.0054, 0.0024, 0.0024, 0.0120],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2251, 0.0443, 0.2796, 0.1203, 0.0860, 0.1430, 0.1016],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1508, 0.0080, 0.0691, 0.5491, 0.0654, 0.0819, 0.0758],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0829,     0.0056,     0.0007,     0.0572,     0.8354,     0.0046,
            0.0136], grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2578, 0.0113, 0.1361, 0.1291, 0.1617, 0.1698, 0.1343],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1279, 0.1527, 0.1015, 0.1192, 0.1087, 0.1279, 0.2622],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  32.65778124332428
siam score:  -0.86005944
printing an ep nov before normalisation:  30.071876049041748
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.561]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]] [[30.766]
 [39.348]
 [30.766]
 [30.766]
 [30.766]
 [30.766]
 [30.766]] [[0.958]
 [1.139]
 [0.958]
 [0.958]
 [0.958]
 [0.958]
 [0.958]]
actor:  1 policy actor:  1  step number:  35 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  32.53027942453811
maxi score, test score, baseline:  -0.5214733333333333 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.4432297656863
printing an ep nov before normalisation:  25.595322923482442
printing an ep nov before normalisation:  32.07299826731794
actor:  1 policy actor:  1  step number:  62 total reward:  0.15333333333333243  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  46.53459226165996
actor:  1 policy actor:  1  step number:  54 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.5214733333333333 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5214733333333333 0.6673333333333336 0.6673333333333336
maxi score, test score, baseline:  -0.5214733333333333 0.6673333333333336 0.6673333333333336
actor:  1 policy actor:  1  step number:  40 total reward:  0.48666666666666647  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  36.93803834865305
printing an ep nov before normalisation:  39.923813343048096
Printing some Q and Qe and total Qs values:  [[0.69 ]
 [0.695]
 [0.687]
 [0.672]
 [0.612]
 [0.61 ]
 [0.672]] [[29.166]
 [31.023]
 [29.262]
 [26.566]
 [36.589]
 [37.455]
 [26.566]] [[1.221]
 [1.287]
 [1.221]
 [1.118]
 [1.387]
 [1.413]
 [1.118]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5214733333333333 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.5214733333333333 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.6477, 0.0100, 0.0635, 0.0741, 0.0675, 0.0695, 0.0677],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0141, 0.9503, 0.0054, 0.0078, 0.0022, 0.0023, 0.0180],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2671, 0.0353, 0.3630, 0.0840, 0.0827, 0.0797, 0.0882],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1415, 0.0193, 0.0854, 0.3681, 0.1054, 0.1327, 0.1474],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1684, 0.0020, 0.0632, 0.0802, 0.5612, 0.0655, 0.0596],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.2037, 0.0013, 0.2048, 0.0782, 0.0771, 0.3692, 0.0657],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1936, 0.1181, 0.1453, 0.1326, 0.1240, 0.1549, 0.1315],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.5214733333333333 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.6104, 0.0164, 0.0753, 0.0692, 0.0747, 0.0568, 0.0972],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0139, 0.9395, 0.0061, 0.0162, 0.0035, 0.0041, 0.0168],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2575, 0.0144, 0.4345, 0.0749, 0.0662, 0.0804, 0.0720],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1897, 0.0758, 0.0671, 0.4311, 0.0477, 0.0475, 0.1411],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2070, 0.0573, 0.0989, 0.1121, 0.3057, 0.1190, 0.0998],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1553, 0.0225, 0.1181, 0.1696, 0.0634, 0.3940, 0.0771],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1690, 0.2437, 0.0684, 0.0959, 0.1170, 0.0622, 0.2438],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  51 total reward:  0.38666666666666605  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.527]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]] [[37.088]
 [47.733]
 [37.088]
 [37.088]
 [37.088]
 [37.088]
 [37.088]] [[1.649]
 [2.204]
 [1.649]
 [1.649]
 [1.649]
 [1.649]
 [1.649]]
printing an ep nov before normalisation:  45.17355383839776
maxi score, test score, baseline:  -0.5214733333333333 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5214733333333333 0.6673333333333336 0.6673333333333336
maxi score, test score, baseline:  -0.5214733333333333 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]] [[41.232]
 [32.443]
 [32.443]
 [32.443]
 [32.443]
 [32.443]
 [32.443]] [[2.266]
 [1.899]
 [1.899]
 [1.899]
 [1.899]
 [1.899]
 [1.899]]
actor:  0 policy actor:  0  step number:  57 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  16.956957196265577
maxi score, test score, baseline:  -0.5191533333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5191533333333334 0.6673333333333336 0.6673333333333336
maxi score, test score, baseline:  -0.5191533333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.56480079025707
actor:  1 policy actor:  1  step number:  50 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5191533333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5191533333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 25.928697590569687
Printing some Q and Qe and total Qs values:  [[0.658]
 [0.844]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]] [[52.265]
 [61.815]
 [52.265]
 [52.265]
 [52.265]
 [52.265]
 [52.265]] [[0.658]
 [0.844]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]]
printing an ep nov before normalisation:  0.01152995582060612
printing an ep nov before normalisation:  43.48276412660855
printing an ep nov before normalisation:  36.17413438105619
actions average: 
K:  2  action  0 :  tensor([0.6093, 0.0428, 0.0716, 0.0801, 0.0841, 0.0581, 0.0539],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0057, 0.9504, 0.0104, 0.0116, 0.0050, 0.0038, 0.0131],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2053, 0.0095, 0.3911, 0.1116, 0.0639, 0.1571, 0.0616],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2260, 0.0450, 0.1031, 0.3592, 0.0969, 0.0931, 0.0768],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1589, 0.0391, 0.0721, 0.0972, 0.5092, 0.0703, 0.0531],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.3058, 0.0117, 0.2064, 0.1007, 0.1010, 0.1918, 0.0826],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1146, 0.3590, 0.0668, 0.0686, 0.0809, 0.0580, 0.2521],
       grad_fn=<DivBackward0>)
siam score:  -0.87256604
actor:  1 policy actor:  1  step number:  60 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.5191533333333334 0.6673333333333336 0.6673333333333336
printing an ep nov before normalisation:  26.867921549956442
Printing some Q and Qe and total Qs values:  [[-0.032]
 [-0.096]
 [-0.014]
 [-0.021]
 [-0.021]
 [-0.024]
 [-0.021]] [[14.674]
 [15.982]
 [13.025]
 [12.479]
 [12.82 ]
 [12.564]
 [12.5  ]] [[0.6  ]
 [0.596]
 [0.544]
 [0.511]
 [0.527]
 [0.512]
 [0.512]]
printing an ep nov before normalisation:  36.81251353347064
printing an ep nov before normalisation:  34.10682201385498
maxi score, test score, baseline:  -0.5191533333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  48 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  36.675133041639775
printing an ep nov before normalisation:  25.52164087597847
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]] [[28.249]
 [28.249]
 [28.249]
 [28.249]
 [28.249]
 [28.249]
 [28.249]] [[0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5165533333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.18651804653342
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5165533333333333 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  51 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5139933333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5139933333333334 0.6673333333333336 0.6673333333333336
maxi score, test score, baseline:  -0.5139933333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5139933333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8821858
maxi score, test score, baseline:  -0.5139933333333334 0.6673333333333336 0.6673333333333336
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 34.97791246309542
maxi score, test score, baseline:  -0.5139933333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5139933333333334 0.6673333333333336 0.6673333333333336
actor:  1 policy actor:  1  step number:  45 total reward:  0.41333333333333333  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[ 0.051]
 [ 0.399]
 [ 0.25 ]
 [ 0.227]
 [ 0.227]
 [ 0.227]
 [-0.011]] [[39.542]
 [34.847]
 [36.472]
 [32.393]
 [32.393]
 [32.393]
 [37.098]] [[0.61 ]
 [0.832]
 [0.727]
 [0.595]
 [0.595]
 [0.595]
 [0.482]]
maxi score, test score, baseline:  -0.5139933333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  0.6133333333333334  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  39.290766152427004
siam score:  -0.87975144
printing an ep nov before normalisation:  46.118112054394864
printing an ep nov before normalisation:  39.77687108987035
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5139933333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5139933333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.5139933333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5139933333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  67 total reward:  0.02666666666666595  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  71.09050004032547
printing an ep nov before normalisation:  51.34866901423557
maxi score, test score, baseline:  -0.5139933333333334 0.6673333333333336 0.6673333333333336
printing an ep nov before normalisation:  41.45321749168379
printing an ep nov before normalisation:  27.81926155090332
printing an ep nov before normalisation:  39.998448477712046
maxi score, test score, baseline:  -0.5139933333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.222245844080845
printing an ep nov before normalisation:  39.23837025428913
maxi score, test score, baseline:  -0.5139933333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.5089, 0.0466, 0.0812, 0.0979, 0.1001, 0.0819, 0.0834],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0140, 0.9398, 0.0148, 0.0068, 0.0026, 0.0069, 0.0151],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1723, 0.0311, 0.3611, 0.0888, 0.0787, 0.1742, 0.0938],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1416, 0.0379, 0.0919, 0.4810, 0.0743, 0.0893, 0.0840],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2848, 0.0042, 0.0376, 0.0613, 0.4791, 0.0844, 0.0485],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2011, 0.0052, 0.0876, 0.1258, 0.1341, 0.3750, 0.0714],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1759, 0.2469, 0.1092, 0.0520, 0.0364, 0.0650, 0.3145],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  49 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.045]
 [-0.021]
 [-0.045]
 [-0.045]
 [-0.045]
 [-0.045]
 [-0.045]] [[46.114]
 [58.061]
 [46.114]
 [46.114]
 [46.114]
 [46.114]
 [46.114]] [[1.044]
 [1.431]
 [1.044]
 [1.044]
 [1.044]
 [1.044]
 [1.044]]
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.637]
 [0.561]
 [0.468]
 [0.561]
 [0.477]
 [0.465]] [[24.72 ]
 [30.765]
 [30.36 ]
 [24.645]
 [30.36 ]
 [24.657]
 [24.807]] [[0.778]
 [1.126]
 [1.039]
 [0.785]
 [1.039]
 [0.795]
 [0.787]]
maxi score, test score, baseline:  -0.5139933333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.514241404985604
maxi score, test score, baseline:  -0.5139933333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5139933333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5139933333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.32639827816398
maxi score, test score, baseline:  -0.5139933333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5139933333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  8.964833617210388
printing an ep nov before normalisation:  9.461386005083721
maxi score, test score, baseline:  -0.5139933333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]] [[38.519]
 [38.519]
 [38.519]
 [38.519]
 [38.519]
 [38.519]
 [38.519]] [[0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.976]]
maxi score, test score, baseline:  -0.5139933333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.129696074743606
printing an ep nov before normalisation:  46.23127586451425
actor:  1 policy actor:  1  step number:  51 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  34.18018635669126
siam score:  -0.8702363
actor:  1 policy actor:  1  step number:  51 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  52 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5139933333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5139933333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.33571730092165
printing an ep nov before normalisation:  39.959763115177736
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.461]
 [0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]] [[43.712]
 [54.583]
 [43.712]
 [43.712]
 [43.712]
 [43.712]
 [43.712]] [[0.495]
 [0.711]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]]
maxi score, test score, baseline:  -0.5139933333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.578536281774355
maxi score, test score, baseline:  -0.5139933333333334 0.6673333333333336 0.6673333333333336
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  52 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.51134 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.4933333333333335  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  46.20437977391451
printing an ep nov before normalisation:  37.35108987469542
actor:  1 policy actor:  1  step number:  56 total reward:  0.3133333333333327  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.51134 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.52466801337503
printing an ep nov before normalisation:  53.63170617096474
maxi score, test score, baseline:  -0.51134 0.6673333333333336 0.6673333333333336
maxi score, test score, baseline:  -0.51134 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.51134 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.062]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.062]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]]
maxi score, test score, baseline:  -0.51134 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.20439660549164
Printing some Q and Qe and total Qs values:  [[0.398]
 [0.438]
 [0.398]
 [0.398]
 [0.398]
 [0.398]
 [0.398]] [[36.693]
 [34.382]
 [36.693]
 [36.693]
 [36.693]
 [36.693]
 [36.693]] [[1.234]
 [1.166]
 [1.234]
 [1.234]
 [1.234]
 [1.234]
 [1.234]]
printing an ep nov before normalisation:  30.550919000576528
maxi score, test score, baseline:  -0.51134 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.51134 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.51134 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.51134 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.29333333333333267  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.51134 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.5466666666666667  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.11 ]
 [0.113]
 [0.11 ]
 [0.11 ]
 [0.117]
 [0.03 ]
 [0.11 ]] [[29.114]
 [43.002]
 [29.114]
 [29.114]
 [37.894]
 [29.87 ]
 [29.114]] [[0.654]
 [1.118]
 [0.654]
 [0.654]
 [0.952]
 [0.599]
 [0.654]]
actions average: 
K:  1  action  0 :  tensor([0.6893, 0.0382, 0.0539, 0.0532, 0.0621, 0.0579, 0.0453],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0088, 0.9140, 0.0161, 0.0136, 0.0035, 0.0036, 0.0404],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2362, 0.0039, 0.1849, 0.1354, 0.1246, 0.2152, 0.0998],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1792, 0.0390, 0.1281, 0.2808, 0.1179, 0.1385, 0.1165],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1968, 0.0094, 0.0947, 0.1538, 0.3516, 0.1058, 0.0879],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1890, 0.1112, 0.2196, 0.0823, 0.0739, 0.1777, 0.1463],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1955, 0.0568, 0.1404, 0.1166, 0.1076, 0.1047, 0.2784],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  28.02710643079045
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]] [[53.871]
 [52.991]
 [52.991]
 [52.991]
 [52.991]
 [52.991]
 [52.991]] [[1.954]
 [1.955]
 [1.955]
 [1.955]
 [1.955]
 [1.955]
 [1.955]]
maxi score, test score, baseline:  -0.51134 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  1.0
siam score:  -0.86684394
maxi score, test score, baseline:  -0.51134 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
line 256 mcts: sample exp_bonus 34.73611410785593
maxi score, test score, baseline:  -0.51134 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.4399999999999995  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.297]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]] [[30.252]
 [23.997]
 [23.997]
 [23.997]
 [23.997]
 [23.997]
 [23.997]] [[1.805]
 [1.432]
 [1.432]
 [1.432]
 [1.432]
 [1.432]
 [1.432]]
maxi score, test score, baseline:  -0.51134 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 34.43803787231445
actor:  1 policy actor:  1  step number:  53 total reward:  0.18666666666666631  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 39.91341173298071
Printing some Q and Qe and total Qs values:  [[-0.055]
 [-0.055]
 [-0.055]
 [-0.055]
 [-0.055]
 [-0.055]
 [-0.055]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.055]
 [-0.055]
 [-0.055]
 [-0.055]
 [-0.055]
 [-0.055]
 [-0.055]]
printing an ep nov before normalisation:  27.9385463756657
maxi score, test score, baseline:  -0.51134 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.2504441680196
printing an ep nov before normalisation:  57.27697737409252
actor:  1 policy actor:  1  step number:  38 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.51134 0.6673333333333336 0.6673333333333336
maxi score, test score, baseline:  -0.51134 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  68 total reward:  0.01999999999999935  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  52.51882298355972
maxi score, test score, baseline:  -0.51134 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.12634807162814
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.524]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]] [[27.515]
 [35.238]
 [26.327]
 [26.327]
 [26.327]
 [26.327]
 [26.327]] [[1.508]
 [1.97 ]
 [1.453]
 [1.453]
 [1.453]
 [1.453]
 [1.453]]
maxi score, test score, baseline:  -0.51134 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.51134 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.35851487058842
actor:  1 policy actor:  1  step number:  47 total reward:  0.3066666666666664  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  33.766481185405496
printing an ep nov before normalisation:  37.556400084111345
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.677]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]] [[30.038]
 [33.734]
 [30.038]
 [30.038]
 [30.038]
 [30.038]
 [30.038]] [[0.763]
 [0.955]
 [0.763]
 [0.763]
 [0.763]
 [0.763]
 [0.763]]
printing an ep nov before normalisation:  36.37493134418887
maxi score, test score, baseline:  -0.51134 0.6673333333333336 0.6673333333333336
maxi score, test score, baseline:  -0.51134 0.6673333333333336 0.6673333333333336
printing an ep nov before normalisation:  36.03422610206179
actor:  1 policy actor:  1  step number:  42 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  38.06661935281628
printing an ep nov before normalisation:  45.24494647979736
printing an ep nov before normalisation:  44.312017366799715
Printing some Q and Qe and total Qs values:  [[0.077]
 [0.313]
 [0.077]
 [0.077]
 [0.077]
 [0.077]
 [0.077]] [[42.223]
 [45.142]
 [42.223]
 [42.223]
 [42.223]
 [42.223]
 [42.223]] [[0.615]
 [0.919]
 [0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.615]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  40 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.20915291030568
maxi score, test score, baseline:  -0.5084200000000001 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.66736150852129
actor:  1 policy actor:  1  step number:  52 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.5084200000000001 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5084200000000001 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.833]
 [0.865]
 [0.833]
 [0.833]
 [0.833]
 [0.833]
 [0.833]] [[31.435]
 [41.747]
 [31.435]
 [31.435]
 [31.435]
 [31.435]
 [31.435]] [[1.024]
 [1.166]
 [1.024]
 [1.024]
 [1.024]
 [1.024]
 [1.024]]
maxi score, test score, baseline:  -0.5084200000000001 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.5216, 0.0366, 0.0767, 0.0975, 0.0980, 0.0979, 0.0717],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0315, 0.8741, 0.0168, 0.0294, 0.0105, 0.0039, 0.0340],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2491, 0.0143, 0.3359, 0.1023, 0.1042, 0.1064, 0.0878],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1310, 0.1083, 0.0623, 0.3939, 0.1308, 0.0883, 0.0854],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2009, 0.0592, 0.0909, 0.1111, 0.3374, 0.1176, 0.0828],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1536, 0.0134, 0.1423, 0.1347, 0.1142, 0.3529, 0.0889],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2173, 0.0090, 0.1256, 0.1603, 0.1973, 0.1470, 0.1435],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  62 total reward:  0.2066666666666659  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  70 total reward:  0.13999999999999924  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5084200000000001 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  0.4933333333333332  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.5084200000000001 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.311673171335837
maxi score, test score, baseline:  -0.5084200000000001 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.5084200000000001 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.087787628173828
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  58 total reward:  0.16666666666666585  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  59 total reward:  0.06666666666666587  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.5062866666666668 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.053]
 [0.249]
 [0.054]
 [0.053]
 [0.062]
 [0.054]
 [0.054]] [[43.754]
 [42.164]
 [37.25 ]
 [36.069]
 [39.055]
 [37.25 ]
 [37.25 ]] [[0.285]
 [0.467]
 [0.228]
 [0.216]
 [0.252]
 [0.228]
 [0.228]]
maxi score, test score, baseline:  -0.5062866666666668 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 62.27913948025565
printing an ep nov before normalisation:  41.75105624728732
maxi score, test score, baseline:  -0.5062866666666668 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.86704232915899
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.479]
 [0.386]
 [0.37 ]
 [0.351]
 [0.423]
 [0.346]] [[32.59 ]
 [30.975]
 [29.652]
 [31.385]
 [32.365]
 [31.552]
 [33.093]] [[0.602]
 [0.729]
 [0.613]
 [0.626]
 [0.623]
 [0.681]
 [0.63 ]]
Printing some Q and Qe and total Qs values:  [[0.256]
 [0.523]
 [0.308]
 [0.406]
 [0.287]
 [0.317]
 [0.435]] [[33.045]
 [35.085]
 [30.813]
 [30.177]
 [31.559]
 [33.583]
 [28.709]] [[0.434]
 [0.722]
 [0.462]
 [0.553]
 [0.449]
 [0.5  ]
 [0.567]]
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.456]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]] [[28.057]
 [32.541]
 [28.057]
 [28.057]
 [28.057]
 [28.057]
 [28.057]] [[1.17]
 [1.51]
 [1.17]
 [1.17]
 [1.17]
 [1.17]
 [1.17]]
maxi score, test score, baseline:  -0.5062866666666668 0.6673333333333336 0.6673333333333336
printing an ep nov before normalisation:  33.86880177034324
printing an ep nov before normalisation:  38.945368082261695
actor:  1 policy actor:  1  step number:  49 total reward:  0.42666666666666664  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  53.401252799308054
maxi score, test score, baseline:  -0.5062866666666668 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.38745976706492
actor:  1 policy actor:  1  step number:  54 total reward:  0.36666666666666614  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.265]
 [0.384]
 [0.277]
 [0.286]
 [0.282]
 [0.282]
 [0.387]] [[35.406]
 [32.348]
 [32.524]
 [33.151]
 [33.984]
 [34.709]
 [50.209]] [[0.932]
 [0.947]
 [0.846]
 [0.876]
 [0.901]
 [0.926]
 [1.556]]
printing an ep nov before normalisation:  42.694079107164704
Printing some Q and Qe and total Qs values:  [[0.604]
 [0.7  ]
 [0.589]
 [0.599]
 [0.62 ]
 [0.695]
 [0.619]] [[30.674]
 [34.605]
 [32.325]
 [31.51 ]
 [32.832]
 [37.385]
 [33.792]] [[0.604]
 [0.7  ]
 [0.589]
 [0.599]
 [0.62 ]
 [0.695]
 [0.619]]
siam score:  -0.87081736
Printing some Q and Qe and total Qs values:  [[0.338]
 [0.422]
 [0.311]
 [0.305]
 [0.309]
 [0.368]
 [0.338]] [[29.957]
 [31.398]
 [27.712]
 [27.141]
 [26.959]
 [30.489]
 [29.957]] [[0.631]
 [0.744]
 [0.559]
 [0.542]
 [0.541]
 [0.672]
 [0.631]]
printing an ep nov before normalisation:  30.310256395499362
actor:  1 policy actor:  1  step number:  80 total reward:  0.01999999999999924  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5062866666666668 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.33677275457048
maxi score, test score, baseline:  -0.5062866666666668 0.6673333333333336 0.6673333333333336
maxi score, test score, baseline:  -0.5062866666666668 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5062866666666668 0.6673333333333336 0.6673333333333336
maxi score, test score, baseline:  -0.5062866666666668 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.3178785241757
printing an ep nov before normalisation:  28.913213446047
actor:  0 policy actor:  0  step number:  44 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8703421
maxi score, test score, baseline:  -0.5035533333333334 0.6673333333333336 0.6673333333333336
actions average: 
K:  1  action  0 :  tensor([0.4689, 0.0435, 0.0980, 0.0912, 0.1216, 0.0874, 0.0893],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0208, 0.9228, 0.0096, 0.0132, 0.0079, 0.0061, 0.0196],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1780, 0.0946, 0.5663, 0.0288, 0.0209, 0.0541, 0.0573],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1429, 0.0164, 0.0869, 0.4504, 0.1223, 0.0901, 0.0910],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2648, 0.0050, 0.0710, 0.0922, 0.4068, 0.0870, 0.0732],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1952, 0.0038, 0.1295, 0.1194, 0.1444, 0.3118, 0.0959],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2503, 0.0795, 0.1041, 0.1017, 0.0925, 0.1258, 0.2461],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[-0.067]
 [-0.061]
 [-0.067]
 [-0.067]
 [-0.067]
 [-0.067]
 [-0.067]] [[14.376]
 [33.85 ]
 [14.376]
 [14.376]
 [14.376]
 [14.376]
 [14.376]] [[0.012]
 [0.221]
 [0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.012]]
maxi score, test score, baseline:  -0.5035533333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5035533333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.19041524844294
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5035533333333334 0.6673333333333336 0.6673333333333336
maxi score, test score, baseline:  -0.5035533333333334 0.6673333333333336 0.6673333333333336
siam score:  -0.8718898
actions average: 
K:  0  action  0 :  tensor([0.6512, 0.0159, 0.0615, 0.0562, 0.1120, 0.0526, 0.0506],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0076,     0.9750,     0.0019,     0.0033,     0.0007,     0.0009,
            0.0105], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2302, 0.1096, 0.3257, 0.0564, 0.0513, 0.0620, 0.1649],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1882, 0.1325, 0.0996, 0.2475, 0.1048, 0.1046, 0.1228],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1629, 0.0018, 0.0923, 0.1160, 0.4567, 0.0877, 0.0826],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1847, 0.0060, 0.1570, 0.0808, 0.0886, 0.4125, 0.0705],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2554, 0.1325, 0.1198, 0.1059, 0.1033, 0.0869, 0.1962],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.574]
 [0.471]
 [0.468]
 [0.471]
 [0.471]
 [0.467]] [[35.166]
 [30.889]
 [34.406]
 [36.432]
 [34.406]
 [34.406]
 [30.246]] [[0.795]
 [0.824]
 [0.773]
 [0.801]
 [0.773]
 [0.773]
 [0.706]]
maxi score, test score, baseline:  -0.5035533333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5035533333333334 0.6673333333333336 0.6673333333333336
maxi score, test score, baseline:  -0.5035533333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5035533333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  56 total reward:  0.4733333333333334  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5035533333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.66644987536998
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.529]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]] [[14.5  ]
 [22.924]
 [14.5  ]
 [14.5  ]
 [14.5  ]
 [14.5  ]
 [14.5  ]] [[0.605]
 [0.835]
 [0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]]
actor:  1 policy actor:  1  step number:  72 total reward:  0.019999999999998797  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5035533333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.87556916
maxi score, test score, baseline:  -0.5035533333333334 0.6673333333333336 0.6673333333333336
maxi score, test score, baseline:  -0.5035533333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.88380930471469
maxi score, test score, baseline:  -0.5035533333333334 0.6673333333333336 0.6673333333333336
actor:  1 policy actor:  1  step number:  34 total reward:  0.54  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  28.49931328726932
printing an ep nov before normalisation:  56.48656804924628
maxi score, test score, baseline:  -0.5035533333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5035533333333334 0.6673333333333336 0.6673333333333336
printing an ep nov before normalisation:  46.106651706649615
siam score:  -0.87708724
maxi score, test score, baseline:  -0.5035533333333334 0.6673333333333336 0.6673333333333336
printing an ep nov before normalisation:  48.79075978147746
maxi score, test score, baseline:  -0.5035533333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5035533333333334 0.6673333333333336 0.6673333333333336
printing an ep nov before normalisation:  53.53989599799035
maxi score, test score, baseline:  -0.5035533333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.5745, 0.0370, 0.0684, 0.0739, 0.0815, 0.0876, 0.0770],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0062, 0.9446, 0.0044, 0.0066, 0.0014, 0.0015, 0.0353],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1797, 0.0991, 0.3436, 0.0634, 0.0465, 0.1611, 0.1065],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2191, 0.0230, 0.1148, 0.2751, 0.1473, 0.1083, 0.1124],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.3501, 0.0147, 0.0795, 0.1169, 0.2457, 0.0875, 0.1057],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2259, 0.0224, 0.1085, 0.1933, 0.0949, 0.2400, 0.1150],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2133, 0.2894, 0.0762, 0.1039, 0.0689, 0.0721, 0.1762],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.285]
 [0.084]
 [0.084]
 [0.084]
 [0.094]
 [0.084]
 [0.084]] [[46.075]
 [38.173]
 [38.173]
 [38.173]
 [40.339]
 [38.173]
 [38.173]] [[0.932]
 [0.567]
 [0.567]
 [0.567]
 [0.622]
 [0.567]
 [0.567]]
maxi score, test score, baseline:  -0.5035533333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.108]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]] [[50.887]
 [51.083]
 [51.083]
 [51.083]
 [51.083]
 [51.083]
 [51.083]] [[2.608]
 [1.993]
 [1.993]
 [1.993]
 [1.993]
 [1.993]
 [1.993]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5035533333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.26596279219186
actions average: 
K:  4  action  0 :  tensor([0.6383, 0.0121, 0.0554, 0.0784, 0.0807, 0.0726, 0.0626],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0327, 0.9140, 0.0093, 0.0093, 0.0047, 0.0029, 0.0271],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2170, 0.0652, 0.2745, 0.1098, 0.1034, 0.1131, 0.1171],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0248, 0.0112, 0.0036, 0.5924, 0.3070, 0.0229, 0.0381],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2457, 0.1404, 0.0737, 0.1195, 0.1710, 0.1468, 0.1029],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1164, 0.1491, 0.1988, 0.1110, 0.0850, 0.2759, 0.0638],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2768, 0.0041, 0.2551, 0.1093, 0.0436, 0.0382, 0.2729],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  25.14625426932882
Printing some Q and Qe and total Qs values:  [[0.343]
 [0.41 ]
 [0.347]
 [0.357]
 [0.346]
 [0.391]
 [0.344]] [[28.073]
 [29.164]
 [25.255]
 [25.953]
 [27.074]
 [30.136]
 [26.915]] [[0.827]
 [0.927]
 [0.744]
 [0.775]
 [0.799]
 [0.938]
 [0.792]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5035533333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.271506786346436
actor:  1 policy actor:  1  step number:  59 total reward:  0.31999999999999906  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.5035533333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5035533333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.33619089310121
maxi score, test score, baseline:  -0.5035533333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.5739, 0.0316, 0.0678, 0.0716, 0.1053, 0.0700, 0.0799],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0130, 0.9461, 0.0094, 0.0093, 0.0029, 0.0040, 0.0152],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1628, 0.0426, 0.4442, 0.0906, 0.0673, 0.0994, 0.0931],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2638, 0.0041, 0.1354, 0.3159, 0.0935, 0.0836, 0.1037],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.3320, 0.0044, 0.0891, 0.1022, 0.2561, 0.1033, 0.1129],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1808, 0.1392, 0.1298, 0.0720, 0.0959, 0.3225, 0.0598],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2004, 0.1303, 0.1149, 0.1042, 0.1154, 0.1228, 0.2121],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  27.656853199005127
printing an ep nov before normalisation:  71.7895904591133
actor:  1 policy actor:  1  step number:  56 total reward:  0.24666666666666626  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5035533333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.00022157403520850494
actor:  1 policy actor:  1  step number:  58 total reward:  0.16666666666666574  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.096]
 [0.224]
 [0.1  ]
 [0.092]
 [0.092]
 [0.092]
 [0.1  ]] [[34.983]
 [38.974]
 [33.991]
 [30.603]
 [30.736]
 [30.887]
 [30.528]] [[0.259]
 [0.426]
 [0.254]
 [0.213]
 [0.214]
 [0.216]
 [0.22 ]]
maxi score, test score, baseline:  -0.5035533333333334 0.6673333333333336 0.6673333333333336
maxi score, test score, baseline:  -0.5035533333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.652467822950854
maxi score, test score, baseline:  -0.5035533333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.545410259158746
actor:  1 policy actor:  1  step number:  56 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5035533333333334 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.17458624302632
Printing some Q and Qe and total Qs values:  [[0.39]
 [0.39]
 [0.39]
 [0.39]
 [0.39]
 [0.39]
 [0.39]] [[36.68]
 [36.68]
 [36.68]
 [36.68]
 [36.68]
 [36.68]
 [36.68]] [[2.24]
 [2.24]
 [2.24]
 [2.24]
 [2.24]
 [2.24]
 [2.24]]
actor:  0 policy actor:  0  step number:  51 total reward:  0.17333333333333278  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.9121783623525
maxi score, test score, baseline:  -0.5012066666666667 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5012066666666667 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.46610917955181
maxi score, test score, baseline:  -0.5012066666666667 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.042]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]] [[27.827]
 [25.077]
 [25.077]
 [25.077]
 [25.077]
 [25.077]
 [25.077]] [[1.094]
 [0.928]
 [0.928]
 [0.928]
 [0.928]
 [0.928]
 [0.928]]
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]] [[36.878]
 [27.028]
 [27.028]
 [27.028]
 [27.028]
 [27.028]
 [27.028]] [[1.87 ]
 [1.634]
 [1.634]
 [1.634]
 [1.634]
 [1.634]
 [1.634]]
actor:  1 policy actor:  1  step number:  62 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  59.752015826822614
maxi score, test score, baseline:  -0.5012066666666667 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.48250783886497
maxi score, test score, baseline:  -0.5012066666666667 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.84481716156006
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.652]
 [0.652]
 [0.679]
 [0.652]
 [0.652]
 [0.652]] [[40.049]
 [40.049]
 [40.049]
 [38.646]
 [40.049]
 [40.049]
 [40.049]] [[0.954]
 [0.954]
 [0.954]
 [0.964]
 [0.954]
 [0.954]
 [0.954]]
printing an ep nov before normalisation:  30.89214563369751
printing an ep nov before normalisation:  31.048251304512853
actor:  1 policy actor:  1  step number:  71 total reward:  0.06666666666666576  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  58 total reward:  0.21999999999999975  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5012066666666667 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5012066666666667 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.5130, 0.0702, 0.0558, 0.0833, 0.1157, 0.1020, 0.0600],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0322, 0.9167, 0.0113, 0.0082, 0.0073, 0.0116, 0.0126],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1326, 0.0198, 0.4489, 0.0942, 0.0892, 0.1288, 0.0865],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1426, 0.0656, 0.0599, 0.4392, 0.1063, 0.1206, 0.0658],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2978, 0.0055, 0.0651, 0.0874, 0.3682, 0.1080, 0.0680],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1798, 0.0304, 0.1733, 0.0853, 0.0728, 0.3937, 0.0647],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.3452, 0.0031, 0.1008, 0.1076, 0.1180, 0.1241, 0.2012],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  43 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  43.68122326885492
siam score:  -0.8836435
printing an ep nov before normalisation:  49.853599312346546
maxi score, test score, baseline:  -0.4983800000000001 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4983800000000001 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.463890075683594
maxi score, test score, baseline:  -0.4983800000000001 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  41 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.49542 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.49542 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.49542 0.6673333333333336 0.6673333333333336
maxi score, test score, baseline:  -0.49542 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.49542 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.5176, 0.0385, 0.0825, 0.0933, 0.0790, 0.0919, 0.0972],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0156, 0.8951, 0.0213, 0.0176, 0.0056, 0.0064, 0.0383],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2258, 0.0129, 0.2271, 0.1331, 0.0962, 0.1821, 0.1227],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2911, 0.0092, 0.0830, 0.2764, 0.0761, 0.1747, 0.0895],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2023, 0.0071, 0.0914, 0.1187, 0.3638, 0.1153, 0.1015],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2909, 0.0391, 0.0854, 0.1312, 0.1223, 0.2263, 0.1048],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.3821, 0.0039, 0.1012, 0.1486, 0.1233, 0.1059, 0.1350],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  44 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.49542 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.88232255
printing an ep nov before normalisation:  32.28015937294216
printing an ep nov before normalisation:  28.33958714731521
Printing some Q and Qe and total Qs values:  [[0.767]
 [0.77 ]
 [0.77 ]
 [0.769]
 [0.769]
 [0.77 ]
 [0.769]] [[3.584]
 [4.945]
 [5.406]
 [5.886]
 [5.982]
 [7.971]
 [3.569]] [[0.862]
 [0.901]
 [0.914]
 [0.926]
 [0.928]
 [0.982]
 [0.864]]
actor:  1 policy actor:  1  step number:  56 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.49542 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.505675920958225
printing an ep nov before normalisation:  28.42909741610942
maxi score, test score, baseline:  -0.49542 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  55 total reward:  0.22666666666666635  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  40.29096602078547
actor:  1 policy actor:  1  step number:  36 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  60.983652186005294
printing an ep nov before normalisation:  43.936105892719645
Printing some Q and Qe and total Qs values:  [[0.832]
 [0.893]
 [0.832]
 [0.858]
 [0.832]
 [0.827]
 [0.857]] [[26.512]
 [36.255]
 [26.766]
 [31.717]
 [21.341]
 [23.581]
 [27.393]] [[0.832]
 [0.893]
 [0.832]
 [0.858]
 [0.832]
 [0.827]
 [0.857]]
actor:  0 policy actor:  0  step number:  42 total reward:  0.4999999999999999  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  61.04535849967201
maxi score, test score, baseline:  -0.4899666666666668 0.6673333333333336 0.6673333333333336
maxi score, test score, baseline:  -0.4899666666666668 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4899666666666668 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.551 0.061 0.184 0.143 0.041 0.02  0.   ]
maxi score, test score, baseline:  -0.4899666666666668 0.6673333333333336 0.6673333333333336
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.4899666666666668 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.4816, 0.0955, 0.0875, 0.0935, 0.0855, 0.0792, 0.0771],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0229, 0.9132, 0.0090, 0.0275, 0.0060, 0.0032, 0.0183],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1040, 0.0804, 0.5233, 0.0651, 0.0641, 0.0846, 0.0785],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2755, 0.0023, 0.1028, 0.2640, 0.1282, 0.1098, 0.1174],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1608, 0.1308, 0.0423, 0.0803, 0.4884, 0.0492, 0.0483],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1955, 0.0095, 0.1460, 0.0940, 0.1124, 0.3576, 0.0850],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2237, 0.0093, 0.0764, 0.1360, 0.1048, 0.1219, 0.3279],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.4899666666666668 0.6673333333333336 0.6673333333333336
maxi score, test score, baseline:  -0.4899666666666668 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.5000000000000001  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  52 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.4899666666666668 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4899666666666668 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.254]
 [0.411]
 [0.254]
 [0.254]
 [0.254]
 [0.254]
 [0.254]] [[54.479]
 [59.202]
 [54.479]
 [54.479]
 [54.479]
 [54.479]
 [54.479]] [[1.769]
 [2.101]
 [1.769]
 [1.769]
 [1.769]
 [1.769]
 [1.769]]
actor:  0 policy actor:  0  step number:  61 total reward:  0.0799999999999993  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.034]
 [-0.057]
 [-0.057]
 [-0.057]
 [-0.057]
 [-0.057]
 [-0.057]] [[31.084]
 [20.56 ]
 [20.56 ]
 [20.56 ]
 [20.56 ]
 [20.56 ]
 [20.56 ]] [[1.245]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]]
maxi score, test score, baseline:  -0.4878066666666668 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.23960756905948
siam score:  -0.8752882
printing an ep nov before normalisation:  40.76562716305316
Starting evaluation
Printing some Q and Qe and total Qs values:  [[-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]]
maxi score, test score, baseline:  -0.4878066666666668 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  67.6251384447518
printing an ep nov before normalisation:  40.599877569297405
printing an ep nov before normalisation:  42.36048868510423
siam score:  -0.8746997
maxi score, test score, baseline:  -0.4878066666666668 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.56016646861211
printing an ep nov before normalisation:  53.1156175650644
Printing some Q and Qe and total Qs values:  [[0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]] [[54.616]
 [54.616]
 [54.616]
 [54.616]
 [54.616]
 [54.616]
 [54.616]] [[0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]]
maxi score, test score, baseline:  -0.4878066666666668 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.21756996871776
line 256 mcts: sample exp_bonus 43.59517645674404
printing an ep nov before normalisation:  45.5093187679966
maxi score, test score, baseline:  -0.4878066666666668 0.6673333333333336 0.6673333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.406371464917186
Printing some Q and Qe and total Qs values:  [[0.774]
 [0.91 ]
 [0.798]
 [0.78 ]
 [0.826]
 [0.826]
 [0.789]] [[32.05 ]
 [34.274]
 [32.927]
 [35.063]
 [26.2  ]
 [26.2  ]
 [31.073]] [[0.774]
 [0.91 ]
 [0.798]
 [0.78 ]
 [0.826]
 [0.826]
 [0.789]]
Printing some Q and Qe and total Qs values:  [[0.963]
 [1.01 ]
 [0.963]
 [0.963]
 [0.963]
 [0.963]
 [0.963]] [[41.033]
 [49.971]
 [41.033]
 [41.033]
 [41.033]
 [41.033]
 [41.033]] [[0.963]
 [1.01 ]
 [0.963]
 [0.963]
 [0.963]
 [0.963]
 [0.963]]
Printing some Q and Qe and total Qs values:  [[0.857]
 [0.97 ]
 [0.875]
 [0.875]
 [0.875]
 [0.875]
 [0.875]] [[31.965]
 [44.736]
 [26.613]
 [26.613]
 [26.613]
 [26.613]
 [26.613]] [[0.857]
 [0.97 ]
 [0.875]
 [0.875]
 [0.875]
 [0.875]
 [0.875]]
Printing some Q and Qe and total Qs values:  [[1.027]
 [1.027]
 [1.018]
 [1.018]
 [1.018]
 [1.018]
 [1.018]] [[37.283]
 [37.963]
 [35.684]
 [35.684]
 [35.684]
 [35.684]
 [35.684]] [[1.027]
 [1.027]
 [1.018]
 [1.018]
 [1.018]
 [1.018]
 [1.018]]
printing an ep nov before normalisation:  37.17557105346016
printing an ep nov before normalisation:  57.21202301882155
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333333  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6533333333333333  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  28 total reward:  0.6333333333333333  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  28 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  49.18569098190865
siam score:  -0.8746693
maxi score, test score, baseline:  -0.42059333333333343 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.42059333333333343 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.42059333333333343 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.42059333333333343 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  33 total reward:  0.6533333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.42059333333333343 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.132850646972656
Printing some Q and Qe and total Qs values:  [[-0.052]
 [-0.061]
 [-0.03 ]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]] [[56.385]
 [48.514]
 [53.03 ]
 [48.514]
 [48.514]
 [48.514]
 [48.514]] [[0.554]
 [0.425]
 [0.525]
 [0.425]
 [0.425]
 [0.425]
 [0.425]]
Printing some Q and Qe and total Qs values:  [[0.351]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.356]
 [0.501]] [[42.2  ]
 [40.826]
 [40.826]
 [40.826]
 [40.826]
 [41.375]
 [40.826]] [[2.17 ]
 [2.22 ]
 [2.22 ]
 [2.22 ]
 [2.22 ]
 [2.114]
 [2.22 ]]
maxi score, test score, baseline:  -0.42059333333333343 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.79946460370367
actor:  1 policy actor:  1  step number:  46 total reward:  0.31333333333333313  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  23.31012872169338
maxi score, test score, baseline:  -0.42059333333333343 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.42059333333333343 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  0.56  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.42059333333333343 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.42059333333333343 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.08111572265625
printing an ep nov before normalisation:  37.76709464268649
line 256 mcts: sample exp_bonus 48.74233412876175
Printing some Q and Qe and total Qs values:  [[0.281]
 [0.507]
 [0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]] [[56.991]
 [51.873]
 [54.515]
 [54.515]
 [54.515]
 [54.515]
 [54.515]] [[1.475]
 [1.503]
 [1.461]
 [1.461]
 [1.461]
 [1.461]
 [1.461]]
printing an ep nov before normalisation:  54.69470992430967
printing an ep nov before normalisation:  32.2947096824646
actor:  0 policy actor:  0  step number:  39 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  46.4183217732272
actor:  1 policy actor:  1  step number:  53 total reward:  0.4266666666666661  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.41768666666666676 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.042]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]] [[67.583]
 [67.583]
 [67.583]
 [67.583]
 [67.583]
 [67.583]
 [67.583]] [[0.98]
 [0.98]
 [0.98]
 [0.98]
 [0.98]
 [0.98]
 [0.98]]
Printing some Q and Qe and total Qs values:  [[0.067]
 [0.283]
 [0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.067]] [[36.738]
 [38.606]
 [36.738]
 [36.738]
 [36.738]
 [36.738]
 [36.738]] [[0.964]
 [1.268]
 [0.964]
 [0.964]
 [0.964]
 [0.964]
 [0.964]]
maxi score, test score, baseline:  -0.41768666666666676 0.6803333333333333 0.6803333333333333
printing an ep nov before normalisation:  48.0213915083946
siam score:  -0.87704635
printing an ep nov before normalisation:  35.24632033608426
actor:  1 policy actor:  1  step number:  58 total reward:  0.2599999999999991  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  38.37025451092862
maxi score, test score, baseline:  -0.41768666666666676 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.4466666666666663  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 51.01818820354946
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.442]
 [0.287]
 [0.326]
 [0.326]
 [0.326]
 [0.326]] [[43.73 ]
 [47.737]
 [50.249]
 [43.73 ]
 [43.73 ]
 [43.73 ]
 [43.73 ]] [[1.01 ]
 [1.259]
 [1.187]
 [1.01 ]
 [1.01 ]
 [1.01 ]
 [1.01 ]]
maxi score, test score, baseline:  -0.41768666666666676 0.6803333333333333 0.6803333333333333
maxi score, test score, baseline:  -0.41768666666666676 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.07329719415128
maxi score, test score, baseline:  -0.41768666666666676 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.376]
 [0.475]
 [0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.376]] [[36.827]
 [40.73 ]
 [36.827]
 [36.827]
 [36.827]
 [36.827]
 [36.827]] [[1.031]
 [1.265]
 [1.031]
 [1.031]
 [1.031]
 [1.031]
 [1.031]]
printing an ep nov before normalisation:  0.00444571789728343
line 256 mcts: sample exp_bonus 30.321391564621432
printing an ep nov before normalisation:  28.981801098183762
actor:  1 policy actor:  1  step number:  66 total reward:  0.3799999999999992  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  3  action  0 :  tensor([0.5879, 0.0568, 0.0592, 0.0502, 0.1008, 0.0811, 0.0640],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0124, 0.9391, 0.0133, 0.0081, 0.0017, 0.0074, 0.0180],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.2261, 0.2044, 0.1316, 0.1157, 0.1097, 0.1064, 0.1062],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1321, 0.2601, 0.1230, 0.2207, 0.0575, 0.0916, 0.1150],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.3347, 0.0387, 0.0966, 0.0787, 0.2587, 0.0733, 0.1193],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1629, 0.0528, 0.1245, 0.1278, 0.1013, 0.3450, 0.0857],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2356, 0.0975, 0.1326, 0.1290, 0.1362, 0.1123, 0.1568],
       grad_fn=<DivBackward0>)
actions average: 
K:  1  action  0 :  tensor([0.5469, 0.0336, 0.0812, 0.0846, 0.0866, 0.0827, 0.0843],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0037,     0.9687,     0.0104,     0.0025,     0.0005,     0.0012,
            0.0129], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2561, 0.0024, 0.2301, 0.1176, 0.1472, 0.1462, 0.1003],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2691, 0.0172, 0.0783, 0.3979, 0.0768, 0.0899, 0.0709],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1754, 0.0246, 0.0906, 0.1344, 0.3733, 0.1179, 0.0838],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2386, 0.0063, 0.1298, 0.1226, 0.1306, 0.2670, 0.1051],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2788, 0.0419, 0.1275, 0.1536, 0.1315, 0.1539, 0.1127],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  45.26894372994644
actor:  1 policy actor:  1  step number:  62 total reward:  0.39333333333333287  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.92961084587029
maxi score, test score, baseline:  -0.41768666666666676 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.41768666666666676 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 27.676796142397873
maxi score, test score, baseline:  -0.41768666666666676 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  46 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  27.442632850139365
actor:  1 policy actor:  1  step number:  60 total reward:  0.3133333333333328  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  47.596676561105475
siam score:  -0.87309295
Printing some Q and Qe and total Qs values:  [[-0.016]
 [-0.002]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]] [[58.76 ]
 [56.721]
 [56.559]
 [56.559]
 [56.559]
 [56.559]
 [56.559]] [[0.892]
 [0.845]
 [0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.816]]
printing an ep nov before normalisation:  61.63935833946083
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.41516666666666674 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.41516666666666674 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8737532
maxi score, test score, baseline:  -0.41516666666666674 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  47 total reward:  0.5200000000000002  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  57.293471505557775
maxi score, test score, baseline:  -0.41516666666666674 0.6803333333333333 0.6803333333333333
line 256 mcts: sample exp_bonus 0.0
siam score:  -0.8764174
printing an ep nov before normalisation:  41.11147263491905
maxi score, test score, baseline:  -0.41516666666666674 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.41516666666666674 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.41516666666666674 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.613]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]] [[25.222]
 [27.671]
 [25.222]
 [25.222]
 [25.222]
 [25.222]
 [25.222]] [[1.037]
 [1.135]
 [1.037]
 [1.037]
 [1.037]
 [1.037]
 [1.037]]
maxi score, test score, baseline:  -0.41516666666666674 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.913]
 [0.913]
 [0.913]
 [0.913]
 [0.913]
 [0.913]
 [0.913]] [[40.174]
 [40.174]
 [40.174]
 [40.174]
 [40.174]
 [40.174]
 [40.174]] [[0.913]
 [0.913]
 [0.913]
 [0.913]
 [0.913]
 [0.913]
 [0.913]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  51 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.41516666666666674 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.119]
 [ 0.187]
 [ 0.119]
 [ 0.119]
 [-0.01 ]
 [-0.006]
 [ 0.119]] [[32.137]
 [35.998]
 [32.137]
 [32.137]
 [31.502]
 [31.768]
 [32.137]] [[0.964]
 [1.26 ]
 [0.964]
 [0.964]
 [0.797]
 [0.816]
 [0.964]]
Printing some Q and Qe and total Qs values:  [[0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]] [[34.966]
 [34.966]
 [34.966]
 [34.966]
 [34.966]
 [34.966]
 [34.966]] [[0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]]
actor:  0 policy actor:  0  step number:  50 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  33.562556179347816
printing an ep nov before normalisation:  27.83703796988644
printing an ep nov before normalisation:  33.632798194885254
printing an ep nov before normalisation:  47.038331489966225
actor:  1 policy actor:  1  step number:  45 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.41246000000000005 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.41246000000000005 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.729263363229826
actor:  1 policy actor:  1  step number:  50 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.41246000000000005 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.41246000000000005 0.6803333333333333 0.6803333333333333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.41246000000000005 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 50.16684211503858
actor:  1 policy actor:  1  step number:  59 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.41246000000000005 0.6803333333333333 0.6803333333333333
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.461]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]] [[30.359]
 [40.58 ]
 [30.359]
 [30.359]
 [30.359]
 [30.359]
 [30.359]] [[1.413]
 [1.89 ]
 [1.413]
 [1.413]
 [1.413]
 [1.413]
 [1.413]]
siam score:  -0.87528384
maxi score, test score, baseline:  -0.41246000000000005 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.69 ]
 [0.696]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]] [[35.318]
 [45.493]
 [35.318]
 [35.318]
 [35.318]
 [35.318]
 [35.318]] [[1.101]
 [1.282]
 [1.101]
 [1.101]
 [1.101]
 [1.101]
 [1.101]]
printing an ep nov before normalisation:  38.70211698761554
maxi score, test score, baseline:  -0.41246000000000005 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.39333333333333265  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  73 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  49 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
siam score:  -0.8720087
maxi score, test score, baseline:  -0.4099000000000001 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.2778410418775
printing an ep nov before normalisation:  27.3709080343091
maxi score, test score, baseline:  -0.4099000000000001 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.955906867980957
actor:  1 policy actor:  1  step number:  46 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  32.48343229293823
actor:  1 policy actor:  1  step number:  55 total reward:  0.38666666666666594  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.4099000000000001 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.198818877485635
maxi score, test score, baseline:  -0.4099000000000001 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4099000000000001 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4099000000000001 0.6803333333333333 0.6803333333333333
maxi score, test score, baseline:  -0.4099000000000001 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4099000000000001 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.42741490129729
printing an ep nov before normalisation:  30.65889918590706
maxi score, test score, baseline:  -0.4099000000000001 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.83720266815231
maxi score, test score, baseline:  -0.4099000000000001 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4099000000000001 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.34668135498041
actions average: 
K:  0  action  0 :  tensor([0.3826, 0.0242, 0.1336, 0.1202, 0.1237, 0.1141, 0.1015],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0072,     0.9519,     0.0172,     0.0087,     0.0006,     0.0016,
            0.0129], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1623, 0.0142, 0.5134, 0.0844, 0.0651, 0.0927, 0.0679],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2054, 0.1274, 0.0702, 0.3089, 0.0962, 0.0811, 0.1108],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.3596, 0.0018, 0.0797, 0.0775, 0.3514, 0.0663, 0.0637],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1777, 0.0064, 0.1590, 0.1418, 0.1275, 0.2771, 0.1104],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2783, 0.1136, 0.0999, 0.1256, 0.1170, 0.1083, 0.1572],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.4099000000000001 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.3533333333333327  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  55 total reward:  0.1999999999999994  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  35.79486385232867
printing an ep nov before normalisation:  51.860306833764525
maxi score, test score, baseline:  -0.4099000000000001 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.017]
 [-0.05 ]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.019]
 [-0.036]] [[64.267]
 [64.529]
 [65.577]
 [65.577]
 [65.577]
 [67.009]
 [65.577]] [[1.86 ]
 [1.838]
 [1.899]
 [1.899]
 [1.899]
 [1.981]
 [1.899]]
maxi score, test score, baseline:  -0.4099000000000001 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.1733333333333329  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.40990000000000004 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.57893466949463
maxi score, test score, baseline:  -0.40990000000000004 0.6803333333333333 0.6803333333333333
printing an ep nov before normalisation:  34.81446027755737
maxi score, test score, baseline:  -0.40990000000000004 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.12565296097382
maxi score, test score, baseline:  -0.40990000000000004 0.6803333333333333 0.6803333333333333
printing an ep nov before normalisation:  55.77119858858255
printing an ep nov before normalisation:  24.257993698120117
actor:  1 policy actor:  1  step number:  44 total reward:  0.54  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8792169
printing an ep nov before normalisation:  41.22585387305943
Printing some Q and Qe and total Qs values:  [[-0.021]
 [ 0.159]
 [-0.02 ]
 [-0.022]
 [-0.025]
 [-0.026]
 [-0.027]] [[28.456]
 [51.728]
 [28.727]
 [29.188]
 [29.283]
 [29.826]
 [29.725]] [[0.504]
 [1.528]
 [0.514]
 [0.529]
 [0.529]
 [0.548]
 [0.543]]
siam score:  -0.8782286
printing an ep nov before normalisation:  61.04362803935399
maxi score, test score, baseline:  -0.40990000000000004 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.40990000000000004 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]] [[46.396]
 [46.396]
 [46.396]
 [46.396]
 [46.396]
 [46.396]
 [46.396]] [[1.049]
 [1.049]
 [1.049]
 [1.049]
 [1.049]
 [1.049]
 [1.049]]
maxi score, test score, baseline:  -0.40990000000000004 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.5200, 0.0876, 0.0686, 0.0838, 0.0793, 0.0788, 0.0819],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0166, 0.9248, 0.0055, 0.0044, 0.0171, 0.0020, 0.0296],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.2541, 0.0010, 0.3297, 0.1139, 0.0926, 0.1337, 0.0750],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1144, 0.0332, 0.0742, 0.4060, 0.1194, 0.1146, 0.1381],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.3047, 0.0104, 0.0890, 0.1013, 0.2992, 0.1080, 0.0874],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1459, 0.0045, 0.1429, 0.1053, 0.0846, 0.4526, 0.0641],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2475, 0.0724, 0.1102, 0.1060, 0.1899, 0.0870, 0.1870],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
printing an ep nov before normalisation:  27.13024867888969
maxi score, test score, baseline:  -0.40990000000000004 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.518]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]] [[37.239]
 [33.742]
 [37.239]
 [37.239]
 [37.239]
 [37.239]
 [37.239]] [[0.895]
 [0.858]
 [0.895]
 [0.895]
 [0.895]
 [0.895]
 [0.895]]
printing an ep nov before normalisation:  58.514036940369536
actor:  1 policy actor:  1  step number:  52 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  29.99425343683795
siam score:  -0.8718506
maxi score, test score, baseline:  -0.40990000000000004 0.6803333333333333 0.6803333333333333
maxi score, test score, baseline:  -0.40990000000000004 0.6803333333333333 0.6803333333333333
Printing some Q and Qe and total Qs values:  [[0.182]
 [0.376]
 [0.164]
 [0.164]
 [0.21 ]
 [0.164]
 [0.164]] [[43.091]
 [50.672]
 [47.476]
 [47.476]
 [44.394]
 [47.476]
 [47.476]] [[1.064]
 [1.553]
 [1.217]
 [1.217]
 [1.143]
 [1.217]
 [1.217]]
actor:  1 policy actor:  1  step number:  59 total reward:  0.2533333333333324  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  34.69050554070303
siam score:  -0.8695084
Printing some Q and Qe and total Qs values:  [[0.353]
 [0.49 ]
 [0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.353]] [[28.427]
 [39.959]
 [28.427]
 [28.427]
 [28.427]
 [28.427]
 [28.427]] [[0.981]
 [1.616]
 [0.981]
 [0.981]
 [0.981]
 [0.981]
 [0.981]]
maxi score, test score, baseline:  -0.40990000000000004 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.00053558370759
actor:  1 policy actor:  1  step number:  42 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
actor:  0 policy actor:  0  step number:  44 total reward:  0.36666666666666614  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  44.462456425966565
Printing some Q and Qe and total Qs values:  [[-0.026]
 [-0.023]
 [-0.032]
 [-0.032]
 [-0.025]
 [-0.032]
 [-0.032]] [[70.056]
 [64.828]
 [69.276]
 [69.276]
 [68.455]
 [69.276]
 [69.276]] [[1.877]
 [1.737]
 [1.849]
 [1.849]
 [1.834]
 [1.849]
 [1.849]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  74 total reward:  0.09999999999999865  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.4071666666666668 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.04717263495075
actor:  0 policy actor:  0  step number:  57 total reward:  0.11999999999999944  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.40492666666666677 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.00047986308544523126
printing an ep nov before normalisation:  38.79908323121904
printing an ep nov before normalisation:  40.84397786477821
printing an ep nov before normalisation:  32.02406167984009
siam score:  -0.8681166
actor:  1 policy actor:  1  step number:  48 total reward:  0.5400000000000001  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.40492666666666677 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.40492666666666677 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.40929196265973
maxi score, test score, baseline:  -0.40492666666666677 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 47.74896403823841
actions average: 
K:  1  action  0 :  tensor([0.5746, 0.0053, 0.1034, 0.0877, 0.0802, 0.0716, 0.0772],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0061, 0.9573, 0.0049, 0.0089, 0.0018, 0.0013, 0.0196],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2472, 0.0118, 0.4445, 0.0597, 0.0599, 0.1240, 0.0529],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1783, 0.0136, 0.0622, 0.4493, 0.1554, 0.0900, 0.0513],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1884, 0.0055, 0.0560, 0.0544, 0.5737, 0.0789, 0.0430],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2438, 0.0109, 0.1328, 0.0954, 0.1328, 0.2829, 0.1014],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1931, 0.2518, 0.0680, 0.0948, 0.0741, 0.0686, 0.2497],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  25.725698471069336
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  31.5516002817477
printing an ep nov before normalisation:  44.724530000259314
actor:  1 policy actor:  1  step number:  38 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  43.68878470526801
actor:  1 policy actor:  1  step number:  68 total reward:  0.19333333333333258  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8707135
maxi score, test score, baseline:  -0.40492666666666677 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.196306536523565
maxi score, test score, baseline:  -0.40492666666666677 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.67831702840633
printing an ep nov before normalisation:  64.45202162950712
maxi score, test score, baseline:  -0.40492666666666677 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.40492666666666677 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.40492666666666677 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.40492666666666677 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0
printing an ep nov before normalisation:  67.51405992021114
printing an ep nov before normalisation:  62.551508050664204
actor:  1 policy actor:  1  step number:  59 total reward:  0.10666666666666624  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.228]
 [0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]] [[35.067]
 [37.766]
 [37.766]
 [37.766]
 [37.766]
 [37.766]
 [37.766]] [[1.895]
 [2.132]
 [2.132]
 [2.132]
 [2.132]
 [2.132]
 [2.132]]
maxi score, test score, baseline:  -0.40492666666666677 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.619484108132834
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  36.431778869182686
printing an ep nov before normalisation:  38.37237364168321
actor:  1 policy actor:  1  step number:  47 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  31.66682964240132
actions average: 
K:  2  action  0 :  tensor([0.5416, 0.0507, 0.0769, 0.0753, 0.1014, 0.0751, 0.0790],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0396, 0.8887, 0.0097, 0.0200, 0.0104, 0.0054, 0.0262],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2415, 0.0011, 0.4280, 0.0782, 0.0893, 0.0896, 0.0724],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1591, 0.1653, 0.0506, 0.4792, 0.0347, 0.0364, 0.0747],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.3783, 0.0166, 0.0821, 0.0932, 0.2557, 0.0890, 0.0851],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1781, 0.0045, 0.1072, 0.1191, 0.1381, 0.3644, 0.0885],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2987, 0.1272, 0.0722, 0.0922, 0.0989, 0.0971, 0.2138],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.40492666666666677 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  51 total reward:  0.23999999999999966  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  67.49128895018214
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.40244666666666673 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.47590732720105
maxi score, test score, baseline:  -0.40244666666666673 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.40244666666666673 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.40244666666666673 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.40244666666666673 0.6803333333333333 0.6803333333333333
maxi score, test score, baseline:  -0.40244666666666673 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.40244666666666673 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  63 total reward:  0.22666666666666613  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.522]
 [0.356]
 [0.382]
 [0.356]
 [0.356]
 [0.356]] [[31.435]
 [33.572]
 [33.764]
 [30.718]
 [33.764]
 [33.764]
 [33.764]] [[0.601]
 [0.738]
 [0.574]
 [0.568]
 [0.574]
 [0.574]
 [0.574]]
maxi score, test score, baseline:  -0.40244666666666673 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.91936971660544
actor:  1 policy actor:  1  step number:  61 total reward:  0.306666666666666  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  35.41723514789608
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.426]
 [0.374]
 [0.359]
 [0.374]
 [0.374]
 [0.362]] [[30.987]
 [30.366]
 [27.758]
 [25.574]
 [27.758]
 [27.758]
 [26.762]] [[0.605]
 [0.615]
 [0.536]
 [0.499]
 [0.536]
 [0.536]
 [0.513]]
printing an ep nov before normalisation:  30.399198393050614
maxi score, test score, baseline:  -0.40244666666666673 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.40244666666666673 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.39333333333333276  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  62 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  0.333
siam score:  -0.87584764
maxi score, test score, baseline:  -0.40244666666666673 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.503]
 [0.441]
 [0.449]
 [0.441]
 [0.484]
 [0.433]] [[35.906]
 [37.776]
 [39.463]
 [38.079]
 [39.266]
 [41.485]
 [39.182]] [[0.912]
 [1.004]
 [0.977]
 [0.956]
 [0.972]
 [1.061]
 [0.963]]
actions average: 
K:  2  action  0 :  tensor([0.5556, 0.0172, 0.0847, 0.0888, 0.0897, 0.0886, 0.0753],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0107, 0.9473, 0.0080, 0.0090, 0.0030, 0.0025, 0.0194],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2161, 0.0269, 0.3347, 0.1162, 0.0802, 0.1391, 0.0868],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1475, 0.0936, 0.0725, 0.4647, 0.0483, 0.0709, 0.1025],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2263, 0.0197, 0.1033, 0.1134, 0.3079, 0.1295, 0.0999],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2607, 0.0025, 0.2079, 0.0971, 0.0911, 0.2579, 0.0828],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1172, 0.0306, 0.2980, 0.1392, 0.1013, 0.1117, 0.2019],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.40244666666666673 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.40244666666666673 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  3.001680568104348e-05
actor:  0 policy actor:  0  step number:  51 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.679]
 [0.679]
 [0.679]
 [0.679]
 [0.679]
 [0.683]
 [0.679]] [[19.252]
 [19.252]
 [19.252]
 [19.252]
 [19.252]
 [19.567]
 [19.252]] [[0.679]
 [0.679]
 [0.679]
 [0.679]
 [0.679]
 [0.683]
 [0.679]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3998600000000001 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.75000831786869
Printing some Q and Qe and total Qs values:  [[0.85 ]
 [0.901]
 [0.829]
 [0.811]
 [0.878]
 [0.808]
 [0.858]] [[26.189]
 [28.731]
 [26.771]
 [20.968]
 [28.906]
 [21.167]
 [27.635]] [[0.85 ]
 [0.901]
 [0.829]
 [0.811]
 [0.878]
 [0.808]
 [0.858]]
maxi score, test score, baseline:  -0.3998600000000001 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.000897479330778
maxi score, test score, baseline:  -0.3998600000000001 0.6803333333333333 0.6803333333333333
actor:  0 policy actor:  0  step number:  63 total reward:  0.05333333333333257  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  72 total reward:  0.006666666666665599  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  40.176568881639476
printing an ep nov before normalisation:  45.127235078489875
maxi score, test score, baseline:  -0.3977533333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3977533333333334 0.6803333333333333 0.6803333333333333
maxi score, test score, baseline:  -0.3977533333333334 0.6803333333333333 0.6803333333333333
printing an ep nov before normalisation:  30.857953275707704
actor:  1 policy actor:  1  step number:  42 total reward:  0.48666666666666647  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.3977533333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.343]
 [0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]] [[59.947]
 [70.521]
 [59.947]
 [59.947]
 [59.947]
 [59.947]
 [59.947]] [[1.465]
 [1.782]
 [1.465]
 [1.465]
 [1.465]
 [1.465]
 [1.465]]
maxi score, test score, baseline:  -0.3977533333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.3399999999999993  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  4  action  0 :  tensor([0.5419, 0.0402, 0.0931, 0.0935, 0.0672, 0.0680, 0.0960],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0407, 0.8679, 0.0132, 0.0299, 0.0099, 0.0067, 0.0316],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1884, 0.0057, 0.3176, 0.1320, 0.1150, 0.1431, 0.0981],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.2206,     0.0005,     0.0462,     0.5565,     0.0421,     0.0529,
            0.0813], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2401, 0.0859, 0.1140, 0.1145, 0.2091, 0.0979, 0.1385],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2300, 0.0048, 0.3641, 0.1263, 0.0597, 0.0968, 0.1184],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2462, 0.1681, 0.1427, 0.0943, 0.0823, 0.0860, 0.1805],
       grad_fn=<DivBackward0>)
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
actor:  0 policy actor:  0  step number:  62 total reward:  0.04666666666666597  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  35.5484526390425
Printing some Q and Qe and total Qs values:  [[0.673]
 [0.723]
 [0.673]
 [0.673]
 [0.673]
 [0.673]
 [0.673]] [[25.64 ]
 [30.261]
 [25.64 ]
 [25.64 ]
 [25.64 ]
 [25.64 ]
 [25.64 ]] [[1.314]
 [1.567]
 [1.314]
 [1.314]
 [1.314]
 [1.314]
 [1.314]]
printing an ep nov before normalisation:  64.03881726934843
actor:  1 policy actor:  1  step number:  44 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  68.04998172278876
printing an ep nov before normalisation:  43.653984903049746
maxi score, test score, baseline:  -0.3956600000000001 0.6803333333333333 0.6803333333333333
maxi score, test score, baseline:  -0.3956600000000001 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3956600000000001 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.411783923448354
maxi score, test score, baseline:  -0.3956600000000001 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3956600000000001 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3956600000000001 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 36.940145376357485
actor:  1 policy actor:  1  step number:  56 total reward:  0.35333333333333283  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.3956600000000001 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.085138763371674
printing an ep nov before normalisation:  39.95475591988554
actor:  0 policy actor:  0  step number:  55 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 45.392200053758515
maxi score, test score, baseline:  -0.3930733333333335 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.899]
 [0.977]
 [0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]] [[27.946]
 [32.045]
 [27.946]
 [27.946]
 [27.946]
 [27.946]
 [27.946]] [[0.899]
 [0.977]
 [0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  52 total reward:  0.206666666666666  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  68.1093366668322
printing an ep nov before normalisation:  36.337442479804594
maxi score, test score, baseline:  -0.3906600000000001 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  14.068436622619629
printing an ep nov before normalisation:  58.54148638830452
printing an ep nov before normalisation:  42.953836062941726
printing an ep nov before normalisation:  33.7877997928845
printing an ep nov before normalisation:  32.47829484271678
printing an ep nov before normalisation:  57.5010511610243
printing an ep nov before normalisation:  23.873642306941473
Printing some Q and Qe and total Qs values:  [[-0.019]
 [ 0.06 ]
 [-0.024]
 [-0.025]
 [-0.018]
 [-0.007]
 [-0.025]] [[36.121]
 [35.106]
 [35.11 ]
 [35.652]
 [36.996]
 [36.436]
 [36.731]] [[0.905]
 [0.926]
 [0.843]
 [0.872]
 [0.954]
 [0.934]
 [0.933]]
printing an ep nov before normalisation:  36.256294472188486
Printing some Q and Qe and total Qs values:  [[-0.017]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.017]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]]
printing an ep nov before normalisation:  31.747271696059045
actor:  0 policy actor:  0  step number:  54 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.38824666666666674 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.38824666666666674 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]] [[44.898]
 [44.898]
 [44.898]
 [44.898]
 [44.898]
 [44.898]
 [44.898]] [[1.346]
 [1.346]
 [1.346]
 [1.346]
 [1.346]
 [1.346]
 [1.346]]
actor:  0 policy actor:  0  step number:  60 total reward:  0.13999999999999957  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  25.16130140811842
Printing some Q and Qe and total Qs values:  [[0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]] [[37.163]
 [37.163]
 [37.163]
 [37.163]
 [37.163]
 [37.163]
 [37.163]] [[0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]]
printing an ep nov before normalisation:  21.692113460369136
actor:  1 policy actor:  1  step number:  41 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  1.0
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.933]
 [0.974]
 [0.933]
 [0.933]
 [0.933]
 [0.933]
 [0.921]] [[29.438]
 [29.934]
 [29.438]
 [29.438]
 [29.438]
 [29.438]
 [28.896]] [[0.933]
 [0.974]
 [0.933]
 [0.933]
 [0.933]
 [0.933]
 [0.921]]
maxi score, test score, baseline:  -0.3859666666666668 0.6803333333333333 0.6803333333333333
printing an ep nov before normalisation:  37.57501788220326
printing an ep nov before normalisation:  41.690262249421615
actor:  0 policy actor:  0  step number:  35 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  3.9444864056008555
printing an ep nov before normalisation:  28.78757250858049
maxi score, test score, baseline:  -0.3829533333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.7633176176401
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.3829533333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.947055291431777
maxi score, test score, baseline:  -0.3829533333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  42 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  32.73687280129909
printing an ep nov before normalisation:  30.689767806140424
maxi score, test score, baseline:  -0.3801933333333335 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.81485142676787
maxi score, test score, baseline:  -0.3801933333333335 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.343]
 [0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]] [[28.695]
 [34.226]
 [28.695]
 [28.695]
 [28.695]
 [28.695]
 [28.695]] [[0.666]
 [0.84 ]
 [0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]]
printing an ep nov before normalisation:  28.10929587337941
actor:  1 policy actor:  1  step number:  46 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.3801933333333335 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  71 total reward:  0.21333333333333215  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3801933333333335 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.990167138838366
Printing some Q and Qe and total Qs values:  [[0.37 ]
 [0.395]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]] [[52.153]
 [56.861]
 [52.153]
 [52.153]
 [52.153]
 [52.153]
 [52.153]] [[0.606]
 [0.665]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]]
maxi score, test score, baseline:  -0.3801933333333335 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.013270378112793
printing an ep nov before normalisation:  47.06105807829512
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3801933333333335 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3801933333333335 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  44 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  0.333
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.3126],
        [0.5998],
        [0.5998],
        [0.7946],
        [-0.0000],
        [-0.0000],
        [0.0144],
        [0.0126],
        [0.1609],
        [0.7906]], dtype=torch.float64)
-0.032346567066 0.28021395655938913
-0.070771701198 0.5290721900259303
-0.070771701198 0.5290721900259303
-0.09703970119800001 0.6975881657113597
0.99 0.99
0.94099335 0.94099335
-0.045026434398 -0.030671634903242113
-0.09703970119800001 -0.08446553320290942
-0.071031754398 0.08983161515281318
-0.07116375439799999 0.7194689243069301
printing an ep nov before normalisation:  22.668959444515114
Printing some Q and Qe and total Qs values:  [[0.82 ]
 [0.859]
 [0.808]
 [0.816]
 [0.815]
 [0.814]
 [0.806]] [[27.9  ]
 [31.596]
 [30.281]
 [29.802]
 [29.591]
 [29.762]
 [30.605]] [[0.82 ]
 [0.859]
 [0.808]
 [0.816]
 [0.815]
 [0.814]
 [0.806]]
actor:  1 policy actor:  1  step number:  76 total reward:  0.04666666666666597  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.3801933333333335 0.6803333333333333 0.6803333333333333
Printing some Q and Qe and total Qs values:  [[0.772]
 [0.806]
 [0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.772]] [[33.688]
 [29.795]
 [33.688]
 [33.688]
 [33.688]
 [33.688]
 [33.688]] [[0.772]
 [0.806]
 [0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.772]]
printing an ep nov before normalisation:  21.09348773956299
printing an ep nov before normalisation:  27.904040813446045
printing an ep nov before normalisation:  30.24762745159485
actor:  0 policy actor:  0  step number:  52 total reward:  0.13999999999999946  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.3779133333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3779133333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  65 total reward:  0.21333333333333304  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3754866666666667 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.32213401314701
maxi score, test score, baseline:  -0.3754866666666667 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3754866666666667 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3754866666666667 0.6803333333333333 0.6803333333333333
printing an ep nov before normalisation:  31.23382568359375
printing an ep nov before normalisation:  35.31343410652119
maxi score, test score, baseline:  -0.3754866666666667 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.045]
 [0.179]
 [0.126]
 [0.04 ]
 [0.039]
 [0.039]
 [0.04 ]] [[27.095]
 [37.114]
 [35.404]
 [26.942]
 [27.147]
 [27.285]
 [27.23 ]] [[0.193]
 [0.555]
 [0.463]
 [0.185]
 [0.189]
 [0.191]
 [0.191]]
maxi score, test score, baseline:  -0.3754866666666667 0.6803333333333333 0.6803333333333333
Printing some Q and Qe and total Qs values:  [[0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]] [[26.266]
 [26.266]
 [26.266]
 [26.266]
 [26.266]
 [26.266]
 [26.266]] [[1.423]
 [1.423]
 [1.423]
 [1.423]
 [1.423]
 [1.423]
 [1.423]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.3754866666666667 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.33191490778229
maxi score, test score, baseline:  -0.3754866666666667 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.66 ]
 [0.725]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [0.66 ]] [[35.205]
 [38.69 ]
 [35.205]
 [35.205]
 [35.205]
 [35.205]
 [35.205]] [[0.891]
 [0.998]
 [0.891]
 [0.891]
 [0.891]
 [0.891]
 [0.891]]
maxi score, test score, baseline:  -0.3754866666666667 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.238478660583496
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.63 ]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]] [[27.206]
 [29.751]
 [27.206]
 [27.206]
 [27.206]
 [27.206]
 [27.206]] [[0.722]
 [0.841]
 [0.722]
 [0.722]
 [0.722]
 [0.722]
 [0.722]]
printing an ep nov before normalisation:  34.24167226888237
printing an ep nov before normalisation:  54.366747670040034
printing an ep nov before normalisation:  33.740962358586174
maxi score, test score, baseline:  -0.3754866666666667 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  37 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.3725266666666668 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.3725266666666668 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.3725266666666668 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.903842949857236
Printing some Q and Qe and total Qs values:  [[0.779]
 [0.865]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]] [[29.116]
 [33.309]
 [29.116]
 [29.116]
 [29.116]
 [29.116]
 [29.116]] [[0.779]
 [0.865]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]]
printing an ep nov before normalisation:  26.774428669909874
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3725266666666668 0.6803333333333333 0.6803333333333333
actor:  1 policy actor:  1  step number:  56 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.3725266666666668 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3725266666666668 0.6803333333333333 0.6803333333333333
maxi score, test score, baseline:  -0.3725266666666668 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.826343912440294
maxi score, test score, baseline:  -0.3725266666666668 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3725266666666668 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.02017545700073
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [-0.039]
 [ 0.249]
 [ 0.022]
 [-0.01 ]
 [ 0.11 ]
 [-0.054]] [[47.952]
 [44.502]
 [37.939]
 [38.557]
 [47.952]
 [37.551]
 [42.45 ]] [[1.657]
 [1.494]
 [1.528]
 [1.325]
 [1.657]
 [1.373]
 [1.399]]
siam score:  -0.8675049
printing an ep nov before normalisation:  45.68347609841856
actor:  0 policy actor:  0  step number:  53 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.36975333333333343 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.36975333333333343 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.36975333333333343 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  65 total reward:  0.18666666666666643  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.36975333333333343 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.36975333333333343 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.36975333333333343 0.6803333333333333 0.6803333333333333
actor:  1 policy actor:  1  step number:  42 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.36975333333333343 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.914718989969984
actor:  1 policy actor:  1  step number:  56 total reward:  0.12666666666666615  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.36975333333333343 0.6803333333333333 0.6803333333333333
printing an ep nov before normalisation:  30.883825486655983
maxi score, test score, baseline:  -0.36975333333333343 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.6020, 0.0389, 0.0707, 0.0808, 0.0709, 0.0694, 0.0673],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0060, 0.9634, 0.0036, 0.0137, 0.0016, 0.0010, 0.0106],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2069, 0.0110, 0.1978, 0.1732, 0.1224, 0.1442, 0.1446],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0832, 0.0257, 0.0604, 0.6313, 0.0666, 0.0525, 0.0803],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2229, 0.0264, 0.0740, 0.0962, 0.4405, 0.0742, 0.0658],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1784, 0.0458, 0.1226, 0.1469, 0.1076, 0.2945, 0.1041],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2599, 0.0832, 0.0877, 0.1173, 0.1016, 0.1132, 0.2372],
       grad_fn=<DivBackward0>)
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.36975333333333343 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  49 total reward:  0.46666666666666634  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  31.529845213196793
maxi score, test score, baseline:  -0.36975333333333343 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  56 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.3673933333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.270957528353204
maxi score, test score, baseline:  -0.3673933333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.076]
 [0.027]
 [0.028]
 [0.03 ]
 [0.019]
 [0.033]] [[28.465]
 [32.491]
 [22.693]
 [22.067]
 [24.358]
 [24.569]
 [25.971]] [[0.011]
 [0.076]
 [0.027]
 [0.028]
 [0.03 ]
 [0.019]
 [0.033]]
printing an ep nov before normalisation:  49.407821730354634
siam score:  -0.86767244
Printing some Q and Qe and total Qs values:  [[-0.057]
 [-0.04 ]
 [-0.022]
 [-0.045]
 [-0.171]
 [-0.042]
 [-0.045]] [[48.535]
 [55.434]
 [41.934]
 [46.257]
 [42.233]
 [32.867]
 [53.061]] [[ 0.142]
 [ 0.204]
 [ 0.133]
 [ 0.138]
 [-0.014]
 [ 0.053]
 [ 0.184]]
maxi score, test score, baseline:  -0.3673933333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3673933333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3673933333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.87391757965088
maxi score, test score, baseline:  -0.3673933333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.934]
 [0.985]
 [0.943]
 [0.947]
 [0.939]
 [0.918]
 [0.953]] [[26.557]
 [32.775]
 [30.432]
 [26.402]
 [27.016]
 [30.013]
 [25.955]] [[0.934]
 [0.985]
 [0.943]
 [0.947]
 [0.939]
 [0.918]
 [0.953]]
actor:  0 policy actor:  0  step number:  61 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]] [[24.738]
 [24.738]
 [24.738]
 [24.738]
 [24.738]
 [24.738]
 [24.738]] [[1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]]
maxi score, test score, baseline:  -0.3649133333333335 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  69 total reward:  0.013333333333332531  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.3628866666666668 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.106]
 [-0.008]
 [-0.142]
 [-0.023]
 [-0.164]
 [-0.189]
 [-0.203]] [[47.954]
 [45.864]
 [46.445]
 [48.43 ]
 [49.951]
 [46.169]
 [44.802]] [[1.115]
 [1.094]
 [0.993]
 [1.224]
 [1.17 ]
 [0.93 ]
 [0.838]]
actor:  1 policy actor:  1  step number:  37 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.623]
 [0.815]
 [0.76 ]
 [0.728]
 [0.646]
 [0.659]
 [0.755]] [[35.593]
 [35.046]
 [33.45 ]
 [36.195]
 [39.07 ]
 [36.949]
 [35.054]] [[0.623]
 [0.815]
 [0.76 ]
 [0.728]
 [0.646]
 [0.659]
 [0.755]]
maxi score, test score, baseline:  -0.3628866666666668 0.6803333333333333 0.6803333333333333
printing an ep nov before normalisation:  27.73883819580078
actor:  0 policy actor:  0  step number:  44 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.061]
 [-0.081]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.061]
 [-0.081]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]]
actor:  0 policy actor:  0  step number:  37 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.35716666666666674 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  1.5628124376161168
maxi score, test score, baseline:  -0.35716666666666674 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  15.550029277801514
maxi score, test score, baseline:  -0.35716666666666674 0.6803333333333333 0.6803333333333333
maxi score, test score, baseline:  -0.35716666666666674 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.356488345699624
maxi score, test score, baseline:  -0.35716666666666674 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  73 total reward:  0.2933333333333321  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.35716666666666674 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.85387616596582
printing an ep nov before normalisation:  29.302447047750107
printing an ep nov before normalisation:  32.83241298368606
maxi score, test score, baseline:  -0.35716666666666674 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.530838012695312
printing an ep nov before normalisation:  40.62559245085296
maxi score, test score, baseline:  -0.35716666666666674 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.35716666666666674 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.85607051849365
actor:  0 policy actor:  0  step number:  55 total reward:  0.22666666666666624  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.35471333333333344 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  45 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  27.277052213937313
maxi score, test score, baseline:  -0.35210000000000014 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.745]
 [0.754]
 [0.755]
 [0.753]
 [0.754]
 [0.724]
 [0.847]] [[23.43 ]
 [22.339]
 [28.108]
 [17.991]
 [26.096]
 [30.587]
 [25.764]] [[0.745]
 [0.754]
 [0.755]
 [0.753]
 [0.754]
 [0.724]
 [0.847]]
actor:  0 policy actor:  0  step number:  50 total reward:  0.24666666666666603  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  35.91870316560711
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.583]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.503]
 [0.49 ]] [[39.966]
 [33.454]
 [41.155]
 [41.155]
 [41.155]
 [41.615]
 [41.155]] [[1.126]
 [1.023]
 [1.144]
 [1.144]
 [1.144]
 [1.17 ]
 [1.144]]
maxi score, test score, baseline:  -0.3496066666666668 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  0.5666666666666667  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  28.83451406163997
maxi score, test score, baseline:  -0.3496066666666668 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.391857551670487
actions average: 
K:  4  action  0 :  tensor([0.6133, 0.0652, 0.0509, 0.0921, 0.0714, 0.0514, 0.0556],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0168, 0.9357, 0.0075, 0.0078, 0.0026, 0.0021, 0.0275],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.3548, 0.0162, 0.1375, 0.1319, 0.1069, 0.1181, 0.1344],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2677, 0.0723, 0.0980, 0.2514, 0.0728, 0.1095, 0.1282],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2133, 0.0105, 0.0844, 0.1818, 0.3079, 0.1097, 0.0923],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2976, 0.0905, 0.1041, 0.1085, 0.0855, 0.2212, 0.0926],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.3231, 0.0424, 0.1127, 0.1266, 0.1495, 0.1339, 0.1117],
       grad_fn=<DivBackward0>)
actions average: 
K:  2  action  0 :  tensor([0.5555, 0.0254, 0.0744, 0.0737, 0.0960, 0.0764, 0.0985],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0107, 0.9352, 0.0082, 0.0099, 0.0016, 0.0017, 0.0326],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1894, 0.0876, 0.2923, 0.1097, 0.0852, 0.1128, 0.1232],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2337, 0.1258, 0.0890, 0.2624, 0.0858, 0.1043, 0.0990],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2082, 0.0114, 0.1079, 0.1323, 0.3128, 0.1192, 0.1081],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1325, 0.0388, 0.2118, 0.0775, 0.0503, 0.4397, 0.0494],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0661, 0.1150, 0.1814, 0.0785, 0.0432, 0.0768, 0.4391],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.3496066666666668 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.271952516144562
maxi score, test score, baseline:  -0.3496066666666668 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.264922027350718
actor:  1 policy actor:  1  step number:  53 total reward:  0.2533333333333331  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3496066666666668 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3496066666666668 0.6803333333333333 0.6803333333333333
maxi score, test score, baseline:  -0.3496066666666668 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.27333333333333276  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  26.452268164584467
Printing some Q and Qe and total Qs values:  [[0.113]
 [0.187]
 [0.031]
 [0.031]
 [0.031]
 [0.031]
 [0.031]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.113]
 [0.187]
 [0.031]
 [0.031]
 [0.031]
 [0.031]
 [0.031]]
printing an ep nov before normalisation:  41.20611509902363
maxi score, test score, baseline:  -0.3496066666666668 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4333333333333329  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3496066666666668 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]] [[39.327]
 [39.327]
 [39.327]
 [39.327]
 [39.327]
 [39.327]
 [39.327]] [[1.427]
 [1.427]
 [1.427]
 [1.427]
 [1.427]
 [1.427]
 [1.427]]
maxi score, test score, baseline:  -0.3496066666666668 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3496066666666668 0.6803333333333333 0.6803333333333333
maxi score, test score, baseline:  -0.3496066666666668 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.678024744366667
maxi score, test score, baseline:  -0.3496066666666668 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.3866666666666666  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.34960666666666673 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.34960666666666673 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.637099287596506
printing an ep nov before normalisation:  16.07591184697354
maxi score, test score, baseline:  -0.34960666666666673 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.258]
 [0.384]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]] [[44.216]
 [44.771]
 [44.216]
 [44.216]
 [44.216]
 [44.216]
 [44.216]] [[0.49 ]
 [0.621]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]]
maxi score, test score, baseline:  -0.34960666666666673 0.6803333333333333 0.6803333333333333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  36.543857588507464
actor:  1 policy actor:  1  step number:  74 total reward:  0.21999999999999886  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  0  action  0 :  tensor([0.6172, 0.0081, 0.0522, 0.0975, 0.1173, 0.0638, 0.0438],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0070, 0.9676, 0.0030, 0.0066, 0.0019, 0.0010, 0.0129],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2380, 0.0066, 0.3251, 0.1185, 0.1095, 0.1332, 0.0691],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1251, 0.0141, 0.0521, 0.5879, 0.0936, 0.0897, 0.0375],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2222, 0.0034, 0.1148, 0.1279, 0.3053, 0.1269, 0.0996],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.2214, 0.0028, 0.1666, 0.0986, 0.1137, 0.3198, 0.0771],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1950, 0.1260, 0.1283, 0.1353, 0.1465, 0.1450, 0.1239],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  50.62751984467875
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.5535187002599
Printing some Q and Qe and total Qs values:  [[0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]] [[36.634]
 [36.634]
 [36.634]
 [36.634]
 [36.634]
 [36.634]
 [36.634]] [[1.074]
 [1.074]
 [1.074]
 [1.074]
 [1.074]
 [1.074]
 [1.074]]
maxi score, test score, baseline:  -0.34960666666666673 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8716641
line 256 mcts: sample exp_bonus 22.477016069509304
printing an ep nov before normalisation:  35.930547202900776
using explorer policy with actor:  1
siam score:  -0.8715199
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.34960666666666673 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.946401596069336
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.34960666666666673 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.09679205216701
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.34960666666666673 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  24.3401715100054
printing an ep nov before normalisation:  54.167816624124676
maxi score, test score, baseline:  -0.3496066666666668 0.6803333333333333 0.6803333333333333
actor:  0 policy actor:  0  step number:  45 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.032]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.032]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]]
maxi score, test score, baseline:  -0.3468333333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3468333333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.546210765838623
siam score:  -0.8664441
maxi score, test score, baseline:  -0.3468333333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.338597293513708
maxi score, test score, baseline:  -0.3468333333333334 0.6803333333333333 0.6803333333333333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3468333333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.45333333333333337  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 37.16612215745552
maxi score, test score, baseline:  -0.3468333333333334 0.6803333333333333 0.6803333333333333
Printing some Q and Qe and total Qs values:  [[0.6  ]
 [0.717]
 [0.651]
 [0.637]
 [0.614]
 [0.604]
 [0.61 ]] [[38.129]
 [38.291]
 [33.753]
 [33.756]
 [36.301]
 [34.996]
 [33.538]] [[0.6  ]
 [0.717]
 [0.651]
 [0.637]
 [0.614]
 [0.604]
 [0.61 ]]
printing an ep nov before normalisation:  44.19308662414551
maxi score, test score, baseline:  -0.3468333333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3468333333333335 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3468333333333335 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.460252539083584
maxi score, test score, baseline:  -0.3468333333333335 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3468333333333335 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  49 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.3440066666666668 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3440066666666668 0.6803333333333333 0.6803333333333333
printing an ep nov before normalisation:  33.868669880279995
actions average: 
K:  4  action  0 :  tensor([0.6422, 0.0143, 0.0428, 0.0900, 0.1052, 0.0542, 0.0513],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0099, 0.9371, 0.0122, 0.0207, 0.0026, 0.0028, 0.0147],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1617, 0.0225, 0.5037, 0.0775, 0.0753, 0.0951, 0.0643],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.3340, 0.2046, 0.0629, 0.2395, 0.0512, 0.0524, 0.0554],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2746, 0.0561, 0.1036, 0.0614, 0.3488, 0.1145, 0.0411],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2276, 0.0010, 0.1396, 0.1262, 0.1262, 0.2765, 0.1029],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([    0.0061,     0.4669,     0.0175,     0.1634,     0.0003,     0.0011,
            0.3447], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  29.482287400163866
maxi score, test score, baseline:  -0.3440066666666668 0.6803333333333333 0.6803333333333333
maxi score, test score, baseline:  -0.3440066666666668 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.424806142134486
maxi score, test score, baseline:  -0.3440066666666668 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.11943244934082
printing an ep nov before normalisation:  19.526878386067263
maxi score, test score, baseline:  -0.3440066666666668 0.6803333333333333 0.6803333333333333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3440066666666668 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.3440066666666668 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  56 total reward:  0.44666666666666666  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.34111333333333343 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.02  0.571 0.082 0.02  0.143 0.143 0.02 ]
siam score:  -0.8710637
Printing some Q and Qe and total Qs values:  [[0.34]
 [0.51]
 [0.34]
 [0.34]
 [0.34]
 [0.34]
 [0.34]] [[46.378]
 [50.233]
 [46.378]
 [46.378]
 [46.378]
 [46.378]
 [46.378]] [[1.041]
 [1.298]
 [1.041]
 [1.041]
 [1.041]
 [1.041]
 [1.041]]
maxi score, test score, baseline:  -0.3411133333333335 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3411133333333335 0.6803333333333333 0.6803333333333333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.036]
 [-0.041]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.045]] [[35.317]
 [53.294]
 [35.317]
 [35.317]
 [35.317]
 [35.317]
 [46.377]] [[0.11 ]
 [0.254]
 [0.11 ]
 [0.11 ]
 [0.11 ]
 [0.11 ]
 [0.192]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.3411133333333335 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.499]
 [0.477]
 [0.473]
 [0.477]
 [0.473]
 [0.474]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.469]
 [0.499]
 [0.477]
 [0.473]
 [0.477]
 [0.473]
 [0.474]]
maxi score, test score, baseline:  -0.3411133333333335 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3411133333333335 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.540533542633057
Printing some Q and Qe and total Qs values:  [[0.237]
 [0.299]
 [0.238]
 [0.238]
 [0.238]
 [0.239]
 [0.238]] [[23.623]
 [27.388]
 [24.269]
 [24.082]
 [23.933]
 [24.11 ]
 [24.136]] [[0.446]
 [0.572]
 [0.458]
 [0.455]
 [0.452]
 [0.456]
 [0.456]]
actor:  0 policy actor:  0  step number:  39 total reward:  0.52  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.33807333333333345 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 31.33133761518514
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.33807333333333345 0.6803333333333333 0.6803333333333333
actor:  0 policy actor:  0  step number:  59 total reward:  0.173333333333333  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8689735
maxi score, test score, baseline:  -0.3357266666666668 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.756183278642318
printing an ep nov before normalisation:  37.78714885205172
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.751]
 [0.679]
 [0.678]
 [0.68 ]
 [0.68 ]
 [0.677]] [[22.655]
 [21.282]
 [21.839]
 [21.879]
 [22.039]
 [22.381]
 [23.203]] [[0.652]
 [0.751]
 [0.679]
 [0.678]
 [0.68 ]
 [0.68 ]
 [0.677]]
maxi score, test score, baseline:  -0.3357266666666668 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.104808807373047
actor:  0 policy actor:  0  step number:  69 total reward:  0.10666666666666602  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.33351333333333344 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.33351333333333344 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  50 total reward:  0.24666666666666626  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.3310200000000001 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8592548
printing an ep nov before normalisation:  38.62040922444022
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.346996307373047
printing an ep nov before normalisation:  22.027855878316704
actor:  0 policy actor:  0  step number:  60 total reward:  0.21999999999999964  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.582]
 [0.495]
 [0.421]
 [0.442]
 [0.422]
 [0.426]] [[25.845]
 [25.281]
 [26.292]
 [27.135]
 [27.595]
 [27.425]
 [27.191]] [[1.673]
 [1.771]
 [1.791]
 [1.805]
 [1.874]
 [1.836]
 [1.816]]
printing an ep nov before normalisation:  22.123385656996383
maxi score, test score, baseline:  -0.3285800000000001 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.773]
 [0.773]
 [0.773]
 [0.773]
 [0.683]
 [0.773]
 [0.773]] [[38.81 ]
 [38.81 ]
 [38.81 ]
 [38.81 ]
 [43.405]
 [38.81 ]
 [38.81 ]] [[2.193]
 [2.193]
 [2.193]
 [2.193]
 [2.35 ]
 [2.193]
 [2.193]]
printing an ep nov before normalisation:  60.07441213011222
actor:  1 policy actor:  1  step number:  43 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3285800000000001 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.238738709321524
maxi score, test score, baseline:  -0.3285800000000001 0.6803333333333333 0.6803333333333333
maxi score, test score, baseline:  -0.3285800000000001 0.6803333333333333 0.6803333333333333
maxi score, test score, baseline:  -0.32858000000000015 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.32858000000000015 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 45.704568276380556
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.047]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]] [[13.288]
 [12.917]
 [12.917]
 [12.917]
 [12.917]
 [12.917]
 [12.917]] [[0.971]
 [0.953]
 [0.953]
 [0.953]
 [0.953]
 [0.953]
 [0.953]]
actor:  1 policy actor:  1  step number:  59 total reward:  0.22666666666666624  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.32858000000000015 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.32858000000000015 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.099]
 [0.29 ]
 [0.135]
 [0.099]
 [0.099]
 [0.099]
 [0.099]] [[38.582]
 [39.579]
 [44.323]
 [38.582]
 [38.582]
 [38.582]
 [38.582]] [[0.85 ]
 [1.076]
 [1.091]
 [0.85 ]
 [0.85 ]
 [0.85 ]
 [0.85 ]]
Printing some Q and Qe and total Qs values:  [[0.068]
 [0.253]
 [0.081]
 [0.093]
 [0.077]
 [0.056]
 [0.057]] [[26.4  ]
 [36.115]
 [34.656]
 [35.504]
 [39.404]
 [39.257]
 [30.444]] [[0.484]
 [1.044]
 [0.816]
 [0.861]
 [0.995]
 [0.968]
 [0.629]]
line 256 mcts: sample exp_bonus 37.85861152013363
printing an ep nov before normalisation:  47.73533729863558
maxi score, test score, baseline:  -0.32858000000000015 0.6803333333333333 0.6803333333333333
maxi score, test score, baseline:  -0.32858000000000015 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.575432484477076
printing an ep nov before normalisation:  33.56518687572103
printing an ep nov before normalisation:  57.477943159225255
printing an ep nov before normalisation:  40.096117180316654
printing an ep nov before normalisation:  55.31442590700899
actor:  1 policy actor:  1  step number:  44 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  48 total reward:  0.4466666666666663  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.32858000000000015 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.049]
 [-0.016]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]] [[23.668]
 [49.308]
 [23.668]
 [23.668]
 [23.668]
 [23.668]
 [23.668]] [[0.2  ]
 [0.759]
 [0.2  ]
 [0.2  ]
 [0.2  ]
 [0.2  ]
 [0.2  ]]
printing an ep nov before normalisation:  54.51859935790899
maxi score, test score, baseline:  -0.32858000000000015 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.51962600763029
siam score:  -0.87308884
maxi score, test score, baseline:  -0.32858000000000015 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.039442528138665
printing an ep nov before normalisation:  34.726574064138
actor:  1 policy actor:  1  step number:  49 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  0.020371569075194884
actor:  1 policy actor:  1  step number:  53 total reward:  0.4266666666666662  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  36.69745705896488
printing an ep nov before normalisation:  0.21705157021997934
actor:  1 policy actor:  1  step number:  44 total reward:  0.5000000000000001  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.29068301512098
maxi score, test score, baseline:  -0.32858000000000015 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.055508797907564
maxi score, test score, baseline:  -0.32858000000000015 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.68671565024492
maxi score, test score, baseline:  -0.32858000000000015 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.803225994110107
maxi score, test score, baseline:  -0.32858000000000015 0.6803333333333333 0.6803333333333333
maxi score, test score, baseline:  -0.32858000000000015 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.72392118393221
maxi score, test score, baseline:  -0.32858000000000015 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.32858000000000015 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.32858000000000015 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.639]
 [0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]] [[27.751]
 [35.297]
 [27.751]
 [27.751]
 [27.751]
 [27.751]
 [27.751]] [[1.683]
 [2.123]
 [1.683]
 [1.683]
 [1.683]
 [1.683]
 [1.683]]
printing an ep nov before normalisation:  53.60415843943747
maxi score, test score, baseline:  -0.32858000000000015 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.398]
 [0.398]
 [0.398]
 [0.398]
 [0.398]
 [0.398]
 [0.398]] [[43.003]
 [43.003]
 [43.003]
 [43.003]
 [43.003]
 [43.003]
 [43.003]] [[1.718]
 [1.718]
 [1.718]
 [1.718]
 [1.718]
 [1.718]
 [1.718]]
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.364]
 [0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]] [[45.649]
 [46.924]
 [45.649]
 [45.649]
 [45.649]
 [45.649]
 [45.649]] [[1.535]
 [1.624]
 [1.535]
 [1.535]
 [1.535]
 [1.535]
 [1.535]]
printing an ep nov before normalisation:  36.597299931059524
actor:  1 policy actor:  1  step number:  52 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.32858000000000015 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.32858000000000015 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  62 total reward:  0.15333333333333277  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.32858000000000015 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]]
printing an ep nov before normalisation:  37.65248911721366
maxi score, test score, baseline:  -0.32858000000000015 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.32858000000000015 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.58 ]
 [0.443]
 [0.443]
 [0.443]
 [0.443]
 [0.443]] [[41.594]
 [39.539]
 [38.553]
 [38.553]
 [38.553]
 [38.553]
 [38.553]] [[0.713]
 [0.847]
 [0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]]
maxi score, test score, baseline:  -0.32858000000000015 0.6803333333333333 0.6803333333333333
Printing some Q and Qe and total Qs values:  [[0.762]
 [0.805]
 [0.762]
 [0.74 ]
 [0.762]
 [0.762]
 [0.762]] [[25.158]
 [29.316]
 [25.158]
 [25.749]
 [25.158]
 [25.158]
 [25.158]] [[0.931]
 [1.026]
 [0.931]
 [0.917]
 [0.931]
 [0.931]
 [0.931]]
printing an ep nov before normalisation:  0.0053035438732251805
actor:  1 policy actor:  1  step number:  49 total reward:  0.3066666666666661  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.779]
 [0.853]
 [0.756]
 [0.813]
 [0.764]
 [0.745]
 [0.842]] [[35.257]
 [34.568]
 [32.217]
 [32.575]
 [30.157]
 [34.27 ]
 [33.662]] [[0.779]
 [0.853]
 [0.756]
 [0.813]
 [0.764]
 [0.745]
 [0.842]]
maxi score, test score, baseline:  -0.32858000000000015 0.6803333333333333 0.6803333333333333
actor:  0 policy actor:  0  step number:  43 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.038]
 [-0.012]
 [-0.038]
 [-0.038]
 [-0.038]
 [-0.038]
 [-0.038]] [[24.687]
 [35.892]
 [24.687]
 [24.687]
 [24.687]
 [24.687]
 [24.687]] [[0.154]
 [0.288]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]]
printing an ep nov before normalisation:  34.37241984906499
printing an ep nov before normalisation:  33.84566854099868
maxi score, test score, baseline:  -0.3258333333333335 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3258333333333335 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  57 total reward:  0.11999999999999933  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.3258733333333334 0.6803333333333333 0.6803333333333333
siam score:  -0.85622615
maxi score, test score, baseline:  -0.3258733333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3258733333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.3258733333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  27.96229977259277
actor:  1 policy actor:  1  step number:  68 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3258733333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 36.534535571228744
maxi score, test score, baseline:  -0.3258733333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.3258733333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  40 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  36.72985185772097
Printing some Q and Qe and total Qs values:  [[-0.052]
 [-0.034]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]] [[34.262]
 [49.931]
 [34.262]
 [34.262]
 [34.262]
 [34.262]
 [34.262]] [[0.249]
 [0.512]
 [0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.249]]
printing an ep nov before normalisation:  37.24012510486586
maxi score, test score, baseline:  -0.3258733333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3258733333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3258733333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  53 total reward:  0.23999999999999944  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  33.90836318002136
printing an ep nov before normalisation:  33.35776924580231
maxi score, test score, baseline:  -0.3258733333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.599]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]] [[29.952]
 [33.943]
 [29.952]
 [29.952]
 [29.952]
 [29.952]
 [29.952]] [[1.017]
 [1.204]
 [1.017]
 [1.017]
 [1.017]
 [1.017]
 [1.017]]
line 256 mcts: sample exp_bonus 47.474289976586284
maxi score, test score, baseline:  -0.3258733333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.87984026334405
maxi score, test score, baseline:  -0.3258733333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  65 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.3258733333333334 0.6803333333333333 0.6803333333333333
maxi score, test score, baseline:  -0.3258733333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.64563694470692
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.451134800373644
printing an ep nov before normalisation:  32.87576928703423
printing an ep nov before normalisation:  34.642064571380615
actions average: 
K:  1  action  0 :  tensor([0.5426, 0.0058, 0.0778, 0.0852, 0.1242, 0.0901, 0.0743],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0170, 0.9327, 0.0167, 0.0054, 0.0036, 0.0080, 0.0165],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1787, 0.0306, 0.3208, 0.1245, 0.0901, 0.1551, 0.1001],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2760, 0.0413, 0.1020, 0.2574, 0.1154, 0.1156, 0.0923],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1530, 0.0241, 0.0501, 0.0551, 0.6071, 0.0521, 0.0584],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2001, 0.0409, 0.0817, 0.0886, 0.1171, 0.3887, 0.0830],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1644, 0.2059, 0.1026, 0.1091, 0.1022, 0.1028, 0.2130],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  28.07910442352295
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.84913146
printing an ep nov before normalisation:  29.219525146587692
actor:  1 policy actor:  1  step number:  53 total reward:  0.22666666666666646  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.707]
 [0.798]
 [0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]] [[29.971]
 [35.082]
 [29.971]
 [29.971]
 [29.971]
 [29.971]
 [29.971]] [[1.056]
 [1.314]
 [1.056]
 [1.056]
 [1.056]
 [1.056]
 [1.056]]
printing an ep nov before normalisation:  36.09780311584473
maxi score, test score, baseline:  -0.3258733333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.3258733333333334 0.6803333333333333 0.6803333333333333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3258733333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.77 ]
 [0.856]
 [0.762]
 [0.742]
 [0.737]
 [0.786]
 [0.766]] [[26.209]
 [29.628]
 [26.396]
 [21.894]
 [22.158]
 [28.787]
 [22.458]] [[0.77 ]
 [0.856]
 [0.762]
 [0.742]
 [0.737]
 [0.786]
 [0.766]]
maxi score, test score, baseline:  -0.3258733333333334 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.801]
 [0.848]
 [0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.815]] [[26.488]
 [31.618]
 [25.849]
 [25.849]
 [25.849]
 [25.849]
 [25.849]] [[0.801]
 [0.848]
 [0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.815]]
printing an ep nov before normalisation:  52.74156159735312
printing an ep nov before normalisation:  37.242017478792675
actor:  0 policy actor:  0  step number:  46 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.32319333333333344 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.605]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]] [[35.132]
 [31.602]
 [35.132]
 [35.132]
 [35.132]
 [35.132]
 [35.132]] [[0.735]
 [0.803]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  49 total reward:  0.3999999999999996  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  21.278960740971474
printing an ep nov before normalisation:  48.408113969373744
printing an ep nov before normalisation:  41.8828821182251
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.631]
 [0.481]
 [0.478]
 [0.475]
 [0.481]
 [0.481]] [[52.629]
 [46.851]
 [52.629]
 [52.429]
 [52.463]
 [52.629]
 [52.629]] [[1.672]
 [1.62 ]
 [1.672]
 [1.662]
 [1.66 ]
 [1.672]
 [1.672]]
printing an ep nov before normalisation:  49.35383710142375
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.584]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]] [[46.373]
 [41.535]
 [46.373]
 [46.373]
 [46.373]
 [46.373]
 [46.373]] [[1.354]
 [1.24 ]
 [1.354]
 [1.354]
 [1.354]
 [1.354]
 [1.354]]
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.524]
 [0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.401]] [[37.931]
 [39.14 ]
 [34.512]
 [34.512]
 [34.512]
 [34.512]
 [34.512]] [[1.506]
 [1.705]
 [1.354]
 [1.354]
 [1.354]
 [1.354]
 [1.354]]
actor:  1 policy actor:  1  step number:  64 total reward:  0.3399999999999992  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  43 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  27.125985202810824
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.05916252827379
actor:  1 policy actor:  1  step number:  57 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  41.21265355871655
maxi score, test score, baseline:  -0.32319333333333344 0.6803333333333333 0.6803333333333333
maxi score, test score, baseline:  -0.32319333333333344 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.525]
 [0.436]
 [0.436]
 [0.436]
 [0.436]
 [0.436]] [[45.07 ]
 [50.374]
 [39.419]
 [39.419]
 [39.419]
 [39.419]
 [39.419]] [[1.332]
 [1.644]
 [1.161]
 [1.161]
 [1.161]
 [1.161]
 [1.161]]
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.286]
 [0.411]
 [0.414]
 [0.418]
 [0.413]
 [0.446]] [[57.218]
 [47.293]
 [54.361]
 [56.18 ]
 [57.124]
 [58.869]
 [55.084]] [[1.534]
 [1.164]
 [1.479]
 [1.532]
 [1.561]
 [1.603]
 [1.534]]
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]] [[45.433]
 [45.433]
 [45.433]
 [45.433]
 [45.433]
 [45.433]
 [45.433]] [[0.783]
 [0.783]
 [0.783]
 [0.783]
 [0.783]
 [0.783]
 [0.783]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  12.835155725479126
maxi score, test score, baseline:  -0.32319333333333344 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  42 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  1.333
siam score:  -0.8536718
Printing some Q and Qe and total Qs values:  [[ 0.174]
 [-0.187]
 [ 0.002]
 [-0.045]
 [-0.068]
 [-0.011]
 [-0.045]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.174]
 [-0.187]
 [ 0.002]
 [-0.045]
 [-0.068]
 [-0.011]
 [-0.045]]
maxi score, test score, baseline:  -0.32319333333333344 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.65106053352287
maxi score, test score, baseline:  -0.32319333333333344 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85585815
maxi score, test score, baseline:  -0.32319333333333344 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.32319333333333344 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  16.09891414642334
Printing some Q and Qe and total Qs values:  [[0.228]
 [0.314]
 [0.228]
 [0.116]
 [0.228]
 [0.111]
 [0.228]] [[26.783]
 [30.31 ]
 [26.783]
 [18.303]
 [26.783]
 [18.313]
 [26.783]] [[0.414]
 [0.566]
 [0.414]
 [0.143]
 [0.414]
 [0.138]
 [0.414]]
maxi score, test score, baseline:  -0.32319333333333344 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.12561171896934
siam score:  -0.85836405
printing an ep nov before normalisation:  33.97672107695863
actor:  0 policy actor:  0  step number:  48 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  33.222510523491025
actor:  1 policy actor:  1  step number:  52 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  1.333
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.703]
 [0.804]
 [0.704]
 [0.733]
 [0.582]
 [0.64 ]
 [0.776]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.703]
 [0.804]
 [0.704]
 [0.733]
 [0.582]
 [0.64 ]
 [0.776]]
printing an ep nov before normalisation:  0.0
actor:  1 policy actor:  1  step number:  58 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.32072666666666677 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.5703955980163
printing an ep nov before normalisation:  30.371952056884766
printing an ep nov before normalisation:  38.869513441474744
printing an ep nov before normalisation:  35.83007097244263
maxi score, test score, baseline:  -0.32072666666666677 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  30.123918091222382
printing an ep nov before normalisation:  40.8789772288493
printing an ep nov before normalisation:  32.0128607749939
line 256 mcts: sample exp_bonus 49.07479433309087
printing an ep nov before normalisation:  29.18892841925878
maxi score, test score, baseline:  -0.32072666666666677 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  42.628373171566
siam score:  -0.8607121
maxi score, test score, baseline:  -0.26219333333333344 0.6803333333333333 0.6803333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  28 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[0.548]
 [0.523]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]] [[22.131]
 [29.984]
 [22.131]
 [22.131]
 [22.131]
 [22.131]
 [22.131]] [[1.114]
 [1.481]
 [1.114]
 [1.114]
 [1.114]
 [1.114]
 [1.114]]
actor:  0 policy actor:  0  step number:  30 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  2
actor:  1 policy actor:  1  step number:  45 total reward:  0.5866666666666668  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.2555533333333334 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2555533333333334 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2555533333333334 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.27421940744401
actor:  1 policy actor:  1  step number:  47 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.2555533333333334 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2555533333333334 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
actor:  0 policy actor:  0  step number:  50 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.25300666666666677 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.85 ]
 [0.732]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]] [[41.738]
 [39.126]
 [39.203]
 [41.028]
 [41.028]
 [41.028]
 [41.028]] [[0.651]
 [0.85 ]
 [0.732]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]]
actions average: 
K:  4  action  0 :  tensor([0.5853, 0.0689, 0.0507, 0.0710, 0.0877, 0.0708, 0.0656],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0114, 0.9449, 0.0082, 0.0112, 0.0024, 0.0048, 0.0171],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0837, 0.0333, 0.5143, 0.0712, 0.0477, 0.1648, 0.0849],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2370, 0.1499, 0.0781, 0.2249, 0.1036, 0.1197, 0.0868],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.3185, 0.0098, 0.0659, 0.1012, 0.3235, 0.0646, 0.1165],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.3413, 0.1149, 0.0917, 0.1207, 0.0934, 0.1226, 0.1154],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1974, 0.0189, 0.1159, 0.1349, 0.0900, 0.1213, 0.3217],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.25300666666666677 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.033827967150735
printing an ep nov before normalisation:  32.87503819154945
maxi score, test score, baseline:  -0.25300666666666677 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.931785106658936
maxi score, test score, baseline:  -0.25300666666666677 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  48 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.2502733333333334 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.6292, 0.0029, 0.0533, 0.0707, 0.1099, 0.0729, 0.0613],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0075, 0.9535, 0.0038, 0.0151, 0.0040, 0.0040, 0.0121],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1244, 0.0065, 0.4851, 0.0703, 0.0814, 0.1403, 0.0920],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2175, 0.0145, 0.0794, 0.3534, 0.0936, 0.0978, 0.1439],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0724,     0.0003,     0.0223,     0.0334,     0.8050,     0.0329,
            0.0337], grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0684, 0.0035, 0.0702, 0.0860, 0.0777, 0.6391, 0.0551],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0636, 0.2697, 0.1484, 0.1337, 0.0569, 0.0738, 0.2539],
       grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 64.6562162333932
printing an ep nov before normalisation:  37.88154780484228
using explorer policy with actor:  1
printing an ep nov before normalisation:  11.339654922485352
actions average: 
K:  4  action  0 :  tensor([0.5882, 0.0193, 0.0692, 0.0628, 0.1105, 0.0680, 0.0821],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0222, 0.9160, 0.0179, 0.0052, 0.0185, 0.0032, 0.0169],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2153, 0.0170, 0.2659, 0.1101, 0.1065, 0.1340, 0.1513],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2186, 0.1609, 0.0763, 0.2038, 0.1254, 0.1026, 0.1124],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1940, 0.0119, 0.0644, 0.1074, 0.4771, 0.0720, 0.0733],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2409, 0.0198, 0.3370, 0.0711, 0.0947, 0.1810, 0.0556],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2042, 0.0223, 0.1444, 0.1153, 0.1334, 0.1044, 0.2759],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  27.042512893676758
maxi score, test score, baseline:  -0.2502733333333334 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.27000804397638
maxi score, test score, baseline:  -0.2502733333333334 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2502733333333334 0.6886666666666666 0.6886666666666666
maxi score, test score, baseline:  -0.2502733333333334 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.33394017183289
printing an ep nov before normalisation:  24.135394227856146
actor:  1 policy actor:  1  step number:  33 total reward:  0.5733333333333333  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  55 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  48 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 44.33133357211683
printing an ep nov before normalisation:  30.93912124633789
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.2502733333333334 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.374]
 [0.423]
 [0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]] [[29.714]
 [28.905]
 [29.714]
 [29.714]
 [29.714]
 [29.714]
 [29.714]] [[1.612]
 [1.591]
 [1.612]
 [1.612]
 [1.612]
 [1.612]
 [1.612]]
maxi score, test score, baseline:  -0.2502733333333334 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2502733333333334 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  34.78995551898583
actor:  1 policy actor:  1  step number:  56 total reward:  0.16666666666666607  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.2502733333333334 0.6886666666666666 0.6886666666666666
actor:  0 policy actor:  0  step number:  68 total reward:  0.21999999999999886  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  38.812011918761904
printing an ep nov before normalisation:  31.27629280090332
printing an ep nov before normalisation:  45.26538701472614
Printing some Q and Qe and total Qs values:  [[0.783]
 [0.844]
 [0.785]
 [0.785]
 [0.785]
 [0.785]
 [0.785]] [[31.887]
 [30.87 ]
 [30.418]
 [30.418]
 [30.418]
 [30.418]
 [30.418]] [[0.783]
 [0.844]
 [0.785]
 [0.785]
 [0.785]
 [0.785]
 [0.785]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
actor:  0 policy actor:  0  step number:  45 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  32.36949872319536
actor:  1 policy actor:  1  step number:  49 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  25.365588665008545
maxi score, test score, baseline:  -0.2451133333333334 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2451133333333334 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  1.705433874121809e-05
maxi score, test score, baseline:  -0.2451133333333334 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  74 total reward:  0.153333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.2451133333333334 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.950841903686523
maxi score, test score, baseline:  -0.2451133333333334 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2451133333333334 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2451133333333334 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  71 total reward:  0.0799999999999993  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  37.956650257110596
actor:  1 policy actor:  1  step number:  42 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.2451133333333334 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.416]
 [0.537]
 [0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.416]] [[45.808]
 [39.965]
 [45.808]
 [45.808]
 [45.808]
 [45.808]
 [45.808]] [[1.01 ]
 [1.008]
 [1.01 ]
 [1.01 ]
 [1.01 ]
 [1.01 ]
 [1.01 ]]
maxi score, test score, baseline:  -0.2451133333333334 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.25333333333333297  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.2451133333333334 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2451133333333334 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  40 total reward:  0.47333333333333305  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  32.18234949952669
printing an ep nov before normalisation:  28.096904643723516
actor:  0 policy actor:  0  step number:  48 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.24194000000000013 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  79.55978654223055
printing an ep nov before normalisation:  40.94091016297875
maxi score, test score, baseline:  -0.24194000000000013 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  56 total reward:  0.13999999999999946  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.24194000000000013 0.6886666666666666 0.6886666666666666
maxi score, test score, baseline:  -0.24194000000000013 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.5637, 0.0512, 0.0792, 0.0670, 0.0821, 0.0634, 0.0934],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0098, 0.9519, 0.0081, 0.0089, 0.0033, 0.0050, 0.0130],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1701, 0.0545, 0.4545, 0.0748, 0.0585, 0.1090, 0.0786],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0988, 0.1268, 0.0508, 0.5491, 0.0532, 0.0595, 0.0618],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.3644, 0.0045, 0.0605, 0.0770, 0.3876, 0.0489, 0.0570],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1589, 0.0087, 0.1293, 0.1054, 0.1030, 0.4218, 0.0729],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2709, 0.0129, 0.1357, 0.1033, 0.0754, 0.0863, 0.3155],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.24194000000000013 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.24194000000000013 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.24194000000000013 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.24194000000000013 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  49 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  43.58988774110801
printing an ep nov before normalisation:  44.47327100916262
maxi score, test score, baseline:  -0.2393000000000001 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.389]
 [0.23 ]
 [0.069]
 [0.283]
 [0.212]
 [0.067]
 [0.283]] [[39.035]
 [25.132]
 [27.07 ]
 [38.314]
 [36.555]
 [33.39 ]
 [38.314]] [[1.241]
 [0.551]
 [0.465]
 [1.108]
 [0.97 ]
 [0.703]
 [1.108]]
printing an ep nov before normalisation:  34.69872605616902
printing an ep nov before normalisation:  36.27873146203045
printing an ep nov before normalisation:  66.24782381902459
printing an ep nov before normalisation:  40.08192539215088
maxi score, test score, baseline:  -0.2393000000000001 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.006985767050480263
printing an ep nov before normalisation:  25.801124572753906
actor:  1 policy actor:  1  step number:  57 total reward:  0.3199999999999994  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.2393000000000001 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2393000000000001 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.34 ]
 [0.409]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.314]] [[31.299]
 [35.918]
 [31.299]
 [31.299]
 [31.299]
 [31.299]
 [29.252]] [[1.038]
 [1.308]
 [1.038]
 [1.038]
 [1.038]
 [1.038]
 [0.924]]
printing an ep nov before normalisation:  35.296876430511475
maxi score, test score, baseline:  -0.2393000000000001 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.452]
 [0.46 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]] [[38.272]
 [37.727]
 [34.159]
 [34.159]
 [34.159]
 [34.159]
 [34.159]] [[1.365]
 [1.351]
 [1.245]
 [1.245]
 [1.245]
 [1.245]
 [1.245]]
actor:  1 policy actor:  1  step number:  60 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.322]
 [0.428]
 [0.339]
 [0.334]
 [0.334]
 [0.336]
 [0.332]] [[26.358]
 [28.559]
 [23.951]
 [24.756]
 [24.626]
 [24.889]
 [25.106]] [[1.147]
 [1.388]
 [1.016]
 [1.061]
 [1.053]
 [1.071]
 [1.08 ]]
actor:  0 policy actor:  0  step number:  46 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  25.622001985080768
maxi score, test score, baseline:  -0.23656666666666679 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.97946075766146
printing an ep nov before normalisation:  49.71528116880714
printing an ep nov before normalisation:  31.329605575341812
siam score:  -0.87300855
maxi score, test score, baseline:  -0.23656666666666679 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.23656666666666679 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.922444304753995
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.354]] [[47.367]
 [47.367]
 [47.367]
 [47.367]
 [47.367]
 [47.367]
 [47.367]] [[0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]]
actor:  1 policy actor:  1  step number:  62 total reward:  0.20666666666666567  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  45.06764670217145
printing an ep nov before normalisation:  38.83150153874452
Printing some Q and Qe and total Qs values:  [[0.114]
 [0.19 ]
 [0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]] [[40.793]
 [35.736]
 [40.608]
 [40.608]
 [40.608]
 [40.608]
 [40.608]] [[1.781]
 [1.478]
 [1.761]
 [1.761]
 [1.761]
 [1.761]
 [1.761]]
maxi score, test score, baseline:  -0.23656666666666679 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.23656666666666679 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.23656666666666679 0.6886666666666666 0.6886666666666666
printing an ep nov before normalisation:  61.11150912414993
maxi score, test score, baseline:  -0.23656666666666679 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.23656666666666679 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.23656666666666679 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.23656666666666679 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.196]
 [0.526]
 [0.196]
 [0.196]
 [0.196]
 [0.196]
 [0.196]] [[32.362]
 [ 0.   ]
 [32.362]
 [32.362]
 [32.362]
 [32.362]
 [32.362]] [[0.196]
 [0.526]
 [0.196]
 [0.196]
 [0.196]
 [0.196]
 [0.196]]
actor:  1 policy actor:  1  step number:  60 total reward:  0.259999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  43 total reward:  0.5333333333333333  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.23656666666666679 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.630901905976813
siam score:  -0.8732798
printing an ep nov before normalisation:  30.266665355526044
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.878]
 [0.878]
 [0.878]
 [0.878]
 [0.71 ]
 [0.878]
 [0.878]] [[32.452]
 [32.452]
 [32.452]
 [32.452]
 [36.216]
 [32.452]
 [32.452]] [[2.31 ]
 [2.31 ]
 [2.31 ]
 [2.31 ]
 [2.377]
 [2.31 ]
 [2.31 ]]
maxi score, test score, baseline:  -0.23656666666666679 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  38 total reward:  0.5666666666666667  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.23656666666666679 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.654]
 [0.654]
 [0.649]
 [0.654]
 [0.637]
 [0.632]] [[21.347]
 [16.062]
 [16.062]
 [16.519]
 [16.062]
 [16.782]
 [17.129]] [[1.402]
 [1.159]
 [1.159]
 [1.186]
 [1.159]
 [1.193]
 [1.213]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.23656666666666676 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.13333333333333297  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.23656666666666676 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.01074375641017
Printing some Q and Qe and total Qs values:  [[-0.023]
 [-0.001]
 [-0.02 ]
 [-0.025]
 [-0.022]
 [-0.026]
 [-0.02 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.023]
 [-0.001]
 [-0.02 ]
 [-0.025]
 [-0.022]
 [-0.026]
 [-0.02 ]]
printing an ep nov before normalisation:  24.17025867469343
maxi score, test score, baseline:  -0.23656666666666676 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.4813, 0.0394, 0.0984, 0.0854, 0.0915, 0.1043, 0.0997],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0140, 0.9457, 0.0075, 0.0090, 0.0066, 0.0062, 0.0111],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1816, 0.0602, 0.3623, 0.0860, 0.0821, 0.1105, 0.1172],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1698, 0.0146, 0.1695, 0.2927, 0.1181, 0.1405, 0.0948],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2101, 0.0583, 0.0842, 0.0877, 0.3693, 0.0990, 0.0914],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.2360, 0.0029, 0.1355, 0.1084, 0.1032, 0.3113, 0.1026],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1941, 0.1284, 0.1651, 0.0974, 0.1175, 0.1387, 0.1589],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.23656666666666676 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.94801073879842
actor:  1 policy actor:  1  step number:  39 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8638034
actor:  1 policy actor:  1  step number:  61 total reward:  0.13333333333333286  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  1  action  0 :  tensor([0.6570, 0.0121, 0.0601, 0.0748, 0.0778, 0.0583, 0.0599],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0169, 0.9373, 0.0097, 0.0100, 0.0038, 0.0053, 0.0171],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1794, 0.0105, 0.4233, 0.1091, 0.0902, 0.1126, 0.0748],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2411, 0.0058, 0.0864, 0.3757, 0.0838, 0.1154, 0.0919],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2737, 0.0143, 0.0774, 0.1153, 0.3069, 0.1083, 0.1041],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2649, 0.0392, 0.1117, 0.1050, 0.1019, 0.2761, 0.1012],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2899, 0.1047, 0.0685, 0.0952, 0.0550, 0.0420, 0.3447],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.23656666666666676 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.09286004398082
printing an ep nov before normalisation:  38.44672590606686
printing an ep nov before normalisation:  40.81018311636789
maxi score, test score, baseline:  -0.23656666666666676 0.6886666666666666 0.6886666666666666
maxi score, test score, baseline:  -0.23656666666666676 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.40666666666666607  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.23656666666666676 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.21999999999999964  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.23656666666666676 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.133835627166594
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.23656666666666676 0.6886666666666666 0.6886666666666666
actor:  0 policy actor:  0  step number:  44 total reward:  0.39333333333333287  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  34.364231476741516
siam score:  -0.8501663
maxi score, test score, baseline:  -0.23378000000000013 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.08660843110547
actor:  1 policy actor:  1  step number:  50 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  46 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.23118000000000008 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.23118000000000008 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.23118000000000008 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.23118000000000008 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.17484495183332
printing an ep nov before normalisation:  52.59404672509451
maxi score, test score, baseline:  -0.23118000000000008 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  68 total reward:  0.03333333333333277  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.22911333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.025057614846922
Printing some Q and Qe and total Qs values:  [[0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]] [[29.163]
 [29.163]
 [29.163]
 [29.163]
 [29.163]
 [29.163]
 [29.163]] [[0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]]
maxi score, test score, baseline:  -0.22911333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.078736305236816
actor:  0 policy actor:  0  step number:  39 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.2261800000000001 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.566079845126296
maxi score, test score, baseline:  -0.2261800000000001 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8650902
maxi score, test score, baseline:  -0.2261800000000001 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.84372285056787
printing an ep nov before normalisation:  38.460633331039155
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.2261800000000001 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2261800000000001 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.09789752960205
maxi score, test score, baseline:  -0.2261800000000001 0.6886666666666666 0.6886666666666666
maxi score, test score, baseline:  -0.2261800000000001 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86328614
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  58 total reward:  0.326666666666666  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.22618000000000016 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.041 0.49  0.02  0.061 0.245 0.082 0.061]
Printing some Q and Qe and total Qs values:  [[0.144]
 [0.172]
 [0.144]
 [0.144]
 [0.144]
 [0.144]
 [0.144]] [[30.004]
 [38.869]
 [30.004]
 [30.004]
 [30.004]
 [30.004]
 [30.004]] [[0.651]
 [1.059]
 [0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]]
printing an ep nov before normalisation:  24.332823753356934
Printing some Q and Qe and total Qs values:  [[0.828]
 [0.943]
 [0.842]
 [0.873]
 [0.824]
 [0.873]
 [0.847]] [[29.518]
 [30.811]
 [33.932]
 [29.175]
 [34.915]
 [29.175]
 [30.127]] [[0.828]
 [0.943]
 [0.842]
 [0.873]
 [0.824]
 [0.873]
 [0.847]]
printing an ep nov before normalisation:  20.819499492645264
maxi score, test score, baseline:  -0.22618000000000016 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.45798827147729
maxi score, test score, baseline:  -0.22618000000000016 0.6886666666666666 0.6886666666666666
printing an ep nov before normalisation:  34.9277775264156
actor:  0 policy actor:  0  step number:  49 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.027]
 [-0.004]
 [-0.027]
 [-0.027]
 [-0.028]
 [-0.027]
 [-0.027]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.027]
 [-0.004]
 [-0.027]
 [-0.027]
 [-0.028]
 [-0.027]
 [-0.027]]
actor:  1 policy actor:  1  step number:  60 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.22351333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.22351333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 64.55933843732373
maxi score, test score, baseline:  -0.22351333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
siam score:  -0.85964584
actor:  1 policy actor:  1  step number:  61 total reward:  0.19999999999999962  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  25.240919589996338
maxi score, test score, baseline:  -0.22351333333333345 0.6886666666666666 0.6886666666666666
actor:  1 policy actor:  1  step number:  45 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.22351333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8585349
Printing some Q and Qe and total Qs values:  [[0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]] [[33.684]
 [33.684]
 [33.684]
 [33.684]
 [33.684]
 [33.684]
 [33.684]] [[1.153]
 [1.153]
 [1.153]
 [1.153]
 [1.153]
 [1.153]
 [1.153]]
maxi score, test score, baseline:  -0.22351333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  65 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  39 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  42 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.22351333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86311907
Printing some Q and Qe and total Qs values:  [[-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[27.935]
 [27.935]
 [27.935]
 [27.935]
 [27.935]
 [27.935]
 [27.935]] [[0.178]
 [0.178]
 [0.178]
 [0.178]
 [0.178]
 [0.178]
 [0.178]]
maxi score, test score, baseline:  -0.22351333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.263]
 [0.328]
 [0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]] [[39.334]
 [44.057]
 [39.334]
 [39.334]
 [39.334]
 [39.334]
 [39.334]] [[0.46 ]
 [0.571]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  47 total reward:  0.35999999999999943  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.22351333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.22351333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]] [[34.261]
 [34.261]
 [34.261]
 [34.261]
 [34.261]
 [34.261]
 [34.261]] [[1.279]
 [1.279]
 [1.279]
 [1.279]
 [1.279]
 [1.279]
 [1.279]]
maxi score, test score, baseline:  -0.22351333333333345 0.6886666666666666 0.6886666666666666
maxi score, test score, baseline:  -0.22351333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.22351333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.22351333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.5  ]
 [0.352]
 [0.419]
 [0.438]
 [0.424]
 [0.453]] [[35.987]
 [36.169]
 [37.801]
 [36.854]
 [37.15 ]
 [37.029]
 [36.169]] [[1.254]
 [1.325]
 [1.258]
 [1.278]
 [1.312]
 [1.292]
 [1.278]]
maxi score, test score, baseline:  -0.22351333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  42 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  50 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.730804443359375
actor:  1 policy actor:  1  step number:  52 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.04 ]
 [-0.016]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.04 ]
 [-0.016]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]]
printing an ep nov before normalisation:  40.743000475141066
Printing some Q and Qe and total Qs values:  [[-0.033]
 [-0.014]
 [-0.039]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.039]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.033]
 [-0.014]
 [-0.039]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.039]]
maxi score, test score, baseline:  -0.22062000000000012 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  38 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  1  action  0 :  tensor([0.5608, 0.0358, 0.0682, 0.0638, 0.1030, 0.0752, 0.0932],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0038, 0.9649, 0.0021, 0.0065, 0.0011, 0.0012, 0.0204],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2663, 0.0061, 0.2770, 0.0764, 0.1032, 0.1633, 0.1077],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0952, 0.0998, 0.0410, 0.5250, 0.0766, 0.0635, 0.0989],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2881, 0.0037, 0.0706, 0.1460, 0.3256, 0.0806, 0.0853],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1249, 0.0060, 0.1314, 0.0786, 0.1008, 0.4653, 0.0931],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.3652, 0.0125, 0.1353, 0.1006, 0.1160, 0.1316, 0.1388],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  34.953546558785064
printing an ep nov before normalisation:  40.437722251655615
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.209]
 [0.209]
 [0.209]
 [0.207]
 [0.209]
 [0.209]] [[40.632]
 [44.785]
 [44.785]
 [44.785]
 [31.632]
 [44.785]
 [44.785]] [[1.179]
 [1.155]
 [1.155]
 [1.155]
 [0.703]
 [1.155]
 [1.155]]
actor:  0 policy actor:  0  step number:  65 total reward:  0.3333333333333327  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.21500666666666682 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.75773510729664
maxi score, test score, baseline:  -0.21500666666666682 0.6886666666666666 0.6886666666666666
maxi score, test score, baseline:  -0.21500666666666682 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.21500666666666682 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.901]
 [0.901]
 [0.901]
 [0.901]
 [0.901]
 [0.901]
 [0.901]] [[37.254]
 [37.254]
 [37.254]
 [37.254]
 [37.254]
 [37.254]
 [37.254]] [[0.901]
 [0.901]
 [0.901]
 [0.901]
 [0.901]
 [0.901]
 [0.901]]
maxi score, test score, baseline:  -0.21500666666666682 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.21500666666666682 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.73025176337783
maxi score, test score, baseline:  -0.21500666666666682 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.732832598366116
maxi score, test score, baseline:  -0.21500666666666682 0.6886666666666666 0.6886666666666666
maxi score, test score, baseline:  -0.21500666666666682 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.016670636865022
actor:  1 policy actor:  1  step number:  60 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8572298
actor:  1 policy actor:  1  step number:  41 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  22.21071652184045
maxi score, test score, baseline:  -0.21500666666666682 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.0885772573069
printing an ep nov before normalisation:  25.82953929901123
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.76509969285467
actor:  0 policy actor:  0  step number:  54 total reward:  0.1799999999999996  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 35.56977586936768
printing an ep nov before normalisation:  34.33596134185791
Printing some Q and Qe and total Qs values:  [[0.265]
 [0.256]
 [0.162]
 [0.162]
 [0.162]
 [0.162]
 [0.162]] [[29.417]
 [25.858]
 [29.172]
 [29.172]
 [29.172]
 [29.172]
 [29.172]] [[0.87 ]
 [0.722]
 [0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]]
siam score:  -0.8569851
printing an ep nov before normalisation:  33.62928451340238
actor:  1 policy actor:  1  step number:  40 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.2126466666666668 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.71233367919922
maxi score, test score, baseline:  -0.2126466666666668 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  62 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  35.70912921570929
Printing some Q and Qe and total Qs values:  [[-0.016]
 [ 0.031]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.016]
 [ 0.031]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]]
Printing some Q and Qe and total Qs values:  [[-0.038]
 [-0.017]
 [-0.039]
 [-0.038]
 [-0.038]
 [-0.039]
 [-0.038]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.038]
 [-0.017]
 [-0.039]
 [-0.038]
 [-0.038]
 [-0.039]
 [-0.038]]
actor:  0 policy actor:  0  step number:  33 total reward:  0.5866666666666667  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8597159
printing an ep nov before normalisation:  29.312553977730005
siam score:  -0.85920286
actor:  1 policy actor:  1  step number:  51 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.063]
 [-0.063]
 [-0.063]
 [-0.037]
 [-0.063]
 [-0.063]
 [-0.063]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.063]
 [-0.063]
 [-0.063]
 [-0.037]
 [-0.063]
 [-0.063]
 [-0.063]]
maxi score, test score, baseline:  -0.20947333333333346 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.614]
 [0.72 ]
 [0.643]
 [0.636]
 [0.643]
 [0.643]
 [0.724]] [[47.153]
 [40.972]
 [46.86 ]
 [42.46 ]
 [46.86 ]
 [46.86 ]
 [39.309]] [[0.614]
 [0.72 ]
 [0.643]
 [0.636]
 [0.643]
 [0.643]
 [0.724]]
printing an ep nov before normalisation:  35.77063350401627
printing an ep nov before normalisation:  47.75125519161656
Printing some Q and Qe and total Qs values:  [[0.357]
 [0.418]
 [0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.357]] [[33.411]
 [34.941]
 [33.411]
 [33.411]
 [33.411]
 [33.411]
 [33.411]] [[1.872]
 [2.056]
 [1.872]
 [1.872]
 [1.872]
 [1.872]
 [1.872]]
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.435]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]] [[36.608]
 [41.08 ]
 [36.608]
 [36.608]
 [36.608]
 [36.608]
 [36.608]] [[1.71]
 [2.1 ]
 [1.71]
 [1.71]
 [1.71]
 [1.71]
 [1.71]]
Printing some Q and Qe and total Qs values:  [[0.587]
 [0.717]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.643]] [[25.284]
 [28.047]
 [27.222]
 [27.222]
 [27.222]
 [27.222]
 [26.546]] [[1.644]
 [2.012]
 [1.932]
 [1.932]
 [1.932]
 [1.932]
 [1.809]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.20947333333333346 0.6886666666666666 0.6886666666666666
actor:  1 policy actor:  1  step number:  42 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.20947333333333348 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.64491823255266
maxi score, test score, baseline:  -0.20947333333333348 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.041 0.592 0.224 0.02  0.02  0.041 0.061]
Printing some Q and Qe and total Qs values:  [[0.812]
 [0.885]
 [0.813]
 [0.798]
 [0.815]
 [0.838]
 [0.817]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.812]
 [0.885]
 [0.813]
 [0.798]
 [0.815]
 [0.838]
 [0.817]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [ 0.8427],
        [ 0.8133],
        [ 0.8296],
        [-0.1700],
        [-0.1221],
        [ 0.9203],
        [ 0.4783],
        [ 0.4927],
        [ 0.8381]], dtype=torch.float64)
0.9117834164999999 0.9117834164999999
-0.045546567066 0.7971856208030608
-0.058094434398 0.755229930979983
-0.09703970119800001 0.7325387091039941
-0.032346567066 -0.20230809760142263
-0.032346567066 -0.1544757476073064
-0.045414567066 0.8748884831972317
-0.07129443439800001 0.4070403675834591
-0.057834381198 0.4348204028737916
-0.032346567066 0.8057503371134643
maxi score, test score, baseline:  -0.20947333333333348 0.6886666666666666 0.6886666666666666
maxi score, test score, baseline:  -0.20947333333333348 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  66 total reward:  0.15333333333333277  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  39.76121306335163
printing an ep nov before normalisation:  37.01478760462295
maxi score, test score, baseline:  -0.20947333333333348 0.6886666666666666 0.6886666666666666
printing an ep nov before normalisation:  32.31877498465877
actor:  0 policy actor:  0  step number:  59 total reward:  0.27999999999999925  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  53 total reward:  0.3333333333333327  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.20691333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.737442382360165
maxi score, test score, baseline:  -0.20691333333333345 0.6886666666666666 0.6886666666666666
maxi score, test score, baseline:  -0.20691333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.20691333333333345 0.6886666666666666 0.6886666666666666
actor:  1 policy actor:  1  step number:  51 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  0.667
siam score:  -0.86097544
printing an ep nov before normalisation:  46.635075063030186
Printing some Q and Qe and total Qs values:  [[0.292]
 [0.476]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]] [[33.01 ]
 [32.051]
 [30.059]
 [30.059]
 [30.059]
 [30.059]
 [30.059]] [[1.896]
 [2.003]
 [1.726]
 [1.726]
 [1.726]
 [1.726]
 [1.726]]
maxi score, test score, baseline:  -0.20691333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.20691333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.335487152445296
maxi score, test score, baseline:  -0.20691333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.85247508736463
printing an ep nov before normalisation:  25.48626549214554
actor:  0 policy actor:  0  step number:  68 total reward:  0.09999999999999931  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  29.739645050101544
Printing some Q and Qe and total Qs values:  [[0.624]
 [0.699]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]] [[24.423]
 [28.672]
 [24.423]
 [24.423]
 [24.423]
 [24.423]
 [24.423]] [[2.046]
 [2.699]
 [2.046]
 [2.046]
 [2.046]
 [2.046]
 [2.046]]
maxi score, test score, baseline:  -0.2047133333333335 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.727]
 [0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]] [[27.424]
 [26.855]
 [28.628]
 [28.628]
 [28.628]
 [28.628]
 [28.628]] [[2.378]
 [2.467]
 [2.512]
 [2.512]
 [2.512]
 [2.512]
 [2.512]]
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.573]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]] [[25.491]
 [29.824]
 [25.491]
 [25.491]
 [25.491]
 [25.491]
 [25.491]] [[1.048]
 [1.301]
 [1.048]
 [1.048]
 [1.048]
 [1.048]
 [1.048]]
actor:  1 policy actor:  1  step number:  70 total reward:  0.05999999999999939  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  53 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.2047133333333335 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.33831036855958
maxi score, test score, baseline:  -0.2047133333333335 0.6886666666666666 0.6886666666666666
actions average: 
K:  0  action  0 :  tensor([0.5285, 0.0034, 0.0944, 0.0941, 0.0964, 0.1001, 0.0831],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0046,     0.9795,     0.0023,     0.0040,     0.0016,     0.0007,
            0.0073], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1144, 0.0115, 0.6090, 0.0721, 0.0555, 0.0654, 0.0721],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2746, 0.0039, 0.1053, 0.3223, 0.1023, 0.1123, 0.0792],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2908, 0.0055, 0.0436, 0.0724, 0.4858, 0.0511, 0.0508],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1577, 0.0042, 0.1737, 0.1296, 0.1185, 0.3199, 0.0964],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2130, 0.0941, 0.1256, 0.1317, 0.1280, 0.1216, 0.1859],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  63 total reward:  0.09333333333333271  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  35.84712028503418
printing an ep nov before normalisation:  42.35368248076433
printing an ep nov before normalisation:  30.22952872073997
siam score:  -0.8653991
actor:  1 policy actor:  1  step number:  53 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.2047133333333335 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2047133333333335 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2047133333333335 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2047133333333335 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.2047133333333335 0.6886666666666666 0.6886666666666666
printing an ep nov before normalisation:  34.16274094325503
printing an ep nov before normalisation:  39.35506384769075
maxi score, test score, baseline:  -0.2071400000000001 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2071400000000001 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.585]
 [0.716]
 [0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.585]] [[31.095]
 [32.812]
 [31.095]
 [31.095]
 [31.095]
 [31.095]
 [31.095]] [[1.729]
 [1.969]
 [1.729]
 [1.729]
 [1.729]
 [1.729]
 [1.729]]
maxi score, test score, baseline:  -0.2071400000000001 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2071400000000001 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.568]
 [0.411]
 [0.393]
 [0.411]
 [0.411]
 [0.411]] [[31.604]
 [28.719]
 [26.327]
 [31.983]
 [26.327]
 [26.327]
 [26.327]] [[0.954]
 [1.008]
 [0.763]
 [0.953]
 [0.763]
 [0.763]
 [0.763]]
printing an ep nov before normalisation:  0.03269708850993425
actor:  1 policy actor:  1  step number:  53 total reward:  0.2266666666666659  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.2071400000000001 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.58108019389759
maxi score, test score, baseline:  -0.2071400000000001 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.328]
 [0.43 ]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]] [[35.359]
 [34.902]
 [35.359]
 [35.359]
 [35.359]
 [35.359]
 [35.359]] [[1.709]
 [1.777]
 [1.709]
 [1.709]
 [1.709]
 [1.709]
 [1.709]]
maxi score, test score, baseline:  -0.2071400000000001 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.437]
 [0.499]
 [0.429]
 [0.428]
 [0.446]
 [0.431]
 [0.431]] [[41.619]
 [38.753]
 [41.154]
 [42.05 ]
 [41.981]
 [42.199]
 [42.221]] [[1.838]
 [1.713]
 [1.799]
 [1.856]
 [1.869]
 [1.869]
 [1.871]]
printing an ep nov before normalisation:  42.61346302725633
printing an ep nov before normalisation:  32.87363673365814
printing an ep nov before normalisation:  38.93335287390512
Printing some Q and Qe and total Qs values:  [[0.777]
 [0.865]
 [0.777]
 [0.777]
 [0.777]
 [0.777]
 [0.777]] [[44.811]
 [41.657]
 [44.811]
 [44.811]
 [44.811]
 [44.811]
 [44.811]] [[0.777]
 [0.865]
 [0.777]
 [0.777]
 [0.777]
 [0.777]
 [0.777]]
maxi score, test score, baseline:  -0.2071400000000001 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2071400000000001 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.298012256622314
actor:  1 policy actor:  1  step number:  56 total reward:  0.4199999999999995  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  18.38096160934862
actor:  1 policy actor:  1  step number:  69 total reward:  0.026666666666665728  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  48.807122342117225
Printing some Q and Qe and total Qs values:  [[0.826]
 [0.819]
 [0.848]
 [0.887]
 [0.722]
 [0.887]
 [0.887]] [[18.846]
 [20.596]
 [18.629]
 [17.752]
 [18.708]
 [17.752]
 [17.752]] [[0.826]
 [0.819]
 [0.848]
 [0.887]
 [0.722]
 [0.887]
 [0.887]]
printing an ep nov before normalisation:  30.73191061106039
siam score:  -0.86524534
actor:  0 policy actor:  0  step number:  61 total reward:  0.22666666666666646  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.20468666666666682 0.6886666666666666 0.6886666666666666
maxi score, test score, baseline:  -0.20468666666666682 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.6972377076371
actions average: 
K:  2  action  0 :  tensor([0.5319, 0.0389, 0.0674, 0.0901, 0.1306, 0.0605, 0.0805],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0062,     0.9607,     0.0076,     0.0033,     0.0008,     0.0111,
            0.0102], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0496, 0.0043, 0.5635, 0.0432, 0.0542, 0.2454, 0.0398],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2968, 0.0591, 0.1179, 0.2983, 0.0803, 0.0760, 0.0716],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.3461, 0.0680, 0.0906, 0.1169, 0.1876, 0.0977, 0.0932],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1583, 0.0654, 0.1493, 0.0807, 0.0759, 0.4280, 0.0424],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2309, 0.0406, 0.1091, 0.1319, 0.1640, 0.1300, 0.1935],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  21.77843607359712
actor:  1 policy actor:  1  step number:  50 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.20468666666666682 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.614713191986084
maxi score, test score, baseline:  -0.20468666666666682 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.20468666666666682 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.3733333333333333  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.20468666666666682 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.20468666666666682 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.939]
 [0.939]
 [0.939]
 [0.939]
 [0.939]
 [0.939]
 [0.939]] [[48.789]
 [48.789]
 [48.789]
 [48.789]
 [48.789]
 [48.789]
 [48.789]] [[0.939]
 [0.939]
 [0.939]
 [0.939]
 [0.939]
 [0.939]
 [0.939]]
maxi score, test score, baseline:  -0.20468666666666682 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.66378639467608
maxi score, test score, baseline:  -0.20468666666666682 0.6886666666666666 0.6886666666666666
maxi score, test score, baseline:  -0.20468666666666682 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.20468666666666682 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.971]
 [1.006]
 [0.971]
 [0.971]
 [0.971]
 [0.971]
 [0.971]] [[28.344]
 [31.055]
 [28.344]
 [28.344]
 [28.344]
 [28.344]
 [28.344]] [[0.971]
 [1.006]
 [0.971]
 [0.971]
 [0.971]
 [0.971]
 [0.971]]
maxi score, test score, baseline:  -0.20468666666666682 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 37.671619762450675
maxi score, test score, baseline:  -0.20468666666666682 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.20468666666666682 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.018]
 [-0.041]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]] [[ 0.   ]
 [45.809]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.723]
 [ 0.691]
 [-0.723]
 [-0.723]
 [-0.723]
 [-0.723]
 [-0.723]]
printing an ep nov before normalisation:  24.541073789442606
printing an ep nov before normalisation:  29.656695046530935
actions average: 
K:  3  action  0 :  tensor([0.4412, 0.0068, 0.0856, 0.1095, 0.1480, 0.1140, 0.0949],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0051, 0.9782, 0.0015, 0.0043, 0.0021, 0.0015, 0.0073],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1768, 0.0282, 0.3260, 0.0946, 0.0925, 0.1882, 0.0937],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2052, 0.0962, 0.0672, 0.4024, 0.0777, 0.0434, 0.1079],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1499, 0.0105, 0.0743, 0.0729, 0.5534, 0.0736, 0.0654],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1383, 0.0292, 0.1428, 0.1211, 0.1004, 0.3801, 0.0881],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1741, 0.3991, 0.1057, 0.0640, 0.0601, 0.0359, 0.1610],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  40.117454710170215
printing an ep nov before normalisation:  42.79851876387009
maxi score, test score, baseline:  -0.20468666666666682 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8559355
line 256 mcts: sample exp_bonus 42.794852318502066
actor:  1 policy actor:  1  step number:  44 total reward:  0.4999999999999999  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.20468666666666682 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2046866666666668 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.2046866666666668 0.6886666666666666 0.6886666666666666
maxi score, test score, baseline:  -0.2046866666666668 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  39 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.072]
 [0.134]
 [0.072]
 [0.072]
 [0.072]
 [0.072]
 [0.072]] [[26.872]
 [33.819]
 [26.872]
 [26.872]
 [26.872]
 [26.872]
 [26.872]] [[0.392]
 [0.629]
 [0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.392]]
printing an ep nov before normalisation:  40.378917013094345
maxi score, test score, baseline:  -0.2046866666666668 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.355313254099386
Printing some Q and Qe and total Qs values:  [[0.095]
 [0.279]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]] [[27.424]
 [36.697]
 [27.424]
 [27.424]
 [27.424]
 [27.424]
 [27.424]] [[0.404]
 [0.792]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]]
actor:  1 policy actor:  1  step number:  70 total reward:  0.1133333333333324  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2046866666666668 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.55730336896492
maxi score, test score, baseline:  -0.2046866666666668 0.6886666666666666 0.6886666666666666
printing an ep nov before normalisation:  35.00757406180819
printing an ep nov before normalisation:  28.457598691223335
actor:  1 policy actor:  1  step number:  46 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  43 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  39.27356942709188
siam score:  -0.86206555
maxi score, test score, baseline:  -0.20178000000000013 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.048924446105957
printing an ep nov before normalisation:  44.36783693039786
actor:  0 policy actor:  0  step number:  55 total reward:  0.2933333333333331  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  36.753785426639126
maxi score, test score, baseline:  -0.19919333333333347 0.6886666666666666 0.6886666666666666
maxi score, test score, baseline:  -0.19919333333333347 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.19919333333333347 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.626919280364454
maxi score, test score, baseline:  -0.19919333333333347 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.58507997044007
printing an ep nov before normalisation:  49.17923468123079
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.19919333333333347 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85825205
maxi score, test score, baseline:  -0.19919333333333347 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.299]
 [0.287]
 [0.287]
 [0.287]
 [0.287]
 [0.287]
 [0.287]] [[44.895]
 [45.745]
 [45.745]
 [45.745]
 [45.745]
 [45.745]
 [45.745]] [[1.743]
 [1.785]
 [1.785]
 [1.785]
 [1.785]
 [1.785]
 [1.785]]
actions average: 
K:  4  action  0 :  tensor([0.4251, 0.0250, 0.0963, 0.1120, 0.1219, 0.1171, 0.1026],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0123, 0.9077, 0.0059, 0.0297, 0.0042, 0.0025, 0.0378],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2086, 0.0018, 0.4090, 0.0891, 0.0766, 0.1054, 0.1095],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1450, 0.0070, 0.0876, 0.3471, 0.1413, 0.1666, 0.1055],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1578, 0.0669, 0.0872, 0.1215, 0.3025, 0.1691, 0.0951],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2540, 0.0042, 0.1204, 0.0557, 0.0515, 0.4711, 0.0432],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1490, 0.0778, 0.0685, 0.1673, 0.1127, 0.0978, 0.3269],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.19919333333333342 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.37002648247613
Printing some Q and Qe and total Qs values:  [[-0.035]
 [-0.241]
 [-0.05 ]
 [-0.05 ]
 [-0.032]
 [-0.05 ]
 [-0.05 ]] [[70.136]
 [59.118]
 [48.716]
 [48.716]
 [71.612]
 [48.716]
 [48.716]] [[0.736]
 [0.336]
 [0.345]
 [0.345]
 [0.765]
 [0.345]
 [0.345]]
maxi score, test score, baseline:  -0.19919333333333342 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.931144984486394
maxi score, test score, baseline:  -0.19919333333333342 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.19919333333333342 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  70 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.19919333333333342 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.19919333333333342 0.6886666666666666 0.6886666666666666
maxi score, test score, baseline:  -0.19919333333333342 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.39999999999999936  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  51.95511157780501
maxi score, test score, baseline:  -0.19919333333333342 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.44686510875523
printing an ep nov before normalisation:  46.07432964228329
printing an ep nov before normalisation:  67.18141596689811
printing an ep nov before normalisation:  67.42098423338217
maxi score, test score, baseline:  -0.19919333333333342 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.19919333333333342 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.19919333333333342 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.19919333333333342 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.222059418875396
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.19919333333333342 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.806795120239258
actions average: 
K:  3  action  0 :  tensor([0.5094, 0.0450, 0.0600, 0.1125, 0.0979, 0.0651, 0.1101],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0074, 0.9641, 0.0067, 0.0040, 0.0018, 0.0032, 0.0127],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1631, 0.0494, 0.3739, 0.1221, 0.0867, 0.1092, 0.0954],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1624, 0.0155, 0.1112, 0.3902, 0.1012, 0.1138, 0.1057],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2436, 0.0319, 0.0973, 0.1425, 0.2722, 0.1119, 0.1005],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1121, 0.0294, 0.0951, 0.1246, 0.0922, 0.4759, 0.0706],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1505, 0.1119, 0.0635, 0.1034, 0.0804, 0.0470, 0.4432],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  50 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 32.213974469327795
actions average: 
K:  3  action  0 :  tensor([0.6211, 0.0036, 0.0619, 0.0966, 0.0939, 0.0627, 0.0603],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0179, 0.9327, 0.0088, 0.0102, 0.0043, 0.0034, 0.0228],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1657, 0.0444, 0.3745, 0.1107, 0.1140, 0.1073, 0.0834],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1265, 0.0229, 0.0982, 0.4351, 0.1246, 0.0952, 0.0976],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.3090, 0.0557, 0.0583, 0.1290, 0.3229, 0.0692, 0.0558],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1975, 0.0392, 0.1971, 0.1295, 0.1388, 0.2155, 0.0825],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.3024, 0.0951, 0.1418, 0.1327, 0.1052, 0.0783, 0.1444],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  37.02668795337031
maxi score, test score, baseline:  -0.19919333333333342 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.19919333333333342 0.6886666666666666 0.6886666666666666
maxi score, test score, baseline:  -0.19919333333333342 0.6886666666666666 0.6886666666666666
maxi score, test score, baseline:  -0.19919333333333342 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.19919333333333342 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.19919333333333342 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.19919333333333342 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.19919333333333342 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.19919333333333342 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.19919333333333342 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.19919333333333342 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.21333333333333315  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.19919333333333342 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.19919333333333342 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.19919333333333342 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.16571987703156
printing an ep nov before normalisation:  28.76523846356402
actor:  1 policy actor:  1  step number:  47 total reward:  0.4266666666666663  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.19919333333333342 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.920649954107496
maxi score, test score, baseline:  -0.19919333333333342 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.19919333333333342 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.00017279802676739564
actor:  0 policy actor:  0  step number:  50 total reward:  0.4066666666666663  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  47 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.19363333333333344 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.90884087866555
Printing some Q and Qe and total Qs values:  [[0.143]
 [0.143]
 [0.143]
 [0.143]
 [0.143]
 [0.143]
 [0.143]] [[44.114]
 [44.114]
 [44.114]
 [44.114]
 [44.114]
 [44.114]
 [44.114]] [[0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]]
printing an ep nov before normalisation:  37.88427731425971
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.19363333333333344 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.19363333333333344 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.19363333333333344 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.323639438004776
siam score:  -0.8478122
actor:  1 policy actor:  1  step number:  55 total reward:  0.2933333333333328  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.19363333333333344 0.6886666666666666 0.6886666666666666
actor:  1 policy actor:  1  step number:  61 total reward:  0.13333333333333308  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.19363333333333344 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.19363333333333344 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.24697448165033
printing an ep nov before normalisation:  38.43295482510024
Printing some Q and Qe and total Qs values:  [[0.34 ]
 [0.527]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]] [[45.118]
 [46.368]
 [45.118]
 [45.118]
 [45.118]
 [45.118]
 [45.118]] [[0.978]
 [1.194]
 [0.978]
 [0.978]
 [0.978]
 [0.978]
 [0.978]]
maxi score, test score, baseline:  -0.19363333333333344 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.69894829937604
maxi score, test score, baseline:  -0.19363333333333344 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.829379682171016
actor:  0 policy actor:  0  step number:  56 total reward:  0.2733333333333331  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.1910866666666668 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  53.227649988605656
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.1910866666666668 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.66 ]
 [0.684]
 [0.663]
 [0.636]
 [0.629]
 [0.615]
 [0.687]] [[31.541]
 [32.161]
 [32.163]
 [33.536]
 [34.787]
 [34.928]
 [30.941]] [[0.66 ]
 [0.684]
 [0.663]
 [0.636]
 [0.629]
 [0.615]
 [0.687]]
printing an ep nov before normalisation:  49.61450046814846
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  42.36085109472548
printing an ep nov before normalisation:  33.00806521366212
maxi score, test score, baseline:  -0.1910866666666668 0.6886666666666666 0.6886666666666666
maxi score, test score, baseline:  -0.1910866666666668 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.1910866666666668 0.6886666666666666 0.6886666666666666
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.8419769
maxi score, test score, baseline:  -0.1910866666666668 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.16639497542341
maxi score, test score, baseline:  -0.1910866666666668 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  63 total reward:  0.22666666666666568  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  72 total reward:  0.04666666666666608  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.18654000000000012 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.18654000000000012 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.823620497886694
maxi score, test score, baseline:  -0.18654000000000012 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.347076852559574
actor:  1 policy actor:  1  step number:  69 total reward:  0.18666666666666598  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.18654000000000012 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.190444034281803
maxi score, test score, baseline:  -0.18654000000000012 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.4199999999999996  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.18654000000000012 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.469348472037325
printing an ep nov before normalisation:  2.461937469888653e-05
actor:  1 policy actor:  1  step number:  44 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  49.89463710821883
maxi score, test score, baseline:  -0.18654000000000012 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.18654000000000012 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.18654000000000012 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.18654000000000012 0.6886666666666666 0.6886666666666666
maxi score, test score, baseline:  -0.18654000000000012 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.27999999999999914  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.18654000000000012 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.66168513055394
maxi score, test score, baseline:  -0.18654000000000012 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.1933333333333327  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.18654000000000012 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.18654000000000012 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.041]
 [-0.02 ]
 [-0.019]
 [-0.006]
 [-0.039]
 [-0.019]
 [-0.002]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.041]
 [-0.02 ]
 [-0.019]
 [-0.006]
 [-0.039]
 [-0.019]
 [-0.002]]
maxi score, test score, baseline:  -0.18654000000000012 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.701]
 [0.55 ]
 [0.583]
 [0.553]
 [0.566]
 [0.557]] [[14.427]
 [14.839]
 [17.336]
 [17.105]
 [17.336]
 [17.843]
 [17.617]] [[0.848]
 [1.162]
 [1.088]
 [1.114]
 [1.091]
 [1.12 ]
 [1.104]]
actor:  1 policy actor:  1  step number:  59 total reward:  0.3333333333333326  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  37.3621288166606
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.18654000000000012 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.18654000000000012 0.6886666666666666 0.6886666666666666
Printing some Q and Qe and total Qs values:  [[0.202]
 [0.324]
 [0.202]
 [0.311]
 [0.205]
 [0.205]
 [0.204]] [[29.603]
 [30.223]
 [29.737]
 [28.687]
 [29.896]
 [29.569]
 [29.468]] [[0.874]
 [1.023]
 [0.88 ]
 [0.943]
 [0.89 ]
 [0.876]
 [0.87 ]]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.264]
 [0.014]
 [0.064]
 [0.014]
 [0.014]
 [0.014]] [[43.935]
 [46.433]
 [43.935]
 [40.626]
 [43.935]
 [43.935]
 [43.935]] [[1.42 ]
 [1.801]
 [1.42 ]
 [1.296]
 [1.42 ]
 [1.42 ]
 [1.42 ]]
maxi score, test score, baseline:  -0.18654000000000012 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.18654000000000012 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8483331
printing an ep nov before normalisation:  30.87098105841847
maxi score, test score, baseline:  -0.18654000000000012 0.6886666666666666 0.6886666666666666
printing an ep nov before normalisation:  28.876994916305243
actor:  1 policy actor:  1  step number:  52 total reward:  0.44666666666666666  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.629]
 [0.721]
 [0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]] [[28.511]
 [32.409]
 [28.511]
 [28.511]
 [28.511]
 [28.511]
 [28.511]] [[1.29 ]
 [1.565]
 [1.29 ]
 [1.29 ]
 [1.29 ]
 [1.29 ]
 [1.29 ]]
printing an ep nov before normalisation:  30.770734496539465
actor:  1 policy actor:  1  step number:  55 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.18654000000000012 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3399999999999994  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.18654000000000012 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.18654000000000012 0.6886666666666666 0.6886666666666666
maxi score, test score, baseline:  -0.18654000000000012 0.6886666666666666 0.6886666666666666
maxi score, test score, baseline:  -0.18654000000000012 0.6886666666666666 0.6886666666666666
printing an ep nov before normalisation:  40.865069353226396
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.18654000000000012 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.168175220489502
maxi score, test score, baseline:  -0.18654000000000012 0.6886666666666666 0.6886666666666666
maxi score, test score, baseline:  -0.18654000000000012 0.6886666666666666 0.6886666666666666
printing an ep nov before normalisation:  39.99797187146205
actor:  0 policy actor:  0  step number:  63 total reward:  0.01333333333333242  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  33.79567401780843
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.696466775931846
printing an ep nov before normalisation:  40.02236957859416
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
Printing some Q and Qe and total Qs values:  [[0.31 ]
 [0.436]
 [0.31 ]
 [0.31 ]
 [0.31 ]
 [0.31 ]
 [0.31 ]] [[29.517]
 [34.583]
 [29.517]
 [29.517]
 [29.517]
 [29.517]
 [29.517]] [[0.553]
 [0.748]
 [0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.553]]
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.556]
 [0.685]
 [0.512]
 [0.519]
 [0.486]
 [0.555]
 [0.638]] [[21.631]
 [19.627]
 [25.019]
 [25.094]
 [24.4  ]
 [25.735]
 [21.91 ]] [[1.738]
 [1.757]
 [1.878]
 [1.89 ]
 [1.819]
 [1.96 ]
 [1.835]]
actor:  1 policy actor:  1  step number:  56 total reward:  0.28666666666666607  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
printing an ep nov before normalisation:  16.750483848235422
Printing some Q and Qe and total Qs values:  [[-0.033]
 [-0.033]
 [-0.033]
 [ 0.054]
 [-0.033]
 [-0.033]
 [-0.033]] [[65.51 ]
 [65.51 ]
 [65.51 ]
 [67.318]
 [65.51 ]
 [65.51 ]
 [65.51 ]] [[1.113]
 [1.113]
 [1.113]
 [1.254]
 [1.113]
 [1.113]
 [1.113]]
actor:  1 policy actor:  1  step number:  71 total reward:  0.18666666666666654  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.207728647858936
printing an ep nov before normalisation:  52.20874786376953
actor:  1 policy actor:  1  step number:  66 total reward:  0.15333333333333277  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.21721213155221
printing an ep nov before normalisation:  36.93196781279636
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
printing an ep nov before normalisation:  40.12988335701949
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
line 256 mcts: sample exp_bonus 33.968404949081105
actor:  1 policy actor:  1  step number:  52 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  65 total reward:  0.1999999999999994  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  78 total reward:  0.07333333333333203  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 30.11175501966473
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.190107293977675
printing an ep nov before normalisation:  42.99233272114902
printing an ep nov before normalisation:  38.506308351370315
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  42 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  0.27999999999999936  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  38 total reward:  0.5666666666666667  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.68770855126055
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.463711588254515
printing an ep nov before normalisation:  36.990441837610554
printing an ep nov before normalisation:  15.529041863758541
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  0.5133333333333339  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  57.09760897236957
actor:  1 policy actor:  1  step number:  53 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  1  action  0 :  tensor([0.4993, 0.0275, 0.0834, 0.0808, 0.1050, 0.0979, 0.1060],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0050, 0.9740, 0.0052, 0.0029, 0.0023, 0.0031, 0.0075],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1434, 0.0031, 0.4755, 0.0733, 0.0819, 0.1429, 0.0798],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2009, 0.0109, 0.0813, 0.2393, 0.1628, 0.2262, 0.0786],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1980, 0.0029, 0.0969, 0.0833, 0.4195, 0.1147, 0.0846],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1323, 0.0053, 0.1169, 0.0865, 0.1094, 0.4776, 0.0720],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1624, 0.0424, 0.1016, 0.1059, 0.1138, 0.1339, 0.3399],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  46.80791650274138
actor:  1 policy actor:  1  step number:  59 total reward:  0.19999999999999918  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  32.03619956970215
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.98464554101676
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.7  ]
 [0.677]
 [0.68 ]
 [0.657]
 [0.677]
 [0.682]] [[27.209]
 [27.912]
 [24.987]
 [26.225]
 [29.589]
 [24.987]
 [27.1  ]] [[1.108]
 [1.17 ]
 [1.053]
 [1.096]
 [1.18 ]
 [1.053]
 [1.126]]
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
actor:  1 policy actor:  1  step number:  51 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.41118163598708
Printing some Q and Qe and total Qs values:  [[0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]] [[37.326]
 [37.326]
 [37.326]
 [37.326]
 [37.326]
 [37.326]
 [37.326]] [[1.731]
 [1.731]
 [1.731]
 [1.731]
 [1.731]
 [1.731]
 [1.731]]
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.18451333333333345 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.54530263107396
printing an ep nov before normalisation:  25.329065321689317
Printing some Q and Qe and total Qs values:  [[1.007]
 [1.014]
 [1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]] [[34.628]
 [28.629]
 [34.628]
 [34.628]
 [34.628]
 [34.628]
 [34.628]] [[1.007]
 [1.014]
 [1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]]
actor:  0 policy actor:  0  step number:  59 total reward:  0.0799999999999994  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  50 total reward:  0.2999999999999994  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  58 total reward:  0.0733333333333327  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.1802066666666668 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.89769769501897
maxi score, test score, baseline:  -0.1802066666666668 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.18314012168963
siam score:  -0.8398184
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.62971353530884
maxi score, test score, baseline:  -0.1802066666666668 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.409231776181734
printing an ep nov before normalisation:  29.2093563079834
printing an ep nov before normalisation:  24.622266894124152
maxi score, test score, baseline:  -0.1802066666666668 0.6886666666666666 0.6886666666666666
actor:  1 policy actor:  1  step number:  42 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  2  action  0 :  tensor([0.5065, 0.0470, 0.0921, 0.0848, 0.1203, 0.0778, 0.0715],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0112, 0.9329, 0.0149, 0.0114, 0.0051, 0.0066, 0.0180],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2507, 0.0123, 0.2797, 0.1281, 0.1288, 0.1060, 0.0943],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1338, 0.1171, 0.0791, 0.4368, 0.0652, 0.0808, 0.0871],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2849, 0.0129, 0.0861, 0.0887, 0.3789, 0.0752, 0.0733],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1710, 0.0015, 0.1281, 0.0935, 0.1166, 0.3959, 0.0934],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2333, 0.0047, 0.0841, 0.1156, 0.0848, 0.0794, 0.3982],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  0.017229977069064262
actor:  1 policy actor:  1  step number:  46 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  44 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  54.00202013135293
maxi score, test score, baseline:  -0.1802066666666668 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.1802066666666668 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.816]
 [0.835]
 [0.767]
 [0.767]
 [0.767]
 [0.767]
 [0.767]] [[66.796]
 [54.571]
 [62.958]
 [62.958]
 [62.958]
 [62.958]
 [62.958]] [[0.816]
 [0.835]
 [0.767]
 [0.767]
 [0.767]
 [0.767]
 [0.767]]
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.514]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]] [[33.663]
 [35.194]
 [33.663]
 [33.663]
 [33.663]
 [33.663]
 [33.663]] [[1.087]
 [1.211]
 [1.087]
 [1.087]
 [1.087]
 [1.087]
 [1.087]]
maxi score, test score, baseline:  -0.1802066666666668 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.1802066666666668 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.537]
 [0.601]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]] [[35.246]
 [35.4  ]
 [31.068]
 [31.068]
 [31.068]
 [31.068]
 [31.068]] [[1.359]
 [1.429]
 [1.181]
 [1.181]
 [1.181]
 [1.181]
 [1.181]]
maxi score, test score, baseline:  -0.1802066666666668 0.6886666666666666 0.6886666666666666
maxi score, test score, baseline:  -0.1802066666666668 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.1802066666666668 0.6886666666666666 0.6886666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
actor:  1 policy actor:  1  step number:  55 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  30.05996152538021
actor:  1 policy actor:  1  step number:  58 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  37.42475590644653
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.1802066666666668 0.6886666666666666 0.6886666666666666
Printing some Q and Qe and total Qs values:  [[0.7  ]
 [0.837]
 [0.7  ]
 [0.668]
 [0.7  ]
 [0.7  ]
 [0.758]] [[33.647]
 [35.355]
 [33.647]
 [32.358]
 [33.647]
 [33.647]
 [34.075]] [[0.7  ]
 [0.837]
 [0.7  ]
 [0.668]
 [0.7  ]
 [0.7  ]
 [0.758]]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  31.610537535935652
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.49168226320623
printing an ep nov before normalisation:  39.26324651207615
printing an ep nov before normalisation:  31.56667026496831
Printing some Q and Qe and total Qs values:  [[0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]] [[30.361]
 [30.361]
 [30.361]
 [30.361]
 [30.361]
 [30.361]
 [30.361]] [[0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]]
maxi score, test score, baseline:  -0.1802066666666668 0.6886666666666666 0.6886666666666666
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  30.361080516526712
printing an ep nov before normalisation:  26.465641742748808
Printing some Q and Qe and total Qs values:  [[1.025]
 [1.025]
 [1.025]
 [1.025]
 [1.025]
 [1.025]
 [1.025]] [[28.22]
 [28.22]
 [28.22]
 [28.22]
 [28.22]
 [28.22]
 [28.22]] [[1.025]
 [1.025]
 [1.025]
 [1.025]
 [1.025]
 [1.025]
 [1.025]]
maxi score, test score, baseline:  -0.1802066666666668 0.6886666666666666 0.6886666666666666
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333333  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  52.22576802209105
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
siam score:  -0.8346222
Printing some Q and Qe and total Qs values:  [[ 0.067]
 [-0.01 ]
 [-0.056]
 [-0.009]
 [-0.002]
 [-0.05 ]
 [-0.012]] [[42.809]
 [40.449]
 [37.423]
 [40.188]
 [39.954]
 [40.921]
 [37.076]] [[0.298]
 [0.199]
 [0.124]
 [0.197]
 [0.202]
 [0.163]
 [0.165]]
printing an ep nov before normalisation:  29.48089599609375
Printing some Q and Qe and total Qs values:  [[-0.004]
 [ 0.098]
 [-0.001]
 [-0.009]
 [-0.008]
 [-0.014]
 [-0.014]] [[34.887]
 [42.07 ]
 [38.366]
 [34.316]
 [33.044]
 [31.923]
 [31.75 ]] [[0.142]
 [0.297]
 [0.172]
 [0.132]
 [0.124]
 [0.11 ]
 [0.109]]
maxi score, test score, baseline:  -0.12438000000000012 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.12438000000000012 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.55603330866697
siam score:  -0.8361059
Printing some Q and Qe and total Qs values:  [[0.221]
 [0.291]
 [0.221]
 [0.221]
 [0.221]
 [0.221]
 [0.221]] [[50.459]
 [54.363]
 [50.459]
 [50.459]
 [50.459]
 [50.459]
 [50.459]] [[0.492]
 [0.603]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]]
actor:  1 policy actor:  1  step number:  56 total reward:  0.1666666666666663  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.12438000000000012 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.12438000000000012 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.12438000000000012 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.0933333333333326  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8388361
actor:  1 policy actor:  1  step number:  69 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.12438000000000012 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.12438000000000012 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.12438000000000012 0.6900000000000002 0.6900000000000002
Printing some Q and Qe and total Qs values:  [[0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]] [[35.248]
 [35.248]
 [35.248]
 [35.248]
 [35.248]
 [35.248]
 [35.248]] [[0.804]
 [0.804]
 [0.804]
 [0.804]
 [0.804]
 [0.804]
 [0.804]]
Printing some Q and Qe and total Qs values:  [[0.749]
 [0.82 ]
 [0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.749]] [[41.519]
 [35.518]
 [41.519]
 [41.519]
 [41.519]
 [41.519]
 [41.519]] [[0.749]
 [0.82 ]
 [0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.749]]
printing an ep nov before normalisation:  31.922870287264317
Printing some Q and Qe and total Qs values:  [[0.795]
 [0.952]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]] [[37.069]
 [28.503]
 [33.563]
 [33.563]
 [33.563]
 [33.563]
 [33.563]] [[0.795]
 [0.952]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]]
printing an ep nov before normalisation:  36.4857318424496
maxi score, test score, baseline:  -0.12438000000000012 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.12438000000000012 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.12438000000000012 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  48 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  0.00035037310738061933
actor:  1 policy actor:  1  step number:  44 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.038]
 [0.192]
 [0.127]
 [0.028]
 [0.031]
 [0.027]
 [0.127]] [[33.768]
 [39.12 ]
 [39.827]
 [31.84 ]
 [34.29 ]
 [31.481]
 [39.827]] [[0.719]
 [1.101]
 [1.065]
 [0.627]
 [0.734]
 [0.611]
 [1.065]]
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]] [[36.073]
 [36.073]
 [36.073]
 [36.073]
 [36.073]
 [36.073]
 [36.073]] [[1.24]
 [1.24]
 [1.24]
 [1.24]
 [1.24]
 [1.24]
 [1.24]]
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]] [[44.596]
 [44.596]
 [44.596]
 [44.596]
 [44.596]
 [44.596]
 [44.596]] [[1.718]
 [1.718]
 [1.718]
 [1.718]
 [1.718]
 [1.718]
 [1.718]]
maxi score, test score, baseline:  -0.12180666666666676 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.12180666666666676 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.656617844693514
Printing some Q and Qe and total Qs values:  [[0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]] [[0.003]
 [0.002]
 [0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]]
actor:  0 policy actor:  0  step number:  53 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  29.780184057673317
maxi score, test score, baseline:  -0.11919333333333346 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4466666666666663  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  53 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  42.79775128168374
line 256 mcts: sample exp_bonus 36.593653558214406
printing an ep nov before normalisation:  43.431387815744124
printing an ep nov before normalisation:  52.97628078621424
printing an ep nov before normalisation:  49.303918997973355
maxi score, test score, baseline:  -0.11919333333333346 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.023]
 [-0.008]
 [-0.023]
 [-0.031]
 [-0.023]
 [-0.023]
 [-0.023]] [[56.645]
 [55.877]
 [56.645]
 [55.859]
 [56.645]
 [56.645]
 [56.645]] [[1.587]
 [1.56 ]
 [1.587]
 [1.537]
 [1.587]
 [1.587]
 [1.587]]
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.437]
 [0.385]
 [0.303]
 [0.301]
 [0.385]
 [0.385]] [[31.977]
 [31.842]
 [31.977]
 [26.198]
 [26.299]
 [31.977]
 [31.977]] [[0.385]
 [0.437]
 [0.385]
 [0.303]
 [0.301]
 [0.385]
 [0.385]]
printing an ep nov before normalisation:  33.34336638065293
printing an ep nov before normalisation:  41.97240417901074
printing an ep nov before normalisation:  47.706630932962625
printing an ep nov before normalisation:  36.85540671831821
printing an ep nov before normalisation:  37.69829432554898
maxi score, test score, baseline:  -0.11919333333333346 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.11919333333333346 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.55 ]
 [0.642]
 [0.588]
 [0.585]
 [0.62 ]
 [0.559]
 [0.585]] [[28.934]
 [30.074]
 [29.676]
 [29.515]
 [29.004]
 [30.576]
 [29.515]] [[1.512]
 [1.682]
 [1.601]
 [1.587]
 [1.587]
 [1.633]
 [1.587]]
maxi score, test score, baseline:  -0.11919333333333346 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.23623898756215
maxi score, test score, baseline:  -0.11919333333333346 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.11919333333333346 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  50 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  63 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.11683333333333344 0.6900000000000002 0.6900000000000002
Printing some Q and Qe and total Qs values:  [[-0.035]
 [-0.008]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.035]
 [-0.008]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]]
actions average: 
K:  2  action  0 :  tensor([0.5480, 0.0502, 0.0698, 0.0670, 0.0796, 0.0671, 0.1182],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0305, 0.9126, 0.0098, 0.0098, 0.0073, 0.0062, 0.0238],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1012, 0.0260, 0.6401, 0.0336, 0.0296, 0.0766, 0.0930],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.3003, 0.0322, 0.1045, 0.1428, 0.1480, 0.1351, 0.1372],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1686, 0.0093, 0.0833, 0.0748, 0.4605, 0.1310, 0.0725],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1158, 0.0096, 0.1287, 0.0762, 0.0888, 0.4929, 0.0881],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1575, 0.3026, 0.0837, 0.0911, 0.0876, 0.1093, 0.1681],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.11683333333333344 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.11683333333333344 0.6900000000000002 0.6900000000000002
printing an ep nov before normalisation:  39.485628604888916
actor:  1 policy actor:  1  step number:  55 total reward:  0.21333333333333304  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.11683333333333344 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.424]
 [0.562]
 [0.46 ]
 [0.515]
 [0.502]
 [0.469]
 [0.471]] [[24.701]
 [17.62 ]
 [24.195]
 [23.653]
 [23.422]
 [24.403]
 [23.578]] [[2.174]
 [1.811]
 [2.175]
 [2.191]
 [2.161]
 [2.198]
 [2.142]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.2799999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  54 total reward:  0.2733333333333332  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.11683333333333344 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.11683333333333344 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  2.7447757133813866e-06
actor:  1 policy actor:  1  step number:  51 total reward:  0.3466666666666661  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.11683333333333344 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.11683333333333344 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.11683333333333344 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.107]
 [-0.107]
 [ 0.   ]] [[ 0.013]
 [ 0.012]
 [ 0.013]
 [ 0.013]
 [34.012]
 [34.012]
 [ 0.013]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [1.148]
 [1.148]
 [0.   ]]
maxi score, test score, baseline:  -0.11683333333333344 0.6900000000000002 0.6900000000000002
printing an ep nov before normalisation:  48.63841253226659
actor:  1 policy actor:  1  step number:  52 total reward:  0.2999999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.013]
 [-0.   ]
 [-0.017]
 [-0.008]
 [-0.022]
 [-0.216]
 [-0.006]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.013]
 [-0.   ]
 [-0.017]
 [-0.008]
 [-0.022]
 [-0.216]
 [-0.006]]
Printing some Q and Qe and total Qs values:  [[0.153]
 [0.153]
 [0.153]
 [0.153]
 [0.153]
 [0.153]
 [0.153]] [[58.91]
 [58.91]
 [58.91]
 [58.91]
 [58.91]
 [58.91]
 [58.91]] [[1.198]
 [1.198]
 [1.198]
 [1.198]
 [1.198]
 [1.198]
 [1.198]]
maxi score, test score, baseline:  -0.11683333333333346 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.11683333333333346 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.337]
 [0.433]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]] [[48.893]
 [54.257]
 [48.893]
 [48.893]
 [48.893]
 [48.893]
 [48.893]] [[1.634]
 [2.003]
 [1.634]
 [1.634]
 [1.634]
 [1.634]
 [1.634]]
printing an ep nov before normalisation:  48.73277845079698
printing an ep nov before normalisation:  39.09987447887972
Printing some Q and Qe and total Qs values:  [[0.467]
 [0.554]
 [0.529]
 [0.425]
 [0.426]
 [0.451]
 [0.529]] [[27.529]
 [30.661]
 [28.457]
 [28.735]
 [28.778]
 [29.108]
 [28.457]] [[1.494]
 [1.838]
 [1.632]
 [1.551]
 [1.555]
 [1.607]
 [1.632]]
maxi score, test score, baseline:  -0.11683333333333346 0.6900000000000002 0.6900000000000002
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.718]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]] [[25.191]
 [30.427]
 [25.191]
 [25.191]
 [25.191]
 [25.191]
 [25.191]] [[1.393]
 [1.79 ]
 [1.393]
 [1.393]
 [1.393]
 [1.393]
 [1.393]]
printing an ep nov before normalisation:  30.018888693604026
line 256 mcts: sample exp_bonus 41.727338459912744
maxi score, test score, baseline:  -0.11683333333333346 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.788]
 [0.794]
 [0.766]
 [0.766]
 [0.73 ]
 [0.681]
 [0.795]] [[28.193]
 [27.798]
 [26.573]
 [26.573]
 [35.808]
 [38.077]
 [26.302]] [[1.477]
 [1.465]
 [1.381]
 [1.381]
 [1.763]
 [1.816]
 [1.398]]
actor:  1 policy actor:  1  step number:  38 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.11683333333333346 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  0.29999999999999927  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  37.144917821949655
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
Printing some Q and Qe and total Qs values:  [[-0.013]
 [ 0.013]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]] [[67.039]
 [63.481]
 [65.201]
 [65.201]
 [65.201]
 [65.201]
 [65.201]] [[1.426]
 [1.318]
 [1.355]
 [1.355]
 [1.355]
 [1.355]
 [1.355]]
maxi score, test score, baseline:  -0.11683333333333346 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.11683333333333346 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.3298837244534931
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.427]
 [0.382]
 [0.379]
 [0.379]
 [0.378]
 [0.379]] [[25.875]
 [23.292]
 [25.185]
 [25.978]
 [26.173]
 [26.446]
 [26.165]] [[2.3  ]
 [2.009]
 [2.215]
 [2.316]
 [2.342]
 [2.378]
 [2.342]]
Printing some Q and Qe and total Qs values:  [[0.65 ]
 [0.676]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]] [[35.945]
 [40.223]
 [35.945]
 [35.945]
 [35.945]
 [35.945]
 [35.945]] [[2.355]
 [2.676]
 [2.355]
 [2.355]
 [2.355]
 [2.355]
 [2.355]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.5666666666666669  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]] [[37.101]
 [37.101]
 [37.101]
 [37.101]
 [37.101]
 [37.101]
 [37.101]] [[1.945]
 [1.945]
 [1.945]
 [1.945]
 [1.945]
 [1.945]
 [1.945]]
maxi score, test score, baseline:  -0.11683333333333346 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.24209527939098
maxi score, test score, baseline:  -0.11683333333333346 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.11683333333333346 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.0799999999999994  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.11683333333333346 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.5321, 0.0151, 0.0986, 0.0829, 0.1048, 0.0754, 0.0912],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0045, 0.9462, 0.0173, 0.0062, 0.0016, 0.0017, 0.0224],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2200, 0.0269, 0.1546, 0.1487, 0.1705, 0.1465, 0.1329],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1420, 0.0350, 0.1121, 0.4088, 0.1115, 0.1037, 0.0869],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2116, 0.0346, 0.0947, 0.1054, 0.3645, 0.0997, 0.0895],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1364, 0.0075, 0.2220, 0.0776, 0.0781, 0.3994, 0.0789],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1215, 0.0919, 0.1150, 0.1473, 0.0828, 0.0858, 0.3556],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.11683333333333346 0.6900000000000002 0.6900000000000002
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.567]
 [0.557]
 [0.549]
 [0.504]
 [0.491]
 [0.549]] [[22.835]
 [26.576]
 [26.971]
 [22.835]
 [20.414]
 [20.941]
 [22.835]] [[1.116]
 [1.3  ]
 [1.308]
 [1.116]
 [0.964]
 [0.975]
 [1.116]]
maxi score, test score, baseline:  -0.11683333333333346 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.41333333333333333  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  53 total reward:  0.2933333333333331  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  62.968942856351816
actions average: 
K:  4  action  0 :  tensor([0.5911, 0.0155, 0.0775, 0.0779, 0.1001, 0.0595, 0.0784],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0174, 0.9050, 0.0114, 0.0301, 0.0040, 0.0041, 0.0280],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2383, 0.0020, 0.3883, 0.0911, 0.1053, 0.1121, 0.0630],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2775, 0.0814, 0.0921, 0.2535, 0.0917, 0.0752, 0.1286],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2353, 0.0010, 0.0406, 0.1526, 0.4860, 0.0261, 0.0584],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0582, 0.0984, 0.0746, 0.0345, 0.0302, 0.6608, 0.0433],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2341, 0.0662, 0.1256, 0.1130, 0.1122, 0.0995, 0.2493],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.1142466666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.576]
 [0.457]
 [0.457]
 [0.457]
 [0.457]
 [0.5  ]] [[48.803]
 [51.387]
 [48.803]
 [48.803]
 [48.803]
 [48.803]
 [49.566]] [[0.457]
 [0.576]
 [0.457]
 [0.457]
 [0.457]
 [0.457]
 [0.5  ]]
Printing some Q and Qe and total Qs values:  [[0.708]
 [0.841]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]] [[29.741]
 [32.491]
 [29.741]
 [29.741]
 [29.741]
 [29.741]
 [29.741]] [[0.708]
 [0.841]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]]
printing an ep nov before normalisation:  29.827341571445043
maxi score, test score, baseline:  -0.11424666666666682 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.706]
 [0.706]
 [0.706]
 [0.706]
 [0.706]
 [0.706]
 [0.706]] [[32.31]
 [32.31]
 [32.31]
 [32.31]
 [32.31]
 [32.31]
 [32.31]] [[0.706]
 [0.706]
 [0.706]
 [0.706]
 [0.706]
 [0.706]
 [0.706]]
maxi score, test score, baseline:  -0.11424666666666682 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.11424666666666682 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  46 total reward:  0.3133333333333328  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 0.0
actor:  1 policy actor:  1  step number:  58 total reward:  0.3399999999999994  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.11162000000000012 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.11162000000000012 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.602]
 [0.49 ]
 [0.454]
 [0.49 ]
 [0.463]
 [0.49 ]] [[44.362]
 [39.765]
 [44.362]
 [44.986]
 [44.362]
 [48.052]
 [44.362]] [[1.039]
 [1.059]
 [1.039]
 [1.015]
 [1.039]
 [1.087]
 [1.039]]
Printing some Q and Qe and total Qs values:  [[ 0.003]
 [ 0.007]
 [ 0.006]
 [ 0.005]
 [-0.118]
 [-0.118]
 [-0.118]] [[ 0.41 ]
 [ 0.39 ]
 [ 0.396]
 [ 0.422]
 [30.01 ]
 [30.01 ]
 [30.01 ]] [[0.005]
 [0.008]
 [0.007]
 [0.007]
 [0.469]
 [0.469]
 [0.469]]
actor:  0 policy actor:  0  step number:  69 total reward:  0.23999999999999988  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.10914000000000015 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.857]
 [0.857]
 [0.857]
 [0.857]
 [0.857]
 [0.857]
 [0.857]] [[41.54]
 [41.54]
 [41.54]
 [41.54]
 [41.54]
 [41.54]
 [41.54]] [[0.857]
 [0.857]
 [0.857]
 [0.857]
 [0.857]
 [0.857]
 [0.857]]
actor:  1 policy actor:  1  step number:  60 total reward:  0.08666666666666611  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  47 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  59 total reward:  0.2666666666666664  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.1066066666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.1066066666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.1066066666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8313795
printing an ep nov before normalisation:  30.493484872628855
printing an ep nov before normalisation:  22.932376861572266
maxi score, test score, baseline:  -0.1066066666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.39670888213961
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.1066066666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.1066066666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  59.40803822603447
maxi score, test score, baseline:  -0.1066066666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  61 total reward:  0.4133333333333329  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  3  action  0 :  tensor([0.6068, 0.0384, 0.0649, 0.0584, 0.0693, 0.0824, 0.0800],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0109, 0.9358, 0.0099, 0.0070, 0.0054, 0.0159, 0.0152],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.2150, 0.1334, 0.2602, 0.1131, 0.0777, 0.0989, 0.1018],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0850, 0.0191, 0.0418, 0.6484, 0.0779, 0.0597, 0.0681],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2569, 0.1122, 0.1000, 0.1292, 0.1939, 0.1064, 0.1013],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.3117, 0.0011, 0.1377, 0.0930, 0.1003, 0.2931, 0.0632],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2346, 0.1685, 0.0645, 0.0720, 0.0339, 0.0451, 0.3813],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  37.68253790083845
printing an ep nov before normalisation:  28.569698333740234
maxi score, test score, baseline:  -0.1066066666666668 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.1066066666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.591446631299654
Printing some Q and Qe and total Qs values:  [[0.234]
 [0.374]
 [0.219]
 [0.234]
 [0.234]
 [0.234]
 [0.234]] [[37.177]
 [43.852]
 [41.409]
 [37.177]
 [37.177]
 [37.177]
 [37.177]] [[0.432]
 [0.638]
 [0.459]
 [0.432]
 [0.432]
 [0.432]
 [0.432]]
maxi score, test score, baseline:  -0.1066066666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.1066066666666668 0.6900000000000002 0.6900000000000002
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
actor:  1 policy actor:  1  step number:  67 total reward:  0.13333333333333286  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.1066066666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.13124107510301
maxi score, test score, baseline:  -0.1066066666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.24666666666666603  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  2  action  0 :  tensor([0.4402, 0.0187, 0.1023, 0.0996, 0.1169, 0.0970, 0.1254],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0215, 0.9200, 0.0073, 0.0106, 0.0073, 0.0042, 0.0290],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1224, 0.0393, 0.4705, 0.0651, 0.0659, 0.1488, 0.0881],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1479, 0.0310, 0.0880, 0.3997, 0.1148, 0.0915, 0.1271],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.4005, 0.0170, 0.0634, 0.0816, 0.2907, 0.0754, 0.0715],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1666, 0.0288, 0.1689, 0.0474, 0.0367, 0.5082, 0.0434],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2009, 0.0800, 0.0824, 0.1597, 0.1211, 0.0964, 0.2595],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.1066066666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.447463035583496
Printing some Q and Qe and total Qs values:  [[0.276]
 [0.473]
 [0.276]
 [0.276]
 [0.276]
 [0.276]
 [0.276]] [[34.236]
 [40.163]
 [34.236]
 [34.236]
 [34.236]
 [34.236]
 [34.236]] [[0.599]
 [0.924]
 [0.599]
 [0.599]
 [0.599]
 [0.599]
 [0.599]]
maxi score, test score, baseline:  -0.1066066666666668 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.1066066666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.38666666666666616  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.1066066666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.1066066666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[36.764]
 [36.764]
 [36.764]
 [36.764]
 [36.764]
 [36.764]
 [36.764]] [[0.275]
 [0.275]
 [0.275]
 [0.275]
 [0.275]
 [0.275]
 [0.275]]
printing an ep nov before normalisation:  47.18923086649095
maxi score, test score, baseline:  -0.1066066666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  41 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.1066066666666668 0.6900000000000002 0.6900000000000002
printing an ep nov before normalisation:  26.730838150626372
Printing some Q and Qe and total Qs values:  [[0.078]
 [0.295]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.068]] [[33.874]
 [35.992]
 [34.933]
 [34.933]
 [34.933]
 [34.933]
 [33.412]] [[0.62 ]
 [0.903]
 [0.711]
 [0.711]
 [0.711]
 [0.711]
 [0.596]]
printing an ep nov before normalisation:  48.670981499978545
printing an ep nov before normalisation:  31.818670789399906
maxi score, test score, baseline:  -0.1066066666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  39 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.1066066666666668 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.1066066666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.151856422424316
actor:  1 policy actor:  1  step number:  53 total reward:  0.22666666666666624  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.1066066666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.1066066666666668 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.1066066666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.1066066666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.1066066666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.09459879160683
actor:  1 policy actor:  1  step number:  56 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.1066066666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8385001
actor:  0 policy actor:  0  step number:  49 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  62 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  6.239300773813738e-05
printing an ep nov before normalisation:  57.18828237328467
printing an ep nov before normalisation:  29.551256765516282
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.601]
 [0.538]
 [0.563]
 [0.471]
 [0.47 ]
 [0.473]] [[36.041]
 [45.944]
 [38.932]
 [43.655]
 [25.887]
 [25.661]
 [25.628]] [[0.638]
 [0.8  ]
 [0.695]
 [0.749]
 [0.55 ]
 [0.548]
 [0.551]]
printing an ep nov before normalisation:  49.45170425631943
maxi score, test score, baseline:  -0.10399333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.779]
 [0.788]
 [0.788]
 [0.779]
 [0.779]
 [0.779]
 [0.779]] [[24.686]
 [34.257]
 [28.745]
 [24.686]
 [24.686]
 [24.686]
 [24.686]] [[0.98 ]
 [1.066]
 [1.021]
 [0.98 ]
 [0.98 ]
 [0.98 ]
 [0.98 ]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.10399333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.10399333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.10399333333333347 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.10399333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.51 ]
 [0.531]
 [0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.521]] [[29.332]
 [31.275]
 [28.402]
 [28.402]
 [28.402]
 [28.402]
 [28.402]] [[1.015]
 [1.1  ]
 [0.996]
 [0.996]
 [0.996]
 [0.996]
 [0.996]]
printing an ep nov before normalisation:  33.005556670170215
maxi score, test score, baseline:  -0.10399333333333347 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.10399333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.6611629826532
actor:  1 policy actor:  1  step number:  50 total reward:  0.5133333333333335  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  60 total reward:  0.05999999999999939  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  39 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.10187333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.207]
 [0.276]
 [0.203]
 [0.203]
 [0.203]
 [0.203]
 [0.203]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.207]
 [0.276]
 [0.203]
 [0.203]
 [0.203]
 [0.203]
 [0.203]]
line 256 mcts: sample exp_bonus 0.0
deleting a thread, now have 2 threads
Frames:  153072 train batches done:  17937 episodes:  3970
siam score:  -0.83691454
maxi score, test score, baseline:  -0.10187333333333344 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.10187333333333344 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  30 total reward:  0.6333333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.10128666666666679 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.10128666666666679 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.10128666666666679 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.10128666666666679 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.10128666666666679 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.35999999999999943  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.10128666666666679 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.10128666666666679 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.10128666666666679 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.10128666666666679 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.10128666666666679 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.10128666666666679 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.10128666666666679 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.10128666666666679 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.10128666666666679 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.10128666666666679 0.6900000000000002 0.6900000000000002
printing an ep nov before normalisation:  2.7356315968063427e-06
actor:  1 policy actor:  1  step number:  42 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.10128666666666679 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.4066666666666666  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  4  action  0 :  tensor([0.5549, 0.0895, 0.0711, 0.0685, 0.0748, 0.0772, 0.0640],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0083, 0.9429, 0.0050, 0.0099, 0.0053, 0.0050, 0.0235],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1402, 0.1309, 0.2575, 0.0904, 0.0578, 0.1496, 0.1737],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1564, 0.0541, 0.0746, 0.4203, 0.0690, 0.1078, 0.1177],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2720, 0.0704, 0.0834, 0.1136, 0.2637, 0.1016, 0.0953],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2769, 0.0272, 0.1400, 0.0617, 0.1055, 0.3302, 0.0585],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1545, 0.2522, 0.0586, 0.0824, 0.0558, 0.0700, 0.3264],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.10128666666666679 0.6900000000000002 0.6900000000000002
printing an ep nov before normalisation:  20.439429127502226
actions average: 
K:  1  action  0 :  tensor([0.5518, 0.0057, 0.0878, 0.0806, 0.0932, 0.0811, 0.0999],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0048, 0.9583, 0.0038, 0.0072, 0.0010, 0.0012, 0.0237],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1745, 0.0014, 0.5102, 0.0629, 0.0576, 0.1410, 0.0525],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1517, 0.1780, 0.0728, 0.3195, 0.0463, 0.0516, 0.1802],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2490, 0.0199, 0.0858, 0.0965, 0.3789, 0.0839, 0.0859],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2131, 0.0066, 0.1128, 0.1261, 0.1078, 0.3264, 0.1073],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2015, 0.1414, 0.1071, 0.1600, 0.0973, 0.1154, 0.1773],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  33.8919472694397
printing an ep nov before normalisation:  38.745315074920654
siam score:  -0.83967364
maxi score, test score, baseline:  -0.10128666666666679 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.10128666666666679 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.4073113168533
actor:  0 policy actor:  0  step number:  60 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.09890000000000011 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.903]
 [0.949]
 [0.903]
 [0.903]
 [0.903]
 [0.903]
 [0.903]] [[31.573]
 [32.573]
 [31.573]
 [31.573]
 [31.573]
 [31.573]
 [31.573]] [[0.903]
 [0.949]
 [0.903]
 [0.903]
 [0.903]
 [0.903]
 [0.903]]
maxi score, test score, baseline:  -0.09890000000000011 0.6900000000000002 0.6900000000000002
printing an ep nov before normalisation:  28.707781178656052
actor:  0 policy actor:  0  step number:  51 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  4  action  0 :  tensor([0.4717, 0.0527, 0.0720, 0.1046, 0.1323, 0.0811, 0.0856],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0084, 0.9354, 0.0045, 0.0108, 0.0034, 0.0028, 0.0346],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1963, 0.0728, 0.2142, 0.1386, 0.1321, 0.1331, 0.1129],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1782, 0.0228, 0.0676, 0.4305, 0.1020, 0.0698, 0.1290],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2293, 0.0648, 0.0622, 0.0791, 0.4305, 0.0729, 0.0612],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1895, 0.0988, 0.1068, 0.1277, 0.1239, 0.2620, 0.0913],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1082, 0.1907, 0.0967, 0.1043, 0.0395, 0.0607, 0.3999],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  42.13945165979218
maxi score, test score, baseline:  -0.09610000000000013 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.14467402906863
actor:  0 policy actor:  0  step number:  45 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.09338000000000012 0.6900000000000002 0.6900000000000002
printing an ep nov before normalisation:  45.82920750075144
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.09338000000000012 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.252]
 [0.345]
 [0.252]
 [0.252]
 [0.252]
 [0.252]
 [0.252]] [[43.591]
 [43.206]
 [43.591]
 [43.591]
 [43.591]
 [43.591]
 [43.591]] [[1.344]
 [1.418]
 [1.344]
 [1.344]
 [1.344]
 [1.344]
 [1.344]]
maxi score, test score, baseline:  -0.09338000000000012 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.607078619947426
printing an ep nov before normalisation:  18.919038772583008
maxi score, test score, baseline:  -0.09338000000000012 0.6900000000000002 0.6900000000000002
actions average: 
K:  3  action  0 :  tensor([0.5236, 0.0127, 0.0974, 0.0974, 0.0998, 0.0819, 0.0873],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0127, 0.9352, 0.0076, 0.0144, 0.0056, 0.0062, 0.0183],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.2089, 0.0091, 0.3376, 0.1030, 0.0928, 0.1106, 0.1380],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2095, 0.0763, 0.1349, 0.1463, 0.1357, 0.1525, 0.1448],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1559, 0.0050, 0.1114, 0.1274, 0.3525, 0.1448, 0.1031],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1600, 0.0285, 0.1209, 0.0978, 0.0931, 0.4472, 0.0524],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1893, 0.0848, 0.0790, 0.1077, 0.0643, 0.0936, 0.3814],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  60 total reward:  0.17999999999999927  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  44.938220019796184
maxi score, test score, baseline:  -0.09338000000000012 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.09338000000000012 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.93784236907959
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  40 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.09338000000000012 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.09338000000000012 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3333333333333328  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.09338000000000012 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.202]
 [ 0.   ]
 [-0.202]
 [-0.202]
 [-0.202]
 [-0.202]
 [-0.202]] [[23.666]
 [ 0.   ]
 [23.666]
 [23.666]
 [23.666]
 [23.666]
 [23.666]] [[1.282]
 [0.   ]
 [1.282]
 [1.282]
 [1.282]
 [1.282]
 [1.282]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.09338000000000012 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.09338000000000012 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.88477016905761
printing an ep nov before normalisation:  44.61228847503662
maxi score, test score, baseline:  -0.09338000000000012 0.6900000000000002 0.6900000000000002
printing an ep nov before normalisation:  25.767066478729248
actor:  0 policy actor:  0  step number:  44 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.09315333333333346 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.09315333333333346 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  40 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  47.67986166862542
actor:  1 policy actor:  1  step number:  37 total reward:  0.5866666666666669  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  54.31151764188995
maxi score, test score, baseline:  -0.09315333333333346 0.6900000000000002 0.6900000000000002
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  19.170779278943062
actor:  1 policy actor:  1  step number:  51 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  26.279236124643436
printing an ep nov before normalisation:  35.90903266084079
maxi score, test score, baseline:  -0.09315333333333346 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.94313942245345
actor:  0 policy actor:  0  step number:  52 total reward:  0.35333333333333283  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  42.838053703308105
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.09044666666666679 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.09044666666666679 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.09044666666666679 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.560742532020114
maxi score, test score, baseline:  -0.09044666666666679 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.09044666666666679 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.84992486
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.09044666666666679 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.09044666666666679 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.09044666666666679 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.70820791414011
maxi score, test score, baseline:  -0.09044666666666677 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.09044666666666677 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.64015176760579
printing an ep nov before normalisation:  32.20566009038508
maxi score, test score, baseline:  -0.09044666666666677 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  0.4933333333333332  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  24.588334560394287
actor:  1 policy actor:  1  step number:  57 total reward:  0.42666666666666664  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8413945
maxi score, test score, baseline:  -0.09044666666666677 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.281]
 [0.282]
 [0.281]
 [0.281]
 [0.281]
 [0.281]
 [0.281]] [[37.249]
 [43.418]
 [37.249]
 [37.249]
 [37.249]
 [37.249]
 [37.249]] [[0.959]
 [1.166]
 [0.959]
 [0.959]
 [0.959]
 [0.959]
 [0.959]]
printing an ep nov before normalisation:  52.944629969247906
maxi score, test score, baseline:  -0.09044666666666677 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.83663744
maxi score, test score, baseline:  -0.09044666666666677 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.75079914536243
maxi score, test score, baseline:  -0.09044666666666677 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.25333333333333274  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.09044666666666677 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.5888, 0.0517, 0.0571, 0.0689, 0.0956, 0.0811, 0.0568],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0120, 0.9343, 0.0081, 0.0121, 0.0014, 0.0045, 0.0276],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1419, 0.0751, 0.4951, 0.0519, 0.0827, 0.0899, 0.0635],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0271, 0.2503, 0.0154, 0.6125, 0.0402, 0.0106, 0.0439],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.3355, 0.0075, 0.0856, 0.0909, 0.2952, 0.0986, 0.0866],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1054, 0.0078, 0.0738, 0.0867, 0.1147, 0.5514, 0.0602],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2453, 0.1802, 0.0759, 0.1559, 0.0850, 0.0867, 0.1709],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.09044666666666677 0.6900000000000002 0.6900000000000002
actor:  1 policy actor:  1  step number:  53 total reward:  0.2666666666666664  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]]
maxi score, test score, baseline:  -0.09044666666666677 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.320199738494175
maxi score, test score, baseline:  -0.09044666666666677 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.909712865971905
actor:  1 policy actor:  1  step number:  41 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  39.22856423584239
printing an ep nov before normalisation:  35.05391592847449
maxi score, test score, baseline:  -0.09044666666666677 0.6900000000000002 0.6900000000000002
actor:  0 policy actor:  0  step number:  41 total reward:  0.5866666666666669  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.08727333333333347 0.6900000000000002 0.6900000000000002
line 256 mcts: sample exp_bonus 60.675957317250784
Printing some Q and Qe and total Qs values:  [[0.23 ]
 [0.597]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]] [[68.938]
 [67.233]
 [64.606]
 [64.606]
 [64.606]
 [64.606]
 [64.606]] [[1.929]
 [2.239]
 [2.055]
 [2.055]
 [2.055]
 [2.055]
 [2.055]]
maxi score, test score, baseline:  -0.08727333333333347 0.6900000000000002 0.6900000000000002
actor:  0 policy actor:  0  step number:  42 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.08456666666666679 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.08456666666666679 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.08456666666666679 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.8488566546761
Printing some Q and Qe and total Qs values:  [[ 0.   ]
 [ 0.   ]
 [-0.168]
 [ 0.   ]
 [-0.168]
 [ 0.   ]
 [ 0.   ]] [[ 0.   ]
 [ 0.   ]
 [33.871]
 [ 0.   ]
 [33.871]
 [ 0.   ]
 [ 0.   ]] [[0.   ]
 [0.   ]
 [1.417]
 [0.   ]
 [1.417]
 [0.   ]
 [0.   ]]
printing an ep nov before normalisation:  53.50509591307146
printing an ep nov before normalisation:  33.41875550306702
maxi score, test score, baseline:  -0.08456666666666679 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.08456666666666679 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.08456666666666679 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.650078227907734
printing an ep nov before normalisation:  37.32121495859769
actor:  0 policy actor:  0  step number:  53 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  28.08984253870757
Printing some Q and Qe and total Qs values:  [[0.23 ]
 [0.303]
 [0.282]
 [0.297]
 [0.235]
 [0.231]
 [0.238]] [[22.54 ]
 [30.673]
 [28.263]
 [29.473]
 [20.7  ]
 [21.011]
 [20.957]] [[0.371]
 [0.536]
 [0.489]
 [0.517]
 [0.356]
 [0.356]
 [0.362]]
maxi score, test score, baseline:  -0.08198000000000015 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.896445715712666
printing an ep nov before normalisation:  25.201688136767434
maxi score, test score, baseline:  -0.08198000000000015 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.08198000000000015 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.25902053545305
maxi score, test score, baseline:  -0.08198000000000015 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.58432102763693
maxi score, test score, baseline:  -0.08198000000000015 0.6900000000000002 0.6900000000000002
actor:  1 policy actor:  1  step number:  52 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.08198000000000015 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.08198000000000015 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.50560759646175
printing an ep nov before normalisation:  39.842214840845415
siam score:  -0.8455333
maxi score, test score, baseline:  -0.08198000000000015 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.593661829694838
actor:  1 policy actor:  1  step number:  48 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.08198000000000015 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.08198000000000015 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.08198000000000015 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.08198000000000015 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.55914386030559
printing an ep nov before normalisation:  33.85922908782959
actor:  0 policy actor:  0  step number:  50 total reward:  0.4066666666666662  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.0791666666666668 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.0791666666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.352]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]] [[56.093]
 [66.797]
 [56.093]
 [56.093]
 [56.093]
 [56.093]
 [56.093]] [[1.46 ]
 [1.695]
 [1.46 ]
 [1.46 ]
 [1.46 ]
 [1.46 ]
 [1.46 ]]
actor:  1 policy actor:  1  step number:  34 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.0791666666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.5089, 0.0784, 0.0619, 0.0859, 0.0901, 0.0789, 0.0960],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0184, 0.9352, 0.0056, 0.0114, 0.0062, 0.0048, 0.0183],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1292, 0.1833, 0.3425, 0.0796, 0.0524, 0.1419, 0.0711],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1462, 0.0595, 0.0676, 0.4094, 0.0709, 0.1016, 0.1450],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1770, 0.0771, 0.0716, 0.1340, 0.3841, 0.0860, 0.0702],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1577, 0.1379, 0.1039, 0.0786, 0.0579, 0.3805, 0.0834],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2025, 0.2539, 0.0878, 0.1217, 0.1221, 0.0856, 0.1264],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.0791666666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0791666666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.02  0.878 0.02  0.02  0.02  0.02  0.02 ]
maxi score, test score, baseline:  -0.0791666666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0791666666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.84015185
maxi score, test score, baseline:  -0.0791666666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 46.426077728301756
Printing some Q and Qe and total Qs values:  [[ 0.493]
 [ 0.354]
 [-0.09 ]
 [ 0.312]
 [ 0.309]
 [-0.103]
 [ 0.298]] [[48.396]
 [34.287]
 [35.453]
 [30.167]
 [31.597]
 [33.505]
 [32.403]] [[1.519]
 [0.908]
 [0.503]
 [0.728]
 [0.773]
 [0.425]
 [0.789]]
maxi score, test score, baseline:  -0.0791666666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0791666666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.43487127871377
actor:  1 policy actor:  1  step number:  56 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  34.025640550762425
siam score:  -0.8335305
maxi score, test score, baseline:  -0.0791666666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]] [[32.015]
 [32.015]
 [32.015]
 [32.015]
 [32.015]
 [32.015]
 [32.015]] [[1.594]
 [1.594]
 [1.594]
 [1.594]
 [1.594]
 [1.594]
 [1.594]]
maxi score, test score, baseline:  -0.0791666666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0791666666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0791666666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.095538800136055
printing an ep nov before normalisation:  0.0009214384428446465
actor:  1 policy actor:  1  step number:  55 total reward:  0.2533333333333331  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.011]
 [-0.013]
 [-0.005]
 [-0.016]
 [-0.017]
 [-0.011]
 [-0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.011]
 [-0.013]
 [-0.005]
 [-0.016]
 [-0.017]
 [-0.011]
 [-0.001]]
maxi score, test score, baseline:  -0.08119333333333345 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  60 total reward:  0.20666666666666567  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.08100666666666678 0.6900000000000002 0.6900000000000002
printing an ep nov before normalisation:  34.07870443095252
maxi score, test score, baseline:  -0.08100666666666678 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.15198415960715
maxi score, test score, baseline:  -0.08100666666666678 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.591573771858148
siam score:  -0.8336934
maxi score, test score, baseline:  -0.08100666666666678 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.54]
 [0.54]
 [0.54]
 [0.54]
 [0.54]
 [0.54]
 [0.54]] [[31.856]
 [31.856]
 [31.856]
 [31.856]
 [31.856]
 [31.856]
 [31.856]] [[1.898]
 [1.898]
 [1.898]
 [1.898]
 [1.898]
 [1.898]
 [1.898]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.31333333333333313  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.08100666666666678 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.08100666666666678 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.08100666666666678 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.826692150056076
maxi score, test score, baseline:  -0.08100666666666678 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  57 total reward:  0.38666666666666605  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  32.647404271481825
printing an ep nov before normalisation:  49.88960668374302
Printing some Q and Qe and total Qs values:  [[0.767]
 [0.822]
 [0.742]
 [0.74 ]
 [0.763]
 [0.701]
 [0.763]] [[33.822]
 [33.932]
 [31.192]
 [30.301]
 [36.62 ]
 [30.596]
 [36.62 ]] [[1.277]
 [1.336]
 [1.171]
 [1.141]
 [1.359]
 [1.112]
 [1.359]]
maxi score, test score, baseline:  -0.08100666666666678 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.08100666666666678 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.531146331129634
printing an ep nov before normalisation:  29.173482997910938
maxi score, test score, baseline:  -0.08100666666666678 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 25.31926963394759
maxi score, test score, baseline:  -0.08100666666666678 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.3466666666666661  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.08100666666666678 0.6900000000000002 0.6900000000000002
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.08100666666666678 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.08100666666666678 0.6900000000000002 0.6900000000000002
siam score:  -0.8325072
printing an ep nov before normalisation:  36.132221486447314
printing an ep nov before normalisation:  34.28594917292181
maxi score, test score, baseline:  -0.08100666666666678 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.08100666666666678 0.6900000000000002 0.6900000000000002
printing an ep nov before normalisation:  25.69548031873245
actor:  1 policy actor:  1  step number:  71 total reward:  0.38666666666666616  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  57 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.08100666666666678 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8327135
printing an ep nov before normalisation:  43.9096564121132
printing an ep nov before normalisation:  43.99744563455924
maxi score, test score, baseline:  -0.08100666666666678 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.83278763
maxi score, test score, baseline:  -0.08100666666666678 0.6900000000000002 0.6900000000000002
printing an ep nov before normalisation:  27.883931355036616
actor:  1 policy actor:  1  step number:  36 total reward:  0.5266666666666667  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.08100666666666678 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  51 total reward:  0.3199999999999993  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.08100666666666678 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.08100666666666678 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.1098081236699
printing an ep nov before normalisation:  39.08050746409461
maxi score, test score, baseline:  -0.08100666666666678 0.6900000000000002 0.6900000000000002
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]] [[30.341]
 [30.341]
 [30.341]
 [30.341]
 [30.341]
 [30.341]
 [30.341]] [[1.587]
 [1.587]
 [1.587]
 [1.587]
 [1.587]
 [1.587]
 [1.587]]
printing an ep nov before normalisation:  30.884779452805137
printing an ep nov before normalisation:  35.40173061496033
maxi score, test score, baseline:  -0.08100666666666678 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.13510808950654
actor:  1 policy actor:  1  step number:  49 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.08100666666666678 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.571]
 [0.566]
 [0.566]
 [0.566]
 [0.517]
 [0.566]] [[27.815]
 [35.708]
 [27.815]
 [27.815]
 [27.815]
 [29.588]
 [27.815]] [[2.106]
 [2.571]
 [2.106]
 [2.106]
 [2.106]
 [2.16 ]
 [2.106]]
maxi score, test score, baseline:  -0.08100666666666678 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  49.52703068971638
maxi score, test score, baseline:  -0.08100666666666678 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.0810066666666668 0.6900000000000002 0.6900000000000002
Printing some Q and Qe and total Qs values:  [[-0.015]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.017]] [[10.387]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [26.153]] [[ 0.207]
 [-0.172]
 [-0.172]
 [-0.172]
 [-0.172]
 [-0.172]
 [ 0.792]]
printing an ep nov before normalisation:  0.015154098360881108
actor:  0 policy actor:  0  step number:  43 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.333
UNIT TEST: sample policy line 217 mcts : [0.02  0.449 0.02  0.469 0.    0.041 0.   ]
printing an ep nov before normalisation:  50.15711203453323
siam score:  -0.84433794
Printing some Q and Qe and total Qs values:  [[0.289]
 [0.289]
 [0.289]
 [0.289]
 [0.289]
 [0.289]
 [0.289]] [[46.547]
 [46.547]
 [46.547]
 [46.547]
 [46.547]
 [46.547]
 [46.547]] [[0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]]
printing an ep nov before normalisation:  33.119046555896034
printing an ep nov before normalisation:  61.52596914246961
maxi score, test score, baseline:  -0.07807333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.18964973224291
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.538]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]] [[28.299]
 [39.096]
 [28.299]
 [28.299]
 [28.299]
 [28.299]
 [28.299]] [[1.105]
 [1.719]
 [1.105]
 [1.105]
 [1.105]
 [1.105]
 [1.105]]
maxi score, test score, baseline:  -0.07807333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.48  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  22.94527530670166
printing an ep nov before normalisation:  41.6113343768934
maxi score, test score, baseline:  -0.07807333333333347 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.07807333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.07807333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.089974948215854
maxi score, test score, baseline:  -0.07807333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.576]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]] [[24.517]
 [20.675]
 [22.966]
 [22.966]
 [22.966]
 [22.966]
 [22.966]] [[1.267]
 [1.257]
 [1.167]
 [1.167]
 [1.167]
 [1.167]
 [1.167]]
actor:  1 policy actor:  1  step number:  40 total reward:  0.4999999999999999  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.026]
 [ 0.004]
 [-0.021]
 [-0.027]
 [-0.027]
 [-0.024]
 [-0.029]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.026]
 [ 0.004]
 [-0.021]
 [-0.027]
 [-0.027]
 [-0.024]
 [-0.029]]
maxi score, test score, baseline:  -0.07807333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.07807333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  74 total reward:  0.05999999999999961  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.07807333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.07807333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.8443127
maxi score, test score, baseline:  -0.07807333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.517670154571533
printing an ep nov before normalisation:  30.329148879107766
maxi score, test score, baseline:  -0.07807333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.730061531066895
actor:  1 policy actor:  1  step number:  49 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.07807333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.07807333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.3199999999999994  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.629]
 [0.674]
 [0.124]
 [0.629]
 [0.629]
 [0.629]
 [0.629]] [[64.408]
 [65.733]
 [67.034]
 [64.408]
 [64.408]
 [64.408]
 [64.408]] [[1.815]
 [1.892]
 [1.375]
 [1.815]
 [1.815]
 [1.815]
 [1.815]]
maxi score, test score, baseline:  -0.07807333333333347 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.07807333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.84022063
Printing some Q and Qe and total Qs values:  [[0.716]
 [0.8  ]
 [0.716]
 [0.717]
 [0.717]
 [0.777]
 [0.717]] [[19.41 ]
 [30.989]
 [15.19 ]
 [15.069]
 [15.09 ]
 [26.016]
 [15.178]] [[0.716]
 [0.8  ]
 [0.716]
 [0.717]
 [0.717]
 [0.777]
 [0.717]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.07807333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.292372173972822
maxi score, test score, baseline:  -0.07807333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.07807333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.07807333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  83 total reward:  0.05333333333333168  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  42.546922693446334
printing an ep nov before normalisation:  34.495277404785156
Printing some Q and Qe and total Qs values:  [[0.629]
 [0.722]
 [0.679]
 [0.607]
 [0.648]
 [0.608]
 [0.707]] [[23.138]
 [26.722]
 [24.153]
 [20.881]
 [22.486]
 [21.877]
 [28.602]] [[1.028]
 [1.257]
 [1.116]
 [0.919]
 [1.022]
 [0.958]
 [1.315]]
maxi score, test score, baseline:  -0.07596666666666681 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  39 total reward:  0.4933333333333333  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  44.653803633497134
maxi score, test score, baseline:  -0.07596666666666681 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.07596666666666681 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.813]
 [0.846]
 [0.843]
 [0.813]
 [0.814]
 [0.813]
 [0.789]] [[30.794]
 [29.828]
 [30.577]
 [30.349]
 [30.603]
 [30.833]
 [32.284]] [[0.813]
 [0.846]
 [0.843]
 [0.813]
 [0.814]
 [0.813]
 [0.789]]
maxi score, test score, baseline:  -0.07596666666666681 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.4314, 0.0734, 0.0765, 0.0937, 0.1333, 0.0804, 0.1114],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0070, 0.9471, 0.0035, 0.0062, 0.0020, 0.0020, 0.0323],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1390, 0.0512, 0.4674, 0.0750, 0.0652, 0.0671, 0.1351],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1539, 0.0231, 0.0970, 0.4217, 0.0887, 0.0981, 0.1175],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1894, 0.0486, 0.0464, 0.0708, 0.5200, 0.0465, 0.0783],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1341, 0.0058, 0.1561, 0.1073, 0.1146, 0.3939, 0.0882],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1822, 0.1251, 0.1707, 0.1208, 0.1025, 0.1251, 0.1737],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  54 total reward:  0.23333333333333306  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.07639333333333345 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.588]
 [0.588]
 [0.588]
 [0.588]
 [0.588]
 [0.588]] [[41.877]
 [39.427]
 [39.427]
 [39.427]
 [39.427]
 [39.427]
 [39.427]] [[1.391]
 [1.445]
 [1.445]
 [1.445]
 [1.445]
 [1.445]
 [1.445]]
maxi score, test score, baseline:  -0.07639333333333345 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  39 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  34.55057760522709
maxi score, test score, baseline:  -0.07639333333333345 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.07639333333333345 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.66067491444963
maxi score, test score, baseline:  -0.07639333333333345 0.6900000000000002 0.6900000000000002
actor:  1 policy actor:  1  step number:  41 total reward:  0.3999999999999999  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]] [[32.728]
 [32.728]
 [32.728]
 [32.728]
 [32.728]
 [32.728]
 [32.728]] [[33.264]
 [33.264]
 [33.264]
 [33.264]
 [33.264]
 [33.264]
 [33.264]]
actor:  1 policy actor:  1  step number:  43 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.07639333333333345 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.07639333333333345 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.77169739436486
Printing some Q and Qe and total Qs values:  [[0.351]
 [0.535]
 [0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]] [[45.492]
 [42.957]
 [45.492]
 [45.492]
 [45.492]
 [45.492]
 [45.492]] [[2.118]
 [2.13 ]
 [2.118]
 [2.118]
 [2.118]
 [2.118]
 [2.118]]
maxi score, test score, baseline:  -0.07639333333333345 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.07639333333333345 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.733]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]] [[29.943]
 [31.155]
 [29.943]
 [29.943]
 [29.943]
 [29.943]
 [29.943]] [[2.225]
 [2.469]
 [2.225]
 [2.225]
 [2.225]
 [2.225]
 [2.225]]
printing an ep nov before normalisation:  29.47479248046875
actor:  1 policy actor:  1  step number:  50 total reward:  0.33999999999999997  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.739]
 [0.764]
 [0.741]
 [0.741]
 [0.741]
 [0.741]
 [0.741]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.739]
 [0.764]
 [0.741]
 [0.741]
 [0.741]
 [0.741]
 [0.741]]
maxi score, test score, baseline:  -0.07639333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.07639333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.41811277928211
maxi score, test score, baseline:  -0.07639333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.07639333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.07639333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.926862674667703
actions average: 
K:  3  action  0 :  tensor([0.5390, 0.0144, 0.0803, 0.0920, 0.0914, 0.1028, 0.0801],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0037,     0.9840,     0.0009,     0.0032,     0.0012,     0.0006,
            0.0064], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1227, 0.0455, 0.3542, 0.1577, 0.1270, 0.0911, 0.1017],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1590, 0.0711, 0.0835, 0.2996, 0.1430, 0.1331, 0.1107],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2742, 0.0811, 0.0700, 0.0889, 0.3088, 0.1174, 0.0597],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.2361, 0.0280, 0.1683, 0.1377, 0.1264, 0.2002, 0.1033],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1564, 0.2208, 0.0917, 0.1449, 0.0962, 0.1002, 0.1899],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  51 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.254]
 [0.379]
 [0.293]
 [0.277]
 [0.275]
 [0.363]
 [0.275]] [[34.766]
 [33.306]
 [34.344]
 [34.717]
 [34.91 ]
 [32.603]
 [35.806]] [[0.254]
 [0.379]
 [0.293]
 [0.277]
 [0.275]
 [0.363]
 [0.275]]
actor:  0 policy actor:  0  step number:  52 total reward:  0.3399999999999995  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  35.6205208374556
maxi score, test score, baseline:  -0.07638000000000013 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.07638000000000013 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.839265
maxi score, test score, baseline:  -0.07638000000000013 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.361664285159534
maxi score, test score, baseline:  -0.07638000000000013 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.07638000000000013 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.07638000000000013 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  53 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.07659333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.07659333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.47747487673416
actions average: 
K:  2  action  0 :  tensor([0.6258, 0.0566, 0.0635, 0.0674, 0.0614, 0.0662, 0.0591],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0096, 0.9467, 0.0083, 0.0140, 0.0047, 0.0043, 0.0124],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0434, 0.0026, 0.6655, 0.0612, 0.0469, 0.1209, 0.0596],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2155, 0.0182, 0.1386, 0.2589, 0.1125, 0.1385, 0.1177],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2052, 0.0014, 0.0894, 0.0723, 0.4515, 0.0993, 0.0810],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1772, 0.0136, 0.1280, 0.1152, 0.1203, 0.3709, 0.0748],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1411, 0.3940, 0.0750, 0.1131, 0.0634, 0.0837, 0.1296],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  36.10579511797739
printing an ep nov before normalisation:  36.91688134140095
actor:  1 policy actor:  1  step number:  44 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.07659333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.759778523574774
printing an ep nov before normalisation:  37.76533258576861
printing an ep nov before normalisation:  43.1403480830048
printing an ep nov before normalisation:  44.569333113710144
printing an ep nov before normalisation:  34.511045195446776
printing an ep nov before normalisation:  25.40191686400351
actions average: 
K:  3  action  0 :  tensor([0.5996, 0.0924, 0.0574, 0.0616, 0.0544, 0.0707, 0.0639],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0229, 0.9118, 0.0089, 0.0189, 0.0059, 0.0048, 0.0267],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1603, 0.0078, 0.4815, 0.0710, 0.0438, 0.1165, 0.1190],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.3218, 0.0013, 0.1025, 0.1980, 0.1427, 0.1251, 0.1085],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1808, 0.0059, 0.0634, 0.1028, 0.4591, 0.1100, 0.0781],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1905, 0.0039, 0.1953, 0.1390, 0.1075, 0.2137, 0.1501],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2752, 0.2070, 0.0721, 0.1098, 0.0950, 0.0890, 0.1519],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  40 total reward:  0.5  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  25.163909958488286
actor:  1 policy actor:  1  step number:  39 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.07659333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.476]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]] [[44.451]
 [46.58 ]
 [44.451]
 [44.451]
 [44.451]
 [44.451]
 [44.451]] [[1.39 ]
 [1.418]
 [1.39 ]
 [1.39 ]
 [1.39 ]
 [1.39 ]
 [1.39 ]]
printing an ep nov before normalisation:  61.831303201480964
Printing some Q and Qe and total Qs values:  [[0.888]
 [0.781]
 [0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]] [[35.828]
 [41.845]
 [35.828]
 [35.828]
 [35.828]
 [35.828]
 [35.828]] [[1.488]
 [1.539]
 [1.488]
 [1.488]
 [1.488]
 [1.488]
 [1.488]]
printing an ep nov before normalisation:  39.938512273200026
maxi score, test score, baseline:  -0.07659333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.738]
 [0.701]
 [0.639]
 [0.681]
 [0.623]
 [0.701]] [[41.447]
 [44.541]
 [36.162]
 [42.238]
 [40.353]
 [42.235]
 [36.162]] [[1.233]
 [1.424]
 [1.172]
 [1.265]
 [1.259]
 [1.25 ]
 [1.172]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.07659333333333347 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  0.5466666666666666  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.107]
 [0.37 ]
 [0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]] [[32.443]
 [54.936]
 [32.443]
 [32.443]
 [32.443]
 [32.443]
 [32.443]] [[0.663]
 [1.68 ]
 [0.663]
 [0.663]
 [0.663]
 [0.663]
 [0.663]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.07948666666666679 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0794866666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0794866666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.0794866666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0794866666666668 0.6900000000000002 0.6900000000000002
printing an ep nov before normalisation:  58.701093044971984
maxi score, test score, baseline:  -0.0794866666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.77083985899459
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  -0.0794866666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0794866666666668 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.0794866666666668 0.6900000000000002 0.6900000000000002
line 256 mcts: sample exp_bonus 38.29785495280364
actor:  1 policy actor:  1  step number:  54 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  43 total reward:  0.4933333333333333  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  48.58601542026671
maxi score, test score, baseline:  -0.0794866666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0794866666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0794866666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0794866666666668 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.0794866666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0794866666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.0794866666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  0.333
siam score:  -0.83966506
maxi score, test score, baseline:  -0.0794866666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0794866666666668 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.0794866666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.11064275149155
printing an ep nov before normalisation:  35.20562629588501
printing an ep nov before normalisation:  34.58604424031176
line 256 mcts: sample exp_bonus 32.57920051885329
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.573]
 [0.387]
 [0.403]
 [0.375]
 [0.383]
 [0.402]] [[35.112]
 [27.741]
 [35.68 ]
 [34.993]
 [34.877]
 [35.783]
 [37.663]] [[1.906]
 [1.762]
 [1.916]
 [1.903]
 [1.87 ]
 [1.917]
 [2.017]]
actor:  1 policy actor:  1  step number:  59 total reward:  0.14666666666666617  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0794866666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0794866666666668 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.0794866666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.66586651352051
maxi score, test score, baseline:  -0.0794866666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  59 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  31.06130806840579
maxi score, test score, baseline:  -0.07682000000000014 0.6900000000000002 0.6900000000000002
printing an ep nov before normalisation:  29.307130120645958
actor:  1 policy actor:  1  step number:  38 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.07682000000000014 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.07682000000000014 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.4829, 0.0375, 0.0773, 0.1231, 0.1101, 0.0931, 0.0761],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0205, 0.9209, 0.0093, 0.0180, 0.0030, 0.0031, 0.0252],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1937, 0.0134, 0.2337, 0.1742, 0.1260, 0.1529, 0.1062],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.3684, 0.0004, 0.0915, 0.1700, 0.1157, 0.1404, 0.1135],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1971, 0.0042, 0.0994, 0.1240, 0.3778, 0.1057, 0.0918],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1275, 0.0042, 0.1372, 0.1533, 0.1051, 0.3781, 0.0946],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0861, 0.4142, 0.0656, 0.0995, 0.0617, 0.0677, 0.2052],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  38.55692604372365
printing an ep nov before normalisation:  44.53439625933944
printing an ep nov before normalisation:  4.942592113366118e-05
actor:  1 policy actor:  1  step number:  44 total reward:  0.4066666666666662  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.07682000000000014 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.07682000000000014 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.07682000000000014 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.83353484
maxi score, test score, baseline:  -0.07682000000000014 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.89219308366172
Printing some Q and Qe and total Qs values:  [[0.28 ]
 [0.352]
 [0.277]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]] [[37.146]
 [37.903]
 [37.78 ]
 [37.146]
 [37.146]
 [37.146]
 [37.146]] [[0.921]
 [1.019]
 [0.94 ]
 [0.921]
 [0.921]
 [0.921]
 [0.921]]
printing an ep nov before normalisation:  38.52014330018087
Printing some Q and Qe and total Qs values:  [[0.192]
 [0.829]
 [0.701]
 [0.805]
 [0.623]
 [0.756]
 [0.845]] [[33.344]
 [32.122]
 [34.982]
 [31.179]
 [29.706]
 [29.228]
 [33.489]] [[0.192]
 [0.829]
 [0.701]
 [0.805]
 [0.623]
 [0.756]
 [0.845]]
printing an ep nov before normalisation:  0.0010096110511881307
actor:  1 policy actor:  1  step number:  50 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.07682000000000012 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.07682000000000012 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.95926883618109
maxi score, test score, baseline:  -0.07682000000000012 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.07682000000000012 0.6900000000000002 0.6900000000000002
actor:  0 policy actor:  0  step number:  48 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  31.158980117789376
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.07430000000000013 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  74 total reward:  0.04666666666666586  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.07430000000000013 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.052]
 [ 0.126]
 [ 0.074]
 [ 0.017]
 [-0.002]
 [ 0.074]
 [ 0.021]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.052]
 [ 0.126]
 [ 0.074]
 [ 0.017]
 [-0.002]
 [ 0.074]
 [ 0.021]]
siam score:  -0.8373226
maxi score, test score, baseline:  -0.07430000000000013 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.07430000000000013 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.02293317741363
maxi score, test score, baseline:  -0.07430000000000013 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.07430000000000013 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 37.35772393783016
printing an ep nov before normalisation:  43.54394177053116
printing an ep nov before normalisation:  33.70606178458578
printing an ep nov before normalisation:  32.70441974821847
maxi score, test score, baseline:  -0.07430000000000013 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.07430000000000013 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  39 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.07430000000000013 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.40356379260088
printing an ep nov before normalisation:  25.128069279234793
actor:  1 policy actor:  1  step number:  49 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.645]
 [0.754]
 [0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]] [[52.61 ]
 [48.623]
 [52.61 ]
 [52.61 ]
 [52.61 ]
 [52.61 ]
 [52.61 ]] [[1.505]
 [1.504]
 [1.505]
 [1.505]
 [1.505]
 [1.505]
 [1.505]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.07430000000000013 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.07430000000000013 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.07430000000000013 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  42.83888816833496
maxi score, test score, baseline:  -0.07430000000000013 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.07430000000000013 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.07430000000000013 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  25.917013523525103
printing an ep nov before normalisation:  0.0
actor:  1 policy actor:  1  step number:  50 total reward:  0.33999999999999986  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.07430000000000013 0.6900000000000002 0.6900000000000002
printing an ep nov before normalisation:  51.36094227348477
printing an ep nov before normalisation:  44.05306871120891
maxi score, test score, baseline:  -0.07430000000000013 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 36.335274425222636
maxi score, test score, baseline:  -0.07430000000000013 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.07430000000000013 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.07430000000000013 0.6900000000000002 0.6900000000000002
printing an ep nov before normalisation:  25.137880846000176
actor:  0 policy actor:  0  step number:  45 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0781666666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  47.88773536682129
Printing some Q and Qe and total Qs values:  [[1.007]
 [1.016]
 [1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]] [[36.116]
 [32.157]
 [36.116]
 [36.116]
 [36.116]
 [36.116]
 [36.116]] [[1.007]
 [1.016]
 [1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]]
siam score:  -0.8288909
maxi score, test score, baseline:  -0.0781666666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.082]
 [0.166]
 [0.116]
 [0.036]
 [0.036]
 [0.108]
 [0.105]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.082]
 [0.166]
 [0.116]
 [0.036]
 [0.036]
 [0.108]
 [0.105]]
printing an ep nov before normalisation:  28.718145946685482
actor:  0 policy actor:  0  step number:  52 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.08206000000000013 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.79517092080027
printing an ep nov before normalisation:  35.72233017306982
maxi score, test score, baseline:  -0.08206000000000013 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.87861442565918
printing an ep nov before normalisation:  37.30934048389747
maxi score, test score, baseline:  -0.08206000000000013 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  59 total reward:  0.03999999999999937  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.08328666666666683 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.08328666666666683 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.663170384759113
actions average: 
K:  0  action  0 :  tensor([0.5347, 0.0016, 0.0775, 0.0788, 0.1410, 0.0743, 0.0923],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0062, 0.9596, 0.0035, 0.0040, 0.0022, 0.0022, 0.0223],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1242, 0.0451, 0.5145, 0.0773, 0.0644, 0.0897, 0.0848],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2034, 0.0008, 0.1262, 0.3202, 0.1128, 0.1155, 0.1212],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1652, 0.0013, 0.1089, 0.1171, 0.3681, 0.1334, 0.1060],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1581, 0.0272, 0.1717, 0.1064, 0.0940, 0.3469, 0.0958],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1531, 0.1345, 0.1269, 0.1369, 0.1247, 0.1382, 0.1858],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.08328666666666683 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.08328666666666683 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.367 0.02  0.245 0.082 0.041 0.204 0.041]
maxi score, test score, baseline:  -0.08328666666666683 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.08328666666666683 0.6900000000000002 0.6900000000000002
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]] [[32.182]
 [32.182]
 [32.182]
 [32.182]
 [32.182]
 [32.182]
 [32.182]] [[1.552]
 [1.552]
 [1.552]
 [1.552]
 [1.552]
 [1.552]
 [1.552]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.08328666666666683 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]] [[48.185]
 [48.132]
 [48.132]
 [48.132]
 [48.132]
 [48.132]
 [48.132]] [[2.174]
 [2.078]
 [2.078]
 [2.078]
 [2.078]
 [2.078]
 [2.078]]
maxi score, test score, baseline:  -0.08328666666666683 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.3839, 0.0529, 0.0816, 0.1259, 0.1016, 0.1066, 0.1475],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0103, 0.9391, 0.0048, 0.0098, 0.0045, 0.0031, 0.0284],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1586, 0.0011, 0.4051, 0.0770, 0.0894, 0.1454, 0.1233],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1901, 0.1230, 0.0849, 0.2256, 0.1219, 0.1169, 0.1376],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1848, 0.0854, 0.0975, 0.0839, 0.2908, 0.1309, 0.1267],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.2142, 0.0129, 0.1161, 0.0946, 0.0900, 0.3678, 0.1043],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2012, 0.0480, 0.1095, 0.1375, 0.1333, 0.1290, 0.2414],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  32.656678313469364
maxi score, test score, baseline:  -0.08328666666666683 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.593]
 [0.573]
 [0.561]
 [0.575]
 [0.575]
 [0.558]] [[27.121]
 [29.003]
 [26.094]
 [26.652]
 [28.559]
 [28.134]
 [27.001]] [[2.   ]
 [2.212]
 [1.895]
 [1.94 ]
 [2.149]
 [2.106]
 [1.972]]
printing an ep nov before normalisation:  31.340473147488094
printing an ep nov before normalisation:  31.238367557525635
actor:  1 policy actor:  1  step number:  51 total reward:  0.2933333333333328  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  51 total reward:  0.38666666666666605  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.08328666666666683 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.08328666666666683 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.08328666666666683 0.6900000000000002 0.6900000000000002
actions average: 
K:  2  action  0 :  tensor([0.6475, 0.0193, 0.0489, 0.0733, 0.1006, 0.0524, 0.0580],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0273, 0.8862, 0.0143, 0.0173, 0.0093, 0.0082, 0.0374],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0826, 0.0057, 0.6592, 0.0602, 0.0490, 0.0754, 0.0679],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2453, 0.0013, 0.1104, 0.2945, 0.1232, 0.1090, 0.1163],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2282, 0.0790, 0.0849, 0.0913, 0.3381, 0.0723, 0.1063],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2587, 0.0011, 0.1329, 0.0902, 0.0894, 0.3327, 0.0950],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0867, 0.2582, 0.1823, 0.0596, 0.0559, 0.0539, 0.3034],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  19.561212062835693
maxi score, test score, baseline:  -0.08328666666666683 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  64 total reward:  0.07333333333333236  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  26.48964181915168
printing an ep nov before normalisation:  34.36919215526089
siam score:  -0.81754875
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.197]
 [0.28 ]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]] [[40.824]
 [46.794]
 [40.824]
 [40.824]
 [40.824]
 [40.824]
 [40.824]] [[0.562]
 [0.734]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]]
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.557]
 [0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]] [[35.326]
 [40.999]
 [35.326]
 [35.326]
 [35.326]
 [35.326]
 [35.326]] [[0.78 ]
 [0.837]
 [0.78 ]
 [0.78 ]
 [0.78 ]
 [0.78 ]
 [0.78 ]]
maxi score, test score, baseline:  -0.08767333333333348 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.08767333333333348 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.812273025512695
printing an ep nov before normalisation:  13.76161300031697
maxi score, test score, baseline:  -0.08767333333333348 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.2133333333333327  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  54 total reward:  0.4866666666666669  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  38.663088936606826
maxi score, test score, baseline:  -0.08767333333333348 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.08767333333333348 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.08767333333333348 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.038744073774627
maxi score, test score, baseline:  -0.08767333333333348 0.6900000000000002 0.6900000000000002
printing an ep nov before normalisation:  24.196720820412487
printing an ep nov before normalisation:  0.056732284301688196
printing an ep nov before normalisation:  27.18207557802944
actor:  0 policy actor:  0  step number:  46 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.782]
 [0.834]
 [0.785]
 [0.785]
 [0.785]
 [0.785]
 [0.785]] [[18.996]
 [23.729]
 [18.857]
 [18.857]
 [18.857]
 [18.857]
 [18.857]] [[0.782]
 [0.834]
 [0.785]
 [0.785]
 [0.785]
 [0.785]
 [0.785]]
maxi score, test score, baseline:  -0.08798000000000016 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  68 total reward:  0.059999999999999054  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  23.998854160308838
maxi score, test score, baseline:  -0.08900666666666682 0.6900000000000002 0.6900000000000002
actor:  0 policy actor:  0  step number:  49 total reward:  0.22666666666666602  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.08976666666666681 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.095]
 [0.252]
 [0.125]
 [0.072]
 [0.08 ]
 [0.07 ]
 [0.121]] [[36.206]
 [31.54 ]
 [41.647]
 [34.673]
 [35.238]
 [35.408]
 [33.936]] [[0.428]
 [0.507]
 [0.549]
 [0.379]
 [0.397]
 [0.39 ]
 [0.416]]
maxi score, test score, baseline:  -0.08976666666666681 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.9407696723938
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.08976666666666681 0.6900000000000002 0.6900000000000002
printing an ep nov before normalisation:  26.551319985081587
maxi score, test score, baseline:  -0.08976666666666681 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  30.686259845983045
siam score:  -0.8215895
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.25487894732224
actor:  1 policy actor:  1  step number:  46 total reward:  0.4199999999999996  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.08976666666666681 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.13298624325689
maxi score, test score, baseline:  -0.08976666666666681 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.08976666666666681 0.6900000000000002 0.6900000000000002
printing an ep nov before normalisation:  34.41617107880789
printing an ep nov before normalisation:  37.569841698972745
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.08976666666666681 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  46 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.28]
 [0.28]
 [0.28]
 [0.28]
 [0.28]
 [0.28]
 [0.28]] [[49.123]
 [49.123]
 [49.123]
 [49.123]
 [49.123]
 [49.123]
 [49.123]] [[1.476]
 [1.476]
 [1.476]
 [1.476]
 [1.476]
 [1.476]
 [1.476]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.73716504919601
printing an ep nov before normalisation:  43.073561977228714
maxi score, test score, baseline:  -0.0933666666666668 0.6900000000000002 0.6900000000000002
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.649]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.499]] [[30.851]
 [43.688]
 [41.022]
 [41.022]
 [41.022]
 [41.022]
 [29.061]] [[1.106]
 [1.845]
 [1.636]
 [1.636]
 [1.636]
 [1.636]
 [1.006]]
printing an ep nov before normalisation:  30.952975651238763
printing an ep nov before normalisation:  26.144505741759044
Printing some Q and Qe and total Qs values:  [[0.678]
 [0.726]
 [0.677]
 [0.696]
 [0.681]
 [0.669]
 [0.688]] [[45.46 ]
 [51.051]
 [46.574]
 [46.555]
 [47.043]
 [47.888]
 [46.878]] [[1.617]
 [1.838]
 [1.651]
 [1.669]
 [1.67 ]
 [1.683]
 [1.671]]
printing an ep nov before normalisation:  33.410945486277434
siam score:  -0.8226436
actor:  1 policy actor:  1  step number:  39 total reward:  0.56  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.0933666666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0933666666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.467]
 [ 0.613]
 [ 0.111]
 [ 0.572]
 [ 0.562]
 [-0.006]
 [ 0.585]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.467]
 [ 0.613]
 [ 0.111]
 [ 0.572]
 [ 0.562]
 [-0.006]
 [ 0.585]]
maxi score, test score, baseline:  -0.0933666666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  39 total reward:  0.5466666666666666  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.0933666666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.003091231825685
maxi score, test score, baseline:  -0.09611333333333348 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.09611333333333348 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.09611333333333348 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  59 total reward:  0.14666666666666583  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.09650000000000016 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.09650000000000016 0.6900000000000002 0.6900000000000002
actor:  1 policy actor:  1  step number:  37 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.09650000000000016 0.6900000000000002 0.6900000000000002
Printing some Q and Qe and total Qs values:  [[-0.029]
 [ 0.105]
 [-0.023]
 [-0.006]
 [-0.006]
 [-0.032]
 [ 0.003]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.029]
 [ 0.105]
 [-0.023]
 [-0.006]
 [-0.006]
 [-0.032]
 [ 0.003]]
Printing some Q and Qe and total Qs values:  [[0.814]
 [0.857]
 [0.807]
 [0.793]
 [0.821]
 [0.773]
 [0.837]] [[34.814]
 [36.1  ]
 [35.173]
 [35.116]
 [36.473]
 [38.306]
 [34.91 ]] [[0.814]
 [0.857]
 [0.807]
 [0.793]
 [0.821]
 [0.773]
 [0.837]]
maxi score, test score, baseline:  -0.09650000000000016 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.43952061259476
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.60860218176129
maxi score, test score, baseline:  -0.0989266666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0989266666666668 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0989266666666668 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  -0.09892666666666682 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.09892666666666682 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.09892666666666682 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  3.461545276195466e-05
actor:  1 policy actor:  1  step number:  39 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.09892666666666682 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.99651061878252
printing an ep nov before normalisation:  23.799259048523854
actor:  1 policy actor:  1  step number:  48 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.09892666666666682 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.5806, 0.0253, 0.0586, 0.0746, 0.1123, 0.0682, 0.0803],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0280, 0.9127, 0.0129, 0.0099, 0.0067, 0.0061, 0.0237],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0554, 0.1306, 0.4839, 0.0732, 0.0443, 0.1228, 0.0898],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2458, 0.0289, 0.1187, 0.1548, 0.1218, 0.1386, 0.1913],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1870, 0.0172, 0.0587, 0.1012, 0.4747, 0.0700, 0.0912],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1689, 0.0138, 0.1343, 0.1298, 0.1235, 0.2959, 0.1339],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([    0.0026,     0.9320,     0.0014,     0.0132,     0.0002,     0.0003,
            0.0504], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.09892666666666682 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.91082956465824
Printing some Q and Qe and total Qs values:  [[0.929]
 [0.987]
 [0.929]
 [0.929]
 [0.929]
 [0.929]
 [1.   ]] [[46.392]
 [48.931]
 [46.392]
 [46.392]
 [46.392]
 [46.392]
 [49.207]] [[0.929]
 [0.987]
 [0.929]
 [0.929]
 [0.929]
 [0.929]
 [1.   ]]
maxi score, test score, baseline:  -0.09892666666666682 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  57 total reward:  0.14666666666666606  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
Starting evaluation
maxi score, test score, baseline:  -0.09663333333333349 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.034]
 [0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.011]
 [0.034]
 [0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]]
siam score:  -0.8155577
Printing some Q and Qe and total Qs values:  [[0.393]
 [0.469]
 [0.424]
 [0.368]
 [0.352]
 [0.353]
 [0.354]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.393]
 [0.469]
 [0.424]
 [0.368]
 [0.352]
 [0.353]
 [0.354]]
maxi score, test score, baseline:  -0.09663333333333349 0.6900000000000002 0.6900000000000002
printing an ep nov before normalisation:  37.47783388410296
printing an ep nov before normalisation:  44.92239562632837
printing an ep nov before normalisation:  45.81542701281428
printing an ep nov before normalisation:  45.91821402463589
printing an ep nov before normalisation:  35.94981428521128
printing an ep nov before normalisation:  37.05679208231929
actor:  0 policy actor:  0  step number:  40 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  27.154604514235785
maxi score, test score, baseline:  -0.09374000000000014 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.36981988216509
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.188]
 [0.188]
 [0.188]
 [0.188]
 [0.188]
 [0.188]] [[32.267]
 [23.768]
 [23.768]
 [23.768]
 [23.768]
 [23.768]
 [23.768]] [[1.81 ]
 [1.034]
 [1.034]
 [1.034]
 [1.034]
 [1.034]
 [1.034]]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.036993333333333475 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.036993333333333475 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.2999999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.036993333333333475 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8209402
Printing some Q and Qe and total Qs values:  [[0.276]
 [0.301]
 [0.276]
 [0.276]
 [0.276]
 [0.276]
 [0.276]] [[32.812]
 [35.107]
 [32.812]
 [32.812]
 [32.812]
 [32.812]
 [32.812]] [[1.788]
 [2.005]
 [1.788]
 [1.788]
 [1.788]
 [1.788]
 [1.788]]
maxi score, test score, baseline:  -0.036993333333333475 0.6933333333333334 0.6933333333333334
printing an ep nov before normalisation:  50.856322050094604
maxi score, test score, baseline:  -0.036993333333333475 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.440258344225064
siam score:  -0.81828225
actor:  1 policy actor:  1  step number:  60 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.037]
 [-0.005]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.037]
 [-0.005]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]]
maxi score, test score, baseline:  -0.036993333333333475 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.89501430612365
maxi score, test score, baseline:  -0.036993333333333475 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.036993333333333475 0.6933333333333334 0.6933333333333334
actor:  1 policy actor:  1  step number:  41 total reward:  0.5466666666666667  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.036993333333333475 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.036993333333333475 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.435129463441925
printing an ep nov before normalisation:  37.73604466930529
maxi score, test score, baseline:  -0.036993333333333475 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
actor:  1 policy actor:  1  step number:  40 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.036993333333333475 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.036993333333333475 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  43 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.03671333333333347 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.755295753479004
Printing some Q and Qe and total Qs values:  [[0.575]
 [0.61 ]
 [0.647]
 [0.647]
 [0.647]
 [0.647]
 [0.647]] [[42.88 ]
 [48.194]
 [41.839]
 [41.839]
 [41.839]
 [41.839]
 [41.839]] [[1.548]
 [1.758]
 [1.585]
 [1.585]
 [1.585]
 [1.585]
 [1.585]]
actor:  1 policy actor:  1  step number:  33 total reward:  0.64  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  57 total reward:  0.11999999999999944  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]] [[46.968]
 [45.623]
 [45.623]
 [45.623]
 [45.623]
 [45.623]
 [45.623]] [[1.791]
 [1.493]
 [1.493]
 [1.493]
 [1.493]
 [1.493]
 [1.493]]
printing an ep nov before normalisation:  28.340446814668326
maxi score, test score, baseline:  -0.034473333333333474 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.798677834898996
printing an ep nov before normalisation:  23.344905698522425
siam score:  -0.81322235
actor:  1 policy actor:  1  step number:  48 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.034473333333333474 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.034473333333333474 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.719072910567387
actions average: 
K:  1  action  0 :  tensor([0.4709, 0.0129, 0.0856, 0.1149, 0.1039, 0.1236, 0.0882],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0078, 0.9409, 0.0043, 0.0051, 0.0056, 0.0044, 0.0320],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2223, 0.0588, 0.2477, 0.1202, 0.0927, 0.1334, 0.1249],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2120, 0.0300, 0.0981, 0.2758, 0.1420, 0.1309, 0.1111],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2382, 0.0128, 0.0659, 0.1118, 0.4178, 0.0869, 0.0666],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1871, 0.0028, 0.1450, 0.0538, 0.0401, 0.5221, 0.0492],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1679, 0.0553, 0.0972, 0.0981, 0.0893, 0.1357, 0.3564],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  57 total reward:  0.09333333333333271  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.034473333333333474 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.112]
 [0.215]
 [0.159]
 [0.108]
 [0.178]
 [0.11 ]
 [0.107]] [[30.539]
 [31.502]
 [30.944]
 [29.722]
 [31.755]
 [29.696]
 [29.873]] [[1.018]
 [1.181]
 [1.09 ]
 [0.962]
 [1.161]
 [0.963]
 [0.971]]
maxi score, test score, baseline:  -0.034473333333333474 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.034473333333333474 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.034473333333333474 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.191305687264666
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.034473333333333474 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.05937329364956
maxi score, test score, baseline:  -0.034473333333333474 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.034473333333333474 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.619813892379675
maxi score, test score, baseline:  -0.034473333333333474 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0012109936832871426
actor:  1 policy actor:  1  step number:  45 total reward:  0.5600000000000002  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.034473333333333474 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  -0.034473333333333474 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.034473333333333474 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.36070843630085
printing an ep nov before normalisation:  30.42375895752757
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  60 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  46.014282776926684
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  63 total reward:  0.22666666666666602  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.88076066970825
maxi score, test score, baseline:  -0.03202000000000013 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  32.11780786514282
actor:  1 policy actor:  1  step number:  46 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  31.291438078131563
maxi score, test score, baseline:  -0.03202000000000013 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.81260145
siam score:  -0.81349826
maxi score, test score, baseline:  -0.03202000000000013 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  55 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.02954000000000015 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 33.99142481179312
printing an ep nov before normalisation:  28.083517649827563
printing an ep nov before normalisation:  23.601392334798987
actor:  1 policy actor:  1  step number:  47 total reward:  0.25333333333333297  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  32.267916889988676
Printing some Q and Qe and total Qs values:  [[0.712]
 [0.809]
 [0.719]
 [0.713]
 [0.715]
 [0.747]
 [0.714]] [[27.093]
 [28.279]
 [26.532]
 [26.381]
 [26.358]
 [27.566]
 [26.242]] [[0.712]
 [0.809]
 [0.719]
 [0.713]
 [0.715]
 [0.747]
 [0.714]]
printing an ep nov before normalisation:  40.36091108440897
printing an ep nov before normalisation:  27.860414459018436
maxi score, test score, baseline:  -0.02954000000000015 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.813]
 [0.92 ]
 [0.813]
 [0.838]
 [0.813]
 [0.813]
 [0.888]] [[25.404]
 [28.271]
 [25.404]
 [29.978]
 [25.404]
 [25.404]
 [29.517]] [[0.813]
 [0.92 ]
 [0.813]
 [0.838]
 [0.813]
 [0.813]
 [0.888]]
printing an ep nov before normalisation:  45.516304848811586
printing an ep nov before normalisation:  40.29873282714014
actor:  0 policy actor:  0  step number:  51 total reward:  0.27999999999999936  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  3  action  0 :  tensor([0.4332, 0.0462, 0.1134, 0.1117, 0.0837, 0.0993, 0.1126],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0105, 0.9003, 0.0116, 0.0247, 0.0036, 0.0070, 0.0423],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1630, 0.0008, 0.5655, 0.0579, 0.0483, 0.0893, 0.0753],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1971, 0.0419, 0.1492, 0.1549, 0.1512, 0.1737, 0.1320],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1907, 0.0284, 0.1095, 0.1367, 0.2805, 0.1380, 0.1163],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1512, 0.0090, 0.1032, 0.1237, 0.0730, 0.4400, 0.0998],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1276, 0.0160, 0.0920, 0.2360, 0.1304, 0.1102, 0.2877],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.026980000000000157 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.726]
 [0.762]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]] [[30.95 ]
 [29.459]
 [30.95 ]
 [30.95 ]
 [30.95 ]
 [30.95 ]
 [30.95 ]] [[1.375]
 [1.352]
 [1.375]
 [1.375]
 [1.375]
 [1.375]
 [1.375]]
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.323]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.255]] [[41.061]
 [45.39 ]
 [41.061]
 [41.061]
 [41.061]
 [41.061]
 [47.723]] [[0.508]
 [0.581]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.534]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.026980000000000157 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  42 total reward:  0.47333333333333305  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.423]
 [0.349]
 [0.32 ]
 [0.322]
 [0.32 ]
 [0.349]] [[19.756]
 [21.866]
 [21.896]
 [19.404]
 [19.297]
 [19.234]
 [21.896]] [[1.697]
 [2.126]
 [2.056]
 [1.634]
 [1.619]
 [1.608]
 [2.056]]
maxi score, test score, baseline:  -0.026980000000000157 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.60321676500979
printing an ep nov before normalisation:  40.1968289589713
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.479]
 [0.506]
 [0.498]
 [0.479]
 [0.479]
 [0.501]] [[30.978]
 [31.714]
 [31.312]
 [31.214]
 [31.714]
 [31.714]
 [31.077]] [[2.251]
 [2.336]
 [2.316]
 [2.295]
 [2.336]
 [2.336]
 [2.282]]
maxi score, test score, baseline:  -0.026980000000000157 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.026980000000000157 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.22666666666666635  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.034]
 [-0.031]
 [-0.031]
 [-0.034]
 [-0.034]
 [-0.038]
 [-0.036]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.034]
 [-0.031]
 [-0.031]
 [-0.034]
 [-0.034]
 [-0.038]
 [-0.036]]
maxi score, test score, baseline:  -0.026980000000000157 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.026980000000000157 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.29]
 [0.29]
 [0.29]
 [0.29]
 [0.29]
 [0.29]
 [0.29]] [[47.959]
 [47.959]
 [47.959]
 [47.959]
 [47.959]
 [47.959]
 [47.959]] [[1.366]
 [1.366]
 [1.366]
 [1.366]
 [1.366]
 [1.366]
 [1.366]]
printing an ep nov before normalisation:  45.26407085623667
maxi score, test score, baseline:  -0.026980000000000157 0.6933333333333334 0.6933333333333334
actor:  1 policy actor:  1  step number:  63 total reward:  0.09333333333333271  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.034]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.017]
 [-0.021]
 [-0.021]] [[57.068]
 [45.34 ]
 [45.34 ]
 [45.34 ]
 [55.728]
 [45.34 ]
 [45.34 ]] [[1.433]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [1.383]
 [0.87 ]
 [0.87 ]]
printing an ep nov before normalisation:  33.81729088207357
printing an ep nov before normalisation:  15.04987831352814
maxi score, test score, baseline:  -0.026980000000000157 0.6933333333333334 0.6933333333333334
Printing some Q and Qe and total Qs values:  [[0.097]
 [0.338]
 [0.108]
 [0.108]
 [0.108]
 [0.108]
 [0.108]] [[34.326]
 [36.052]
 [35.733]
 [35.733]
 [35.733]
 [35.733]
 [35.733]] [[1.533]
 [1.896]
 [1.643]
 [1.643]
 [1.643]
 [1.643]
 [1.643]]
actions average: 
K:  3  action  0 :  tensor([0.5617, 0.0500, 0.0612, 0.0735, 0.0932, 0.0676, 0.0927],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0021,     0.9778,     0.0038,     0.0031,     0.0008,     0.0019,
            0.0105], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1940, 0.1219, 0.2243, 0.0951, 0.0910, 0.1536, 0.1201],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0495, 0.2492, 0.0170, 0.4580, 0.0911, 0.0388, 0.0964],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1600, 0.1083, 0.0976, 0.1238, 0.2762, 0.1210, 0.1132],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1080, 0.0361, 0.1382, 0.0758, 0.0597, 0.5007, 0.0815],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1433, 0.1737, 0.1201, 0.1020, 0.0854, 0.1264, 0.2491],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.026980000000000157 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.518]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]] [[31.492]
 [31.143]
 [31.143]
 [31.143]
 [31.143]
 [31.143]
 [31.143]] [[2.518]
 [2.462]
 [2.462]
 [2.462]
 [2.462]
 [2.462]
 [2.462]]
actor:  1 policy actor:  1  step number:  56 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.026980000000000157 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.124]
 [0.253]
 [0.124]
 [0.124]
 [0.124]
 [0.124]
 [0.124]] [[37.414]
 [42.322]
 [37.414]
 [37.414]
 [37.414]
 [37.414]
 [37.414]] [[0.974]
 [1.318]
 [0.974]
 [0.974]
 [0.974]
 [0.974]
 [0.974]]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.026980000000000157 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  -0.026980000000000157 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.026980000000000157 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  36 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  59 total reward:  0.3333333333333328  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.64 ]
 [0.655]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.64 ]
 [0.655]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]]
maxi score, test score, baseline:  -0.023873333333333486 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.092144086154306
maxi score, test score, baseline:  -0.023873333333333486 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.023873333333333486 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.023873333333333486 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.657790147549072
maxi score, test score, baseline:  -0.023873333333333486 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.023873333333333486 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  -0.023873333333333486 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  48 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  54 total reward:  0.5000000000000004  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.029]
 [-0.009]
 [-0.029]
 [-0.029]
 [-0.029]
 [-0.029]
 [-0.029]] [[ 9.189]
 [29.808]
 [ 9.189]
 [ 9.189]
 [ 9.189]
 [ 9.189]
 [ 9.189]] [[0.058]
 [0.459]
 [0.058]
 [0.058]
 [0.058]
 [0.058]
 [0.058]]
printing an ep nov before normalisation:  37.553863525390625
printing an ep nov before normalisation:  31.96066453015801
printing an ep nov before normalisation:  30.31224743352583
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.478]
 [0.446]
 [0.441]
 [0.504]
 [0.442]
 [0.504]] [[29.753]
 [31.287]
 [30.181]
 [29.79 ]
 [34.05 ]
 [29.481]
 [34.05 ]] [[1.962]
 [2.145]
 [2.002]
 [1.958]
 [2.449]
 [1.927]
 [2.449]]
maxi score, test score, baseline:  -0.021060000000000138 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  55 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  40 total reward:  0.5266666666666667  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  38.9401436758607
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.01844666666666681 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.69701053098157
maxi score, test score, baseline:  -0.01844666666666681 0.6933333333333334 0.6933333333333334
printing an ep nov before normalisation:  38.08549880981445
printing an ep nov before normalisation:  32.12340553499737
maxi score, test score, baseline:  -0.01844666666666681 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  38 total reward:  0.48666666666666647  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.018060000000000166 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.619]
 [0.733]
 [0.619]
 [0.619]
 [0.619]
 [0.619]
 [0.619]] [[40.566]
 [34.228]
 [40.566]
 [40.566]
 [40.566]
 [40.566]
 [40.566]] [[0.619]
 [0.733]
 [0.619]
 [0.619]
 [0.619]
 [0.619]
 [0.619]]
maxi score, test score, baseline:  -0.018060000000000166 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  -0.018060000000000166 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.81851655688793
maxi score, test score, baseline:  -0.018060000000000166 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.35 ]
 [0.333]
 [0.333]
 [0.333]
 [0.33 ]
 [0.333]
 [0.333]] [[41.144]
 [50.947]
 [50.947]
 [50.947]
 [35.911]
 [50.947]
 [50.947]] [[1.049]
 [1.332]
 [1.332]
 [1.332]
 [0.869]
 [1.332]
 [1.332]]
printing an ep nov before normalisation:  36.69442850682667
maxi score, test score, baseline:  -0.018060000000000166 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.01806000000000016 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.01806000000000016 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.112]
 [0.438]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]] [[35.464]
 [32.689]
 [33.578]
 [33.578]
 [33.578]
 [33.578]
 [33.578]] [[0.393]
 [0.676]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]]
maxi score, test score, baseline:  -0.01806000000000016 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.794793717881326
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.631041765532487
printing an ep nov before normalisation:  25.466762822780655
Printing some Q and Qe and total Qs values:  [[0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]] [[37.86]
 [37.86]
 [37.86]
 [37.86]
 [37.86]
 [37.86]
 [37.86]] [[2.305]
 [2.305]
 [2.305]
 [2.305]
 [2.305]
 [2.305]
 [2.305]]
maxi score, test score, baseline:  -0.01806000000000016 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.69488167936429
maxi score, test score, baseline:  -0.01806000000000016 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.62812422857158
actor:  1 policy actor:  1  step number:  59 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.01806000000000016 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.84556293487549
actor:  0 policy actor:  0  step number:  49 total reward:  0.2933333333333331  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.015473333333333481 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.81277422688674
printing an ep nov before normalisation:  52.72845081498289
printing an ep nov before normalisation:  32.67930507659912
printing an ep nov before normalisation:  24.52829360961914
actor:  1 policy actor:  1  step number:  39 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.015473333333333481 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.015473333333333481 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.015473333333333481 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.015473333333333481 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.015473333333333481 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  53 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.02  0.388 0.02  0.02  0.163 0.02  0.367]
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8150206
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.656]
 [0.572]
 [0.532]
 [0.572]
 [0.572]
 [0.611]] [[30.877]
 [32.224]
 [30.877]
 [36.046]
 [30.877]
 [30.877]
 [29.705]] [[0.791]
 [0.891]
 [0.791]
 [0.815]
 [0.791]
 [0.791]
 [0.815]]
siam score:  -0.81278455
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.455]
 [0.367]
 [0.286]
 [0.367]
 [0.315]
 [0.303]] [[34.67 ]
 [32.736]
 [35.102]
 [35.411]
 [35.102]
 [34.185]
 [36.278]] [[1.14 ]
 [1.227]
 [1.245]
 [1.177]
 [1.245]
 [1.152]
 [1.233]]
siam score:  -0.8133964
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.348]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]] [[43.648]
 [45.922]
 [43.648]
 [43.648]
 [43.648]
 [43.648]
 [43.648]] [[1.522]
 [1.674]
 [1.522]
 [1.522]
 [1.522]
 [1.522]
 [1.522]]
printing an ep nov before normalisation:  33.859896659851074
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.48  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  45 total reward:  0.33333333333333315  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.10637669449217
Printing some Q and Qe and total Qs values:  [[0.116]
 [0.218]
 [0.093]
 [0.084]
 [0.093]
 [0.134]
 [0.093]] [[46.209]
 [43.573]
 [41.786]
 [45.741]
 [41.786]
 [43.998]
 [41.786]] [[1.117]
 [1.108]
 [0.909]
 [1.065]
 [0.909]
 [1.043]
 [0.909]]
Printing some Q and Qe and total Qs values:  [[0.146]
 [0.352]
 [0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.373]] [[35.432]
 [38.077]
 [36.25 ]
 [36.25 ]
 [36.25 ]
 [36.25 ]
 [35.05 ]] [[0.798]
 [1.111]
 [0.952]
 [0.952]
 [0.952]
 [0.952]
 [1.009]]
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.879184246063232
printing an ep nov before normalisation:  34.38672065734863
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  41 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.674]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]] [[44.347]
 [43.291]
 [45.262]
 [45.262]
 [45.262]
 [45.262]
 [45.262]] [[1.529]
 [1.529]
 [1.604]
 [1.604]
 [1.604]
 [1.604]
 [1.604]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.28666666666666607  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
using explorer policy with actor:  1
siam score:  -0.82341546
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[ 0.231]
 [ 0.15 ]
 [ 0.15 ]
 [-0.014]
 [ 0.15 ]
 [ 0.15 ]
 [ 0.15 ]] [[40.024]
 [34.867]
 [34.867]
 [32.004]
 [34.867]
 [34.867]
 [34.867]] [[1.837]
 [1.417]
 [1.417]
 [1.064]
 [1.417]
 [1.417]
 [1.417]]
printing an ep nov before normalisation:  39.3964958190918
actor:  1 policy actor:  1  step number:  57 total reward:  0.13333333333333297  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  32.16510987435634
actor:  1 policy actor:  1  step number:  45 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  35.87260279738967
siam score:  -0.82563186
siam score:  -0.8260101
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.615]
 [0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]] [[28.126]
 [30.324]
 [28.126]
 [28.126]
 [28.126]
 [28.126]
 [28.126]] [[1.597]
 [1.783]
 [1.597]
 [1.597]
 [1.597]
 [1.597]
 [1.597]]
printing an ep nov before normalisation:  33.33118043292372
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
actor:  1 policy actor:  1  step number:  57 total reward:  0.4266666666666662  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  24.950243967930085
printing an ep nov before normalisation:  53.8373569783551
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  45 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  40.535784803963246
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
printing an ep nov before normalisation:  40.678433741084866
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  51 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  48 total reward:  0.4066666666666666  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  34 total reward:  0.5933333333333333  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  40.13815548604662
using explorer policy with actor:  1
siam score:  -0.8200793
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.578]
 [0.531]
 [0.531]] [[20.609]
 [20.609]
 [20.609]
 [20.609]
 [27.469]
 [20.609]
 [20.609]] [[1.264]
 [1.264]
 [1.264]
 [1.264]
 [2.013]
 [1.264]
 [1.264]]
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
actor:  1 policy actor:  1  step number:  52 total reward:  0.29999999999999916  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.229]
 [0.31 ]
 [0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.229]] [[39.266]
 [43.443]
 [39.266]
 [39.266]
 [39.266]
 [39.266]
 [39.266]] [[1.256]
 [1.516]
 [1.256]
 [1.256]
 [1.256]
 [1.256]
 [1.256]]
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
siam score:  -0.82014143
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  36.54039174701282
actor:  1 policy actor:  1  step number:  43 total reward:  0.4266666666666663  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.764]
 [0.764]
 [0.764]
 [0.764]
 [0.764]
 [0.764]
 [0.764]] [[48.143]
 [48.143]
 [48.143]
 [48.143]
 [48.143]
 [48.143]
 [48.143]] [[2.389]
 [2.389]
 [2.389]
 [2.389]
 [2.389]
 [2.389]
 [2.389]]
siam score:  -0.82115555
actor:  1 policy actor:  1  step number:  49 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  39.015877597037615
printing an ep nov before normalisation:  38.331180106597714
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  57 total reward:  0.1866666666666661  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  0
printing an ep nov before normalisation:  39.33936069315146
maxi score, test score, baseline:  -0.013153333333333473 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  0 policy actor:  1  step number:  58 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.894]
 [0.959]
 [0.874]
 [0.923]
 [0.923]
 [0.923]
 [0.923]] [[38.448]
 [40.959]
 [37.151]
 [31.265]
 [31.265]
 [31.265]
 [31.265]] [[0.894]
 [0.959]
 [0.874]
 [0.923]
 [0.923]
 [0.923]
 [0.923]]
printing an ep nov before normalisation:  33.04301022805532
maxi score, test score, baseline:  -0.010553333333333479 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.010553333333333479 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  29.72994556057052
maxi score, test score, baseline:  -0.010553333333333479 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  -0.010553333333333479 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  -0.010553333333333479 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  0 policy actor:  0  step number:  61 total reward:  0.0799999999999994  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.008393333333333489 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  -0.008393333333333489 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  33.262147080870356
printing an ep nov before normalisation:  37.057811323081644
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.008393333333333489 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  47 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.008393333333333489 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  -0.008393333333333489 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  0 policy actor:  0  step number:  55 total reward:  0.23999999999999977  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.005913333333333485 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  -0.005913333333333485 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.976]] [[31.418]
 [31.418]
 [31.418]
 [31.418]
 [31.418]
 [31.418]
 [31.418]] [[0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.976]]
actor:  0 policy actor:  1  step number:  49 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  45.154076386493536
maxi score, test score, baseline:  -0.0031133333333334804 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  48 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.0031133333333334804 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  -0.0031133333333334804 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.521]
 [0.546]
 [0.474]
 [0.467]
 [0.478]
 [0.466]
 [0.465]] [[29.069]
 [29.427]
 [27.7  ]
 [27.588]
 [28.554]
 [27.289]
 [27.975]] [[1.213]
 [1.254]
 [1.104]
 [1.091]
 [1.146]
 [1.077]
 [1.107]]
printing an ep nov before normalisation:  34.20803345443896
printing an ep nov before normalisation:  42.138696257164824
maxi score, test score, baseline:  -0.0031133333333334804 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  32.35219602453308
actor:  1 policy actor:  1  step number:  55 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  0.0
actor:  1 policy actor:  1  step number:  45 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.0031133333333334804 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  -0.0031133333333334804 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.294]
 [0.385]
 [0.236]
 [0.294]
 [0.294]
 [0.294]
 [0.294]] [[32.441]
 [35.205]
 [32.435]
 [32.441]
 [32.441]
 [32.441]
 [32.441]] [[1.01 ]
 [1.22 ]
 [0.952]
 [1.01 ]
 [1.01 ]
 [1.01 ]
 [1.01 ]]
maxi score, test score, baseline:  -0.0031133333333334804 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  23.993585109710693
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.309]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]] [[26.197]
 [32.845]
 [26.197]
 [26.197]
 [26.197]
 [26.197]
 [26.197]] [[0.967]
 [1.317]
 [0.967]
 [0.967]
 [0.967]
 [0.967]
 [0.967]]
maxi score, test score, baseline:  -0.0031133333333334804 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  54 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.0031133333333334804 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  37.10741490966328
printing an ep nov before normalisation:  33.47870111465454
printing an ep nov before normalisation:  33.26010704040527
maxi score, test score, baseline:  -0.0031133333333334804 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  45 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  29.002315005187967
maxi score, test score, baseline:  -0.0031133333333334804 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  60.532935279750106
maxi score, test score, baseline:  -0.0031133333333334804 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  32.19152822081428
actor:  1 policy actor:  1  step number:  56 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  32.91618551299636
maxi score, test score, baseline:  -0.0031133333333334804 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  -0.0031133333333334804 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  -0.0031133333333334804 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  43.0667144069636
maxi score, test score, baseline:  -0.0031133333333334804 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
line 256 mcts: sample exp_bonus 33.484608840261
maxi score, test score, baseline:  -0.0031133333333334804 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  32.7163533384861
actor:  1 policy actor:  1  step number:  40 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  33.579271804402396
printing an ep nov before normalisation:  0.0
Printing some Q and Qe and total Qs values:  [[0.084]
 [0.177]
 [0.084]
 [0.084]
 [0.084]
 [0.084]
 [0.084]] [[51.228]
 [48.487]
 [51.228]
 [51.228]
 [51.228]
 [51.228]
 [51.228]] [[0.327]
 [0.394]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.003113333333333488 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  32.50939130783081
printing an ep nov before normalisation:  33.41087103016398
using another actor
siam score:  -0.8039636
maxi score, test score, baseline:  -0.003113333333333488 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  52.2108043430168
printing an ep nov before normalisation:  29.33491296515871
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.003113333333333488 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  0 policy actor:  1  step number:  39 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.00016666666666650655 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  38.38775081506765
actor:  1 policy actor:  1  step number:  69 total reward:  0.13333333333333286  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.00016666666666650655 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.00016666666666650655 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
siam score:  -0.8085292
maxi score, test score, baseline:  0.00016666666666650655 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  37.42386873467556
printing an ep nov before normalisation:  27.039216731812754
maxi score, test score, baseline:  0.00016666666666650655 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  61 total reward:  0.07999999999999952  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.02]
 [-0.02]
 [-0.02]
 [-0.02]
 [-0.02]
 [-0.02]
 [-0.02]] [[55.116]
 [55.116]
 [55.116]
 [55.116]
 [55.116]
 [55.116]
 [55.116]] [[1.263]
 [1.263]
 [1.263]
 [1.263]
 [1.263]
 [1.263]
 [1.263]]
maxi score, test score, baseline:  0.00016666666666650655 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.00016666666666650655 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  20.482664108276367
Printing some Q and Qe and total Qs values:  [[0.691]
 [0.764]
 [0.661]
 [0.68 ]
 [0.669]
 [0.677]
 [0.732]] [[14.169]
 [14.065]
 [13.97 ]
 [13.782]
 [13.689]
 [14.054]
 [14.382]] [[1.065]
 [1.136]
 [1.03 ]
 [1.044]
 [1.031]
 [1.049]
 [1.112]]
printing an ep nov before normalisation:  23.364570910291025
maxi score, test score, baseline:  0.00016666666666650655 0.6933333333333334 0.6933333333333334
actor:  1 policy actor:  1  step number:  63 total reward:  0.4266666666666663  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.00016666666666650655 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  43.388336622611895
printing an ep nov before normalisation:  39.84008095432974
printing an ep nov before normalisation:  21.55585937588331
maxi score, test score, baseline:  0.00016666666666650655 0.6933333333333334 0.6933333333333334
actor:  1 policy actor:  1  step number:  50 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  30.522572994232178
maxi score, test score, baseline:  0.00016666666666650655 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.00016666666666650655 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.00016666666666650655 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.00016666666666650655 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.00016666666666650655 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.00016666666666650655 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  30.474915507511557
maxi score, test score, baseline:  0.00016666666666650655 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  32.5820225325236
siam score:  -0.82244575
actor:  1 policy actor:  1  step number:  65 total reward:  0.3599999999999992  reward:  1.0 rdn_beta:  1.333
from probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  0 policy actor:  0  step number:  42 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -6.6666666668183414e-06 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  -6.6666666668183414e-06 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  37.56639234550158
maxi score, test score, baseline:  -6.6666666668183414e-06 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  -6.6666666668183414e-06 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  -6.6666666668183414e-06 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  -6.6666666668183414e-06 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  30.798447804678283
actor:  0 policy actor:  0  step number:  63 total reward:  0.07999999999999952  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  31.35890409799204
Printing some Q and Qe and total Qs values:  [[0.726]
 [0.772]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.699]] [[27.299]
 [30.415]
 [27.299]
 [27.299]
 [27.299]
 [27.299]
 [26.756]] [[0.954]
 [1.049]
 [0.954]
 [0.954]
 [0.954]
 [0.954]
 [0.919]]
maxi score, test score, baseline:  -0.0006066666666668339 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  64 total reward:  0.15333333333333254  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.0006066666666668339 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  33.25226986454813
actor:  1 policy actor:  1  step number:  51 total reward:  0.22666666666666624  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  28.463527117448468
printing an ep nov before normalisation:  52.56863612895028
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.154]
 [0.199]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]] [[36.248]
 [39.288]
 [36.248]
 [36.248]
 [36.248]
 [36.248]
 [36.248]] [[1.001]
 [1.216]
 [1.001]
 [1.001]
 [1.001]
 [1.001]
 [1.001]]
actor:  1 policy actor:  1  step number:  43 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.0006066666666668339 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.212]
 [0.226]
 [0.181]
 [0.265]
 [0.265]
 [0.265]
 [0.196]] [[29.   ]
 [29.768]
 [28.502]
 [31.726]
 [31.726]
 [31.726]
 [29.821]] [[1.426]
 [1.505]
 [1.354]
 [1.709]
 [1.709]
 [1.709]
 [1.48 ]]
maxi score, test score, baseline:  -0.0006066666666668339 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  -0.0006066666666668339 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  -0.0006066666666668339 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  -0.0006066666666668339 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  -0.0006066666666668339 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.456]
 [0.424]
 [0.423]
 [0.411]
 [0.423]
 [0.422]] [[34.577]
 [32.692]
 [34.018]
 [34.076]
 [34.053]
 [34.204]
 [33.809]] [[1.862]
 [1.743]
 [1.822]
 [1.826]
 [1.812]
 [1.837]
 [1.803]]
maxi score, test score, baseline:  -0.0006066666666668339 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.657]
 [0.7  ]
 [0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]] [[29.489]
 [33.207]
 [29.489]
 [29.489]
 [29.489]
 [29.489]
 [29.489]] [[1.781]
 [2.068]
 [1.781]
 [1.781]
 [1.781]
 [1.781]
 [1.781]]
line 256 mcts: sample exp_bonus 30.70772194363533
maxi score, test score, baseline:  -0.0006066666666668339 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  -0.0006066666666668339 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  49 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.096]
 [0.096]
 [0.088]
 [0.096]
 [0.096]
 [0.096]
 [0.096]] [[13.761]
 [13.761]
 [13.926]
 [13.761]
 [13.761]
 [13.761]
 [13.761]] [[0.248]
 [0.248]
 [0.243]
 [0.248]
 [0.248]
 [0.248]
 [0.248]]
printing an ep nov before normalisation:  48.64476363095459
siam score:  -0.82006055
printing an ep nov before normalisation:  31.297199990540868
actor:  1 policy actor:  1  step number:  34 total reward:  0.5666666666666667  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.0006066666666668339 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  0 policy actor:  0  step number:  61 total reward:  0.026666666666665728  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.0010200000000001538 0.6933333333333334 0.6933333333333334
from probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  -0.0010200000000001538 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  -0.0010200000000001538 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  -0.0010200000000001538 0.6933333333333334 0.6933333333333334
siam score:  -0.812201
maxi score, test score, baseline:  -0.0010200000000001608 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  -0.0010200000000001608 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  61 total reward:  0.3333333333333327  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.0010200000000001608 0.6933333333333334 0.6933333333333334
printing an ep nov before normalisation:  40.845706114700114
maxi score, test score, baseline:  -0.0010200000000001608 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  46.89352959487449
printing an ep nov before normalisation:  32.23989320850834
actor:  1 policy actor:  1  step number:  44 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.0010200000000001608 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  33.03979422686451
printing an ep nov before normalisation:  10.07931781377124
actor:  1 policy actor:  1  step number:  55 total reward:  0.17333333333333278  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.0010200000000001608 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
using explorer policy with actor:  1
printing an ep nov before normalisation:  27.99500301894581
maxi score, test score, baseline:  -0.0010200000000001608 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actions average: 
K:  4  action  0 :  tensor([0.4868, 0.0428, 0.1020, 0.0833, 0.0929, 0.0939, 0.0983],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0142, 0.9308, 0.0119, 0.0110, 0.0051, 0.0089, 0.0181],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1294, 0.0634, 0.1885, 0.1359, 0.1568, 0.1267, 0.1994],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1752, 0.0891, 0.0904, 0.3388, 0.0712, 0.1256, 0.1097],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1757, 0.0614, 0.0491, 0.0539, 0.5492, 0.0520, 0.0587],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2182, 0.0175, 0.1110, 0.1112, 0.1143, 0.3354, 0.0925],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2213, 0.2592, 0.0601, 0.0741, 0.0442, 0.0611, 0.2801],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  41.52998777424451
actions average: 
K:  2  action  0 :  tensor([0.6168, 0.0033, 0.0643, 0.0834, 0.0898, 0.0763, 0.0662],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0110, 0.9155, 0.0048, 0.0249, 0.0035, 0.0035, 0.0367],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1451, 0.0101, 0.4730, 0.1430, 0.0533, 0.1034, 0.0721],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1295, 0.0052, 0.0637, 0.5540, 0.0757, 0.0873, 0.0845],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2965, 0.0048, 0.0741, 0.1216, 0.3423, 0.0830, 0.0775],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1814, 0.2437, 0.1256, 0.1226, 0.1135, 0.1009, 0.1123],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1947, 0.1227, 0.0753, 0.2021, 0.1498, 0.1028, 0.1527],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  53 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.0008600000000001571 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
siam score:  -0.8107935
actor:  1 policy actor:  1  step number:  47 total reward:  0.45333333333333303  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8105356
maxi score, test score, baseline:  -0.0008600000000001571 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  34.70608472824097
printing an ep nov before normalisation:  50.85795953933757
printing an ep nov before normalisation:  46.42363527630574
Printing some Q and Qe and total Qs values:  [[0.869]
 [0.935]
 [0.869]
 [0.869]
 [0.869]
 [0.869]
 [0.869]] [[51.575]
 [50.334]
 [51.575]
 [51.575]
 [51.575]
 [51.575]
 [51.575]] [[0.869]
 [0.935]
 [0.869]
 [0.869]
 [0.869]
 [0.869]
 [0.869]]
maxi score, test score, baseline:  -0.0008600000000001571 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  -0.0008600000000001571 0.6933333333333334 0.6933333333333334
line 256 mcts: sample exp_bonus 43.642939496807806
line 256 mcts: sample exp_bonus 31.60523044545729
siam score:  -0.8076941
actor:  0 policy actor:  1  step number:  45 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  1.333
siam score:  -0.8105814
maxi score, test score, baseline:  0.0019133333333331763 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  0 policy actor:  1  step number:  49 total reward:  0.37333333333333274  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  44.10759789203909
maxi score, test score, baseline:  0.004659999999999839 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  45 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.041]
 [0.145]
 [0.041]
 [0.041]
 [0.041]
 [0.041]
 [0.041]] [[53.259]
 [56.487]
 [53.259]
 [53.259]
 [53.259]
 [53.259]
 [53.259]] [[1.289]
 [1.547]
 [1.289]
 [1.289]
 [1.289]
 [1.289]
 [1.289]]
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.58 ]
 [0.492]
 [0.492]
 [0.446]
 [0.473]
 [0.492]] [[38.539]
 [36.208]
 [38.539]
 [38.539]
 [40.768]
 [41.167]
 [38.539]] [[1.004]
 [1.03 ]
 [1.004]
 [1.004]
 [1.017]
 [1.054]
 [1.004]]
maxi score, test score, baseline:  0.004659999999999839 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.08121421574311
actor:  1 policy actor:  1  step number:  47 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  48.16129684448242
printing an ep nov before normalisation:  41.8625998325587
maxi score, test score, baseline:  0.004659999999999839 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.004659999999999839 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]] [[27.72]
 [27.72]
 [27.72]
 [27.72]
 [27.72]
 [27.72]
 [27.72]] [[0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]]
printing an ep nov before normalisation:  50.0689293753743
actor:  0 policy actor:  0  step number:  49 total reward:  0.30666666666666653  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.007273333333333176 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.44 ]
 [0.592]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]] [[45.526]
 [41.15 ]
 [45.526]
 [45.526]
 [45.526]
 [45.526]
 [45.526]] [[1.117]
 [1.152]
 [1.117]
 [1.117]
 [1.117]
 [1.117]
 [1.117]]
printing an ep nov before normalisation:  43.23162858393459
maxi score, test score, baseline:  0.007273333333333176 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.007273333333333176 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.007273333333333176 0.6933333333333334 0.6933333333333334
actions average: 
K:  1  action  0 :  tensor([0.4856, 0.0083, 0.0918, 0.0955, 0.1384, 0.0912, 0.0893],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0165, 0.9179, 0.0208, 0.0106, 0.0041, 0.0052, 0.0249],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1124, 0.0040, 0.6222, 0.0610, 0.0505, 0.0913, 0.0586],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1682, 0.0599, 0.0840, 0.3808, 0.0823, 0.0992, 0.1256],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1881, 0.0063, 0.0564, 0.0681, 0.5760, 0.0546, 0.0506],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1296, 0.0007, 0.1521, 0.0632, 0.0512, 0.5321, 0.0710],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1412, 0.0086, 0.1582, 0.1172, 0.0576, 0.0874, 0.4298],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  23.47886760741069
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.514]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]] [[19.034]
 [23.611]
 [19.034]
 [19.034]
 [19.034]
 [19.034]
 [19.034]] [[1.035]
 [1.126]
 [1.035]
 [1.035]
 [1.035]
 [1.035]
 [1.035]]
actor:  1 policy actor:  1  step number:  56 total reward:  0.24666666666666648  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.007273333333333176 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  0 policy actor:  0  step number:  54 total reward:  0.29999999999999916  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]] [[35.141]
 [35.141]
 [35.141]
 [35.141]
 [35.141]
 [35.141]
 [35.141]] [[1.582]
 [1.582]
 [1.582]
 [1.582]
 [1.582]
 [1.582]
 [1.582]]
maxi score, test score, baseline:  0.007179999999999842 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
siam score:  -0.81243753
maxi score, test score, baseline:  0.007179999999999842 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
line 256 mcts: sample exp_bonus 26.22841039827382
maxi score, test score, baseline:  0.007179999999999842 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  54 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  52 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.007179999999999842 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  46.77359580993652
maxi score, test score, baseline:  0.007179999999999842 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.007179999999999842 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.007179999999999842 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  36.194556827782755
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.471]
 [0.408]
 [0.384]
 [0.384]
 [0.384]
 [0.384]] [[42.492]
 [43.387]
 [41.293]
 [38.384]
 [38.384]
 [38.384]
 [38.384]] [[1.858]
 [1.956]
 [1.747]
 [1.522]
 [1.522]
 [1.522]
 [1.522]]
printing an ep nov before normalisation:  33.98722321615737
printing an ep nov before normalisation:  37.79783008678494
Printing some Q and Qe and total Qs values:  [[0.735]
 [0.848]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]] [[30.373]
 [31.495]
 [30.373]
 [30.373]
 [30.373]
 [30.373]
 [30.373]] [[0.735]
 [0.848]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]]
maxi score, test score, baseline:  0.007179999999999842 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  57 total reward:  0.3999999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  69 total reward:  0.14666666666666595  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.006899999999999834 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.006899999999999834 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.006899999999999834 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  48.532701944295994
maxi score, test score, baseline:  0.006899999999999834 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  0 policy actor:  1  step number:  49 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.009673333333333162 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.009673333333333162 0.6933333333333334 0.6933333333333334
printing an ep nov before normalisation:  39.24499244568097
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.334]
 [0.512]
 [0.512]
 [0.519]
 [0.511]
 [0.506]] [[27.221]
 [23.827]
 [19.963]
 [19.963]
 [21.148]
 [21.051]
 [21.906]] [[1.543]
 [1.285]
 [1.308]
 [1.308]
 [1.363]
 [1.351]
 [1.38 ]]
actor:  1 policy actor:  1  step number:  62 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  36.91499984672617
printing an ep nov before normalisation:  64.89709479901362
printing an ep nov before normalisation:  40.986387916168255
maxi score, test score, baseline:  0.009673333333333162 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
line 256 mcts: sample exp_bonus 29.200440757312293
maxi score, test score, baseline:  0.009673333333333162 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  48 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.009673333333333162 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.009673333333333162 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  53 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  0.333
from probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
siam score:  -0.8132821
printing an ep nov before normalisation:  41.571233753531175
maxi score, test score, baseline:  0.009673333333333162 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  34.11904420225793
maxi score, test score, baseline:  0.009673333333333166 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  33.335888385772705
maxi score, test score, baseline:  0.009673333333333166 0.6933333333333334 0.6933333333333334
printing an ep nov before normalisation:  42.84618838635998
actor:  1 policy actor:  1  step number:  62 total reward:  0.19333333333333247  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  47.34506790645279
maxi score, test score, baseline:  0.009673333333333166 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  30.003883712582777
Printing some Q and Qe and total Qs values:  [[0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]] [[56.033]
 [56.033]
 [56.033]
 [56.033]
 [56.033]
 [56.033]
 [56.033]] [[1.135]
 [1.135]
 [1.135]
 [1.135]
 [1.135]
 [1.135]
 [1.135]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.41333333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.009673333333333166 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.009673333333333166 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  0.009673333333333166 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.009673333333333166 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
siam score:  -0.81003815
maxi score, test score, baseline:  0.009673333333333166 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.009673333333333166 0.6933333333333334 0.6933333333333334
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.009673333333333166 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  59 total reward:  0.13333333333333286  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.009673333333333166 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  52 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.009673333333333166 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.44 ]
 [0.466]
 [0.44 ]
 [0.384]
 [0.44 ]
 [0.44 ]
 [0.396]] [[36.233]
 [38.592]
 [36.233]
 [36.464]
 [36.233]
 [36.233]
 [37.514]] [[1.401]
 [1.539]
 [1.401]
 [1.356]
 [1.401]
 [1.401]
 [1.418]]
maxi score, test score, baseline:  0.009673333333333166 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.009673333333333166 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.009673333333333166 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
line 256 mcts: sample exp_bonus 34.165117966766154
maxi score, test score, baseline:  0.009673333333333166 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.009673333333333166 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  25.040550940254683
maxi score, test score, baseline:  0.009673333333333166 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  35.51534822270204
printing an ep nov before normalisation:  0.0009804911957189688
actor:  1 policy actor:  1  step number:  63 total reward:  0.05333333333333279  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.009673333333333166 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  72 total reward:  0.08666666666666589  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  46.10133450224203
maxi score, test score, baseline:  0.009673333333333166 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  33.764818642898945
maxi score, test score, baseline:  0.009673333333333166 0.6933333333333334 0.6933333333333334
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.622]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.598]] [[26.848]
 [25.928]
 [26.004]
 [26.004]
 [26.004]
 [26.004]
 [25.979]] [[0.502]
 [0.622]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.598]]
printing an ep nov before normalisation:  36.03294026910124
printing an ep nov before normalisation:  26.0211181640625
maxi score, test score, baseline:  0.009673333333333166 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  0 policy actor:  0  step number:  38 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.010073333333333176 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  61 total reward:  0.17333333333333278  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.010073333333333176 0.6933333333333334 0.6933333333333334
printing an ep nov before normalisation:  22.0162034034729
maxi score, test score, baseline:  0.010073333333333176 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.010073333333333176 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  36.90710773991931
maxi score, test score, baseline:  0.010073333333333176 0.6933333333333334 0.6933333333333334
actor:  1 policy actor:  1  step number:  68 total reward:  0.19333333333333258  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.010073333333333176 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  48 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.26 ]
 [0.26 ]
 [0.26 ]
 [0.26 ]
 [0.26 ]
 [0.179]
 [0.26 ]] [[34.539]
 [34.539]
 [34.539]
 [34.539]
 [34.539]
 [35.703]
 [34.539]] [[0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.512]
 [0.572]]
printing an ep nov before normalisation:  23.758500799331596
printing an ep nov before normalisation:  37.62185711711423
printing an ep nov before normalisation:  37.40627898119455
maxi score, test score, baseline:  0.010073333333333176 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.010073333333333176 0.6933333333333334 0.6933333333333334
printing an ep nov before normalisation:  29.01961326599121
maxi score, test score, baseline:  0.010073333333333176 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.010073333333333176 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  67 total reward:  0.11999999999999933  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  18.725571632385254
maxi score, test score, baseline:  0.010073333333333176 0.6933333333333334 0.6933333333333334
actor:  0 policy actor:  0  step number:  58 total reward:  0.11333333333333284  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.012299999999999832 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  30.81804146862055
printing an ep nov before normalisation:  33.76381484312171
printing an ep nov before normalisation:  25.605142422051674
actor:  1 policy actor:  1  step number:  43 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.012299999999999832 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  47 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  1.333
from probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  29.739568982237206
printing an ep nov before normalisation:  32.28580951690674
printing an ep nov before normalisation:  25.058357737467013
printing an ep nov before normalisation:  33.342437744140625
maxi score, test score, baseline:  0.012299999999999832 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  45 total reward:  0.4933333333333333  reward:  1.0 rdn_beta:  0.667
from probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.012299999999999825 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  26.903506340198895
maxi score, test score, baseline:  0.012299999999999825 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.012299999999999825 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.009966666666666499 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  45.43284490233636
maxi score, test score, baseline:  0.009966666666666499 0.6933333333333334 0.6933333333333334
actor:  1 policy actor:  1  step number:  45 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.009966666666666499 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.009966666666666499 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actions average: 
K:  4  action  0 :  tensor([0.3489, 0.0144, 0.0879, 0.1923, 0.1313, 0.1179, 0.1074],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0076, 0.9433, 0.0040, 0.0088, 0.0022, 0.0020, 0.0321],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1474, 0.0857, 0.4553, 0.0666, 0.0428, 0.1061, 0.0961],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1548, 0.0176, 0.0375, 0.3621, 0.2504, 0.0677, 0.1099],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2507, 0.0025, 0.0846, 0.1166, 0.3549, 0.1086, 0.0821],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1135, 0.0200, 0.0781, 0.1727, 0.0868, 0.4680, 0.0609],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1654, 0.1433, 0.0508, 0.1136, 0.0719, 0.0581, 0.3968],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  31.094016446142394
maxi score, test score, baseline:  0.009966666666666499 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  35.835502838579444
printing an ep nov before normalisation:  49.85655745902822
maxi score, test score, baseline:  0.009966666666666499 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.533]
 [0.463]
 [0.463]
 [0.463]
 [0.515]
 [0.463]] [[33.581]
 [36.849]
 [31.971]
 [31.971]
 [31.971]
 [34.757]
 [31.971]] [[1.649]
 [1.915]
 [1.46 ]
 [1.46 ]
 [1.46 ]
 [1.733]
 [1.46 ]]
maxi score, test score, baseline:  0.009966666666666499 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  62 total reward:  0.15333333333333277  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.009966666666666499 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.009966666666666499 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.009966666666666499 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.612]
 [0.464]
 [0.476]
 [0.425]
 [0.42 ]
 [0.501]] [[21.218]
 [19.82 ]
 [21.897]
 [23.568]
 [23.679]
 [24.343]
 [22.722]] [[1.757]
 [1.834]
 [1.813]
 [1.928]
 [1.885]
 [1.92 ]
 [1.901]]
maxi score, test score, baseline:  0.009966666666666499 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.009966666666666499 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  56 total reward:  0.11333333333333284  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.009966666666666499 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.009966666666666499 0.6933333333333334 0.6933333333333334
actions average: 
K:  0  action  0 :  tensor([0.5680, 0.0039, 0.0661, 0.0733, 0.1086, 0.0848, 0.0954],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0104,     0.9712,     0.0019,     0.0029,     0.0007,     0.0010,
            0.0118], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0636, 0.0009, 0.6845, 0.0490, 0.0533, 0.1002, 0.0485],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1695, 0.0050, 0.1155, 0.3338, 0.1067, 0.1110, 0.1585],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1770, 0.0025, 0.1232, 0.1171, 0.2944, 0.1461, 0.1397],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1397, 0.0039, 0.1623, 0.0834, 0.0950, 0.3993, 0.1164],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2225, 0.0384, 0.0956, 0.0959, 0.1092, 0.0860, 0.3523],
       grad_fn=<DivBackward0>)
actions average: 
K:  2  action  0 :  tensor([0.5636, 0.0185, 0.0538, 0.0529, 0.1052, 0.0606, 0.1454],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0184, 0.9528, 0.0049, 0.0078, 0.0037, 0.0044, 0.0079],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0981, 0.0618, 0.4030, 0.0945, 0.0574, 0.1362, 0.1491],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2388, 0.0548, 0.0839, 0.3234, 0.0915, 0.0880, 0.1194],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2568, 0.0225, 0.0285, 0.0354, 0.5655, 0.0366, 0.0546],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1248, 0.0141, 0.1452, 0.1030, 0.0767, 0.4297, 0.1066],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1214, 0.1145, 0.0535, 0.0995, 0.0880, 0.0853, 0.4377],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  61 total reward:  0.10666666666666613  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  33.87723507556273
Printing some Q and Qe and total Qs values:  [[0.458]
 [0.497]
 [0.454]
 [0.43 ]
 [0.459]
 [0.449]
 [0.42 ]] [[31.028]
 [29.361]
 [32.382]
 [32.431]
 [32.776]
 [33.432]
 [32.318]] [[0.685]
 [0.7  ]
 [0.701]
 [0.678]
 [0.712]
 [0.711]
 [0.665]]
maxi score, test score, baseline:  0.009966666666666499 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  29.584471430395855
actor:  1 policy actor:  1  step number:  57 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.009966666666666499 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.22105750198307
actor:  0 policy actor:  0  step number:  48 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.012779999999999824 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.012779999999999824 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  29.260985332766673
maxi score, test score, baseline:  0.012779999999999824 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  41.20143139765769
maxi score, test score, baseline:  0.012779999999999824 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  44 total reward:  0.5533333333333333  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  26.112799644470215
actor:  1 policy actor:  1  step number:  45 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.442]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]] [[32.131]
 [28.013]
 [32.131]
 [32.131]
 [32.131]
 [32.131]
 [32.131]] [[1.753]
 [1.507]
 [1.753]
 [1.753]
 [1.753]
 [1.753]
 [1.753]]
maxi score, test score, baseline:  0.012779999999999824 0.6933333333333334 0.6933333333333334
printing an ep nov before normalisation:  30.09962016702609
printing an ep nov before normalisation:  36.08971255039173
maxi score, test score, baseline:  0.012779999999999824 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  25.835368633270264
actor:  1 policy actor:  1  step number:  51 total reward:  0.4266666666666663  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.012779999999999824 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.012779999999999824 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  0 policy actor:  0  step number:  65 total reward:  0.10666666666666591  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 42.821953177564744
printing an ep nov before normalisation:  28.630106449127197
actor:  1 policy actor:  1  step number:  37 total reward:  0.56  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  46.57924123351076
maxi score, test score, baseline:  0.014993333333333156 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
using another actor
printing an ep nov before normalisation:  44.31312580374609
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.403]
 [0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.359]] [[35.925]
 [37.582]
 [35.925]
 [35.925]
 [35.925]
 [35.925]
 [38.04 ]] [[0.536]
 [0.614]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.575]]
maxi score, test score, baseline:  0.014993333333333156 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.014993333333333156 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[-0.208]
 [-0.216]
 [-0.212]
 [-0.209]
 [-0.218]
 [-0.209]
 [-0.205]] [[6.348]
 [5.646]
 [6.052]
 [3.954]
 [6.227]
 [6.743]
 [7.269]] [[ 0.007]
 [-0.025]
 [-0.008]
 [-0.076]
 [-0.008]
 [ 0.019]
 [ 0.041]]
maxi score, test score, baseline:  0.014993333333333156 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [ 0.224]
 [ 0.075]
 [ 0.093]
 [ 0.082]
 [ 0.13 ]
 [ 0.117]] [[37.649]
 [44.308]
 [35.608]
 [30.769]
 [21.1  ]
 [37.225]
 [36.196]] [[0.519]
 [0.932]
 [0.548]
 [0.435]
 [0.162]
 [0.646]
 [0.606]]
maxi score, test score, baseline:  0.014993333333333156 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  49.74965901958002
printing an ep nov before normalisation:  34.01517868041992
maxi score, test score, baseline:  0.014993333333333156 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  0 policy actor:  1  step number:  61 total reward:  0.15999999999999914  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.252]
 [0.285]
 [0.252]
 [0.252]
 [0.252]
 [0.252]
 [0.252]] [[40.468]
 [43.479]
 [40.468]
 [40.468]
 [40.468]
 [40.468]
 [40.468]] [[1.09 ]
 [1.239]
 [1.09 ]
 [1.09 ]
 [1.09 ]
 [1.09 ]
 [1.09 ]]
maxi score, test score, baseline:  0.017313333333333153 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.017313333333333153 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.017313333333333153 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  0.0
from probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  50 total reward:  0.31333333333333313  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.017313333333333153 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.017313333333333153 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
siam score:  -0.8221954
maxi score, test score, baseline:  0.014966666666666486 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  50 total reward:  0.35333333333333317  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  3  action  0 :  tensor([0.4596, 0.0057, 0.0896, 0.1154, 0.1285, 0.1016, 0.0996],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0077, 0.9493, 0.0059, 0.0112, 0.0040, 0.0044, 0.0175],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0342, 0.0221, 0.6960, 0.0723, 0.0114, 0.1234, 0.0407],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1405, 0.1120, 0.1316, 0.2956, 0.0970, 0.1006, 0.1227],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2634, 0.0116, 0.0437, 0.0620, 0.5352, 0.0449, 0.0392],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.2361, 0.0081, 0.1070, 0.1377, 0.1295, 0.2416, 0.1400],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1036, 0.0201, 0.0970, 0.0651, 0.0522, 0.0464, 0.6157],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  46 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.014966666666666486 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.014966666666666486 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.014966666666666486 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  32.753896084151634
maxi score, test score, baseline:  0.014966666666666486 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.014966666666666486 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.014966666666666486 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  51 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.014966666666666486 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.014966666666666486 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  51 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.014966666666666486 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.014966666666666486 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.014966666666666486 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.014966666666666486 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  59 total reward:  0.21333333333333304  reward:  1.0 rdn_beta:  1.333
from probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.014966666666666486 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  68 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  27.87097692489624
printing an ep nov before normalisation:  29.247605743134734
printing an ep nov before normalisation:  32.22697837272067
maxi score, test score, baseline:  0.014966666666666486 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
line 256 mcts: sample exp_bonus 29.651495830320286
actor:  0 policy actor:  0  step number:  58 total reward:  0.24666666666666637  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.01745999999999983 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.01745999999999983 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.679]
 [0.679]
 [0.679]
 [0.679]
 [0.679]
 [0.658]
 [0.679]] [[27.848]
 [27.848]
 [27.848]
 [27.848]
 [27.848]
 [28.679]
 [27.848]] [[0.994]
 [0.994]
 [0.994]
 [0.994]
 [0.994]
 [0.991]
 [0.994]]
siam score:  -0.80843824
actor:  1 policy actor:  1  step number:  44 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.01745999999999983 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
from probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.01745999999999983 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  30.095320897401134
maxi score, test score, baseline:  0.015179999999999827 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.015179999999999827 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actions average: 
K:  1  action  0 :  tensor([0.6489, 0.0229, 0.0464, 0.0693, 0.0716, 0.0461, 0.0948],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0086, 0.9475, 0.0058, 0.0121, 0.0061, 0.0059, 0.0141],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1345, 0.0043, 0.5459, 0.0718, 0.0742, 0.0860, 0.0834],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1601, 0.1634, 0.0584, 0.3319, 0.1425, 0.0629, 0.0807],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2128, 0.0030, 0.0833, 0.0943, 0.4199, 0.0941, 0.0926],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1661, 0.0320, 0.1067, 0.1243, 0.1213, 0.3299, 0.1196],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1950, 0.0707, 0.1438, 0.1001, 0.1079, 0.0950, 0.2875],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.015179999999999827 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.015179999999999827 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  0 policy actor:  1  step number:  35 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  46.541991233825684
maxi score, test score, baseline:  0.018326666666666495 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.018326666666666495 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.018326666666666495 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.018326666666666495 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
line 256 mcts: sample exp_bonus 12.428337534086788
printing an ep nov before normalisation:  34.48150244280745
siam score:  -0.81938624
maxi score, test score, baseline:  0.018326666666666495 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.018326666666666495 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.018326666666666495 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  54 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.004]
 [-0.004]
 [-0.025]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]] [[ 0.   ]
 [ 0.   ]
 [33.625]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.157]
 [-0.157]
 [ 1.121]
 [-0.157]
 [-0.157]
 [-0.157]
 [-0.157]]
printing an ep nov before normalisation:  42.183411157343635
line 256 mcts: sample exp_bonus 35.59878936434702
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.544]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]] [[39.497]
 [38.657]
 [39.497]
 [39.497]
 [39.497]
 [39.497]
 [39.497]] [[1.191]
 [1.228]
 [1.191]
 [1.191]
 [1.191]
 [1.191]
 [1.191]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  31.56196870575227
maxi score, test score, baseline:  0.018326666666666495 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.018326666666666495 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.018326666666666495 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  44 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  51 total reward:  0.48  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.018326666666666495 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  34.06637867354471
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.05420793250684
using explorer policy with actor:  1
siam score:  -0.8222977
printing an ep nov before normalisation:  40.29671425109443
printing an ep nov before normalisation:  28.92111301422119
actor:  1 policy actor:  1  step number:  41 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.018326666666666495 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[-0.075]
 [-0.075]
 [-0.075]
 [-0.075]
 [-0.075]
 [-0.075]
 [-0.075]] [[29.957]
 [29.957]
 [29.957]
 [29.957]
 [29.957]
 [29.957]
 [29.957]] [[1.586]
 [1.586]
 [1.586]
 [1.586]
 [1.586]
 [1.586]
 [1.586]]
printing an ep nov before normalisation:  25.720512866973877
actor:  1 policy actor:  1  step number:  60 total reward:  0.17999999999999916  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.14577865600586
maxi score, test score, baseline:  0.018326666666666495 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  51.6374866183098
siam score:  -0.8182555
maxi score, test score, baseline:  0.018326666666666495 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.018326666666666495 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
line 256 mcts: sample exp_bonus 29.640007548252036
actor:  1 policy actor:  1  step number:  41 total reward:  0.4933333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.018326666666666495 0.6933333333333334 0.6933333333333334
printing an ep nov before normalisation:  34.1940497809959
actor:  1 policy actor:  1  step number:  59 total reward:  0.41333333333333266  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.018326666666666495 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  43.12694887785329
maxi score, test score, baseline:  0.018326666666666495 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  23.02949578329488
printing an ep nov before normalisation:  27.97087122105811
maxi score, test score, baseline:  0.018326666666666495 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.018326666666666495 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  30.56638613209894
maxi score, test score, baseline:  0.018326666666666495 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  52 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.018326666666666495 0.6933333333333334 0.6933333333333334
Printing some Q and Qe and total Qs values:  [[0.556]
 [0.621]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]] [[27.422]
 [30.848]
 [27.422]
 [27.422]
 [27.422]
 [27.422]
 [27.422]] [[1.547]
 [1.861]
 [1.547]
 [1.547]
 [1.547]
 [1.547]
 [1.547]]
actor:  1 policy actor:  1  step number:  42 total reward:  0.47333333333333305  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.018326666666666495 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.018326666666666495 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  40.335486673697574
Printing some Q and Qe and total Qs values:  [[0.258]
 [0.284]
 [0.027]
 [0.258]
 [0.258]
 [0.258]
 [0.258]] [[34.971]
 [34.052]
 [39.464]
 [34.971]
 [34.971]
 [34.971]
 [34.971]] [[0.536]
 [0.551]
 [0.36 ]
 [0.536]
 [0.536]
 [0.536]
 [0.536]]
maxi score, test score, baseline:  0.018326666666666495 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.603]
 [0.576]
 [0.533]
 [0.567]
 [0.534]
 [0.562]] [[26.01 ]
 [30.24 ]
 [25.59 ]
 [24.695]
 [28.606]
 [24.805]
 [24.749]] [[1.214]
 [1.495]
 [1.24 ]
 [1.153]
 [1.379]
 [1.159]
 [1.185]]
line 256 mcts: sample exp_bonus 26.07111692428589
maxi score, test score, baseline:  0.018326666666666495 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  37 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.018326666666666495 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actions average: 
K:  4  action  0 :  tensor([0.5893, 0.0243, 0.0626, 0.0828, 0.0858, 0.0816, 0.0737],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0020, 0.9057, 0.0110, 0.0400, 0.0016, 0.0144, 0.0254],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1435, 0.0385, 0.4051, 0.0975, 0.0607, 0.1627, 0.0920],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1800, 0.0363, 0.1108, 0.2980, 0.0841, 0.1233, 0.1675],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1696, 0.0232, 0.0731, 0.1059, 0.4258, 0.1066, 0.0958],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.3085, 0.0200, 0.0974, 0.1248, 0.1814, 0.1303, 0.1376],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2353, 0.3243, 0.0768, 0.0832, 0.0695, 0.0767, 0.1343],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.018326666666666495 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  62 total reward:  0.2333333333333325  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.018326666666666495 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.018326666666666495 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  48.91300547956976
Printing some Q and Qe and total Qs values:  [[0.156]
 [0.164]
 [0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]] [[40.067]
 [44.514]
 [40.067]
 [40.067]
 [40.067]
 [40.067]
 [40.067]] [[0.605]
 [0.712]
 [0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]]
actor:  0 policy actor:  1  step number:  39 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.072]
 [0.024]
 [0.024]
 [0.024]
 [0.024]
 [0.024]] [[45.923]
 [49.611]
 [45.923]
 [45.923]
 [45.923]
 [45.923]
 [45.923]] [[1.47 ]
 [1.739]
 [1.47 ]
 [1.47 ]
 [1.47 ]
 [1.47 ]
 [1.47 ]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.44000000000000017  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.017846666666666497 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  43.01985863036429
Printing some Q and Qe and total Qs values:  [[0.172]
 [0.347]
 [0.172]
 [0.172]
 [0.172]
 [0.171]
 [0.172]] [[39.118]
 [45.483]
 [39.118]
 [39.118]
 [39.118]
 [44.017]
 [39.118]] [[0.587]
 [0.894]
 [0.587]
 [0.587]
 [0.587]
 [0.687]
 [0.587]]
actor:  1 policy actor:  1  step number:  58 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.017846666666666497 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.791]
 [0.669]
 [0.669]
 [0.669]
 [0.673]
 [0.669]
 [0.669]] [[45.727]
 [33.936]
 [33.936]
 [33.936]
 [37.365]
 [33.936]
 [33.936]] [[0.791]
 [0.669]
 [0.669]
 [0.669]
 [0.673]
 [0.669]
 [0.669]]
printing an ep nov before normalisation:  30.58708667755127
actor:  1 policy actor:  1  step number:  38 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  1.0
Starting evaluation
maxi score, test score, baseline:  0.017846666666666497 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.59 ]
 [0.638]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.59 ]
 [0.638]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]]
maxi score, test score, baseline:  0.017846666666666497 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  28.014638253599937
printing an ep nov before normalisation:  32.600298438385266
maxi score, test score, baseline:  0.017846666666666497 0.6933333333333334 0.6933333333333334
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  53.802910358147685
printing an ep nov before normalisation:  52.826595306396484
printing an ep nov before normalisation:  50.981854597494085
actor:  0 policy actor:  0  step number:  31 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  37.86449822180404
maxi score, test score, baseline:  0.017659999999999825 0.6933333333333334 0.6933333333333334
line 256 mcts: sample exp_bonus 31.25132396242896
printing an ep nov before normalisation:  27.42230162906497
printing an ep nov before normalisation:  29.021320126403097
printing an ep nov before normalisation:  32.25010318808422
printing an ep nov before normalisation:  6.436822916590188
printing an ep nov before normalisation:  1.652253160955297e-05
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  0.00023434785589415696
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]] [[40.28]
 [40.28]
 [40.28]
 [40.28]
 [40.28]
 [40.28]
 [40.28]] [[1.63]
 [1.63]
 [1.63]
 [1.63]
 [1.63]
 [1.63]
 [1.63]]
maxi score, test score, baseline:  0.030259999999999846 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.030259999999999846 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  35.297558723259826
maxi score, test score, baseline:  0.030259999999999846 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.030259999999999846 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  31.92574417796596
printing an ep nov before normalisation:  32.949124681588174
using another actor
printing an ep nov before normalisation:  30.860612931887193
maxi score, test score, baseline:  0.030259999999999846 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  60 total reward:  0.23333333333333262  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  56.66803503565367
using explorer policy with actor:  1
maxi score, test score, baseline:  0.03025999999999984 0.6930000000000001 0.6930000000000001
actions average: 
K:  2  action  0 :  tensor([0.5116, 0.0095, 0.0798, 0.1003, 0.1105, 0.1005, 0.0878],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0112, 0.9306, 0.0061, 0.0125, 0.0033, 0.0052, 0.0311],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1367, 0.0151, 0.4116, 0.1098, 0.0939, 0.0868, 0.1460],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1061, 0.0482, 0.0954, 0.3292, 0.1158, 0.1667, 0.1387],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1667, 0.0252, 0.0911, 0.0923, 0.3446, 0.1270, 0.1530],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1485, 0.0276, 0.1808, 0.0970, 0.1028, 0.3506, 0.0928],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.3390, 0.3135, 0.0339, 0.0715, 0.0656, 0.0434, 0.1330],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]] [[39.733]
 [39.733]
 [39.733]
 [39.733]
 [39.733]
 [39.733]
 [39.733]] [[1.721]
 [1.721]
 [1.721]
 [1.721]
 [1.721]
 [1.721]
 [1.721]]
printing an ep nov before normalisation:  27.48976469039917
maxi score, test score, baseline:  0.03025999999999984 0.6930000000000001 0.6930000000000001
actor:  1 policy actor:  1  step number:  41 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.03025999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  48 total reward:  0.5533333333333336  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.03025999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.03025999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  61.70182191253582
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.449]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]] [[39.131]
 [41.648]
 [39.131]
 [39.131]
 [39.131]
 [39.131]
 [39.131]] [[0.917]
 [1.101]
 [0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.83681737937929
maxi score, test score, baseline:  0.03025999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  48 total reward:  0.4066666666666662  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  61.45765462134666
maxi score, test score, baseline:  0.03025999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  55.92732136237443
maxi score, test score, baseline:  0.03025999999999984 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.03025999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.03025999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using another actor
Printing some Q and Qe and total Qs values:  [[0.198]
 [0.227]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.198]
 [0.227]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  36.27181018843814
maxi score, test score, baseline:  0.03025999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.03025999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  32.1315860748291
maxi score, test score, baseline:  0.03025999999999984 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.03025999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
actor:  0 policy actor:  1  step number:  40 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.030259999999999853 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  25.693794260426206
printing an ep nov before normalisation:  30.027214599108188
line 256 mcts: sample exp_bonus 43.91989516907818
maxi score, test score, baseline:  0.030259999999999853 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  34.15354238403383
actor:  1 policy actor:  1  step number:  62 total reward:  0.326666666666666  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.030259999999999853 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.030259999999999853 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.030259999999999853 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  45 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]]
maxi score, test score, baseline:  0.030259999999999853 0.6930000000000001 0.6930000000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.030259999999999853 0.6930000000000001 0.6930000000000001
actor:  1 policy actor:  1  step number:  34 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  42 total reward:  0.48666666666666647  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.030259999999999853 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  43.483313290037465
maxi score, test score, baseline:  0.030259999999999853 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  45.41083360930581
maxi score, test score, baseline:  0.030259999999999853 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.254]
 [0.421]
 [0.254]
 [0.254]
 [0.254]
 [0.254]
 [0.254]] [[48.282]
 [49.961]
 [48.282]
 [48.282]
 [48.282]
 [48.282]
 [48.282]] [[1.217]
 [1.438]
 [1.217]
 [1.217]
 [1.217]
 [1.217]
 [1.217]]
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]] [[36.275]
 [36.275]
 [36.275]
 [36.275]
 [36.275]
 [36.275]
 [36.275]] [[1.657]
 [1.657]
 [1.657]
 [1.657]
 [1.657]
 [1.657]
 [1.657]]
maxi score, test score, baseline:  0.030259999999999853 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  18.106600654883067
printing an ep nov before normalisation:  37.497980979555614
maxi score, test score, baseline:  0.030259999999999853 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  69 total reward:  0.10666666666666613  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.030259999999999853 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.030259999999999853 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.030259999999999853 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  25.887334665496333
actor:  1 policy actor:  1  step number:  49 total reward:  0.37333333333333274  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  49.788880348205566
actor:  0 policy actor:  0  step number:  39 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  43.76161779676165
maxi score, test score, baseline:  0.03084666666666649 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  33.43727796577381
Printing some Q and Qe and total Qs values:  [[0.865]
 [0.9  ]
 [0.865]
 [0.865]
 [0.865]
 [0.865]
 [0.865]] [[36.026]
 [34.361]
 [36.026]
 [36.026]
 [36.026]
 [36.026]
 [36.026]] [[0.865]
 [0.9  ]
 [0.865]
 [0.865]
 [0.865]
 [0.865]
 [0.865]]
Printing some Q and Qe and total Qs values:  [[0.863]
 [0.93 ]
 [0.846]
 [0.844]
 [0.846]
 [0.846]
 [0.847]] [[16.713]
 [29.329]
 [15.65 ]
 [15.813]
 [15.871]
 [15.948]
 [16.277]] [[0.863]
 [0.93 ]
 [0.846]
 [0.844]
 [0.846]
 [0.846]
 [0.847]]
actor:  0 policy actor:  1  step number:  47 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.03085999999999983 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.027953333333333174 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.027953333333333174 0.6930000000000001 0.6930000000000001
printing an ep nov before normalisation:  42.2673636308134
printing an ep nov before normalisation:  36.85131969953087
printing an ep nov before normalisation:  34.28886413574219
actions average: 
K:  3  action  0 :  tensor([0.7273, 0.0015, 0.0497, 0.0538, 0.0636, 0.0463, 0.0577],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0092, 0.9355, 0.0056, 0.0157, 0.0061, 0.0039, 0.0240],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0909, 0.0025, 0.6926, 0.0265, 0.0265, 0.1207, 0.0404],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2161, 0.1476, 0.1086, 0.1541, 0.1199, 0.0925, 0.1612],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1100, 0.0023, 0.0703, 0.0876, 0.5802, 0.0605, 0.0892],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1456, 0.0054, 0.1826, 0.0951, 0.0794, 0.3995, 0.0924],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1690, 0.2055, 0.1217, 0.1464, 0.1395, 0.1031, 0.1148],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  28.940320014953613
printing an ep nov before normalisation:  21.935453097775195
actor:  1 policy actor:  1  step number:  48 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.027953333333333174 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  67 total reward:  0.1733333333333329  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.027953333333333174 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.027953333333333174 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  41.437036832431204
maxi score, test score, baseline:  0.027953333333333174 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.027953333333333174 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.027953333333333174 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.027953333333333174 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  34.741353095685945
maxi score, test score, baseline:  0.027953333333333174 0.6930000000000001 0.6930000000000001
actor:  0 policy actor:  0  step number:  73 total reward:  0.03999999999999915  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.03003333333333316 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  23.496599197387695
maxi score, test score, baseline:  0.03003333333333316 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.03003333333333316 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  61 total reward:  0.2666666666666665  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.03003333333333316 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  50.1870549098336
siam score:  -0.81416285
maxi score, test score, baseline:  0.03003333333333316 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.03003333333333316 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  0.0605020680495727
actor:  1 policy actor:  1  step number:  42 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
siam score:  -0.81348586
maxi score, test score, baseline:  0.03003333333333316 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.03003333333333316 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.03003333333333316 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.03003333333333316 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  59 total reward:  0.3066666666666661  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  35.914961410922956
printing an ep nov before normalisation:  31.475251574410237
maxi score, test score, baseline:  0.03003333333333316 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  26.403357711636488
actor:  0 policy actor:  0  step number:  63 total reward:  0.4533333333333335  reward:  1.0 rdn_beta:  1.333
from probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.032939999999999844 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.032939999999999844 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
siam score:  -0.813585
actor:  0 policy actor:  1  step number:  46 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
printing an ep nov before normalisation:  38.17959413713313
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  46 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
siam score:  -0.8157066
actions average: 
K:  0  action  0 :  tensor([0.7279, 0.0056, 0.0452, 0.0536, 0.0639, 0.0411, 0.0627],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0056, 0.9638, 0.0027, 0.0081, 0.0017, 0.0021, 0.0160],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0765, 0.0115, 0.5768, 0.0879, 0.0543, 0.1134, 0.0796],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1256, 0.0485, 0.0969, 0.3026, 0.1191, 0.1328, 0.1746],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1849, 0.0009, 0.0643, 0.0832, 0.5322, 0.0572, 0.0774],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1771, 0.0016, 0.1113, 0.1024, 0.0727, 0.4345, 0.1003],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2136, 0.0221, 0.1272, 0.1307, 0.1032, 0.1199, 0.2833],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  51 total reward:  0.37333333333333285  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.533]
 [0.457]
 [0.457]
 [0.457]
 [0.457]
 [0.457]] [[45.599]
 [50.096]
 [45.599]
 [45.599]
 [45.599]
 [45.599]
 [45.599]] [[1.683]
 [1.963]
 [1.683]
 [1.683]
 [1.683]
 [1.683]
 [1.683]]
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  28.855205543562974
actor:  1 policy actor:  1  step number:  46 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  57 total reward:  0.1466666666666664  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  41.492136152365376
using explorer policy with actor:  1
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  50 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  47 total reward:  0.38666666666666605  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.406]
 [0.517]
 [0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]] [[34.153]
 [33.169]
 [34.153]
 [34.153]
 [34.153]
 [34.153]
 [34.153]] [[1.026]
 [1.1  ]
 [1.026]
 [1.026]
 [1.026]
 [1.026]
 [1.026]]
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  33.74847305579926
printing an ep nov before normalisation:  47.94337270937581
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.304]
 [0.341]
 [0.299]
 [0.342]
 [0.189]
 [0.322]] [[42.638]
 [37.016]
 [41.242]
 [39.357]
 [45.185]
 [44.346]
 [42.689]] [[1.194]
 [0.853]
 [1.036]
 [0.929]
 [1.172]
 [0.99 ]
 [1.066]]
printing an ep nov before normalisation:  50.35616150905454
actor:  1 policy actor:  1  step number:  44 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
siam score:  -0.82677156
Printing some Q and Qe and total Qs values:  [[ 0.039]
 [-0.023]
 [ 0.039]
 [ 0.039]
 [ 0.039]
 [ 0.039]
 [ 0.039]] [[ 0.   ]
 [44.429]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.308]
 [ 0.696]
 [-0.308]
 [-0.308]
 [-0.308]
 [-0.308]
 [-0.308]]
printing an ep nov before normalisation:  42.6217786670578
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  31.340493852590537
actor:  1 policy actor:  1  step number:  45 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
printing an ep nov before normalisation:  28.75239849090576
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  65 total reward:  0.10666666666666591  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  46.97585421653949
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]] [[40.279]
 [39.779]
 [39.779]
 [39.779]
 [39.779]
 [39.779]
 [39.779]] [[1.058]
 [0.957]
 [0.957]
 [0.957]
 [0.957]
 [0.957]
 [0.957]]
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  30.03319133212323
actor:  1 policy actor:  1  step number:  59 total reward:  0.25333333333333274  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.16 ]
 [0.304]
 [0.16 ]
 [0.16 ]
 [0.16 ]
 [0.16 ]
 [0.16 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.16 ]
 [0.304]
 [0.16 ]
 [0.16 ]
 [0.16 ]
 [0.16 ]
 [0.16 ]]
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  36.325320320425476
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  41.21687721886187
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  28.2459540124733
printing an ep nov before normalisation:  34.27589184196918
Printing some Q and Qe and total Qs values:  [[0.495]
 [0.608]
 [0.518]
 [0.495]
 [0.535]
 [0.495]
 [0.495]] [[25.878]
 [31.507]
 [27.715]
 [25.878]
 [31.8  ]
 [25.878]
 [25.878]] [[0.781]
 [1.06 ]
 [0.858]
 [0.781]
 [0.996]
 [0.781]
 [0.781]]
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.533307911285828
printing an ep nov before normalisation:  34.03508560391231
actor:  1 policy actor:  1  step number:  45 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
actor:  1 policy actor:  1  step number:  48 total reward:  0.3666666666666659  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  3  action  0 :  tensor([0.5500, 0.0310, 0.0641, 0.0664, 0.1150, 0.0869, 0.0866],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0144, 0.9255, 0.0053, 0.0077, 0.0036, 0.0035, 0.0399],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1007, 0.0024, 0.5917, 0.0534, 0.0432, 0.1471, 0.0615],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1970, 0.1258, 0.0619, 0.2442, 0.1321, 0.1103, 0.1287],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2556, 0.0325, 0.0839, 0.0915, 0.3599, 0.0790, 0.0976],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1865, 0.0913, 0.0866, 0.0663, 0.0598, 0.3977, 0.1118],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1626, 0.1600, 0.0618, 0.0667, 0.0446, 0.0417, 0.4625],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
siam score:  -0.81722015
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
siam score:  -0.81553465
actor:  1 policy actor:  1  step number:  48 total reward:  0.31333333333333313  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  50 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.911]
 [0.911]
 [0.911]
 [0.911]
 [0.911]
 [0.911]
 [0.911]] [[45.198]
 [45.198]
 [45.198]
 [45.198]
 [45.198]
 [45.198]
 [45.198]] [[0.911]
 [0.911]
 [0.911]
 [0.911]
 [0.911]
 [0.911]
 [0.911]]
line 256 mcts: sample exp_bonus 32.92885479055096
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  32.826094232727954
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.03312666666666651 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  0 policy actor:  0  step number:  43 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.03595333333333317 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  45 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  42.083308665281464
maxi score, test score, baseline:  0.03595333333333317 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  64.23181153424531
maxi score, test score, baseline:  0.03595333333333317 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  43.14209299711045
using explorer policy with actor:  1
maxi score, test score, baseline:  0.03595333333333317 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.03595333333333317 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.03595333333333317 0.6930000000000001 0.6930000000000001
actor:  0 policy actor:  0  step number:  47 total reward:  0.33333333333333315  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.37 ]
 [0.545]
 [0.409]
 [0.403]
 [0.381]
 [0.41 ]
 [0.404]] [[24.389]
 [17.573]
 [25.287]
 [25.213]
 [24.763]
 [24.972]
 [24.386]] [[1.528]
 [1.376]
 [1.609]
 [1.6  ]
 [1.556]
 [1.596]
 [1.562]]
maxi score, test score, baseline:  0.03861999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  47 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.03861999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  51 total reward:  0.2799999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.03861999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  66 total reward:  0.0599999999999995  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.03861999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
siam score:  -0.8104547
maxi score, test score, baseline:  0.03861999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  0 policy actor:  0  step number:  49 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.121]
 [0.266]
 [0.121]
 [0.121]
 [0.121]
 [0.121]
 [0.121]] [[40.076]
 [48.765]
 [40.076]
 [40.076]
 [40.076]
 [40.076]
 [40.076]] [[0.771]
 [1.19 ]
 [0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]]
maxi score, test score, baseline:  0.04123333333333317 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  53.85982708266538
actor:  1 policy actor:  1  step number:  65 total reward:  0.173333333333333  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.04123333333333317 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  49.20413904387856
maxi score, test score, baseline:  0.04123333333333317 0.6930000000000001 0.6930000000000001
printing an ep nov before normalisation:  25.307152011701298
maxi score, test score, baseline:  0.04123333333333317 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.04123333333333317 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.04123333333333317 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.04123333333333317 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.04123333333333317 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  60.22471113119373
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  29.056033990499458
printing an ep nov before normalisation:  29.505808168036868
actions average: 
K:  3  action  0 :  tensor([0.5453, 0.0074, 0.0611, 0.1044, 0.1033, 0.0965, 0.0821],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0128, 0.9406, 0.0084, 0.0063, 0.0014, 0.0016, 0.0288],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1892, 0.0137, 0.4012, 0.0798, 0.0758, 0.0999, 0.1404],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1210, 0.1671, 0.0865, 0.3342, 0.0765, 0.0935, 0.1212],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2278, 0.0016, 0.0566, 0.0650, 0.4872, 0.0788, 0.0830],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0915, 0.0358, 0.1418, 0.1876, 0.0840, 0.3865, 0.0727],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2145, 0.0127, 0.1358, 0.0893, 0.0549, 0.0810, 0.4118],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  26.47610277985747
actor:  1 policy actor:  1  step number:  50 total reward:  0.326666666666666  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  38.23778610787625
printing an ep nov before normalisation:  39.68899282627791
Printing some Q and Qe and total Qs values:  [[0.177]
 [0.212]
 [0.222]
 [0.222]
 [0.186]
 [0.167]
 [0.203]] [[47.795]
 [53.054]
 [53.756]
 [53.756]
 [44.455]
 [46.955]
 [48.053]] [[1.764]
 [2.163]
 [2.222]
 [2.222]
 [1.542]
 [1.697]
 [1.808]]
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  29.16330563939298
actions average: 
K:  2  action  0 :  tensor([0.5167, 0.0503, 0.0956, 0.0698, 0.0855, 0.0822, 0.0999],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0123, 0.9186, 0.0092, 0.0152, 0.0015, 0.0077, 0.0355],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1987, 0.0040, 0.4386, 0.0897, 0.0822, 0.0978, 0.0890],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1343, 0.0162, 0.0848, 0.4362, 0.1271, 0.1020, 0.0994],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.3314, 0.1084, 0.0893, 0.0876, 0.1998, 0.0944, 0.0891],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1115, 0.1162, 0.1244, 0.0747, 0.0709, 0.4194, 0.0829],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1610, 0.0900, 0.1020, 0.1144, 0.0976, 0.0944, 0.3406],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  29.591465593680052
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  53 total reward:  0.586666666666667  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  43.005598969303115
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  36.57849914280278
printing an ep nov before normalisation:  40.2027750603704
siam score:  -0.8009627
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  65 total reward:  0.07999999999999952  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  45 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actions average: 
K:  4  action  0 :  tensor([0.3504, 0.0495, 0.1349, 0.1155, 0.1122, 0.1177, 0.1198],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0216, 0.8617, 0.0264, 0.0235, 0.0069, 0.0147, 0.0452],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2051, 0.0152, 0.2598, 0.1283, 0.1269, 0.1385, 0.1263],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1121, 0.0023, 0.0768, 0.5737, 0.0868, 0.0830, 0.0653],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1657, 0.0033, 0.0512, 0.0719, 0.5703, 0.0963, 0.0412],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1931, 0.0058, 0.1182, 0.0896, 0.0984, 0.3982, 0.0967],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1691, 0.1435, 0.1278, 0.1134, 0.0919, 0.1398, 0.2146],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  40.7714985086659
actor:  1 policy actor:  1  step number:  53 total reward:  0.39999999999999936  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actions average: 
K:  2  action  0 :  tensor([0.6214, 0.0089, 0.0506, 0.0597, 0.1120, 0.0712, 0.0762],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0024,     0.9588,     0.0037,     0.0026,     0.0007,     0.0008,
            0.0310], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1827, 0.0761, 0.2566, 0.1095, 0.1093, 0.1110, 0.1548],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0890, 0.0048, 0.0456, 0.5217, 0.1561, 0.0977, 0.0851],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2261, 0.0063, 0.0751, 0.1152, 0.3702, 0.1037, 0.1034],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1386, 0.0185, 0.1193, 0.0760, 0.0803, 0.4741, 0.0932],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0452, 0.1702, 0.0456, 0.0469, 0.0348, 0.0341, 0.6231],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  35.518038272857666
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
siam score:  -0.8125937
actor:  1 policy actor:  1  step number:  63 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.266]
 [0.026]
 [0.026]
 [0.026]
 [0.127]
 [0.026]] [[37.715]
 [35.809]
 [37.715]
 [37.715]
 [37.715]
 [45.597]
 [37.715]] [[0.412]
 [0.609]
 [0.412]
 [0.412]
 [0.412]
 [0.688]
 [0.412]]
printing an ep nov before normalisation:  30.714537527083166
printing an ep nov before normalisation:  64.54164981842041
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actions average: 
K:  4  action  0 :  tensor([0.4302, 0.0464, 0.0934, 0.1106, 0.0988, 0.1157, 0.1050],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0055, 0.9321, 0.0041, 0.0155, 0.0048, 0.0087, 0.0293],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0574, 0.1023, 0.5630, 0.0508, 0.0382, 0.1436, 0.0446],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1455, 0.0012, 0.0837, 0.2758, 0.2365, 0.1448, 0.1125],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0957, 0.0890, 0.0629, 0.0966, 0.5231, 0.0637, 0.0690],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1611, 0.0164, 0.1515, 0.1494, 0.0837, 0.2852, 0.1527],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0870, 0.2914, 0.0623, 0.0743, 0.0401, 0.0679, 0.3771],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  68 total reward:  0.2466666666666658  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  48 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
printing an ep nov before normalisation:  29.50675345580899
printing an ep nov before normalisation:  35.004122257232666
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.581]
 [0.371]
 [0.371]] [[32.91 ]
 [32.91 ]
 [32.91 ]
 [32.91 ]
 [32.767]
 [32.91 ]
 [32.91 ]] [[0.873]
 [0.873]
 [0.873]
 [0.873]
 [1.079]
 [0.873]
 [0.873]]
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  31.078791618347168
Printing some Q and Qe and total Qs values:  [[-0.008]
 [ 0.063]
 [-0.01 ]
 [-0.007]
 [ 0.076]
 [-0.007]
 [ 0.076]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.008]
 [ 0.063]
 [-0.01 ]
 [-0.007]
 [ 0.076]
 [-0.007]
 [ 0.076]]
printing an ep nov before normalisation:  19.52137947249973
actor:  1 policy actor:  1  step number:  58 total reward:  0.326666666666666  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  46.41830558460926
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.146]
 [0.146]
 [0.146]
 [0.146]
 [0.146]
 [0.2  ]
 [0.146]] [[48.056]
 [48.056]
 [48.056]
 [48.056]
 [48.056]
 [46.925]
 [48.056]] [[1.426]
 [1.426]
 [1.426]
 [1.426]
 [1.426]
 [1.43 ]
 [1.426]]
Printing some Q and Qe and total Qs values:  [[0.285]
 [0.211]
 [0.228]
 [0.207]
 [0.264]
 [0.228]
 [0.213]] [[45.491]
 [43.211]
 [46.738]
 [38.013]
 [45.47 ]
 [46.738]
 [39.914]] [[0.714]
 [0.597]
 [0.681]
 [0.495]
 [0.693]
 [0.681]
 [0.536]]
Printing some Q and Qe and total Qs values:  [[0.222]
 [0.272]
 [0.22 ]
 [0.215]
 [0.234]
 [0.234]
 [0.213]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.222]
 [0.272]
 [0.22 ]
 [0.215]
 [0.234]
 [0.234]
 [0.213]]
printing an ep nov before normalisation:  60.15495578951331
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.03861999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  33.21773965012938
printing an ep nov before normalisation:  40.57052117081242
actor:  0 policy actor:  0  step number:  34 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.04167333333333319 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.04167333333333319 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  70 total reward:  0.059999999999999054  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.04167333333333319 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.04167333333333319 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.04167333333333319 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.04167333333333319 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.04167333333333319 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.04167333333333319 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
siam score:  -0.81923157
actor:  1 policy actor:  1  step number:  59 total reward:  0.17333333333333267  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.209]
 [0.264]
 [0.209]
 [0.209]
 [0.209]
 [0.209]
 [0.209]] [[42.38 ]
 [43.867]
 [42.38 ]
 [42.38 ]
 [42.38 ]
 [42.38 ]
 [42.38 ]] [[0.696]
 [0.788]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]]
maxi score, test score, baseline:  0.04167333333333318 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.04167333333333318 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.04167333333333318 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  0 policy actor:  1  step number:  57 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0442066666666665 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.0442066666666665 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  30.37619642523067
actor:  1 policy actor:  1  step number:  56 total reward:  0.5000000000000002  reward:  1.0 rdn_beta:  0.667
using another actor
maxi score, test score, baseline:  0.0442066666666665 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.0442066666666665 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  41.39743635098387
Printing some Q and Qe and total Qs values:  [[ 0.501]
 [ 0.354]
 [ 0.354]
 [-0.011]
 [ 0.449]
 [ 0.259]
 [ 0.354]] [[37.551]
 [32.839]
 [32.839]
 [36.595]
 [38.979]
 [39.149]
 [32.839]] [[1.813]
 [1.309]
 [1.309]
 [1.228]
 [1.868]
 [1.691]
 [1.309]]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0442066666666665 0.6930000000000001 0.6930000000000001
actor:  1 policy actor:  1  step number:  44 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  4  action  0 :  tensor([0.4240, 0.0041, 0.0748, 0.0970, 0.1905, 0.0927, 0.1170],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0059, 0.9442, 0.0029, 0.0130, 0.0021, 0.0022, 0.0297],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1144, 0.0541, 0.3662, 0.1559, 0.0899, 0.1031, 0.1165],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1356, 0.1102, 0.0849, 0.2505, 0.1665, 0.1441, 0.1082],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2033, 0.0014, 0.1226, 0.1676, 0.2046, 0.1785, 0.1221],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0965, 0.0574, 0.0808, 0.1095, 0.1275, 0.4381, 0.0903],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2220, 0.0864, 0.1328, 0.1425, 0.0901, 0.1388, 0.1874],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0442066666666665 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.084]
 [0.176]
 [0.103]
 [0.114]
 [0.108]
 [0.102]
 [0.106]] [[52.212]
 [49.215]
 [51.783]
 [50.45 ]
 [50.818]
 [52.294]
 [52.131]] [[1.205]
 [1.169]
 [1.205]
 [1.16 ]
 [1.17 ]
 [1.227]
 [1.223]]
printing an ep nov before normalisation:  33.03104183363819
line 256 mcts: sample exp_bonus 43.05239051386201
printing an ep nov before normalisation:  40.53533591261348
actor:  1 policy actor:  1  step number:  52 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  48 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0442066666666665 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  38.42881365002003
from probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.0442066666666665 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  24.53983061389959
maxi score, test score, baseline:  0.0442066666666665 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.0442066666666665 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.0442066666666665 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.0442066666666665 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  56 total reward:  0.19333333333333302  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0442066666666665 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  66.03785165652322
maxi score, test score, baseline:  0.041873333333333165 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  55.17929381350192
maxi score, test score, baseline:  0.041873333333333165 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  28.769948479145654
maxi score, test score, baseline:  0.041873333333333165 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  41.19550694375026
actor:  0 policy actor:  0  step number:  43 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.04467333333333316 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.04467333333333316 0.6930000000000001 0.6930000000000001
printing an ep nov before normalisation:  37.79299736022949
actor:  1 policy actor:  1  step number:  46 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.04467333333333316 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.04467333333333316 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  0 policy actor:  1  step number:  44 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  34.49586532239434
printing an ep nov before normalisation:  35.38525917631109
Printing some Q and Qe and total Qs values:  [[0.774]
 [0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.775]] [[6.724]
 [5.103]
 [5.941]
 [6.212]
 [6.087]
 [2.865]
 [6.9  ]] [[0.908]
 [0.876]
 [0.892]
 [0.898]
 [0.895]
 [0.831]
 [0.912]]
actor:  1 policy actor:  1  step number:  42 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  32.64704942703247
Printing some Q and Qe and total Qs values:  [[0.104]
 [0.21 ]
 [0.104]
 [0.104]
 [0.104]
 [0.104]
 [0.104]] [[43.736]
 [48.004]
 [43.736]
 [43.736]
 [43.736]
 [43.736]
 [43.736]] [[0.927]
 [1.202]
 [0.927]
 [0.927]
 [0.927]
 [0.927]
 [0.927]]
printing an ep nov before normalisation:  36.151833994929014
maxi score, test score, baseline:  0.047486666666666504 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.047486666666666504 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.607]
 [0.566]
 [0.563]
 [0.559]
 [0.561]
 [0.568]] [[37.795]
 [36.375]
 [33.863]
 [37.944]
 [38.538]
 [37.616]
 [37.625]] [[1.796]
 [1.804]
 [1.68 ]
 [1.811]
 [1.826]
 [1.798]
 [1.805]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.047486666666666504 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.047486666666666504 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.907]
 [0.888]
 [0.871]
 [0.856]
 [0.825]
 [0.564]
 [0.883]] [[41.685]
 [39.006]
 [38.906]
 [43.661]
 [39.276]
 [40.971]
 [38.433]] [[2.773]
 [2.573]
 [2.549]
 [2.856]
 [2.528]
 [2.381]
 [2.529]]
printing an ep nov before normalisation:  43.548028085297574
maxi score, test score, baseline:  0.047486666666666504 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  0.2133333333333327  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  40.02900366415673
maxi score, test score, baseline:  0.047486666666666504 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.047486666666666504 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  37.26147715026671
printing an ep nov before normalisation:  45.96092589649538
Printing some Q and Qe and total Qs values:  [[0.407]
 [0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]] [[32.572]
 [25.267]
 [25.267]
 [25.267]
 [25.267]
 [25.267]
 [25.267]] [[1.644]
 [1.358]
 [1.358]
 [1.358]
 [1.358]
 [1.358]
 [1.358]]
maxi score, test score, baseline:  0.047486666666666504 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.047486666666666504 0.6930000000000001 0.6930000000000001
actor:  1 policy actor:  1  step number:  67 total reward:  0.1466666666666656  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  37.427003750364364
actor:  0 policy actor:  1  step number:  51 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.05009999999999983 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.05009999999999983 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.05009999999999983 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.05009999999999983 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.05009999999999983 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.05009999999999983 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.05009999999999983 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
siam score:  -0.80410695
maxi score, test score, baseline:  0.05009999999999983 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  59 total reward:  0.2666666666666664  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.05009999999999983 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.05009999999999983 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.05009999999999983 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  34.915372555993976
actions average: 
K:  3  action  0 :  tensor([0.5925, 0.0020, 0.0532, 0.0397, 0.2310, 0.0481, 0.0334],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0298, 0.8800, 0.0123, 0.0180, 0.0114, 0.0101, 0.0383],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0610, 0.0097, 0.5047, 0.0292, 0.0384, 0.3027, 0.0543],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2151, 0.0342, 0.1116, 0.2110, 0.1496, 0.1167, 0.1617],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1781, 0.0054, 0.1508, 0.1280, 0.1706, 0.2254, 0.1416],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1253, 0.0189, 0.2876, 0.0798, 0.0832, 0.3016, 0.1037],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1580, 0.0571, 0.1097, 0.1064, 0.1108, 0.1052, 0.3528],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.05009999999999983 0.6930000000000001 0.6930000000000001
actor:  1 policy actor:  1  step number:  68 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 33.999355361563715
maxi score, test score, baseline:  0.05009999999999983 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  30.748724247258956
actor:  0 policy actor:  1  step number:  66 total reward:  0.2866666666666654  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  29.413242680669377
maxi score, test score, baseline:  0.050073333333333164 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using explorer policy with actor:  1
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([0.6771, 0.0622, 0.0406, 0.0405, 0.0870, 0.0483, 0.0443],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0070, 0.9647, 0.0044, 0.0059, 0.0024, 0.0025, 0.0131],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1785, 0.0820, 0.2661, 0.1011, 0.1156, 0.1357, 0.1211],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1057, 0.1350, 0.0505, 0.4796, 0.0761, 0.0735, 0.0796],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2961, 0.0018, 0.0820, 0.0638, 0.4011, 0.0827, 0.0725],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1209, 0.0104, 0.0797, 0.0698, 0.0570, 0.5943, 0.0679],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1049, 0.0786, 0.0879, 0.1900, 0.0924, 0.1002, 0.3461],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  55 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.050073333333333164 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using explorer policy with actor:  0
siam score:  -0.8083807
maxi score, test score, baseline:  0.050073333333333164 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.050073333333333164 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.050073333333333164 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  0 policy actor:  1  step number:  37 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.05313999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.05313999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.05313999999999984 0.6930000000000001 0.6930000000000001
siam score:  -0.81251246
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.67 ]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]] [[28.254]
 [31.767]
 [28.254]
 [28.254]
 [28.254]
 [28.254]
 [28.254]] [[0.94 ]
 [1.132]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [0.94 ]]
actor:  0 policy actor:  0  step number:  63 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.052379999999999836 0.6930000000000001 0.6930000000000001
actor:  1 policy actor:  1  step number:  34 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.052379999999999836 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.052379999999999836 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.052379999999999836 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.052379999999999836 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.771]
 [0.708]
 [0.709]
 [0.707]
 [0.696]
 [0.675]] [[33.022]
 [30.898]
 [32.468]
 [32.872]
 [33.026]
 [34.06 ]
 [34.083]] [[2.348]
 [2.259]
 [2.35 ]
 [2.39 ]
 [2.403]
 [2.493]
 [2.475]]
actor:  1 policy actor:  1  step number:  43 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.052379999999999836 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.052379999999999836 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  75 total reward:  0.239999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.052379999999999836 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.052379999999999836 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  38.18947356909795
maxi score, test score, baseline:  0.052379999999999836 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.05237999999999983 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  43 total reward:  0.3599999999999999  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  48.32150751135615
printing an ep nov before normalisation:  35.9131641888844
printing an ep nov before normalisation:  38.09691055761353
UNIT TEST: sample policy line 217 mcts : [0.163 0.265 0.204 0.041 0.245 0.041 0.041]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.052379999999999836 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  43 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  2.0
from probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.052379999999999836 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]] [[43.664]
 [43.664]
 [43.664]
 [43.664]
 [43.664]
 [43.664]
 [43.664]] [[0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]]
maxi score, test score, baseline:  0.052379999999999836 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.052379999999999836 0.6930000000000001 0.6930000000000001
printing an ep nov before normalisation:  43.623857795125815
printing an ep nov before normalisation:  25.685307972959357
printing an ep nov before normalisation:  40.23717403411865
Printing some Q and Qe and total Qs values:  [[0.991]
 [0.991]
 [0.991]
 [0.991]
 [0.991]
 [0.991]
 [0.991]] [[24.759]
 [24.759]
 [24.759]
 [24.759]
 [24.759]
 [24.759]
 [24.759]] [[0.991]
 [0.991]
 [0.991]
 [0.991]
 [0.991]
 [0.991]
 [0.991]]
maxi score, test score, baseline:  0.052379999999999836 0.6930000000000001 0.6930000000000001
actor:  0 policy actor:  0  step number:  56 total reward:  0.1799999999999996  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.05473999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.05473999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.05473999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  38.89793422021729
maxi score, test score, baseline:  0.05473999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3933333333333333  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 46.03184669880472
maxi score, test score, baseline:  0.05473999999999984 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.05473999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.05473999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.151]
 [0.194]
 [0.167]
 [0.159]
 [0.133]
 [0.21 ]
 [0.21 ]] [[46.296]
 [44.389]
 [45.266]
 [46.602]
 [49.137]
 [46.56 ]
 [46.56 ]] [[1.288]
 [1.237]
 [1.253]
 [1.312]
 [1.412]
 [1.361]
 [1.361]]
Printing some Q and Qe and total Qs values:  [[-0.029]
 [-0.053]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]] [[48.261]
 [46.793]
 [48.384]
 [48.384]
 [48.384]
 [48.384]
 [48.384]] [[1.843]
 [1.727]
 [1.868]
 [1.868]
 [1.868]
 [1.868]
 [1.868]]
printing an ep nov before normalisation:  37.62313760566722
maxi score, test score, baseline:  0.05473999999999984 0.6930000000000001 0.6930000000000001
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.497]
 [0.424]
 [0.423]
 [0.42 ]
 [0.419]
 [0.46 ]] [[24.169]
 [25.811]
 [23.727]
 [23.643]
 [23.631]
 [23.616]
 [24.816]] [[1.25 ]
 [1.445]
 [1.221]
 [1.213]
 [1.21 ]
 [1.208]
 [1.336]]
siam score:  -0.8077245
maxi score, test score, baseline:  0.05473999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  41.11936595582023
actor:  1 policy actor:  1  step number:  65 total reward:  0.2933333333333328  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.05473999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.095]
 [0.115]
 [0.03 ]
 [0.03 ]
 [0.03 ]
 [0.03 ]
 [0.03 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.095]
 [0.115]
 [0.03 ]
 [0.03 ]
 [0.03 ]
 [0.03 ]
 [0.03 ]]
actor:  1 policy actor:  1  step number:  61 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  30.247509341161017
printing an ep nov before normalisation:  42.79018351137499
maxi score, test score, baseline:  0.05473999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
siam score:  -0.8176996
printing an ep nov before normalisation:  28.537213802337646
maxi score, test score, baseline:  0.05473999999999984 0.6930000000000001 0.6930000000000001
actor:  0 policy actor:  1  step number:  54 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8156483
maxi score, test score, baseline:  0.05469999999999983 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  45.09427879829539
maxi score, test score, baseline:  0.05469999999999983 0.6930000000000001 0.6930000000000001
Printing some Q and Qe and total Qs values:  [[-0.028]
 [ 0.047]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.006]
 [-0.028]] [[36.864]
 [41.098]
 [36.864]
 [36.864]
 [36.864]
 [39.705]
 [36.864]] [[1.555]
 [2.03 ]
 [1.555]
 [1.555]
 [1.555]
 [1.844]
 [1.555]]
line 256 mcts: sample exp_bonus 41.4649147776684
maxi score, test score, baseline:  0.05469999999999983 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  49 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.05469999999999983 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  37.14557753668891
maxi score, test score, baseline:  0.05469999999999983 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actions average: 
K:  4  action  0 :  tensor([0.4759, 0.0033, 0.0973, 0.1055, 0.0921, 0.1235, 0.1024],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0153, 0.9151, 0.0059, 0.0209, 0.0083, 0.0076, 0.0269],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0725, 0.0011, 0.5825, 0.0610, 0.0736, 0.0923, 0.1170],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1004, 0.2655, 0.0609, 0.2783, 0.0843, 0.1084, 0.1022],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2644, 0.0066, 0.0846, 0.1350, 0.3036, 0.1074, 0.0985],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1942, 0.1666, 0.1062, 0.1417, 0.1264, 0.1528, 0.1122],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1853, 0.1681, 0.0945, 0.1948, 0.0898, 0.1291, 0.1384],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  35.75118308894164
maxi score, test score, baseline:  0.05469999999999983 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
siam score:  -0.79867446
actor:  0 policy actor:  0  step number:  50 total reward:  0.24666666666666626  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  61 total reward:  0.2133333333333327  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.05451333333333316 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.043]
 [0.02 ]
 [0.036]
 [0.031]
 [0.04 ]
 [0.038]
 [0.02 ]] [[39.162]
 [42.273]
 [37.691]
 [39.174]
 [39.365]
 [39.345]
 [39.029]] [[0.874]
 [0.981]
 [0.805]
 [0.862]
 [0.879]
 [0.876]
 [0.845]]
maxi score, test score, baseline:  0.05451333333333316 0.6930000000000001 0.6930000000000001
printing an ep nov before normalisation:  40.77255551226314
using explorer policy with actor:  1
maxi score, test score, baseline:  0.05451333333333316 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.05451333333333316 0.6930000000000001 0.6930000000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.05451333333333316 0.6930000000000001 0.6930000000000001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  0.05451333333333316 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.05451333333333316 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  48 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.05451333333333316 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  56 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.05451333333333316 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.593]
 [0.477]
 [0.485]
 [0.48 ]
 [0.477]
 [0.477]] [[45.885]
 [40.239]
 [41.316]
 [44.305]
 [43.756]
 [41.316]
 [41.316]] [[1.417]
 [1.329]
 [1.25 ]
 [1.363]
 [1.339]
 [1.25 ]
 [1.25 ]]
maxi score, test score, baseline:  0.05451333333333316 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  60 total reward:  0.25999999999999945  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.05451333333333316 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.05451333333333316 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  33.352148502518475
printing an ep nov before normalisation:  25.539622423039486
printing an ep nov before normalisation:  0.0
actor:  0 policy actor:  1  step number:  46 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.054259999999999836 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.054259999999999836 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.054259999999999836 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.261]
 [0.263]
 [0.218]
 [0.218]
 [0.284]
 [0.218]
 [0.218]] [[45.981]
 [44.519]
 [39.153]
 [39.153]
 [46.172]
 [39.153]
 [39.153]] [[1.146]
 [1.089]
 [0.828]
 [0.828]
 [1.177]
 [0.828]
 [0.828]]
maxi score, test score, baseline:  0.054259999999999836 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.054259999999999836 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  31.769004405756075
using explorer policy with actor:  1
maxi score, test score, baseline:  0.054259999999999836 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
siam score:  -0.79796034
maxi score, test score, baseline:  0.054259999999999836 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.492]
 [0.425]
 [0.425]
 [0.373]
 [0.425]
 [0.402]] [[29.342]
 [25.292]
 [28.168]
 [28.168]
 [28.575]
 [28.168]
 [28.772]] [[1.389]
 [1.408]
 [1.446]
 [1.446]
 [1.408]
 [1.446]
 [1.444]]
maxi score, test score, baseline:  0.054259999999999836 0.6930000000000001 0.6930000000000001
actor:  1 policy actor:  1  step number:  63 total reward:  0.2266666666666659  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.054259999999999836 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.054259999999999836 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.69 ]
 [0.153]
 [0.153]
 [0.153]
 [0.178]
 [0.153]
 [0.153]] [[34.628]
 [43.539]
 [43.539]
 [43.539]
 [31.17 ]
 [43.539]
 [43.539]] [[0.866]
 [0.407]
 [0.407]
 [0.407]
 [0.324]
 [0.407]
 [0.407]]
Printing some Q and Qe and total Qs values:  [[0.202]
 [0.351]
 [0.328]
 [0.408]
 [0.237]
 [0.26 ]
 [0.313]] [[39.024]
 [40.578]
 [42.095]
 [36.28 ]
 [36.389]
 [43.201]
 [38.248]] [[0.705]
 [0.901]
 [0.924]
 [0.828]
 [0.66 ]
 [0.889]
 [0.792]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.139]
 [0.17 ]
 [0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]] [[36.634]
 [43.224]
 [36.634]
 [36.634]
 [36.634]
 [36.634]
 [36.634]] [[1.03 ]
 [1.374]
 [1.03 ]
 [1.03 ]
 [1.03 ]
 [1.03 ]
 [1.03 ]]
printing an ep nov before normalisation:  36.620580969759864
maxi score, test score, baseline:  0.054259999999999836 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
siam score:  -0.79467314
maxi score, test score, baseline:  0.054259999999999836 0.6930000000000001 0.6930000000000001
using explorer policy with actor:  1
siam score:  -0.7964346
maxi score, test score, baseline:  0.054259999999999836 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  40.66915321397563
actor:  1 policy actor:  1  step number:  57 total reward:  0.23999999999999966  reward:  1.0 rdn_beta:  1.667
siam score:  -0.79584503
Printing some Q and Qe and total Qs values:  [[-0.012]
 [ 0.083]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.013]
 [-0.011]] [[56.082]
 [55.62 ]
 [52.519]
 [52.519]
 [52.519]
 [54.303]
 [54.026]] [[0.668]
 [0.751]
 [0.568]
 [0.568]
 [0.568]
 [0.62 ]
 [0.614]]
maxi score, test score, baseline:  0.054259999999999836 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.054259999999999836 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  54 total reward:  0.3799999999999992  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.054259999999999836 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  39.30561369688415
printing an ep nov before normalisation:  22.99478580014574
Printing some Q and Qe and total Qs values:  [[0.78 ]
 [0.896]
 [0.78 ]
 [0.78 ]
 [0.78 ]
 [0.78 ]
 [0.78 ]] [[27.274]
 [27.32 ]
 [27.274]
 [27.274]
 [27.274]
 [27.274]
 [27.274]] [[0.78 ]
 [0.896]
 [0.78 ]
 [0.78 ]
 [0.78 ]
 [0.78 ]
 [0.78 ]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  45.463068617217665
Printing some Q and Qe and total Qs values:  [[0.616]
 [0.681]
 [0.641]
 [0.61 ]
 [0.641]
 [0.641]
 [0.641]] [[26.679]
 [31.389]
 [26.478]
 [23.451]
 [26.478]
 [26.478]
 [26.478]] [[0.616]
 [0.681]
 [0.641]
 [0.61 ]
 [0.641]
 [0.641]
 [0.641]]
maxi score, test score, baseline:  0.054259999999999836 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.054259999999999836 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  0 policy actor:  1  step number:  58 total reward:  0.12666666666666615  reward:  1.0 rdn_beta:  1.667
from probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.0539266666666665 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.779]
 [0.901]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]] [[25.471]
 [28.506]
 [25.471]
 [25.471]
 [25.471]
 [25.471]
 [25.471]] [[0.779]
 [0.901]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]]
Printing some Q and Qe and total Qs values:  [[0.6  ]
 [0.65 ]
 [0.519]
 [0.553]
 [0.556]
 [0.527]
 [0.606]] [[28.808]
 [32.2  ]
 [23.171]
 [25.499]
 [25.563]
 [23.379]
 [28.306]] [[0.6  ]
 [0.65 ]
 [0.519]
 [0.553]
 [0.556]
 [0.527]
 [0.606]]
printing an ep nov before normalisation:  33.28708116176367
printing an ep nov before normalisation:  23.677888245465795
actor:  0 policy actor:  1  step number:  56 total reward:  0.24666666666666603  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  60.374762411921935
actor:  0 policy actor:  0  step number:  43 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  51 total reward:  0.22666666666666613  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.258]
 [0.408]
 [0.258]
 [0.258]
 [0.258]
 [0.254]
 [0.258]] [[49.883]
 [50.109]
 [49.883]
 [49.883]
 [49.883]
 [44.362]
 [49.883]] [[1.459]
 [1.617]
 [1.459]
 [1.459]
 [1.459]
 [1.254]
 [1.459]]
line 256 mcts: sample exp_bonus 40.61842571124495
printing an ep nov before normalisation:  24.5529842376709
printing an ep nov before normalisation:  29.72705776192153
actor:  1 policy actor:  1  step number:  40 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.05924666666666651 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  51 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  35 total reward:  0.4933333333333333  reward:  1.0 rdn_beta:  2.0
UNIT TEST: sample policy line 217 mcts : [0.204 0.184 0.122 0.102 0.143 0.102 0.143]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.05924666666666651 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  37.91727293897222
printing an ep nov before normalisation:  36.45447692193781
Printing some Q and Qe and total Qs values:  [[0.856]
 [0.856]
 [0.856]
 [0.856]
 [0.856]
 [0.856]
 [0.856]] [[45.085]
 [45.085]
 [45.085]
 [45.085]
 [45.085]
 [45.085]
 [45.085]] [[0.856]
 [0.856]
 [0.856]
 [0.856]
 [0.856]
 [0.856]
 [0.856]]
maxi score, test score, baseline:  0.05924666666666651 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actions average: 
K:  2  action  0 :  tensor([0.4617, 0.1037, 0.0666, 0.0708, 0.1164, 0.0577, 0.1231],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0220,     0.9444,     0.0039,     0.0052,     0.0093,     0.0009,
            0.0143], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1402, 0.1097, 0.2888, 0.0845, 0.0883, 0.1677, 0.1207],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1525, 0.1226, 0.0920, 0.3274, 0.0850, 0.1040, 0.1166],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1854, 0.0196, 0.0783, 0.1032, 0.4163, 0.0994, 0.0977],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1172, 0.0066, 0.1119, 0.0636, 0.0625, 0.5094, 0.1288],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1016, 0.1905, 0.0675, 0.0626, 0.0489, 0.0492, 0.4796],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  54 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  49 total reward:  0.2933333333333331  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.05935333333333316 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.05935333333333316 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.05935333333333316 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.05935333333333316 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  44.91751670837402
siam score:  -0.8099324
maxi score, test score, baseline:  0.05935333333333316 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  59.93261593046046
actions average: 
K:  3  action  0 :  tensor([0.5422, 0.0122, 0.0778, 0.0792, 0.1347, 0.0726, 0.0813],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0244, 0.9314, 0.0041, 0.0112, 0.0024, 0.0013, 0.0252],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1338, 0.0029, 0.3793, 0.1004, 0.0819, 0.2018, 0.0999],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1682, 0.0490, 0.1348, 0.1585, 0.1956, 0.1614, 0.1324],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.3000, 0.0136, 0.1541, 0.1246, 0.1485, 0.1203, 0.1391],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1484, 0.0047, 0.1186, 0.1110, 0.1664, 0.3818, 0.0691],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1229, 0.1728, 0.1072, 0.1031, 0.0884, 0.0770, 0.3286],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  35.93568741570336
Printing some Q and Qe and total Qs values:  [[0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.819]] [[29.171]
 [29.171]
 [29.171]
 [29.171]
 [29.171]
 [29.171]
 [29.171]] [[0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.819]]
printing an ep nov before normalisation:  30.697407408049557
siam score:  -0.80808014
printing an ep nov before normalisation:  28.629283993491132
actor:  1 policy actor:  1  step number:  46 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.05935333333333316 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  23.964752372098555
maxi score, test score, baseline:  0.05935333333333316 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  0 policy actor:  0  step number:  62 total reward:  0.2066666666666659  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.05957999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  37.058651470530606
maxi score, test score, baseline:  0.05957999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.1  ]
 [0.177]
 [0.1  ]
 [0.1  ]
 [0.057]
 [0.1  ]
 [0.1  ]] [[32.769]
 [40.19 ]
 [32.769]
 [32.769]
 [40.632]
 [32.769]
 [32.769]] [[0.66 ]
 [1.061]
 [0.66 ]
 [0.66 ]
 [0.96 ]
 [0.66 ]
 [0.66 ]]
maxi score, test score, baseline:  0.05957999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.05957999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.05957999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  49 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.05957999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.05957999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
actions average: 
K:  3  action  0 :  tensor([0.4948, 0.0416, 0.0650, 0.0662, 0.1239, 0.0754, 0.1331],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0059, 0.9410, 0.0041, 0.0277, 0.0023, 0.0025, 0.0165],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0544, 0.0542, 0.6000, 0.0531, 0.0487, 0.0907, 0.0989],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2242, 0.0881, 0.1045, 0.1830, 0.0992, 0.1240, 0.1768],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1805, 0.0052, 0.1317, 0.1677, 0.1741, 0.1755, 0.1654],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0671, 0.1004, 0.2225, 0.0966, 0.0927, 0.3236, 0.0971],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1284, 0.1718, 0.0734, 0.1664, 0.0923, 0.0936, 0.2740],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.05957999999999984 0.6930000000000001 0.6930000000000001
printing an ep nov before normalisation:  36.174081674831704
printing an ep nov before normalisation:  24.05774039367524
printing an ep nov before normalisation:  32.60373799723888
printing an ep nov before normalisation:  30.95268726348877
printing an ep nov before normalisation:  30.275751776665036
actor:  1 policy actor:  1  step number:  55 total reward:  0.3466666666666661  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.581]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]] [[47.885]
 [41.945]
 [47.885]
 [47.885]
 [47.885]
 [47.885]
 [47.885]] [[1.372]
 [1.3  ]
 [1.372]
 [1.372]
 [1.372]
 [1.372]
 [1.372]]
printing an ep nov before normalisation:  44.565659049221665
actor:  1 policy actor:  1  step number:  53 total reward:  0.3333333333333328  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  42.047589522613485
maxi score, test score, baseline:  0.05957999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.05957999999999984 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.05957999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  42 total reward:  0.5400000000000001  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  42 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  51.72301315607325
maxi score, test score, baseline:  0.05957999999999984 0.6930000000000001 0.6930000000000001
printing an ep nov before normalisation:  30.83838068828875
actions average: 
K:  0  action  0 :  tensor([0.4624, 0.0009, 0.0688, 0.1087, 0.1592, 0.0993, 0.1007],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0078, 0.9316, 0.0064, 0.0065, 0.0015, 0.0031, 0.0431],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1393, 0.0012, 0.4386, 0.1056, 0.1221, 0.1061, 0.0871],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1984, 0.0070, 0.0897, 0.4084, 0.0940, 0.1016, 0.1009],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2525, 0.0006, 0.0677, 0.0866, 0.4139, 0.0894, 0.0893],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1509, 0.0004, 0.1156, 0.1024, 0.1395, 0.4070, 0.0842],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2080, 0.0204, 0.1524, 0.1404, 0.0883, 0.1527, 0.2378],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.05957999999999984 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.056966666666666506 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.056966666666666506 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  52 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8023895
maxi score, test score, baseline:  0.056966666666666506 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
siam score:  -0.7997335
maxi score, test score, baseline:  0.056966666666666506 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.056966666666666506 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  53.42055877601895
Printing some Q and Qe and total Qs values:  [[0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]] [[42.582]
 [42.582]
 [42.582]
 [42.582]
 [42.582]
 [42.582]
 [42.582]] [[1.73]
 [1.73]
 [1.73]
 [1.73]
 [1.73]
 [1.73]
 [1.73]]
maxi score, test score, baseline:  0.056966666666666506 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.056966666666666506 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  62 total reward:  0.12666666666666615  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.056966666666666506 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3666666666666667  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.056966666666666506 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.142]
 [0.188]
 [0.136]
 [0.115]
 [0.145]
 [0.149]
 [0.149]] [[43.337]
 [39.437]
 [41.827]
 [39.302]
 [43.489]
 [42.904]
 [41.79 ]] [[0.638]
 [0.597]
 [0.599]
 [0.521]
 [0.644]
 [0.636]
 [0.611]]
maxi score, test score, baseline:  0.056966666666666506 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
siam score:  -0.7999861
maxi score, test score, baseline:  0.056966666666666506 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  27.944781774284593
maxi score, test score, baseline:  0.056966666666666506 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  44 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  46 total reward:  0.47333333333333305  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  34.26004107446756
Printing some Q and Qe and total Qs values:  [[0.067]
 [0.311]
 [0.062]
 [0.06 ]
 [0.091]
 [0.19 ]
 [0.19 ]] [[33.161]
 [37.091]
 [32.966]
 [32.973]
 [34.245]
 [37.202]
 [37.202]] [[0.221]
 [0.504]
 [0.214]
 [0.212]
 [0.255]
 [0.385]
 [0.385]]
printing an ep nov before normalisation:  35.52558806403889
siam score:  -0.7993532
printing an ep nov before normalisation:  39.26743653597148
maxi score, test score, baseline:  0.056966666666666506 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.056966666666666506 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.056966666666666506 0.6930000000000001 0.6930000000000001
actor:  1 policy actor:  1  step number:  53 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.056966666666666506 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  26.41143047182584
maxi score, test score, baseline:  0.056966666666666506 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  26.493700934090707
maxi score, test score, baseline:  0.056966666666666506 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  66 total reward:  0.12666666666666615  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.13046646351364
maxi score, test score, baseline:  0.056966666666666506 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  52.603380399979415
siam score:  -0.789021
using explorer policy with actor:  1
maxi score, test score, baseline:  0.056966666666666506 0.6930000000000001 0.6930000000000001
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  70 total reward:  0.05999999999999961  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.056966666666666506 0.6930000000000001 0.6930000000000001
siam score:  -0.7886876
Printing some Q and Qe and total Qs values:  [[0.117]
 [0.256]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]] [[40.951]
 [43.322]
 [40.951]
 [40.951]
 [40.951]
 [40.951]
 [40.951]] [[1.316]
 [1.591]
 [1.316]
 [1.316]
 [1.316]
 [1.316]
 [1.316]]
maxi score, test score, baseline:  0.056966666666666506 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.056966666666666506 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  32.602479305390005
maxi score, test score, baseline:  0.056966666666666506 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.056966666666666506 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.056966666666666506 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
line 256 mcts: sample exp_bonus 46.34436173342042
from probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.056966666666666506 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
from probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.056966666666666506 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.056966666666666506 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  43.36067207654205
maxi score, test score, baseline:  0.054233333333333175 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.054233333333333175 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.054233333333333175 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.054233333333333175 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  37.321259954542185
actor:  0 policy actor:  1  step number:  54 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  35.055642959280895
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.481]
 [0.468]
 [0.47 ]
 [0.469]
 [0.479]
 [0.46 ]] [[26.232]
 [26.384]
 [26.753]
 [25.924]
 [26.705]
 [26.807]
 [26.279]] [[2.123]
 [2.143]
 [2.176]
 [2.075]
 [2.171]
 [2.194]
 [2.109]]
printing an ep nov before normalisation:  27.158695474625556
maxi score, test score, baseline:  0.05421999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.78 ]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.78 ]] [[23.754]
 [25.242]
 [25.242]
 [25.242]
 [25.242]
 [25.242]
 [13.113]] [[2.013]
 [2.049]
 [2.049]
 [2.049]
 [2.049]
 [2.049]
 [1.461]]
printing an ep nov before normalisation:  39.11921095583971
actor:  1 policy actor:  1  step number:  59 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  40.88131196448081
maxi score, test score, baseline:  0.05421999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  26.693360020330395
actor:  1 policy actor:  1  step number:  65 total reward:  0.37333333333333285  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.05421999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  0 policy actor:  0  step number:  39 total reward:  0.5600000000000002  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  52.33385559851662
maxi score, test score, baseline:  0.05733999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.446]
 [0.559]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]] [[34.725]
 [35.352]
 [35.355]
 [35.355]
 [35.355]
 [35.355]
 [35.355]] [[1.253]
 [1.395]
 [1.285]
 [1.285]
 [1.285]
 [1.285]
 [1.285]]
maxi score, test score, baseline:  0.05733999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
siam score:  -0.7953824
maxi score, test score, baseline:  0.05733999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  43 total reward:  0.6533333333333335  reward:  1.0 rdn_beta:  1.0
from probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.05733999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  43.769582016367174
maxi score, test score, baseline:  0.05733999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.05733999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using explorer policy with actor:  0
printing an ep nov before normalisation:  31.15731232897045
actor:  0 policy actor:  1  step number:  40 total reward:  0.5266666666666667  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  1  step number:  53 total reward:  0.25333333333333286  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  46.112067679480354
printing an ep nov before normalisation:  47.15307198987029
maxi score, test score, baseline:  0.06289999999999984 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.06289999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.717]
 [0.718]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]] [[34.036]
 [28.055]
 [34.036]
 [34.036]
 [34.036]
 [34.036]
 [34.036]] [[1.285]
 [1.111]
 [1.285]
 [1.285]
 [1.285]
 [1.285]
 [1.285]]
printing an ep nov before normalisation:  44.34819128879741
maxi score, test score, baseline:  0.06289999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  26.73241257705468
maxi score, test score, baseline:  0.06289999999999984 0.6930000000000001 0.6930000000000001
printing an ep nov before normalisation:  0.0
actor:  1 policy actor:  1  step number:  50 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.06289999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.06289999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actions average: 
K:  2  action  0 :  tensor([0.5818, 0.0587, 0.0628, 0.0577, 0.0897, 0.0659, 0.0834],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0090, 0.9507, 0.0054, 0.0066, 0.0033, 0.0049, 0.0202],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1275, 0.0261, 0.5122, 0.0622, 0.0524, 0.1213, 0.0983],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1216, 0.0379, 0.1405, 0.3911, 0.0935, 0.1339, 0.0816],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1559, 0.0055, 0.0548, 0.0936, 0.5487, 0.0808, 0.0607],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1300, 0.0020, 0.1029, 0.0809, 0.1008, 0.4792, 0.1041],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2002, 0.1228, 0.1304, 0.1187, 0.1240, 0.1233, 0.1806],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.06289999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.06289999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  31.821494102478027
printing an ep nov before normalisation:  32.72696495056152
printing an ep nov before normalisation:  23.6959804596353
maxi score, test score, baseline:  0.06289999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.06289999999999984 0.6930000000000001 0.6930000000000001
using explorer policy with actor:  1
printing an ep nov before normalisation:  25.016289295463938
maxi score, test score, baseline:  0.06289999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  61 total reward:  0.09333333333333249  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.06289999999999984 0.6930000000000001 0.6930000000000001
printing an ep nov before normalisation:  53.57661753250354
maxi score, test score, baseline:  0.06289999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.06289999999999984 0.6930000000000001 0.6930000000000001
printing an ep nov before normalisation:  22.210769761330056
actor:  1 policy actor:  1  step number:  38 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.06289999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.611]
 [0.597]
 [0.631]
 [0.637]
 [0.539]
 [0.538]] [[25.515]
 [30.909]
 [28.7  ]
 [28.869]
 [28.894]
 [25.582]
 [25.724]] [[0.926]
 [1.202]
 [1.102]
 [1.143]
 [1.15 ]
 [0.924]
 [0.93 ]]
Printing some Q and Qe and total Qs values:  [[0.142]
 [0.162]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]] [[51.367]
 [56.569]
 [51.367]
 [51.367]
 [51.367]
 [51.367]
 [51.367]] [[1.173]
 [1.397]
 [1.173]
 [1.173]
 [1.173]
 [1.173]
 [1.173]]
actor:  1 policy actor:  1  step number:  39 total reward:  0.4933333333333333  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  60.50476321465255
printing an ep nov before normalisation:  50.06175970537696
maxi score, test score, baseline:  0.06289999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.06289999999999984 0.6930000000000001 0.6930000000000001
actor:  1 policy actor:  1  step number:  59 total reward:  0.13333333333333286  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  64.89891765248097
using explorer policy with actor:  1
maxi score, test score, baseline:  0.06289999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.836]
 [0.945]
 [0.836]
 [0.836]
 [0.836]
 [0.836]
 [0.816]] [[25.976]
 [30.379]
 [25.976]
 [25.976]
 [25.976]
 [25.976]
 [28.171]] [[0.836]
 [0.945]
 [0.836]
 [0.836]
 [0.836]
 [0.836]
 [0.816]]
printing an ep nov before normalisation:  32.664523124694824
maxi score, test score, baseline:  0.06289999999999984 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  34.97317883977829
printing an ep nov before normalisation:  21.82554194123414
actor:  0 policy actor:  0  step number:  52 total reward:  0.32666666666666655  reward:  1.0 rdn_beta:  1.333
from probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.06555333333333316 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  69 total reward:  0.09333333333333227  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.108]
 [0.219]
 [0.12 ]
 [0.197]
 [0.112]
 [0.112]
 [0.114]] [[29.816]
 [37.238]
 [33.883]
 [32.646]
 [29.443]
 [29.5  ]
 [29.749]] [[0.442]
 [0.714]
 [0.542]
 [0.592]
 [0.438]
 [0.439]
 [0.447]]
printing an ep nov before normalisation:  43.98241726062211
maxi score, test score, baseline:  0.06555333333333316 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.06555333333333316 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.06555333333333316 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  0 policy actor:  0  step number:  49 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.06813999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.06813999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
line 256 mcts: sample exp_bonus 44.837912371561075
actor:  1 policy actor:  1  step number:  68 total reward:  0.09999999999999876  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.06813999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  34.87732240442915
maxi score, test score, baseline:  0.06813999999999985 0.6930000000000001 0.6930000000000001
printing an ep nov before normalisation:  74.67549749872359
actor:  1 policy actor:  1  step number:  58 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.06813999999999985 0.6930000000000001 0.6930000000000001
printing an ep nov before normalisation:  39.15970858663682
maxi score, test score, baseline:  0.06813999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  43.32088833132042
printing an ep nov before normalisation:  41.5367931123018
printing an ep nov before normalisation:  40.3164382529369
printing an ep nov before normalisation:  25.676684172043693
maxi score, test score, baseline:  0.06813999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  46 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.06813999999999985 0.6930000000000001 0.6930000000000001
printing an ep nov before normalisation:  64.14559641994147
printing an ep nov before normalisation:  37.018446922302246
printing an ep nov before normalisation:  33.67397469143835
maxi score, test score, baseline:  0.06813999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  50.13026576201367
maxi score, test score, baseline:  0.06813999999999985 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.06569999999999983 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  0 policy actor:  1  step number:  43 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0658466666666665 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using another actor
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.42964583039269
Printing some Q and Qe and total Qs values:  [[1.5  ]
 [1.5  ]
 [0.018]
 [0.012]
 [0.013]
 [1.5  ]
 [0.013]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.5  ]
 [1.5  ]
 [0.018]
 [0.012]
 [0.013]
 [1.5  ]
 [0.013]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  0.0658466666666665 0.6930000000000001 0.6930000000000001
printing an ep nov before normalisation:  44.26541328430176
maxi score, test score, baseline:  0.0658466666666665 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.0658466666666665 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  34.46480612696756
siam score:  -0.7866031
siam score:  -0.7849672
actor:  1 policy actor:  1  step number:  39 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0658466666666665 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.0658466666666665 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  57.70061667287066
printing an ep nov before normalisation:  43.41874769115191
maxi score, test score, baseline:  0.0658466666666665 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  59 total reward:  0.13333333333333264  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0658466666666665 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.0658466666666665 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  0 policy actor:  0  step number:  40 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.06889999999999985 0.6930000000000001 0.6930000000000001
maxi score, test score, baseline:  0.06889999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.06889999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.06889999999999985 0.6930000000000001 0.6930000000000001
actor:  1 policy actor:  1  step number:  59 total reward:  0.10666666666666613  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  34.743898255484446
maxi score, test score, baseline:  0.06889999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  54 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.06889999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  38.790664812743714
maxi score, test score, baseline:  0.06889999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.06889999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  36 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.06889999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.06889999999999985 0.6930000000000001 0.6930000000000001
Starting evaluation
Printing some Q and Qe and total Qs values:  [[-0.062]
 [ 0.369]
 [ 0.181]
 [ 0.267]
 [ 0.177]
 [ 0.186]
 [ 0.317]] [[49.806]
 [43.081]
 [47.278]
 [45.384]
 [45.053]
 [41.226]
 [44.368]] [[1.102]
 [1.25 ]
 [1.239]
 [1.245]
 [1.14 ]
 [0.988]
 [1.252]]
Printing some Q and Qe and total Qs values:  [[0.279]
 [0.347]
 [0.311]
 [0.266]
 [0.272]
 [0.263]
 [0.284]] [[41.042]
 [40.97 ]
 [41.551]
 [45.423]
 [45.977]
 [47.59 ]
 [39.632]] [[0.279]
 [0.347]
 [0.311]
 [0.266]
 [0.272]
 [0.263]
 [0.284]]
Printing some Q and Qe and total Qs values:  [[0.585]
 [0.613]
 [0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.585]] [[47.034]
 [55.471]
 [47.034]
 [47.034]
 [47.034]
 [47.034]
 [47.034]] [[0.585]
 [0.613]
 [0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.585]]
maxi score, test score, baseline:  0.06889999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  59.07371965952909
printing an ep nov before normalisation:  40.32031496111862
Printing some Q and Qe and total Qs values:  [[0.703]
 [0.903]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.726]] [[51.022]
 [49.441]
 [51.022]
 [51.022]
 [51.022]
 [51.022]
 [52.413]] [[0.703]
 [0.903]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.726]]
maxi score, test score, baseline:  0.06889999999999985 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  45.57961940502508
printing an ep nov before normalisation:  40.72463840036513
printing an ep nov before normalisation:  30.34125290751782
printing an ep nov before normalisation:  32.83253867781505
printing an ep nov before normalisation:  29.732100953509544
printing an ep nov before normalisation:  34.87228186237919
actor:  0 policy actor:  1  step number:  51 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  36.852408172285806
maxi score, test score, baseline:  0.06904666666666649 0.6930000000000001 0.6930000000000001
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[1.012]
 [1.012]
 [1.012]
 [1.012]
 [1.012]
 [1.012]
 [1.012]] [[31.941]
 [31.941]
 [31.941]
 [31.941]
 [31.941]
 [31.941]
 [31.941]] [[1.012]
 [1.012]
 [1.012]
 [1.012]
 [1.012]
 [1.012]
 [1.012]]
printing an ep nov before normalisation:  33.35749626159668
actor:  0 policy actor:  0  step number:  25 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[0.263]
 [0.261]
 [0.261]
 [0.262]
 [0.263]
 [1.004]
 [0.262]] [[ 0.246]
 [ 0.224]
 [ 0.23 ]
 [ 0.237]
 [ 0.24 ]
 [31.802]
 [ 0.227]] [[0.263]
 [0.261]
 [0.261]
 [0.262]
 [0.263]
 [1.004]
 [0.262]]
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[ 0.085]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]] [[59.413]
 [56.366]
 [56.366]
 [56.366]
 [56.366]
 [56.366]
 [56.366]] [[0.718]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]]
line 256 mcts: sample exp_bonus 34.18477882200162
maxi score, test score, baseline:  0.08363333333333318 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.08363333333333318 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.08363333333333318 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  52 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  41.658965093391046
maxi score, test score, baseline:  0.08363333333333318 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.08363333333333318 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.08363333333333318 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
line 256 mcts: sample exp_bonus 30.492614703346764
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.579]
 [0.41 ]
 [0.498]
 [0.498]
 [0.422]
 [0.534]] [[33.24 ]
 [32.804]
 [31.356]
 [33.24 ]
 [33.24 ]
 [31.828]
 [32.656]] [[1.444]
 [1.498]
 [1.242]
 [1.444]
 [1.444]
 [1.282]
 [1.444]]
maxi score, test score, baseline:  0.08363333333333318 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  36.67802437409978
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  58 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.08363333333333318 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  37 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  1.333
from probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  33.9480996131897
using explorer policy with actor:  0
printing an ep nov before normalisation:  39.41097796269028
actor:  0 policy actor:  0  step number:  47 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  61 total reward:  0.15999999999999903  reward:  1.0 rdn_beta:  1.333
siam score:  -0.79135114
maxi score, test score, baseline:  0.08213999999999984 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.08213999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.08213999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
siam score:  -0.7904475
actor:  0 policy actor:  0  step number:  47 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  42.3388030155792
maxi score, test score, baseline:  0.0818866666666665 0.6930000000000002 0.6930000000000002
actor:  0 policy actor:  0  step number:  37 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  39.875030517578125
maxi score, test score, baseline:  0.08183333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  29.216386724952002
printing an ep nov before normalisation:  38.220432457442364
printing an ep nov before normalisation:  40.88522520782368
Printing some Q and Qe and total Qs values:  [[0.766]
 [0.799]
 [0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]] [[27.524]
 [30.077]
 [27.524]
 [27.524]
 [27.524]
 [27.524]
 [27.524]] [[0.766]
 [0.799]
 [0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]]
printing an ep nov before normalisation:  41.01511936702412
printing an ep nov before normalisation:  28.25636763494805
maxi score, test score, baseline:  0.08183333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  24.184746742248535
actor:  0 policy actor:  0  step number:  53 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0816866666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  51 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  28.854211749938624
maxi score, test score, baseline:  0.0816866666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  33.869892517238426
maxi score, test score, baseline:  0.0816866666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  27.5109481573536
maxi score, test score, baseline:  0.0816866666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  0.020411031323135376
actor:  1 policy actor:  1  step number:  48 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0816866666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.0816866666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.689]
 [0.71 ]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]] [[28.348]
 [28.682]
 [28.348]
 [28.348]
 [28.348]
 [28.348]
 [28.348]] [[1.889]
 [1.937]
 [1.889]
 [1.889]
 [1.889]
 [1.889]
 [1.889]]
maxi score, test score, baseline:  0.0816866666666665 0.6930000000000002 0.6930000000000002
actor:  1 policy actor:  1  step number:  63 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  50.45042289944889
printing an ep nov before normalisation:  31.95041877183407
printing an ep nov before normalisation:  25.40383815765381
maxi score, test score, baseline:  0.0816866666666665 0.6930000000000002 0.6930000000000002
Printing some Q and Qe and total Qs values:  [[0.622]
 [0.65 ]
 [0.622]
 [0.622]
 [0.622]
 [0.622]
 [0.622]] [[27.036]
 [29.404]
 [27.036]
 [27.036]
 [27.036]
 [27.036]
 [27.036]] [[1.292]
 [1.434]
 [1.292]
 [1.292]
 [1.292]
 [1.292]
 [1.292]]
maxi score, test score, baseline:  0.0816866666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  38 total reward:  0.5266666666666667  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0816866666666665 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.0816866666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  52 total reward:  0.21999999999999975  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0816866666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0816866666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.0816866666666665 0.6930000000000002 0.6930000000000002
printing an ep nov before normalisation:  27.17848473670456
maxi score, test score, baseline:  0.0816866666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
siam score:  -0.78845704
maxi score, test score, baseline:  0.0816866666666665 0.6930000000000002 0.6930000000000002
actor:  1 policy actor:  1  step number:  46 total reward:  0.4466666666666663  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  44 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0816866666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.14 ]
 [0.377]
 [0.14 ]
 [0.14 ]
 [0.14 ]
 [0.14 ]
 [0.14 ]] [[29.4  ]
 [38.608]
 [29.4  ]
 [29.4  ]
 [29.4  ]
 [29.4  ]
 [29.4  ]] [[0.458]
 [0.872]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]]
printing an ep nov before normalisation:  46.445522788022714
maxi score, test score, baseline:  0.0816866666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.0816866666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  53 total reward:  0.3599999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0816866666666665 0.6930000000000002 0.6930000000000002
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0816866666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  57 total reward:  0.4  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]]
maxi score, test score, baseline:  0.0816866666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
from probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.0816866666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.0816866666666665 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.0816866666666665 0.6930000000000002 0.6930000000000002
printing an ep nov before normalisation:  27.448137839086776
Printing some Q and Qe and total Qs values:  [[0.778]
 [0.449]
 [0.666]
 [0.644]
 [0.772]
 [0.711]
 [0.662]] [[32.2  ]
 [32.03 ]
 [31.554]
 [30.813]
 [29.502]
 [29.426]
 [31.672]] [[0.778]
 [0.449]
 [0.666]
 [0.644]
 [0.772]
 [0.711]
 [0.662]]
actor:  0 policy actor:  0  step number:  41 total reward:  0.5200000000000002  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  57 total reward:  0.37333333333333285  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.08181999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]] [[57.23]
 [57.23]
 [57.23]
 [57.23]
 [57.23]
 [57.23]
 [57.23]] [[0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]]
maxi score, test score, baseline:  0.08181999999999984 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.08181999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
siam score:  -0.79278517
maxi score, test score, baseline:  0.08181999999999984 0.6930000000000002 0.6930000000000002
printing an ep nov before normalisation:  35.495420834028906
printing an ep nov before normalisation:  24.398121833723998
maxi score, test score, baseline:  0.08181999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  32.860440103364084
actor:  1 policy actor:  1  step number:  45 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.08181999999999984 0.6930000000000002 0.6930000000000002
line 256 mcts: sample exp_bonus 26.873176065942523
maxi score, test score, baseline:  0.08181999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  61.71207623236162
actor:  1 policy actor:  1  step number:  73 total reward:  0.17333333333333278  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.08181999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.08181999999999984 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.08181999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.08181999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actions average: 
K:  0  action  0 :  tensor([0.6243, 0.0035, 0.0440, 0.0634, 0.1392, 0.0633, 0.0624],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0072, 0.9201, 0.0060, 0.0193, 0.0047, 0.0087, 0.0341],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0385, 0.0015, 0.7539, 0.0302, 0.0324, 0.0527, 0.0908],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1116, 0.0272, 0.0783, 0.3718, 0.0991, 0.1287, 0.1833],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2138, 0.0048, 0.0584, 0.0919, 0.4769, 0.0694, 0.0850],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1300, 0.0032, 0.1399, 0.0937, 0.0824, 0.4550, 0.0958],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2272, 0.0117, 0.1342, 0.1716, 0.1421, 0.1452, 0.1681],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  57 total reward:  0.23999999999999932  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.182]
 [0.275]
 [0.156]
 [0.204]
 [0.136]
 [0.156]
 [0.122]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.182]
 [0.275]
 [0.156]
 [0.204]
 [0.136]
 [0.156]
 [0.122]]
maxi score, test score, baseline:  0.08181999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  30.28855316595172
maxi score, test score, baseline:  0.08181999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  27.202021761608464
actor:  1 policy actor:  1  step number:  56 total reward:  0.31333333333333313  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  51.488855469558885
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.086]
 [0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[56.943]
 [55.48 ]
 [54.41 ]
 [54.41 ]
 [54.41 ]
 [54.41 ]
 [54.41 ]] [[1.036]
 [1.069]
 [0.948]
 [0.948]
 [0.948]
 [0.948]
 [0.948]]
printing an ep nov before normalisation:  48.913254737854004
maxi score, test score, baseline:  0.08181999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  28.521089211922007
actor:  1 policy actor:  1  step number:  48 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.08181999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  30.923890997517645
actor:  1 policy actor:  1  step number:  52 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.08181999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  47.59249753120433
Printing some Q and Qe and total Qs values:  [[0.822]
 [0.834]
 [0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.822]] [[50.066]
 [43.703]
 [50.066]
 [50.066]
 [50.066]
 [50.066]
 [50.066]] [[0.822]
 [0.834]
 [0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.822]]
maxi score, test score, baseline:  0.08181999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.08181999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.08181999999999984 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.08181999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  28.078441619873047
maxi score, test score, baseline:  0.08181999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  50 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  53.67223059571296
maxi score, test score, baseline:  0.08181999999999984 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.08181999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.08181999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using another actor
from probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.07909999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  30.828223510011053
printing an ep nov before normalisation:  34.76123857151455
maxi score, test score, baseline:  0.07909999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actions average: 
K:  0  action  0 :  tensor([0.6531, 0.0046, 0.0406, 0.0642, 0.0827, 0.0783, 0.0766],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0069,     0.9412,     0.0047,     0.0049,     0.0007,     0.0025,
            0.0391], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1750, 0.0832, 0.2462, 0.1064, 0.1008, 0.1484, 0.1401],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0473, 0.0521, 0.0262, 0.6601, 0.0863, 0.0667, 0.0612],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1921, 0.0027, 0.0357, 0.0638, 0.5710, 0.0721, 0.0626],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1207, 0.0047, 0.0784, 0.0681, 0.0520, 0.5870, 0.0890],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1532, 0.1421, 0.1359, 0.1155, 0.1062, 0.1464, 0.2008],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]] [[55.28 ]
 [49.286]
 [49.286]
 [49.286]
 [49.286]
 [49.286]
 [49.286]] [[0.413]
 [0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]]
maxi score, test score, baseline:  0.07909999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  0 policy actor:  0  step number:  36 total reward:  0.5  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  47 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  49.07596005526197
Printing some Q and Qe and total Qs values:  [[0.139]
 [0.222]
 [0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]] [[34.677]
 [36.965]
 [34.677]
 [34.677]
 [34.677]
 [34.677]
 [34.677]] [[1.814]
 [2.114]
 [1.814]
 [1.814]
 [1.814]
 [1.814]
 [1.814]]
maxi score, test score, baseline:  0.0824866666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.0824866666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.0824866666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
siam score:  -0.8040703
maxi score, test score, baseline:  0.0824866666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.0824866666666665 0.6930000000000002 0.6930000000000002
actor:  1 policy actor:  1  step number:  57 total reward:  0.34666666666666657  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0824866666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.0824866666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  3.287465517587407e-06
actor:  1 policy actor:  1  step number:  63 total reward:  0.0666666666666661  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0824866666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  0 policy actor:  0  step number:  38 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.08545999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  48.733588900506135
maxi score, test score, baseline:  0.08545999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.08545999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  66 total reward:  0.16666666666666652  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.08545999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.148]
 [0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.137]
 [0.112]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.148]
 [0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.137]
 [0.112]]
printing an ep nov before normalisation:  67.07354391489834
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.70956229984614
actor:  1 policy actor:  1  step number:  41 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  26.36033535003662
maxi score, test score, baseline:  0.08545999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  30.89764533940892
maxi score, test score, baseline:  0.08545999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.08545999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.335]
 [0.575]
 [0.335]
 [0.335]
 [0.335]
 [0.335]
 [0.335]] [[36.21 ]
 [34.616]
 [36.21 ]
 [36.21 ]
 [36.21 ]
 [36.21 ]
 [36.21 ]] [[1.297]
 [1.454]
 [1.297]
 [1.297]
 [1.297]
 [1.297]
 [1.297]]
actor:  1 policy actor:  1  step number:  40 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.08545999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.08545999999999983 0.6930000000000002 0.6930000000000002
actor:  1 policy actor:  1  step number:  54 total reward:  0.3133333333333328  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  35.47338947440901
maxi score, test score, baseline:  0.08545999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actions average: 
K:  3  action  0 :  tensor([0.5024, 0.0127, 0.0844, 0.0811, 0.1044, 0.0929, 0.1221],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0071, 0.9532, 0.0052, 0.0065, 0.0014, 0.0016, 0.0250],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1511, 0.0998, 0.4015, 0.0555, 0.0896, 0.1166, 0.0859],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1794, 0.1292, 0.0880, 0.2494, 0.1240, 0.0993, 0.1307],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2514, 0.0052, 0.1143, 0.1023, 0.2637, 0.1200, 0.1431],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1085, 0.0084, 0.1485, 0.0698, 0.1014, 0.4920, 0.0714],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2422, 0.1037, 0.1117, 0.1038, 0.0990, 0.1247, 0.2150],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[-0.03]
 [-0.03]
 [-0.03]
 [-0.03]
 [-0.03]
 [-0.03]
 [-0.03]] [[67.46]
 [67.46]
 [67.46]
 [67.46]
 [67.46]
 [67.46]
 [67.46]] [[1.637]
 [1.637]
 [1.637]
 [1.637]
 [1.637]
 [1.637]
 [1.637]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.08545999999999983 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.08545999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.08545999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.08545999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  42.669149922353235
maxi score, test score, baseline:  0.08545999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  55 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.08545999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.08545999999999983 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.08545999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  39.98455308574856
maxi score, test score, baseline:  0.08545999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[-0.105]
 [-0.106]
 [-0.106]
 [-0.106]
 [-0.106]
 [-0.106]
 [-0.106]] [[30.938]
 [25.362]
 [25.362]
 [25.362]
 [25.362]
 [25.362]
 [25.362]] [[0.913]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]]
line 256 mcts: sample exp_bonus 21.834928395032332
maxi score, test score, baseline:  0.08545999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.08545999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.19 ]
 [0.167]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]] [[ 0.   ]
 [40.963]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.343]
 [ 1.006]
 [-0.343]
 [-0.343]
 [-0.343]
 [-0.343]
 [-0.343]]
maxi score, test score, baseline:  0.08545999999999983 0.6930000000000002 0.6930000000000002
actor:  0 policy actor:  0  step number:  49 total reward:  0.2799999999999998  reward:  1.0 rdn_beta:  0.667
from probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  32.152366638183594
maxi score, test score, baseline:  0.08801999999999982 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.08801999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  30.84261276902975
printing an ep nov before normalisation:  38.64400625228882
maxi score, test score, baseline:  0.08801999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  0 policy actor:  1  step number:  45 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.333
using another actor
maxi score, test score, baseline:  0.09073999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.09073999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
siam score:  -0.7908922
printing an ep nov before normalisation:  33.39604912779689
Printing some Q and Qe and total Qs values:  [[0.782]
 [0.854]
 [0.761]
 [0.761]
 [0.761]
 [0.794]
 [0.809]] [[38.545]
 [29.838]
 [34.504]
 [34.504]
 [34.504]
 [36.713]
 [29.827]] [[0.782]
 [0.854]
 [0.761]
 [0.761]
 [0.761]
 [0.794]
 [0.809]]
printing an ep nov before normalisation:  41.64498720653964
printing an ep nov before normalisation:  39.66035200530331
Printing some Q and Qe and total Qs values:  [[0.443]
 [0.513]
 [0.435]
 [0.438]
 [0.437]
 [0.438]
 [0.444]] [[30.055]
 [32.487]
 [30.792]
 [31.54 ]
 [32.516]
 [33.231]
 [31.916]] [[0.899]
 [1.042]
 [0.914]
 [0.939]
 [0.966]
 [0.988]
 [0.956]]
maxi score, test score, baseline:  0.09073999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  0 policy actor:  1  step number:  57 total reward:  0.17333333333333278  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  51.875096318449614
maxi score, test score, baseline:  0.09308666666666648 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  24.986729621887207
maxi score, test score, baseline:  0.09308666666666648 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  0 policy actor:  0  step number:  38 total reward:  0.5133333333333333  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.09611333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  0 policy actor:  0  step number:  40 total reward:  0.4199999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.09652666666666648 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.09652666666666648 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.09652666666666648 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.797359196909756
printing an ep nov before normalisation:  33.58197346164347
maxi score, test score, baseline:  0.09652666666666648 0.6930000000000002 0.6930000000000002
actor:  1 policy actor:  1  step number:  34 total reward:  0.5533333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.09652666666666648 0.6930000000000002 0.6930000000000002
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
actions average: 
K:  3  action  0 :  tensor([0.4339, 0.0513, 0.0708, 0.1157, 0.1466, 0.0833, 0.0984],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0058, 0.9577, 0.0046, 0.0060, 0.0017, 0.0021, 0.0222],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1038, 0.1407, 0.4029, 0.0708, 0.0807, 0.1194, 0.0818],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1903, 0.0078, 0.0777, 0.4078, 0.1089, 0.1038, 0.1037],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1959, 0.0222, 0.0470, 0.0800, 0.5499, 0.0353, 0.0698],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1814, 0.0122, 0.1691, 0.1348, 0.1069, 0.2548, 0.1409],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2071, 0.2377, 0.0505, 0.0574, 0.0403, 0.0403, 0.3667],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.09652666666666648 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  54 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.09652666666666648 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.909]
 [0.979]
 [0.844]
 [0.885]
 [0.895]
 [0.851]
 [0.93 ]] [[24.698]
 [29.892]
 [27.366]
 [26.011]
 [28.526]
 [31.055]
 [25.521]] [[0.909]
 [0.979]
 [0.844]
 [0.885]
 [0.895]
 [0.851]
 [0.93 ]]
maxi score, test score, baseline:  0.09652666666666648 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  6.42241545278921e-05
actor:  0 policy actor:  0  step number:  54 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.09909999999999981 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.142]
 [0.215]
 [0.02 ]
 [0.142]
 [0.142]
 [0.079]
 [0.142]] [[ 0.   ]
 [40.379]
 [46.093]
 [ 0.   ]
 [ 0.   ]
 [38.063]
 [ 0.   ]] [[-0.456]
 [ 0.55 ]
 [ 0.487]
 [-0.456]
 [-0.456]
 [ 0.36 ]
 [-0.456]]
printing an ep nov before normalisation:  44.004020412059184
maxi score, test score, baseline:  0.09909999999999981 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  51 total reward:  0.23999999999999977  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.09909999999999981 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.09909999999999981 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  43 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.048]
 [0.136]
 [0.048]
 [0.061]
 [0.059]
 [0.048]
 [0.048]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.048]
 [0.136]
 [0.048]
 [0.061]
 [0.059]
 [0.048]
 [0.048]]
Printing some Q and Qe and total Qs values:  [[0.682]
 [0.756]
 [0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.682]] [[33.457]
 [30.033]
 [33.457]
 [33.457]
 [33.457]
 [33.457]
 [33.457]] [[1.617]
 [1.507]
 [1.617]
 [1.617]
 [1.617]
 [1.617]
 [1.617]]
maxi score, test score, baseline:  0.09909999999999981 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  36.27577781677246
maxi score, test score, baseline:  0.09909999999999981 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.09909999999999981 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  0 policy actor:  1  step number:  50 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  64 total reward:  0.2466666666666657  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.10193999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.10193999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.10193999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  34.580697196336594
maxi score, test score, baseline:  0.10193999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  36.211908222777296
printing an ep nov before normalisation:  29.250245094299316
actor:  1 policy actor:  1  step number:  52 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  1.333
using another actor
actor:  0 policy actor:  0  step number:  60 total reward:  0.1933333333333327  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  45.408875727898554
maxi score, test score, baseline:  0.1043266666666665 0.6930000000000002 0.6930000000000002
printing an ep nov before normalisation:  35.640127658843994
maxi score, test score, baseline:  0.1043266666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  39.05297247568326
printing an ep nov before normalisation:  26.510376015534764
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  48 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  44.550578934805735
maxi score, test score, baseline:  0.10711333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  59 total reward:  0.2133333333333327  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  45.60915876860806
maxi score, test score, baseline:  0.10711333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.10711333333333316 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.10711333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.361]
 [0.45 ]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]] [[58.555]
 [53.537]
 [59.503]
 [59.503]
 [59.503]
 [59.503]
 [59.503]] [[1.317]
 [1.26 ]
 [1.347]
 [1.347]
 [1.347]
 [1.347]
 [1.347]]
maxi score, test score, baseline:  0.10711333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.10711333333333316 0.6930000000000002 0.6930000000000002
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.48103695989001
maxi score, test score, baseline:  0.10711333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  33.835765501331316
maxi score, test score, baseline:  0.10711333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  39.56917034955031
actions average: 
K:  0  action  0 :  tensor([0.7765, 0.0014, 0.0271, 0.0370, 0.0820, 0.0301, 0.0459],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0067, 0.9462, 0.0036, 0.0082, 0.0022, 0.0029, 0.0302],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0541, 0.0066, 0.6724, 0.0579, 0.0458, 0.0770, 0.0863],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1279, 0.0043, 0.0649, 0.4872, 0.1296, 0.1001, 0.0861],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1694, 0.0021, 0.0539, 0.0777, 0.5506, 0.0596, 0.0867],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0747, 0.0020, 0.1508, 0.0743, 0.0950, 0.5159, 0.0873],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1561, 0.1602, 0.1219, 0.1246, 0.1016, 0.0879, 0.2477],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  39.285172122185934
maxi score, test score, baseline:  0.10711333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  32.09752200500848
actor:  1 policy actor:  1  step number:  66 total reward:  0.1933333333333328  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  59 total reward:  0.2666666666666664  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.10711333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.10711333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.10711333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10711333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  47.712974548339844
printing an ep nov before normalisation:  3.540213526775915e-05
actor:  1 policy actor:  1  step number:  45 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.10711333333333316 0.6930000000000002 0.6930000000000002
printing an ep nov before normalisation:  24.453804576906542
maxi score, test score, baseline:  0.10711333333333316 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.10711333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.34 ]
 [0.793]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.708]] [[43.688]
 [36.383]
 [33.916]
 [33.916]
 [33.916]
 [33.916]
 [37.461]] [[0.34 ]
 [0.793]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.708]]
printing an ep nov before normalisation:  41.26061657396375
Printing some Q and Qe and total Qs values:  [[0.679]
 [0.679]
 [0.679]
 [0.679]
 [0.679]
 [0.679]
 [0.679]] [[34.359]
 [34.359]
 [34.359]
 [34.359]
 [34.359]
 [34.359]
 [34.359]] [[0.926]
 [0.926]
 [0.926]
 [0.926]
 [0.926]
 [0.926]
 [0.926]]
maxi score, test score, baseline:  0.10711333333333316 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.10711333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  61 total reward:  0.2266666666666659  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.10711333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.10711333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  0 policy actor:  0  step number:  52 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  49.24495769406232
maxi score, test score, baseline:  0.10981999999999983 0.6930000000000002 0.6930000000000002
printing an ep nov before normalisation:  42.9411522906285
printing an ep nov before normalisation:  39.29833525072872
maxi score, test score, baseline:  0.10981999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.10981999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.10981999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.10981999999999983 0.6930000000000002 0.6930000000000002
actor:  1 policy actor:  1  step number:  52 total reward:  0.2733333333333332  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.10981999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
from probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.10981999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.895]
 [0.931]
 [0.895]
 [0.895]
 [0.895]
 [0.895]
 [0.895]] [[35.094]
 [41.523]
 [35.094]
 [35.094]
 [35.094]
 [35.094]
 [35.094]] [[0.895]
 [0.931]
 [0.895]
 [0.895]
 [0.895]
 [0.895]
 [0.895]]
maxi score, test score, baseline:  0.10981999999999983 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.10981999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
siam score:  -0.7974684
maxi score, test score, baseline:  0.10981999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
line 256 mcts: sample exp_bonus 40.085303789166645
maxi score, test score, baseline:  0.10981999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  34.578350482396154
maxi score, test score, baseline:  0.10981999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  38 total reward:  0.5  reward:  1.0 rdn_beta:  1.333
from probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11281999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.11281999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.731]
 [0.625]
 [0.596]
 [0.675]
 [0.621]
 [0.643]] [[23.997]
 [28.983]
 [21.832]
 [26.509]
 [29.517]
 [21.7  ]
 [25.043]] [[0.608]
 [0.731]
 [0.625]
 [0.596]
 [0.675]
 [0.621]
 [0.643]]
Printing some Q and Qe and total Qs values:  [[-0.013]
 [-0.021]
 [-0.013]
 [-0.013]
 [-0.022]
 [-0.013]
 [-0.013]] [[ 0.   ]
 [35.485]
 [ 0.   ]
 [ 0.   ]
 [34.254]
 [ 0.   ]
 [ 0.   ]] [[-0.805]
 [ 0.771]
 [-0.805]
 [-0.805]
 [ 0.715]
 [-0.805]
 [-0.805]]
maxi score, test score, baseline:  0.11013999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  46.279693414638594
actor:  1 policy actor:  1  step number:  46 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.45441436767578
maxi score, test score, baseline:  0.11013999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]] [[32.995]
 [32.995]
 [32.995]
 [32.995]
 [32.995]
 [32.995]
 [32.995]] [[1.398]
 [1.398]
 [1.398]
 [1.398]
 [1.398]
 [1.398]
 [1.398]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.566]
 [0.578]
 [0.503]
 [0.503]
 [0.584]
 [0.524]] [[30.069]
 [33.427]
 [31.793]
 [29.001]
 [29.283]
 [32.306]
 [30.341]] [[1.259]
 [1.484]
 [1.421]
 [1.216]
 [1.229]
 [1.45 ]
 [1.3  ]]
maxi score, test score, baseline:  0.11013999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11013999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.73204992003226
maxi score, test score, baseline:  0.11013999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11013999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11013999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.373]
 [0.358]
 [0.373]
 [0.373]
 [0.373]
 [0.373]] [[31.512]
 [31.512]
 [31.518]
 [31.512]
 [31.512]
 [31.512]
 [31.512]] [[1.483]
 [1.483]
 [1.468]
 [1.483]
 [1.483]
 [1.483]
 [1.483]]
maxi score, test score, baseline:  0.11013999999999982 0.6930000000000002 0.6930000000000002
printing an ep nov before normalisation:  0.0011627088525756335
actor:  1 policy actor:  1  step number:  52 total reward:  0.4866666666666669  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11013999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  0.11013999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11013999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actions average: 
K:  3  action  0 :  tensor([0.5555, 0.0064, 0.0675, 0.0686, 0.1299, 0.0852, 0.0870],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0053, 0.9527, 0.0038, 0.0074, 0.0021, 0.0026, 0.0260],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0892, 0.1121, 0.4529, 0.0587, 0.0533, 0.1256, 0.1082],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2186, 0.0260, 0.1199, 0.1462, 0.1429, 0.1886, 0.1578],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1698, 0.0042, 0.1203, 0.1028, 0.3804, 0.1071, 0.1154],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([    0.0008,     0.0008,     0.0156,     0.1155,     0.0133,     0.8517,
            0.0023], grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1528, 0.1418, 0.1154, 0.1033, 0.1042, 0.1215, 0.2611],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.11013999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  33.4454830905328
maxi score, test score, baseline:  0.11013999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11013999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  51 total reward:  0.31999999999999995  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.11013999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  12.6108078716871
maxi score, test score, baseline:  0.11013999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11013999999999982 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.11013999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11013999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.44 ]
 [0.389]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]] [[35.25 ]
 [35.217]
 [35.25 ]
 [35.25 ]
 [35.25 ]
 [35.25 ]
 [35.25 ]] [[1.384]
 [1.331]
 [1.384]
 [1.384]
 [1.384]
 [1.384]
 [1.384]]
Printing some Q and Qe and total Qs values:  [[-0.076]
 [-0.09 ]
 [-0.071]
 [-0.077]
 [-0.069]
 [-0.076]
 [-0.075]] [[13.029]
 [13.768]
 [13.321]
 [13.005]
 [13.553]
 [12.909]
 [13.163]] [[1.06 ]
 [1.111]
 [1.091]
 [1.057]
 [1.113]
 [1.05 ]
 [1.074]]
maxi score, test score, baseline:  0.11013999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11013999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11013999999999982 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.11013999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  38.44119455795041
maxi score, test score, baseline:  0.11013999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11013999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11013999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  36 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11013999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  24.131553561154714
siam score:  -0.7939328
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11013999999999982 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.11013999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11013999999999982 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  0 policy actor:  0  step number:  37 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  43.46744748842849
maxi score, test score, baseline:  0.11309999999999981 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11309999999999981 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
line 256 mcts: sample exp_bonus 48.53688664517707
Printing some Q and Qe and total Qs values:  [[-0.016]
 [-0.021]
 [-0.017]
 [-0.019]
 [-0.017]
 [-0.017]
 [-0.017]] [[55.338]
 [51.658]
 [52.123]
 [51.804]
 [52.123]
 [52.123]
 [52.123]] [[0.635]
 [0.559]
 [0.572]
 [0.563]
 [0.572]
 [0.572]
 [0.572]]
maxi score, test score, baseline:  0.11309999999999981 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11309999999999981 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11309999999999981 0.6930000000000002 0.6930000000000002
printing an ep nov before normalisation:  38.7606780041392
printing an ep nov before normalisation:  26.773405075073242
printing an ep nov before normalisation:  29.77743903948781
Printing some Q and Qe and total Qs values:  [[0.627]
 [0.736]
 [0.627]
 [0.627]
 [0.627]
 [0.627]
 [0.627]] [[24.737]
 [28.656]
 [24.737]
 [24.737]
 [24.737]
 [24.737]
 [24.737]] [[1.016]
 [1.249]
 [1.016]
 [1.016]
 [1.016]
 [1.016]
 [1.016]]
printing an ep nov before normalisation:  33.47084291417557
actor:  1 policy actor:  1  step number:  43 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.11309999999999981 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.581]
 [0.581]
 [0.   ]
 [0.   ]
 [0.   ]] [[ 0.   ]
 [ 0.   ]
 [23.222]
 [23.222]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.   ]
 [0.   ]
 [1.377]
 [1.377]
 [0.   ]
 [0.   ]
 [0.   ]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.2733333333333331  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.11309999999999981 0.6930000000000002 0.6930000000000002
printing an ep nov before normalisation:  37.888354423952386
maxi score, test score, baseline:  0.11309999999999981 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
UNIT TEST: sample policy line 217 mcts : [0.102 0.306 0.082 0.061 0.286 0.061 0.102]
actor:  0 policy actor:  0  step number:  53 total reward:  0.25333333333333263  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11316666666666647 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  65 total reward:  0.02666666666666595  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11316666666666647 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11316666666666647 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  31.261717646445955
maxi score, test score, baseline:  0.11316666666666647 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  41.579774495043466
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.540709720967506
printing an ep nov before normalisation:  36.12609618601371
printing an ep nov before normalisation:  30.149159398965484
maxi score, test score, baseline:  0.11316666666666647 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11316666666666647 0.6930000000000002 0.6930000000000002
actor:  1 policy actor:  1  step number:  73 total reward:  0.05333333333333257  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11316666666666647 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11316666666666647 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
siam score:  -0.7877214
maxi score, test score, baseline:  0.11316666666666647 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  27.061320030213228
actor:  1 policy actor:  1  step number:  59 total reward:  0.25333333333333263  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.11081999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11081999999999984 0.6930000000000002 0.6930000000000002
line 256 mcts: sample exp_bonus 0.02068984317929683
maxi score, test score, baseline:  0.11081999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  67 total reward:  0.15999999999999925  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11081999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11081999999999984 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.11081999999999984 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.11081999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11081999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11081999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11081999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11081999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  38.02630392032359
maxi score, test score, baseline:  0.11081999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11081999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  41 total reward:  0.48  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11081999999999984 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.11081999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11081999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  67 total reward:  0.18666666666666565  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11081999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  33.169271569639356
actor:  1 policy actor:  1  step number:  52 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  31.590973374276853
Printing some Q and Qe and total Qs values:  [[ 0.065]
 [ 0.109]
 [-0.   ]
 [ 0.015]
 [ 0.022]
 [ 0.041]
 [ 0.07 ]] [[30.224]
 [32.971]
 [24.568]
 [24.538]
 [24.565]
 [28.058]
 [30.702]] [[0.259]
 [0.333]
 [0.134]
 [0.148]
 [0.155]
 [0.212]
 [0.27 ]]
siam score:  -0.78660005
maxi score, test score, baseline:  0.11081999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  61 total reward:  0.2133333333333325  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  53.36630344390869
line 256 mcts: sample exp_bonus 35.60793399810791
maxi score, test score, baseline:  0.11081999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  31.942794667573406
maxi score, test score, baseline:  0.11081999999999984 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.11081999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  28.486773146243348
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.264]
 [0.443]
 [0.264]
 [0.264]
 [0.237]
 [0.264]
 [0.264]] [[36.48 ]
 [40.112]
 [36.48 ]
 [36.48 ]
 [37.379]
 [36.48 ]
 [36.48 ]] [[0.79 ]
 [1.063]
 [0.79 ]
 [0.79 ]
 [0.786]
 [0.79 ]
 [0.79 ]]
printing an ep nov before normalisation:  0.0003576792417447905
actor:  1 policy actor:  1  step number:  77 total reward:  0.23999999999999932  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  27.991065033356477
printing an ep nov before normalisation:  32.99445794722822
actions average: 
K:  2  action  0 :  tensor([0.5386, 0.0363, 0.0563, 0.0630, 0.1462, 0.0741, 0.0855],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0167, 0.9008, 0.0091, 0.0128, 0.0054, 0.0017, 0.0538],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1320, 0.0021, 0.4012, 0.0740, 0.0882, 0.1682, 0.1342],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1913, 0.0690, 0.0852, 0.3299, 0.1191, 0.0985, 0.1070],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1727, 0.0371, 0.0923, 0.0965, 0.3511, 0.1304, 0.1199],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1492, 0.0062, 0.1564, 0.0725, 0.0852, 0.4499, 0.0806],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1495, 0.1338, 0.1041, 0.1159, 0.1160, 0.1280, 0.2528],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.11081999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  57 total reward:  0.27999999999999936  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.11081999999999984 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.11081999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  0 policy actor:  1  step number:  50 total reward:  0.25999999999999945  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11333999999999983 0.6930000000000002 0.6930000000000002
using explorer policy with actor:  1
siam score:  -0.78669906
actions average: 
K:  0  action  0 :  tensor([0.6953, 0.0098, 0.0445, 0.0617, 0.0613, 0.0555, 0.0720],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0032,     0.9675,     0.0037,     0.0026,     0.0004,     0.0009,
            0.0217], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0525, 0.0139, 0.5567, 0.0643, 0.0432, 0.1546, 0.1148],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1685, 0.0449, 0.1017, 0.2327, 0.1211, 0.1425, 0.1885],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2010, 0.0016, 0.0759, 0.1141, 0.3678, 0.1228, 0.1168],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1237, 0.0024, 0.0984, 0.1026, 0.1113, 0.4434, 0.1181],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0988, 0.0727, 0.0813, 0.0915, 0.0753, 0.0841, 0.4963],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  31.363588309601635
actor:  1 policy actor:  1  step number:  59 total reward:  0.1999999999999994  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  39 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.11333999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11333999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  36.032729148864746
maxi score, test score, baseline:  0.11333999999999983 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.11333999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  32.432034297789556
maxi score, test score, baseline:  0.11333999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  0 policy actor:  0  step number:  58 total reward:  0.1933333333333328  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.491372661764565
printing an ep nov before normalisation:  42.085853875213964
printing an ep nov before normalisation:  27.482466767877074
maxi score, test score, baseline:  0.1131266666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  59 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]] [[36.865]
 [36.865]
 [36.865]
 [36.865]
 [36.865]
 [36.865]
 [36.865]] [[1.179]
 [1.179]
 [1.179]
 [1.179]
 [1.179]
 [1.179]
 [1.179]]
maxi score, test score, baseline:  0.1131266666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  29.503065189626046
printing an ep nov before normalisation:  25.48626484309125
printing an ep nov before normalisation:  0.003897418886822379
actor:  0 policy actor:  1  step number:  43 total reward:  0.42666666666666664  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11597999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  54 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.11597999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actions average: 
K:  0  action  0 :  tensor([0.6279, 0.1181, 0.0377, 0.0425, 0.0671, 0.0491, 0.0576],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0038, 0.9624, 0.0025, 0.0063, 0.0011, 0.0012, 0.0226],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0713, 0.0030, 0.5818, 0.0629, 0.0722, 0.1478, 0.0610],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2140, 0.0720, 0.0685, 0.3194, 0.1115, 0.0908, 0.1238],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1982, 0.0171, 0.0805, 0.0711, 0.4663, 0.0963, 0.0704],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1413, 0.0028, 0.1533, 0.0887, 0.0786, 0.4541, 0.0812],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2489, 0.0898, 0.1132, 0.1188, 0.1188, 0.1277, 0.1828],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  47.27315551243012
printing an ep nov before normalisation:  33.12347995071069
maxi score, test score, baseline:  0.11597999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11597999999999983 0.6930000000000002 0.6930000000000002
printing an ep nov before normalisation:  27.378134305209304
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.429]
 [0.326]
 [0.326]
 [0.255]
 [0.326]
 [0.326]] [[31.906]
 [35.569]
 [31.906]
 [31.906]
 [34.817]
 [31.906]
 [31.906]] [[0.979]
 [1.245]
 [0.979]
 [0.979]
 [1.038]
 [0.979]
 [0.979]]
printing an ep nov before normalisation:  44.58968290772744
printing an ep nov before normalisation:  36.37995282980024
maxi score, test score, baseline:  0.11597999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.769]
 [0.769]
 [0.769]
 [0.769]
 [0.769]
 [0.769]
 [0.769]] [[41.597]
 [38.957]
 [38.957]
 [38.957]
 [38.957]
 [38.957]
 [38.957]] [[2.043]
 [1.895]
 [1.895]
 [1.895]
 [1.895]
 [1.895]
 [1.895]]
actor:  1 policy actor:  1  step number:  39 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  28.757774829864502
actor:  1 policy actor:  1  step number:  54 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11597999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]] [[36.038]
 [36.038]
 [36.038]
 [36.038]
 [36.038]
 [36.038]
 [36.038]] [[1.348]
 [1.348]
 [1.348]
 [1.348]
 [1.348]
 [1.348]
 [1.348]]
maxi score, test score, baseline:  0.11597999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  33.066912151435446
maxi score, test score, baseline:  0.11597999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  24.78937304342068
actor:  0 policy actor:  0  step number:  63 total reward:  0.2133333333333327  reward:  1.0 rdn_beta:  1.0
from probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11608666666666648 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  26.609866864342848
printing an ep nov before normalisation:  27.163876119491757
maxi score, test score, baseline:  0.11608666666666648 0.6930000000000002 0.6930000000000002
printing an ep nov before normalisation:  20.077972412109375
printing an ep nov before normalisation:  24.40895536705576
Printing some Q and Qe and total Qs values:  [[0.186]
 [0.259]
 [0.208]
 [0.198]
 [0.208]
 [0.195]
 [0.189]] [[19.768]
 [23.416]
 [18.304]
 [18.892]
 [18.304]
 [18.779]
 [19.373]] [[0.186]
 [0.259]
 [0.208]
 [0.198]
 [0.208]
 [0.195]
 [0.189]]
maxi score, test score, baseline:  0.11339333333333317 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.11339333333333317 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using explorer policy with actor:  1
printing an ep nov before normalisation:  25.154449593979322
maxi score, test score, baseline:  0.11339333333333317 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  49.2358624658465
printing an ep nov before normalisation:  31.57436783300586
maxi score, test score, baseline:  0.11339333333333317 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  34.527222339324354
actor:  0 policy actor:  1  step number:  59 total reward:  0.23999999999999944  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.11587333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
siam score:  -0.7910879
maxi score, test score, baseline:  0.11587333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11587333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.83275125759361
actions average: 
K:  3  action  0 :  tensor([0.5545, 0.0163, 0.0727, 0.0779, 0.1157, 0.0797, 0.0833],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0054, 0.9362, 0.0044, 0.0079, 0.0021, 0.0030, 0.0410],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.2140, 0.0009, 0.1608, 0.1654, 0.1569, 0.1493, 0.1528],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1183, 0.0115, 0.1413, 0.4537, 0.0806, 0.1005, 0.0940],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1663, 0.0091, 0.0411, 0.0766, 0.6032, 0.0387, 0.0650],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.2783, 0.0010, 0.1356, 0.1344, 0.1449, 0.1578, 0.1480],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1171, 0.4265, 0.0600, 0.0742, 0.0575, 0.0518, 0.2128],
       grad_fn=<DivBackward0>)
actions average: 
K:  1  action  0 :  tensor([0.4021, 0.0069, 0.1200, 0.1124, 0.1184, 0.1236, 0.1165],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0083, 0.9433, 0.0029, 0.0092, 0.0039, 0.0032, 0.0291],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1263, 0.0306, 0.4640, 0.0742, 0.0662, 0.0829, 0.1560],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1122, 0.0037, 0.0720, 0.4777, 0.1338, 0.1081, 0.0926],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1984, 0.0301, 0.0983, 0.0994, 0.3896, 0.0669, 0.1172],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1509, 0.0084, 0.1455, 0.1115, 0.1082, 0.4092, 0.0664],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1942, 0.0736, 0.0815, 0.0857, 0.1246, 0.0883, 0.3522],
       grad_fn=<DivBackward0>)
actions average: 
K:  3  action  0 :  tensor([0.4404, 0.0532, 0.0727, 0.0913, 0.1482, 0.0954, 0.0989],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0247, 0.8707, 0.0201, 0.0251, 0.0196, 0.0113, 0.0284],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0596, 0.0148, 0.6301, 0.0457, 0.0080, 0.0448, 0.1971],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1702, 0.0201, 0.1318, 0.3190, 0.1139, 0.1192, 0.1257],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2522, 0.0078, 0.0815, 0.1287, 0.3245, 0.1288, 0.0764],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1036, 0.0022, 0.0690, 0.0470, 0.0416, 0.7068, 0.0299],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1744, 0.0665, 0.0903, 0.1451, 0.1079, 0.1178, 0.2980],
       grad_fn=<DivBackward0>)
siam score:  -0.7892006
maxi score, test score, baseline:  0.11587333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
UNIT TEST: sample policy line 217 mcts : [0.02  0.592 0.122 0.184 0.02  0.02  0.041]
actor:  1 policy actor:  1  step number:  49 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  49.39977171837236
actor:  1 policy actor:  1  step number:  65 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  0.667
using another actor
maxi score, test score, baseline:  0.11587333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.624]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]] [[44.991]
 [45.109]
 [44.991]
 [44.991]
 [44.991]
 [44.991]
 [44.991]] [[1.808]
 [1.89 ]
 [1.808]
 [1.808]
 [1.808]
 [1.808]
 [1.808]]
printing an ep nov before normalisation:  32.846004113249954
printing an ep nov before normalisation:  22.879547242527106
Printing some Q and Qe and total Qs values:  [[0.746]
 [0.793]
 [0.746]
 [0.727]
 [0.72 ]
 [0.719]
 [0.746]] [[30.177]
 [26.677]
 [30.177]
 [28.832]
 [29.364]
 [29.628]
 [30.177]] [[0.746]
 [0.793]
 [0.746]
 [0.727]
 [0.72 ]
 [0.719]
 [0.746]]
printing an ep nov before normalisation:  24.445245737642608
maxi score, test score, baseline:  0.11587333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  54 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  39.77843660134576
actor:  0 policy actor:  1  step number:  54 total reward:  0.2999999999999994  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11583333333333315 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
from probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11583333333333315 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.11583333333333315 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.702]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]] [[18.403]
 [30.493]
 [30.493]
 [30.493]
 [30.493]
 [30.493]
 [30.493]] [[1.535]
 [2.069]
 [2.069]
 [2.069]
 [2.069]
 [2.069]
 [2.069]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.11583333333333315 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11583333333333315 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11583333333333315 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11583333333333315 0.6930000000000002 0.6930000000000002
actor:  0 policy actor:  0  step number:  51 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.11847333333333317 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11847333333333317 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  41.77597691739595
printing an ep nov before normalisation:  52.75049431436893
printing an ep nov before normalisation:  36.42481746869894
printing an ep nov before normalisation:  29.828396693153454
actions average: 
K:  3  action  0 :  tensor([    0.8134,     0.0007,     0.0288,     0.0285,     0.0449,     0.0317,
            0.0520], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0320, 0.8252, 0.0505, 0.0220, 0.0201, 0.0204, 0.0298],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1053, 0.0107, 0.5178, 0.0834, 0.0762, 0.1070, 0.0997],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2329, 0.0006, 0.1274, 0.2233, 0.1402, 0.1401, 0.1355],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1356, 0.0010, 0.0768, 0.0768, 0.5462, 0.0659, 0.0977],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1766, 0.0010, 0.2005, 0.1138, 0.1143, 0.2637, 0.1301],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1436, 0.2176, 0.1127, 0.0776, 0.0541, 0.1545, 0.2399],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  36.09312385968776
line 256 mcts: sample exp_bonus 42.409897201356586
maxi score, test score, baseline:  0.11847333333333317 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.1159266666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]] [[23.514]
 [23.514]
 [23.514]
 [23.514]
 [23.514]
 [23.514]
 [23.514]] [[0.54]
 [0.54]
 [0.54]
 [0.54]
 [0.54]
 [0.54]
 [0.54]]
actions average: 
K:  2  action  0 :  tensor([0.5424, 0.0040, 0.0734, 0.1048, 0.1180, 0.0765, 0.0809],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0157, 0.9429, 0.0055, 0.0090, 0.0046, 0.0058, 0.0165],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1129, 0.0070, 0.4864, 0.1331, 0.0777, 0.0855, 0.0974],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1560, 0.0114, 0.1060, 0.4232, 0.1252, 0.1045, 0.0738],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.3292, 0.0251, 0.1021, 0.1012, 0.2226, 0.0997, 0.1201],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0992, 0.0438, 0.0903, 0.1221, 0.0982, 0.4857, 0.0607],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0922, 0.2393, 0.0810, 0.1074, 0.0701, 0.0917, 0.3182],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  37.8275341324207
maxi score, test score, baseline:  0.1159266666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actions average: 
K:  4  action  0 :  tensor([0.4465, 0.0095, 0.0891, 0.0973, 0.1301, 0.1265, 0.1010],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0064,     0.9435,     0.0157,     0.0051,     0.0006,     0.0004,
            0.0284], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1263, 0.0651, 0.5207, 0.0670, 0.0631, 0.0662, 0.0915],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0987, 0.1161, 0.0936, 0.3306, 0.0939, 0.1104, 0.1567],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1656, 0.1743, 0.0868, 0.1039, 0.2657, 0.1159, 0.0878],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0725, 0.0571, 0.0764, 0.0542, 0.0301, 0.6358, 0.0739],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1422, 0.0254, 0.1429, 0.1480, 0.1266, 0.1466, 0.2682],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1159266666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  24.918017387390137
printing an ep nov before normalisation:  31.65999794559501
actions average: 
K:  4  action  0 :  tensor([0.4443, 0.0022, 0.0846, 0.0989, 0.1206, 0.1377, 0.1117],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0076, 0.9201, 0.0064, 0.0112, 0.0049, 0.0055, 0.0444],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1801, 0.0050, 0.3001, 0.1144, 0.0895, 0.1414, 0.1694],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1951, 0.0912, 0.0902, 0.1489, 0.2021, 0.1539, 0.1186],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1477, 0.0991, 0.1034, 0.1002, 0.2821, 0.1221, 0.1454],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1163, 0.0012, 0.2229, 0.0455, 0.0327, 0.5592, 0.0222],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2366, 0.0061, 0.0981, 0.1054, 0.1134, 0.1489, 0.2916],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  69 total reward:  0.06666666666666599  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1159266666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1159266666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1159266666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.1159266666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
siam score:  -0.7936008
maxi score, test score, baseline:  0.1159266666666665 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.1159266666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.1159266666666665 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.1159266666666665 0.6930000000000002 0.6930000000000002
actor:  1 policy actor:  1  step number:  56 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.102]
 [0.102]
 [0.102]
 [0.102]
 [0.102]
 [0.102]
 [0.102]] [[29.65]
 [29.65]
 [29.65]
 [29.65]
 [29.65]
 [29.65]
 [29.65]] [[0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  39.558998516951846
maxi score, test score, baseline:  0.1159266666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.1159266666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  63.921082619690154
maxi score, test score, baseline:  0.1159266666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.618]
 [0.487]
 [0.492]
 [0.483]
 [0.48 ]
 [0.503]] [[32.918]
 [35.112]
 [32.918]
 [34.499]
 [34.389]
 [35.07 ]
 [35.129]] [[1.098]
 [1.309]
 [1.098]
 [1.162]
 [1.148]
 [1.171]
 [1.195]]
printing an ep nov before normalisation:  49.205577244957006
maxi score, test score, baseline:  0.1159266666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.1159266666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
line 256 mcts: sample exp_bonus 34.34471181032209
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11335333333333315 0.6930000000000002 0.6930000000000002
printing an ep nov before normalisation:  29.175541624900745
printing an ep nov before normalisation:  42.07296847782588
maxi score, test score, baseline:  0.11335333333333315 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  62 total reward:  0.12666666666666615  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.11335333333333315 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.393]
 [0.405]
 [0.304]
 [0.372]
 [0.393]
 [0.333]
 [0.408]] [[31.923]
 [32.552]
 [32.293]
 [34.036]
 [31.923]
 [39.375]
 [31.562]] [[1.057]
 [1.094]
 [0.983]
 [1.121]
 [1.057]
 [1.299]
 [1.057]]
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.511]
 [0.516]
 [0.479]
 [0.5  ]
 [0.476]
 [0.456]] [[34.617]
 [33.097]
 [34.115]
 [34.629]
 [35.171]
 [36.561]
 [34.237]] [[1.393]
 [1.337]
 [1.399]
 [1.391]
 [1.443]
 [1.497]
 [1.346]]
maxi score, test score, baseline:  0.11335333333333315 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.685]
 [0.675]
 [0.675]
 [0.675]
 [0.498]
 [0.599]] [[27.914]
 [24.234]
 [27.914]
 [27.914]
 [27.914]
 [27.005]
 [26.781]] [[1.99 ]
 [1.662]
 [1.99 ]
 [1.99 ]
 [1.99 ]
 [1.73 ]
 [1.81 ]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.28666666666666607  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.834576295386405
printing an ep nov before normalisation:  41.12032117317745
maxi score, test score, baseline:  0.11335333333333315 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11335333333333315 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11335333333333315 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11335333333333315 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  23.070617943932014
printing an ep nov before normalisation:  36.5412390500477
maxi score, test score, baseline:  0.11335333333333315 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11335333333333315 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  41 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  1.667
main train batch thing paused
add a thread
Adding thread: now have 3 threads
actor:  1 policy actor:  1  step number:  68 total reward:  0.09999999999999898  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  63.50483161434845
maxi score, test score, baseline:  0.11335333333333315 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.11335333333333315 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  32.53023540665839
maxi score, test score, baseline:  0.11335333333333315 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11335333333333315 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  28.30005168914795
maxi score, test score, baseline:  0.11335333333333315 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  0 policy actor:  1  step number:  54 total reward:  0.21999999999999964  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.11308666666666649 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11308666666666649 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  30.626788846215824
printing an ep nov before normalisation:  49.51824188232422
siam score:  -0.7807734
maxi score, test score, baseline:  0.11308666666666649 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  24.889280485917652
maxi score, test score, baseline:  0.11308666666666649 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  30.086743845233652
printing an ep nov before normalisation:  38.79008728159949
printing an ep nov before normalisation:  27.133203411733753
actor:  0 policy actor:  0  step number:  46 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  68 total reward:  0.09999999999999931  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  56.79860334017763
printing an ep nov before normalisation:  55.383348122249764
maxi score, test score, baseline:  0.1130466666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.1130466666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  35.698298222094614
printing an ep nov before normalisation:  45.83444349067398
actor:  1 policy actor:  1  step number:  41 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  39.90601186467315
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.1130466666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.1130466666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  40.23951867621147
maxi score, test score, baseline:  0.1130466666666665 0.6930000000000002 0.6930000000000002
Printing some Q and Qe and total Qs values:  [[0.387]
 [0.466]
 [0.353]
 [0.351]
 [0.354]
 [0.387]
 [0.354]] [[38.238]
 [38.374]
 [30.734]
 [30.964]
 [31.133]
 [38.238]
 [31.282]] [[1.542]
 [1.63 ]
 [1.042]
 [1.054]
 [1.068]
 [1.542]
 [1.078]]
actor:  1 policy actor:  1  step number:  58 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1130466666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  20.02005484872545
line 256 mcts: sample exp_bonus 19.529504452438783
maxi score, test score, baseline:  0.1130466666666665 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  37.547834400028975
printing an ep nov before normalisation:  28.50470495617424
printing an ep nov before normalisation:  39.49173229872607
maxi score, test score, baseline:  0.11023333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11023333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11023333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  73 total reward:  0.10666666666666569  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
actor:  1 policy actor:  1  step number:  39 total reward:  0.56  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.224]
 [0.382]
 [0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.224]] [[47.365]
 [46.468]
 [47.365]
 [47.365]
 [47.365]
 [47.365]
 [47.365]] [[1.607]
 [1.715]
 [1.607]
 [1.607]
 [1.607]
 [1.607]
 [1.607]]
printing an ep nov before normalisation:  35.01146027260032
actor:  1 policy actor:  1  step number:  39 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11023333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
siam score:  -0.7792729
maxi score, test score, baseline:  0.11023333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  41.360525920508486
maxi score, test score, baseline:  0.11023333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actions average: 
K:  2  action  0 :  tensor([0.5638, 0.0164, 0.0643, 0.0710, 0.0919, 0.0676, 0.1251],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0148, 0.9169, 0.0104, 0.0137, 0.0049, 0.0058, 0.0335],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1273, 0.0156, 0.4164, 0.0976, 0.1054, 0.1204, 0.1173],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2168, 0.0110, 0.1380, 0.1307, 0.1506, 0.1491, 0.2038],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1860, 0.0219, 0.0856, 0.1098, 0.3814, 0.0884, 0.1269],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1010, 0.0197, 0.1307, 0.0620, 0.0415, 0.5623, 0.0827],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1890, 0.0650, 0.1333, 0.0954, 0.0959, 0.1150, 0.3065],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.11023333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  53 total reward:  0.23999999999999977  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11023333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.11023333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11023333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11023333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  48 total reward:  0.4733333333333334  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11023333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  37.371764183044434
maxi score, test score, baseline:  0.11023333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.21 ]
 [0.23 ]
 [0.192]
 [0.192]
 [0.195]
 [0.227]
 [0.188]] [[37.569]
 [40.683]
 [36.697]
 [36.087]
 [36.378]
 [38.169]
 [36.471]] [[1.081]
 [1.239]
 [1.024]
 [0.997]
 [1.013]
 [1.124]
 [1.01 ]]
maxi score, test score, baseline:  0.11023333333333316 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.11023333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.329]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]] [[52.591]
 [49.713]
 [48.324]
 [48.324]
 [48.324]
 [48.324]
 [48.324]] [[1.42 ]
 [1.328]
 [1.232]
 [1.232]
 [1.232]
 [1.232]
 [1.232]]
maxi score, test score, baseline:  0.11023333333333316 0.6930000000000002 0.6930000000000002
actor:  1 policy actor:  1  step number:  54 total reward:  0.43333333333333335  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  4  action  0 :  tensor([0.4646, 0.0439, 0.0842, 0.0968, 0.1341, 0.0757, 0.1006],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0080, 0.9461, 0.0040, 0.0066, 0.0021, 0.0032, 0.0300],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1406, 0.0050, 0.3491, 0.1140, 0.1152, 0.1837, 0.0924],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([nan, nan, nan, nan, nan, nan, nan], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1580, 0.0037, 0.0650, 0.1208, 0.4451, 0.1212, 0.0863],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2095, 0.0468, 0.1381, 0.1430, 0.1027, 0.2312, 0.1287],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1927, 0.0170, 0.1261, 0.2304, 0.1356, 0.1486, 0.1496],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.193]
 [0.226]
 [0.226]
 [0.226]
 [0.226]
 [0.226]
 [0.226]] [[50.741]
 [43.558]
 [43.558]
 [43.558]
 [43.558]
 [43.558]
 [43.558]] [[1.701]
 [1.375]
 [1.375]
 [1.375]
 [1.375]
 [1.375]
 [1.375]]
maxi score, test score, baseline:  0.11023333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  58 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  45.483077797539146
maxi score, test score, baseline:  0.11023333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.589]
 [0.48 ]
 [0.49 ]
 [0.493]
 [0.438]
 [0.499]] [[41.016]
 [35.644]
 [39.012]
 [38.952]
 [39.021]
 [39.287]
 [38.543]] [[2.103]
 [1.82 ]
 [1.957]
 [1.964]
 [1.972]
 [1.935]
 [1.942]]
Printing some Q and Qe and total Qs values:  [[0.213]
 [0.356]
 [0.213]
 [0.213]
 [0.213]
 [0.213]
 [0.213]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.213]
 [0.356]
 [0.213]
 [0.213]
 [0.213]
 [0.213]
 [0.213]]
maxi score, test score, baseline:  0.11023333333333316 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  41.84648513793945
actor:  1 policy actor:  1  step number:  35 total reward:  0.5333333333333333  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  0.11023333333333317 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  28.829029974957926
maxi score, test score, baseline:  0.11023333333333317 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
siam score:  -0.79022074
maxi score, test score, baseline:  0.11023333333333317 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11023333333333317 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  58 total reward:  0.2199999999999993  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.141]
 [0.141]
 [0.131]
 [0.141]
 [0.141]
 [0.141]
 [0.141]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.141]
 [0.141]
 [0.131]
 [0.141]
 [0.141]
 [0.141]
 [0.141]]
Printing some Q and Qe and total Qs values:  [[0.281]
 [0.251]
 [0.251]
 [0.251]
 [0.226]
 [0.251]
 [0.251]] [[52.335]
 [49.844]
 [49.844]
 [49.844]
 [46.131]
 [49.844]
 [49.844]] [[1.828]
 [1.662]
 [1.662]
 [1.662]
 [1.431]
 [1.662]
 [1.662]]
printing an ep nov before normalisation:  43.63638399170667
Printing some Q and Qe and total Qs values:  [[0.45 ]
 [0.509]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]] [[39.581]
 [46.911]
 [39.581]
 [39.581]
 [39.581]
 [39.581]
 [39.581]] [[1.908]
 [2.452]
 [1.908]
 [1.908]
 [1.908]
 [1.908]
 [1.908]]
maxi score, test score, baseline:  0.11023333333333317 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  43.19784378002404
maxi score, test score, baseline:  0.11023333333333317 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  29.43183660507202
maxi score, test score, baseline:  0.11023333333333317 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  0 policy actor:  1  step number:  44 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11304666666666649 0.6930000000000002 0.6930000000000002
line 256 mcts: sample exp_bonus 29.56623413680358
maxi score, test score, baseline:  0.11304666666666649 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.11304666666666649 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11304666666666649 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  63 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  34.66507234648987
maxi score, test score, baseline:  0.11309999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  69 total reward:  0.09333333333333216  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.11309999999999983 0.6930000000000002 0.6930000000000002
actor:  1 policy actor:  1  step number:  54 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  42 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.11384666666666649 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  33.28539763667284
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.531]] [[16.157]
 [17.718]
 [17.718]
 [17.718]
 [17.718]
 [17.718]
 [15.698]] [[1.533]
 [1.716]
 [1.716]
 [1.716]
 [1.716]
 [1.716]
 [1.475]]
maxi score, test score, baseline:  0.11384666666666649 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  67 total reward:  0.11999999999999911  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  23.220938365339713
actor:  1 policy actor:  1  step number:  44 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11384666666666649 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actions average: 
K:  2  action  0 :  tensor([0.6671, 0.0218, 0.0526, 0.0534, 0.0674, 0.0633, 0.0743],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0086, 0.9228, 0.0058, 0.0138, 0.0030, 0.0049, 0.0411],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0920, 0.0159, 0.4557, 0.0826, 0.0842, 0.1453, 0.1244],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0747, 0.0296, 0.0672, 0.5248, 0.0749, 0.1435, 0.0854],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1213, 0.0012, 0.0590, 0.0691, 0.5996, 0.0747, 0.0751],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1188, 0.0288, 0.1909, 0.0971, 0.1060, 0.3305, 0.1279],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0438, 0.0964, 0.1304, 0.1517, 0.0378, 0.0651, 0.4748],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  65.42613726657828
printing an ep nov before normalisation:  38.106558322906494
printing an ep nov before normalisation:  25.74465274810791
actor:  0 policy actor:  0  step number:  62 total reward:  0.12666666666666626  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.467308170493595
printing an ep nov before normalisation:  36.91674692828024
maxi score, test score, baseline:  0.11609999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.261]
 [0.255]
 [0.261]
 [0.261]
 [0.261]
 [0.261]
 [0.261]] [[55.454]
 [66.498]
 [55.454]
 [55.454]
 [55.454]
 [55.454]
 [55.454]] [[1.031]
 [1.297]
 [1.031]
 [1.031]
 [1.031]
 [1.031]
 [1.031]]
maxi score, test score, baseline:  0.11609999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  46.881730855051266
maxi score, test score, baseline:  0.11609999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  49 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  32.439664147277966
printing an ep nov before normalisation:  27.74959087371826
maxi score, test score, baseline:  0.11609999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  28.911923265502104
actor:  1 policy actor:  1  step number:  54 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11609999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
siam score:  -0.78228766
actor:  1 policy actor:  1  step number:  37 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.11609999999999983 0.6930000000000002 0.6930000000000002
printing an ep nov before normalisation:  56.02375144812035
maxi score, test score, baseline:  0.11609999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11609999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11609999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11609999999999983 0.6930000000000002 0.6930000000000002
printing an ep nov before normalisation:  57.7430613091343
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.623]
 [0.535]
 [0.526]
 [0.522]
 [0.548]
 [0.555]] [[22.506]
 [22.821]
 [26.104]
 [25.621]
 [25.335]
 [25.87 ]
 [26.18 ]] [[1.524]
 [1.582]
 [1.633]
 [1.603]
 [1.587]
 [1.636]
 [1.656]]
maxi score, test score, baseline:  0.11609999999999983 0.6930000000000002 0.6930000000000002
actor:  1 policy actor:  1  step number:  66 total reward:  0.04666666666666597  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11609999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11609999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  36.02555158540996
maxi score, test score, baseline:  0.11609999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actions average: 
K:  4  action  0 :  tensor([0.3893, 0.0027, 0.0759, 0.0970, 0.2002, 0.0942, 0.1408],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0039, 0.9409, 0.0029, 0.0154, 0.0010, 0.0013, 0.0348],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0605, 0.0175, 0.8079, 0.0270, 0.0144, 0.0505, 0.0222],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1381, 0.0307, 0.1149, 0.2642, 0.1305, 0.1971, 0.1245],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1409, 0.0033, 0.1071, 0.0656, 0.3849, 0.2184, 0.0799],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1107, 0.0081, 0.1309, 0.1014, 0.1112, 0.4431, 0.0945],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2149, 0.0163, 0.1646, 0.1522, 0.1148, 0.1303, 0.2069],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  18.71225613437165
actor:  1 policy actor:  1  step number:  57 total reward:  0.3599999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.11609999999999983 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  0 policy actor:  1  step number:  58 total reward:  0.32666666666666655  reward:  1.0 rdn_beta:  0.667
from probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[-0.032]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.052]
 [ 0.034]
 [-0.027]] [[53.465]
 [56.795]
 [56.795]
 [56.795]
 [54.513]
 [51.436]
 [56.795]] [[1.445]
 [1.661]
 [1.661]
 [1.661]
 [1.492]
 [1.384]
 [1.661]]
maxi score, test score, baseline:  0.11875333333333317 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.11875333333333317 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.11875333333333317 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.353]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]] [[41.389]
 [46.193]
 [41.389]
 [41.389]
 [41.389]
 [41.389]
 [41.389]] [[1.814]
 [2.225]
 [1.814]
 [1.814]
 [1.814]
 [1.814]
 [1.814]]
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.581]
 [0.54 ]
 [0.557]
 [0.535]
 [0.572]
 [0.538]] [[32.695]
 [31.303]
 [32.08 ]
 [30.758]
 [31.764]
 [31.095]
 [31.663]] [[1.584]
 [1.561]
 [1.571]
 [1.501]
 [1.546]
 [1.539]
 [1.542]]
maxi score, test score, baseline:  0.11875333333333317 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]] [[34.707]
 [34.707]
 [34.707]
 [34.707]
 [34.707]
 [34.707]
 [34.707]] [[69.697]
 [69.697]
 [69.697]
 [69.697]
 [69.697]
 [69.697]
 [69.697]]
printing an ep nov before normalisation:  19.591643740668466
actor:  0 policy actor:  1  step number:  49 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.015]
 [-0.004]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.015]
 [-0.004]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]]
actor:  1 policy actor:  1  step number:  70 total reward:  0.3933333333333334  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.12136666666666651 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.12136666666666651 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.12136666666666651 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  40.30863285064697
actions average: 
K:  2  action  0 :  tensor([0.6684, 0.0026, 0.0459, 0.0731, 0.0904, 0.0599, 0.0598],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0125, 0.9161, 0.0120, 0.0129, 0.0084, 0.0061, 0.0320],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1259, 0.0238, 0.4850, 0.0839, 0.1027, 0.0992, 0.0795],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2067, 0.0364, 0.0994, 0.1986, 0.1650, 0.1732, 0.1206],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1503, 0.0019, 0.0559, 0.0698, 0.5717, 0.0798, 0.0706],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1002, 0.0232, 0.0880, 0.0841, 0.1282, 0.5019, 0.0745],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1117, 0.0949, 0.0844, 0.0783, 0.0656, 0.0989, 0.4662],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  47.64599254454381
printing an ep nov before normalisation:  48.87748694001194
maxi score, test score, baseline:  0.12136666666666651 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.12136666666666651 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  37.19029442207934
printing an ep nov before normalisation:  42.729443155722464
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12136666666666651 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.12136666666666651 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  0.2133333333333327  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12136666666666651 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.12136666666666651 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.12136666666666651 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.12136666666666651 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  64 total reward:  0.36666666666666603  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12136666666666651 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
maxi score, test score, baseline:  0.12136666666666651 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
actor:  1 policy actor:  1  step number:  58 total reward:  0.3533333333333325  reward:  1.0 rdn_beta:  1.333
Starting evaluation
maxi score, test score, baseline:  0.12136666666666651 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Printing some Q and Qe and total Qs values:  [[0.692]
 [0.852]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]] [[42.849]
 [40.435]
 [38.626]
 [38.626]
 [38.626]
 [38.626]
 [38.626]] [[0.692]
 [0.852]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]]
Printing some Q and Qe and total Qs values:  [[0.685]
 [0.794]
 [0.685]
 [0.657]
 [0.655]
 [0.653]
 [0.658]] [[37.397]
 [45.719]
 [37.397]
 [42.108]
 [43.177]
 [43.612]
 [41.658]] [[0.685]
 [0.794]
 [0.685]
 [0.657]
 [0.655]
 [0.653]
 [0.658]]
Printing some Q and Qe and total Qs values:  [[0.829]
 [0.877]
 [0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]] [[55.22 ]
 [52.797]
 [55.22 ]
 [55.22 ]
 [55.22 ]
 [55.22 ]
 [55.22 ]] [[0.829]
 [0.877]
 [0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]]
printing an ep nov before normalisation:  45.550605636492065
printing an ep nov before normalisation:  50.54025799355165
printing an ep nov before normalisation:  48.17359442675902
maxi score, test score, baseline:  0.12136666666666651 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  38.748833801515275
Printing some Q and Qe and total Qs values:  [[0.823]
 [0.878]
 [0.823]
 [0.823]
 [0.823]
 [0.823]
 [0.823]] [[39.197]
 [34.725]
 [39.197]
 [39.197]
 [39.197]
 [39.197]
 [39.197]] [[0.823]
 [0.878]
 [0.823]
 [0.823]
 [0.823]
 [0.823]
 [0.823]]
printing an ep nov before normalisation:  33.021026995202185
maxi score, test score, baseline:  0.12136666666666651 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  0.12136666666666651 0.6930000000000002 0.6930000000000002
probs:  [0.11594257160986725, 0.11594257160986725, 0.11594257160986725, 0.42028714195066386, 0.11594257160986725, 0.11594257160986725]
printing an ep nov before normalisation:  55.22328130350774
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333333  reward:  1.0 rdn_beta:  2
actor:  1 policy actor:  1  step number:  52 total reward:  0.4866666666666669  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.12607333333333315 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12607333333333315 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12607333333333315 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12607333333333315 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12607333333333315 0.6933333333333335 0.6933333333333335
printing an ep nov before normalisation:  41.81218471458545
actor:  1 policy actor:  1  step number:  55 total reward:  0.48  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.12607333333333315 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
siam score:  -0.7846156
actor:  0 policy actor:  0  step number:  45 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1254066666666665 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actions average: 
K:  2  action  0 :  tensor([0.4453, 0.0117, 0.0746, 0.0894, 0.1956, 0.0848, 0.0986],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0077, 0.9375, 0.0090, 0.0066, 0.0014, 0.0030, 0.0349],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0986, 0.0073, 0.5814, 0.0623, 0.0498, 0.1143, 0.0864],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1485, 0.3092, 0.0778, 0.1508, 0.0766, 0.0672, 0.1699],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2051, 0.0028, 0.0521, 0.0490, 0.5670, 0.0367, 0.0874],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1522, 0.0093, 0.1345, 0.1267, 0.1100, 0.3493, 0.1180],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1786, 0.0868, 0.1204, 0.1680, 0.1289, 0.1365, 0.1808],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1254066666666665 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  42 total reward:  0.4466666666666663  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1254066666666665 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.1254066666666665 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.453]
 [0.374]
 [0.372]
 [0.373]
 [0.393]
 [0.429]] [[25.736]
 [28.233]
 [24.682]
 [24.854]
 [25.045]
 [27.466]
 [27.239]] [[1.126]
 [1.348]
 [1.069]
 [1.077]
 [1.089]
 [1.245]
 [1.268]]
actor:  0 policy actor:  0  step number:  46 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  41.78114365283989
printing an ep nov before normalisation:  36.39550646035196
printing an ep nov before normalisation:  42.05781032073166
printing an ep nov before normalisation:  41.26387975523272
actor:  1 policy actor:  1  step number:  54 total reward:  0.3533333333333325  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  56.852093288524415
Printing some Q and Qe and total Qs values:  [[0.189]
 [0.277]
 [0.23 ]
 [0.189]
 [0.189]
 [0.134]
 [0.189]] [[50.08 ]
 [48.535]
 [53.021]
 [50.08 ]
 [50.08 ]
 [46.473]
 [50.08 ]] [[1.075]
 [1.115]
 [1.206]
 [1.075]
 [1.075]
 [0.91 ]
 [1.075]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.2933333333333328  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.69344392416664
actor:  1 policy actor:  1  step number:  40 total reward:  0.5000000000000001  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12473999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  57.087256095118704
maxi score, test score, baseline:  0.12473999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  39.802543912649114
maxi score, test score, baseline:  0.12473999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12473999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  33.92066102864767
printing an ep nov before normalisation:  28.026890302361487
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12257999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12257999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  67 total reward:  0.22666666666666635  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12257999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  42.437580656880016
maxi score, test score, baseline:  0.12257999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  62 total reward:  0.2599999999999998  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  3  action  0 :  tensor([0.4822, 0.0197, 0.0803, 0.0783, 0.1153, 0.1092, 0.1151],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0035, 0.9502, 0.0035, 0.0106, 0.0020, 0.0028, 0.0274],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1236, 0.0488, 0.3919, 0.0772, 0.0943, 0.1205, 0.1438],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1322, 0.0172, 0.0979, 0.2947, 0.0999, 0.1647, 0.1935],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1578, 0.0631, 0.0640, 0.0953, 0.4383, 0.0978, 0.0838],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1425, 0.0282, 0.1432, 0.1651, 0.1259, 0.2003, 0.1948],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2163, 0.0132, 0.1460, 0.1239, 0.1178, 0.1360, 0.2468],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  45.26082033157371
maxi score, test score, baseline:  0.12257999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.12257999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  37 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.12257999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12257999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.704]
 [0.784]
 [0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.704]] [[29.449]
 [34.553]
 [29.449]
 [29.449]
 [29.449]
 [29.449]
 [29.449]] [[1.358]
 [1.64 ]
 [1.358]
 [1.358]
 [1.358]
 [1.358]
 [1.358]]
printing an ep nov before normalisation:  25.23331749175372
printing an ep nov before normalisation:  0.016598002260934663
siam score:  -0.7855879
maxi score, test score, baseline:  0.12257999999999983 0.6933333333333335 0.6933333333333335
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.12257999999999983 0.6933333333333335 0.6933333333333335
actor:  1 policy actor:  1  step number:  48 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12257999999999983 0.6933333333333335 0.6933333333333335
printing an ep nov before normalisation:  44.18035956403121
siam score:  -0.7870206
maxi score, test score, baseline:  0.12257999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12257999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
siam score:  -0.7884224
maxi score, test score, baseline:  0.12036666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  51 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11807333333333314 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.11807333333333314 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.11807333333333314 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.754189623699645
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  46.34377811789934
maxi score, test score, baseline:  0.11807333333333314 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.11807333333333314 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  0 policy actor:  0  step number:  44 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.12072666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  35 total reward:  0.6533333333333335  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  44.43194123070011
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.9618392164224
maxi score, test score, baseline:  0.12072666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12072666666666648 0.6933333333333335 0.6933333333333335
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12072666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  54 total reward:  0.36666666666666603  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12072666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  41.94783338302078
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  45 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.004]
 [ 0.124]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.004]
 [ 0.124]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]]
printing an ep nov before normalisation:  46.09839861171598
Printing some Q and Qe and total Qs values:  [[ 0.105]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.013]] [[56.49 ]
 [55.079]
 [55.079]
 [55.079]
 [55.079]
 [55.079]
 [55.079]] [[0.724]
 [0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]]
maxi score, test score, baseline:  0.12072666666666648 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.12072666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12072666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12072666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  34.38056392123474
printing an ep nov before normalisation:  36.70323363630954
actor:  1 policy actor:  1  step number:  64 total reward:  0.206666666666666  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  34.39050619507062
Printing some Q and Qe and total Qs values:  [[0.318]
 [0.327]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]] [[32.582]
 [33.87 ]
 [33.876]
 [33.876]
 [33.876]
 [33.876]
 [33.876]] [[1.623]
 [1.719]
 [1.64 ]
 [1.64 ]
 [1.64 ]
 [1.64 ]
 [1.64 ]]
maxi score, test score, baseline:  0.12072666666666648 0.6933333333333335 0.6933333333333335
Printing some Q and Qe and total Qs values:  [[0.674]
 [0.83 ]
 [0.725]
 [0.786]
 [0.699]
 [0.725]
 [0.769]] [[38.821]
 [35.637]
 [54.629]
 [31.884]
 [36.775]
 [52.108]
 [34.061]] [[0.674]
 [0.83 ]
 [0.725]
 [0.786]
 [0.699]
 [0.725]
 [0.769]]
printing an ep nov before normalisation:  33.12805655707384
printing an ep nov before normalisation:  33.24669316811947
maxi score, test score, baseline:  0.12072666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12072666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  30.902431227548323
printing an ep nov before normalisation:  29.05936895030254
maxi score, test score, baseline:  0.12072666666666648 0.6933333333333335 0.6933333333333335
actor:  1 policy actor:  1  step number:  53 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12072666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  0 policy actor:  1  step number:  60 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.12313999999999982 0.6933333333333335 0.6933333333333335
printing an ep nov before normalisation:  37.42452456320959
maxi score, test score, baseline:  0.12313999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  54 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12313999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12313999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  45 total reward:  0.5600000000000003  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  41 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 36.31632349587476
maxi score, test score, baseline:  0.12313999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12313999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  58.967249625714324
Printing some Q and Qe and total Qs values:  [[0.266]
 [0.522]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]] [[42.226]
 [51.558]
 [42.226]
 [42.226]
 [42.226]
 [42.226]
 [42.226]] [[1.15 ]
 [1.787]
 [1.15 ]
 [1.15 ]
 [1.15 ]
 [1.15 ]
 [1.15 ]]
line 256 mcts: sample exp_bonus 29.42732400235189
printing an ep nov before normalisation:  35.19808205958228
maxi score, test score, baseline:  0.12313999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12313999999999982 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.12313999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12313999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  37 total reward:  0.5466666666666667  reward:  1.0 rdn_beta:  2.0
from probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
siam score:  -0.77687836
maxi score, test score, baseline:  0.12313999999999982 0.6933333333333335 0.6933333333333335
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  0.12313999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actions average: 
K:  1  action  0 :  tensor([0.8753, 0.0083, 0.0049, 0.0025, 0.0430, 0.0009, 0.0649],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0137, 0.9000, 0.0101, 0.0174, 0.0045, 0.0045, 0.0499],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0962, 0.0020, 0.5036, 0.0616, 0.0760, 0.1606, 0.1000],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1643, 0.0064, 0.1584, 0.2632, 0.1395, 0.1148, 0.1533],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1950, 0.0229, 0.1035, 0.0913, 0.3845, 0.1092, 0.0934],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1586, 0.0027, 0.1112, 0.0818, 0.1131, 0.4200, 0.1126],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1411, 0.0072, 0.2459, 0.0944, 0.1346, 0.2359, 0.1409],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.12313999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actions average: 
K:  4  action  0 :  tensor([0.4580, 0.0284, 0.0828, 0.0928, 0.1297, 0.0895, 0.1188],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0155, 0.8529, 0.0148, 0.0429, 0.0092, 0.0101, 0.0546],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0386, 0.0203, 0.6384, 0.0376, 0.0618, 0.1460, 0.0573],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1083, 0.2045, 0.0981, 0.1819, 0.1213, 0.0958, 0.1901],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1416, 0.0215, 0.0903, 0.1157, 0.4336, 0.0987, 0.0985],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1537, 0.0342, 0.1771, 0.1321, 0.1183, 0.2057, 0.1788],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2185, 0.0224, 0.1640, 0.1307, 0.1420, 0.1110, 0.2113],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  42.818887543273135
siam score:  -0.7747345
maxi score, test score, baseline:  0.12313999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
siam score:  -0.77693754
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.846]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.719]] [[30.494]
 [27.578]
 [30.494]
 [30.494]
 [30.494]
 [30.494]
 [28.462]] [[0.653]
 [0.846]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.719]]
line 256 mcts: sample exp_bonus 46.602352881852255
maxi score, test score, baseline:  0.12313999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  31.006256182554345
actor:  0 policy actor:  1  step number:  41 total reward:  0.5200000000000001  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  61 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  40.891786470840486
actor:  1 policy actor:  1  step number:  51 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.012]
 [-0.007]
 [-0.012]
 [-0.012]
 [-0.012]
 [-0.012]
 [-0.012]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.012]
 [-0.007]
 [-0.012]
 [-0.012]
 [-0.012]
 [-0.012]
 [-0.012]]
Printing some Q and Qe and total Qs values:  [[0.213]
 [0.275]
 [0.213]
 [0.213]
 [0.213]
 [0.213]
 [0.213]] [[44.577]
 [39.255]
 [44.577]
 [44.577]
 [44.577]
 [44.577]
 [44.577]] [[1.343]
 [1.16 ]
 [1.343]
 [1.343]
 [1.343]
 [1.343]
 [1.343]]
siam score:  -0.7783156
maxi score, test score, baseline:  0.12377999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12377999999999983 0.6933333333333335 0.6933333333333335
printing an ep nov before normalisation:  49.73649373444976
printing an ep nov before normalisation:  48.682830270349356
maxi score, test score, baseline:  0.12377999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12377999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
siam score:  -0.7764516
actor:  1 policy actor:  1  step number:  69 total reward:  0.0799999999999994  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12377999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12377999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  69 total reward:  0.18666666666666598  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  55 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.768]
 [0.807]
 [0.763]
 [0.816]
 [0.774]
 [0.764]
 [0.793]] [[33.198]
 [32.606]
 [32.801]
 [31.75 ]
 [33.955]
 [33.864]
 [32.761]] [[0.768]
 [0.807]
 [0.763]
 [0.816]
 [0.774]
 [0.764]
 [0.793]]
Printing some Q and Qe and total Qs values:  [[0.589]
 [0.643]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]] [[32.459]
 [34.733]
 [32.459]
 [32.459]
 [32.459]
 [32.459]
 [32.459]] [[0.589]
 [0.643]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]]
maxi score, test score, baseline:  0.12087333333333315 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  66.21154732157079
actor:  0 policy actor:  0  step number:  44 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.084]
 [0.211]
 [0.084]
 [0.084]
 [0.084]
 [0.084]
 [0.084]] [[39.229]
 [65.071]
 [39.229]
 [39.229]
 [39.229]
 [39.229]
 [39.229]] [[0.507]
 [1.124]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]]
actor:  1 policy actor:  1  step number:  35 total reward:  0.56  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12117999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12117999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  0 policy actor:  0  step number:  39 total reward:  0.52  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]] [[48.446]
 [48.446]
 [48.446]
 [48.446]
 [48.446]
 [48.446]
 [48.446]] [[1.031]
 [1.031]
 [1.031]
 [1.031]
 [1.031]
 [1.031]
 [1.031]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1214199999999998 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.878]
 [0.878]
 [0.878]
 [0.878]
 [0.878]
 [0.878]
 [0.878]] [[43.28]
 [43.28]
 [43.28]
 [43.28]
 [43.28]
 [43.28]
 [43.28]] [[0.878]
 [0.878]
 [0.878]
 [0.878]
 [0.878]
 [0.878]
 [0.878]]
maxi score, test score, baseline:  0.1214199999999998 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  35.717331384575246
maxi score, test score, baseline:  0.1214199999999998 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  59 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1214199999999998 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  28.328203550358925
actor:  0 policy actor:  1  step number:  67 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12133999999999981 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  53.10004482920459
actor:  1 policy actor:  1  step number:  49 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12133999999999981 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.12133999999999981 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.12133999999999981 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12133999999999981 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12133999999999981 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12133999999999981 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  64 total reward:  0.2599999999999989  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  1.1555564185528056e-05
actor:  1 policy actor:  1  step number:  52 total reward:  0.43333333333333346  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.679]
 [0.711]
 [0.679]
 [0.679]
 [0.679]
 [0.679]
 [0.679]] [[35.968]
 [32.261]
 [35.968]
 [35.968]
 [35.968]
 [35.968]
 [35.968]] [[1.298]
 [1.201]
 [1.298]
 [1.298]
 [1.298]
 [1.298]
 [1.298]]
printing an ep nov before normalisation:  27.002689838409424
maxi score, test score, baseline:  0.12133999999999981 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.12133999999999981 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  70 total reward:  0.13999999999999946  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.12133999999999981 0.6933333333333335 0.6933333333333335
printing an ep nov before normalisation:  26.442079681371826
actor:  1 policy actor:  1  step number:  52 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.12133999999999981 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12133999999999981 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12133999999999981 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  39.404640197753906
printing an ep nov before normalisation:  37.79987335205078
Printing some Q and Qe and total Qs values:  [[0.426]
 [0.463]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]] [[31.116]
 [34.017]
 [31.116]
 [31.116]
 [31.116]
 [31.116]
 [31.116]] [[1.303]
 [1.505]
 [1.303]
 [1.303]
 [1.303]
 [1.303]
 [1.303]]
printing an ep nov before normalisation:  33.610438989384775
maxi score, test score, baseline:  0.12133999999999981 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.12133999999999981 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[ 0.068]
 [-0.08 ]
 [-0.032]
 [ 0.025]
 [ 0.083]
 [ 0.116]
 [-0.014]] [[46.076]
 [44.271]
 [42.457]
 [46.277]
 [47.477]
 [42.318]
 [43.723]] [[1.153]
 [0.915]
 [0.873]
 [1.119]
 [1.237]
 [1.014]
 [0.954]]
printing an ep nov before normalisation:  32.16368579129583
actor:  1 policy actor:  1  step number:  52 total reward:  0.42000000000000004  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  61 total reward:  0.18666666666666587  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12133999999999981 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12133999999999981 0.6933333333333335 0.6933333333333335
line 256 mcts: sample exp_bonus 32.860074520111084
maxi score, test score, baseline:  0.12133999999999981 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.916]
 [0.911]
 [0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]] [[44.519]
 [55.9  ]
 [44.519]
 [44.519]
 [44.519]
 [44.519]
 [44.519]] [[0.916]
 [0.911]
 [0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]]
maxi score, test score, baseline:  0.12133999999999981 0.6933333333333335 0.6933333333333335
printing an ep nov before normalisation:  29.980427065434483
maxi score, test score, baseline:  0.12133999999999981 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  21.443989931697786
printing an ep nov before normalisation:  30.063939094543457
line 256 mcts: sample exp_bonus 28.57702294992644
actor:  0 policy actor:  0  step number:  36 total reward:  0.5  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12433999999999984 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  41.177457390268685
printing an ep nov before normalisation:  24.768220248042176
printing an ep nov before normalisation:  37.2912456513592
siam score:  -0.7752045
maxi score, test score, baseline:  0.12433999999999984 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  44 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.12433999999999984 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
siam score:  -0.7764534
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  45 total reward:  0.4933333333333336  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.12433999999999984 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[-0.004]
 [-0.107]
 [ 0.107]
 [-0.023]
 [-0.025]
 [ 0.07 ]
 [-0.187]] [[43.666]
 [52.122]
 [39.276]
 [44.004]
 [46.245]
 [42.147]
 [42.29 ]] [[1.229]
 [1.56 ]
 [1.113]
 [1.227]
 [1.34 ]
 [1.224]
 [0.975]]
maxi score, test score, baseline:  0.12433999999999984 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12433999999999984 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.12433999999999984 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12433999999999984 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12433999999999984 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]] [[27.684]
 [27.684]
 [27.684]
 [27.684]
 [27.684]
 [27.684]
 [27.684]] [[0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]]
printing an ep nov before normalisation:  28.593583068716946
actor:  1 policy actor:  1  step number:  51 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]]
from probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  57 total reward:  0.3466666666666667  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  1  action  0 :  tensor([0.5717, 0.0030, 0.0574, 0.0709, 0.1227, 0.0746, 0.0996],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0052, 0.9413, 0.0091, 0.0098, 0.0034, 0.0047, 0.0265],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1274, 0.0099, 0.3543, 0.1303, 0.0813, 0.1801, 0.1166],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1977, 0.0051, 0.1049, 0.2825, 0.1452, 0.1344, 0.1302],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2070, 0.0056, 0.0544, 0.0893, 0.4831, 0.0891, 0.0716],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1186, 0.0026, 0.1142, 0.1127, 0.0913, 0.4667, 0.0939],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0800, 0.1319, 0.1630, 0.0670, 0.0402, 0.0469, 0.4710],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  29.25663913059369
siam score:  -0.77098274
actor:  1 policy actor:  1  step number:  59 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  0.333
from probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]] [[44.599]
 [44.599]
 [44.599]
 [44.599]
 [44.599]
 [44.599]
 [44.599]] [[1.454]
 [1.454]
 [1.454]
 [1.454]
 [1.454]
 [1.454]
 [1.454]]
maxi score, test score, baseline:  0.12433999999999984 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.768]
 [0.842]
 [0.778]
 [0.798]
 [0.779]
 [0.755]
 [0.78 ]] [[32.612]
 [33.861]
 [31.324]
 [32.801]
 [33.638]
 [31.436]
 [32.313]] [[0.768]
 [0.842]
 [0.778]
 [0.798]
 [0.779]
 [0.755]
 [0.78 ]]
printing an ep nov before normalisation:  23.966059684753418
printing an ep nov before normalisation:  11.64616544543103
actor:  0 policy actor:  0  step number:  41 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.024]
 [ 0.094]
 [-0.231]
 [-0.006]
 [-0.02 ]
 [-0.006]
 [-0.005]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.024]
 [ 0.094]
 [-0.231]
 [-0.006]
 [-0.02 ]
 [-0.006]
 [-0.005]]
maxi score, test score, baseline:  0.12719333333333313 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.12719333333333313 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12719333333333313 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12719333333333313 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  35.64792798831287
actor:  0 policy actor:  1  step number:  63 total reward:  0.013333333333332642  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1262999999999998 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.107]
 [0.211]
 [0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]] [[44.558]
 [41.894]
 [44.558]
 [44.558]
 [44.558]
 [44.558]
 [44.558]] [[1.178]
 [1.165]
 [1.178]
 [1.178]
 [1.178]
 [1.178]
 [1.178]]
maxi score, test score, baseline:  0.1262999999999998 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  48 total reward:  0.5266666666666667  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  39.298436805485125
maxi score, test score, baseline:  0.1262999999999998 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  22.83468008041382
actor:  1 policy actor:  1  step number:  69 total reward:  0.013333333333332531  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  28.904080762246355
printing an ep nov before normalisation:  26.31502697215624
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.585]
 [0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.534]] [[25.406]
 [27.095]
 [25.406]
 [25.406]
 [25.406]
 [25.406]
 [25.406]] [[1.381]
 [1.564]
 [1.381]
 [1.381]
 [1.381]
 [1.381]
 [1.381]]
printing an ep nov before normalisation:  50.80694671398313
Printing some Q and Qe and total Qs values:  [[0.054]
 [0.115]
 [0.054]
 [0.054]
 [0.054]
 [0.054]
 [0.054]] [[42.554]
 [45.344]
 [42.554]
 [42.554]
 [42.554]
 [42.554]
 [42.554]] [[1.073]
 [1.265]
 [1.073]
 [1.073]
 [1.073]
 [1.073]
 [1.073]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.33333333333333315  reward:  1.0 rdn_beta:  1.333
siam score:  -0.78211474
maxi score, test score, baseline:  0.1262999999999998 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  27.711449518091186
printing an ep nov before normalisation:  29.309814949856314
actor:  0 policy actor:  0  step number:  39 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.12627333333333315 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
from probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12627333333333315 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.22285813676289
printing an ep nov before normalisation:  28.579527717088908
maxi score, test score, baseline:  0.12627333333333315 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  45 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  52 total reward:  0.27333333333333265  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.12627333333333315 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12351333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  0 policy actor:  1  step number:  51 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  1.0
from probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12609999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.64046382904053
printing an ep nov before normalisation:  31.1885716066651
maxi score, test score, baseline:  0.12609999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12609999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12609999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  0 policy actor:  0  step number:  42 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  42.252872805136946
actor:  1 policy actor:  1  step number:  60 total reward:  0.31333333333333324  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.12635333333333315 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12635333333333315 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12635333333333315 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.12635333333333315 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  42.55302429199219
printing an ep nov before normalisation:  31.291361880738872
maxi score, test score, baseline:  0.12635333333333315 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12635333333333315 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
siam score:  -0.780381
line 256 mcts: sample exp_bonus 41.13744804866491
maxi score, test score, baseline:  0.12635333333333315 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12635333333333315 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  67 total reward:  0.06666666666666576  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12635333333333315 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12635333333333315 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12635333333333315 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  50.98394045271495
printing an ep nov before normalisation:  14.034252830701632
maxi score, test score, baseline:  0.12635333333333315 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.682]
 [0.747]
 [0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.682]] [[30.037]
 [27.731]
 [30.037]
 [30.037]
 [30.037]
 [30.037]
 [30.037]] [[1.985]
 [1.878]
 [1.985]
 [1.985]
 [1.985]
 [1.985]
 [1.985]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12635333333333315 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  38.909193569886554
maxi score, test score, baseline:  0.12635333333333315 0.6933333333333335 0.6933333333333335
printing an ep nov before normalisation:  14.947047977842658
actor:  1 policy actor:  1  step number:  64 total reward:  0.07333333333333292  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  33.072642124637035
actor:  1 policy actor:  1  step number:  47 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.12635333333333315 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  45.13431528716299
maxi score, test score, baseline:  0.12635333333333315 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  53 total reward:  0.1866666666666661  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  34.59023962300412
using another actor
printing an ep nov before normalisation:  30.0757110589227
maxi score, test score, baseline:  0.12364666666666649 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  41.835198402404785
maxi score, test score, baseline:  0.12364666666666649 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.696]
 [0.663]
 [0.663]
 [0.663]
 [0.686]
 [0.696]
 [0.663]] [[26.318]
 [26.348]
 [26.348]
 [26.348]
 [23.061]
 [23.145]
 [26.348]] [[2.696]
 [2.667]
 [2.667]
 [2.667]
 [2.252]
 [2.273]
 [2.667]]
Printing some Q and Qe and total Qs values:  [[0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]] [[24.256]
 [24.256]
 [24.256]
 [24.256]
 [24.256]
 [24.256]
 [24.256]] [[2.204]
 [2.204]
 [2.204]
 [2.204]
 [2.204]
 [2.204]
 [2.204]]
maxi score, test score, baseline:  0.12364666666666649 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  25.95456600189209
printing an ep nov before normalisation:  33.014355620660076
maxi score, test score, baseline:  0.12364666666666649 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.884]
 [0.969]
 [0.884]
 [0.884]
 [0.884]
 [0.884]
 [0.884]] [[21.711]
 [23.428]
 [21.711]
 [21.711]
 [21.711]
 [21.711]
 [21.711]] [[0.884]
 [0.969]
 [0.884]
 [0.884]
 [0.884]
 [0.884]
 [0.884]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.4266666666666662  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.076]
 [0.187]
 [0.083]
 [0.083]
 [0.074]
 [0.083]
 [0.083]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.076]
 [0.187]
 [0.083]
 [0.083]
 [0.074]
 [0.083]
 [0.083]]
siam score:  -0.7762581
maxi score, test score, baseline:  0.12364666666666649 0.6933333333333335 0.6933333333333335
actor:  0 policy actor:  1  step number:  40 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7749618
maxi score, test score, baseline:  0.1239266666666665 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  38.321786590124134
line 256 mcts: sample exp_bonus 38.359377669451064
printing an ep nov before normalisation:  44.22175132739942
maxi score, test score, baseline:  0.12155333333333317 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12155333333333317 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  49.417352828697354
maxi score, test score, baseline:  0.12155333333333317 0.6933333333333335 0.6933333333333335
printing an ep nov before normalisation:  37.46855728733902
maxi score, test score, baseline:  0.12155333333333317 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12155333333333317 0.6933333333333335 0.6933333333333335
actor:  1 policy actor:  1  step number:  64 total reward:  0.2199999999999992  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12155333333333317 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12155333333333317 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  0 policy actor:  1  step number:  53 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  45.407507141837414
maxi score, test score, baseline:  0.12155333333333315 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  28.3421972462129
UNIT TEST: sample policy line 217 mcts : [0.388 0.122 0.    0.    0.265 0.143 0.082]
printing an ep nov before normalisation:  26.177846585519575
maxi score, test score, baseline:  0.12155333333333315 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  59 total reward:  0.3199999999999992  reward:  1.0 rdn_beta:  2.0
siam score:  -0.77759844
siam score:  -0.77818036
printing an ep nov before normalisation:  29.400571119778736
actor:  1 policy actor:  1  step number:  59 total reward:  0.14666666666666617  reward:  1.0 rdn_beta:  0.667
from probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.11888666666666649 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  45.24835389179032
printing an ep nov before normalisation:  47.33112245936812
maxi score, test score, baseline:  0.11888666666666649 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.11888666666666649 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.11888666666666649 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.11888666666666649 0.6933333333333335 0.6933333333333335
actor:  0 policy actor:  0  step number:  53 total reward:  0.2933333333333328  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  41 total reward:  0.5200000000000001  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
using explorer policy with actor:  1
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  47 total reward:  0.3866666666666666  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.12143333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.12143333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actions average: 
K:  0  action  0 :  tensor([0.3947, 0.0321, 0.1037, 0.0970, 0.1228, 0.1172, 0.1324],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0043,     0.9594,     0.0013,     0.0014,     0.0005,     0.0003,
            0.0329], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1395, 0.0499, 0.3528, 0.0767, 0.0729, 0.1567, 0.1515],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1665, 0.0660, 0.1025, 0.2755, 0.1239, 0.1250, 0.1408],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1733, 0.0055, 0.0622, 0.0640, 0.5539, 0.0678, 0.0734],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1291, 0.0074, 0.1518, 0.0644, 0.0662, 0.4819, 0.0991],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1760, 0.1436, 0.1562, 0.1015, 0.0990, 0.0947, 0.2290],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.12143333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  40.009237741719865
siam score:  -0.78664637
siam score:  -0.7874678
maxi score, test score, baseline:  0.12143333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  0.12143333333333316 0.6933333333333335 0.6933333333333335
printing an ep nov before normalisation:  38.58106863735185
maxi score, test score, baseline:  0.12143333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.369]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]] [[37.152]
 [42.401]
 [37.152]
 [37.152]
 [37.152]
 [37.152]
 [37.152]] [[0.81 ]
 [0.895]
 [0.81 ]
 [0.81 ]
 [0.81 ]
 [0.81 ]
 [0.81 ]]
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.035]
 [0.019]
 [0.011]
 [0.027]
 [0.005]
 [0.007]] [[33.774]
 [44.053]
 [33.298]
 [32.319]
 [40.114]
 [33.082]
 [33.478]] [[0.341]
 [0.553]
 [0.34 ]
 [0.313]
 [0.472]
 [0.321]
 [0.331]]
maxi score, test score, baseline:  0.12143333333333316 0.6933333333333335 0.6933333333333335
using explorer policy with actor:  0
siam score:  -0.78816557
actions average: 
K:  0  action  0 :  tensor([0.6299, 0.0167, 0.0500, 0.0568, 0.1240, 0.0555, 0.0671],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0081, 0.9541, 0.0021, 0.0041, 0.0010, 0.0012, 0.0294],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1100, 0.0074, 0.4746, 0.0886, 0.0899, 0.1521, 0.0775],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1087, 0.0503, 0.1008, 0.4086, 0.1163, 0.1110, 0.1043],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1949, 0.0025, 0.0918, 0.1325, 0.3416, 0.1159, 0.1208],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0659, 0.0397, 0.1155, 0.0431, 0.0486, 0.6403, 0.0471],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0824, 0.0188, 0.1085, 0.0800, 0.0536, 0.1049, 0.5518],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.12143333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
siam score:  -0.7873882
maxi score, test score, baseline:  0.12143333333333316 0.6933333333333335 0.6933333333333335
actions average: 
K:  2  action  0 :  tensor([0.5078, 0.0137, 0.0952, 0.0865, 0.1026, 0.0935, 0.1007],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0042,     0.9543,     0.0057,     0.0066,     0.0009,     0.0063,
            0.0219], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0484, 0.0185, 0.5763, 0.0874, 0.0589, 0.1479, 0.0626],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1468, 0.0153, 0.1313, 0.2227, 0.1222, 0.2297, 0.1319],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0829, 0.0331, 0.0359, 0.0401, 0.7188, 0.0406, 0.0486],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1436, 0.0018, 0.1783, 0.1166, 0.1252, 0.2965, 0.1380],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1160, 0.2707, 0.1188, 0.1311, 0.0778, 0.1202, 0.1653],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  41.36145850024341
actor:  1 policy actor:  1  step number:  40 total reward:  0.4066666666666666  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.328]
 [0.322]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]] [[46.317]
 [47.162]
 [46.317]
 [46.317]
 [46.317]
 [46.317]
 [46.317]] [[0.972]
 [0.989]
 [0.972]
 [0.972]
 [0.972]
 [0.972]
 [0.972]]
actor:  0 policy actor:  1  step number:  64 total reward:  0.19333333333333202  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.12149999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12149999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  43.64291191101074
Printing some Q and Qe and total Qs values:  [[0.623]
 [0.7  ]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]] [[27.216]
 [24.395]
 [27.216]
 [27.216]
 [27.216]
 [27.216]
 [27.216]] [[1.225]
 [1.21 ]
 [1.225]
 [1.225]
 [1.225]
 [1.225]
 [1.225]]
maxi score, test score, baseline:  0.12149999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12149999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  68 total reward:  0.12666666666666593  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.12149999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12149999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.43]
 [0.43]
 [0.43]
 [0.43]
 [0.43]
 [0.43]
 [0.43]] [[42.642]
 [42.642]
 [42.642]
 [42.642]
 [42.642]
 [42.642]
 [42.642]] [[1.689]
 [1.689]
 [1.689]
 [1.689]
 [1.689]
 [1.689]
 [1.689]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.46716940438939
printing an ep nov before normalisation:  25.00108204735088
actor:  0 policy actor:  1  step number:  55 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.18228274456505
siam score:  -0.7852037
actor:  1 policy actor:  1  step number:  66 total reward:  0.08666666666666623  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  50 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  1.333
using another actor
maxi score, test score, baseline:  0.12132666666666649 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12132666666666649 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.12132666666666649 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
using explorer policy with actor:  0
printing an ep nov before normalisation:  38.410115242004395
actor:  1 policy actor:  1  step number:  46 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12132666666666649 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.12132666666666649 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12132666666666649 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.12132666666666649 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  30.828243413012878
printing an ep nov before normalisation:  32.5844975752477
maxi score, test score, baseline:  0.12132666666666649 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  24.31180477142334
maxi score, test score, baseline:  0.12132666666666649 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  20.737683216251042
maxi score, test score, baseline:  0.12132666666666649 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  39.98509855996602
actor:  0 policy actor:  1  step number:  59 total reward:  0.14666666666666595  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  35.01768492082104
maxi score, test score, baseline:  0.12361999999999981 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12361999999999981 0.6933333333333335 0.6933333333333335
actor:  1 policy actor:  1  step number:  38 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.12361999999999981 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  34.35288665055826
maxi score, test score, baseline:  0.12361999999999981 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  32.86258802233056
maxi score, test score, baseline:  0.12361999999999981 0.6933333333333335 0.6933333333333335
printing an ep nov before normalisation:  21.33078171935862
actor:  1 policy actor:  1  step number:  66 total reward:  0.23333333333333262  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  48 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12361999999999981 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.445]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]] [[27.339]
 [29.201]
 [27.339]
 [27.339]
 [27.339]
 [27.339]
 [27.339]] [[0.585]
 [0.652]
 [0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.585]]
maxi score, test score, baseline:  0.12361999999999981 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  44 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.12361999999999981 0.6933333333333335 0.6933333333333335
Printing some Q and Qe and total Qs values:  [[0.096]
 [0.193]
 [0.122]
 [0.096]
 [0.096]
 [0.096]
 [0.096]] [[36.016]
 [41.001]
 [34.373]
 [36.016]
 [36.016]
 [36.016]
 [36.016]] [[1.093]
 [1.472]
 [1.027]
 [1.093]
 [1.093]
 [1.093]
 [1.093]]
maxi score, test score, baseline:  0.12361999999999981 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actions average: 
K:  1  action  0 :  tensor([0.5279, 0.0132, 0.0628, 0.0664, 0.1158, 0.1061, 0.1078],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0045, 0.9593, 0.0064, 0.0019, 0.0012, 0.0012, 0.0255],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0725, 0.0351, 0.5723, 0.0363, 0.0457, 0.1655, 0.0725],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1805, 0.0055, 0.1055, 0.2804, 0.1321, 0.1491, 0.1468],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1642, 0.0204, 0.0904, 0.0941, 0.4458, 0.0966, 0.0885],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1696, 0.0079, 0.1136, 0.1288, 0.1294, 0.3089, 0.1417],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1683, 0.1118, 0.0899, 0.1483, 0.0939, 0.1054, 0.2824],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  42 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.12361999999999981 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  34.01350498199463
printing an ep nov before normalisation:  37.97299321045867
maxi score, test score, baseline:  0.12361999999999981 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12361999999999981 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  49 total reward:  0.48  reward:  1.0 rdn_beta:  0.667
from probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.229723628429895
maxi score, test score, baseline:  0.12361999999999981 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  27.02510118484497
maxi score, test score, baseline:  0.12361999999999981 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.12361999999999981 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.1210599999999998 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.1210599999999998 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1210599999999998 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.101]
 [0.189]
 [0.122]
 [0.101]
 [0.101]
 [0.093]
 [0.101]] [[35.154]
 [37.14 ]
 [43.715]
 [35.154]
 [35.154]
 [42.213]
 [35.154]] [[1.171]
 [1.32 ]
 [1.453]
 [1.171]
 [1.171]
 [1.378]
 [1.171]]
siam score:  -0.7789109
actor:  1 policy actor:  1  step number:  57 total reward:  0.4800000000000001  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  25.723127142114997
printing an ep nov before normalisation:  34.82795231822373
actor:  1 policy actor:  1  step number:  64 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.25379719659748
maxi score, test score, baseline:  0.1210599999999998 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.1210599999999998 0.6933333333333335 0.6933333333333335
siam score:  -0.776268
actor:  1 policy actor:  1  step number:  62 total reward:  0.23333333333333273  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  46 total reward:  0.44666666666666677  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.12395333333333314 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[-0.002]
 [ 0.151]
 [ 0.038]
 [ 0.046]
 [ 0.041]
 [ 0.038]
 [ 0.123]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.002]
 [ 0.151]
 [ 0.038]
 [ 0.046]
 [ 0.041]
 [ 0.038]
 [ 0.123]]
printing an ep nov before normalisation:  55.36134158550478
maxi score, test score, baseline:  0.12395333333333314 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.12395333333333314 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
line 256 mcts: sample exp_bonus 48.77973584505399
printing an ep nov before normalisation:  37.300539774110455
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.687]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]] [[26.868]
 [30.339]
 [26.868]
 [26.868]
 [26.868]
 [26.868]
 [26.868]] [[1.85 ]
 [2.167]
 [1.85 ]
 [1.85 ]
 [1.85 ]
 [1.85 ]
 [1.85 ]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.2599999999999998  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12395333333333314 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.12395333333333314 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actions average: 
K:  3  action  0 :  tensor([0.5007, 0.0166, 0.1020, 0.0835, 0.1053, 0.0917, 0.1002],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0119, 0.9200, 0.0135, 0.0113, 0.0055, 0.0107, 0.0272],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1556, 0.0041, 0.3899, 0.1005, 0.1100, 0.1287, 0.1113],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1706, 0.0178, 0.1136, 0.2789, 0.1100, 0.1208, 0.1883],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1904, 0.0512, 0.0908, 0.1006, 0.3773, 0.1057, 0.0840],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1039, 0.0158, 0.1536, 0.0727, 0.0966, 0.4870, 0.0702],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0628, 0.1773, 0.0737, 0.1432, 0.1304, 0.0691, 0.3435],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  42.405819399556336
printing an ep nov before normalisation:  38.3814514440742
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[ 0.0586],
        [-0.0000],
        [ 0.5512],
        [-0.0000],
        [ 0.5155],
        [-0.4627],
        [ 0.5840],
        [-0.0000],
        [ 0.5155],
        [ 0.4526]], dtype=torch.float64)
-0.08397170119799999 -0.02538646286372051
-0.7297392703260001 -0.7297392703260001
-0.09703970119800001 0.45418754882677337
-0.0263339999999995 -0.0263339999999995
-0.045026434398 0.4704728847103826
-0.09703970119800001 -0.5597820043764584
-0.045546567066 0.5384263175968856
-0.862554 -0.862554
-0.058094434398 0.45740488471038265
-0.07129443439800001 0.38128518296275604
actor:  0 policy actor:  0  step number:  51 total reward:  0.26666666666666616  reward:  1.0 rdn_beta:  2.0
siam score:  -0.779496
maxi score, test score, baseline:  0.12648666666666647 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12648666666666647 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.12648666666666647 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  40.2670955657959
maxi score, test score, baseline:  0.12648666666666647 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.12648666666666647 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  53.0604916281062
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  1.667
using another actor
maxi score, test score, baseline:  0.12648666666666647 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  31.251279022347575
maxi score, test score, baseline:  0.12648666666666647 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.12648666666666647 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  62 total reward:  0.11333333333333273  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  1  step number:  48 total reward:  0.4066666666666663  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1292999999999998 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  53.843214785024
maxi score, test score, baseline:  0.1292999999999998 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  0 policy actor:  1  step number:  44 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.72328111815658
printing an ep nov before normalisation:  27.4449324348831
maxi score, test score, baseline:  0.13200666666666647 0.6933333333333335 0.6933333333333335
actor:  1 policy actor:  1  step number:  50 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.13200666666666647 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.13200666666666647 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.13200666666666647 0.6933333333333335 0.6933333333333335
printing an ep nov before normalisation:  21.569676399230957
actor:  0 policy actor:  0  step number:  50 total reward:  0.3133333333333327  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.13463333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  32.95125722885132
printing an ep nov before normalisation:  35.46994290837768
printing an ep nov before normalisation:  42.63116792538183
printing an ep nov before normalisation:  34.13931769812277
actor:  1 policy actor:  1  step number:  68 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.13463333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  42.13751558574306
printing an ep nov before normalisation:  23.005072527777486
siam score:  -0.77320373
actor:  1 policy actor:  1  step number:  55 total reward:  0.19999999999999962  reward:  1.0 rdn_beta:  1.333
siam score:  -0.7728636
printing an ep nov before normalisation:  37.88561758242881
maxi score, test score, baseline:  0.13463333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.13463333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.176]
 [0.335]
 [0.22 ]
 [0.176]
 [0.176]
 [0.176]
 [0.178]] [[36.395]
 [42.928]
 [42.789]
 [36.395]
 [36.395]
 [36.395]
 [42.972]] [[0.424]
 [0.667]
 [0.55 ]
 [0.424]
 [0.424]
 [0.424]
 [0.511]]
printing an ep nov before normalisation:  50.83271392772402
maxi score, test score, baseline:  0.13463333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.13463333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.13463333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  0.13463333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  34 total reward:  0.6066666666666667  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.13463333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  42 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  37.236095252092454
Printing some Q and Qe and total Qs values:  [[0.184]
 [0.289]
 [0.184]
 [0.184]
 [0.184]
 [0.184]
 [0.184]] [[34.038]
 [41.041]
 [34.038]
 [34.038]
 [34.038]
 [34.038]
 [34.038]] [[0.613]
 [0.936]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]]
maxi score, test score, baseline:  0.13463333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.13463333333333316 0.6933333333333335 0.6933333333333335
actor:  1 policy actor:  1  step number:  64 total reward:  0.23333333333333295  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.13463333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  50.99731516989231
maxi score, test score, baseline:  0.13463333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  30.685038296706917
maxi score, test score, baseline:  0.13463333333333316 0.6933333333333335 0.6933333333333335
actor:  1 policy actor:  1  step number:  52 total reward:  0.4066666666666662  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  0  action  0 :  tensor([0.5276, 0.0418, 0.0705, 0.1012, 0.0929, 0.0862, 0.0798],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0051, 0.9650, 0.0034, 0.0068, 0.0029, 0.0031, 0.0137],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0779, 0.0278, 0.5047, 0.1042, 0.0765, 0.1004, 0.1085],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1938, 0.0047, 0.1263, 0.1681, 0.1798, 0.1688, 0.1584],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1653, 0.0034, 0.0652, 0.0915, 0.5343, 0.0707, 0.0696],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1286, 0.0095, 0.1060, 0.1129, 0.0966, 0.4637, 0.0827],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1681, 0.2986, 0.1273, 0.0787, 0.0589, 0.0700, 0.1984],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.13463333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  51 total reward:  0.2666666666666664  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.13463333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.342]] [[49.269]
 [49.269]
 [49.269]
 [49.269]
 [49.269]
 [49.269]
 [49.269]] [[1.675]
 [1.675]
 [1.675]
 [1.675]
 [1.675]
 [1.675]
 [1.675]]
maxi score, test score, baseline:  0.13463333333333316 0.6933333333333335 0.6933333333333335
printing an ep nov before normalisation:  34.25788382089917
maxi score, test score, baseline:  0.13463333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.673]
 [0.766]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.649]] [[21.987]
 [27.505]
 [21.807]
 [21.807]
 [21.807]
 [21.807]
 [21.691]] [[1.843]
 [2.23 ]
 [1.813]
 [1.813]
 [1.813]
 [1.813]
 [1.803]]
maxi score, test score, baseline:  0.13463333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  33.38338077248711
actor:  1 policy actor:  1  step number:  51 total reward:  0.30666666666666664  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.13463333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
line 256 mcts: sample exp_bonus 40.238229096567565
actor:  1 policy actor:  1  step number:  48 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  59.10518147537582
printing an ep nov before normalisation:  35.350847244262695
maxi score, test score, baseline:  0.13463333333333316 0.6933333333333335 0.6933333333333335
actor:  1 policy actor:  1  step number:  58 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.2  ]
 [0.232]
 [0.17 ]
 [0.214]
 [0.217]
 [0.174]
 [0.207]] [[40.974]
 [39.787]
 [39.501]
 [40.431]
 [40.374]
 [42.118]
 [40.038]] [[1.811]
 [1.754]
 [1.671]
 [1.785]
 [1.783]
 [1.869]
 [1.748]]
printing an ep nov before normalisation:  32.8540944410635
printing an ep nov before normalisation:  32.72146677726589
actor:  1 policy actor:  1  step number:  39 total reward:  0.48  reward:  1.0 rdn_beta:  2.0
from probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.614]
 [0.399]
 [0.44 ]
 [0.429]
 [0.436]
 [0.399]] [[41.753]
 [33.526]
 [43.508]
 [41.148]
 [41.581]
 [42.477]
 [43.508]] [[1.836]
 [1.385]
 [1.952]
 [1.808]
 [1.831]
 [1.908]
 [1.952]]
Printing some Q and Qe and total Qs values:  [[ 0.099]
 [ 0.098]
 [-0.055]
 [ 0.1  ]
 [ 0.072]
 [-0.022]
 [ 0.084]] [[49.386]
 [53.613]
 [47.977]
 [47.017]
 [53.02 ]
 [51.287]
 [55.014]] [[1.11 ]
 [1.249]
 [0.909]
 [1.032]
 [1.203]
 [1.052]
 [1.281]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  50.905190163785804
maxi score, test score, baseline:  0.13463333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  48.63097305939617
printing an ep nov before normalisation:  34.803688526153564
maxi score, test score, baseline:  0.13463333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.13463333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  69 total reward:  0.16000000000000003  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.13463333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.13463333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.764]
 [0.766]
 [0.73 ]
 [0.725]
 [0.726]
 [0.727]
 [0.74 ]] [[29.088]
 [31.005]
 [24.622]
 [25.23 ]
 [25.076]
 [24.906]
 [25.509]] [[0.764]
 [0.766]
 [0.73 ]
 [0.725]
 [0.726]
 [0.727]
 [0.74 ]]
printing an ep nov before normalisation:  51.87973976135254
maxi score, test score, baseline:  0.13463333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.845]
 [0.822]
 [0.845]
 [0.845]
 [0.845]
 [0.845]
 [0.845]] [[33.019]
 [32.252]
 [33.019]
 [33.019]
 [33.019]
 [33.019]
 [33.019]] [[0.845]
 [0.822]
 [0.845]
 [0.845]
 [0.845]
 [0.845]
 [0.845]]
Printing some Q and Qe and total Qs values:  [[0.204]
 [0.242]
 [0.214]
 [0.214]
 [0.216]
 [0.214]
 [0.157]] [[45.131]
 [44.76 ]
 [40.738]
 [40.738]
 [43.805]
 [40.738]
 [43.086]] [[1.481]
 [1.5  ]
 [1.258]
 [1.258]
 [1.424]
 [1.258]
 [1.326]]
maxi score, test score, baseline:  0.13463333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  0 policy actor:  1  step number:  59 total reward:  0.25333333333333263  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  0.13448666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  38.29638648996429
printing an ep nov before normalisation:  0.0004029913486647274
actor:  1 policy actor:  1  step number:  53 total reward:  0.3866666666666666  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  38.38649452864398
Printing some Q and Qe and total Qs values:  [[0.264]
 [0.475]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]] [[34.849]
 [34.846]
 [34.849]
 [34.849]
 [34.849]
 [34.849]
 [34.849]] [[0.452]
 [0.663]
 [0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]]
line 256 mcts: sample exp_bonus 29.40081883747523
siam score:  -0.7740691
maxi score, test score, baseline:  0.13448666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  24.78812392806977
maxi score, test score, baseline:  0.13448666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  30.375208854675293
printing an ep nov before normalisation:  39.083832099551564
maxi score, test score, baseline:  0.13448666666666648 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.13448666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
siam score:  -0.77984
printing an ep nov before normalisation:  43.52225366393727
maxi score, test score, baseline:  0.13448666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[-0.006]
 [-0.004]
 [ 0.003]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.014]] [[41.845]
 [47.712]
 [48.231]
 [41.845]
 [41.845]
 [41.845]
 [50.466]] [[0.594]
 [0.759]
 [0.781]
 [0.594]
 [0.594]
 [0.594]
 [0.827]]
printing an ep nov before normalisation:  48.51193863032476
printing an ep nov before normalisation:  53.80574143017358
printing an ep nov before normalisation:  41.589932441711426
maxi score, test score, baseline:  0.13448666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.787]
 [0.769]
 [0.769]
 [0.769]
 [0.708]
 [0.729]
 [0.769]] [[32.359]
 [35.237]
 [34.508]
 [35.237]
 [29.825]
 [29.345]
 [35.237]] [[1.921]
 [2.102]
 [2.051]
 [2.102]
 [1.667]
 [1.655]
 [2.102]]
maxi score, test score, baseline:  0.13448666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  35.84052403678608
maxi score, test score, baseline:  0.13448666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  59 total reward:  0.25333333333333286  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.13448666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
from probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  49.59981065146196
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.047772828551487
maxi score, test score, baseline:  0.13448666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  32.48309779551923
siam score:  -0.7755467
maxi score, test score, baseline:  0.13448666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  73 total reward:  0.1066666666666658  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  31.965460513454367
actor:  0 policy actor:  0  step number:  38 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  37.317669838455
printing an ep nov before normalisation:  47.18051905800945
actions average: 
K:  0  action  0 :  tensor([0.5686, 0.0059, 0.0604, 0.0871, 0.1035, 0.0791, 0.0954],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0078, 0.9394, 0.0046, 0.0079, 0.0019, 0.0027, 0.0358],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0992, 0.1122, 0.4404, 0.0964, 0.0660, 0.1032, 0.0827],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2043, 0.0012, 0.1222, 0.2372, 0.1317, 0.1524, 0.1510],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1646, 0.0047, 0.0411, 0.0650, 0.5803, 0.0512, 0.0930],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1239, 0.0008, 0.1041, 0.1096, 0.0907, 0.4782, 0.0925],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1270, 0.1925, 0.0785, 0.1189, 0.0478, 0.0709, 0.3644],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.13453999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.13453999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.13453999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.13453999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.13453999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[-0.098]
 [-0.09 ]
 [-0.092]
 [-0.088]
 [-0.09 ]
 [-0.09 ]
 [-0.087]] [[30.007]
 [26.989]
 [29.872]
 [29.335]
 [29.581]
 [29.513]
 [29.349]] [[0.914]
 [0.728]
 [0.911]
 [0.881]
 [0.895]
 [0.891]
 [0.883]]
printing an ep nov before normalisation:  19.564503858163416
maxi score, test score, baseline:  0.13453999999999983 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.13453999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  59 total reward:  0.0799999999999994  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.489]
 [0.61 ]
 [0.489]
 [0.489]
 [0.493]
 [0.489]
 [0.489]] [[28.332]
 [31.617]
 [28.332]
 [27.207]
 [27.199]
 [28.332]
 [28.332]] [[0.893]
 [1.092]
 [0.893]
 [0.866]
 [0.871]
 [0.893]
 [0.893]]
maxi score, test score, baseline:  0.13453999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  40.13155701339919
using explorer policy with actor:  1
maxi score, test score, baseline:  0.13453999999999983 0.6933333333333335 0.6933333333333335
actor:  1 policy actor:  1  step number:  68 total reward:  0.28666666666666607  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7779243
actions average: 
K:  0  action  0 :  tensor([0.4925, 0.0266, 0.0810, 0.0857, 0.0985, 0.0851, 0.1307],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0048, 0.9697, 0.0038, 0.0047, 0.0025, 0.0024, 0.0121],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1126, 0.0135, 0.4785, 0.0715, 0.0817, 0.1291, 0.1131],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1333, 0.0112, 0.0895, 0.4647, 0.0847, 0.1137, 0.1030],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1357, 0.0094, 0.0595, 0.1123, 0.5174, 0.0995, 0.0661],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1098, 0.0017, 0.0965, 0.0517, 0.0556, 0.6331, 0.0515],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1465, 0.2140, 0.0977, 0.1297, 0.1156, 0.1061, 0.1905],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.13453999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.805]
 [0.751]
 [0.751]
 [0.729]
 [0.756]
 [0.751]
 [0.751]] [[54.555]
 [42.703]
 [42.703]
 [42.447]
 [52.231]
 [42.703]
 [42.703]] [[0.805]
 [0.751]
 [0.751]
 [0.729]
 [0.756]
 [0.751]
 [0.751]]
printing an ep nov before normalisation:  38.57414752253285
printing an ep nov before normalisation:  36.937698417002835
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  0.13240666666666645 0.6933333333333335 0.6933333333333335
printing an ep nov before normalisation:  46.4062832536098
maxi score, test score, baseline:  0.13240666666666645 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.13240666666666645 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.13240666666666645 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  45 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  38.50275393324109
actor:  0 policy actor:  0  step number:  56 total reward:  0.25999999999999934  reward:  1.0 rdn_beta:  0.333
from probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.694]
 [0.586]
 [0.607]
 [0.517]
 [0.531]
 [0.572]] [[32.755]
 [28.911]
 [35.526]
 [32.755]
 [32.755]
 [32.398]
 [29.587]] [[1.441]
 [1.35 ]
 [1.548]
 [1.441]
 [1.351]
 [1.349]
 [1.26 ]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  0.13219333333333314 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  33.765858669978215
actor:  1 policy actor:  1  step number:  54 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.13219333333333314 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.13219333333333314 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.13219333333333314 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  63.30539348094244
maxi score, test score, baseline:  0.13219333333333314 0.6933333333333335 0.6933333333333335
Printing some Q and Qe and total Qs values:  [[0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]] [[31.522]
 [31.522]
 [31.522]
 [31.522]
 [31.522]
 [31.522]
 [31.522]] [[0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]]
maxi score, test score, baseline:  0.13219333333333314 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.13219333333333314 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.13219333333333314 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  61 total reward:  0.2266666666666658  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.13219333333333314 0.6933333333333335 0.6933333333333335
siam score:  -0.77209854
maxi score, test score, baseline:  0.13219333333333314 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.13219333333333314 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
maxi score, test score, baseline:  0.13219333333333314 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
Printing some Q and Qe and total Qs values:  [[0.748]
 [0.803]
 [0.751]
 [0.744]
 [0.745]
 [0.746]
 [0.749]] [[35.414]
 [37.153]
 [34.218]
 [33.371]
 [33.254]
 [33.449]
 [33.88 ]] [[0.748]
 [0.803]
 [0.751]
 [0.744]
 [0.745]
 [0.746]
 [0.749]]
printing an ep nov before normalisation:  31.682024431519636
maxi score, test score, baseline:  0.13219333333333314 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  27.847962379455566
printing an ep nov before normalisation:  42.034068346861844
maxi score, test score, baseline:  0.13219333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actions average: 
K:  4  action  0 :  tensor([0.3849, 0.1003, 0.1382, 0.0781, 0.0455, 0.0507, 0.2023],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0038,     0.8965,     0.0054,     0.0160,     0.0007,     0.0009,
            0.0768], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2216, 0.0030, 0.2320, 0.1224, 0.2028, 0.1073, 0.1109],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1297, 0.0296, 0.0712, 0.4553, 0.1663, 0.0821, 0.0657],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1718, 0.0090, 0.0774, 0.1446, 0.4096, 0.0887, 0.0989],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1762, 0.0166, 0.1717, 0.1425, 0.1668, 0.1631, 0.1631],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2015, 0.0953, 0.1204, 0.1439, 0.1512, 0.1430, 0.1446],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.13219333333333316 0.6933333333333335 0.6933333333333335
printing an ep nov before normalisation:  47.75010108947754
UNIT TEST: sample policy line 217 mcts : [0.041 0.245 0.265 0.122 0.    0.02  0.306]
maxi score, test score, baseline:  0.13219333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
main train batch thing paused
add a thread
Adding thread: now have 4 threads
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  23.11482616110353
line 256 mcts: sample exp_bonus 36.520398881819375
maxi score, test score, baseline:  0.13219333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
main train batch thing paused
add a thread
Adding thread: now have 5 threads
actor:  1 policy actor:  1  step number:  52 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  62 total reward:  0.33999999999999986  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  54.354987531040855
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.412096426719465
printing an ep nov before normalisation:  37.916050008866335
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
actor:  0 policy actor:  0  step number:  50 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.13247333333333314 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.250547654143894
actor:  1 policy actor:  1  step number:  48 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  39.70375033714344
printing an ep nov before normalisation:  54.40268907300727
Printing some Q and Qe and total Qs values:  [[0.126]
 [0.234]
 [0.126]
 [0.126]
 [0.126]
 [0.126]
 [0.126]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.126]
 [0.234]
 [0.126]
 [0.126]
 [0.126]
 [0.126]
 [0.126]]
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.568]
 [0.316]
 [0.535]
 [0.459]
 [0.333]
 [0.54 ]] [[44.814]
 [42.547]
 [44.852]
 [41.55 ]
 [44.814]
 [35.221]
 [42.865]] [[0.695]
 [0.786]
 [0.552]
 [0.743]
 [0.695]
 [0.489]
 [0.76 ]]
Printing some Q and Qe and total Qs values:  [[0.276]
 [0.398]
 [0.276]
 [0.276]
 [0.276]
 [0.276]
 [0.276]] [[43.827]
 [50.237]
 [43.827]
 [43.827]
 [43.827]
 [43.827]
 [43.827]] [[1.196]
 [1.527]
 [1.196]
 [1.196]
 [1.196]
 [1.196]
 [1.196]]
printing an ep nov before normalisation:  44.10266495544235
maxi score, test score, baseline:  0.12964666666666647 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.52831031453124
actions average: 
K:  3  action  0 :  tensor([0.5816, 0.0090, 0.0597, 0.0675, 0.1274, 0.0807, 0.0741],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0071, 0.9460, 0.0083, 0.0062, 0.0051, 0.0044, 0.0230],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1989, 0.0055, 0.2292, 0.1289, 0.1369, 0.1443, 0.1563],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1538, 0.2136, 0.0785, 0.1872, 0.0893, 0.0927, 0.1848],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1813, 0.0236, 0.0844, 0.1023, 0.3855, 0.1210, 0.1019],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0872, 0.0121, 0.1176, 0.0485, 0.0702, 0.5890, 0.0753],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1194, 0.0448, 0.1894, 0.1033, 0.0964, 0.1007, 0.3459],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  61 total reward:  0.33333333333333226  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  25.77065944671631
maxi score, test score, baseline:  0.12964666666666647 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  58.432694174724425
line 256 mcts: sample exp_bonus 27.86144805189331
maxi score, test score, baseline:  0.1266866666666665 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  48.7053963646718
actor:  0 policy actor:  1  step number:  43 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  2.0
Starting evaluation
maxi score, test score, baseline:  0.1272999999999998 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.1272999999999998 0.6933333333333335 0.6933333333333335
printing an ep nov before normalisation:  19.643431671590772
printing an ep nov before normalisation:  50.65658218202836
Printing some Q and Qe and total Qs values:  [[0.37 ]
 [0.472]
 [0.37 ]
 [0.331]
 [0.37 ]
 [0.37 ]
 [0.37 ]] [[37.327]
 [39.707]
 [37.327]
 [35.768]
 [37.327]
 [37.327]
 [37.327]] [[1.395]
 [1.609]
 [1.395]
 [1.284]
 [1.395]
 [1.395]
 [1.395]]
maxi score, test score, baseline:  0.1272999999999998 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.1272999999999998 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
actor:  1 policy actor:  1  step number:  57 total reward:  0.2799999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  61.41069847419191
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  37.42491063391586
printing an ep nov before normalisation:  36.14510925287275
printing an ep nov before normalisation:  37.09659924654043
printing an ep nov before normalisation:  30.8055690589248
printing an ep nov before normalisation:  0.07824939868442016
printing an ep nov before normalisation:  31.664602615389605
maxi score, test score, baseline:  0.1272999999999998 0.6933333333333335 0.6933333333333335
printing an ep nov before normalisation:  41.67897350433046
printing an ep nov before normalisation:  41.27621338361262
Printing some Q and Qe and total Qs values:  [[0.874]
 [0.961]
 [0.874]
 [0.874]
 [0.874]
 [0.874]
 [0.874]] [[30.254]
 [31.89 ]
 [30.254]
 [30.254]
 [30.254]
 [30.254]
 [30.254]] [[0.874]
 [0.961]
 [0.874]
 [0.874]
 [0.874]
 [0.874]
 [0.874]]
printing an ep nov before normalisation:  30.53063205388438
maxi score, test score, baseline:  0.1272999999999998 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
printing an ep nov before normalisation:  31.759574988518864
printing an ep nov before normalisation:  36.54910145309044
printing an ep nov before normalisation:  33.29054653152415
printing an ep nov before normalisation:  37.225999452117634
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.983]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]] [[30.76 ]
 [32.134]
 [30.622]
 [30.622]
 [30.622]
 [30.622]
 [30.622]] [[0.681]
 [0.983]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.631]
 [0.69 ]
 [0.724]
 [0.627]
 [0.724]
 [0.63 ]
 [0.724]] [[31.557]
 [33.85 ]
 [27.25 ]
 [31.775]
 [27.25 ]
 [31.923]
 [27.25 ]] [[0.631]
 [0.69 ]
 [0.724]
 [0.627]
 [0.724]
 [0.63 ]
 [0.724]]
printing an ep nov before normalisation:  34.76615108220277
printing an ep nov before normalisation:  36.832995028472794
actor:  0 policy actor:  0  step number:  39 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  2.0
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  26.89647155521103
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[0.699]
 [0.699]
 [0.692]
 [0.694]
 [0.662]
 [0.62 ]
 [0.632]] [[23.533]
 [30.185]
 [32.15 ]
 [30.955]
 [40.208]
 [39.446]
 [26.991]] [[0.699]
 [0.699]
 [0.692]
 [0.694]
 [0.662]
 [0.62 ]
 [0.632]]
actor:  0 policy actor:  0  step number:  25 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333333  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333333  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333333  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  0.13191333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.1169273009430564, 0.1169273009430564, 0.1169273009430564, 0.41536349528471794, 0.1169273009430564, 0.1169273009430564]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  28 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  28 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  28 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  1 policy actor:  1  step number:  54 total reward:  0.23333333333333262  reward:  1.0 rdn_beta:  0.667
using another actor
from probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
siam score:  -0.7734746
actor:  0 policy actor:  0  step number:  40 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12835333333333315 0.6866666666666668 0.6866666666666668
printing an ep nov before normalisation:  34.81938172077656
siam score:  -0.77933294
maxi score, test score, baseline:  0.12835333333333315 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
maxi score, test score, baseline:  0.12835333333333315 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
printing an ep nov before normalisation:  42.44486923119277
printing an ep nov before normalisation:  26.981482597577745
actor:  1 policy actor:  1  step number:  60 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  41.43690992309504
using explorer policy with actor:  0
maxi score, test score, baseline:  0.12835333333333315 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
Printing some Q and Qe and total Qs values:  [[0.926]
 [0.926]
 [0.926]
 [0.926]
 [0.926]
 [0.926]
 [0.926]] [[45.594]
 [45.594]
 [45.594]
 [45.594]
 [45.594]
 [45.594]
 [45.594]] [[0.926]
 [0.926]
 [0.926]
 [0.926]
 [0.926]
 [0.926]
 [0.926]]
actor:  0 policy actor:  1  step number:  51 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  54.06575770846591
maxi score, test score, baseline:  0.13083333333333313 0.6866666666666668 0.6866666666666668
printing an ep nov before normalisation:  51.48884627729135
from probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
maxi score, test score, baseline:  0.13083333333333313 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
printing an ep nov before normalisation:  39.15679869903411
maxi score, test score, baseline:  0.13083333333333313 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
printing an ep nov before normalisation:  0.0004022361534339325
actor:  0 policy actor:  1  step number:  55 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  32.76307561712285
printing an ep nov before normalisation:  29.005568027496338
maxi score, test score, baseline:  0.13051333333333315 0.6866666666666668 0.6866666666666668
Printing some Q and Qe and total Qs values:  [[0.055]
 [0.077]
 [0.055]
 [0.055]
 [0.055]
 [0.055]
 [0.055]] [[39.725]
 [47.135]
 [39.725]
 [39.725]
 [39.725]
 [39.725]
 [39.725]] [[0.979]
 [1.41 ]
 [0.979]
 [0.979]
 [0.979]
 [0.979]
 [0.979]]
maxi score, test score, baseline:  0.13051333333333315 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
printing an ep nov before normalisation:  31.993264998498393
maxi score, test score, baseline:  0.13051333333333315 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
printing an ep nov before normalisation:  40.64381155878829
maxi score, test score, baseline:  0.13051333333333315 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
printing an ep nov before normalisation:  25.28174973062398
actor:  0 policy actor:  0  step number:  61 total reward:  0.17333333333333278  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  55 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  1.667
using another actor
printing an ep nov before normalisation:  40.292601160345534
Printing some Q and Qe and total Qs values:  [[0.392]
 [0.535]
 [0.392]
 [0.392]
 [0.381]
 [0.386]
 [0.392]] [[34.81 ]
 [32.304]
 [34.81 ]
 [34.81 ]
 [34.376]
 [34.379]
 [34.81 ]] [[1.668]
 [1.632]
 [1.668]
 [1.668]
 [1.626]
 [1.631]
 [1.668]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  28.764149120875768
maxi score, test score, baseline:  0.13033999999999982 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
printing an ep nov before normalisation:  19.790775775909424
Printing some Q and Qe and total Qs values:  [[0.967]
 [0.885]
 [0.967]
 [0.967]
 [0.967]
 [0.967]
 [0.967]] [[37.375]
 [36.18 ]
 [37.375]
 [37.375]
 [37.375]
 [37.375]
 [37.375]] [[0.967]
 [0.885]
 [0.967]
 [0.967]
 [0.967]
 [0.967]
 [0.967]]
actor:  0 policy actor:  0  step number:  66 total reward:  0.27333333333333243  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  55 total reward:  0.4133333333333329  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  38.18637360419455
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]] [[40.65]
 [40.96]
 [40.96]
 [40.96]
 [40.96]
 [40.96]
 [40.96]] [[1.365]
 [1.298]
 [1.298]
 [1.298]
 [1.298]
 [1.298]
 [1.298]]
printing an ep nov before normalisation:  27.833638191223145
siam score:  -0.77759916
printing an ep nov before normalisation:  51.538788715101234
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1248866666666665 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
maxi score, test score, baseline:  0.1248866666666665 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
actor:  1 policy actor:  1  step number:  49 total reward:  0.2666666666666664  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.696]
 [0.783]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]] [[25.62 ]
 [31.782]
 [25.62 ]
 [25.62 ]
 [25.62 ]
 [25.62 ]
 [25.62 ]] [[1.086]
 [1.424]
 [1.086]
 [1.086]
 [1.086]
 [1.086]
 [1.086]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3866666666666666  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  1  step number:  52 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  1.333
from probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
actor:  0 policy actor:  1  step number:  49 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12809999999999983 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
maxi score, test score, baseline:  0.12809999999999983 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.12809999999999983 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
printing an ep nov before normalisation:  49.35148636852929
Printing some Q and Qe and total Qs values:  [[0.211]
 [0.36 ]
 [0.211]
 [0.261]
 [0.253]
 [0.211]
 [0.251]] [[29.767]
 [33.912]
 [29.767]
 [36.135]
 [35.938]
 [29.767]
 [36.593]] [[1.294]
 [1.781]
 [1.294]
 [1.863]
 [1.838]
 [1.294]
 [1.889]]
maxi score, test score, baseline:  0.12809999999999983 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
maxi score, test score, baseline:  0.12809999999999983 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.641]
 [0.558]
 [0.558]
 [0.596]
 [0.631]
 [0.606]] [[29.099]
 [33.672]
 [29.099]
 [29.099]
 [25.441]
 [33.598]
 [24.329]] [[1.738]
 [2.158]
 [1.738]
 [1.738]
 [1.506]
 [2.142]
 [1.435]]
maxi score, test score, baseline:  0.12809999999999983 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
printing an ep nov before normalisation:  32.43808902815417
Printing some Q and Qe and total Qs values:  [[0.667]
 [0.732]
 [0.387]
 [0.638]
 [0.626]
 [0.668]
 [0.659]] [[21.383]
 [19.95 ]
 [19.835]
 [20.061]
 [19.62 ]
 [20.724]
 [20.29 ]] [[2.337]
 [2.29 ]
 [1.936]
 [2.204]
 [2.158]
 [2.286]
 [2.243]]
actor:  1 policy actor:  1  step number:  59 total reward:  0.4399999999999996  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  54 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.049]
 [0.006]
 [0.006]
 [0.006]
 [0.003]
 [0.009]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.049]
 [0.006]
 [0.006]
 [0.006]
 [0.003]
 [0.009]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.48  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  47.88802315119334
printing an ep nov before normalisation:  49.76753730575889
maxi score, test score, baseline:  0.12809999999999983 0.6866666666666668 0.6866666666666668
line 256 mcts: sample exp_bonus 31.150097998983092
printing an ep nov before normalisation:  53.03157964810439
maxi score, test score, baseline:  0.12809999999999983 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
printing an ep nov before normalisation:  41.20113313175551
Printing some Q and Qe and total Qs values:  [[0.52 ]
 [0.496]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]] [[41.223]
 [37.416]
 [41.223]
 [41.223]
 [41.223]
 [41.223]
 [41.223]] [[0.941]
 [0.839]
 [0.941]
 [0.941]
 [0.941]
 [0.941]
 [0.941]]
Printing some Q and Qe and total Qs values:  [[ 0.321]
 [ 0.414]
 [ 0.321]
 [ 0.363]
 [-0.131]
 [ 0.321]
 [ 0.321]] [[38.861]
 [38.695]
 [38.861]
 [35.068]
 [35.953]
 [38.861]
 [38.861]] [[0.776]
 [0.865]
 [0.776]
 [0.733]
 [0.259]
 [0.776]
 [0.776]]
maxi score, test score, baseline:  0.12809999999999983 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
from probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
printing an ep nov before normalisation:  26.152535607107907
Printing some Q and Qe and total Qs values:  [[0.526]
 [0.883]
 [0.596]
 [0.527]
 [0.519]
 [0.596]
 [0.516]] [[28.   ]
 [34.375]
 [31.381]
 [29.196]
 [31.01 ]
 [31.381]
 [28.806]] [[0.729]
 [1.226]
 [0.873]
 [0.757]
 [0.788]
 [0.873]
 [0.737]]
printing an ep nov before normalisation:  39.38933121077968
maxi score, test score, baseline:  0.12809999999999983 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
printing an ep nov before normalisation:  38.969093196577674
printing an ep nov before normalisation:  51.942166240500555
actor:  1 policy actor:  1  step number:  88 total reward:  0.1933333333333328  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  48 total reward:  0.37999999999999934  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  40.339054318307426
Printing some Q and Qe and total Qs values:  [[0.132]
 [0.215]
 [0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.132]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.132]
 [0.215]
 [0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.132]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  39 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  42.16432320110123
maxi score, test score, baseline:  0.12863333333333316 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
maxi score, test score, baseline:  0.12863333333333316 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.12863333333333316 0.6866666666666668 0.6866666666666668
printing an ep nov before normalisation:  34.41484237076368
maxi score, test score, baseline:  0.12863333333333316 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]] [[26.154]
 [26.154]
 [26.154]
 [26.154]
 [26.154]
 [26.154]
 [26.154]] [[18.083]
 [18.083]
 [18.083]
 [18.083]
 [18.083]
 [18.083]
 [18.083]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  50 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  45 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  31.667429220369087
maxi score, test score, baseline:  0.12863333333333316 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
printing an ep nov before normalisation:  43.08577060699463
actor:  1 policy actor:  1  step number:  56 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.12863333333333316 0.6866666666666668 0.6866666666666668
actor:  0 policy actor:  0  step number:  45 total reward:  0.39999999999999947  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.261]
 [0.316]
 [0.241]
 [0.257]
 [0.255]
 [0.241]
 [0.241]] [[41.627]
 [38.535]
 [33.194]
 [36.636]
 [42.095]
 [33.194]
 [33.194]] [[1.384]
 [1.303]
 [0.992]
 [1.16 ]
 [1.399]
 [0.992]
 [0.992]]
maxi score, test score, baseline:  0.1288466666666665 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
line 256 mcts: sample exp_bonus 37.98945454398765
printing an ep nov before normalisation:  38.42768921069952
maxi score, test score, baseline:  0.1288466666666665 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
printing an ep nov before normalisation:  40.96588611602783
maxi score, test score, baseline:  0.1288466666666665 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
printing an ep nov before normalisation:  37.528304489125695
printing an ep nov before normalisation:  39.59472749531665
maxi score, test score, baseline:  0.1288466666666665 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.1288466666666665 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
printing an ep nov before normalisation:  31.39018035856247
maxi score, test score, baseline:  0.1288466666666665 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
printing an ep nov before normalisation:  49.66489096040494
actor:  1 policy actor:  1  step number:  45 total reward:  0.5200000000000002  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  53 total reward:  0.33333333333333315  reward:  1.0 rdn_beta:  1.0
from probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
Printing some Q and Qe and total Qs values:  [[ 0.003]
 [ 0.104]
 [ 0.032]
 [ 0.032]
 [-0.005]
 [ 0.02 ]
 [ 0.032]] [[56.726]
 [52.022]
 [47.296]
 [47.296]
 [43.34 ]
 [43.196]
 [47.296]] [[0.975]
 [0.906]
 [0.664]
 [0.664]
 [0.483]
 [0.503]
 [0.664]]
printing an ep nov before normalisation:  25.45616626739502
actions average: 
K:  2  action  0 :  tensor([0.4678, 0.0293, 0.1017, 0.0879, 0.1253, 0.0800, 0.1079],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0131, 0.9447, 0.0077, 0.0123, 0.0024, 0.0026, 0.0172],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1398, 0.0142, 0.3928, 0.0805, 0.0945, 0.1318, 0.1464],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1159, 0.0291, 0.1436, 0.3380, 0.1032, 0.1690, 0.1012],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2375, 0.0034, 0.1101, 0.1091, 0.3239, 0.0955, 0.1205],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1432, 0.0212, 0.1150, 0.0941, 0.0973, 0.4015, 0.1276],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2027, 0.1040, 0.1126, 0.1159, 0.0990, 0.1029, 0.2630],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1288466666666665 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
printing an ep nov before normalisation:  50.44870915951313
line 256 mcts: sample exp_bonus 27.98057717694539
actor:  1 policy actor:  1  step number:  58 total reward:  0.24666666666666626  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  62 total reward:  0.3266666666666659  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  30 total reward:  0.5933333333333334  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.12992666666666647 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
maxi score, test score, baseline:  0.12992666666666647 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
printing an ep nov before normalisation:  30.97819565140395
maxi score, test score, baseline:  0.12992666666666647 0.6866666666666668 0.6866666666666668
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  44.542370331530854
printing an ep nov before normalisation:  45.773762308002105
Printing some Q and Qe and total Qs values:  [[0.232]
 [0.303]
 [0.185]
 [0.188]
 [0.232]
 [0.232]
 [0.232]] [[57.757]
 [50.12 ]
 [45.751]
 [49.794]
 [57.757]
 [57.757]
 [57.757]] [[1.565]
 [1.334]
 [1.044]
 [1.207]
 [1.565]
 [1.565]
 [1.565]]
printing an ep nov before normalisation:  48.9624222076121
maxi score, test score, baseline:  0.12992666666666647 0.6866666666666668 0.6866666666666668
printing an ep nov before normalisation:  49.47494467766832
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.533]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]] [[30.158]
 [32.767]
 [30.158]
 [30.158]
 [30.158]
 [30.158]
 [30.158]] [[1.107]
 [1.2  ]
 [1.107]
 [1.107]
 [1.107]
 [1.107]
 [1.107]]
maxi score, test score, baseline:  0.12992666666666647 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
maxi score, test score, baseline:  0.12992666666666647 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
Printing some Q and Qe and total Qs values:  [[0.52 ]
 [0.635]
 [0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.521]] [[32.533]
 [31.837]
 [34.246]
 [34.246]
 [34.246]
 [34.246]
 [34.246]] [[1.572]
 [1.643]
 [1.682]
 [1.682]
 [1.682]
 [1.682]
 [1.682]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12992666666666647 0.6866666666666668 0.6866666666666668
probs:  [0.10258382036296351, 0.10258382036296351, 0.10258382036296351, 0.48708089818518246, 0.10258382036296351, 0.10258382036296351]
actor:  1 policy actor:  1  step number:  57 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  45 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  62 total reward:  0.1933333333333328  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  32 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  29.52643253419981
maxi score, test score, baseline:  0.12783333333333316 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
maxi score, test score, baseline:  0.12783333333333316 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12783333333333316 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  32.785422137892574
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]] [[30.973]
 [30.973]
 [30.973]
 [30.973]
 [30.973]
 [30.973]
 [30.973]] [[0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]]
printing an ep nov before normalisation:  42.881094100363924
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.124]
 [0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[41.425]
 [43.924]
 [38.66 ]
 [38.66 ]
 [38.66 ]
 [38.66 ]
 [38.66 ]] [[0.449]
 [0.605]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]]
actor:  0 policy actor:  1  step number:  45 total reward:  0.3333333333333328  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  47 total reward:  0.4800000000000002  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.12791333333333313 0.6866666666666668 0.6866666666666668
printing an ep nov before normalisation:  45.846628179585416
actor:  0 policy actor:  0  step number:  48 total reward:  0.24666666666666637  reward:  1.0 rdn_beta:  1.667
from probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  53 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  42.67607752905384
Printing some Q and Qe and total Qs values:  [[0.616]
 [0.681]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]] [[28.148]
 [41.066]
 [30.352]
 [29.293]
 [30.352]
 [30.352]
 [30.352]] [[0.616]
 [0.681]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]]
printing an ep nov before normalisation:  42.860537922396695
maxi score, test score, baseline:  0.12824666666666648 0.6866666666666668 0.6866666666666668
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.85523842639401
printing an ep nov before normalisation:  38.17111228850626
maxi score, test score, baseline:  0.12824666666666648 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
maxi score, test score, baseline:  0.12824666666666648 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  26.924376487731934
maxi score, test score, baseline:  0.12824666666666648 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  27.15821290358158
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
actor:  1 policy actor:  1  step number:  68 total reward:  0.13999999999999968  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  69 total reward:  0.18666666666666576  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.12824666666666648 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
actor:  0 policy actor:  0  step number:  49 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  46.49305985593466
maxi score, test score, baseline:  0.12871333333333312 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  35.66355947183237
maxi score, test score, baseline:  0.12871333333333312 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  24.458846663504758
printing an ep nov before normalisation:  37.73842287083018
maxi score, test score, baseline:  0.12871333333333312 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.12871333333333312 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  31.036219596862793
printing an ep nov before normalisation:  29.4316265049577
maxi score, test score, baseline:  0.12871333333333312 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  27.331690788269043
actions average: 
K:  0  action  0 :  tensor([0.5725, 0.0010, 0.0752, 0.0843, 0.0971, 0.0771, 0.0928],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0022,     0.9836,     0.0015,     0.0023,     0.0008,     0.0009,
            0.0086], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1403, 0.0025, 0.5102, 0.0522, 0.0549, 0.0846, 0.1552],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1850, 0.0171, 0.1271, 0.2293, 0.1523, 0.1258, 0.1635],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1999, 0.0129, 0.0913, 0.0831, 0.4178, 0.1068, 0.0881],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1468, 0.0041, 0.1507, 0.1161, 0.1300, 0.3382, 0.1142],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1786, 0.0113, 0.0872, 0.1088, 0.1081, 0.0845, 0.4214],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  61 total reward:  0.1866666666666661  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12871333333333312 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
actor:  0 policy actor:  0  step number:  38 total reward:  0.5666666666666668  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  29.710453891707406
printing an ep nov before normalisation:  32.92886266402526
maxi score, test score, baseline:  0.12883333333333313 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  30.850254588207573
actor:  1 policy actor:  1  step number:  51 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  1.667
using another actor
printing an ep nov before normalisation:  22.751328522304306
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]] [[40.693]
 [40.693]
 [40.693]
 [40.693]
 [40.693]
 [40.693]
 [40.693]] [[1.629]
 [1.629]
 [1.629]
 [1.629]
 [1.629]
 [1.629]
 [1.629]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.83905649069757
actions average: 
K:  4  action  0 :  tensor([0.3737, 0.0427, 0.0710, 0.1141, 0.2268, 0.0965, 0.0751],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0154, 0.9541, 0.0060, 0.0050, 0.0030, 0.0034, 0.0131],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1434, 0.0724, 0.2886, 0.1078, 0.1217, 0.1431, 0.1230],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2632, 0.0245, 0.1142, 0.1379, 0.1923, 0.1430, 0.1249],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1766, 0.0016, 0.0789, 0.0969, 0.4561, 0.0948, 0.0949],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1659, 0.0106, 0.1137, 0.1306, 0.1533, 0.2979, 0.1279],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1451, 0.0585, 0.1058, 0.1877, 0.1745, 0.1355, 0.1929],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  60 total reward:  0.4733333333333335  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.12883333333333313 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
actor:  0 policy actor:  1  step number:  39 total reward:  0.6933333333333336  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1294599999999998 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
actions average: 
K:  3  action  0 :  tensor([0.4160, 0.0229, 0.0651, 0.0962, 0.2200, 0.0969, 0.0830],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0134, 0.8792, 0.0211, 0.0204, 0.0062, 0.0076, 0.0521],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1107, 0.0058, 0.4951, 0.1023, 0.0690, 0.1381, 0.0790],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1720, 0.0013, 0.1429, 0.1585, 0.1554, 0.1642, 0.2056],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1548, 0.0020, 0.0845, 0.1440, 0.4128, 0.0961, 0.1057],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1216, 0.0021, 0.0647, 0.1174, 0.1161, 0.5202, 0.0578],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1247, 0.0432, 0.1599, 0.1136, 0.0955, 0.1004, 0.3627],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1294599999999998 0.6866666666666668 0.6866666666666668
Printing some Q and Qe and total Qs values:  [[0.169]
 [0.239]
 [0.219]
 [0.219]
 [0.193]
 [0.219]
 [0.219]] [[38.436]
 [36.818]
 [29.216]
 [29.216]
 [36.815]
 [29.216]
 [29.216]] [[1.315]
 [1.303]
 [0.895]
 [0.895]
 [1.256]
 [0.895]
 [0.895]]
maxi score, test score, baseline:  0.1294599999999998 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  30.61557277578837
maxi score, test score, baseline:  0.1294599999999998 0.6866666666666668 0.6866666666666668
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1294599999999998 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
actor:  0 policy actor:  1  step number:  52 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  55 total reward:  0.44000000000000017  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12983333333333316 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  31.462452665732208
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12983333333333316 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  39.92691455470054
actor:  1 policy actor:  1  step number:  43 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.436]
 [0.394]
 [0.385]
 [0.394]
 [0.37 ]
 [0.391]] [[37.892]
 [34.067]
 [39.986]
 [38.971]
 [40.153]
 [41.786]
 [38.089]] [[1.236]
 [1.096]
 [1.323]
 [1.268]
 [1.33 ]
 [1.38 ]
 [1.234]]
maxi score, test score, baseline:  0.12983333333333316 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.12983333333333316 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
maxi score, test score, baseline:  0.12983333333333316 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.543]
 [0.48 ]
 [0.481]
 [0.457]
 [0.434]
 [0.454]] [[32.005]
 [30.689]
 [27.832]
 [29.001]
 [31.61 ]
 [31.089]
 [32.005]] [[1.675]
 [1.68 ]
 [1.432]
 [1.508]
 [1.653]
 [1.596]
 [1.675]]
maxi score, test score, baseline:  0.12983333333333316 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
maxi score, test score, baseline:  0.12983333333333316 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.12983333333333316 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
using explorer policy with actor:  1
printing an ep nov before normalisation:  22.095872473476064
Printing some Q and Qe and total Qs values:  [[0.633]
 [0.764]
 [0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]] [[25.925]
 [24.662]
 [25.925]
 [25.925]
 [25.925]
 [25.925]
 [25.925]] [[2.561]
 [2.526]
 [2.561]
 [2.561]
 [2.561]
 [2.561]
 [2.561]]
actor:  1 policy actor:  1  step number:  43 total reward:  0.5200000000000002  reward:  1.0 rdn_beta:  1.0
using another actor
maxi score, test score, baseline:  0.12983333333333316 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
maxi score, test score, baseline:  0.12983333333333316 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  28.583900878982554
actor:  1 policy actor:  1  step number:  70 total reward:  0.28666666666666607  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  3.981684429012091
printing an ep nov before normalisation:  24.458961486816406
maxi score, test score, baseline:  0.12983333333333316 0.6866666666666668 0.6866666666666668
printing an ep nov before normalisation:  39.60467763210032
printing an ep nov before normalisation:  21.53955706109384
actor:  1 policy actor:  1  step number:  59 total reward:  0.37333333333333274  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  67 total reward:  0.1466666666666665  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12983333333333316 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
Printing some Q and Qe and total Qs values:  [[0.72 ]
 [0.693]
 [0.72 ]
 [0.72 ]
 [0.72 ]
 [0.72 ]
 [0.72 ]] [[46.246]
 [49.638]
 [46.246]
 [46.246]
 [46.246]
 [46.246]
 [46.246]] [[1.743]
 [1.853]
 [1.743]
 [1.743]
 [1.743]
 [1.743]
 [1.743]]
maxi score, test score, baseline:  0.12983333333333316 0.6866666666666668 0.6866666666666668
Printing some Q and Qe and total Qs values:  [[0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.814]] [[38.07 ]
 [38.07 ]
 [38.07 ]
 [38.07 ]
 [38.07 ]
 [38.07 ]
 [35.227]] [[0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.814]]
printing an ep nov before normalisation:  35.75985299171347
printing an ep nov before normalisation:  41.71360492706299
actor:  0 policy actor:  1  step number:  41 total reward:  0.52  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  42 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  39.988276595053016
printing an ep nov before normalisation:  31.704911513434595
siam score:  -0.761291
Printing some Q and Qe and total Qs values:  [[0.874]
 [0.845]
 [0.813]
 [0.81 ]
 [0.858]
 [0.839]
 [0.856]] [[31.722]
 [31.669]
 [36.353]
 [37.385]
 [32.213]
 [32.837]
 [32.426]] [[0.874]
 [0.845]
 [0.813]
 [0.81 ]
 [0.858]
 [0.839]
 [0.856]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12748666666666647 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
actor:  1 policy actor:  1  step number:  49 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  1.0
using another actor
printing an ep nov before normalisation:  24.768829108386097
printing an ep nov before normalisation:  18.511565944344053
printing an ep nov before normalisation:  35.48030899442215
maxi score, test score, baseline:  0.12748666666666647 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
actor:  0 policy actor:  1  step number:  50 total reward:  0.2999999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.13008666666666646 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.13008666666666646 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
maxi score, test score, baseline:  0.13008666666666646 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.13008666666666646 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
actor:  0 policy actor:  0  step number:  52 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  38.686462694048906
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12996666666666648 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
Printing some Q and Qe and total Qs values:  [[-0.   ]
 [ 0.085]
 [ 0.037]
 [ 0.063]
 [-0.049]
 [-0.09 ]
 [-0.006]] [[30.06 ]
 [32.485]
 [31.918]
 [31.06 ]
 [21.905]
 [21.432]
 [33.208]] [[0.981]
 [1.146]
 [1.079]
 [1.078]
 [0.664]
 [0.608]
 [1.079]]
Printing some Q and Qe and total Qs values:  [[0.682]
 [0.739]
 [0.715]
 [0.708]
 [0.682]
 [0.677]
 [0.676]] [[34.606]
 [35.38 ]
 [35.19 ]
 [35.008]
 [35.374]
 [35.123]
 [34.258]] [[1.585]
 [1.683]
 [1.649]
 [1.633]
 [1.626]
 [1.608]
 [1.56 ]]
Printing some Q and Qe and total Qs values:  [[0.143]
 [0.143]
 [0.143]
 [0.143]
 [0.143]
 [0.143]
 [0.143]] [[32.879]
 [32.879]
 [32.879]
 [32.879]
 [32.879]
 [32.879]
 [32.879]] [[0.81]
 [0.81]
 [0.81]
 [0.81]
 [0.81]
 [0.81]
 [0.81]]
printing an ep nov before normalisation:  23.81910764959189
actor:  0 policy actor:  1  step number:  48 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.13047333333333316 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  27.00172800633722
Printing some Q and Qe and total Qs values:  [[-0.006]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[40.437]
 [43.661]
 [43.661]
 [43.661]
 [43.661]
 [43.661]
 [43.661]] [[1.21 ]
 [1.328]
 [1.328]
 [1.328]
 [1.328]
 [1.328]
 [1.328]]
printing an ep nov before normalisation:  32.722560241765294
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.13047333333333316 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
actor:  1 policy actor:  1  step number:  51 total reward:  0.33333333333333326  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  27.244389797102855
maxi score, test score, baseline:  0.13047333333333316 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
actor:  1 policy actor:  1  step number:  51 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.432003555299737
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  67 total reward:  0.11999999999999933  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
siam score:  -0.76456785
maxi score, test score, baseline:  0.13047333333333316 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
actor:  0 policy actor:  0  step number:  41 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.13337999999999983 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.13337999999999983 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
actor:  1 policy actor:  1  step number:  58 total reward:  0.4066666666666663  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.13337999999999983 0.6866666666666668 0.6866666666666668
printing an ep nov before normalisation:  47.941846659717534
printing an ep nov before normalisation:  36.21703863143921
actions average: 
K:  2  action  0 :  tensor([0.5410, 0.0594, 0.0542, 0.0660, 0.1531, 0.0648, 0.0616],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0017,     0.9909,     0.0006,     0.0014,     0.0010,     0.0010,
            0.0033], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1136, 0.0250, 0.4501, 0.0877, 0.0889, 0.1148, 0.1200],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1491, 0.1342, 0.1113, 0.1763, 0.1559, 0.1478, 0.1254],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.1358,     0.0006,     0.0273,     0.0447,     0.7080,     0.0397,
            0.0440], grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1210, 0.0048, 0.1801, 0.0951, 0.1126, 0.3707, 0.1158],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1641, 0.0926, 0.1078, 0.1262, 0.1294, 0.1262, 0.2537],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.13337999999999983 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
Printing some Q and Qe and total Qs values:  [[0.325]
 [0.405]
 [0.325]
 [0.325]
 [0.325]
 [0.325]
 [0.325]] [[45.748]
 [44.907]
 [45.748]
 [45.748]
 [45.748]
 [45.748]
 [45.748]] [[1.744]
 [1.78 ]
 [1.744]
 [1.744]
 [1.744]
 [1.744]
 [1.744]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.771]
 [0.82 ]
 [0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]] [[34.034]
 [30.557]
 [34.034]
 [34.034]
 [34.034]
 [34.034]
 [34.034]] [[0.771]
 [0.82 ]
 [0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]]
siam score:  -0.76800174
printing an ep nov before normalisation:  35.37147045135498
printing an ep nov before normalisation:  36.16199936431151
printing an ep nov before normalisation:  28.674541079788067
using explorer policy with actor:  1
maxi score, test score, baseline:  0.13089999999999982 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  37.20486402511597
siam score:  -0.7703664
maxi score, test score, baseline:  0.13089999999999982 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  23.92305868764666
maxi score, test score, baseline:  0.13089999999999982 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  38.595223000934126
printing an ep nov before normalisation:  34.62307123293702
maxi score, test score, baseline:  0.13089999999999982 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
actor:  1 policy actor:  1  step number:  53 total reward:  0.45333333333333337  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.13089999999999982 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  32.6396107673645
printing an ep nov before normalisation:  62.790737128467796
printing an ep nov before normalisation:  37.18586071977549
Printing some Q and Qe and total Qs values:  [[0.104]
 [0.664]
 [0.311]
 [0.414]
 [0.508]
 [0.249]
 [0.53 ]] [[42.927]
 [38.823]
 [44.102]
 [36.74 ]
 [43.238]
 [37.683]
 [38.488]] [[0.677]
 [1.129]
 [0.915]
 [0.824]
 [1.089]
 [0.684]
 [0.986]]
maxi score, test score, baseline:  0.13089999999999982 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
actor:  1 policy actor:  1  step number:  35 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.13089999999999982 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
maxi score, test score, baseline:  0.13089999999999982 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.13089999999999982 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([0.5187, 0.0060, 0.0797, 0.0794, 0.1252, 0.0963, 0.0947],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0092, 0.9597, 0.0051, 0.0046, 0.0022, 0.0028, 0.0166],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1380, 0.0413, 0.3892, 0.1010, 0.0926, 0.1261, 0.1117],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1256, 0.0477, 0.0887, 0.3725, 0.1312, 0.1226, 0.1117],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0674,     0.0019,     0.0003,     0.0064,     0.9206,     0.0022,
            0.0011], grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0840, 0.0017, 0.1566, 0.0783, 0.0723, 0.5040, 0.1032],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1727, 0.0234, 0.1072, 0.1293, 0.1025, 0.1220, 0.3431],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  58 total reward:  0.13999999999999946  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  29.13196571678257
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.463]
 [0.452]
 [0.463]
 [0.482]
 [0.359]
 [0.05 ]] [[32.741]
 [42.442]
 [35.065]
 [42.442]
 [29.297]
 [29.868]
 [34.23 ]] [[1.435]
 [1.796]
 [1.449]
 [1.796]
 [1.216]
 [1.119]
 [1.009]]
maxi score, test score, baseline:  0.13089999999999982 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
actor:  1 policy actor:  1  step number:  79 total reward:  0.03999999999999915  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  3  action  0 :  tensor([0.4491, 0.0019, 0.0890, 0.1120, 0.1180, 0.1121, 0.1178],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0163, 0.9145, 0.0115, 0.0095, 0.0080, 0.0085, 0.0316],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1264, 0.0071, 0.4474, 0.1048, 0.0910, 0.1000, 0.1232],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1793, 0.0462, 0.1050, 0.3963, 0.1008, 0.0744, 0.0979],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1397, 0.0007, 0.0551, 0.0712, 0.5711, 0.0737, 0.0885],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1521, 0.0300, 0.1035, 0.1062, 0.1011, 0.4180, 0.0890],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1317, 0.1724, 0.1073, 0.1468, 0.1457, 0.1264, 0.1697],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.13089999999999982 0.6866666666666668 0.6866666666666668
actor:  1 policy actor:  1  step number:  66 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 31.24616606797774
UNIT TEST: sample policy line 217 mcts : [0.041 0.592 0.061 0.041 0.02  0.204 0.041]
printing an ep nov before normalisation:  35.010610489939715
actor:  1 policy actor:  1  step number:  62 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  28.38507093939912
Printing some Q and Qe and total Qs values:  [[0.175]
 [0.067]
 [0.067]
 [0.067]
 [0.176]
 [0.067]
 [0.067]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.175]
 [0.067]
 [0.067]
 [0.067]
 [0.176]
 [0.067]
 [0.067]]
Printing some Q and Qe and total Qs values:  [[0.849]
 [0.883]
 [0.849]
 [0.849]
 [0.849]
 [0.849]
 [0.849]] [[36.551]
 [40.232]
 [36.551]
 [36.551]
 [36.551]
 [36.551]
 [36.551]] [[0.849]
 [0.883]
 [0.849]
 [0.849]
 [0.849]
 [0.849]
 [0.849]]
printing an ep nov before normalisation:  29.578519832399152
printing an ep nov before normalisation:  37.13837282314693
printing an ep nov before normalisation:  43.80600672771797
actor:  1 policy actor:  1  step number:  41 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  59 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  53 total reward:  0.6000000000000003  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  34.129125382469525
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  49 total reward:  0.40000000000000013  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  16.264009402610107
actor:  1 policy actor:  1  step number:  72 total reward:  0.01999999999999935  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1288599999999998 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.1288599999999998 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.1288599999999998 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
Printing some Q and Qe and total Qs values:  [[0.709]
 [0.765]
 [0.702]
 [0.708]
 [0.706]
 [0.705]
 [0.709]] [[25.121]
 [23.312]
 [23.628]
 [23.864]
 [23.757]
 [24.328]
 [24.278]] [[1.932]
 [1.901]
 [1.853]
 [1.87 ]
 [1.863]
 [1.89 ]
 [1.892]]
printing an ep nov before normalisation:  31.494879120293824
actor:  1 policy actor:  1  step number:  43 total reward:  0.52  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1288599999999998 0.6866666666666668 0.6866666666666668
actor:  0 policy actor:  1  step number:  45 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1286466666666665 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  39.30158086251575
maxi score, test score, baseline:  0.1286466666666665 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
maxi score, test score, baseline:  0.1286466666666665 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.1286466666666665 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  32.13213443131051
printing an ep nov before normalisation:  46.23003109385469
maxi score, test score, baseline:  0.1286466666666665 0.6866666666666668 0.6866666666666668
siam score:  -0.76993424
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.00522903769392
Printing some Q and Qe and total Qs values:  [[0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.749]] [[32.278]
 [32.278]
 [32.278]
 [32.278]
 [32.278]
 [32.278]
 [32.278]] [[0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.749]]
maxi score, test score, baseline:  0.1286466666666665 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
siam score:  -0.76917005
printing an ep nov before normalisation:  28.682634778321802
siam score:  -0.76974815
maxi score, test score, baseline:  0.1286466666666665 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
Printing some Q and Qe and total Qs values:  [[0.665]
 [0.665]
 [0.665]
 [0.666]
 [0.666]
 [0.666]
 [0.666]] [[4.878]
 [6.238]
 [6.035]
 [6.93 ]
 [4.324]
 [5.935]
 [4.039]] [[0.821]
 [0.865]
 [0.859]
 [0.888]
 [0.805]
 [0.857]
 [0.796]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  2  action  0 :  tensor([0.6025, 0.0147, 0.0456, 0.0584, 0.1068, 0.0850, 0.0871],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0206, 0.9199, 0.0067, 0.0051, 0.0027, 0.0034, 0.0416],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1006, 0.0546, 0.4801, 0.0804, 0.0693, 0.1202, 0.0948],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1362, 0.0559, 0.0795, 0.3128, 0.1254, 0.1284, 0.1618],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1794, 0.0095, 0.0791, 0.1259, 0.3421, 0.1302, 0.1339],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2002, 0.0203, 0.0999, 0.1125, 0.1294, 0.2889, 0.1489],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1409, 0.0653, 0.1623, 0.0989, 0.1364, 0.1265, 0.2696],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  22.992054797038296
actor:  0 policy actor:  0  step number:  48 total reward:  0.3399999999999995  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  66 total reward:  0.0733333333333327  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12887333333333315 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
actor:  0 policy actor:  0  step number:  47 total reward:  0.3066666666666664  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  40.65558525165003
printing an ep nov before normalisation:  70.67569320950197
maxi score, test score, baseline:  0.12887333333333315 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.12887333333333315 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
maxi score, test score, baseline:  0.12887333333333315 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  35.83141940544911
printing an ep nov before normalisation:  46.22542844441276
printing an ep nov before normalisation:  41.26408576965332
maxi score, test score, baseline:  0.12887333333333315 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  38.39834323626522
Printing some Q and Qe and total Qs values:  [[0.82 ]
 [0.865]
 [0.82 ]
 [0.82 ]
 [0.82 ]
 [0.82 ]
 [0.82 ]] [[36.088]
 [39.132]
 [36.088]
 [36.088]
 [36.088]
 [36.088]
 [36.088]] [[0.82 ]
 [0.865]
 [0.82 ]
 [0.82 ]
 [0.82 ]
 [0.82 ]
 [0.82 ]]
maxi score, test score, baseline:  0.12887333333333315 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  29.4303800272437
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.63198471069336
actor:  1 policy actor:  1  step number:  57 total reward:  0.4800000000000001  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]] [[18.462]
 [18.462]
 [18.462]
 [18.462]
 [18.462]
 [18.462]
 [18.462]] [[2.189]
 [2.189]
 [2.189]
 [2.189]
 [2.189]
 [2.189]
 [2.189]]
actor:  0 policy actor:  0  step number:  55 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  60 total reward:  0.19333333333333302  reward:  1.0 rdn_beta:  1.333
using another actor
actor:  1 policy actor:  1  step number:  68 total reward:  0.20666666666666633  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1291266666666665 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
maxi score, test score, baseline:  0.1291266666666665 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  41.50450071045501
Printing some Q and Qe and total Qs values:  [[0.872]
 [0.96 ]
 [0.916]
 [0.914]
 [0.92 ]
 [0.914]
 [0.872]] [[30.612]
 [33.07 ]
 [30.541]
 [31.302]
 [31.493]
 [30.308]
 [30.612]] [[0.872]
 [0.96 ]
 [0.916]
 [0.914]
 [0.92 ]
 [0.914]
 [0.872]]
maxi score, test score, baseline:  0.1291266666666665 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
actor:  0 policy actor:  1  step number:  54 total reward:  0.28666666666666607  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  34.60599660873413
maxi score, test score, baseline:  0.13169999999999982 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  45.67612306931622
maxi score, test score, baseline:  0.13169999999999982 0.6866666666666668 0.6866666666666668
printing an ep nov before normalisation:  43.2541828057467
maxi score, test score, baseline:  0.13169999999999982 0.6866666666666668 0.6866666666666668
printing an ep nov before normalisation:  48.175662306171695
actor:  1 policy actor:  1  step number:  50 total reward:  0.5000000000000003  reward:  1.0 rdn_beta:  1.333
from probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  28.233435562184788
actor:  1 policy actor:  1  step number:  63 total reward:  0.4666666666666669  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.13169999999999982 0.6866666666666668 0.6866666666666668
actor:  1 policy actor:  1  step number:  66 total reward:  0.3133333333333336  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[ 0.004]
 [ 0.101]
 [ 0.039]
 [-0.005]
 [-0.006]
 [ 0.039]
 [ 0.018]] [[40.101]
 [42.327]
 [ 0.   ]
 [41.608]
 [41.7  ]
 [ 0.   ]
 [39.059]] [[ 0.223]
 [ 0.34 ]
 [-0.1  ]
 [ 0.228]
 [ 0.228]
 [-0.1  ]
 [ 0.228]]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.447]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]] [[25.026]
 [28.312]
 [25.026]
 [25.026]
 [25.026]
 [25.026]
 [25.026]] [[1.382]
 [1.513]
 [1.382]
 [1.382]
 [1.382]
 [1.382]
 [1.382]]
printing an ep nov before normalisation:  41.0332090446598
Printing some Q and Qe and total Qs values:  [[0.323]
 [0.458]
 [0.323]
 [0.323]
 [0.323]
 [0.323]
 [0.323]] [[42.788]
 [50.692]
 [42.788]
 [42.788]
 [42.788]
 [42.788]
 [42.788]] [[0.544]
 [0.759]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  40.09291774295507
siam score:  -0.7833315
actor:  1 policy actor:  1  step number:  53 total reward:  0.4933333333333335  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  28.63654851913452
printing an ep nov before normalisation:  28.297058942893763
maxi score, test score, baseline:  0.13169999999999982 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
maxi score, test score, baseline:  0.13169999999999982 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
actor:  0 policy actor:  0  step number:  44 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  52 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  30.09049445316603
printing an ep nov before normalisation:  46.31243153244368
printing an ep nov before normalisation:  36.39806270599365
printing an ep nov before normalisation:  21.453542709350586
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.043]
 [0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]] [[21.72 ]
 [38.731]
 [21.72 ]
 [21.72 ]
 [21.72 ]
 [21.72 ]
 [21.72 ]] [[0.362]
 [0.918]
 [0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]]
maxi score, test score, baseline:  0.13460666666666649 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.13460666666666649 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
actor:  1 policy actor:  1  step number:  61 total reward:  0.27999999999999936  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  1  action  0 :  tensor([0.4308, 0.0270, 0.0829, 0.1093, 0.1583, 0.0943, 0.0973],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0038,     0.9850,     0.0016,     0.0023,     0.0007,     0.0009,
            0.0057], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1433, 0.0201, 0.4930, 0.0603, 0.0760, 0.1071, 0.1002],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1193, 0.4681, 0.0500, 0.0715, 0.0790, 0.1056, 0.1066],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1868, 0.0049, 0.0793, 0.0997, 0.4385, 0.1024, 0.0884],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1294, 0.0010, 0.0984, 0.0935, 0.0874, 0.5065, 0.0837],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1489, 0.0890, 0.0839, 0.1022, 0.0951, 0.0702, 0.4107],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  36.26868396377387
printing an ep nov before normalisation:  47.160373348743384
maxi score, test score, baseline:  0.13460666666666649 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
maxi score, test score, baseline:  0.13460666666666649 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  21.40017876709932
printing an ep nov before normalisation:  26.94285727382039
maxi score, test score, baseline:  0.13460666666666649 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
maxi score, test score, baseline:  0.13460666666666649 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.13460666666666649 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
maxi score, test score, baseline:  0.13460666666666649 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
actor:  1 policy actor:  1  step number:  77 total reward:  0.3466666666666659  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.551]
 [0.586]
 [0.559]
 [0.566]
 [0.567]
 [0.568]
 [0.562]] [[25.407]
 [23.894]
 [25.575]
 [25.038]
 [25.054]
 [25.068]
 [25.759]] [[1.409]
 [1.338]
 [1.43 ]
 [1.398]
 [1.401]
 [1.402]
 [1.446]]
printing an ep nov before normalisation:  20.667274292666484
actor:  1 policy actor:  1  step number:  65 total reward:  0.27999999999999936  reward:  1.0 rdn_beta:  1.0
using another actor
maxi score, test score, baseline:  0.13460666666666649 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.13460666666666649 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  33.62506063770718
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.13460666666666649 0.6866666666666668 0.6866666666666668
siam score:  -0.7755113
printing an ep nov before normalisation:  47.20859586230923
printing an ep nov before normalisation:  43.79493486547212
maxi score, test score, baseline:  0.13460666666666649 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.576]
 [0.546]
 [0.563]
 [0.581]
 [0.546]
 [0.546]] [[34.145]
 [34.743]
 [34.145]
 [34.124]
 [35.036]
 [34.145]
 [34.145]] [[1.894]
 [1.972]
 [1.894]
 [1.909]
 [2.002]
 [1.894]
 [1.894]]
printing an ep nov before normalisation:  54.48920022934638
printing an ep nov before normalisation:  31.320574331118422
maxi score, test score, baseline:  0.13460666666666649 0.6866666666666668 0.6866666666666668
printing an ep nov before normalisation:  0.020878391980261313
actions average: 
K:  3  action  0 :  tensor([0.5799, 0.0250, 0.0663, 0.0691, 0.0951, 0.0772, 0.0874],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0099, 0.9476, 0.0056, 0.0093, 0.0024, 0.0047, 0.0204],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1708, 0.0593, 0.2914, 0.1163, 0.1053, 0.1477, 0.1092],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1671, 0.0807, 0.1276, 0.2422, 0.1168, 0.1171, 0.1485],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2669, 0.0863, 0.0468, 0.0516, 0.4278, 0.0661, 0.0545],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1200, 0.1358, 0.1311, 0.0820, 0.1290, 0.3226, 0.0796],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1483, 0.0137, 0.1189, 0.1402, 0.0977, 0.1184, 0.3627],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[ 0.06 ]
 [ 0.103]
 [ 0.036]
 [ 0.036]
 [-0.002]
 [ 0.03 ]
 [ 0.042]] [[44.1  ]
 [42.613]
 [39.953]
 [39.953]
 [25.056]
 [27.235]
 [37.907]] [[1.21 ]
 [1.183]
 [0.99 ]
 [0.99 ]
 [0.248]
 [0.383]
 [0.899]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.3666666666666666  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  45.68208039591766
actor:  0 policy actor:  1  step number:  55 total reward:  0.27999999999999936  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1371666666666665 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  7.616529652446502
printing an ep nov before normalisation:  32.772125075913955
maxi score, test score, baseline:  0.1371666666666665 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.1371666666666665 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  54 total reward:  0.36666666666666614  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  54 total reward:  0.5400000000000005  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  46.71957340455043
printing an ep nov before normalisation:  44.07814237806262
printing an ep nov before normalisation:  52.55444492109819
Printing some Q and Qe and total Qs values:  [[0.891]
 [0.937]
 [0.956]
 [0.902]
 [0.891]
 [0.891]
 [0.778]] [[37.916]
 [34.301]
 [37.74 ]
 [32.244]
 [37.916]
 [37.916]
 [45.48 ]] [[0.891]
 [0.937]
 [0.956]
 [0.902]
 [0.891]
 [0.891]
 [0.778]]
maxi score, test score, baseline:  0.1371666666666665 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
actor:  0 policy actor:  1  step number:  43 total reward:  0.6000000000000002  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.13753999999999983 0.6866666666666668 0.6866666666666668
actor:  1 policy actor:  1  step number:  52 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  47.19392367391618
printing an ep nov before normalisation:  36.12948800579347
maxi score, test score, baseline:  0.13753999999999983 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  21.679728917505756
actor:  1 policy actor:  1  step number:  49 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  0.13753999999999983 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
actor:  0 policy actor:  0  step number:  50 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  30.941028594970703
maxi score, test score, baseline:  0.13743333333333313 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
Printing some Q and Qe and total Qs values:  [[0.528]
 [0.665]
 [0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]] [[28.998]
 [33.187]
 [28.998]
 [28.998]
 [28.998]
 [28.998]
 [28.998]] [[1.429]
 [1.84 ]
 [1.429]
 [1.429]
 [1.429]
 [1.429]
 [1.429]]
printing an ep nov before normalisation:  41.6850177944375
maxi score, test score, baseline:  0.13743333333333313 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  42.702516247518005
actor:  1 policy actor:  1  step number:  40 total reward:  0.6333333333333334  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  39.179359220227724
maxi score, test score, baseline:  0.13743333333333313 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
Printing some Q and Qe and total Qs values:  [[0.831]
 [0.893]
 [0.824]
 [0.831]
 [0.826]
 [0.822]
 [0.813]] [[36.807]
 [35.965]
 [36.452]
 [37.991]
 [37.693]
 [35.307]
 [33.378]] [[0.831]
 [0.893]
 [0.824]
 [0.831]
 [0.826]
 [0.822]
 [0.813]]
maxi score, test score, baseline:  0.13743333333333313 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
actor:  1 policy actor:  1  step number:  53 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  44 total reward:  0.43333333333333335  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  32.71109031346731
maxi score, test score, baseline:  0.1402999999999998 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
actor:  1 policy actor:  1  step number:  67 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.121]
 [0.031]
 [0.022]
 [0.022]
 [0.018]
 [0.024]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.018]
 [0.121]
 [0.031]
 [0.022]
 [0.022]
 [0.018]
 [0.024]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1402999999999998 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
maxi score, test score, baseline:  0.1402999999999998 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  56 total reward:  0.17999999999999938  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  52 total reward:  0.33999999999999997  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1402999999999998 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
maxi score, test score, baseline:  0.1402999999999998 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.27303054483431
siam score:  -0.7743274
printing an ep nov before normalisation:  29.302430417140236
actor:  1 policy actor:  1  step number:  56 total reward:  0.3666666666666666  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  36.760732950420035
maxi score, test score, baseline:  0.1402999999999998 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.1402999999999998 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  29.295591170019474
maxi score, test score, baseline:  0.1402999999999998 0.6866666666666668 0.6866666666666668
actor:  1 policy actor:  1  step number:  48 total reward:  0.4066666666666666  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  43 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  32.067375888041866
actor:  1 policy actor:  1  step number:  53 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  43.118324279785156
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.68408073011861
Printing some Q and Qe and total Qs values:  [[0.376]
 [0.482]
 [0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.376]] [[36.756]
 [36.431]
 [36.756]
 [36.756]
 [36.756]
 [36.756]
 [36.756]] [[1.189]
 [1.28 ]
 [1.189]
 [1.189]
 [1.189]
 [1.189]
 [1.189]]
printing an ep nov before normalisation:  41.98438878472612
maxi score, test score, baseline:  0.13725999999999985 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  35.69838047027588
maxi score, test score, baseline:  0.13725999999999985 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  38.63961125494035
printing an ep nov before normalisation:  34.36529757518283
actor:  0 policy actor:  0  step number:  61 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.667
siam score:  -0.77618486
maxi score, test score, baseline:  0.1352599999999998 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
maxi score, test score, baseline:  0.1352599999999998 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
actor:  1 policy actor:  1  step number:  48 total reward:  0.42000000000000004  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  41 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1352599999999998 0.6866666666666668 0.6866666666666668
printing an ep nov before normalisation:  54.61655837557841
Printing some Q and Qe and total Qs values:  [[-0.067]
 [ 0.345]
 [ 0.218]
 [ 0.218]
 [ 0.218]
 [ 0.218]
 [ 0.132]] [[49.505]
 [46.872]
 [45.117]
 [45.117]
 [45.117]
 [45.117]
 [49.954]] [[1.039]
 [1.341]
 [1.14 ]
 [1.14 ]
 [1.14 ]
 [1.14 ]
 [1.257]]
printing an ep nov before normalisation:  30.013413429260254
maxi score, test score, baseline:  0.1352599999999998 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  42.189426064822385
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.709]
 [0.766]
 [0.602]
 [0.669]
 [0.668]
 [0.704]
 [0.683]] [[27.131]
 [28.677]
 [29.362]
 [24.418]
 [25.27 ]
 [29.077]
 [27.306]] [[0.959]
 [1.04 ]
 [0.888]
 [0.874]
 [0.887]
 [0.985]
 [0.936]]
printing an ep nov before normalisation:  48.554277224735394
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.33173280391706
maxi score, test score, baseline:  0.1352599999999998 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  35.447959899902344
actions average: 
K:  3  action  0 :  tensor([0.2081, 0.0463, 0.1268, 0.1523, 0.2099, 0.1353, 0.1213],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0198,     0.9278,     0.0047,     0.0068,     0.0008,     0.0004,
            0.0397], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0644, 0.1188, 0.5083, 0.0980, 0.0697, 0.0769, 0.0640],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1893, 0.0750, 0.2910, 0.1010, 0.1056, 0.1149, 0.1232],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2183, 0.0114, 0.0697, 0.0595, 0.5008, 0.0809, 0.0595],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0819, 0.0078, 0.2406, 0.0768, 0.0514, 0.4802, 0.0612],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1166, 0.3329, 0.1058, 0.1099, 0.0929, 0.1061, 0.1358],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  60 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  63 total reward:  0.2933333333333328  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  1.4661067996672728
maxi score, test score, baseline:  0.1352599999999998 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
siam score:  -0.7698373
actor:  1 policy actor:  1  step number:  40 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  27.01777935028076
siam score:  -0.77082944
maxi score, test score, baseline:  0.1352599999999998 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  40.13054477112113
maxi score, test score, baseline:  0.1352599999999998 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  45.20244399598792
actor:  1 policy actor:  1  step number:  54 total reward:  0.4866666666666668  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1352599999999998 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.52807948817602
actor:  0 policy actor:  0  step number:  43 total reward:  0.37333333333333285  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  40 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  34.623552876212926
Printing some Q and Qe and total Qs values:  [[0.248]
 [0.248]
 [0.296]
 [0.248]
 [0.248]
 [0.248]
 [0.248]] [[34.126]
 [34.126]
 [43.528]
 [34.126]
 [34.126]
 [34.126]
 [34.126]] [[0.936]
 [0.936]
 [1.498]
 [0.936]
 [0.936]
 [0.936]
 [0.936]]
printing an ep nov before normalisation:  35.46177299893497
printing an ep nov before normalisation:  36.93009645433954
using explorer policy with actor:  1
maxi score, test score, baseline:  0.13307333333333315 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.13307333333333315 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3733333333333334  reward:  1.0 rdn_beta:  1.667
from probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.14698788910632
maxi score, test score, baseline:  0.13307333333333315 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
maxi score, test score, baseline:  0.13307333333333315 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
Printing some Q and Qe and total Qs values:  [[0.246]
 [0.246]
 [0.246]
 [0.246]
 [0.246]
 [0.17 ]
 [0.246]] [[34.961]
 [34.961]
 [34.961]
 [34.961]
 [34.961]
 [26.936]
 [34.961]] [[2.354]
 [2.354]
 [2.354]
 [2.354]
 [2.354]
 [1.503]
 [2.354]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.13307333333333315 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
using explorer policy with actor:  1
printing an ep nov before normalisation:  3.489552227620152e-05
maxi score, test score, baseline:  0.13307333333333315 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  25.27544075353229
actor:  1 policy actor:  1  step number:  51 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.726]
 [0.734]
 [0.726]
 [0.717]
 [0.726]
 [0.714]
 [0.726]] [[31.056]
 [30.782]
 [31.056]
 [28.226]
 [31.056]
 [27.307]
 [31.056]] [[0.726]
 [0.734]
 [0.726]
 [0.717]
 [0.726]
 [0.714]
 [0.726]]
maxi score, test score, baseline:  0.13307333333333315 0.6866666666666668 0.6866666666666668
actor:  0 policy actor:  1  step number:  69 total reward:  0.14666666666666628  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  48 total reward:  0.5000000000000003  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  58 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1350866666666665 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
maxi score, test score, baseline:  0.1350866666666665 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  50.365664720713
maxi score, test score, baseline:  0.1350866666666665 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  44.299830581283025
printing an ep nov before normalisation:  37.859874368628496
printing an ep nov before normalisation:  44.26750391783155
printing an ep nov before normalisation:  40.54043043816443
maxi score, test score, baseline:  0.1350866666666665 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
actor:  1 policy actor:  1  step number:  41 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.052]
 [0.076]
 [0.052]
 [0.01 ]
 [0.009]
 [0.006]
 [0.009]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.052]
 [0.076]
 [0.052]
 [0.01 ]
 [0.009]
 [0.006]
 [0.009]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  30.04386715584957
actor:  1 policy actor:  1  step number:  48 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1350866666666665 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
actor:  1 policy actor:  1  step number:  49 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1350866666666665 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  27.710968081341274
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]] [[51.415]
 [51.415]
 [51.415]
 [51.415]
 [51.415]
 [51.415]
 [51.415]] [[1.49]
 [1.49]
 [1.49]
 [1.49]
 [1.49]
 [1.49]
 [1.49]]
maxi score, test score, baseline:  0.1328466666666665 0.6866666666666668 0.6866666666666668
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.487]
 [0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]] [[36.848]
 [34.279]
 [36.848]
 [36.848]
 [36.848]
 [36.848]
 [36.848]] [[1.48 ]
 [1.437]
 [1.48 ]
 [1.48 ]
 [1.48 ]
 [1.48 ]
 [1.48 ]]
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]] [[36.669]
 [36.669]
 [36.669]
 [36.669]
 [36.669]
 [36.669]
 [36.669]] [[1.085]
 [1.085]
 [1.085]
 [1.085]
 [1.085]
 [1.085]
 [1.085]]
printing an ep nov before normalisation:  43.564242574702554
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1328466666666665 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  30.36558149245691
printing an ep nov before normalisation:  27.24660982543099
Printing some Q and Qe and total Qs values:  [[0.398]
 [0.509]
 [0.385]
 [0.398]
 [0.398]
 [0.398]
 [0.398]] [[52.831]
 [50.172]
 [50.125]
 [52.831]
 [52.831]
 [52.831]
 [52.831]] [[1.57 ]
 [1.582]
 [1.457]
 [1.57 ]
 [1.57 ]
 [1.57 ]
 [1.57 ]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.37333333333333274  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.034]
 [0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.021]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.021]
 [0.034]
 [0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.021]]
maxi score, test score, baseline:  0.1328466666666665 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
maxi score, test score, baseline:  0.1328466666666665 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
maxi score, test score, baseline:  0.1328466666666665 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
line 256 mcts: sample exp_bonus 42.9726461758871
printing an ep nov before normalisation:  45.50323455017484
printing an ep nov before normalisation:  46.7058929133398
actor:  1 policy actor:  1  step number:  55 total reward:  0.173333333333333  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  55 total reward:  0.4666666666666668  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  23.38702112007124
printing an ep nov before normalisation:  37.002910035763264
Starting evaluation
printing an ep nov before normalisation:  36.98604329473311
actor:  1 policy actor:  1  step number:  53 total reward:  0.30666666666666653  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1328466666666665 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
actor:  1 policy actor:  1  step number:  38 total reward:  0.54  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  43.54136156092592
printing an ep nov before normalisation:  59.764564471181295
Printing some Q and Qe and total Qs values:  [[0.756]
 [0.848]
 [0.758]
 [0.763]
 [0.75 ]
 [0.8  ]
 [0.818]] [[44.641]
 [48.548]
 [43.036]
 [45.121]
 [46.326]
 [47.656]
 [45.847]] [[0.756]
 [0.848]
 [0.758]
 [0.763]
 [0.75 ]
 [0.8  ]
 [0.818]]
maxi score, test score, baseline:  0.1328466666666665 0.6866666666666668 0.6866666666666668
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.706]
 [0.587]
 [0.593]
 [0.593]
 [0.593]
 [0.593]] [[56.775]
 [56.372]
 [54.277]
 [56.775]
 [56.775]
 [56.775]
 [56.775]] [[0.593]
 [0.706]
 [0.587]
 [0.593]
 [0.593]
 [0.593]
 [0.593]]
printing an ep nov before normalisation:  64.3115830849453
printing an ep nov before normalisation:  48.58943663593011
line 256 mcts: sample exp_bonus 41.74250839914134
Printing some Q and Qe and total Qs values:  [[0.925]
 [1.005]
 [0.925]
 [0.925]
 [0.925]
 [0.925]
 [0.925]] [[44.053]
 [41.586]
 [44.053]
 [44.053]
 [44.053]
 [44.053]
 [44.053]] [[0.925]
 [1.005]
 [0.925]
 [0.925]
 [0.925]
 [0.925]
 [0.925]]
printing an ep nov before normalisation:  39.88562982477997
printing an ep nov before normalisation:  38.25060244851594
Printing some Q and Qe and total Qs values:  [[0.839]
 [0.871]
 [0.84 ]
 [0.84 ]
 [0.846]
 [0.854]
 [0.85 ]] [[51.822]
 [40.276]
 [46.558]
 [46.558]
 [53.223]
 [56.315]
 [52.967]] [[0.839]
 [0.871]
 [0.84 ]
 [0.84 ]
 [0.846]
 [0.854]
 [0.85 ]]
maxi score, test score, baseline:  0.1328466666666665 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  60.32323808236116
printing an ep nov before normalisation:  44.87629885361827
maxi score, test score, baseline:  0.1328466666666665 0.6866666666666668 0.6866666666666668
printing an ep nov before normalisation:  32.1980775151478
printing an ep nov before normalisation:  39.53272062987372
printing an ep nov before normalisation:  31.951979519964535
line 256 mcts: sample exp_bonus 29.945722214965574
printing an ep nov before normalisation:  35.71744890645693
Printing some Q and Qe and total Qs values:  [[0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]] [[30.589]
 [30.589]
 [30.589]
 [30.589]
 [30.589]
 [30.589]
 [30.589]] [[0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]]
maxi score, test score, baseline:  0.1328466666666665 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
printing an ep nov before normalisation:  42.47729513858801
line 256 mcts: sample exp_bonus 36.45044447922894
printing an ep nov before normalisation:  28.964486122131348
maxi score, test score, baseline:  0.1328466666666665 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
actor:  0 policy actor:  0  step number:  25 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  8.717700268334738e-06
maxi score, test score, baseline:  0.13453999999999983 0.6866666666666668 0.6866666666666668
probs:  [0.09768739016376601, 0.09768739016376601, 0.09768739016376601, 0.51156304918117, 0.09768739016376601, 0.09768739016376601]
actor:  0 policy actor:  0  step number:  28 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  36.6015100479126
printing an ep nov before normalisation:  33.30812391726942
printing an ep nov before normalisation:  34.56982483550959
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.591]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]] [[31.075]
 [27.674]
 [31.075]
 [31.075]
 [31.075]
 [31.075]
 [31.075]] [[1.318]
 [1.24 ]
 [1.318]
 [1.318]
 [1.318]
 [1.318]
 [1.318]]
Printing some Q and Qe and total Qs values:  [[0.68 ]
 [0.641]
 [0.644]
 [0.628]
 [0.641]
 [0.61 ]
 [0.584]] [[27.319]
 [35.35 ]
 [33.168]
 [27.724]
 [29.651]
 [32.379]
 [30.409]] [[1.049]
 [1.282]
 [1.211]
 [1.011]
 [1.089]
 [1.151]
 [1.058]]
maxi score, test score, baseline:  0.1345266666666665 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.1345266666666665 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  0 policy actor:  1  step number:  51 total reward:  0.25333333333333274  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  27.209803942792405
printing an ep nov before normalisation:  30.858610352480802
printing an ep nov before normalisation:  27.610168214056685
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.697]
 [0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.628]] [[22.207]
 [25.313]
 [22.207]
 [22.207]
 [22.207]
 [22.207]
 [22.207]] [[1.591]
 [1.964]
 [1.591]
 [1.591]
 [1.591]
 [1.591]
 [1.591]]
actions average: 
K:  0  action  0 :  tensor([0.6120, 0.0363, 0.0503, 0.0572, 0.1072, 0.0581, 0.0788],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0041, 0.9605, 0.0038, 0.0044, 0.0010, 0.0015, 0.0248],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0503, 0.0618, 0.6406, 0.0561, 0.0443, 0.0988, 0.0482],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1354, 0.2280, 0.1000, 0.1237, 0.1499, 0.1088, 0.1541],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1276, 0.0014, 0.0751, 0.0660, 0.5641, 0.0914, 0.0744],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1521, 0.0027, 0.1676, 0.0834, 0.1314, 0.3572, 0.1056],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1258, 0.0513, 0.1348, 0.0751, 0.0589, 0.0469, 0.5072],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  55 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  48.37319850921631
actor:  1 policy actor:  1  step number:  62 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.13371333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  1 policy actor:  1  step number:  63 total reward:  0.013333333333332864  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.13371333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  47.649322733618035
maxi score, test score, baseline:  0.13371333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.47090530395508
siam score:  -0.76114655
line 256 mcts: sample exp_bonus 28.182764151538684
actor:  1 policy actor:  1  step number:  52 total reward:  0.3533333333333333  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  31.800419243873712
printing an ep nov before normalisation:  33.13371658325195
maxi score, test score, baseline:  0.13371333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  0 policy actor:  1  step number:  45 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  64.93437138702468
printing an ep nov before normalisation:  56.46737952925615
Printing some Q and Qe and total Qs values:  [[-0.043]
 [ 0.237]
 [-0.044]
 [ 0.163]
 [-0.156]
 [-0.001]
 [ 0.128]] [[31.14 ]
 [41.43 ]
 [33.452]
 [30.531]
 [32.677]
 [31.084]
 [30.483]] [[0.354]
 [0.966]
 [0.427]
 [0.54 ]
 [0.29 ]
 [0.394]
 [0.503]]
Printing some Q and Qe and total Qs values:  [[0.338]
 [0.326]
 [0.338]
 [0.338]
 [0.338]
 [0.281]
 [0.338]] [[42.853]
 [42.471]
 [42.853]
 [42.853]
 [42.853]
 [47.725]
 [42.853]] [[1.429]
 [1.399]
 [1.429]
 [1.429]
 [1.429]
 [1.615]
 [1.429]]
printing an ep nov before normalisation:  48.96111644394481
maxi score, test score, baseline:  0.13324666666666649 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
maxi score, test score, baseline:  0.13324666666666649 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  43.58936917483828
printing an ep nov before normalisation:  33.9996337890625
actor:  1 policy actor:  1  step number:  58 total reward:  0.17999999999999927  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  28.79944362747536
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  0.13324666666666649 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  43.50305954409093
maxi score, test score, baseline:  0.13324666666666649 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  4.735222773888381e-06
actor:  1 policy actor:  1  step number:  48 total reward:  0.43333333333333346  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.599]
 [0.557]
 [0.443]
 [0.448]
 [0.557]
 [0.557]] [[40.432]
 [38.068]
 [40.432]
 [41.96 ]
 [42.474]
 [40.432]
 [40.432]] [[1.97 ]
 [1.832]
 [1.97 ]
 [1.971]
 [2.015]
 [1.97 ]
 [1.97 ]]
actor:  1 policy actor:  1  step number:  62 total reward:  0.13999999999999935  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  65 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
actor:  1 policy actor:  1  step number:  57 total reward:  0.23999999999999966  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.13324666666666649 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.13324666666666649 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
maxi score, test score, baseline:  0.13324666666666649 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  44.97482196139613
maxi score, test score, baseline:  0.13324666666666649 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.13324666666666649 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
maxi score, test score, baseline:  0.13324666666666649 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  30.859396871032448
actor:  1 policy actor:  1  step number:  49 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.703]
 [0.603]
 [0.654]
 [0.654]
 [0.654]
 [0.654]] [[26.54 ]
 [31.004]
 [25.637]
 [29.113]
 [29.113]
 [29.113]
 [29.113]] [[1.302]
 [1.738]
 [1.265]
 [1.557]
 [1.557]
 [1.557]
 [1.557]]
maxi score, test score, baseline:  0.13324666666666649 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  40.201695340652726
printing an ep nov before normalisation:  0.0014675992645152292
actor:  1 policy actor:  1  step number:  41 total reward:  0.4933333333333333  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  29.744512813590188
printing an ep nov before normalisation:  38.27359676361084
printing an ep nov before normalisation:  25.221840201393015
actor:  1 policy actor:  1  step number:  49 total reward:  0.6133333333333336  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  54 total reward:  0.1933333333333328  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  25.1773951963449
actor:  1 policy actor:  1  step number:  66 total reward:  0.09999999999999898  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  40.733873706299846
printing an ep nov before normalisation:  36.852651183647396
actor:  1 policy actor:  1  step number:  40 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  56 total reward:  0.21999999999999964  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.13324666666666649 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  41.77000182015556
maxi score, test score, baseline:  0.13324666666666649 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
maxi score, test score, baseline:  0.13324666666666649 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  1 policy actor:  1  step number:  45 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.13324666666666649 0.6933333333333335 0.6933333333333335
printing an ep nov before normalisation:  39.013675941144825
printing an ep nov before normalisation:  51.700283579830504
printing an ep nov before normalisation:  30.513010382877777
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.055356384648093
using explorer policy with actor:  1
printing an ep nov before normalisation:  29.05003636094638
actor:  1 policy actor:  1  step number:  55 total reward:  0.22666666666666624  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.13324666666666649 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actions average: 
K:  1  action  0 :  tensor([0.5794, 0.0727, 0.0628, 0.0614, 0.0729, 0.0669, 0.0841],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0077, 0.9099, 0.0110, 0.0127, 0.0011, 0.0019, 0.0556],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1336, 0.0050, 0.5723, 0.0755, 0.0559, 0.0867, 0.0710],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1278, 0.0346, 0.1008, 0.3834, 0.1198, 0.1135, 0.1200],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2038, 0.0192, 0.0850, 0.0934, 0.3983, 0.1075, 0.0926],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1608, 0.0146, 0.1214, 0.0629, 0.0614, 0.4295, 0.1493],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1214, 0.1909, 0.0865, 0.0737, 0.0469, 0.0476, 0.4329],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  45.695269256199516
maxi score, test score, baseline:  0.13324666666666649 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
from probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  1 policy actor:  1  step number:  47 total reward:  0.34666666666666657  reward:  1.0 rdn_beta:  1.667
siam score:  -0.7652677
maxi score, test score, baseline:  0.13324666666666649 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
line 256 mcts: sample exp_bonus 37.49453248383764
printing an ep nov before normalisation:  8.79556353083899e-06
actor:  1 policy actor:  1  step number:  72 total reward:  0.12666666666666537  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  42 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.544]
 [0.456]
 [0.396]
 [0.468]
 [0.396]
 [0.396]] [[38.812]
 [38.9  ]
 [38.219]
 [31.932]
 [39.436]
 [31.932]
 [31.932]] [[1.716]
 [1.827]
 [1.695]
 [1.232]
 [1.784]
 [1.232]
 [1.232]]
maxi score, test score, baseline:  0.13345999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  30.00497672906122
printing an ep nov before normalisation:  34.20658722351568
actor:  1 policy actor:  1  step number:  53 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  40 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.365]
 [0.471]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]] [[51.045]
 [50.936]
 [51.045]
 [51.045]
 [51.045]
 [51.045]
 [51.045]] [[1.367]
 [1.469]
 [1.367]
 [1.367]
 [1.367]
 [1.367]
 [1.367]]
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.417]
 [0.455]
 [0.443]
 [0.277]
 [0.349]
 [0.422]] [[48.44 ]
 [46.783]
 [41.498]
 [46.661]
 [51.935]
 [45.458]
 [46.971]] [[1.121]
 [1.133]
 [0.978]
 [1.155]
 [1.181]
 [1.017]
 [1.145]]
Printing some Q and Qe and total Qs values:  [[0.34 ]
 [0.429]
 [0.382]
 [0.408]
 [0.34 ]
 [0.365]
 [0.415]] [[40.393]
 [41.524]
 [45.676]
 [42.134]
 [40.393]
 [47.975]
 [41.876]] [[1.056]
 [1.19 ]
 [1.309]
 [1.194]
 [1.056]
 [1.385]
 [1.19 ]]
maxi score, test score, baseline:  0.13345999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  0 policy actor:  1  step number:  61 total reward:  0.1999999999999994  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  3  action  0 :  tensor([0.5613, 0.0057, 0.0953, 0.0621, 0.1172, 0.0797, 0.0786],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0053, 0.9478, 0.0034, 0.0081, 0.0026, 0.0021, 0.0307],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1468, 0.1031, 0.1796, 0.1122, 0.1286, 0.1748, 0.1548],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1127, 0.2014, 0.0784, 0.3086, 0.1017, 0.0711, 0.1261],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2190, 0.0067, 0.0703, 0.0639, 0.4591, 0.1017, 0.0794],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1070, 0.0074, 0.1179, 0.0740, 0.0927, 0.5093, 0.0918],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1689, 0.2205, 0.0785, 0.0863, 0.1051, 0.0942, 0.2467],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.13312666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
maxi score, test score, baseline:  0.13312666666666648 0.6933333333333335 0.6933333333333335
actor:  1 policy actor:  1  step number:  48 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.13312666666666648 0.6933333333333335 0.6933333333333335
actor:  1 policy actor:  1  step number:  53 total reward:  0.5600000000000003  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.13312666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
maxi score, test score, baseline:  0.13312666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
maxi score, test score, baseline:  0.13312666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
siam score:  -0.76283246
maxi score, test score, baseline:  0.13312666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  1 policy actor:  1  step number:  55 total reward:  0.3333333333333328  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.13312666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
maxi score, test score, baseline:  0.13312666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
maxi score, test score, baseline:  0.13312666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
Printing some Q and Qe and total Qs values:  [[0.588]
 [0.644]
 [0.588]
 [0.588]
 [0.588]
 [0.588]
 [0.588]] [[38.003]
 [35.83 ]
 [38.003]
 [38.003]
 [38.003]
 [38.003]
 [38.003]] [[1.251]
 [1.241]
 [1.251]
 [1.251]
 [1.251]
 [1.251]
 [1.251]]
maxi score, test score, baseline:  0.13312666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  66 total reward:  0.15333333333333288  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  47.06557575442088
printing an ep nov before normalisation:  24.252490997314453
actor:  1 policy actor:  1  step number:  59 total reward:  0.2533333333333324  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  34.82089557623993
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.424]
 [0.38 ]
 [0.395]
 [0.383]
 [0.397]
 [0.4  ]] [[31.605]
 [33.364]
 [32.007]
 [31.242]
 [32.04 ]
 [31.637]
 [31.331]] [[1.239]
 [1.365]
 [1.252]
 [1.228]
 [1.256]
 [1.25 ]
 [1.238]]
maxi score, test score, baseline:  0.13312666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actions average: 
K:  0  action  0 :  tensor([0.4974, 0.0054, 0.0575, 0.0677, 0.2188, 0.0733, 0.0799],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0030,     0.9277,     0.0173,     0.0181,     0.0008,     0.0011,
            0.0320], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1083, 0.0403, 0.3162, 0.1228, 0.1050, 0.1733, 0.1341],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1643, 0.0812, 0.1178, 0.2185, 0.1182, 0.1470, 0.1531],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1264, 0.0008, 0.0467, 0.0611, 0.6211, 0.0713, 0.0725],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1549, 0.0209, 0.0961, 0.1316, 0.1076, 0.3765, 0.1125],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1396, 0.0943, 0.0880, 0.1268, 0.0964, 0.1210, 0.3339],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  41.71376514137176
actor:  1 policy actor:  1  step number:  55 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  26.715535364837873
maxi score, test score, baseline:  0.13312666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
from probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
maxi score, test score, baseline:  0.13312666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  1 policy actor:  1  step number:  40 total reward:  0.5  reward:  1.0 rdn_beta:  1.667
from probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  1 policy actor:  1  step number:  40 total reward:  0.5533333333333333  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  38.364343264027646
printing an ep nov before normalisation:  49.224910736083984
actor:  1 policy actor:  1  step number:  57 total reward:  0.17333333333333278  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.13068666666666648 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  33.9702768946756
printing an ep nov before normalisation:  41.20196835487866
printing an ep nov before normalisation:  36.39617646181806
actor:  0 policy actor:  1  step number:  39 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.13081999999999985 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
Printing some Q and Qe and total Qs values:  [[0.52 ]
 [0.639]
 [0.542]
 [0.555]
 [0.511]
 [0.55 ]
 [0.571]] [[31.437]
 [30.813]
 [33.604]
 [33.302]
 [36.797]
 [34.17 ]
 [30.18 ]] [[1.915]
 [1.991]
 [2.088]
 [2.08 ]
 [2.278]
 [2.134]
 [1.879]]
maxi score, test score, baseline:  0.13081999999999985 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
maxi score, test score, baseline:  0.13081999999999985 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  1 policy actor:  1  step number:  50 total reward:  0.5  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  52 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.13039333333333317 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
maxi score, test score, baseline:  0.13039333333333317 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
line 256 mcts: sample exp_bonus 38.60232304339186
maxi score, test score, baseline:  0.13039333333333317 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
maxi score, test score, baseline:  0.13039333333333317 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  1 policy actor:  1  step number:  63 total reward:  0.15999999999999936  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  51.779154661894694
maxi score, test score, baseline:  0.13039333333333317 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
maxi score, test score, baseline:  0.13039333333333317 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
maxi score, test score, baseline:  0.13039333333333317 0.6933333333333335 0.6933333333333335
actor:  0 policy actor:  0  step number:  63 total reward:  0.18666666666666598  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.12997999999999982 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.12997999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  1 policy actor:  1  step number:  60 total reward:  0.206666666666666  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  31.2966227918829
printing an ep nov before normalisation:  0.016628001064873388
maxi score, test score, baseline:  0.12997999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  49.74037183209426
printing an ep nov before normalisation:  42.10577917128018
actor:  1 policy actor:  1  step number:  62 total reward:  0.12666666666666615  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  55 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.489]
 [0.534]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]] [[31.849]
 [35.957]
 [31.849]
 [31.849]
 [31.849]
 [31.849]
 [31.849]] [[1.138]
 [1.351]
 [1.138]
 [1.138]
 [1.138]
 [1.138]
 [1.138]]
Printing some Q and Qe and total Qs values:  [[0.238]
 [0.181]
 [0.181]
 [0.181]
 [0.181]
 [0.181]
 [0.181]] [[42.355]
 [40.351]
 [40.351]
 [40.351]
 [40.351]
 [40.351]
 [40.351]] [[0.765]
 [0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]]
printing an ep nov before normalisation:  49.0436855751556
actor:  1 policy actor:  1  step number:  40 total reward:  0.5000000000000001  reward:  1.0 rdn_beta:  1.333
from probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
maxi score, test score, baseline:  0.12997999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]] [[31.845]
 [31.845]
 [31.845]
 [31.845]
 [31.845]
 [31.845]
 [31.845]] [[0.855]
 [0.855]
 [0.855]
 [0.855]
 [0.855]
 [0.855]
 [0.855]]
maxi score, test score, baseline:  0.12997999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
maxi score, test score, baseline:  0.12997999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  1 policy actor:  1  step number:  66 total reward:  0.03333333333333277  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  32 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1304999999999998 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [ 0.151]
 [-0.005]
 [-0.006]
 [-0.005]
 [-0.006]
 [-0.006]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.01 ]
 [ 0.151]
 [-0.005]
 [-0.006]
 [-0.005]
 [-0.006]
 [-0.006]]
printing an ep nov before normalisation:  35.23658525466555
actor:  1 policy actor:  1  step number:  67 total reward:  0.14666666666666617  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1304999999999998 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
UNIT TEST: sample policy line 217 mcts : [0.571 0.    0.    0.245 0.02  0.    0.163]
actor:  1 policy actor:  1  step number:  62 total reward:  0.16666666666666574  reward:  1.0 rdn_beta:  0.667
using another actor
maxi score, test score, baseline:  0.1304999999999998 0.6933333333333335 0.6933333333333335
line 256 mcts: sample exp_bonus 42.413228662561615
actor:  1 policy actor:  1  step number:  50 total reward:  0.3133333333333328  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 32.010658698579775
printing an ep nov before normalisation:  40.711939136798
printing an ep nov before normalisation:  40.77477476339684
printing an ep nov before normalisation:  41.67240103681333
Printing some Q and Qe and total Qs values:  [[0.804]
 [0.915]
 [0.804]
 [0.804]
 [0.804]
 [0.804]
 [0.815]] [[37.864]
 [33.88 ]
 [37.864]
 [37.864]
 [37.864]
 [37.864]
 [34.784]] [[0.804]
 [0.915]
 [0.804]
 [0.804]
 [0.804]
 [0.804]
 [0.815]]
siam score:  -0.76880735
maxi score, test score, baseline:  0.1304999999999998 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  1 policy actor:  1  step number:  55 total reward:  0.3066666666666661  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1304999999999998 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.1304999999999998 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  29.191644986470543
actor:  0 policy actor:  1  step number:  46 total reward:  0.5400000000000001  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1308466666666665 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  1 policy actor:  1  step number:  44 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  48.562117793259674
printing an ep nov before normalisation:  40.51289078317957
printing an ep nov before normalisation:  16.141698360443115
printing an ep nov before normalisation:  38.765704575800015
actor:  0 policy actor:  0  step number:  44 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1337399999999998 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  0 policy actor:  1  step number:  50 total reward:  0.3133333333333327  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  51 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.905]
 [0.905]
 [0.905]
 [0.905]
 [0.905]
 [0.905]
 [0.905]] [[35.486]
 [35.486]
 [35.486]
 [35.486]
 [35.486]
 [35.486]
 [35.486]] [[0.905]
 [0.905]
 [0.905]
 [0.905]
 [0.905]
 [0.905]
 [0.905]]
printing an ep nov before normalisation:  39.489665031433105
actor:  0 policy actor:  1  step number:  50 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.255]
 [0.377]
 [0.255]
 [0.255]
 [0.256]
 [0.255]
 [0.255]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.255]
 [0.377]
 [0.255]
 [0.255]
 [0.256]
 [0.255]
 [0.255]]
Printing some Q and Qe and total Qs values:  [[0.29]
 [0.29]
 [0.29]
 [0.29]
 [0.29]
 [0.29]
 [0.29]] [[38.572]
 [38.572]
 [38.572]
 [38.572]
 [38.572]
 [38.572]
 [38.572]] [[1.005]
 [1.005]
 [1.005]
 [1.005]
 [1.005]
 [1.005]
 [1.005]]
actor:  0 policy actor:  0  step number:  52 total reward:  0.2733333333333331  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.13615333333333315 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  1 policy actor:  1  step number:  59 total reward:  0.3466666666666661  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  47.218558816748214
actor:  1 policy actor:  1  step number:  53 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  29.447395006944348
maxi score, test score, baseline:  0.13615333333333315 0.6933333333333335 0.6933333333333335
printing an ep nov before normalisation:  22.988925509422117
printing an ep nov before normalisation:  20.63991538781103
actor:  1 policy actor:  1  step number:  67 total reward:  0.0933333333333326  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.548]
 [0.663]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]] [[31.403]
 [31.835]
 [31.403]
 [31.403]
 [31.403]
 [31.403]
 [31.403]] [[1.572]
 [1.716]
 [1.572]
 [1.572]
 [1.572]
 [1.572]
 [1.572]]
maxi score, test score, baseline:  0.13615333333333315 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
maxi score, test score, baseline:  0.13615333333333315 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  42.958476730145925
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  29.945282625319912
actor:  0 policy actor:  0  step number:  52 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  49 total reward:  0.4533333333333335  reward:  1.0 rdn_beta:  1.333
from probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  39.954298238109956
printing an ep nov before normalisation:  35.56752470042491
maxi score, test score, baseline:  0.13851333333333318 0.6933333333333335 0.6933333333333335
printing an ep nov before normalisation:  32.22302313536452
Printing some Q and Qe and total Qs values:  [[0.747]
 [0.747]
 [0.746]
 [0.747]
 [0.747]
 [0.744]
 [0.745]] [[3.854]
 [5.088]
 [3.908]
 [4.866]
 [3.845]
 [4.713]
 [4.791]] [[0.997]
 [1.077]
 [1.   ]
 [1.063]
 [0.997]
 [1.05 ]
 [1.056]]
actor:  1 policy actor:  1  step number:  40 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
siam score:  -0.76904005
maxi score, test score, baseline:  0.13851333333333318 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
maxi score, test score, baseline:  0.13851333333333318 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  0 policy actor:  1  step number:  39 total reward:  0.5466666666666666  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.236]
 [0.238]
 [0.218]
 [0.236]
 [0.236]
 [0.24 ]
 [0.236]] [[40.33 ]
 [44.879]
 [47.653]
 [40.33 ]
 [40.33 ]
 [48.118]
 [40.33 ]] [[0.678]
 [0.781]
 [0.824]
 [0.678]
 [0.678]
 [0.856]
 [0.678]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.3599999999999999  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  63 total reward:  0.013333333333332753  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  49 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  37.160360068873345
siam score:  -0.77288544
using another actor
Printing some Q and Qe and total Qs values:  [[ 0.077]
 [-0.01 ]
 [ 0.09 ]
 [-0.007]
 [ 0.044]
 [ 0.092]
 [-0.018]] [[47.317]
 [51.5  ]
 [41.376]
 [42.917]
 [49.807]
 [47.967]
 [42.01 ]] [[1.022]
 [1.112]
 [0.782]
 [0.75 ]
 [1.094]
 [1.064]
 [0.701]]
maxi score, test score, baseline:  0.1416066666666665 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.1416066666666665 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.1416066666666665 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
line 256 mcts: sample exp_bonus 49.910391617093104
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.837407781437996
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.928]
 [0.952]
 [0.901]
 [0.916]
 [0.898]
 [0.9  ]
 [0.858]] [[27.533]
 [34.932]
 [28.509]
 [27.545]
 [27.787]
 [30.332]
 [34.006]] [[0.928]
 [0.952]
 [0.901]
 [0.916]
 [0.898]
 [0.9  ]
 [0.858]]
printing an ep nov before normalisation:  30.144327990150853
printing an ep nov before normalisation:  38.458356825080386
printing an ep nov before normalisation:  41.07405486432831
line 256 mcts: sample exp_bonus 22.425990104675293
actor:  1 policy actor:  1  step number:  33 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.978]
 [1.001]
 [0.979]
 [0.977]
 [0.98 ]
 [0.978]
 [0.98 ]] [[18.293]
 [20.968]
 [18.234]
 [16.699]
 [18.334]
 [17.592]
 [18.318]] [[0.978]
 [1.001]
 [0.979]
 [0.977]
 [0.98 ]
 [0.978]
 [0.98 ]]
maxi score, test score, baseline:  0.1416066666666665 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  27.173460250292624
actor:  0 policy actor:  1  step number:  53 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  12.65332622004601
actor:  0 policy actor:  1  step number:  70 total reward:  0.1533333333333321  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  60 total reward:  0.13999999999999935  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  43 total reward:  0.5200000000000001  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 28.445531787139632
Printing some Q and Qe and total Qs values:  [[-0.002]
 [ 0.047]
 [-0.244]
 [ 0.021]
 [ 0.004]
 [-0.189]
 [ 0.029]] [[43.371]
 [39.08 ]
 [26.957]
 [29.35 ]
 [30.627]
 [24.16 ]
 [31.749]] [[0.999]
 [0.949]
 [0.378]
 [0.698]
 [0.711]
 [0.369]
 [0.762]]
maxi score, test score, baseline:  0.14155333333333317 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
maxi score, test score, baseline:  0.14155333333333317 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
maxi score, test score, baseline:  0.14155333333333317 0.6933333333333335 0.6933333333333335
Printing some Q and Qe and total Qs values:  [[0.159]
 [0.171]
 [0.163]
 [0.128]
 [0.161]
 [0.165]
 [0.164]] [[34.545]
 [35.793]
 [34.735]
 [34.076]
 [36.13 ]
 [36.351]
 [34.944]] [[1.37 ]
 [1.467]
 [1.388]
 [1.308]
 [1.481]
 [1.5  ]
 [1.403]]
printing an ep nov before normalisation:  33.05092150666656
maxi score, test score, baseline:  0.14155333333333317 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  32.70492708112718
Printing some Q and Qe and total Qs values:  [[0.416]
 [0.384]
 [0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.416]] [[22.885]
 [32.7  ]
 [22.885]
 [22.885]
 [22.885]
 [22.885]
 [22.885]] [[0.731]
 [0.94 ]
 [0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]]
siam score:  -0.77513564
maxi score, test score, baseline:  0.14155333333333317 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
Printing some Q and Qe and total Qs values:  [[0.892]
 [0.892]
 [0.892]
 [0.892]
 [0.892]
 [0.892]
 [0.892]] [[28.731]
 [28.731]
 [28.731]
 [28.731]
 [28.731]
 [28.731]
 [28.731]] [[0.892]
 [0.892]
 [0.892]
 [0.892]
 [0.892]
 [0.892]
 [0.892]]
actor:  1 policy actor:  1  step number:  40 total reward:  0.5000000000000001  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.14155333333333317 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
maxi score, test score, baseline:  0.14155333333333317 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  33.30349942029572
printing an ep nov before normalisation:  42.15371554706484
printing an ep nov before normalisation:  31.43669605255127
printing an ep nov before normalisation:  36.04488154791306
maxi score, test score, baseline:  0.14155333333333317 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.14155333333333317 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  1 policy actor:  1  step number:  60 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  0  action  0 :  tensor([0.4750, 0.0292, 0.0852, 0.0748, 0.1498, 0.0929, 0.0932],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0034,     0.9740,     0.0012,     0.0023,     0.0005,     0.0004,
            0.0183], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0647, 0.0833, 0.5373, 0.0320, 0.0328, 0.0510, 0.1989],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0973, 0.0140, 0.0718, 0.4464, 0.1747, 0.1072, 0.0886],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1952, 0.0031, 0.0670, 0.0706, 0.5084, 0.0775, 0.0783],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0688, 0.0032, 0.1874, 0.0442, 0.0476, 0.5943, 0.0544],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1860, 0.1433, 0.0986, 0.0809, 0.0821, 0.0921, 0.3169],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  47 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.14155333333333317 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.14155333333333317 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  0 policy actor:  1  step number:  48 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  44.14462803821445
printing an ep nov before normalisation:  30.201766470269373
Printing some Q and Qe and total Qs values:  [[0.793]
 [0.833]
 [0.816]
 [0.778]
 [0.816]
 [0.816]
 [0.79 ]] [[26.344]
 [27.613]
 [26.945]
 [25.561]
 [27.621]
 [27.224]
 [26.422]] [[0.793]
 [0.833]
 [0.816]
 [0.778]
 [0.816]
 [0.816]
 [0.79 ]]
actor:  0 policy actor:  0  step number:  50 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.646]
 [0.677]
 [0.32 ]
 [0.311]
 [0.308]
 [0.311]
 [0.328]] [[31.792]
 [32.11 ]
 [28.27 ]
 [29.621]
 [29.803]
 [29.382]
 [28.995]] [[0.878]
 [0.914]
 [0.506]
 [0.514]
 [0.514]
 [0.511]
 [0.523]]
maxi score, test score, baseline:  0.14693999999999982 0.6933333333333335 0.6933333333333335
printing an ep nov before normalisation:  41.75984583266798
Printing some Q and Qe and total Qs values:  [[0.2]
 [0.2]
 [0.2]
 [0.2]
 [0.2]
 [0.2]
 [0.2]] [[49.034]
 [49.034]
 [49.034]
 [49.034]
 [49.034]
 [49.034]
 [49.034]] [[2.191]
 [2.191]
 [2.191]
 [2.191]
 [2.191]
 [2.191]
 [2.191]]
maxi score, test score, baseline:  0.14693999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  30.348713852472898
Printing some Q and Qe and total Qs values:  [[-0.007]
 [-0.007]
 [ 0.032]
 [-0.007]
 [-0.007]
 [ 0.05 ]
 [-0.007]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.007]
 [-0.007]
 [ 0.032]
 [-0.007]
 [-0.007]
 [ 0.05 ]
 [-0.007]]
printing an ep nov before normalisation:  26.415610313415527
printing an ep nov before normalisation:  47.142664711816224
printing an ep nov before normalisation:  39.26222056533483
maxi score, test score, baseline:  0.14693999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  36.528854411759085
actor:  1 policy actor:  1  step number:  48 total reward:  0.35333333333333317  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.369828063561418
maxi score, test score, baseline:  0.14693999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
Printing some Q and Qe and total Qs values:  [[0.2  ]
 [0.328]
 [0.195]
 [0.199]
 [0.294]
 [0.199]
 [0.343]] [[29.788]
 [37.302]
 [28.875]
 [28.852]
 [31.057]
 [28.765]
 [34.046]] [[0.639]
 [1.014]
 [0.604]
 [0.608]
 [0.775]
 [0.605]
 [0.922]]
actor:  1 policy actor:  1  step number:  63 total reward:  0.27999999999999936  reward:  1.0 rdn_beta:  2.0
using another actor
actor:  0 policy actor:  0  step number:  48 total reward:  0.25999999999999945  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  34.93289289289937
maxi score, test score, baseline:  0.14679333333333314 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.41314891961053
line 256 mcts: sample exp_bonus 44.27784298215029
printing an ep nov before normalisation:  37.359308659149164
printing an ep nov before normalisation:  45.710803923439656
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.712]
 [0.649]
 [0.635]
 [0.624]
 [0.629]
 [0.633]] [[24.193]
 [28.328]
 [24.655]
 [24.384]
 [24.292]
 [24.096]
 [24.145]] [[1.23 ]
 [1.492]
 [1.25 ]
 [1.223]
 [1.208]
 [1.202]
 [1.209]]
maxi score, test score, baseline:  0.14679333333333314 0.6933333333333335 0.6933333333333335
Printing some Q and Qe and total Qs values:  [[0.032]
 [0.083]
 [0.032]
 [0.032]
 [0.032]
 [0.032]
 [0.032]] [[21.805]
 [32.601]
 [21.805]
 [21.805]
 [21.805]
 [21.805]
 [21.805]] [[0.362]
 [0.732]
 [0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.2666666666666664  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.14679333333333314 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.14679333333333314 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  1 policy actor:  1  step number:  61 total reward:  0.26666666666666616  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  31.96860286547798
printing an ep nov before normalisation:  38.36572221271962
printing an ep nov before normalisation:  34.96101238710365
maxi score, test score, baseline:  0.14679333333333314 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.14679333333333314 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  38.374690772695175
maxi score, test score, baseline:  0.14679333333333314 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  27.7465351301964
maxi score, test score, baseline:  0.14679333333333314 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  0.07304127344639255
actor:  0 policy actor:  0  step number:  38 total reward:  0.5  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  68 total reward:  0.19333333333333325  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.14395333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
Printing some Q and Qe and total Qs values:  [[0.754]
 [0.819]
 [0.759]
 [0.789]
 [0.759]
 [0.759]
 [0.759]] [[40.187]
 [39.117]
 [ 0.   ]
 [33.348]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.754]
 [0.819]
 [0.759]
 [0.789]
 [0.759]
 [0.759]
 [0.759]]
printing an ep nov before normalisation:  33.4953501779536
maxi score, test score, baseline:  0.14395333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  39.0479215587469
actor:  1 policy actor:  1  step number:  63 total reward:  0.49333333333333373  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.14395333333333316 0.6933333333333335 0.6933333333333335
using explorer policy with actor:  1
maxi score, test score, baseline:  0.14395333333333316 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  41.36590349051505
printing an ep nov before normalisation:  43.825415649536936
Printing some Q and Qe and total Qs values:  [[0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]] [[32.489]
 [32.489]
 [32.489]
 [32.489]
 [32.489]
 [32.489]
 [32.489]] [[0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]]
actor:  0 policy actor:  0  step number:  44 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  52 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.14407333333333314 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  1 policy actor:  1  step number:  39 total reward:  0.6266666666666668  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  0
maxi score, test score, baseline:  0.14407333333333314 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
Printing some Q and Qe and total Qs values:  [[0.848]
 [0.848]
 [0.848]
 [0.848]
 [0.848]
 [0.848]
 [0.848]] [[31.759]
 [31.759]
 [31.759]
 [31.759]
 [31.759]
 [31.759]
 [31.759]] [[0.848]
 [0.848]
 [0.848]
 [0.848]
 [0.848]
 [0.848]
 [0.848]]
printing an ep nov before normalisation:  29.884835849087846
maxi score, test score, baseline:  0.14407333333333314 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  1 policy actor:  1  step number:  60 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.14407333333333314 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.14407333333333314 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  38.78213152534001
actor:  0 policy actor:  1  step number:  44 total reward:  0.54  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  49 total reward:  0.3466666666666661  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  27.81325219652187
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.716]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]] [[23.023]
 [31.336]
 [23.023]
 [23.023]
 [23.023]
 [23.023]
 [23.023]] [[0.944]
 [1.214]
 [0.944]
 [0.944]
 [0.944]
 [0.944]
 [0.944]]
maxi score, test score, baseline:  0.14431333333333315 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  38.869151096227704
Printing some Q and Qe and total Qs values:  [[0.216]
 [0.268]
 [0.216]
 [0.139]
 [0.143]
 [0.136]
 [0.216]] [[30.448]
 [38.759]
 [30.448]
 [29.903]
 [29.701]
 [28.826]
 [30.448]] [[0.703]
 [1.018]
 [0.703]
 [0.608]
 [0.606]
 [0.571]
 [0.703]]
maxi score, test score, baseline:  0.14431333333333315 0.6933333333333335 0.6933333333333335
actor:  1 policy actor:  1  step number:  66 total reward:  0.086666666666666  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  52 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  0.667
from probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.593]
 [0.528]
 [0.566]
 [0.485]
 [0.528]
 [0.528]] [[28.597]
 [30.02 ]
 [35.309]
 [26.267]
 [28.843]
 [35.309]
 [35.309]] [[1.81 ]
 [1.926]
 [2.3  ]
 [1.588]
 [1.721]
 [2.3  ]
 [2.3  ]]
maxi score, test score, baseline:  0.14675333333333312 0.6933333333333335 0.6933333333333335
Printing some Q and Qe and total Qs values:  [[0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]] [[49.894]
 [49.894]
 [49.894]
 [49.894]
 [49.894]
 [49.894]
 [49.894]] [[0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]]
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.401]
 [0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]] [[34.296]
 [33.213]
 [34.296]
 [34.296]
 [34.296]
 [34.296]
 [34.296]] [[1.435]
 [1.441]
 [1.435]
 [1.435]
 [1.435]
 [1.435]
 [1.435]]
printing an ep nov before normalisation:  36.73182042056897
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.6  ]
 [0.677]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]] [[28.651]
 [24.576]
 [28.651]
 [28.651]
 [28.651]
 [28.651]
 [28.651]] [[1.54 ]
 [1.374]
 [1.54 ]
 [1.54 ]
 [1.54 ]
 [1.54 ]
 [1.54 ]]
using explorer policy with actor:  1
siam score:  -0.76861334
maxi score, test score, baseline:  0.14675333333333312 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  1 policy actor:  1  step number:  55 total reward:  0.4  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  37.49226580341041
line 256 mcts: sample exp_bonus 34.43988859471802
printing an ep nov before normalisation:  44.95574040601976
Printing some Q and Qe and total Qs values:  [[0.938]
 [0.985]
 [0.955]
 [0.929]
 [0.924]
 [0.927]
 [0.941]] [[23.693]
 [26.512]
 [22.462]
 [22.891]
 [23.902]
 [23.469]
 [23.796]] [[0.938]
 [0.985]
 [0.955]
 [0.929]
 [0.924]
 [0.927]
 [0.941]]
maxi score, test score, baseline:  0.14675333333333312 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  1 policy actor:  1  step number:  47 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  27.019194866421294
printing an ep nov before normalisation:  24.962492421861455
maxi score, test score, baseline:  0.14675333333333312 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  35.79907982022756
maxi score, test score, baseline:  0.14675333333333312 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  1 policy actor:  1  step number:  42 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.14675333333333312 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
siam score:  -0.7662774
printing an ep nov before normalisation:  35.83590891775918
printing an ep nov before normalisation:  26.925179958343506
line 256 mcts: sample exp_bonus 46.9054572262418
actor:  0 policy actor:  1  step number:  71 total reward:  0.1599999999999988  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  45 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  2.0
using another actor
maxi score, test score, baseline:  0.14933999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
using explorer policy with actor:  0
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.62143100399206
printing an ep nov before normalisation:  34.17811870574951
printing an ep nov before normalisation:  28.580780029296875
maxi score, test score, baseline:  0.14933999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  1 policy actor:  1  step number:  56 total reward:  0.16666666666666619  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  46.215172616122885
printing an ep nov before normalisation:  31.72918755168636
maxi score, test score, baseline:  0.14933999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  38.560117298367295
Printing some Q and Qe and total Qs values:  [[0.072]
 [0.192]
 [0.109]
 [0.039]
 [0.023]
 [0.054]
 [0.109]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.072]
 [0.192]
 [0.109]
 [0.039]
 [0.023]
 [0.054]
 [0.109]]
siam score:  -0.7669368
printing an ep nov before normalisation:  51.396780246884965
actor:  1 policy actor:  1  step number:  37 total reward:  0.6000000000000002  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.14713999999999983 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  32.74272522916994
printing an ep nov before normalisation:  32.192645604461546
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  47 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  43 total reward:  0.48  reward:  1.0 rdn_beta:  0.333
using another actor
Printing some Q and Qe and total Qs values:  [[0.863]
 [0.923]
 [0.81 ]
 [0.816]
 [0.824]
 [0.826]
 [0.819]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.863]
 [0.923]
 [0.81 ]
 [0.816]
 [0.824]
 [0.826]
 [0.819]]
printing an ep nov before normalisation:  55.805505108842944
maxi score, test score, baseline:  0.1473799999999998 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
maxi score, test score, baseline:  0.1473799999999998 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
siam score:  -0.7614
maxi score, test score, baseline:  0.1473799999999998 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  38.035681854607965
actor:  0 policy actor:  1  step number:  36 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  0
printing an ep nov before normalisation:  22.225846888263227
actor:  1 policy actor:  1  step number:  51 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  52.79962293460116
actor:  1 policy actor:  1  step number:  36 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.15043333333333314 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
maxi score, test score, baseline:  0.15043333333333314 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  46.69546272954932
maxi score, test score, baseline:  0.15043333333333314 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
maxi score, test score, baseline:  0.15043333333333314 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.15043333333333314 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  0 policy actor:  1  step number:  47 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  23.76453220455417
maxi score, test score, baseline:  0.15304666666666647 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  66 total reward:  0.2199999999999993  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.15304666666666647 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
siam score:  -0.7575571
maxi score, test score, baseline:  0.15304666666666647 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  44.061732006496996
printing an ep nov before normalisation:  37.538058498195376
maxi score, test score, baseline:  0.15304666666666647 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
maxi score, test score, baseline:  0.15304666666666647 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.15304666666666647 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  22.644705062775905
maxi score, test score, baseline:  0.15304666666666647 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
maxi score, test score, baseline:  0.15304666666666647 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  1 policy actor:  1  step number:  38 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  54 total reward:  0.44666666666666677  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  59 total reward:  0.27999999999999925  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
siam score:  -0.75972766
maxi score, test score, baseline:  0.15304666666666647 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.15304666666666647 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  33.0300090864067
printing an ep nov before normalisation:  54.990617007858404
maxi score, test score, baseline:  0.15304666666666647 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.15304666666666647 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.02677136227582
printing an ep nov before normalisation:  37.05040940518289
maxi score, test score, baseline:  0.15304666666666647 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.938657018058077
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.577]
 [0.538]
 [0.549]
 [0.612]
 [0.538]
 [0.517]] [[27.254]
 [35.366]
 [36.299]
 [27.927]
 [28.426]
 [36.299]
 [34.158]] [[0.812]
 [0.907]
 [0.885]
 [0.74 ]
 [0.812]
 [0.885]
 [0.824]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  51 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.15304666666666647 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.15304666666666647 0.6933333333333335 0.6933333333333335
actor:  1 policy actor:  1  step number:  61 total reward:  0.23999999999999977  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.68 ]
 [0.685]
 [0.734]
 [0.68 ]
 [0.68 ]
 [0.684]
 [0.68 ]] [[28.564]
 [32.911]
 [30.236]
 [28.564]
 [28.564]
 [31.184]
 [28.564]] [[1.148]
 [1.271]
 [1.248]
 [1.148]
 [1.148]
 [1.223]
 [1.148]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.15304666666666647 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  47.586795772754016
actor:  1 policy actor:  1  step number:  62 total reward:  0.17999999999999916  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  53.682362721287355
maxi score, test score, baseline:  0.15304666666666647 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  1 policy actor:  1  step number:  66 total reward:  0.2599999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  52.331080326877085
maxi score, test score, baseline:  0.15304666666666647 0.6933333333333335 0.6933333333333335
printing an ep nov before normalisation:  45.41938781738281
printing an ep nov before normalisation:  43.861470224118676
siam score:  -0.77026665
maxi score, test score, baseline:  0.15304666666666647 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  1 policy actor:  1  step number:  62 total reward:  0.12666666666666593  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.29 ]
 [0.403]
 [0.29 ]
 [0.29 ]
 [0.29 ]
 [0.29 ]
 [0.29 ]] [[44.32 ]
 [43.341]
 [44.32 ]
 [44.32 ]
 [44.32 ]
 [44.32 ]
 [44.32 ]] [[0.576]
 [0.676]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]]
line 256 mcts: sample exp_bonus 46.133698081254295
Printing some Q and Qe and total Qs values:  [[0.151]
 [0.152]
 [0.151]
 [0.151]
 [0.151]
 [0.151]
 [0.151]] [[41.213]
 [46.643]
 [41.213]
 [41.213]
 [41.213]
 [41.213]
 [41.213]] [[1.495]
 [1.825]
 [1.495]
 [1.495]
 [1.495]
 [1.495]
 [1.495]]
Printing some Q and Qe and total Qs values:  [[0.947]
 [0.981]
 [0.937]
 [0.939]
 [0.944]
 [0.928]
 [0.926]] [[31.457]
 [31.846]
 [33.552]
 [31.417]
 [34.305]
 [33.447]
 [33.509]] [[0.947]
 [0.981]
 [0.937]
 [0.939]
 [0.944]
 [0.928]
 [0.926]]
printing an ep nov before normalisation:  34.70118661481153
maxi score, test score, baseline:  0.15304666666666647 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  0 policy actor:  0  step number:  59 total reward:  0.23999999999999944  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  28.474257957814025
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.662]
 [0.674]
 [0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]] [[35.889]
 [30.126]
 [35.889]
 [35.889]
 [35.889]
 [35.889]
 [35.889]] [[0.927]
 [0.865]
 [0.927]
 [0.927]
 [0.927]
 [0.927]
 [0.927]]
printing an ep nov before normalisation:  36.20030851503276
UNIT TEST: sample policy line 217 mcts : [0.02  0.633 0.02  0.041 0.    0.245 0.041]
printing an ep nov before normalisation:  39.350534434320245
maxi score, test score, baseline:  0.15261999999999984 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  28.523146560939626
printing an ep nov before normalisation:  40.53913788842957
printing an ep nov before normalisation:  40.9871254740617
printing an ep nov before normalisation:  36.41266107559204
actor:  1 policy actor:  1  step number:  54 total reward:  0.4600000000000002  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  70 total reward:  0.04666666666666597  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  33.4064054772449
printing an ep nov before normalisation:  33.188109397888184
maxi score, test score, baseline:  0.15261999999999984 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  46.717285546849375
Printing some Q and Qe and total Qs values:  [[0.176]
 [0.231]
 [0.161]
 [0.161]
 [0.161]
 [0.161]
 [0.161]] [[41.624]
 [52.33 ]
 [55.584]
 [55.584]
 [55.584]
 [55.584]
 [55.584]] [[0.91 ]
 [1.425]
 [1.494]
 [1.494]
 [1.494]
 [1.494]
 [1.494]]
Printing some Q and Qe and total Qs values:  [[ 0.026]
 [ 0.026]
 [ 0.094]
 [ 0.039]
 [-0.075]
 [ 0.076]
 [ 0.036]] [[31.939]
 [31.939]
 [35.863]
 [27.459]
 [29.868]
 [31.278]
 [30.821]] [[0.791]
 [0.791]
 [1.061]
 [0.573]
 [0.584]
 [0.807]
 [0.744]]
Printing some Q and Qe and total Qs values:  [[0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]] [[36.669]
 [36.669]
 [36.669]
 [36.669]
 [36.669]
 [36.669]
 [36.669]] [[2.057]
 [2.057]
 [2.057]
 [2.057]
 [2.057]
 [2.057]
 [2.057]]
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.793]
 [0.639]
 [0.721]
 [0.721]
 [0.632]
 [0.643]] [[42.911]
 [45.211]
 [43.095]
 [37.142]
 [37.142]
 [43.463]
 [40.973]] [[0.635]
 [0.793]
 [0.639]
 [0.721]
 [0.721]
 [0.632]
 [0.643]]
maxi score, test score, baseline:  0.15261999999999984 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  1 policy actor:  1  step number:  54 total reward:  0.3933333333333334  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  78 total reward:  0.21999999999999897  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.229]
 [0.139]
 [0.139]
 [0.139]
 [0.252]
 [0.061]
 [0.148]] [[50.523]
 [48.054]
 [48.054]
 [48.054]
 [48.548]
 [39.022]
 [35.477]] [[1.021]
 [0.859]
 [0.859]
 [0.859]
 [0.987]
 [0.517]
 [0.501]]
Printing some Q and Qe and total Qs values:  [[0.174]
 [0.201]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]] [[55.439]
 [69.659]
 [55.439]
 [55.439]
 [55.439]
 [55.439]
 [55.439]] [[0.873]
 [1.305]
 [0.873]
 [0.873]
 [0.873]
 [0.873]
 [0.873]]
printing an ep nov before normalisation:  47.633651562719415
printing an ep nov before normalisation:  48.71825299818798
printing an ep nov before normalisation:  28.065221309661865
actor:  1 policy actor:  1  step number:  44 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  58.751386172311534
actions average: 
K:  1  action  0 :  tensor([0.4799, 0.0520, 0.0754, 0.0895, 0.0894, 0.1046, 0.1091],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0038, 0.9329, 0.0043, 0.0252, 0.0058, 0.0017, 0.0262],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0791, 0.0231, 0.6490, 0.0661, 0.0454, 0.0648, 0.0725],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1695, 0.0575, 0.1077, 0.2011, 0.1643, 0.1603, 0.1397],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1991, 0.0063, 0.0813, 0.1044, 0.3864, 0.1227, 0.0998],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1949, 0.0031, 0.1118, 0.1209, 0.1406, 0.3132, 0.1154],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1476, 0.0894, 0.0973, 0.1146, 0.1213, 0.1130, 0.3168],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.15261999999999984 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  48.761607751563545
printing an ep nov before normalisation:  44.85549449920654
maxi score, test score, baseline:  0.15261999999999984 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  1 policy actor:  1  step number:  40 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  44 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  61 total reward:  0.22666666666666602  reward:  1.0 rdn_beta:  2.0
from probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
Printing some Q and Qe and total Qs values:  [[-0.019]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.019]] [[58.96]
 [58.96]
 [58.96]
 [58.96]
 [58.96]
 [58.96]
 [58.96]] [[1.292]
 [1.292]
 [1.292]
 [1.292]
 [1.292]
 [1.292]
 [1.292]]
maxi score, test score, baseline:  0.15248666666666646 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.03417236206829
line 256 mcts: sample exp_bonus 62.34803359300363
maxi score, test score, baseline:  0.15248666666666646 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  45.67105003040369
Printing some Q and Qe and total Qs values:  [[-0.018]
 [ 0.042]
 [-0.017]
 [-0.017]
 [-0.018]
 [-0.019]
 [-0.018]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.018]
 [ 0.042]
 [-0.017]
 [-0.017]
 [-0.018]
 [-0.019]
 [-0.018]]
maxi score, test score, baseline:  0.15248666666666646 0.6933333333333335 0.6933333333333335
printing an ep nov before normalisation:  39.06868089589014
printing an ep nov before normalisation:  34.457011222839355
printing an ep nov before normalisation:  35.13655170271927
printing an ep nov before normalisation:  30.62458038330078
printing an ep nov before normalisation:  64.44264821424554
using explorer policy with actor:  1
maxi score, test score, baseline:  0.15248666666666646 0.6933333333333335 0.6933333333333335
actor:  1 policy actor:  1  step number:  51 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.252]
 [0.258]
 [0.242]
 [0.252]
 [0.252]
 [0.252]
 [0.252]] [[65.971]
 [58.207]
 [57.73 ]
 [65.971]
 [65.971]
 [65.971]
 [65.971]] [[1.56 ]
 [1.339]
 [1.308]
 [1.56 ]
 [1.56 ]
 [1.56 ]
 [1.56 ]]
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  0.15248666666666646 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  31.59550850671267
actions average: 
K:  4  action  0 :  tensor([0.4759, 0.0036, 0.0738, 0.0969, 0.1272, 0.0944, 0.1281],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0306, 0.8621, 0.0116, 0.0197, 0.0102, 0.0077, 0.0582],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1599, 0.0230, 0.3494, 0.1056, 0.1124, 0.1427, 0.1070],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1445, 0.1430, 0.0736, 0.2972, 0.0998, 0.1261, 0.1158],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2826, 0.0044, 0.1079, 0.1049, 0.2438, 0.1116, 0.1447],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2113, 0.0151, 0.0978, 0.1383, 0.1354, 0.2453, 0.1567],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2235, 0.0072, 0.1284, 0.1464, 0.1175, 0.1940, 0.1829],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.15248666666666646 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  55.06105269831911
printing an ep nov before normalisation:  32.86722183227539
actions average: 
K:  3  action  0 :  tensor([0.5100, 0.0058, 0.0776, 0.0875, 0.1047, 0.1006, 0.1138],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0380, 0.8627, 0.0095, 0.0178, 0.0096, 0.0099, 0.0525],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0496, 0.2227, 0.4107, 0.0788, 0.0760, 0.0876, 0.0746],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0974, 0.1944, 0.0323, 0.4027, 0.0370, 0.0606, 0.1755],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2837, 0.0057, 0.0715, 0.0923, 0.3741, 0.0848, 0.0879],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0993, 0.1590, 0.1706, 0.0680, 0.0692, 0.3266, 0.1074],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1674, 0.1899, 0.0805, 0.0961, 0.1832, 0.0811, 0.2017],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  45 total reward:  0.41333333333333333  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  41.90245188014596
actor:  1 policy actor:  1  step number:  73 total reward:  0.15999999999999903  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  0  action  0 :  tensor([0.4498, 0.0170, 0.0716, 0.0874, 0.1363, 0.0903, 0.1477],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0173, 0.9413, 0.0018, 0.0053, 0.0024, 0.0018, 0.0302],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0599, 0.0020, 0.7001, 0.0429, 0.0500, 0.0864, 0.0587],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1943, 0.0115, 0.0953, 0.2704, 0.1556, 0.1318, 0.1410],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1468, 0.0090, 0.0776, 0.1117, 0.4701, 0.0906, 0.0942],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1239, 0.0081, 0.0858, 0.0685, 0.0796, 0.5534, 0.0807],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1241, 0.1492, 0.1760, 0.0875, 0.1044, 0.1064, 0.2524],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  52.93104166224849
maxi score, test score, baseline:  0.15248666666666646 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  42.84810144547326
from probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
maxi score, test score, baseline:  0.15248666666666646 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
maxi score, test score, baseline:  0.15248666666666646 0.6933333333333335 0.6933333333333335
line 256 mcts: sample exp_bonus 52.61203346286014
printing an ep nov before normalisation:  0.004544522949316843
using explorer policy with actor:  1
maxi score, test score, baseline:  0.15248666666666646 0.6933333333333335 0.6933333333333335
maxi score, test score, baseline:  0.15248666666666646 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  23.16975723584885
printing an ep nov before normalisation:  0.005080525994571872
maxi score, test score, baseline:  0.15248666666666646 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
from probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  43.66122014353453
printing an ep nov before normalisation:  38.220944476299564
printing an ep nov before normalisation:  39.03980517421965
using explorer policy with actor:  0
actions average: 
K:  3  action  0 :  tensor([0.3384, 0.0057, 0.1103, 0.1080, 0.2197, 0.1043, 0.1137],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0067, 0.9418, 0.0033, 0.0231, 0.0013, 0.0011, 0.0227],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1132, 0.0080, 0.4705, 0.0981, 0.0966, 0.1162, 0.0974],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2028, 0.0567, 0.1395, 0.1670, 0.1494, 0.1395, 0.1451],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2074, 0.0071, 0.0957, 0.0935, 0.3714, 0.1131, 0.1117],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1200, 0.0122, 0.1171, 0.0889, 0.0734, 0.5043, 0.0841],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1402, 0.1579, 0.0908, 0.1074, 0.0889, 0.1168, 0.2980],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  42.44395423520198
actor:  1 policy actor:  1  step number:  71 total reward:  0.22666666666666546  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.05 ]
 [0.099]
 [0.035]
 [0.066]
 [0.038]
 [0.039]
 [0.039]] [[37.715]
 [39.549]
 [37.091]
 [37.442]
 [36.325]
 [36.201]
 [36.143]] [[0.484]
 [0.573]
 [0.455]
 [0.494]
 [0.442]
 [0.44 ]
 [0.438]]
line 256 mcts: sample exp_bonus 37.74579229711928
using explorer policy with actor:  0
maxi score, test score, baseline:  0.14967333333333313 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
from probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  54.42760621862747
printing an ep nov before normalisation:  47.93378786041835
actor:  0 policy actor:  1  step number:  44 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.14973999999999985 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  23.67342583339757
printing an ep nov before normalisation:  34.07556365226085
printing an ep nov before normalisation:  27.976665496826172
Printing some Q and Qe and total Qs values:  [[0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.819]] [[45.302]
 [45.302]
 [45.302]
 [45.302]
 [45.302]
 [45.302]
 [45.302]] [[0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.819]]
maxi score, test score, baseline:  0.14719333333333315 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
Printing some Q and Qe and total Qs values:  [[0.277]
 [0.399]
 [0.144]
 [0.277]
 [0.277]
 [0.277]
 [0.339]] [[50.332]
 [48.995]
 [49.105]
 [50.332]
 [50.332]
 [50.332]
 [52.152]] [[1.76 ]
 [1.813]
 [1.563]
 [1.76 ]
 [1.76 ]
 [1.76 ]
 [1.916]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
Printing some Q and Qe and total Qs values:  [[0.577]
 [0.805]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]] [[35.882]
 [38.051]
 [35.792]
 [35.792]
 [35.792]
 [35.792]
 [35.792]] [[0.577]
 [0.805]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]]
actor:  1 policy actor:  1  step number:  58 total reward:  0.09999999999999953  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.14719333333333318 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.14719333333333318 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
actor:  1 policy actor:  1  step number:  65 total reward:  0.27999999999999936  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  46 total reward:  0.5  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.14773999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
Printing some Q and Qe and total Qs values:  [[0.71 ]
 [0.71 ]
 [0.71 ]
 [0.652]
 [0.71 ]
 [0.668]
 [0.71 ]] [[30.42 ]
 [30.42 ]
 [30.42 ]
 [33.883]
 [30.42 ]
 [31.44 ]
 [30.42 ]] [[2.416]
 [2.416]
 [2.416]
 [2.567]
 [2.416]
 [2.436]
 [2.416]]
printing an ep nov before normalisation:  42.37202654627035
printing an ep nov before normalisation:  33.109006703785944
Printing some Q and Qe and total Qs values:  [[0.645]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]] [[18.892]
 [21.351]
 [21.351]
 [21.351]
 [21.351]
 [21.351]
 [21.351]] [[2.645]
 [3.157]
 [3.157]
 [3.157]
 [3.157]
 [3.157]
 [3.157]]
actor:  1 policy actor:  1  step number:  50 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.32585467789602
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.594]
 [0.517]
 [0.485]
 [0.547]
 [0.486]
 [0.517]] [[30.567]
 [34.358]
 [31.56 ]
 [30.838]
 [34.831]
 [30.66 ]
 [31.56 ]] [[1.256]
 [1.536]
 [1.326]
 [1.26 ]
 [1.511]
 [1.253]
 [1.326]]
maxi score, test score, baseline:  0.14773999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  49 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.14773999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  19.245375571995176
Starting evaluation
siam score:  -0.7645803
printing an ep nov before normalisation:  39.19747032207352
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  61 total reward:  0.21333333333333304  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  48.22752354346361
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.551]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.488]] [[41.925]
 [40.859]
 [41.925]
 [41.925]
 [41.925]
 [41.925]
 [39.379]] [[0.487]
 [0.551]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.488]]
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.551]
 [0.491]
 [0.473]
 [0.471]
 [0.446]
 [0.476]] [[50.189]
 [47.345]
 [51.6  ]
 [51.107]
 [52.671]
 [51.869]
 [49.098]] [[0.479]
 [0.551]
 [0.491]
 [0.473]
 [0.471]
 [0.446]
 [0.476]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.607]
 [0.51 ]
 [0.544]
 [0.544]
 [0.544]
 [0.423]] [[48.519]
 [45.847]
 [46.982]
 [48.519]
 [48.519]
 [48.519]
 [47.05 ]] [[0.544]
 [0.607]
 [0.51 ]
 [0.544]
 [0.544]
 [0.544]
 [0.423]]
printing an ep nov before normalisation:  52.270026157395286
Printing some Q and Qe and total Qs values:  [[0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]] [[36.053]
 [36.053]
 [36.053]
 [36.053]
 [36.053]
 [36.053]
 [36.053]] [[0.881]
 [0.881]
 [0.881]
 [0.881]
 [0.881]
 [0.881]
 [0.881]]
Printing some Q and Qe and total Qs values:  [[0.835]
 [0.831]
 [0.852]
 [0.835]
 [0.835]
 [0.832]
 [0.835]] [[36.6  ]
 [37.137]
 [35.22 ]
 [36.6  ]
 [36.6  ]
 [32.676]
 [36.6  ]] [[0.835]
 [0.831]
 [0.852]
 [0.835]
 [0.835]
 [0.832]
 [0.835]]
printing an ep nov before normalisation:  39.54351263653004
maxi score, test score, baseline:  0.14773999999999982 0.6933333333333335 0.6933333333333335
probs:  [0.11256816411321531, 0.11256816411321531, 0.11256816411321531, 0.4371591794339236, 0.11256816411321531, 0.11256816411321531]
printing an ep nov before normalisation:  35.714768244195554
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  37.346402227924095
printing an ep nov before normalisation:  28.899165213658208
Printing some Q and Qe and total Qs values:  [[0.855]
 [0.883]
 [0.855]
 [0.855]
 [0.855]
 [0.855]
 [0.855]] [[28.138]
 [25.456]
 [28.138]
 [28.138]
 [28.138]
 [28.138]
 [28.138]] [[0.855]
 [0.883]
 [0.855]
 [0.855]
 [0.855]
 [0.855]
 [0.855]]
printing an ep nov before normalisation:  37.192726135253906
printing an ep nov before normalisation:  27.70939516046535
printing an ep nov before normalisation:  45.64379345702429
Printing some Q and Qe and total Qs values:  [[0.891]
 [0.947]
 [0.891]
 [0.891]
 [0.891]
 [0.891]
 [0.891]] [[31.207]
 [32.365]
 [31.207]
 [31.207]
 [31.207]
 [31.207]
 [31.207]] [[0.891]
 [0.947]
 [0.891]
 [0.891]
 [0.891]
 [0.891]
 [0.891]]
Printing some Q and Qe and total Qs values:  [[0.874]
 [1.   ]
 [0.874]
 [0.874]
 [0.874]
 [0.874]
 [0.874]] [[29.098]
 [34.613]
 [29.098]
 [29.098]
 [29.098]
 [29.098]
 [29.098]] [[0.874]
 [1.   ]
 [0.874]
 [0.874]
 [0.874]
 [0.874]
 [0.874]]
printing an ep nov before normalisation:  35.751654859663354
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333333  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  0.15973999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.12337791718731629, 0.12337791718731629, 0.12337791718731629, 0.3831104140634186, 0.12337791718731629, 0.12337791718731629]
actor:  1 policy actor:  1  step number:  66 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  45 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.084]
 [0.22 ]
 [0.041]
 [0.044]
 [0.082]
 [0.105]
 [0.083]] [[38.661]
 [37.537]
 [39.248]
 [38.988]
 [38.885]
 [37.521]
 [39.227]] [[0.339]
 [0.461]
 [0.304]
 [0.303]
 [0.34 ]
 [0.345]
 [0.345]]
maxi score, test score, baseline:  0.15973999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
Printing some Q and Qe and total Qs values:  [[0.743]
 [0.843]
 [0.751]
 [0.751]
 [0.751]
 [0.752]
 [0.756]] [[14.117]
 [18.926]
 [14.277]
 [14.277]
 [14.277]
 [14.267]
 [14.132]] [[1.529]
 [1.898]
 [1.546]
 [1.546]
 [1.546]
 [1.547]
 [1.544]]
actor:  1 policy actor:  1  step number:  42 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  52.78507454166416
actor:  1 policy actor:  1  step number:  54 total reward:  0.4066666666666663  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  47.70938220903531
printing an ep nov before normalisation:  47.20551013946533
actor:  1 policy actor:  1  step number:  41 total reward:  0.6666666666666669  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.15635333333333315 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
maxi score, test score, baseline:  0.15635333333333315 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  41.023082774884095
using explorer policy with actor:  1
maxi score, test score, baseline:  0.15635333333333315 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  30.353238582611084
printing an ep nov before normalisation:  23.436878252023476
actor:  1 policy actor:  1  step number:  49 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.15635333333333315 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  38.00082809304593
maxi score, test score, baseline:  0.15635333333333315 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  1 policy actor:  1  step number:  46 total reward:  0.54  reward:  1.0 rdn_beta:  2.0
from probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
maxi score, test score, baseline:  0.15635333333333315 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  0.045858403071861176
actor:  1 policy actor:  1  step number:  59 total reward:  0.27999999999999925  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  44 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.15635333333333315 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.28068974186659
using explorer policy with actor:  1
maxi score, test score, baseline:  0.15635333333333315 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
maxi score, test score, baseline:  0.15635333333333315 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
maxi score, test score, baseline:  0.15635333333333315 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  23.669920180822228
actor:  0 policy actor:  1  step number:  52 total reward:  0.17999999999999938  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  23.60247836981193
actions average: 
K:  3  action  0 :  tensor([0.5712, 0.0421, 0.0816, 0.0800, 0.0766, 0.0683, 0.0802],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0223, 0.9188, 0.0064, 0.0140, 0.0091, 0.0061, 0.0233],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1138, 0.0302, 0.4056, 0.1094, 0.1044, 0.1263, 0.1102],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1754, 0.0142, 0.1062, 0.3465, 0.1043, 0.1292, 0.1243],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1127, 0.1448, 0.0420, 0.0408, 0.5765, 0.0447, 0.0385],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1305, 0.0840, 0.1330, 0.1213, 0.1194, 0.2703, 0.1415],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1241, 0.1978, 0.0933, 0.0853, 0.0605, 0.0885, 0.3504],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  37.04604124411942
actor:  0 policy actor:  1  step number:  52 total reward:  0.28666666666666607  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.15459333333333317 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  23.974060987097918
actions average: 
K:  1  action  0 :  tensor([0.5451, 0.0584, 0.0535, 0.0841, 0.1167, 0.0607, 0.0814],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0162, 0.8673, 0.0178, 0.0370, 0.0054, 0.0117, 0.0446],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1335, 0.0321, 0.3959, 0.1141, 0.0755, 0.1037, 0.1451],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1487, 0.1037, 0.1080, 0.2792, 0.1198, 0.1208, 0.1198],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1626, 0.0008, 0.1110, 0.1044, 0.3681, 0.1213, 0.1317],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0440, 0.0010, 0.1880, 0.0319, 0.0269, 0.6712, 0.0369],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1159, 0.1493, 0.1127, 0.1139, 0.0660, 0.0859, 0.3562],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.776]
 [0.828]
 [0.817]
 [0.777]
 [0.766]
 [0.766]
 [0.768]] [[32.217]
 [31.708]
 [30.374]
 [33.746]
 [32.715]
 [32.156]
 [32.681]] [[1.976]
 [1.995]
 [1.898]
 [2.077]
 [1.999]
 [1.962]
 [1.998]]
maxi score, test score, baseline:  0.15459333333333317 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  64 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  56 total reward:  0.19333333333333302  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.491]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]] [[45.741]
 [44.168]
 [45.741]
 [45.741]
 [45.741]
 [45.741]
 [45.741]] [[2.121]
 [2.061]
 [2.121]
 [2.121]
 [2.121]
 [2.121]
 [2.121]]
maxi score, test score, baseline:  0.15121999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
maxi score, test score, baseline:  0.15121999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
maxi score, test score, baseline:  0.15121999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  1 policy actor:  1  step number:  44 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  30.080350264899696
maxi score, test score, baseline:  0.15121999999999983 0.6960000000000002 0.6960000000000002
printing an ep nov before normalisation:  32.153677486918674
printing an ep nov before normalisation:  43.474006943409755
maxi score, test score, baseline:  0.15121999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
Printing some Q and Qe and total Qs values:  [[0.261]
 [0.298]
 [0.241]
 [0.267]
 [0.244]
 [0.256]
 [0.245]] [[38.569]
 [39.548]
 [32.464]
 [37.048]
 [32.437]
 [37.652]
 [36.5  ]] [[0.762]
 [0.821]
 [0.607]
 [0.734]
 [0.609]
 [0.737]
 [0.7  ]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.15121999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  1 policy actor:  1  step number:  49 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  32.45599610464914
maxi score, test score, baseline:  0.15121999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
Printing some Q and Qe and total Qs values:  [[0.399]
 [0.447]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]] [[52.72 ]
 [44.641]
 [52.885]
 [52.885]
 [52.885]
 [52.885]
 [52.885]] [[0.931]
 [0.835]
 [0.869]
 [0.869]
 [0.869]
 [0.869]
 [0.869]]
actor:  1 policy actor:  1  step number:  70 total reward:  0.1533333333333322  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  43.94870266957336
maxi score, test score, baseline:  0.15121999999999983 0.6960000000000002 0.6960000000000002
maxi score, test score, baseline:  0.15121999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
maxi score, test score, baseline:  0.15121999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  31.981806755065918
printing an ep nov before normalisation:  43.42451569720541
maxi score, test score, baseline:  0.15121999999999983 0.6960000000000002 0.6960000000000002
maxi score, test score, baseline:  0.15121999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
maxi score, test score, baseline:  0.15121999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  0 policy actor:  0  step number:  42 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  33.40202313746537
printing an ep nov before normalisation:  20.299266361946795
actor:  1 policy actor:  1  step number:  39 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.098]
 [0.159]
 [0.094]
 [0.094]
 [0.095]
 [0.095]
 [0.092]] [[26.599]
 [31.285]
 [26.439]
 [26.066]
 [26.236]
 [26.252]
 [26.625]] [[0.494]
 [0.717]
 [0.484]
 [0.472]
 [0.479]
 [0.479]
 [0.488]]
printing an ep nov before normalisation:  37.60775106836163
maxi score, test score, baseline:  0.1507666666666665 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  0 policy actor:  0  step number:  56 total reward:  0.2066666666666659  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  45.924312707947806
actor:  1 policy actor:  1  step number:  49 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  33.2436418201306
siam score:  -0.7592638
printing an ep nov before normalisation:  39.087877634057165
maxi score, test score, baseline:  0.14981999999999981 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
maxi score, test score, baseline:  0.14981999999999981 0.6960000000000002 0.6960000000000002
printing an ep nov before normalisation:  38.68591259528957
actor:  1 policy actor:  1  step number:  60 total reward:  0.16666666666666596  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.14981999999999981 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  1 policy actor:  1  step number:  61 total reward:  0.26666666666666605  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  0.001059479437230948
actor:  1 policy actor:  1  step number:  66 total reward:  0.29999999999999927  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.149]
 [0.205]
 [0.149]
 [0.149]
 [0.149]
 [0.149]
 [0.149]] [[38.646]
 [45.949]
 [38.646]
 [38.646]
 [38.646]
 [38.646]
 [38.646]] [[1.469]
 [2.036]
 [1.469]
 [1.469]
 [1.469]
 [1.469]
 [1.469]]
printing an ep nov before normalisation:  25.152815210095316
actor:  0 policy actor:  1  step number:  56 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  39.86720085140953
printing an ep nov before normalisation:  40.95475717884732
printing an ep nov before normalisation:  29.158496856689453
printing an ep nov before normalisation:  44.89907844494432
siam score:  -0.75292265
printing an ep nov before normalisation:  43.67381445990965
printing an ep nov before normalisation:  41.20939284775778
printing an ep nov before normalisation:  38.366369477137646
maxi score, test score, baseline:  0.1474199999999998 0.6960000000000002 0.6960000000000002
maxi score, test score, baseline:  0.1474199999999998 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  56 total reward:  0.3533333333333327  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  39 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  29.705195814052242
actor:  0 policy actor:  1  step number:  42 total reward:  0.5133333333333332  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  55 total reward:  0.26666666666666605  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.15109999999999982 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  0 policy actor:  0  step number:  54 total reward:  0.2733333333333331  reward:  1.0 rdn_beta:  0.333
using another actor
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.283]
 [0.283]
 [0.283]
 [0.299]
 [0.283]
 [0.283]] [[42.853]
 [40.85 ]
 [40.85 ]
 [40.85 ]
 [42.869]
 [40.85 ]
 [40.85 ]] [[1.19 ]
 [1.107]
 [1.107]
 [1.107]
 [1.203]
 [1.107]
 [1.107]]
maxi score, test score, baseline:  0.15105999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  30.670485704985133
actions average: 
K:  4  action  0 :  tensor([0.3370, 0.0252, 0.1126, 0.1277, 0.1601, 0.1102, 0.1272],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0078, 0.9633, 0.0033, 0.0089, 0.0024, 0.0023, 0.0120],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1104, 0.0289, 0.5119, 0.0845, 0.0823, 0.0803, 0.1017],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1218, 0.0748, 0.1099, 0.2882, 0.1180, 0.1106, 0.1767],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2124, 0.0099, 0.0653, 0.0816, 0.4871, 0.0640, 0.0797],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1515, 0.0183, 0.2133, 0.1571, 0.1359, 0.1866, 0.1373],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1036, 0.1363, 0.1458, 0.1606, 0.0959, 0.0909, 0.2670],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  33.038424620424195
using explorer policy with actor:  0
maxi score, test score, baseline:  0.15105999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  40.36242696974013
printing an ep nov before normalisation:  31.207334457236666
maxi score, test score, baseline:  0.15105999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  28.677379746479367
printing an ep nov before normalisation:  25.44521808018
actor:  1 policy actor:  1  step number:  46 total reward:  0.4066666666666663  reward:  1.0 rdn_beta:  1.333
from probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  22.940539798592738
actor:  1 policy actor:  1  step number:  44 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  51 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1536199999999998 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
Printing some Q and Qe and total Qs values:  [[0.717]
 [0.803]
 [0.73 ]
 [0.715]
 [0.771]
 [0.719]
 [0.732]] [[26.305]
 [32.124]
 [28.567]
 [24.061]
 [29.167]
 [24.337]
 [28.839]] [[0.717]
 [0.803]
 [0.73 ]
 [0.715]
 [0.771]
 [0.719]
 [0.732]]
maxi score, test score, baseline:  0.1536199999999998 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
Printing some Q and Qe and total Qs values:  [[0.816]
 [0.917]
 [0.822]
 [0.847]
 [0.815]
 [0.818]
 [0.85 ]] [[26.965]
 [30.406]
 [26.67 ]
 [30.753]
 [27.455]
 [27.208]
 [30.719]] [[0.816]
 [0.917]
 [0.822]
 [0.847]
 [0.815]
 [0.818]
 [0.85 ]]
maxi score, test score, baseline:  0.1536199999999998 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  31.56015463329647
maxi score, test score, baseline:  0.1536199999999998 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  1 policy actor:  1  step number:  54 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.0
actor:  0 policy actor:  0  step number:  35 total reward:  0.52  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  3  action  0 :  tensor([0.5093, 0.0137, 0.0630, 0.0825, 0.1556, 0.0868, 0.0891],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0076, 0.9320, 0.0094, 0.0174, 0.0028, 0.0025, 0.0283],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1011, 0.1090, 0.3353, 0.0668, 0.0770, 0.2255, 0.0853],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0754, 0.2579, 0.0445, 0.3678, 0.0676, 0.1031, 0.0837],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2038, 0.0094, 0.1030, 0.1028, 0.3676, 0.1020, 0.1115],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1069, 0.0666, 0.1664, 0.0582, 0.0595, 0.4751, 0.0673],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2417, 0.1067, 0.0958, 0.1002, 0.1044, 0.1121, 0.2391],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.885]
 [0.992]
 [0.845]
 [0.92 ]
 [0.845]
 [0.845]
 [0.883]] [[29.81 ]
 [24.506]
 [26.474]
 [30.717]
 [26.474]
 [26.474]
 [31.412]] [[0.885]
 [0.992]
 [0.845]
 [0.92 ]
 [0.845]
 [0.845]
 [0.883]]
maxi score, test score, baseline:  0.15403333333333316 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  0 policy actor:  0  step number:  48 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  0.333
from probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  43.235603364163886
printing an ep nov before normalisation:  38.50520568557516
printing an ep nov before normalisation:  25.63394280409326
siam score:  -0.75893
maxi score, test score, baseline:  0.1541799999999998 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
Printing some Q and Qe and total Qs values:  [[0.584]
 [0.624]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.584]
 [0.624]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]]
maxi score, test score, baseline:  0.1541799999999998 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  0 policy actor:  0  step number:  45 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.001]
 [ 0.115]
 [-0.181]
 [-0.004]
 [ 0.084]
 [-0.125]
 [ 0.021]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.001]
 [ 0.115]
 [-0.181]
 [-0.004]
 [ 0.084]
 [-0.125]
 [ 0.021]]
printing an ep nov before normalisation:  25.85531567674245
actor:  1 policy actor:  1  step number:  54 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  0.0
actor:  0 policy actor:  0  step number:  48 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1525266666666665 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  47.369369154845565
maxi score, test score, baseline:  0.1525266666666665 0.6960000000000002 0.6960000000000002
maxi score, test score, baseline:  0.1525266666666665 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  34.19518709182739
maxi score, test score, baseline:  0.1525266666666665 0.6960000000000002 0.6960000000000002
printing an ep nov before normalisation:  29.700585361020206
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([0.6467, 0.0025, 0.0433, 0.0380, 0.1559, 0.0679, 0.0457],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0077, 0.9438, 0.0044, 0.0069, 0.0024, 0.0030, 0.0319],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0888, 0.0124, 0.5908, 0.0647, 0.0732, 0.1043, 0.0659],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1198, 0.1143, 0.0930, 0.2212, 0.1254, 0.1375, 0.1887],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.3250, 0.0236, 0.0716, 0.0753, 0.3294, 0.0879, 0.0872],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0861, 0.0874, 0.0836, 0.0626, 0.0750, 0.5487, 0.0564],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2033, 0.0817, 0.0801, 0.0729, 0.1445, 0.1241, 0.2934],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1525266666666665 0.6960000000000002 0.6960000000000002
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.492]
 [0.414]
 [0.414]
 [0.423]
 [0.414]
 [0.414]] [[32.374]
 [31.342]
 [33.231]
 [33.231]
 [32.307]
 [33.231]
 [33.231]] [[1.515]
 [1.512]
 [1.57 ]
 [1.57 ]
 [1.513]
 [1.57 ]
 [1.57 ]]
from probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]] [[39.945]
 [39.945]
 [39.945]
 [39.945]
 [39.945]
 [39.945]
 [39.945]] [[1.594]
 [1.594]
 [1.594]
 [1.594]
 [1.594]
 [1.594]
 [1.594]]
maxi score, test score, baseline:  0.1525266666666665 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  62 total reward:  0.0733333333333327  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  53 total reward:  0.31999999999999995  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1525266666666665 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  34.8856850134707
maxi score, test score, baseline:  0.1525266666666665 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actions average: 
K:  0  action  0 :  tensor([0.5698, 0.0081, 0.0637, 0.0806, 0.1163, 0.0824, 0.0790],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0049,     0.9675,     0.0030,     0.0081,     0.0006,     0.0008,
            0.0151], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0685, 0.0041, 0.6274, 0.0537, 0.0524, 0.1103, 0.0837],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2078, 0.0020, 0.1237, 0.3146, 0.0922, 0.1224, 0.1372],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2714, 0.0192, 0.0856, 0.0868, 0.3600, 0.0928, 0.0843],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1093, 0.0027, 0.1410, 0.0902, 0.1010, 0.4770, 0.0787],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1950, 0.1438, 0.0943, 0.1178, 0.1299, 0.0938, 0.2254],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  59 total reward:  0.23999999999999921  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  32.83342830799158
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  62 total reward:  0.25999999999999923  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  32.9432487487793
maxi score, test score, baseline:  0.15173999999999982 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
Printing some Q and Qe and total Qs values:  [[-0.008]
 [ 0.042]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]] [[34.762]
 [46.398]
 [34.762]
 [34.762]
 [34.762]
 [34.762]
 [34.762]] [[0.523]
 [1.243]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]]
maxi score, test score, baseline:  0.15173999999999982 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
Printing some Q and Qe and total Qs values:  [[0.63 ]
 [0.688]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]] [[35.51 ]
 [37.468]
 [35.51 ]
 [35.51 ]
 [35.51 ]
 [35.51 ]
 [35.51 ]] [[0.86 ]
 [0.947]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]]
maxi score, test score, baseline:  0.15173999999999982 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  0.0
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  32.163486044775695
printing an ep nov before normalisation:  36.72178148140423
actor:  1 policy actor:  1  step number:  57 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  45 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  62 total reward:  0.1666666666666664  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  38.647756576538086
printing an ep nov before normalisation:  38.701182366017235
maxi score, test score, baseline:  0.1521266666666665 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  1 policy actor:  1  step number:  56 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  2.0
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.691]
 [0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]] [[24.751]
 [30.925]
 [24.751]
 [24.751]
 [24.751]
 [24.751]
 [24.751]] [[1.178]
 [1.62 ]
 [1.178]
 [1.178]
 [1.178]
 [1.178]
 [1.178]]
printing an ep nov before normalisation:  32.86609127603432
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.029]
 [0.024]
 [0.024]
 [0.024]
 [0.024]
 [0.024]] [[ 0.   ]
 [43.922]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-1.541]
 [ 1.739]
 [-1.541]
 [-1.541]
 [-1.541]
 [-1.541]
 [-1.541]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.5333333333333335  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  37.674031257629395
maxi score, test score, baseline:  0.1521266666666665 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
siam score:  -0.7566142
from probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  33.20798878236082
printing an ep nov before normalisation:  55.61462684442607
actor:  0 policy actor:  0  step number:  47 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.14935333333333317 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  20.024402551928997
actor:  0 policy actor:  0  step number:  59 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.14925999999999984 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
maxi score, test score, baseline:  0.14925999999999984 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
line 256 mcts: sample exp_bonus 29.993533913718764
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
actor:  1 policy actor:  1  step number:  58 total reward:  0.27333333333333265  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  0
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.75749813558482
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.304]
 [0.365]
 [0.365]
 [0.288]
 [0.189]
 [0.272]] [[39.846]
 [32.841]
 [45.534]
 [45.534]
 [44.466]
 [39.384]
 [34.589]] [[1.686]
 [1.078]
 [2.062]
 [2.062]
 [1.908]
 [1.439]
 [1.173]]
printing an ep nov before normalisation:  25.601639585401724
actor:  1 policy actor:  1  step number:  40 total reward:  0.5666666666666668  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.14925999999999984 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  23.115986837876463
maxi score, test score, baseline:  0.14925999999999984 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4066666666666666  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  64 total reward:  0.16666666666666574  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  35.07597886163388
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]] [[12.573]
 [12.573]
 [12.573]
 [12.573]
 [12.573]
 [12.573]
 [12.573]] [[0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  47 total reward:  0.38666666666666616  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.113]
 [0.257]
 [0.052]
 [0.063]
 [0.022]
 [0.069]
 [0.041]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.113]
 [0.257]
 [0.052]
 [0.063]
 [0.022]
 [0.069]
 [0.041]]
maxi score, test score, baseline:  0.1488866666666665 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
line 256 mcts: sample exp_bonus 34.77465970025036
Printing some Q and Qe and total Qs values:  [[0.803]
 [0.876]
 [0.803]
 [0.803]
 [0.803]
 [0.803]
 [0.803]] [[28.01 ]
 [38.083]
 [28.01 ]
 [28.01 ]
 [28.01 ]
 [28.01 ]
 [28.01 ]] [[0.803]
 [0.876]
 [0.803]
 [0.803]
 [0.803]
 [0.803]
 [0.803]]
maxi score, test score, baseline:  0.1488866666666665 0.6960000000000002 0.6960000000000002
printing an ep nov before normalisation:  39.72001327469803
actor:  0 policy actor:  1  step number:  42 total reward:  0.48666666666666647  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.985]
 [0.985]
 [0.985]
 [0.985]
 [0.985]
 [0.985]
 [0.985]] [[29.019]
 [29.019]
 [29.019]
 [29.019]
 [29.019]
 [29.019]
 [29.019]] [[2.25]
 [2.25]
 [2.25]
 [2.25]
 [2.25]
 [2.25]
 [2.25]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.33333333333333315  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  36.27857793166191
line 256 mcts: sample exp_bonus 37.81943184810636
Printing some Q and Qe and total Qs values:  [[0.369]
 [0.471]
 [0.369]
 [0.369]
 [0.369]
 [0.37 ]
 [0.38 ]] [[25.651]
 [24.952]
 [24.211]
 [24.664]
 [24.883]
 [25.113]
 [25.668]] [[1.22 ]
 [1.277]
 [1.128]
 [1.157]
 [1.171]
 [1.186]
 [1.231]]
maxi score, test score, baseline:  0.15185999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  1 policy actor:  1  step number:  74 total reward:  0.23333333333333262  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.15185999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  29.87999200820923
printing an ep nov before normalisation:  15.99193811416626
printing an ep nov before normalisation:  28.755624213844527
actor:  1 policy actor:  1  step number:  58 total reward:  0.29999999999999927  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.15185999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  33.531341552734375
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  46.360465174099645
printing an ep nov before normalisation:  22.256430182291947
maxi score, test score, baseline:  0.15185999999999983 0.6960000000000002 0.6960000000000002
printing an ep nov before normalisation:  44.39295582445521
printing an ep nov before normalisation:  41.829783430562834
actor:  1 policy actor:  1  step number:  49 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  42.50785827636719
actor:  1 policy actor:  1  step number:  38 total reward:  0.5266666666666667  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.14868666666666652 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  34.0596691965276
printing an ep nov before normalisation:  55.35463679878645
printing an ep nov before normalisation:  43.11326376745727
maxi score, test score, baseline:  0.14868666666666652 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  0.00013260087825983646
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.249]
 [0.249]
 [0.249]
 [0.316]
 [0.249]
 [0.249]] [[41.802]
 [34.708]
 [34.708]
 [34.708]
 [36.39 ]
 [34.708]
 [34.708]] [[1.361]
 [0.854]
 [0.854]
 [0.854]
 [0.996]
 [0.854]
 [0.854]]
maxi score, test score, baseline:  0.14868666666666652 0.6960000000000002 0.6960000000000002
printing an ep nov before normalisation:  36.225793350356376
printing an ep nov before normalisation:  33.99762153625488
actor:  1 policy actor:  1  step number:  58 total reward:  0.2199999999999993  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.14868666666666652 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
maxi score, test score, baseline:  0.14868666666666652 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  1 policy actor:  1  step number:  60 total reward:  0.1933333333333328  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  54 total reward:  0.21999999999999964  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  45 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  44.37207933763466
printing an ep nov before normalisation:  38.765358329179676
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.356]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]] [[38.653]
 [39.743]
 [39.94 ]
 [39.94 ]
 [39.94 ]
 [39.94 ]
 [39.94 ]] [[1.703]
 [1.739]
 [1.644]
 [1.644]
 [1.644]
 [1.644]
 [1.644]]
maxi score, test score, baseline:  0.14868666666666652 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
Printing some Q and Qe and total Qs values:  [[-0.104]
 [-0.015]
 [ 0.22 ]
 [-0.015]
 [-0.014]
 [ 0.178]
 [-0.015]] [[35.615]
 [30.695]
 [35.171]
 [30.695]
 [32.053]
 [36.094]
 [30.695]] [[0.984]
 [0.779]
 [1.282]
 [0.779]
 [0.862]
 [1.295]
 [0.779]]
printing an ep nov before normalisation:  40.177764892578125
printing an ep nov before normalisation:  31.50004789188376
printing an ep nov before normalisation:  42.427544593811035
maxi score, test score, baseline:  0.14597999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  1 policy actor:  1  step number:  53 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]] [[48.58]
 [48.58]
 [48.58]
 [48.58]
 [48.58]
 [48.58]
 [48.58]] [[0.952]
 [0.952]
 [0.952]
 [0.952]
 [0.952]
 [0.952]
 [0.952]]
actor:  0 policy actor:  0  step number:  38 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.144]
 [0.209]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]] [[26.396]
 [41.065]
 [36.39 ]
 [36.39 ]
 [36.39 ]
 [36.39 ]
 [36.39 ]] [[0.267]
 [0.753]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]]
printing an ep nov before normalisation:  32.19987541146219
maxi score, test score, baseline:  0.14628666666666648 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  29.279711020465943
actor:  1 policy actor:  1  step number:  54 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  38.28486566313631
Printing some Q and Qe and total Qs values:  [[0.122]
 [0.212]
 [0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]] [[32.621]
 [35.556]
 [32.621]
 [32.621]
 [32.621]
 [32.621]
 [32.621]] [[0.779]
 [0.98 ]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]]
maxi score, test score, baseline:  0.14628666666666648 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
maxi score, test score, baseline:  0.14628666666666648 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  0.0006715729932693648
maxi score, test score, baseline:  0.14628666666666648 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  1 policy actor:  1  step number:  52 total reward:  0.32666666666666655  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.14628666666666648 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  1 policy actor:  1  step number:  73 total reward:  0.13333333333333253  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.14628666666666648 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  35.26470029625362
using another actor
from probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.732]
 [0.734]
 [0.732]
 [0.732]
 [0.732]
 [0.732]
 [0.732]] [[39.782]
 [41.107]
 [39.782]
 [39.782]
 [39.782]
 [39.782]
 [39.782]] [[0.732]
 [0.734]
 [0.732]
 [0.732]
 [0.732]
 [0.732]
 [0.732]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.14347333333333315 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.10269879565104
printing an ep nov before normalisation:  34.643263880944794
siam score:  -0.75094664
actor:  0 policy actor:  0  step number:  39 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  52 total reward:  0.28666666666666607  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  38.552452361206804
actions average: 
K:  2  action  0 :  tensor([0.6912, 0.0028, 0.0355, 0.0397, 0.1168, 0.0549, 0.0592],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0219, 0.8920, 0.0147, 0.0154, 0.0118, 0.0126, 0.0317],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1614, 0.0198, 0.2355, 0.1326, 0.1171, 0.1938, 0.1399],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1203, 0.1108, 0.0949, 0.2663, 0.1471, 0.1604, 0.1002],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1712, 0.1156, 0.0842, 0.0954, 0.3131, 0.1036, 0.1169],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1299, 0.0192, 0.2446, 0.0876, 0.0928, 0.2555, 0.1705],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1170, 0.0232, 0.2342, 0.0863, 0.0718, 0.0693, 0.3981],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  57 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  27.741239278201416
maxi score, test score, baseline:  0.14385999999999982 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  24.266834259033203
actor:  1 policy actor:  1  step number:  41 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.856]
 [0.888]
 [0.856]
 [0.856]
 [0.856]
 [0.856]
 [0.856]] [[40.991]
 [39.371]
 [40.991]
 [40.991]
 [40.991]
 [40.991]
 [40.991]] [[0.856]
 [0.888]
 [0.856]
 [0.856]
 [0.856]
 [0.856]
 [0.856]]
printing an ep nov before normalisation:  42.405867627197416
actor:  0 policy actor:  1  step number:  34 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  40 total reward:  0.5266666666666667  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  43 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.14713999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  1 policy actor:  1  step number:  49 total reward:  0.5333333333333337  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.363]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]] [[43.714]
 [38.96 ]
 [38.96 ]
 [38.96 ]
 [38.96 ]
 [38.96 ]
 [38.96 ]] [[0.363]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.14713999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.14713999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  35.55221071978617
maxi score, test score, baseline:  0.14713999999999983 0.6960000000000002 0.6960000000000002
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
actor:  0 policy actor:  0  step number:  33 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  55 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.06 ]
 [0.096]
 [0.06 ]
 [0.06 ]
 [0.06 ]
 [0.06 ]
 [0.06 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.06 ]
 [0.096]
 [0.06 ]
 [0.06 ]
 [0.06 ]
 [0.06 ]
 [0.06 ]]
maxi score, test score, baseline:  0.1484066666666665 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
from probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  35.511491691154106
printing an ep nov before normalisation:  55.30799715886963
actor:  1 policy actor:  1  step number:  44 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.1484066666666665 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  28.953089325525745
actor:  1 policy actor:  1  step number:  67 total reward:  0.09333333333333238  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.245]
 [0.268]
 [0.223]
 [0.245]
 [0.245]
 [0.245]
 [0.245]] [[37.878]
 [34.991]
 [33.851]
 [37.878]
 [37.878]
 [37.878]
 [37.878]] [[1.912]
 [1.728]
 [1.602]
 [1.912]
 [1.912]
 [1.912]
 [1.912]]
maxi score, test score, baseline:  0.1484066666666665 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
maxi score, test score, baseline:  0.1484066666666665 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  35.335477083258546
printing an ep nov before normalisation:  32.52648273322048
Printing some Q and Qe and total Qs values:  [[-0.106]
 [-0.095]
 [-0.106]
 [-0.113]
 [-0.112]
 [-0.11 ]
 [-0.114]] [[12.222]
 [ 7.532]
 [ 9.213]
 [ 9.177]
 [ 9.029]
 [12.14 ]
 [ 9.405]] [[0.435]
 [0.239]
 [0.303]
 [0.293]
 [0.288]
 [0.428]
 [0.303]]
maxi score, test score, baseline:  0.1484066666666665 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  0 policy actor:  0  step number:  48 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  1.333
siam score:  -0.75580615
maxi score, test score, baseline:  0.14829999999999985 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
maxi score, test score, baseline:  0.14829999999999985 0.6960000000000002 0.6960000000000002
printing an ep nov before normalisation:  50.362824882595916
printing an ep nov before normalisation:  55.371653545431386
actor:  1 policy actor:  1  step number:  57 total reward:  0.5733333333333337  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.14829999999999985 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  0 policy actor:  1  step number:  43 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1484066666666665 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  29.932350790412748
printing an ep nov before normalisation:  28.370785934260997
printing an ep nov before normalisation:  36.458699680136064
actions average: 
K:  2  action  0 :  tensor([0.3998, 0.0036, 0.0861, 0.0773, 0.2430, 0.1078, 0.0823],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0287, 0.8931, 0.0053, 0.0208, 0.0056, 0.0040, 0.0425],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0486, 0.0279, 0.6809, 0.0498, 0.0314, 0.0972, 0.0641],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1369, 0.0284, 0.0799, 0.4404, 0.0859, 0.0972, 0.1314],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0949, 0.0033, 0.0692, 0.0801, 0.5739, 0.1064, 0.0722],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0463, 0.0477, 0.1443, 0.0541, 0.0547, 0.5697, 0.0831],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0816, 0.2240, 0.0431, 0.0495, 0.0610, 0.0433, 0.4975],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1484066666666665 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  41.041011810302734
actor:  1 policy actor:  1  step number:  56 total reward:  0.3266666666666661  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1484066666666665 0.6960000000000002 0.6960000000000002
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1484066666666665 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
Printing some Q and Qe and total Qs values:  [[0.24 ]
 [0.286]
 [0.205]
 [0.2  ]
 [0.177]
 [0.194]
 [0.232]] [[36.575]
 [38.526]
 [38.439]
 [40.368]
 [39.859]
 [38.925]
 [39.51 ]] [[1.622]
 [1.815]
 [1.727]
 [1.868]
 [1.806]
 [1.753]
 [1.835]]
Printing some Q and Qe and total Qs values:  [[0.599]
 [0.607]
 [0.61 ]
 [0.507]
 [0.583]
 [0.601]
 [0.605]] [[41.483]
 [39.892]
 [40.913]
 [41.087]
 [36.187]
 [41.253]
 [41.421]] [[1.783]
 [1.717]
 [1.768]
 [1.672]
 [1.517]
 [1.775]
 [1.787]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1484066666666665 0.6960000000000002 0.6960000000000002
maxi score, test score, baseline:  0.1484066666666665 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
Printing some Q and Qe and total Qs values:  [[0.472]
 [0.576]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]] [[22.774]
 [25.693]
 [22.774]
 [22.774]
 [22.774]
 [22.774]
 [22.774]] [[1.532]
 [1.777]
 [1.532]
 [1.532]
 [1.532]
 [1.532]
 [1.532]]
printing an ep nov before normalisation:  39.15363550186157
actor:  1 policy actor:  1  step number:  56 total reward:  0.40666666666666673  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1484066666666665 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  30.333120749953846
actor:  1 policy actor:  1  step number:  38 total reward:  0.5666666666666669  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  33.94813631532568
printing an ep nov before normalisation:  33.396771793635764
maxi score, test score, baseline:  0.1484066666666665 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
Printing some Q and Qe and total Qs values:  [[0.382]
 [0.423]
 [0.354]
 [0.354]
 [0.354]
 [0.363]
 [0.354]] [[42.65 ]
 [40.857]
 [36.456]
 [36.456]
 [36.456]
 [42.427]
 [36.456]] [[0.948]
 [0.94 ]
 [0.749]
 [0.749]
 [0.749]
 [0.923]
 [0.749]]
printing an ep nov before normalisation:  32.634537029521475
maxi score, test score, baseline:  0.1484066666666665 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  1 policy actor:  1  step number:  57 total reward:  0.31999999999999995  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  31.758638481908672
printing an ep nov before normalisation:  26.90305961011079
printing an ep nov before normalisation:  40.15250530916897
actor:  1 policy actor:  1  step number:  63 total reward:  0.07999999999999918  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.769]
 [0.842]
 [0.763]
 [0.771]
 [0.847]
 [0.767]
 [0.854]] [[33.302]
 [32.806]
 [34.107]
 [32.749]
 [33.096]
 [34.857]
 [32.755]] [[0.769]
 [0.842]
 [0.763]
 [0.771]
 [0.847]
 [0.767]
 [0.854]]
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.7  ]
 [0.656]
 [0.673]
 [0.66 ]
 [0.672]
 [0.664]] [[27.163]
 [27.784]
 [27.127]
 [28.167]
 [26.556]
 [27.327]
 [27.628]] [[1.198]
 [1.245]
 [1.178]
 [1.232]
 [1.162]
 [1.201]
 [1.204]]
actor:  1 policy actor:  1  step number:  60 total reward:  0.24666666666666603  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7629267
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  58 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  1.333
from probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
maxi score, test score, baseline:  0.1484066666666665 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  0 policy actor:  0  step number:  45 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.761]
 [0.831]
 [0.745]
 [0.74 ]
 [0.739]
 [0.73 ]
 [0.738]] [[44.684]
 [50.179]
 [44.917]
 [45.127]
 [45.202]
 [45.336]
 [44.089]] [[0.761]
 [0.831]
 [0.745]
 [0.74 ]
 [0.739]
 [0.73 ]
 [0.738]]
printing an ep nov before normalisation:  41.742892630123485
maxi score, test score, baseline:  0.15123333333333316 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  38.09562843097189
printing an ep nov before normalisation:  46.441086772911014
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.94244146347046
maxi score, test score, baseline:  0.15123333333333316 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actions average: 
K:  3  action  0 :  tensor([0.4977, 0.0345, 0.1004, 0.0794, 0.0896, 0.0890, 0.1094],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0132, 0.8925, 0.0128, 0.0174, 0.0258, 0.0168, 0.0214],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1030, 0.0476, 0.3277, 0.1157, 0.1213, 0.1732, 0.1115],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1653, 0.0611, 0.0980, 0.2392, 0.1499, 0.1242, 0.1623],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1707, 0.0451, 0.0975, 0.1422, 0.2950, 0.1483, 0.1013],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1933, 0.0026, 0.1091, 0.1171, 0.1386, 0.3232, 0.1162],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1158, 0.1855, 0.0800, 0.0895, 0.0677, 0.0736, 0.3879],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.15123333333333316 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
maxi score, test score, baseline:  0.15123333333333316 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  31.390581130981445
actor:  1 policy actor:  1  step number:  56 total reward:  0.09999999999999942  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  45 total reward:  0.37333333333333274  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.15397999999999984 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  37.4260921443425
maxi score, test score, baseline:  0.15397999999999984 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  35.593209993223255
printing an ep nov before normalisation:  50.86801581551933
printing an ep nov before normalisation:  41.091880557152564
maxi score, test score, baseline:  0.15397999999999984 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actions average: 
K:  1  action  0 :  tensor([0.6322, 0.0010, 0.0582, 0.0540, 0.1118, 0.0670, 0.0758],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0110, 0.9233, 0.0076, 0.0121, 0.0035, 0.0042, 0.0382],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0693, 0.0286, 0.6316, 0.0472, 0.0503, 0.1039, 0.0690],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0941, 0.0119, 0.0491, 0.6346, 0.0838, 0.0573, 0.0693],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1907, 0.0031, 0.0930, 0.1107, 0.3621, 0.1125, 0.1279],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([    0.0599,     0.0002,     0.1395,     0.0456,     0.0508,     0.6463,
            0.0578], grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1831, 0.0775, 0.1323, 0.1442, 0.1404, 0.1462, 0.1763],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  34.83532279640228
maxi score, test score, baseline:  0.15397999999999984 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  1 policy actor:  1  step number:  68 total reward:  0.04666666666666586  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
Printing some Q and Qe and total Qs values:  [[0.243]
 [0.308]
 [0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]] [[31.454]
 [38.114]
 [31.454]
 [31.454]
 [31.454]
 [31.454]
 [31.454]] [[0.449]
 [0.609]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]]
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]] [[41.114]
 [42.038]
 [42.038]
 [42.038]
 [42.038]
 [42.038]
 [42.038]] [[0.656]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  36.85203330790841
maxi score, test score, baseline:  0.15397999999999984 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
maxi score, test score, baseline:  0.15397999999999984 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
using explorer policy with actor:  0
printing an ep nov before normalisation:  12.039718627929688
maxi score, test score, baseline:  0.15397999999999984 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  1 policy actor:  1  step number:  64 total reward:  0.15333333333333243  reward:  1.0 rdn_beta:  2.0
from probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
maxi score, test score, baseline:  0.15397999999999984 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  35.85479497909546
maxi score, test score, baseline:  0.15397999999999984 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  1 policy actor:  1  step number:  63 total reward:  0.18666666666666587  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.7  ]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]] [[41.495]
 [41.061]
 [41.495]
 [41.495]
 [41.495]
 [41.495]
 [41.495]] [[0.836]
 [0.94 ]
 [0.836]
 [0.836]
 [0.836]
 [0.836]
 [0.836]]
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.075]
 [0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.018]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.018]
 [0.075]
 [0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.018]]
printing an ep nov before normalisation:  49.60927231496862
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  52 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.15663333333333318 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  1 policy actor:  1  step number:  63 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.15663333333333318 0.6960000000000002 0.6960000000000002
printing an ep nov before normalisation:  41.57656949725235
maxi score, test score, baseline:  0.15663333333333318 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  36.683140356627405
Printing some Q and Qe and total Qs values:  [[-0.008]
 [ 0.155]
 [ 0.139]
 [ 0.155]
 [ 0.155]
 [ 0.155]
 [ 0.155]] [[35.598]
 [36.822]
 [37.881]
 [36.822]
 [36.822]
 [36.822]
 [36.822]] [[0.99 ]
 [1.242]
 [1.303]
 [1.242]
 [1.242]
 [1.242]
 [1.242]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  29.762100906446246
actor:  1 policy actor:  1  step number:  42 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  51 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.15663333333333318 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.457291890214467
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.601]
 [0.52 ]
 [0.506]
 [0.518]
 [0.517]
 [0.494]] [[32.961]
 [31.384]
 [34.479]
 [35.26 ]
 [35.634]
 [35.157]
 [31.87 ]] [[1.062]
 [1.097]
 [1.113]
 [1.124]
 [1.148]
 [1.132]
 [1.006]]
maxi score, test score, baseline:  0.15663333333333318 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  1 policy actor:  1  step number:  49 total reward:  0.44000000000000006  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[ 0.026]
 [-0.17 ]
 [ 0.024]
 [ 0.025]
 [-0.101]
 [ 0.027]
 [ 0.024]] [[18.642]
 [34.101]
 [18.974]
 [18.96 ]
 [31.632]
 [18.846]
 [19.047]] [[0.236]
 [0.37 ]
 [0.241]
 [0.242]
 [0.386]
 [0.242]
 [0.242]]
maxi score, test score, baseline:  0.15663333333333318 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
Printing some Q and Qe and total Qs values:  [[ 0.031]
 [-0.169]
 [ 0.033]
 [-0.124]
 [ 0.031]
 [ 0.03 ]
 [-0.088]] [[14.116]
 [29.412]
 [14.392]
 [28.733]
 [14.253]
 [14.126]
 [24.273]] [[0.19 ]
 [0.328]
 [0.199]
 [0.357]
 [0.194]
 [0.19 ]
 [0.295]]
maxi score, test score, baseline:  0.15663333333333318 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  1 policy actor:  1  step number:  43 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.15663333333333318 0.6960000000000002 0.6960000000000002
Printing some Q and Qe and total Qs values:  [[0.069]
 [0.185]
 [0.062]
 [0.058]
 [0.059]
 [0.059]
 [0.064]] [[38.346]
 [40.371]
 [38.457]
 [39.093]
 [39.676]
 [40.269]
 [40.201]] [[0.443]
 [0.599]
 [0.438]
 [0.447]
 [0.459]
 [0.471]
 [0.475]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  17.48584382985143
maxi score, test score, baseline:  0.15663333333333318 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
using explorer policy with actor:  1
siam score:  -0.7738173
actor:  1 policy actor:  1  step number:  48 total reward:  0.5533333333333337  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  31.338103018655712
maxi score, test score, baseline:  0.15663333333333318 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
maxi score, test score, baseline:  0.15663333333333318 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.15663333333333318 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  1 policy actor:  1  step number:  50 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  1.333
from probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
maxi score, test score, baseline:  0.15663333333333318 0.6960000000000002 0.6960000000000002
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  59.98828496188957
actor:  1 policy actor:  1  step number:  64 total reward:  0.12666666666666604  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  42 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  1.667
siam score:  -0.76342726
maxi score, test score, baseline:  0.15691333333333315 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
Printing some Q and Qe and total Qs values:  [[0.791]
 [0.841]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.788]] [[27.948]
 [28.904]
 [27.948]
 [27.948]
 [27.948]
 [27.948]
 [31.946]] [[0.791]
 [0.841]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.788]]
maxi score, test score, baseline:  0.15691333333333315 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  41.019755276990075
printing an ep nov before normalisation:  38.91694881585493
printing an ep nov before normalisation:  38.603701814729334
actor:  0 policy actor:  0  step number:  39 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.15984666666666653 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
line 256 mcts: sample exp_bonus 23.145474644987384
maxi score, test score, baseline:  0.15984666666666653 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
Printing some Q and Qe and total Qs values:  [[0.28 ]
 [0.382]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]] [[38.632]
 [41.282]
 [38.632]
 [38.632]
 [38.632]
 [38.632]
 [38.632]] [[1.243]
 [1.537]
 [1.243]
 [1.243]
 [1.243]
 [1.243]
 [1.243]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.29259961858095
actor:  1 policy actor:  1  step number:  49 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  64 total reward:  0.28666666666666574  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  51 total reward:  0.42666666666666664  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  1  action  0 :  tensor([0.5078, 0.0105, 0.0708, 0.0801, 0.1670, 0.0867, 0.0771],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0041,     0.9682,     0.0033,     0.0039,     0.0005,     0.0010,
            0.0190], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1784, 0.0123, 0.1281, 0.1144, 0.1528, 0.2843, 0.1296],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1594, 0.2458, 0.0908, 0.1086, 0.1343, 0.1127, 0.1484],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1128, 0.0033, 0.0612, 0.0581, 0.6299, 0.0730, 0.0616],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0747, 0.0011, 0.1543, 0.0502, 0.0714, 0.5822, 0.0661],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1423, 0.1447, 0.1537, 0.0927, 0.1342, 0.1064, 0.2260],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.15984666666666653 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
maxi score, test score, baseline:  0.1598466666666665 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
maxi score, test score, baseline:  0.1598466666666665 0.6960000000000002 0.6960000000000002
maxi score, test score, baseline:  0.1598466666666665 0.6960000000000002 0.6960000000000002
printing an ep nov before normalisation:  40.51765844762315
actor:  0 policy actor:  1  step number:  50 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1600066666666665 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
Printing some Q and Qe and total Qs values:  [[0.031]
 [0.095]
 [0.033]
 [0.032]
 [0.032]
 [0.018]
 [0.072]] [[33.035]
 [43.205]
 [32.612]
 [33.102]
 [33.403]
 [20.255]
 [33.534]] [[0.031]
 [0.095]
 [0.033]
 [0.032]
 [0.032]
 [0.018]
 [0.072]]
actor:  0 policy actor:  1  step number:  40 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  0
printing an ep nov before normalisation:  27.530292350643663
maxi score, test score, baseline:  0.16023333333333314 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
Printing some Q and Qe and total Qs values:  [[0.905]
 [0.903]
 [0.873]
 [0.892]
 [0.839]
 [0.863]
 [0.839]] [[39.68 ]
 [46.359]
 [40.42 ]
 [37.03 ]
 [35.134]
 [39.397]
 [35.134]] [[0.905]
 [0.903]
 [0.873]
 [0.892]
 [0.839]
 [0.863]
 [0.839]]
printing an ep nov before normalisation:  37.30459526977891
actor:  1 policy actor:  1  step number:  47 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[ 0.018]
 [ 0.147]
 [-0.005]
 [ 0.022]
 [-0.005]
 [-0.005]
 [-0.006]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.018]
 [ 0.147]
 [-0.005]
 [ 0.022]
 [-0.005]
 [-0.005]
 [-0.006]]
actor:  0 policy actor:  0  step number:  41 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  2.0
UNIT TEST: sample policy line 217 mcts : [0.245 0.327 0.02  0.02  0.041 0.306 0.041]
maxi score, test score, baseline:  0.1631666666666665 0.6960000000000002 0.6960000000000002
maxi score, test score, baseline:  0.1631666666666665 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
Printing some Q and Qe and total Qs values:  [[0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]] [[42.735]
 [42.735]
 [42.735]
 [42.735]
 [42.735]
 [42.735]
 [42.735]] [[1.416]
 [1.416]
 [1.416]
 [1.416]
 [1.416]
 [1.416]
 [1.416]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.5200000000000002  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  30.930896924415798
using explorer policy with actor:  0
maxi score, test score, baseline:  0.1631666666666665 0.6960000000000002 0.6960000000000002
using explorer policy with actor:  1
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
actor:  0 policy actor:  1  step number:  52 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.57425574185944
printing an ep nov before normalisation:  42.69770666893287
maxi score, test score, baseline:  0.16589999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
maxi score, test score, baseline:  0.16589999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  1 policy actor:  1  step number:  42 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  33.676957348999316
using explorer policy with actor:  1
printing an ep nov before normalisation:  60.51526605843877
maxi score, test score, baseline:  0.16589999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
maxi score, test score, baseline:  0.16589999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  18.13601897532659
actor:  1 policy actor:  1  step number:  45 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  69 total reward:  0.15999999999999936  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.16589999999999983 0.6960000000000002 0.6960000000000002
printing an ep nov before normalisation:  26.72475446152607
siam score:  -0.75779456
printing an ep nov before normalisation:  30.879908886492085
maxi score, test score, baseline:  0.16589999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  37.85761104618843
maxi score, test score, baseline:  0.16589999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
Printing some Q and Qe and total Qs values:  [[0.859]
 [0.874]
 [0.795]
 [0.849]
 [0.796]
 [0.796]
 [0.796]] [[31.099]
 [34.516]
 [32.236]
 [31.678]
 [32.52 ]
 [32.888]
 [33.074]] [[0.859]
 [0.874]
 [0.795]
 [0.849]
 [0.796]
 [0.796]
 [0.796]]
actions average: 
K:  0  action  0 :  tensor([0.6029, 0.0036, 0.0553, 0.0548, 0.1481, 0.0726, 0.0626],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0147, 0.9349, 0.0061, 0.0054, 0.0038, 0.0049, 0.0303],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1420, 0.0040, 0.3830, 0.1175, 0.1088, 0.1291, 0.1157],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1113, 0.0175, 0.0900, 0.4100, 0.1180, 0.1191, 0.1342],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1028, 0.0018, 0.1473, 0.0959, 0.3579, 0.1975, 0.0966],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0556, 0.0102, 0.1197, 0.0372, 0.0378, 0.6843, 0.0551],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1123, 0.2025, 0.1079, 0.1011, 0.1044, 0.1128, 0.2589],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  75 total reward:  0.013333333333332087  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.971]
 [0.971]
 [0.971]
 [0.971]
 [0.971]
 [0.971]
 [0.971]] [[32.719]
 [32.719]
 [32.719]
 [32.719]
 [32.719]
 [32.719]
 [32.719]] [[0.971]
 [0.971]
 [0.971]
 [0.971]
 [0.971]
 [0.971]
 [0.971]]
actor:  0 policy actor:  1  step number:  44 total reward:  0.35333333333333317  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1660866666666665 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  1 policy actor:  1  step number:  54 total reward:  0.3666666666666659  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1660866666666665 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  40.239422539421966
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.401]
 [0.26 ]
 [0.284]
 [0.247]
 [0.286]
 [0.292]] [[46.391]
 [46.581]
 [49.074]
 [46.697]
 [50.111]
 [49.728]
 [47.979]] [[1.225]
 [1.225]
 [1.171]
 [1.112]
 [1.195]
 [1.221]
 [1.165]]
printing an ep nov before normalisation:  30.12655258178711
maxi score, test score, baseline:  0.1660866666666665 0.6960000000000002 0.6960000000000002
maxi score, test score, baseline:  0.1660866666666665 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  1 policy actor:  1  step number:  57 total reward:  0.306666666666666  reward:  1.0 rdn_beta:  0.667
UNIT TEST: sample policy line 217 mcts : [0.327 0.224 0.061 0.061 0.122 0.041 0.163]
maxi score, test score, baseline:  0.1660866666666665 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  46.5509855354056
maxi score, test score, baseline:  0.1660866666666665 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  32.71385420546636
actor:  1 policy actor:  1  step number:  45 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  1.0
siam score:  -0.75043535
using explorer policy with actor:  0
maxi score, test score, baseline:  0.1660866666666665 0.6960000000000002 0.6960000000000002
maxi score, test score, baseline:  0.1660866666666665 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  0 policy actor:  1  step number:  49 total reward:  0.37333333333333285  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  0.00020766723935139453
actor:  1 policy actor:  1  step number:  53 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.918]
 [0.918]
 [0.918]
 [0.918]
 [0.918]
 [0.918]
 [0.918]] [[49.961]
 [49.961]
 [49.961]
 [49.961]
 [49.961]
 [49.961]
 [49.961]] [[0.918]
 [0.918]
 [0.918]
 [0.918]
 [0.918]
 [0.918]
 [0.918]]
printing an ep nov before normalisation:  31.631701124923044
printing an ep nov before normalisation:  49.67946068992179
printing an ep nov before normalisation:  36.33509414640237
printing an ep nov before normalisation:  40.519218677204314
printing an ep nov before normalisation:  50.19313311844153
printing an ep nov before normalisation:  40.354881286621094
printing an ep nov before normalisation:  37.72603511810303
maxi score, test score, baseline:  0.16675333333333317 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.722]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]] [[31.65 ]
 [33.648]
 [31.65 ]
 [31.65 ]
 [31.65 ]
 [31.65 ]
 [31.65 ]] [[0.869]
 [0.963]
 [0.869]
 [0.869]
 [0.869]
 [0.869]
 [0.869]]
actor:  0 policy actor:  1  step number:  57 total reward:  0.1733333333333329  reward:  1.0 rdn_beta:  2.0
from probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  41.38224131150794
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.822]
 [0.9  ]
 [0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.822]] [[35.362]
 [36.644]
 [35.362]
 [35.362]
 [35.362]
 [35.362]
 [35.362]] [[0.822]
 [0.9  ]
 [0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.822]]
actor:  0 policy actor:  1  step number:  44 total reward:  0.44666666666666666  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.16984666666666648 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
Printing some Q and Qe and total Qs values:  [[0.722]
 [0.8  ]
 [0.761]
 [0.732]
 [0.771]
 [0.801]
 [0.732]] [[48.723]
 [46.518]
 [48.051]
 [48.904]
 [46.13 ]
 [45.492]
 [49.079]] [[0.722]
 [0.8  ]
 [0.761]
 [0.732]
 [0.771]
 [0.801]
 [0.732]]
actor:  1 policy actor:  1  step number:  64 total reward:  0.19333333333333236  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  54 total reward:  0.2333333333333325  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  2  action  0 :  tensor([0.5260, 0.0567, 0.0724, 0.0710, 0.1196, 0.0752, 0.0791],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0089, 0.9196, 0.0086, 0.0163, 0.0043, 0.0042, 0.0381],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1216, 0.0156, 0.3736, 0.1037, 0.1258, 0.1374, 0.1223],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1631, 0.0773, 0.1094, 0.2846, 0.1360, 0.1242, 0.1054],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2040, 0.0018, 0.0730, 0.0706, 0.4896, 0.0765, 0.0846],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1104, 0.0047, 0.2598, 0.0721, 0.0841, 0.3792, 0.0897],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1335, 0.2278, 0.0735, 0.1095, 0.0662, 0.0671, 0.3223],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  81 total reward:  0.35999999999999943  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.16939333333333317 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
Printing some Q and Qe and total Qs values:  [[0.802]
 [0.806]
 [0.806]
 [0.806]
 [0.806]
 [0.806]
 [0.806]] [[54.812]
 [52.565]
 [52.565]
 [52.565]
 [52.565]
 [52.565]
 [52.565]] [[0.802]
 [0.806]
 [0.806]
 [0.806]
 [0.806]
 [0.806]
 [0.806]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  39.93147856194425
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.289]
 [0.311]
 [0.289]
 [0.289]
 [0.289]
 [0.289]
 [0.289]] [[43.602]
 [40.866]
 [43.602]
 [43.602]
 [43.602]
 [43.602]
 [43.602]] [[2.289]
 [2.072]
 [2.289]
 [2.289]
 [2.289]
 [2.289]
 [2.289]]
maxi score, test score, baseline:  0.16727333333333316 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  0 policy actor:  0  step number:  34 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.16787333333333318 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.63185161350473
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
actor:  0 policy actor:  0  step number:  43 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17072666666666647 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
UNIT TEST: sample policy line 217 mcts : [0. 1. 0. 0. 0. 0. 0.]
maxi score, test score, baseline:  0.17072666666666647 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
Printing some Q and Qe and total Qs values:  [[0.189]
 [0.318]
 [0.189]
 [0.189]
 [0.169]
 [0.189]
 [0.189]] [[47.169]
 [45.89 ]
 [47.169]
 [47.169]
 [43.651]
 [47.169]
 [47.169]] [[1.504]
 [1.569]
 [1.504]
 [1.504]
 [1.307]
 [1.504]
 [1.504]]
actor:  0 policy actor:  1  step number:  55 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  2.0
from probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
maxi score, test score, baseline:  0.17081999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
maxi score, test score, baseline:  0.17081999999999983 0.6960000000000002 0.6960000000000002
maxi score, test score, baseline:  0.17081999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  43.71144700129637
maxi score, test score, baseline:  0.17081999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17081999999999983 0.6960000000000002 0.6960000000000002
actor:  1 policy actor:  1  step number:  35 total reward:  0.666666666666667  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.855]
 [0.855]
 [0.855]
 [0.855]
 [0.855]
 [0.855]
 [0.855]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.855]
 [0.855]
 [0.855]
 [0.855]
 [0.855]
 [0.855]
 [0.855]]
printing an ep nov before normalisation:  34.05282729004528
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.17081999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  37.90204734852324
printing an ep nov before normalisation:  60.62664749050989
maxi score, test score, baseline:  0.17081999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
actor:  1 policy actor:  1  step number:  47 total reward:  0.4800000000000001  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  53.14289606331574
printing an ep nov before normalisation:  45.59386730194092
printing an ep nov before normalisation:  50.57207500495404
printing an ep nov before normalisation:  42.852331385376765
printing an ep nov before normalisation:  35.94183451519815
printing an ep nov before normalisation:  36.95091438714614
printing an ep nov before normalisation:  37.35911376249798
Printing some Q and Qe and total Qs values:  [[0.972]
 [0.907]
 [0.972]
 [0.972]
 [0.972]
 [0.972]
 [0.972]] [[36.669]
 [32.413]
 [36.669]
 [36.669]
 [36.669]
 [36.669]
 [36.669]] [[0.972]
 [0.907]
 [0.972]
 [0.972]
 [0.972]
 [0.972]
 [0.972]]
line 256 mcts: sample exp_bonus 42.28688160897051
printing an ep nov before normalisation:  29.95700342445233
printing an ep nov before normalisation:  30.853493573309482
printing an ep nov before normalisation:  30.640833751448262
printing an ep nov before normalisation:  33.63428613650455
printing an ep nov before normalisation:  31.330610549321893
maxi score, test score, baseline:  0.17081999999999983 0.6960000000000002 0.6960000000000002
probs:  [0.15686320643734, 0.15686320643734, 0.15686320643734, 0.21568396781329982, 0.15686320643734, 0.15686320643734]
printing an ep nov before normalisation:  44.0893232895245
printing an ep nov before normalisation:  45.04349985832853
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.003]
 [0.002]
 [0.001]] [[0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]]
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  0.18035333333333314 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.424]
 [0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]] [[44.548]
 [46.018]
 [44.548]
 [44.548]
 [44.548]
 [44.548]
 [44.548]] [[1.537]
 [1.675]
 [1.537]
 [1.537]
 [1.537]
 [1.537]
 [1.537]]
actor:  1 policy actor:  1  step number:  66 total reward:  0.11333333333333251  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.4  ]
 [0.471]
 [0.349]
 [0.405]
 [0.4  ]
 [0.367]
 [0.328]] [[40.832]
 [43.172]
 [41.699]
 [42.605]
 [40.832]
 [41.429]
 [40.033]] [[1.678]
 [1.876]
 [1.674]
 [1.779]
 [1.678]
 [1.677]
 [1.562]]
actor:  1 policy actor:  1  step number:  60 total reward:  0.13999999999999957  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.28]
 [0.28]
 [0.28]
 [0.28]
 [0.28]
 [0.28]
 [0.28]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.28]
 [0.28]
 [0.28]
 [0.28]
 [0.28]
 [0.28]
 [0.28]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.2133333333333327  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.18035333333333314 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  0 policy actor:  1  step number:  42 total reward:  0.5  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  52.3255384891392
printing an ep nov before normalisation:  53.41970557221884
maxi score, test score, baseline:  0.1799666666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1799666666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  32.050878870920585
printing an ep nov before normalisation:  35.11260166668106
actor:  0 policy actor:  1  step number:  52 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  19.440672397613525
actor:  1 policy actor:  1  step number:  43 total reward:  0.5733333333333336  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  50 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  1.333
from probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  42.01658794473454
maxi score, test score, baseline:  0.17865999999999982 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  43.501410520054044
maxi score, test score, baseline:  0.17865999999999982 0.6953333333333334 0.6953333333333334
maxi score, test score, baseline:  0.17865999999999982 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.17865999999999982 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  26.544761657714844
maxi score, test score, baseline:  0.1752733333333332 0.6953333333333334 0.6953333333333334
printing an ep nov before normalisation:  49.28161989480492
maxi score, test score, baseline:  0.1752733333333332 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  57 total reward:  0.3733333333333334  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  43.8061993254838
actor:  1 policy actor:  1  step number:  37 total reward:  0.5733333333333334  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  48.3172640893128
printing an ep nov before normalisation:  62.550516999493894
actor:  0 policy actor:  0  step number:  51 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  51 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.185]
 [0.295]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.155]] [[42.338]
 [39.981]
 [42.338]
 [42.338]
 [42.338]
 [42.338]
 [40.403]] [[0.715]
 [0.783]
 [0.715]
 [0.715]
 [0.715]
 [0.715]
 [0.651]]
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.422]
 [0.398]
 [0.39 ]
 [0.386]
 [0.384]
 [0.384]] [[32.163]
 [35.545]
 [31.869]
 [33.408]
 [33.682]
 [32.787]
 [32.669]] [[0.396]
 [0.422]
 [0.398]
 [0.39 ]
 [0.386]
 [0.384]
 [0.384]]
printing an ep nov before normalisation:  46.903724703142
maxi score, test score, baseline:  0.17449999999999982 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.17449999999999982 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  48.41889627054605
printing an ep nov before normalisation:  48.04393191236088
maxi score, test score, baseline:  0.17449999999999982 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  46 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17449999999999982 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
line 256 mcts: sample exp_bonus 33.37616256525333
printing an ep nov before normalisation:  52.93429868422187
maxi score, test score, baseline:  0.17449999999999982 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  35.79064241933745
Printing some Q and Qe and total Qs values:  [[0.705]
 [0.728]
 [0.705]
 [0.705]
 [0.705]
 [0.705]
 [0.705]] [[40.233]
 [42.526]
 [40.233]
 [40.233]
 [40.233]
 [40.233]
 [40.233]] [[1.182]
 [1.255]
 [1.182]
 [1.182]
 [1.182]
 [1.182]
 [1.182]]
Printing some Q and Qe and total Qs values:  [[-0.001]
 [ 0.133]
 [ 0.123]
 [ 0.014]
 [-0.001]
 [ 0.015]
 [ 0.061]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.001]
 [ 0.133]
 [ 0.123]
 [ 0.014]
 [-0.001]
 [ 0.015]
 [ 0.061]]
Printing some Q and Qe and total Qs values:  [[0.619]
 [0.733]
 [0.619]
 [0.619]
 [0.619]
 [0.619]
 [0.619]] [[30.863]
 [29.303]
 [30.863]
 [30.863]
 [30.863]
 [30.863]
 [30.863]] [[1.286]
 [1.336]
 [1.286]
 [1.286]
 [1.286]
 [1.286]
 [1.286]]
actor:  1 policy actor:  1  step number:  44 total reward:  0.5666666666666669  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.432]
 [0.255]
 [0.332]
 [0.334]
 [0.334]
 [0.409]] [[51.456]
 [48.809]
 [52.34 ]
 [48.178]
 [51.456]
 [51.456]
 [50.699]] [[1.432]
 [1.42 ]
 [1.39 ]
 [1.293]
 [1.432]
 [1.432]
 [1.475]]
Printing some Q and Qe and total Qs values:  [[0.833]
 [0.831]
 [0.833]
 [0.833]
 [0.833]
 [0.833]
 [0.833]] [[ 6.133]
 [ 4.272]
 [ 3.689]
 [10.021]
 [10.011]
 [10.239]
 [ 7.532]] [[0.833]
 [0.831]
 [0.833]
 [0.833]
 [0.833]
 [0.833]
 [0.833]]
actor:  0 policy actor:  1  step number:  40 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  54.797251439134484
actions average: 
K:  4  action  0 :  tensor([0.7400, 0.0223, 0.0449, 0.0459, 0.0517, 0.0266, 0.0685],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0043, 0.9602, 0.0035, 0.0043, 0.0043, 0.0041, 0.0193],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2036, 0.0104, 0.2839, 0.1633, 0.1068, 0.1260, 0.1058],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1550, 0.1724, 0.1098, 0.1321, 0.1456, 0.1740, 0.1112],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1998, 0.0030, 0.1013, 0.1179, 0.3606, 0.1242, 0.0932],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1263, 0.0006, 0.1159, 0.0989, 0.1204, 0.4333, 0.1046],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1761, 0.1884, 0.1199, 0.1183, 0.1389, 0.1254, 0.1330],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.17456666666666648 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  21.449460167511845
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  46.4117240451735
siam score:  -0.7574884
actor:  1 policy actor:  1  step number:  68 total reward:  0.11333333333333317  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17456666666666648 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.17456666666666648 0.6953333333333334 0.6953333333333334
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.354]
 [0.482]
 [0.469]
 [0.565]
 [0.482]
 [0.482]] [[38.837]
 [42.016]
 [38.837]
 [32.728]
 [35.686]
 [38.837]
 [38.837]] [[1.226]
 [1.263]
 [1.226]
 [0.897]
 [1.146]
 [1.226]
 [1.226]]
Printing some Q and Qe and total Qs values:  [[0.734]
 [0.795]
 [0.73 ]
 [0.748]
 [0.748]
 [0.736]
 [0.725]] [[33.663]
 [27.219]
 [33.397]
 [32.841]
 [32.061]
 [32.322]
 [31.144]] [[1.826]
 [1.677]
 [1.813]
 [1.813]
 [1.787]
 [1.783]
 [1.734]]
maxi score, test score, baseline:  0.17456666666666648 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  34.77877855300903
actor:  1 policy actor:  1  step number:  47 total reward:  0.4800000000000001  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  40.205504540881776
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  41 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17232666666666652 0.6953333333333334 0.6953333333333334
maxi score, test score, baseline:  0.17232666666666652 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  42.229265817786604
maxi score, test score, baseline:  0.17232666666666652 0.6953333333333334 0.6953333333333334
printing an ep nov before normalisation:  28.32709750169863
line 256 mcts: sample exp_bonus 40.7902323096845
maxi score, test score, baseline:  0.17232666666666652 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.17232666666666652 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.171000950502915
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.127]
 [0.183]
 [0.17 ]
 [0.17 ]
 [0.129]
 [0.137]
 [0.161]] [[35.593]
 [35.97 ]
 [34.487]
 [34.787]
 [35.797]
 [34.979]
 [36.183]] [[0.835]
 [0.905]
 [0.836]
 [0.847]
 [0.845]
 [0.821]
 [0.891]]
actor:  1 policy actor:  1  step number:  65 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.166]
 [0.16 ]
 [0.166]
 [0.166]
 [0.166]
 [0.166]
 [0.166]] [[32.542]
 [39.398]
 [32.542]
 [32.542]
 [32.542]
 [32.542]
 [32.542]] [[0.685]
 [0.908]
 [0.685]
 [0.685]
 [0.685]
 [0.685]
 [0.685]]
UNIT TEST: sample policy line 217 mcts : [0.041 0.408 0.306 0.02  0.184 0.02  0.02 ]
maxi score, test score, baseline:  0.17232666666666652 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.17232666666666652 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
siam score:  -0.757413
maxi score, test score, baseline:  0.17232666666666652 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.042]
 [0.445]
 [0.042]
 [0.042]
 [0.042]
 [0.042]
 [0.042]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.042]
 [0.445]
 [0.042]
 [0.042]
 [0.042]
 [0.042]
 [0.042]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.0004915576212738415
actor:  1 policy actor:  1  step number:  51 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  50 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  50.17438694569786
printing an ep nov before normalisation:  34.42775249481201
Printing some Q and Qe and total Qs values:  [[0.767]
 [0.675]
 [0.667]
 [0.667]
 [0.679]
 [0.667]
 [0.667]] [[38.573]
 [40.519]
 [44.362]
 [44.362]
 [41.642]
 [44.362]
 [44.362]] [[0.767]
 [0.675]
 [0.667]
 [0.667]
 [0.679]
 [0.667]
 [0.667]]
maxi score, test score, baseline:  0.17232666666666652 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  49.94391927372994
maxi score, test score, baseline:  0.17232666666666652 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.17232666666666652 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  49.159235717952285
printing an ep nov before normalisation:  32.416408546002515
maxi score, test score, baseline:  0.17232666666666652 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  0 policy actor:  0  step number:  44 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  59 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17260666666666655 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  41.739811996050335
siam score:  -0.7614611
printing an ep nov before normalisation:  44.92096349111287
maxi score, test score, baseline:  0.17260666666666655 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  35.67404359687872
printing an ep nov before normalisation:  35.961752207306944
Printing some Q and Qe and total Qs values:  [[0.088]
 [0.197]
 [0.07 ]
 [0.068]
 [0.088]
 [0.072]
 [0.075]] [[32.767]
 [36.745]
 [33.449]
 [34.521]
 [32.767]
 [36.406]
 [34.925]] [[0.765]
 [1.032]
 [0.774]
 [0.814]
 [0.765]
 [0.893]
 [0.837]]
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]] [[46.169]
 [46.169]
 [46.169]
 [46.169]
 [46.169]
 [46.169]
 [46.169]] [[1.463]
 [1.463]
 [1.463]
 [1.463]
 [1.463]
 [1.463]
 [1.463]]
actor:  1 policy actor:  1  step number:  43 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  42.29977416236912
printing an ep nov before normalisation:  33.34700311611449
Printing some Q and Qe and total Qs values:  [[0.88 ]
 [0.917]
 [0.88 ]
 [0.88 ]
 [0.88 ]
 [0.88 ]
 [0.88 ]] [[39.954]
 [40.606]
 [39.954]
 [39.954]
 [39.954]
 [39.954]
 [39.954]] [[0.88 ]
 [0.917]
 [0.88 ]
 [0.88 ]
 [0.88 ]
 [0.88 ]
 [0.88 ]]
maxi score, test score, baseline:  0.17260666666666655 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  40.27414798726935
actor:  1 policy actor:  1  step number:  44 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  40 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  28.303241492932194
printing an ep nov before normalisation:  29.107456072002755
printing an ep nov before normalisation:  36.65219259366608
printing an ep nov before normalisation:  57.92126749262449
maxi score, test score, baseline:  0.1701266666666665 0.6953333333333334 0.6953333333333334
actor:  0 policy actor:  1  step number:  63 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  26.84331290362899
maxi score, test score, baseline:  0.17025999999999986 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  49.35151069935322
actor:  1 policy actor:  1  step number:  47 total reward:  0.45333333333333303  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  57.95527723082604
maxi score, test score, baseline:  0.17025999999999986 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.17025999999999986 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  53.1792731872897
printing an ep nov before normalisation:  46.47342639052697
printing an ep nov before normalisation:  42.09304332733154
actor:  1 policy actor:  1  step number:  49 total reward:  0.3733333333333333  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17025999999999986 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  0.10668854758876023
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.489]
 [0.416]
 [0.435]
 [0.334]
 [0.402]
 [0.362]] [[38.093]
 [38.303]
 [37.812]
 [40.585]
 [47.722]
 [39.828]
 [36.055]] [[2.229]
 [2.282]
 [2.164]
 [2.435]
 [2.982]
 [2.334]
 [1.95 ]]
actor:  1 policy actor:  1  step number:  62 total reward:  0.23333333333333328  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  47.41619110107422
maxi score, test score, baseline:  0.17025999999999986 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  54.70335257788714
Printing some Q and Qe and total Qs values:  [[0.182]
 [0.182]
 [0.243]
 [0.182]
 [0.182]
 [0.182]
 [0.182]] [[53.168]
 [53.168]
 [61.198]
 [53.168]
 [53.168]
 [53.168]
 [53.168]] [[0.899]
 [0.899]
 [1.137]
 [0.899]
 [0.899]
 [0.899]
 [0.899]]
Printing some Q and Qe and total Qs values:  [[-0.027]
 [ 0.127]
 [-0.027]
 [-0.026]
 [-0.026]
 [-0.004]
 [ 0.068]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.027]
 [ 0.127]
 [-0.027]
 [-0.026]
 [-0.026]
 [-0.004]
 [ 0.068]]
maxi score, test score, baseline:  0.17025999999999986 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  0.00027746876412493293
actor:  1 policy actor:  1  step number:  49 total reward:  0.5333333333333335  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7584169
maxi score, test score, baseline:  0.17025999999999986 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  34.396125439338334
printing an ep nov before normalisation:  37.14489098715287
printing an ep nov before normalisation:  56.7330513093034
actor:  1 policy actor:  1  step number:  45 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.17025999999999986 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  58.08563158926792
actor:  1 policy actor:  1  step number:  45 total reward:  0.52  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  38.269060518726214
maxi score, test score, baseline:  0.17025999999999986 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  57 total reward:  0.25333333333333286  reward:  1.0 rdn_beta:  1.0
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.787]
 [0.787]
 [0.787]
 [0.787]
 [0.787]
 [0.787]
 [0.787]] [[48.734]
 [48.734]
 [48.734]
 [48.734]
 [48.734]
 [48.734]
 [48.734]] [[0.787]
 [0.787]
 [0.787]
 [0.787]
 [0.787]
 [0.787]
 [0.787]]
printing an ep nov before normalisation:  38.168496624499106
maxi score, test score, baseline:  0.17025999999999986 0.6953333333333334 0.6953333333333334
maxi score, test score, baseline:  0.17025999999999986 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
using another actor
maxi score, test score, baseline:  0.17025999999999986 0.6953333333333334 0.6953333333333334
printing an ep nov before normalisation:  29.343689048237056
siam score:  -0.7626563
maxi score, test score, baseline:  0.17025999999999986 0.6953333333333334 0.6953333333333334
actor:  1 policy actor:  1  step number:  57 total reward:  0.18666666666666643  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  34.01433412476562
maxi score, test score, baseline:  0.17025999999999986 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  32.379220288386406
maxi score, test score, baseline:  0.17025999999999986 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.17025999999999986 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  0 policy actor:  1  step number:  52 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.16991333333333317 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
Printing some Q and Qe and total Qs values:  [[0.135]
 [0.178]
 [0.174]
 [0.136]
 [0.154]
 [0.133]
 [0.135]] [[31.836]
 [37.221]
 [34.57 ]
 [32.877]
 [34.064]
 [33.261]
 [33.477]] [[0.977]
 [1.299]
 [1.157]
 [1.032]
 [1.111]
 [1.048]
 [1.062]]
using explorer policy with actor:  1
using explorer policy with actor:  0
printing an ep nov before normalisation:  55.676334303481454
actor:  1 policy actor:  1  step number:  68 total reward:  0.03333333333333255  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.004]
 [ 0.151]
 [ 0.071]
 [ 0.045]
 [ 0.045]
 [ 0.025]
 [ 0.074]] [[46.456]
 [47.996]
 [45.933]
 [44.955]
 [42.766]
 [46.349]
 [48.852]] [[0.546]
 [0.728]
 [0.613]
 [0.57 ]
 [0.534]
 [0.574]
 [0.665]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.42830653094156
actor:  0 policy actor:  1  step number:  38 total reward:  0.6733333333333336  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  24.98824119567871
Printing some Q and Qe and total Qs values:  [[0.02 ]
 [0.146]
 [0.02 ]
 [0.02 ]
 [0.02 ]
 [0.02 ]
 [0.02 ]] [[34.243]
 [40.885]
 [34.243]
 [34.243]
 [34.243]
 [34.243]
 [34.243]] [[0.934]
 [1.496]
 [0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.934]]
printing an ep nov before normalisation:  40.718126316152635
line 256 mcts: sample exp_bonus 40.26829204730996
printing an ep nov before normalisation:  45.15780859389517
printing an ep nov before normalisation:  26.398922993929034
printing an ep nov before normalisation:  26.894378662109375
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  34 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.331]
 [0.221]
 [0.221]
 [0.221]
 [0.197]
 [0.221]
 [0.221]] [[44.874]
 [41.425]
 [41.425]
 [41.425]
 [44.529]
 [41.425]
 [41.425]] [[0.592]
 [0.449]
 [0.449]
 [0.449]
 [0.455]
 [0.449]
 [0.449]]
actor:  1 policy actor:  1  step number:  50 total reward:  0.5800000000000002  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.6355523452222
using another actor
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([0.3114, 0.0686, 0.1062, 0.0978, 0.1967, 0.1036, 0.1157],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0088, 0.9475, 0.0077, 0.0103, 0.0032, 0.0037, 0.0188],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0673, 0.0195, 0.5482, 0.0536, 0.0651, 0.0792, 0.1671],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1039, 0.0938, 0.1248, 0.2632, 0.1176, 0.0758, 0.2210],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1551, 0.0038, 0.0628, 0.0863, 0.5197, 0.0828, 0.0894],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1559, 0.0020, 0.1334, 0.1064, 0.1234, 0.3481, 0.1308],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1527, 0.1254, 0.1019, 0.1027, 0.1131, 0.1150, 0.2892],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  39.755706787109375
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
Printing some Q and Qe and total Qs values:  [[0.655]
 [0.775]
 [0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.655]] [[45.598]
 [45.761]
 [45.598]
 [45.598]
 [45.598]
 [45.598]
 [45.598]] [[1.772]
 [1.901]
 [1.772]
 [1.772]
 [1.772]
 [1.772]
 [1.772]]
printing an ep nov before normalisation:  29.751802477392417
printing an ep nov before normalisation:  37.84298581137217
printing an ep nov before normalisation:  37.195877760261475
actor:  1 policy actor:  1  step number:  45 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  50 total reward:  0.42000000000000004  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17044666666666652 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.17044666666666652 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  45.438726549572216
maxi score, test score, baseline:  0.17044666666666652 0.6953333333333334 0.6953333333333334
Printing some Q and Qe and total Qs values:  [[0.043]
 [0.136]
 [0.043]
 [0.043]
 [0.043]
 [0.043]
 [0.043]] [[32.303]
 [37.492]
 [32.303]
 [32.303]
 [32.303]
 [32.303]
 [32.303]] [[0.281]
 [0.449]
 [0.281]
 [0.281]
 [0.281]
 [0.281]
 [0.281]]
printing an ep nov before normalisation:  33.992873811073004
actor:  1 policy actor:  1  step number:  34 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17044666666666652 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
Printing some Q and Qe and total Qs values:  [[0.133]
 [0.235]
 [0.133]
 [0.133]
 [0.133]
 [0.133]
 [0.133]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.133]
 [0.235]
 [0.133]
 [0.133]
 [0.133]
 [0.133]
 [0.133]]
actor:  1 policy actor:  1  step number:  56 total reward:  0.33999999999999997  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  50.524256121050115
Printing some Q and Qe and total Qs values:  [[0.362]
 [0.434]
 [0.383]
 [0.387]
 [0.39 ]
 [0.383]
 [0.401]] [[30.016]
 [33.203]
 [31.541]
 [29.627]
 [29.757]
 [31.541]
 [31.399]] [[0.609]
 [0.732]
 [0.655]
 [0.628]
 [0.633]
 [0.655]
 [0.671]]
actions average: 
K:  3  action  0 :  tensor([0.6329, 0.0033, 0.0503, 0.0431, 0.1557, 0.0667, 0.0479],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0096, 0.8967, 0.0079, 0.0283, 0.0070, 0.0077, 0.0428],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0659, 0.0288, 0.6055, 0.0622, 0.0679, 0.1078, 0.0619],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1409, 0.0636, 0.0967, 0.3030, 0.1435, 0.1207, 0.1316],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1830, 0.0035, 0.1179, 0.1300, 0.3279, 0.1190, 0.1187],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1135, 0.0009, 0.0911, 0.1000, 0.1183, 0.4983, 0.0779],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2030, 0.0366, 0.0974, 0.1203, 0.1209, 0.0963, 0.3255],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]] [[39.621]
 [39.621]
 [39.621]
 [39.621]
 [39.621]
 [39.621]
 [39.621]] [[0.77]
 [0.77]
 [0.77]
 [0.77]
 [0.77]
 [0.77]
 [0.77]]
maxi score, test score, baseline:  0.16783333333333317 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  0 policy actor:  0  step number:  40 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  22.280478202224245
maxi score, test score, baseline:  0.16772666666666652 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
line 256 mcts: sample exp_bonus 37.48209232410739
maxi score, test score, baseline:  0.16772666666666652 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  0.0
actor:  1 policy actor:  1  step number:  48 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.16772666666666652 0.6953333333333334 0.6953333333333334
printing an ep nov before normalisation:  42.71422553963998
maxi score, test score, baseline:  0.16772666666666652 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  38.72551236833845
maxi score, test score, baseline:  0.16772666666666652 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  40 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  63 total reward:  0.25333333333333286  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  47.616052624293985
maxi score, test score, baseline:  0.16772666666666652 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  40 total reward:  0.6200000000000003  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7593471
printing an ep nov before normalisation:  47.20652553513787
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.745130491498784
maxi score, test score, baseline:  0.16772666666666652 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  0.0001657503480601008
actor:  1 policy actor:  1  step number:  51 total reward:  0.25333333333333274  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  37.481555321955724
printing an ep nov before normalisation:  35.43101563133182
printing an ep nov before normalisation:  35.67532049864942
printing an ep nov before normalisation:  1.0899629585080106
printing an ep nov before normalisation:  40.20719163689324
siam score:  -0.758007
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.421915690104168
maxi score, test score, baseline:  0.16772666666666652 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.16772666666666652 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  32.28714466094971
maxi score, test score, baseline:  0.16772666666666652 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.16772666666666652 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.16772666666666652 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  64 total reward:  0.2599999999999999  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  46.92739498646358
maxi score, test score, baseline:  0.16772666666666652 0.6953333333333334 0.6953333333333334
maxi score, test score, baseline:  0.16772666666666652 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
Printing some Q and Qe and total Qs values:  [[0.81 ]
 [0.904]
 [0.716]
 [0.774]
 [0.803]
 [0.772]
 [0.837]] [[24.717]
 [19.427]
 [23.863]
 [25.571]
 [24.034]
 [24.16 ]
 [25.016]] [[1.968]
 [1.814]
 [1.833]
 [1.971]
 [1.929]
 [1.903]
 [2.009]]
Printing some Q and Qe and total Qs values:  [[0.275]
 [0.275]
 [0.275]
 [0.275]
 [0.275]
 [0.275]
 [0.275]] [[50.579]
 [50.579]
 [50.579]
 [50.579]
 [50.579]
 [50.579]
 [50.579]] [[1.127]
 [1.127]
 [1.127]
 [1.127]
 [1.127]
 [1.127]
 [1.127]]
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  58 total reward:  0.17999999999999972  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  54.42962722851067
maxi score, test score, baseline:  0.16772666666666652 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  48.26174404600498
maxi score, test score, baseline:  0.16772666666666652 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
siam score:  -0.75972307
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.47545379215875
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.698]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]] [[39.415]
 [39.147]
 [39.415]
 [39.415]
 [39.415]
 [39.415]
 [39.415]] [[0.882]
 [0.986]
 [0.882]
 [0.882]
 [0.882]
 [0.882]
 [0.882]]
printing an ep nov before normalisation:  70.14617712307626
maxi score, test score, baseline:  0.16772666666666652 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  0 policy actor:  1  step number:  57 total reward:  0.10666666666666602  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  55 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  60 total reward:  0.059999999999999276  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  0.0
actor:  1 policy actor:  1  step number:  51 total reward:  0.27999999999999936  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1673533333333332 0.6953333333333334 0.6953333333333334
printing an ep nov before normalisation:  34.20842747839033
printing an ep nov before normalisation:  36.34067464147479
maxi score, test score, baseline:  0.1673533333333332 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  60.79788377131532
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.437]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]] [[51.308]
 [47.6  ]
 [50.648]
 [50.648]
 [50.648]
 [50.648]
 [50.648]] [[2.084]
 [1.883]
 [2.037]
 [2.037]
 [2.037]
 [2.037]
 [2.037]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.89 ]
 [0.972]
 [0.89 ]
 [0.89 ]
 [0.89 ]
 [0.89 ]
 [0.89 ]] [[43.42 ]
 [42.277]
 [43.42 ]
 [43.42 ]
 [43.42 ]
 [43.42 ]
 [43.42 ]] [[0.89 ]
 [0.972]
 [0.89 ]
 [0.89 ]
 [0.89 ]
 [0.89 ]
 [0.89 ]]
printing an ep nov before normalisation:  26.789250373840332
printing an ep nov before normalisation:  39.05244500263825
printing an ep nov before normalisation:  56.29369258880615
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1673533333333332 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.1673533333333332 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  25.519860256805416
actor:  0 policy actor:  0  step number:  48 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  20.005516217390763
Printing some Q and Qe and total Qs values:  [[0.554]
 [0.573]
 [0.549]
 [0.542]
 [0.551]
 [0.543]
 [0.538]] [[31.382]
 [31.06 ]
 [31.282]
 [31.612]
 [31.374]
 [31.527]
 [31.75 ]] [[1.988]
 [1.978]
 [1.974]
 [1.997]
 [1.984]
 [1.991]
 [2.006]]
actor:  1 policy actor:  1  step number:  61 total reward:  0.2666666666666657  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  60 total reward:  0.16666666666666607  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.16765999999999984 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  49.17166554403269
maxi score, test score, baseline:  0.16765999999999984 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.16765999999999984 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  0 policy actor:  1  step number:  54 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1678466666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  61 total reward:  0.37333333333333285  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1656866666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  57.74447504143177
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.66254610196593
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.666]
 [0.635]
 [0.655]
 [0.643]
 [0.569]
 [0.569]] [[32.757]
 [39.072]
 [37.035]
 [39.206]
 [39.312]
 [32.757]
 [32.757]] [[1.381]
 [1.895]
 [1.73 ]
 [1.893]
 [1.888]
 [1.381]
 [1.381]]
printing an ep nov before normalisation:  43.43944072723389
maxi score, test score, baseline:  0.1656866666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  39.275962254977266
using explorer policy with actor:  1
printing an ep nov before normalisation:  16.996310054358226
actor:  1 policy actor:  1  step number:  54 total reward:  0.5133333333333339  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  40.14422087434308
maxi score, test score, baseline:  0.1656866666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  34.055213432160514
actor:  1 policy actor:  1  step number:  48 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  23.5699725151062
maxi score, test score, baseline:  0.1656866666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  38.29786615125458
printing an ep nov before normalisation:  46.30767315724763
printing an ep nov before normalisation:  65.21901341313888
printing an ep nov before normalisation:  50.45151233673096
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.79 ]
 [0.775]
 [0.652]
 [0.769]
 [0.787]
 [0.652]
 [0.763]] [[48.53 ]
 [49.548]
 [47.609]
 [50.106]
 [43.78 ]
 [47.609]
 [52.599]] [[0.79 ]
 [0.775]
 [0.652]
 [0.769]
 [0.787]
 [0.652]
 [0.763]]
printing an ep nov before normalisation:  40.161623756116335
maxi score, test score, baseline:  0.1656866666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  64 total reward:  0.23333333333333262  reward:  1.0 rdn_beta:  1.333
UNIT TEST: sample policy line 217 mcts : [0.347 0.224 0.102 0.082 0.061 0.061 0.122]
printing an ep nov before normalisation:  46.36436642077724
actor:  0 policy actor:  0  step number:  48 total reward:  0.3533333333333333  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  42.24001153774977
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.787]
 [0.822]
 [0.751]
 [0.751]
 [0.799]
 [0.76 ]
 [0.756]] [[42.018]
 [43.953]
 [39.779]
 [39.935]
 [41.636]
 [40.131]
 [40.263]] [[0.787]
 [0.822]
 [0.751]
 [0.751]
 [0.799]
 [0.76 ]
 [0.756]]
actor:  1 policy actor:  1  step number:  62 total reward:  0.35333333333333317  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  1  step number:  53 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  48 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.173]
 [0.077]
 [0.016]
 [0.077]
 [0.077]
 [0.016]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.173]
 [0.077]
 [0.016]
 [0.077]
 [0.077]
 [0.016]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  0.1660466666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.1660466666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.1660466666666665 0.6953333333333334 0.6953333333333334
maxi score, test score, baseline:  0.1660466666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  33.073389942703976
maxi score, test score, baseline:  0.1660466666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  32.73404727267916
maxi score, test score, baseline:  0.1660466666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.1660466666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.1660466666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  24.3973348332473
printing an ep nov before normalisation:  30.154841690351475
actor:  0 policy actor:  1  step number:  50 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.16867333333333318 0.6953333333333334 0.6953333333333334
actor:  0 policy actor:  0  step number:  39 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.16837999999999984 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  43 total reward:  0.48  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.16837999999999984 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  59 total reward:  0.41333333333333355  reward:  1.0 rdn_beta:  1.667
from probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  49.776565983758914
maxi score, test score, baseline:  0.16837999999999984 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  63 total reward:  0.11999999999999955  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.16561999999999985 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  50 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  1.0
siam score:  -0.764989
maxi score, test score, baseline:  0.16561999999999985 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  26.056261137548354
printing an ep nov before normalisation:  18.572974474980768
maxi score, test score, baseline:  0.16561999999999985 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  19.652212032654912
maxi score, test score, baseline:  0.16561999999999985 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  56 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  1.667
from probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
Printing some Q and Qe and total Qs values:  [[0.687]
 [0.739]
 [0.621]
 [0.638]
 [0.674]
 [0.641]
 [0.609]] [[32.209]
 [34.591]
 [29.193]
 [31.812]
 [36.36 ]
 [30.489]
 [33.88 ]] [[0.687]
 [0.739]
 [0.621]
 [0.638]
 [0.674]
 [0.641]
 [0.609]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  36.31748331757442
maxi score, test score, baseline:  0.16345999999999983 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  66.08230508750825
maxi score, test score, baseline:  0.16140666666666653 0.6953333333333334 0.6953333333333334
actor:  1 policy actor:  1  step number:  48 total reward:  0.3533333333333333  reward:  1.0 rdn_beta:  1.333
from probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  42.86102743522698
actor:  0 policy actor:  0  step number:  47 total reward:  0.4133333333333328  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.16423333333333318 0.6953333333333334 0.6953333333333334
printing an ep nov before normalisation:  37.2330379486084
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.067444798091394
maxi score, test score, baseline:  0.16423333333333318 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
line 256 mcts: sample exp_bonus 33.73861995254047
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]] [[38.114]
 [38.87 ]
 [38.87 ]
 [38.87 ]
 [38.87 ]
 [38.87 ]
 [38.87 ]] [[2.562]
 [2.623]
 [2.623]
 [2.623]
 [2.623]
 [2.623]
 [2.623]]
actor:  1 policy actor:  1  step number:  56 total reward:  0.27333333333333276  reward:  1.0 rdn_beta:  1.333
siam score:  -0.76857054
maxi score, test score, baseline:  0.16423333333333318 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
Printing some Q and Qe and total Qs values:  [[0.847]
 [0.949]
 [0.863]
 [0.832]
 [0.839]
 [0.834]
 [0.836]] [[48.757]
 [34.072]
 [45.946]
 [46.502]
 [48.057]
 [48.661]
 [46.172]] [[0.847]
 [0.949]
 [0.863]
 [0.832]
 [0.839]
 [0.834]
 [0.836]]
actor:  1 policy actor:  1  step number:  50 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  39.214646109756096
maxi score, test score, baseline:  0.16423333333333318 0.6953333333333334 0.6953333333333334
maxi score, test score, baseline:  0.16423333333333318 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  0 policy actor:  0  step number:  36 total reward:  0.5933333333333335  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.16493999999999984 0.6953333333333334 0.6953333333333334
maxi score, test score, baseline:  0.16493999999999984 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  46 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.16493999999999984 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
Printing some Q and Qe and total Qs values:  [[0.731]
 [0.751]
 [0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]] [[32.334]
 [31.117]
 [32.334]
 [32.334]
 [32.334]
 [32.334]
 [32.334]] [[1.389]
 [1.361]
 [1.389]
 [1.389]
 [1.389]
 [1.389]
 [1.389]]
maxi score, test score, baseline:  0.16493999999999984 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.459]
 [0.32 ]
 [0.32 ]
 [0.413]
 [0.426]
 [0.32 ]] [[46.556]
 [45.851]
 [40.572]
 [40.572]
 [46.71 ]
 [47.083]
 [40.572]] [[1.229]
 [1.253]
 [0.924]
 [0.924]
 [1.238]
 [1.264]
 [0.924]]
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([0.4359, 0.1187, 0.0595, 0.0711, 0.1724, 0.0621, 0.0803],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0168, 0.8937, 0.0229, 0.0186, 0.0052, 0.0083, 0.0345],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0960, 0.0043, 0.3579, 0.1012, 0.1272, 0.2213, 0.0922],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1206, 0.0674, 0.1220, 0.1843, 0.1514, 0.2226, 0.1316],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2056, 0.0222, 0.0944, 0.1003, 0.4076, 0.0877, 0.0822],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1398, 0.0272, 0.1413, 0.1093, 0.1316, 0.3440, 0.1068],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1567, 0.1150, 0.1289, 0.1519, 0.1471, 0.1196, 0.1808],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.16493999999999984 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
line 256 mcts: sample exp_bonus 27.21035538670133
printing an ep nov before normalisation:  33.21005922966856
printing an ep nov before normalisation:  37.04238394711653
maxi score, test score, baseline:  0.16493999999999984 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  61 total reward:  0.09333333333333305  reward:  1.0 rdn_beta:  0.667
from probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.16493999999999984 0.6953333333333334 0.6953333333333334
maxi score, test score, baseline:  0.16493999999999984 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  51 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  53 total reward:  0.35999999999999943  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.16493999999999984 0.6953333333333334 0.6953333333333334
Printing some Q and Qe and total Qs values:  [[0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.816]] [[41.726]
 [41.726]
 [41.726]
 [41.726]
 [41.726]
 [41.726]
 [41.726]] [[0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.816]]
printing an ep nov before normalisation:  35.81240177154541
printing an ep nov before normalisation:  28.02949470271555
actor:  0 policy actor:  0  step number:  41 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  59 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.32172749118462
printing an ep nov before normalisation:  33.14983222665952
actor:  0 policy actor:  0  step number:  48 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  42.968677551368266
Printing some Q and Qe and total Qs values:  [[0.618]
 [0.352]
 [0.775]
 [0.721]
 [0.687]
 [0.737]
 [0.604]] [[51.313]
 [42.487]
 [43.377]
 [ 0.   ]
 [47.604]
 [47.563]
 [44.271]] [[0.618]
 [0.352]
 [0.775]
 [0.721]
 [0.687]
 [0.737]
 [0.604]]
printing an ep nov before normalisation:  46.58348249051643
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  37.38576004561796
printing an ep nov before normalisation:  39.15637016296387
maxi score, test score, baseline:  0.16495333333333317 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  0.0001844439560727551
maxi score, test score, baseline:  0.16495333333333317 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.16495333333333317 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  57 total reward:  0.22666666666666624  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  29.39189806084607
maxi score, test score, baseline:  0.16495333333333317 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  0 policy actor:  0  step number:  39 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  50 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  2.0
using another actor
maxi score, test score, baseline:  0.16535333333333319 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
Printing some Q and Qe and total Qs values:  [[0.194]
 [0.233]
 [0.194]
 [0.194]
 [0.194]
 [0.194]
 [0.194]] [[45.678]
 [44.221]
 [45.678]
 [45.678]
 [45.678]
 [45.678]
 [45.678]] [[0.724]
 [0.731]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]]
printing an ep nov before normalisation:  33.41116088959289
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]] [[50.765]
 [50.765]
 [50.765]
 [50.765]
 [50.765]
 [50.765]
 [50.765]] [[0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]]
printing an ep nov before normalisation:  43.74973839502894
actor:  1 policy actor:  1  step number:  58 total reward:  0.21999999999999986  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.16535333333333319 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  33.13864159661602
printing an ep nov before normalisation:  45.837426782118236
printing an ep nov before normalisation:  43.08496805750469
actor:  0 policy actor:  1  step number:  41 total reward:  0.5466666666666667  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  43.36850467805892
maxi score, test score, baseline:  0.1658466666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.1658466666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  70 total reward:  0.16666666666666574  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 38.646780775673626
printing an ep nov before normalisation:  29.657037258148193
maxi score, test score, baseline:  0.1658466666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  37.45211362838745
printing an ep nov before normalisation:  36.01887700375725
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1658466666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  46 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1658466666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.1658466666666665 0.6953333333333334 0.6953333333333334
maxi score, test score, baseline:  0.1658466666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.1658466666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
from probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.173]
 [0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.014]
 [0.173]
 [0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]]
maxi score, test score, baseline:  0.1658466666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  49 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  46.828869021844405
printing an ep nov before normalisation:  42.414311722703395
Printing some Q and Qe and total Qs values:  [[0.669]
 [0.69 ]
 [0.613]
 [0.604]
 [0.615]
 [0.604]
 [0.61 ]] [[33.611]
 [33.152]
 [35.069]
 [36.538]
 [36.28 ]
 [36.254]
 [34.839]] [[0.669]
 [0.69 ]
 [0.613]
 [0.604]
 [0.615]
 [0.604]
 [0.61 ]]
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]] [[43.595]
 [43.595]
 [43.595]
 [43.595]
 [43.595]
 [43.595]
 [43.595]] [[0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]]
printing an ep nov before normalisation:  50.49909567623581
printing an ep nov before normalisation:  33.37814807891846
maxi score, test score, baseline:  0.1658466666666665 0.6953333333333334 0.6953333333333334
maxi score, test score, baseline:  0.1658466666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  42.08299160003662
maxi score, test score, baseline:  0.1658466666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.1658466666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  44 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  31.628313234978652
actor:  1 policy actor:  1  step number:  55 total reward:  0.25333333333333274  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  53 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  1.333
deleting a thread, now have 4 threads
Frames:  237024 train batches done:  27764 episodes:  6100
printing an ep nov before normalisation:  32.45735034740954
actor:  0 policy actor:  0  step number:  38 total reward:  0.5133333333333332  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  40 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1692066666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
Printing some Q and Qe and total Qs values:  [[0.597]
 [0.654]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]] [[32.51 ]
 [36.108]
 [32.51 ]
 [32.51 ]
 [32.51 ]
 [32.51 ]
 [32.51 ]] [[0.878]
 [0.987]
 [0.878]
 [0.878]
 [0.878]
 [0.878]
 [0.878]]
maxi score, test score, baseline:  0.1692066666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  30 total reward:  0.6466666666666668  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.62 ]
 [0.331]
 [0.535]
 [0.535]
 [0.535]
 [0.598]] [[31.28 ]
 [32.899]
 [39.301]
 [31.28 ]
 [31.28 ]
 [31.28 ]
 [25.448]] [[0.751]
 [0.854]
 [0.637]
 [0.751]
 [0.751]
 [0.751]
 [0.748]]
maxi score, test score, baseline:  0.1692066666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.1692066666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
Printing some Q and Qe and total Qs values:  [[0.869]
 [0.869]
 [0.869]
 [0.869]
 [0.869]
 [0.891]
 [0.869]] [[35.102]
 [35.102]
 [35.102]
 [35.102]
 [35.102]
 [46.304]
 [35.102]] [[1.084]
 [1.084]
 [1.084]
 [1.084]
 [1.084]
 [1.224]
 [1.084]]
maxi score, test score, baseline:  0.1692066666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.1692066666666665 0.6953333333333334 0.6953333333333334
maxi score, test score, baseline:  0.1692066666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
siam score:  -0.7564315
maxi score, test score, baseline:  0.1692066666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  70 total reward:  0.19333333333333225  reward:  1.0 rdn_beta:  0.333
deleting a thread, now have 3 threads
Frames:  237202 train batches done:  27786 episodes:  6104
maxi score, test score, baseline:  0.1692066666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.1692066666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.1692066666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.1692066666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.1692066666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.1692066666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.1692066666666665 0.6953333333333334 0.6953333333333334
siam score:  -0.75887305
maxi score, test score, baseline:  0.1692066666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.1692066666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.1692066666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.981788297068423
siam score:  -0.7600578
maxi score, test score, baseline:  0.1692066666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
Printing some Q and Qe and total Qs values:  [[0.203]
 [0.231]
 [0.203]
 [0.203]
 [0.203]
 [0.203]
 [0.203]] [[43.76 ]
 [43.383]
 [43.76 ]
 [43.76 ]
 [43.76 ]
 [43.76 ]
 [43.76 ]] [[1.217]
 [1.232]
 [1.217]
 [1.217]
 [1.217]
 [1.217]
 [1.217]]
printing an ep nov before normalisation:  41.855764280975336
printing an ep nov before normalisation:  41.065997848138714
printing an ep nov before normalisation:  30.116424560546875
maxi score, test score, baseline:  0.1692066666666665 0.6953333333333334 0.6953333333333334
maxi score, test score, baseline:  0.1692066666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  35.95257622744082
maxi score, test score, baseline:  0.1692066666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  28.926493798454665
maxi score, test score, baseline:  0.1692066666666665 0.6953333333333334 0.6953333333333334
maxi score, test score, baseline:  0.1692066666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  25.750425980513533
actor:  1 policy actor:  1  step number:  54 total reward:  0.39333333333333276  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  0.0001364705519790732
actor:  1 policy actor:  1  step number:  57 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  44.164783900741966
printing an ep nov before normalisation:  0.003953713985538343
actor:  1 policy actor:  1  step number:  66 total reward:  0.07333333333333258  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  39.07350686029066
printing an ep nov before normalisation:  39.79409743387664
Printing some Q and Qe and total Qs values:  [[0.066]
 [0.124]
 [0.07 ]
 [0.066]
 [0.068]
 [0.07 ]
 [0.065]] [[29.165]
 [32.432]
 [29.378]
 [29.988]
 [29.545]
 [30.655]
 [30.337]] [[0.584]
 [0.766]
 [0.597]
 [0.615]
 [0.601]
 [0.645]
 [0.628]]
maxi score, test score, baseline:  0.1692066666666665 0.6953333333333334 0.6953333333333334
maxi score, test score, baseline:  0.1692066666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.1692066666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  30.49649900409991
actor:  1 policy actor:  1  step number:  44 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  29.16327476487485
maxi score, test score, baseline:  0.1692066666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  26.814142322988893
actor:  1 policy actor:  1  step number:  61 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1692066666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.1692066666666665 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  39.30323535500637
printing an ep nov before normalisation:  41.94780255148168
printing an ep nov before normalisation:  35.51122264213023
using explorer policy with actor:  1
printing an ep nov before normalisation:  24.828694161121476
maxi score, test score, baseline:  0.1692066666666665 0.6953333333333334 0.6953333333333334
printing an ep nov before normalisation:  23.247442079373634
printing an ep nov before normalisation:  31.45207748090222
actor:  1 policy actor:  1  step number:  48 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.027]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]] [[35.533]
 [35.533]
 [35.533]
 [35.533]
 [35.533]
 [35.533]
 [35.533]] [[0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]]
actor:  1 policy actor:  1  step number:  60 total reward:  0.36666666666666614  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  37.9383827576695
printing an ep nov before normalisation:  51.515526976468855
printing an ep nov before normalisation:  34.555760353594415
maxi score, test score, baseline:  0.16623333333333318 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.503]
 [0.418]
 [0.408]
 [0.426]
 [0.427]
 [0.44 ]] [[32.623]
 [30.771]
 [32.022]
 [32.452]
 [31.716]
 [31.587]
 [31.184]] [[1.34 ]
 [1.341]
 [1.32 ]
 [1.331]
 [1.312]
 [1.306]
 [1.298]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  39 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  33.11813937293159
maxi score, test score, baseline:  0.16623333333333318 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actions average: 
K:  0  action  0 :  tensor([0.5496, 0.0306, 0.0649, 0.0693, 0.1078, 0.0614, 0.1163],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0242,     0.9449,     0.0092,     0.0041,     0.0009,     0.0020,
            0.0146], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0501, 0.0039, 0.7569, 0.0317, 0.0363, 0.0807, 0.0403],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1181, 0.0333, 0.0912, 0.4269, 0.0883, 0.0815, 0.1606],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1980, 0.0033, 0.0847, 0.0910, 0.4527, 0.0924, 0.0780],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1651, 0.0584, 0.1549, 0.1358, 0.1343, 0.2197, 0.1318],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1945, 0.0053, 0.1391, 0.1201, 0.1272, 0.1154, 0.2984],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.16623333333333318 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  31.168926633387223
actor:  1 policy actor:  1  step number:  41 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.16623333333333318 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
Printing some Q and Qe and total Qs values:  [[0.646]
 [0.671]
 [0.677]
 [0.582]
 [0.582]
 [0.579]
 [0.646]] [[29.352]
 [30.787]
 [28.924]
 [29.688]
 [30.108]
 [29.894]
 [28.164]] [[1.28 ]
 [1.363]
 [1.293]
 [1.229]
 [1.246]
 [1.235]
 [1.23 ]]
maxi score, test score, baseline:  0.16623333333333318 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  29.769511323999314
maxi score, test score, baseline:  0.16623333333333318 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
Printing some Q and Qe and total Qs values:  [[ 0.02 ]
 [ 0.172]
 [ 0.02 ]
 [ 0.02 ]
 [ 0.02 ]
 [-0.147]
 [ 0.02 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.02 ]
 [ 0.172]
 [ 0.02 ]
 [ 0.02 ]
 [ 0.02 ]
 [-0.147]
 [ 0.02 ]]
maxi score, test score, baseline:  0.16623333333333318 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  41 total reward:  0.42666666666666664  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.16623333333333318 0.6953333333333334 0.6953333333333334
maxi score, test score, baseline:  0.16623333333333318 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  55.49677312565566
Printing some Q and Qe and total Qs values:  [[0.803]
 [0.92 ]
 [0.803]
 [0.803]
 [0.803]
 [0.803]
 [0.803]] [[61.531]
 [56.398]
 [61.531]
 [61.531]
 [61.531]
 [61.531]
 [61.531]] [[0.803]
 [0.92 ]
 [0.803]
 [0.803]
 [0.803]
 [0.803]
 [0.803]]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.960589541553546
printing an ep nov before normalisation:  29.51554635635405
maxi score, test score, baseline:  0.16623333333333318 0.6953333333333334 0.6953333333333334
maxi score, test score, baseline:  0.16623333333333318 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.16623333333333318 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  0 policy actor:  1  step number:  48 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.16665999999999984 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.16665999999999984 0.6953333333333334 0.6953333333333334
maxi score, test score, baseline:  0.16665999999999984 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.16665999999999984 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  42.07355408390619
maxi score, test score, baseline:  0.16665999999999984 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.16665999999999984 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  39.73825270508775
printing an ep nov before normalisation:  26.59811496734619
actor:  0 policy actor:  1  step number:  42 total reward:  0.5400000000000003  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.034]
 [0.034]
 [0.034]
 [0.034]
 [0.034]
 [0.034]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.034]
 [0.034]
 [0.034]
 [0.034]
 [0.034]
 [0.034]
 [0.034]]
maxi score, test score, baseline:  0.16973999999999986 0.6953333333333334 0.6953333333333334
printing an ep nov before normalisation:  40.766859342589946
maxi score, test score, baseline:  0.16973999999999986 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  0.0
printing an ep nov before normalisation:  33.55030507941107
maxi score, test score, baseline:  0.16973999999999986 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  49.91688988011125
printing an ep nov before normalisation:  32.29380688616315
printing an ep nov before normalisation:  30.649802684783936
Printing some Q and Qe and total Qs values:  [[0.614]
 [0.567]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]] [[36.668]
 [48.376]
 [41.982]
 [41.982]
 [41.982]
 [41.982]
 [41.982]] [[0.887]
 [1.054]
 [0.951]
 [0.951]
 [0.951]
 [0.951]
 [0.951]]
maxi score, test score, baseline:  0.16973999999999986 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.16973999999999986 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
Printing some Q and Qe and total Qs values:  [[0.55 ]
 [0.58 ]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]] [[25.679]
 [24.598]
 [24.738]
 [24.738]
 [24.738]
 [24.738]
 [24.738]] [[1.161]
 [1.142]
 [1.12 ]
 [1.12 ]
 [1.12 ]
 [1.12 ]
 [1.12 ]]
actor:  1 policy actor:  1  step number:  58 total reward:  0.28666666666666585  reward:  1.0 rdn_beta:  0.667
from probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.16973999999999986 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actions average: 
K:  1  action  0 :  tensor([0.2991, 0.0063, 0.1205, 0.1229, 0.2122, 0.1200, 0.1190],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0092, 0.9597, 0.0019, 0.0033, 0.0011, 0.0011, 0.0238],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1224, 0.0063, 0.4143, 0.0918, 0.0947, 0.1812, 0.0894],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2008, 0.0056, 0.0657, 0.3072, 0.2198, 0.0952, 0.1057],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1811, 0.0028, 0.1077, 0.1368, 0.3437, 0.1297, 0.0982],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1152, 0.0045, 0.1042, 0.0819, 0.0805, 0.5371, 0.0765],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1156, 0.2581, 0.1003, 0.0498, 0.0384, 0.0449, 0.3929],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  17.18425202419215
printing an ep nov before normalisation:  12.820683312636508
Printing some Q and Qe and total Qs values:  [[0.247]
 [0.431]
 [0.247]
 [0.247]
 [0.247]
 [0.247]
 [0.247]] [[28.623]
 [27.24 ]
 [28.623]
 [28.623]
 [28.623]
 [28.623]
 [28.623]] [[1.475]
 [1.566]
 [1.475]
 [1.475]
 [1.475]
 [1.475]
 [1.475]]
actor:  1 policy actor:  1  step number:  56 total reward:  0.24666666666666648  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.16973999999999986 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  26.918251075167397
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.306114673614502
maxi score, test score, baseline:  0.16973999999999986 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
Printing some Q and Qe and total Qs values:  [[0.405]
 [0.452]
 [0.322]
 [0.325]
 [0.405]
 [0.405]
 [0.405]] [[32.85 ]
 [31.551]
 [33.929]
 [36.865]
 [32.85 ]
 [32.85 ]
 [32.85 ]] [[1.191]
 [1.178]
 [1.16 ]
 [1.299]
 [1.191]
 [1.191]
 [1.191]]
actor:  0 policy actor:  0  step number:  58 total reward:  0.15333333333333277  reward:  1.0 rdn_beta:  0.333
using another actor
maxi score, test score, baseline:  0.17204666666666654 0.6953333333333334 0.6953333333333334
actor:  1 policy actor:  1  step number:  81 total reward:  0.13333333333333208  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17204666666666654 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.17204666666666654 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
using explorer policy with actor:  0
printing an ep nov before normalisation:  42.61082189147316
printing an ep nov before normalisation:  34.68023157533673
siam score:  -0.76294065
actions average: 
K:  2  action  0 :  tensor([0.6023, 0.0154, 0.0571, 0.0598, 0.1168, 0.0742, 0.0744],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0152, 0.9207, 0.0092, 0.0098, 0.0072, 0.0066, 0.0313],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1480, 0.0049, 0.4294, 0.0876, 0.0981, 0.1178, 0.1141],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1711, 0.0762, 0.1449, 0.2634, 0.1036, 0.0891, 0.1518],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2365, 0.0368, 0.1360, 0.1085, 0.2360, 0.1239, 0.1224],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1036, 0.0334, 0.1034, 0.0786, 0.0759, 0.4985, 0.1066],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1298, 0.1572, 0.1698, 0.0858, 0.0838, 0.1179, 0.2557],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  28.387075909400277
actor:  0 policy actor:  1  step number:  40 total reward:  0.5266666666666667  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 36.35075056726751
actor:  1 policy actor:  1  step number:  74 total reward:  0.12666666666666537  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  38.55884671125761
maxi score, test score, baseline:  0.17228666666666653 0.6953333333333334 0.6953333333333334
maxi score, test score, baseline:  0.17228666666666653 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  31.81165276544571
maxi score, test score, baseline:  0.17228666666666653 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17228666666666653 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  32.17399202137031
printing an ep nov before normalisation:  24.05787293968695
actor:  1 policy actor:  1  step number:  43 total reward:  0.5466666666666669  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  56 total reward:  0.1666666666666663  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.15 ]
 [0.218]
 [0.15 ]
 [0.15 ]
 [0.15 ]
 [0.15 ]
 [0.15 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.15 ]
 [0.218]
 [0.15 ]
 [0.15 ]
 [0.15 ]
 [0.15 ]
 [0.15 ]]
maxi score, test score, baseline:  0.17228666666666653 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  41.720592502014924
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  0.17228666666666653 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.17228666666666653 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  54.607852513626334
actor:  0 policy actor:  1  step number:  35 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17329999999999987 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  42.6608393465088
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
printing an ep nov before normalisation:  35.15742146094204
maxi score, test score, baseline:  0.17329999999999987 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  23.258946062624837
printing an ep nov before normalisation:  0.003151765849906951
actor:  1 policy actor:  1  step number:  41 total reward:  0.5866666666666669  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  62 total reward:  0.12666666666666615  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17329999999999987 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.266]
 [0.272]
 [0.266]
 [0.266]
 [0.266]
 [0.247]
 [0.266]] [[30.78 ]
 [33.819]
 [30.78 ]
 [30.78 ]
 [30.78 ]
 [29.021]
 [30.78 ]] [[0.898]
 [1.06 ]
 [0.898]
 [0.898]
 [0.898]
 [0.789]
 [0.898]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.83726879404172
printing an ep nov before normalisation:  40.071866359295136
printing an ep nov before normalisation:  43.36448716205797
actor:  1 policy actor:  1  step number:  54 total reward:  0.40666666666666673  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.38 ]
 [0.4  ]
 [0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]] [[27.88 ]
 [24.782]
 [25.528]
 [25.528]
 [25.528]
 [25.528]
 [25.528]] [[1.306]
 [1.104]
 [1.084]
 [1.084]
 [1.084]
 [1.084]
 [1.084]]
actions average: 
K:  4  action  0 :  tensor([0.4141, 0.0700, 0.0863, 0.1089, 0.1106, 0.0967, 0.1135],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0072, 0.9438, 0.0060, 0.0126, 0.0090, 0.0098, 0.0117],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0003,     0.0006,     0.6266,     0.0073,     0.0008,     0.3578,
            0.0066], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1704, 0.0094, 0.1088, 0.2460, 0.1774, 0.1726, 0.1153],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1447, 0.0879, 0.0272, 0.0825, 0.6002, 0.0347, 0.0229],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0905, 0.1012, 0.1334, 0.1068, 0.1011, 0.3853, 0.0817],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1271, 0.0185, 0.1093, 0.1595, 0.1425, 0.1795, 0.2635],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.17329999999999987 0.6953333333333334 0.6953333333333334
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17329999999999987 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.17329999999999987 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  58 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17329999999999987 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  64 total reward:  0.486666666666667  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17329999999999987 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  46.144682402165124
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17329999999999987 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.17329999999999987 0.6953333333333334 0.6953333333333334
actor:  1 policy actor:  1  step number:  61 total reward:  0.3333333333333328  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17329999999999987 0.6953333333333334 0.6953333333333334
printing an ep nov before normalisation:  37.668586883603204
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17329999999999987 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  53 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.17329999999999987 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  50 total reward:  0.33999999999999997  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  40.490498542785645
maxi score, test score, baseline:  0.17329999999999987 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.17329999999999987 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  34.59798820814449
printing an ep nov before normalisation:  37.305865920343024
maxi score, test score, baseline:  0.17329999999999987 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  54 total reward:  0.44666666666666666  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17329999999999987 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  42 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  1.333
using another actor
printing an ep nov before normalisation:  30.888543128967285
maxi score, test score, baseline:  0.17329999999999987 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.17329999999999987 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
siam score:  -0.75864327
maxi score, test score, baseline:  0.17329999999999987 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  45 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17097999999999988 0.6953333333333334 0.6953333333333334
actor:  1 policy actor:  1  step number:  64 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17097999999999988 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.17097999999999988 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  41.51739000284511
maxi score, test score, baseline:  0.17097999999999988 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.17097999999999988 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  35.16174326391845
printing an ep nov before normalisation:  38.926472663879395
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  0.17097999999999988 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  43 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  48 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  0
maxi score, test score, baseline:  0.17352666666666652 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17352666666666652 0.6953333333333334 0.6953333333333334
actor:  1 policy actor:  1  step number:  60 total reward:  0.23333333333333317  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17352666666666652 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
Printing some Q and Qe and total Qs values:  [[0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.775]] [[34.244]
 [34.244]
 [34.244]
 [34.244]
 [34.244]
 [34.244]
 [34.244]] [[0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.775]]
printing an ep nov before normalisation:  36.71928579233895
maxi score, test score, baseline:  0.17352666666666652 0.6953333333333334 0.6953333333333334
actor:  0 policy actor:  1  step number:  39 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 35.03116960836256
maxi score, test score, baseline:  0.1739933333333332 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  53 total reward:  0.4800000000000002  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1739933333333332 0.6953333333333334 0.6953333333333334
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  49 total reward:  0.5066666666666669  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1739933333333332 0.6953333333333334 0.6953333333333334
line 256 mcts: sample exp_bonus 27.472678292874374
maxi score, test score, baseline:  0.1739933333333332 0.6953333333333334 0.6953333333333334
actor:  1 policy actor:  1  step number:  55 total reward:  0.13333333333333275  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  0  action  0 :  tensor([0.4933, 0.0039, 0.0866, 0.0981, 0.1492, 0.0908, 0.0781],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0062, 0.9571, 0.0038, 0.0051, 0.0021, 0.0019, 0.0237],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1051, 0.0804, 0.3693, 0.1031, 0.1052, 0.1224, 0.1146],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1157, 0.0141, 0.0897, 0.4471, 0.1182, 0.1026, 0.1126],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1532, 0.0010, 0.1056, 0.1096, 0.4232, 0.0946, 0.1127],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0979, 0.0094, 0.1943, 0.0780, 0.0777, 0.4618, 0.0809],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1383, 0.1151, 0.1266, 0.1206, 0.1161, 0.1186, 0.2646],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  22.387821251901688
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.506]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]] [[27.822]
 [27.197]
 [27.822]
 [27.822]
 [27.822]
 [27.822]
 [27.822]] [[0.825]
 [0.851]
 [0.825]
 [0.825]
 [0.825]
 [0.825]
 [0.825]]
maxi score, test score, baseline:  0.1739933333333332 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.1739933333333332 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  46 total reward:  0.5266666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1739933333333332 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  57 total reward:  0.3066666666666664  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  3  action  0 :  tensor([0.4880, 0.0899, 0.0606, 0.0704, 0.1092, 0.0632, 0.1187],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0033, 0.9536, 0.0063, 0.0127, 0.0014, 0.0017, 0.0210],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1134, 0.0101, 0.4864, 0.0990, 0.1001, 0.1036, 0.0872],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1481, 0.0054, 0.1605, 0.2444, 0.1421, 0.1506, 0.1490],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1161, 0.0440, 0.0921, 0.0770, 0.5013, 0.0835, 0.0859],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1204, 0.0012, 0.1200, 0.1195, 0.1007, 0.4316, 0.1067],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1746, 0.2014, 0.0927, 0.1280, 0.1736, 0.1197, 0.1100],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1739933333333332 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.1739933333333332 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.1739933333333332 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.1739933333333332 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
maxi score, test score, baseline:  0.1739933333333332 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
printing an ep nov before normalisation:  33.96393430793073
printing an ep nov before normalisation:  10.644218531505427
maxi score, test score, baseline:  0.1739933333333332 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actor:  1 policy actor:  1  step number:  57 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  31.55556434743453
actor:  1 policy actor:  1  step number:  47 total reward:  0.41333333333333333  reward:  1.0 rdn_beta:  0.667
Starting evaluation
maxi score, test score, baseline:  0.1739933333333332 0.6953333333333334 0.6953333333333334
probs:  [0.15386703512887548, 0.15386703512887548, 0.15386703512887548, 0.23066482435562247, 0.15386703512887548, 0.15386703512887548]
actions average: 
K:  0  action  0 :  tensor([0.4656, 0.0023, 0.0910, 0.0866, 0.1664, 0.0958, 0.0924],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0047,     0.9242,     0.0057,     0.0098,     0.0007,     0.0007,
            0.0541], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1365, 0.0194, 0.3762, 0.0982, 0.0938, 0.1656, 0.1103],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1267, 0.0448, 0.1009, 0.3813, 0.1030, 0.1031, 0.1402],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1881, 0.0095, 0.1399, 0.1738, 0.1709, 0.1567, 0.1611],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1199, 0.0091, 0.1069, 0.0976, 0.0981, 0.4718, 0.0966],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1201, 0.0349, 0.1340, 0.1598, 0.0878, 0.0842, 0.3791],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  47.83326498104236
siam score:  -0.7559583
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.765]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]] [[34.996]
 [39.211]
 [34.996]
 [34.996]
 [34.996]
 [34.996]
 [34.996]] [[0.636]
 [0.765]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]]
printing an ep nov before normalisation:  32.10554486336419
printing an ep nov before normalisation:  34.40777421249973
printing an ep nov before normalisation:  35.26519295521223
printing an ep nov before normalisation:  38.91090622952658
Printing some Q and Qe and total Qs values:  [[0.752]
 [0.881]
 [0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]] [[38.977]
 [36.134]
 [40.463]
 [40.463]
 [40.463]
 [40.463]
 [40.463]] [[0.752]
 [0.881]
 [0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]]
Printing some Q and Qe and total Qs values:  [[0.784]
 [0.894]
 [0.788]
 [0.801]
 [0.781]
 [0.783]
 [0.827]] [[37.699]
 [32.851]
 [36.631]
 [40.837]
 [38.394]
 [38.24 ]
 [38.498]] [[0.784]
 [0.894]
 [0.788]
 [0.801]
 [0.781]
 [0.783]
 [0.827]]
printing an ep nov before normalisation:  32.45667641189645
printing an ep nov before normalisation:  31.08989953994751
printing an ep nov before normalisation:  33.36781978607178
printing an ep nov before normalisation:  41.659337113259014
printing an ep nov before normalisation:  29.846774754301176
actor:  0 policy actor:  0  step number:  24 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  42.52053941837916
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  0.17853999999999984 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  34.44292068481445
printing an ep nov before normalisation:  30.13539583871102
actor:  1 policy actor:  1  step number:  61 total reward:  0.18666666666666598  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17853999999999984 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  57 total reward:  0.09333333333333271  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.877]
 [0.877]
 [0.877]
 [0.878]
 [0.877]
 [0.877]
 [0.878]] [[29.956]
 [30.657]
 [31.857]
 [30.267]
 [32.669]
 [33.866]
 [30.267]] [[0.877]
 [0.877]
 [0.877]
 [0.878]
 [0.877]
 [0.877]
 [0.878]]
printing an ep nov before normalisation:  29.72450001024283
actor:  0 policy actor:  0  step number:  45 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  0.333
from probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17789999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17789999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
Printing some Q and Qe and total Qs values:  [[0.6 ]
 [0.57]
 [0.6 ]
 [0.6 ]
 [0.6 ]
 [0.6 ]
 [0.6 ]] [[32.968]
 [37.579]
 [32.968]
 [32.968]
 [32.968]
 [32.968]
 [32.968]] [[1.679]
 [1.925]
 [1.679]
 [1.679]
 [1.679]
 [1.679]
 [1.679]]
maxi score, test score, baseline:  0.17789999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  33 total reward:  0.6666666666666669  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  2  action  0 :  tensor([0.6068, 0.0110, 0.0619, 0.0732, 0.1145, 0.0630, 0.0696],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0061, 0.9586, 0.0067, 0.0064, 0.0019, 0.0011, 0.0191],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1182, 0.0638, 0.4133, 0.0984, 0.1058, 0.0981, 0.1024],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1105, 0.0621, 0.1006, 0.3504, 0.1037, 0.1388, 0.1339],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2145, 0.0190, 0.0984, 0.1409, 0.3117, 0.1143, 0.1013],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0978, 0.0907, 0.0832, 0.0760, 0.0782, 0.4895, 0.0846],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1809, 0.0868, 0.1345, 0.0909, 0.1198, 0.0727, 0.3144],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  48 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17789999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
siam score:  -0.7626716
maxi score, test score, baseline:  0.17789999999999986 0.6990000000000001 0.6990000000000001
actor:  0 policy actor:  1  step number:  48 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  29.686661124364225
Printing some Q and Qe and total Qs values:  [[0.129]
 [0.244]
 [0.128]
 [0.121]
 [0.127]
 [0.219]
 [0.12 ]] [[29.542]
 [36.527]
 [28.466]
 [31.443]
 [30.506]
 [35.835]
 [30.529]] [[0.529]
 [0.829]
 [0.5  ]
 [0.571]
 [0.553]
 [0.786]
 [0.546]]
maxi score, test score, baseline:  0.17396666666666652 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.17396666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17396666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  32.140746116638184
printing an ep nov before normalisation:  38.82272938462631
actor:  0 policy actor:  1  step number:  30 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  42 total reward:  0.5133333333333335  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17388666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17388666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  40.917946620703354
maxi score, test score, baseline:  0.17388666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17388666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  36 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17388666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  23.58455181121826
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.694]
 [0.601]
 [0.606]
 [0.197]
 [0.591]
 [0.637]] [[32.412]
 [34.   ]
 [36.943]
 [33.01 ]
 [32.884]
 [36.651]
 [32.454]] [[1.669]
 [1.89 ]
 [2.018]
 [1.729]
 [1.31 ]
 [1.986]
 [1.717]]
maxi score, test score, baseline:  0.17388666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  50.91097677939543
actor:  1 policy actor:  1  step number:  61 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17388666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  35.882837772369385
printing an ep nov before normalisation:  38.56579026500427
maxi score, test score, baseline:  0.17388666666666655 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.17388666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  50 total reward:  0.3399999999999995  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  38 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17388666666666655 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.17388666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17388666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17388666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17388666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  37.472105137489734
printing an ep nov before normalisation:  45.888071079290306
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.488]
 [0.333]
 [0.42 ]
 [0.403]
 [0.377]
 [0.455]] [[42.355]
 [33.227]
 [39.41 ]
 [39.436]
 [40.068]
 [41.475]
 [36.903]] [[0.724]
 [0.68 ]
 [0.608]
 [0.695]
 [0.686]
 [0.679]
 [0.696]]
actor:  1 policy actor:  1  step number:  31 total reward:  0.6933333333333336  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17388666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
using explorer policy with actor:  0
using explorer policy with actor:  1
printing an ep nov before normalisation:  14.274764060974121
maxi score, test score, baseline:  0.17388666666666655 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
Printing some Q and Qe and total Qs values:  [[0.924]
 [0.946]
 [0.959]
 [0.959]
 [0.959]
 [0.959]
 [0.959]] [[33.898]
 [38.93 ]
 [29.888]
 [29.888]
 [29.888]
 [29.888]
 [29.888]] [[0.924]
 [0.946]
 [0.959]
 [0.959]
 [0.959]
 [0.959]
 [0.959]]
actor:  1 policy actor:  1  step number:  62 total reward:  0.11333333333333284  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  34 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1769933333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  50.154806737694756
from probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  61.94001658062376
maxi score, test score, baseline:  0.1769933333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  45.451903332302955
printing an ep nov before normalisation:  57.20048305735375
maxi score, test score, baseline:  0.1769933333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
Printing some Q and Qe and total Qs values:  [[0.277]
 [0.348]
 [0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.277]] [[48.256]
 [53.723]
 [48.256]
 [48.256]
 [48.256]
 [48.256]
 [48.256]] [[1.518]
 [1.833]
 [1.518]
 [1.518]
 [1.518]
 [1.518]
 [1.518]]
Printing some Q and Qe and total Qs values:  [[0.791]
 [0.852]
 [0.777]
 [0.737]
 [0.775]
 [0.791]
 [0.793]] [[32.396]
 [27.905]
 [32.391]
 [42.988]
 [43.329]
 [32.719]
 [32.86 ]] [[1.73 ]
 [1.661]
 [1.716]
 [1.983]
 [2.031]
 [1.74 ]
 [1.745]]
maxi score, test score, baseline:  0.1769933333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  48 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  44.4472885987448
actions average: 
K:  4  action  0 :  tensor([0.3656, 0.0163, 0.1359, 0.1303, 0.0960, 0.1108, 0.1451],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0212, 0.8259, 0.0183, 0.0649, 0.0164, 0.0151, 0.0382],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0861, 0.0099, 0.4563, 0.0619, 0.0857, 0.2009, 0.0992],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1340, 0.0013, 0.1480, 0.3458, 0.1291, 0.1416, 0.1002],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1900, 0.0684, 0.0692, 0.0527, 0.4964, 0.0623, 0.0611],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0901, 0.0211, 0.2469, 0.0788, 0.0841, 0.3836, 0.0955],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1373, 0.0980, 0.1307, 0.1482, 0.1205, 0.1054, 0.2600],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1769933333333332 0.6990000000000001 0.6990000000000001
Printing some Q and Qe and total Qs values:  [[0.843]
 [0.843]
 [0.843]
 [0.843]
 [0.843]
 [0.843]
 [0.843]] [[34.073]
 [34.073]
 [34.073]
 [34.073]
 [34.073]
 [34.073]
 [34.073]] [[0.843]
 [0.843]
 [0.843]
 [0.843]
 [0.843]
 [0.843]
 [0.843]]
printing an ep nov before normalisation:  44.088257379208315
actor:  0 policy actor:  0  step number:  53 total reward:  0.25333333333333286  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17660666666666652 0.6990000000000001 0.6990000000000001
printing an ep nov before normalisation:  37.700541346376056
actor:  1 policy actor:  1  step number:  61 total reward:  0.17333333333333312  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17660666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
using another actor
maxi score, test score, baseline:  0.17660666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
Printing some Q and Qe and total Qs values:  [[0.192]
 [0.192]
 [0.192]
 [0.192]
 [0.192]
 [0.192]
 [0.192]] [[39.767]
 [39.767]
 [39.767]
 [39.767]
 [39.767]
 [39.767]
 [39.767]] [[1.175]
 [1.175]
 [1.175]
 [1.175]
 [1.175]
 [1.175]
 [1.175]]
printing an ep nov before normalisation:  39.311230182647705
maxi score, test score, baseline:  0.17660666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.336]
 [0.329]
 [0.336]
 [0.372]
 [0.336]
 [0.342]] [[57.25 ]
 [42.592]
 [58.724]
 [42.592]
 [56.32 ]
 [42.592]
 [50.204]] [[1.239]
 [0.565]
 [1.245]
 [0.565]
 [1.186]
 [0.565]
 [0.896]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  34.905233619178674
line 256 mcts: sample exp_bonus 46.45648033869372
maxi score, test score, baseline:  0.17660666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  54.95853538315788
actor:  0 policy actor:  1  step number:  44 total reward:  0.48666666666666647  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1765666666666665 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  48.23467998483695
printing an ep nov before normalisation:  29.657531051712606
printing an ep nov before normalisation:  56.78556830726452
maxi score, test score, baseline:  0.1765666666666665 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  53.34597178816329
printing an ep nov before normalisation:  46.1112117767334
Printing some Q and Qe and total Qs values:  [[0.264]
 [0.298]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]] [[43.785]
 [44.812]
 [43.785]
 [43.785]
 [43.785]
 [43.785]
 [43.785]] [[1.606]
 [1.702]
 [1.606]
 [1.606]
 [1.606]
 [1.606]
 [1.606]]
maxi score, test score, baseline:  0.1765666666666665 0.6990000000000001 0.6990000000000001
actor:  1 policy actor:  1  step number:  55 total reward:  0.3733333333333334  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1765666666666665 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.1765666666666665 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  41 total reward:  0.56  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.05247763987434
maxi score, test score, baseline:  0.1765666666666665 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  0 policy actor:  0  step number:  56 total reward:  0.21999999999999964  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  51 total reward:  0.3999999999999999  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.034364543450813
printing an ep nov before normalisation:  27.530103054972663
maxi score, test score, baseline:  0.17625999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  1.9826438801828772e-05
actor:  1 policy actor:  1  step number:  45 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17625999999999986 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.17625999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17625999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
Printing some Q and Qe and total Qs values:  [[0.374]
 [0.574]
 [0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]] [[50.809]
 [58.049]
 [50.809]
 [50.809]
 [50.809]
 [50.809]
 [50.809]] [[0.586]
 [0.851]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]]
maxi score, test score, baseline:  0.17625999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
using another actor
maxi score, test score, baseline:  0.17625999999999986 0.6990000000000001 0.6990000000000001
Printing some Q and Qe and total Qs values:  [[0.983]
 [1.02 ]
 [0.983]
 [0.983]
 [0.983]
 [0.983]
 [0.983]] [[62.788]
 [66.249]
 [62.788]
 [62.788]
 [62.788]
 [62.788]
 [62.788]] [[0.983]
 [1.02 ]
 [0.983]
 [0.983]
 [0.983]
 [0.983]
 [0.983]]
Printing some Q and Qe and total Qs values:  [[0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]] [[47.2]
 [47.2]
 [47.2]
 [47.2]
 [47.2]
 [47.2]
 [47.2]] [[1.059]
 [1.059]
 [1.059]
 [1.059]
 [1.059]
 [1.059]
 [1.059]]
siam score:  -0.75437486
printing an ep nov before normalisation:  28.267984469070292
maxi score, test score, baseline:  0.17625999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17625999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17625999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  50 total reward:  0.40666666666666684  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  0  action  0 :  tensor([0.7159, 0.0187, 0.0498, 0.0496, 0.0507, 0.0486, 0.0667],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0051, 0.9715, 0.0036, 0.0037, 0.0018, 0.0023, 0.0121],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0874, 0.0850, 0.5055, 0.0601, 0.0561, 0.1010, 0.1049],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1321, 0.0124, 0.0879, 0.3021, 0.1522, 0.1520, 0.1613],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1486, 0.0027, 0.0828, 0.0838, 0.4985, 0.0842, 0.0993],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0709, 0.0040, 0.1916, 0.0818, 0.0525, 0.5119, 0.0873],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2070, 0.1278, 0.0921, 0.0939, 0.0986, 0.1087, 0.2720],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.17625999999999983 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17625999999999983 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.865210513641735
printing an ep nov before normalisation:  40.28458336374616
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.319040751611084
maxi score, test score, baseline:  0.17625999999999983 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  42.82555693681744
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17625999999999983 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  29.042365010397017
actor:  1 policy actor:  1  step number:  50 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  54 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  39.501007397969566
printing an ep nov before normalisation:  32.29689376548282
actor:  1 policy actor:  1  step number:  51 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17625999999999983 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17625999999999983 0.6990000000000001 0.6990000000000001
printing an ep nov before normalisation:  34.726776478451754
printing an ep nov before normalisation:  29.83813762664795
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0226],
        [ 0.8199],
        [ 0.3670],
        [ 0.1530],
        [-0.0000],
        [ 0.6430],
        [ 0.8561],
        [-0.0000],
        [ 0.5689],
        [-0.0000]], dtype=torch.float64)
-0.032346567066 -0.054983253304550825
-0.05796375439799999 0.7618867082669102
-0.032346567066 0.33466384917776665
-0.058614567066 0.09439966217666958
-0.6861360000000003 -0.6861360000000003
-0.083839701198 0.5592094520122012
-0.084359833866 0.7717631630022064
0.99 0.99
-0.09703970119800001 0.47190210971912583
-0.9516369486359999 -0.9516369486359999
printing an ep nov before normalisation:  38.87601837446299
Printing some Q and Qe and total Qs values:  [[0.669]
 [0.856]
 [0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.669]] [[28.248]
 [29.381]
 [28.248]
 [28.248]
 [28.248]
 [28.248]
 [28.248]] [[0.9  ]
 [1.106]
 [0.9  ]
 [0.9  ]
 [0.9  ]
 [0.9  ]
 [0.9  ]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.734]
 [0.781]
 [0.743]
 [0.775]
 [0.659]
 [0.662]
 [0.734]] [[26.596]
 [26.234]
 [26.803]
 [26.075]
 [24.322]
 [24.209]
 [25.211]] [[0.955]
 [0.997]
 [0.966]
 [0.988]
 [0.846]
 [0.847]
 [0.935]]
maxi score, test score, baseline:  0.17625999999999983 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.17625999999999983 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17625999999999983 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  37.26737807798599
maxi score, test score, baseline:  0.17625999999999983 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  51 total reward:  0.35999999999999943  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  42 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17625999999999983 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  34.55484401176775
maxi score, test score, baseline:  0.17625999999999983 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  0 policy actor:  1  step number:  41 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  33.42097768052365
maxi score, test score, baseline:  0.17697999999999983 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  42 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.615]
 [0.73 ]
 [0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.615]] [[21.501]
 [24.053]
 [21.501]
 [21.501]
 [21.501]
 [21.501]
 [21.501]] [[0.896]
 [1.063]
 [0.896]
 [0.896]
 [0.896]
 [0.896]
 [0.896]]
maxi score, test score, baseline:  0.17697999999999983 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  35.624073301554176
actor:  1 policy actor:  1  step number:  62 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17697999999999983 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.17697999999999983 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  35.66289134561146
actor:  0 policy actor:  0  step number:  42 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1768333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1768333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
Printing some Q and Qe and total Qs values:  [[0.123]
 [0.258]
 [0.123]
 [0.123]
 [0.123]
 [0.123]
 [0.123]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.123]
 [0.258]
 [0.123]
 [0.123]
 [0.123]
 [0.123]
 [0.123]]
maxi score, test score, baseline:  0.1768333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  39.251838065122676
actor:  1 policy actor:  1  step number:  45 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1768333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  41.06155609617896
maxi score, test score, baseline:  0.1768333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.1768333333333332 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.1768333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.1768333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  48 total reward:  0.43333333333333335  reward:  1.0 rdn_beta:  0.667
siam score:  -0.76917654
maxi score, test score, baseline:  0.1768333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  20.41463904191733
siam score:  -0.7666397
maxi score, test score, baseline:  0.1768333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.1768333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  60 total reward:  0.07333333333333292  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 53.028663466497264
maxi score, test score, baseline:  0.1768333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  41.61471164076477
maxi score, test score, baseline:  0.1768333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  22.444589138031006
maxi score, test score, baseline:  0.1768333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.63540065413502
line 256 mcts: sample exp_bonus 43.90732974018989
actor:  1 policy actor:  1  step number:  56 total reward:  0.42000000000000015  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1768333333333332 0.6990000000000001 0.6990000000000001
printing an ep nov before normalisation:  46.578125889832386
actor:  1 policy actor:  1  step number:  57 total reward:  0.37333333333333274  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1768333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
Printing some Q and Qe and total Qs values:  [[0.115]
 [0.296]
 [0.067]
 [0.067]
 [0.15 ]
 [0.174]
 [0.067]] [[43.146]
 [37.51 ]
 [38.775]
 [38.775]
 [42.946]
 [44.676]
 [38.775]] [[1.06 ]
 [0.964]
 [0.797]
 [0.797]
 [1.085]
 [1.194]
 [0.797]]
actions average: 
K:  2  action  0 :  tensor([0.3853, 0.0234, 0.1059, 0.1108, 0.1391, 0.1191, 0.1164],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0033, 0.9527, 0.0038, 0.0093, 0.0023, 0.0025, 0.0261],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1462, 0.0747, 0.3036, 0.1020, 0.1213, 0.1142, 0.1381],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1979, 0.0208, 0.1316, 0.1368, 0.2025, 0.1578, 0.1528],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1567, 0.0218, 0.0789, 0.0759, 0.4971, 0.0887, 0.0808],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0831, 0.0822, 0.1204, 0.0638, 0.0883, 0.4866, 0.0756],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1343, 0.0202, 0.0982, 0.1597, 0.1626, 0.1476, 0.2774],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1768333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  33.10641808041329
from probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  35 total reward:  0.6000000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1768333333333332 0.6990000000000001 0.6990000000000001
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.99708731821013
actor:  1 policy actor:  1  step number:  50 total reward:  0.38  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1768333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.1768333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.352]
 [0.259]
 [0.223]
 [0.253]
 [0.259]
 [0.246]] [[38.855]
 [33.577]
 [34.16 ]
 [31.363]
 [33.761]
 [34.16 ]
 [34.605]] [[2.378]
 [1.876]
 [1.836]
 [1.548]
 [1.794]
 [1.836]
 [1.863]]
actor:  0 policy actor:  1  step number:  60 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.017]
 [ 0.153]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.017]
 [ 0.153]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]]
siam score:  -0.75902474
maxi score, test score, baseline:  0.1768333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  0.1768333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  33.83874438156538
printing an ep nov before normalisation:  32.80848564461922
line 256 mcts: sample exp_bonus 28.612117844194287
maxi score, test score, baseline:  0.1768333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  27.63383388519287
actor:  0 policy actor:  1  step number:  51 total reward:  0.3333333333333326  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  31.166278595265915
actor:  1 policy actor:  1  step number:  60 total reward:  0.33999999999999997  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  13.961253591178458
maxi score, test score, baseline:  0.1740066666666665 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.1740066666666665 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  26.13158087764262
maxi score, test score, baseline:  0.1740066666666665 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  21.189526785884112
printing an ep nov before normalisation:  29.01237966608818
printing an ep nov before normalisation:  25.67504917465996
maxi score, test score, baseline:  0.1740066666666665 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  1.8145877218069018
maxi score, test score, baseline:  0.1740066666666665 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
using explorer policy with actor:  0
using explorer policy with actor:  0
printing an ep nov before normalisation:  18.73660683631897
maxi score, test score, baseline:  0.1740066666666665 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.1713933333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  47.30405080346048
Printing some Q and Qe and total Qs values:  [[0.338]
 [0.338]
 [0.217]
 [0.338]
 [0.351]
 [0.338]
 [0.317]] [[49.502]
 [39.218]
 [49.23 ]
 [39.218]
 [50.247]
 [39.218]
 [49.924]] [[1.262]
 [0.741]
 [1.128]
 [0.741]
 [1.313]
 [0.741]
 [1.262]]
printing an ep nov before normalisation:  34.40194368362427
printing an ep nov before normalisation:  29.363698959350586
printing an ep nov before normalisation:  37.166789372309644
printing an ep nov before normalisation:  50.10220975053647
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1713933333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  0 policy actor:  0  step number:  44 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  55 total reward:  0.14666666666666606  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]] [[44.351]
 [44.351]
 [44.351]
 [44.351]
 [44.351]
 [44.351]
 [44.351]] [[2.146]
 [2.146]
 [2.146]
 [2.146]
 [2.146]
 [2.146]
 [2.146]]
Printing some Q and Qe and total Qs values:  [[0.762]
 [0.762]
 [0.762]
 [0.762]
 [0.762]
 [0.762]
 [0.762]] [[35.48]
 [35.48]
 [35.48]
 [35.48]
 [35.48]
 [35.48]
 [35.48]] [[1.616]
 [1.616]
 [1.616]
 [1.616]
 [1.616]
 [1.616]
 [1.616]]
printing an ep nov before normalisation:  50.237654119370674
maxi score, test score, baseline:  0.1744466666666665 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  62 total reward:  0.31333333333333346  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1744466666666665 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.1744466666666665 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.1744466666666665 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  49 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  50 total reward:  0.4866666666666668  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  36.03968059085471
actions average: 
K:  4  action  0 :  tensor([0.5263, 0.0246, 0.0933, 0.0918, 0.1008, 0.0863, 0.0768],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0175, 0.8960, 0.0163, 0.0252, 0.0121, 0.0110, 0.0219],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1040, 0.0060, 0.4814, 0.0813, 0.0770, 0.1801, 0.0701],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1768, 0.1035, 0.1555, 0.1573, 0.1407, 0.1610, 0.1052],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1255, 0.0012, 0.0946, 0.1125, 0.3827, 0.2116, 0.0719],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1448, 0.0760, 0.1310, 0.1352, 0.1849, 0.2433, 0.0847],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2306, 0.1028, 0.1273, 0.1213, 0.1263, 0.1224, 0.1694],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  34.82908896571056
maxi score, test score, baseline:  0.1744466666666665 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.1744466666666665 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  51.827339534770246
maxi score, test score, baseline:  0.1744466666666665 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.1744466666666665 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  0 policy actor:  1  step number:  51 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  42.695849453432395
printing an ep nov before normalisation:  28.533854484558105
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
actor:  1 policy actor:  1  step number:  41 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17432666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17432666666666652 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.17432666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
line 256 mcts: sample exp_bonus 30.389271833611424
actor:  1 policy actor:  1  step number:  53 total reward:  0.23999999999999966  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  51 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  35.5310845375061
Printing some Q and Qe and total Qs values:  [[-0.24]
 [ 0.  ]
 [-0.24]
 [-0.24]
 [-0.24]
 [-0.24]
 [-0.24]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.24]
 [ 0.  ]
 [-0.24]
 [-0.24]
 [-0.24]
 [-0.24]
 [-0.24]]
Printing some Q and Qe and total Qs values:  [[1.022]
 [1.022]
 [1.014]
 [1.014]
 [0.995]
 [0.937]
 [1.014]] [[37.769]
 [35.88 ]
 [35.385]
 [35.385]
 [42.241]
 [43.44 ]
 [35.385]] [[1.022]
 [1.022]
 [1.014]
 [1.014]
 [0.995]
 [0.937]
 [1.014]]
maxi score, test score, baseline:  0.17432666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17432666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
line 256 mcts: sample exp_bonus 53.65828628875866
maxi score, test score, baseline:  0.17432666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  45 total reward:  0.5866666666666669  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.158]
 [0.149]
 [0.118]
 [0.139]
 [0.107]
 [0.15 ]] [[12.974]
 [30.124]
 [28.143]
 [20.198]
 [25.487]
 [13.157]
 [28.47 ]] [[0.153]
 [0.389]
 [0.361]
 [0.253]
 [0.325]
 [0.173]
 [0.365]]
printing an ep nov before normalisation:  55.65361936863312
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17432666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actions average: 
K:  0  action  0 :  tensor([0.7213, 0.0020, 0.0375, 0.0416, 0.1030, 0.0446, 0.0501],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0033,     0.9799,     0.0029,     0.0023,     0.0008,     0.0012,
            0.0095], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0488, 0.0015, 0.6667, 0.0407, 0.0410, 0.1592, 0.0422],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1361, 0.0186, 0.1232, 0.3471, 0.1477, 0.1064, 0.1208],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2045, 0.0025, 0.0971, 0.0999, 0.3844, 0.1027, 0.1088],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1421, 0.0011, 0.1586, 0.1003, 0.1105, 0.3844, 0.1030],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1107, 0.1537, 0.1331, 0.1497, 0.0991, 0.0942, 0.2595],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[-0.032]
 [ 0.4  ]
 [ 0.381]
 [ 0.332]
 [-0.074]
 [ 0.247]
 [ 0.408]] [[34.778]
 [29.543]
 [28.732]
 [26.19 ]
 [32.181]
 [27.992]
 [29.65 ]] [[0.548]
 [0.831]
 [0.789]
 [0.668]
 [0.432]
 [0.634]
 [0.842]]
maxi score, test score, baseline:  0.17432666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17432666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.1717933333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  42 total reward:  0.44666666666666666  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actions average: 
K:  0  action  0 :  tensor([0.4980, 0.0023, 0.0741, 0.1180, 0.1076, 0.1032, 0.0968],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0049, 0.9161, 0.0126, 0.0173, 0.0011, 0.0032, 0.0448],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0835, 0.0039, 0.5706, 0.0913, 0.0687, 0.0923, 0.0897],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1348, 0.0038, 0.1047, 0.3445, 0.1604, 0.1215, 0.1304],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1708, 0.0040, 0.1002, 0.1381, 0.3339, 0.1274, 0.1256],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1197, 0.0039, 0.0864, 0.1581, 0.1414, 0.3829, 0.1076],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1897, 0.0842, 0.1001, 0.1101, 0.0942, 0.1011, 0.3207],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  53.32961585671137
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  0.37333333333333274  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]] [[32.108]
 [32.108]
 [32.108]
 [32.108]
 [32.108]
 [32.108]
 [32.108]] [[1.957]
 [1.957]
 [1.957]
 [1.957]
 [1.957]
 [1.957]
 [1.957]]
printing an ep nov before normalisation:  35.98132762753041
maxi score, test score, baseline:  0.1717933333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  48 total reward:  0.6600000000000003  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  55.76143731860232
actor:  0 policy actor:  0  step number:  42 total reward:  0.5133333333333332  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17481999999999984 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17481999999999984 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17481999999999984 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17481999999999984 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17481999999999984 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  49 total reward:  0.44000000000000006  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17481999999999984 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17481999999999984 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17481999999999984 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
line 256 mcts: sample exp_bonus 36.89216143519036
maxi score, test score, baseline:  0.17481999999999984 0.6990000000000001 0.6990000000000001
actor:  1 policy actor:  1  step number:  63 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17481999999999984 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  48 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.32 ]
 [0.334]
 [0.276]
 [0.238]
 [0.32 ]
 [0.32 ]
 [0.32 ]] [[56.408]
 [51.024]
 [50.474]
 [50.005]
 [56.408]
 [56.408]
 [56.408]] [[1.98 ]
 [1.749]
 [1.667]
 [1.607]
 [1.98 ]
 [1.98 ]
 [1.98 ]]
Printing some Q and Qe and total Qs values:  [[0.205]
 [0.243]
 [0.18 ]
 [0.205]
 [0.205]
 [0.205]
 [0.205]] [[40.129]
 [44.529]
 [35.881]
 [40.129]
 [40.129]
 [40.129]
 [40.129]] [[1.386]
 [1.652]
 [1.138]
 [1.386]
 [1.386]
 [1.386]
 [1.386]]
printing an ep nov before normalisation:  37.92707920074463
printing an ep nov before normalisation:  57.20599645319069
maxi score, test score, baseline:  0.17481999999999984 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.479]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]] [[34.675]
 [45.863]
 [34.675]
 [34.675]
 [34.675]
 [34.675]
 [34.675]] [[1.566]
 [1.894]
 [1.566]
 [1.566]
 [1.566]
 [1.566]
 [1.566]]
maxi score, test score, baseline:  0.17481999999999984 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  38.55269908905029
siam score:  -0.76676637
actor:  1 policy actor:  1  step number:  40 total reward:  0.5666666666666667  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17481999999999984 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.17481999999999984 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
siam score:  -0.76811355
maxi score, test score, baseline:  0.17481999999999984 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  44 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17481999999999984 0.6990000000000001 0.6990000000000001
actor:  1 policy actor:  1  step number:  54 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  36.265904902835565
printing an ep nov before normalisation:  42.3017692565918
printing an ep nov before normalisation:  29.892024993896484
actor:  1 policy actor:  1  step number:  45 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17481999999999984 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  40.56443805072188
printing an ep nov before normalisation:  29.534604548201838
maxi score, test score, baseline:  0.17481999999999984 0.6990000000000001 0.6990000000000001
actor:  0 policy actor:  0  step number:  47 total reward:  0.44000000000000006  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  48 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  39.60062019114629
Printing some Q and Qe and total Qs values:  [[0.768]
 [0.859]
 [0.768]
 [0.76 ]
 [0.754]
 [0.755]
 [0.787]] [[36.453]
 [35.102]
 [34.603]
 [37.11 ]
 [40.293]
 [36.252]
 [36.299]] [[0.768]
 [0.859]
 [0.768]
 [0.76 ]
 [0.754]
 [0.755]
 [0.787]]
printing an ep nov before normalisation:  50.640715244198816
maxi score, test score, baseline:  0.17769999999999983 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  42.96001127813359
printing an ep nov before normalisation:  29.56760883331299
printing an ep nov before normalisation:  0.0008835566850962095
maxi score, test score, baseline:  0.17769999999999983 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  0.05926231596447451
actor:  1 policy actor:  1  step number:  54 total reward:  0.23333333333333295  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.443]
 [0.512]
 [0.443]
 [0.443]
 [0.443]
 [0.443]
 [0.443]] [[28.462]
 [31.991]
 [28.462]
 [28.462]
 [28.462]
 [28.462]
 [28.462]] [[1.118]
 [1.351]
 [1.118]
 [1.118]
 [1.118]
 [1.118]
 [1.118]]
maxi score, test score, baseline:  0.17489999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17489999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  33.72841276608923
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
actor:  1 policy actor:  1  step number:  44 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17489999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17489999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  23.659278704069635
maxi score, test score, baseline:  0.17489999999999986 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.17489999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  40.50058913483064
maxi score, test score, baseline:  0.17489999999999986 0.6990000000000001 0.6990000000000001
printing an ep nov before normalisation:  34.42983568238083
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17489999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17489999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  60 total reward:  0.0733333333333328  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17489999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  39.01381710320921
maxi score, test score, baseline:  0.17489999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  31.848184489258458
actor:  1 policy actor:  1  step number:  67 total reward:  0.11999999999999944  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  33.93658383343176
Printing some Q and Qe and total Qs values:  [[0.728]
 [0.729]
 [0.733]
 [0.731]
 [0.682]
 [0.644]
 [0.723]] [[36.932]
 [33.326]
 [33.444]
 [34.143]
 [38.294]
 [37.879]
 [34.216]] [[1.23 ]
 [1.129]
 [1.137]
 [1.155]
 [1.222]
 [1.173]
 [1.149]]
maxi score, test score, baseline:  0.17489999999999986 0.6990000000000001 0.6990000000000001
line 256 mcts: sample exp_bonus 5.7406871879361886e-05
actor:  1 policy actor:  1  step number:  44 total reward:  0.5000000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17489999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  50 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17489999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  37.751587617212884
maxi score, test score, baseline:  0.17489999999999986 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.17489999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  64 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  38 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17489999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
siam score:  -0.76405793
maxi score, test score, baseline:  0.17489999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  0 policy actor:  1  step number:  66 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  38.29660261466137
line 256 mcts: sample exp_bonus 57.612131074808325
maxi score, test score, baseline:  0.1744466666666665 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  24.777668874859412
maxi score, test score, baseline:  0.1744466666666665 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
from probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  0 policy actor:  1  step number:  40 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17217999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  29.302382267859766
maxi score, test score, baseline:  0.17217999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  37.38907785779489
maxi score, test score, baseline:  0.17217999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17217999999999986 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  20.937234934157654
actor:  0 policy actor:  0  step number:  46 total reward:  0.3266666666666661  reward:  1.0 rdn_beta:  1.667
using another actor
actor:  1 policy actor:  1  step number:  69 total reward:  0.05333333333333257  reward:  1.0 rdn_beta:  0.333
from probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  32.196727683982786
maxi score, test score, baseline:  0.1717666666666665 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[ 0.649]
 [ 0.789]
 [-0.007]
 [ 0.608]
 [ 0.61 ]
 [ 0.306]
 [ 0.648]] [[32.169]
 [34.026]
 [27.998]
 [25.442]
 [24.753]
 [24.69 ]
 [23.106]] [[ 0.649]
 [ 0.789]
 [-0.007]
 [ 0.608]
 [ 0.61 ]
 [ 0.306]
 [ 0.648]]
actions average: 
K:  4  action  0 :  tensor([0.4994, 0.0049, 0.1403, 0.0757, 0.0810, 0.0612, 0.1375],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0263, 0.8542, 0.0244, 0.0289, 0.0085, 0.0075, 0.0502],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1357, 0.0035, 0.3858, 0.1250, 0.1185, 0.1080, 0.1235],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1046, 0.0575, 0.0851, 0.3976, 0.1109, 0.0903, 0.1541],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2189, 0.0256, 0.1325, 0.1046, 0.2995, 0.1008, 0.1181],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0803, 0.0028, 0.1983, 0.1067, 0.0944, 0.4128, 0.1047],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1095, 0.1227, 0.0818, 0.1332, 0.0996, 0.1033, 0.3498],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  31.87941074371338
printing an ep nov before normalisation:  43.486335667554144
maxi score, test score, baseline:  0.16944666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  33.044549224300575
maxi score, test score, baseline:  0.16944666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.16944666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.16944666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  44.461695348636404
maxi score, test score, baseline:  0.16944666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.16944666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  43.16382161104649
maxi score, test score, baseline:  0.16944666666666652 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.16944666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.16944666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  0 policy actor:  1  step number:  45 total reward:  0.4266666666666662  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17229999999999987 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  38 total reward:  0.5266666666666667  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17229999999999987 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17229999999999987 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.17229999999999987 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17229999999999987 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17229999999999987 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  42.55184866512145
printing an ep nov before normalisation:  29.45164306285239
maxi score, test score, baseline:  0.17229999999999987 0.6990000000000001 0.6990000000000001
printing an ep nov before normalisation:  36.81910658228518
maxi score, test score, baseline:  0.17229999999999987 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  38 total reward:  0.54  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  54 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17473999999999987 0.6990000000000001 0.6990000000000001
line 256 mcts: sample exp_bonus 42.02702168449152
actions average: 
K:  1  action  0 :  tensor([0.6403, 0.0304, 0.0557, 0.0534, 0.0933, 0.0590, 0.0679],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0132, 0.8909, 0.0229, 0.0222, 0.0104, 0.0105, 0.0299],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0740, 0.0892, 0.5261, 0.0584, 0.0629, 0.1010, 0.0884],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1222, 0.2027, 0.1211, 0.2052, 0.0966, 0.0947, 0.1576],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1869, 0.0629, 0.1128, 0.1181, 0.2672, 0.1457, 0.1064],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1066, 0.0021, 0.1308, 0.0912, 0.0983, 0.4845, 0.0865],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1286, 0.0320, 0.0991, 0.1067, 0.1012, 0.0898, 0.4426],
       grad_fn=<DivBackward0>)
actions average: 
K:  0  action  0 :  tensor([0.4871, 0.0037, 0.0911, 0.0884, 0.1370, 0.0919, 0.1007],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0086, 0.9407, 0.0112, 0.0082, 0.0044, 0.0036, 0.0233],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1583, 0.0027, 0.3102, 0.1072, 0.1164, 0.1823, 0.1230],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1200, 0.0148, 0.1091, 0.3751, 0.1411, 0.1647, 0.0753],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1661, 0.0016, 0.1175, 0.1066, 0.4032, 0.0998, 0.1051],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1156, 0.0113, 0.1443, 0.0939, 0.0983, 0.4441, 0.0925],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1941, 0.0092, 0.1200, 0.1204, 0.1500, 0.1306, 0.2758],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  30.87842139523311
maxi score, test score, baseline:  0.17473999999999987 0.6990000000000001 0.6990000000000001
using another actor
from probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
siam score:  -0.7645758
printing an ep nov before normalisation:  36.12795829772949
actor:  0 policy actor:  0  step number:  40 total reward:  0.6066666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1729933333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  43 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1729933333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  37.47638363604043
printing an ep nov before normalisation:  54.22377262743846
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.95219221413737
maxi score, test score, baseline:  0.1729933333333332 0.6990000000000001 0.6990000000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1729933333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  42 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 21.52279322043974
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.731]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]] [[28.545]
 [27.964]
 [28.545]
 [28.545]
 [28.545]
 [28.545]
 [28.545]] [[2.403]
 [2.421]
 [2.403]
 [2.403]
 [2.403]
 [2.403]
 [2.403]]
actions average: 
K:  3  action  0 :  tensor([0.5341, 0.0165, 0.0637, 0.0849, 0.1160, 0.0821, 0.1027],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0141, 0.8675, 0.0212, 0.0181, 0.0059, 0.0041, 0.0691],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0915, 0.0512, 0.4243, 0.0756, 0.0726, 0.1771, 0.1079],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1356, 0.0094, 0.1051, 0.3282, 0.1623, 0.1404, 0.1189],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1835, 0.0086, 0.0719, 0.0720, 0.4870, 0.0922, 0.0850],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0911, 0.0053, 0.1130, 0.1014, 0.0953, 0.4968, 0.0971],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1948, 0.0125, 0.1621, 0.1078, 0.1480, 0.2040, 0.1708],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1729933333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
Printing some Q and Qe and total Qs values:  [[0.216]
 [0.305]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]] [[35.75 ]
 [43.837]
 [35.75 ]
 [35.75 ]
 [35.75 ]
 [35.75 ]
 [35.75 ]] [[0.835]
 [1.197]
 [0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]]
Printing some Q and Qe and total Qs values:  [[0.291]
 [0.321]
 [0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]] [[57.205]
 [56.067]
 [57.205]
 [57.205]
 [57.205]
 [57.205]
 [57.205]] [[1.624]
 [1.602]
 [1.624]
 [1.624]
 [1.624]
 [1.624]
 [1.624]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  44 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  2.0
UNIT TEST: sample policy line 217 mcts : [0.633 0.163 0.02  0.    0.163 0.    0.02 ]
printing an ep nov before normalisation:  39.27681122447515
maxi score, test score, baseline:  0.1729933333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.1729933333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  27.353881508139317
actor:  1 policy actor:  1  step number:  52 total reward:  0.35333333333333283  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.294]
 [0.527]
 [0.359]
 [0.324]
 [0.294]
 [0.294]
 [0.246]] [[35.497]
 [40.333]
 [37.837]
 [37.239]
 [35.497]
 [35.497]
 [38.54 ]] [[0.671]
 [1.01 ]
 [0.787]
 [0.739]
 [0.671]
 [0.671]
 [0.69 ]]
maxi score, test score, baseline:  0.1729933333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.1729933333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.1729933333333332 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.1729933333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  38 total reward:  0.5  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  37.57168462491666
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.50448792080376
maxi score, test score, baseline:  0.1729933333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  58 total reward:  0.3399999999999992  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  30.9363488980512
printing an ep nov before normalisation:  43.48015163251504
maxi score, test score, baseline:  0.1729933333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.1729933333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.1729933333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  46.23640894921944
actor:  1 policy actor:  1  step number:  50 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 28.762995089601617
actor:  0 policy actor:  1  step number:  41 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.17069999999999985 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17069999999999985 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17069999999999985 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17069999999999985 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17069999999999985 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  39.05796787752839
siam score:  -0.76326454
printing an ep nov before normalisation:  39.114177224938
maxi score, test score, baseline:  0.17069999999999985 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  62 total reward:  0.3533333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17069999999999985 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  46.111549122365815
actor:  1 policy actor:  1  step number:  56 total reward:  0.43333333333333335  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  31.239609842237858
printing an ep nov before normalisation:  31.049065556286376
printing an ep nov before normalisation:  38.15176894103957
Printing some Q and Qe and total Qs values:  [[0.798]
 [0.908]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]] [[36.336]
 [34.768]
 [35.552]
 [35.552]
 [35.552]
 [35.552]
 [35.552]] [[0.798]
 [0.908]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]]
printing an ep nov before normalisation:  35.83320634422021
printing an ep nov before normalisation:  34.566905419131814
maxi score, test score, baseline:  0.17069999999999985 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  0 policy actor:  0  step number:  37 total reward:  0.52  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  21.77351221754874
maxi score, test score, baseline:  0.17148666666666654 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  56.39549534847056
maxi score, test score, baseline:  0.17148666666666654 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  65 total reward:  0.13333333333333286  reward:  1.0 rdn_beta:  0.667
using another actor
actor:  1 policy actor:  1  step number:  69 total reward:  0.39999999999999947  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17148666666666654 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.17148666666666654 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actions average: 
K:  4  action  0 :  tensor([0.4578, 0.0221, 0.0819, 0.0911, 0.1050, 0.0733, 0.1688],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0053, 0.9642, 0.0077, 0.0051, 0.0018, 0.0016, 0.0142],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0600, 0.0072, 0.6058, 0.0931, 0.0841, 0.0979, 0.0520],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1332, 0.0320, 0.1697, 0.2881, 0.1226, 0.1173, 0.1370],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1354, 0.0445, 0.0659, 0.1872, 0.4274, 0.0691, 0.0704],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0936, 0.0050, 0.1309, 0.1637, 0.1375, 0.3019, 0.1674],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1302, 0.0613, 0.1750, 0.2123, 0.1284, 0.1128, 0.1800],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.17148666666666654 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  35.873284339904785
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
actor:  0 policy actor:  1  step number:  37 total reward:  0.5466666666666667  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17208666666666653 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17208666666666653 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.72 ]
 [0.662]
 [0.657]
 [0.66 ]
 [0.655]
 [0.65 ]] [[26.795]
 [25.706]
 [27.007]
 [26.801]
 [27.207]
 [27.213]
 [27.473]] [[0.94 ]
 [0.975]
 [0.945]
 [0.936]
 [0.948]
 [0.943]
 [0.944]]
actor:  1 policy actor:  1  step number:  59 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17208666666666653 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  0 policy actor:  0  step number:  45 total reward:  0.52  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  1  action  0 :  tensor([0.6865, 0.0053, 0.0556, 0.0556, 0.0724, 0.0610, 0.0637],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0030, 0.9599, 0.0025, 0.0030, 0.0011, 0.0011, 0.0294],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0722, 0.0036, 0.5030, 0.0915, 0.0566, 0.1865, 0.0866],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1387, 0.0082, 0.0982, 0.4380, 0.1035, 0.1130, 0.1004],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1250, 0.0177, 0.0806, 0.0796, 0.5517, 0.0740, 0.0714],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1323, 0.0197, 0.1089, 0.1034, 0.1173, 0.4129, 0.1056],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1455, 0.1535, 0.1056, 0.1031, 0.0898, 0.0739, 0.3286],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.844]
 [0.873]
 [0.851]
 [0.851]
 [0.846]
 [0.832]
 [0.842]] [[37.259]
 [36.013]
 [35.359]
 [38.55 ]
 [37.579]
 [37.526]
 [37.391]] [[0.844]
 [0.873]
 [0.851]
 [0.851]
 [0.846]
 [0.832]
 [0.842]]
maxi score, test score, baseline:  0.17229999999999987 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actions average: 
K:  1  action  0 :  tensor([0.6789, 0.0014, 0.0727, 0.0545, 0.0678, 0.0611, 0.0635],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0152, 0.9137, 0.0073, 0.0208, 0.0119, 0.0051, 0.0260],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0880, 0.0076, 0.5418, 0.0736, 0.0768, 0.1078, 0.1043],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1376, 0.1797, 0.1020, 0.2741, 0.0625, 0.1022, 0.1418],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1563, 0.0054, 0.0602, 0.0709, 0.5526, 0.0732, 0.0816],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0886, 0.0217, 0.1492, 0.1311, 0.0814, 0.4398, 0.0883],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1103, 0.1918, 0.0688, 0.1531, 0.0587, 0.0591, 0.3581],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.17229999999999987 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  0 policy actor:  1  step number:  36 total reward:  0.54  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  45 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  43.51044968209711
maxi score, test score, baseline:  0.17279333333333322 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  50.117855986681555
printing an ep nov before normalisation:  34.401303393606604
maxi score, test score, baseline:  0.17279333333333322 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17279333333333322 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  43.38832586691173
maxi score, test score, baseline:  0.17279333333333322 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  43 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  41.1348885406827
actor:  1 policy actor:  1  step number:  47 total reward:  0.39999999999999947  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17279333333333322 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17279333333333322 0.6990000000000001 0.6990000000000001
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  50 total reward:  0.21999999999999942  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  38.23511136779242
printing an ep nov before normalisation:  44.937743459429065
actor:  0 policy actor:  1  step number:  40 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1758733333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.1758733333333332 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.1758733333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  0 policy actor:  0  step number:  56 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.566]
 [0.475]
 [0.475]
 [0.472]
 [0.474]
 [0.479]] [[38.388]
 [35.641]
 [37.659]
 [37.659]
 [39.028]
 [39.35 ]
 [39.282]] [[1.088]
 [1.1  ]
 [1.071]
 [1.071]
 [1.111]
 [1.122]
 [1.126]]
actions average: 
K:  0  action  0 :  tensor([0.3696, 0.0720, 0.0808, 0.1059, 0.1576, 0.1094, 0.1047],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0103, 0.9320, 0.0065, 0.0048, 0.0023, 0.0025, 0.0416],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0838, 0.0024, 0.5172, 0.0973, 0.1298, 0.1278, 0.0417],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1538, 0.0008, 0.1164, 0.3557, 0.1407, 0.1108, 0.1217],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1858, 0.0006, 0.0948, 0.1389, 0.3629, 0.1040, 0.1129],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1131, 0.0036, 0.1026, 0.1289, 0.1195, 0.4315, 0.1008],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1269, 0.0981, 0.0821, 0.1545, 0.0925, 0.1180, 0.3278],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  41.84588432312012
maxi score, test score, baseline:  0.1783133333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  26.009140014648438
maxi score, test score, baseline:  0.1783133333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  36.71918326557795
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.815]] [[38.678]
 [38.678]
 [38.678]
 [38.678]
 [38.678]
 [38.678]
 [38.678]] [[0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.815]]
printing an ep nov before normalisation:  36.07322931289673
actor:  1 policy actor:  1  step number:  55 total reward:  0.33333333333333326  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1783133333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.1783133333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.1783133333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  0 policy actor:  0  step number:  62 total reward:  0.0733333333333327  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  34.996301658348365
maxi score, test score, baseline:  0.17772666666666653 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
line 256 mcts: sample exp_bonus 34.787736940144185
maxi score, test score, baseline:  0.17772666666666653 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17772666666666653 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  49 total reward:  0.42666666666666664  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  29.091155529022217
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  50 total reward:  0.4466666666666669  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.643]
 [0.643]
 [0.643]
 [0.643]
 [0.643]
 [0.643]
 [0.643]] [[36.273]
 [36.273]
 [36.273]
 [36.273]
 [36.273]
 [36.273]
 [36.273]] [[1.665]
 [1.665]
 [1.665]
 [1.665]
 [1.665]
 [1.665]
 [1.665]]
maxi score, test score, baseline:  0.17749999999999985 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17749999999999985 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  57 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  35.811389330532
maxi score, test score, baseline:  0.17749999999999985 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.17749999999999985 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
using explorer policy with actor:  1
using another actor
siam score:  -0.7696148
actor:  1 policy actor:  1  step number:  40 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  45.83105953137348
printing an ep nov before normalisation:  37.767990832589845
maxi score, test score, baseline:  0.17749999999999985 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17749999999999985 0.6990000000000001 0.6990000000000001
printing an ep nov before normalisation:  33.979558808682356
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.462]
 [0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]] [[33.138]
 [33.261]
 [33.138]
 [33.138]
 [33.138]
 [33.138]
 [33.138]] [[0.667]
 [0.713]
 [0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]]
actor:  0 policy actor:  0  step number:  46 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1772333333333332 0.6990000000000001 0.6990000000000001
printing an ep nov before normalisation:  30.821184543052855
maxi score, test score, baseline:  0.1772333333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  47 total reward:  0.37333333333333274  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  54 total reward:  0.31333333333333324  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1773533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.1773533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  33.66290291569333
Printing some Q and Qe and total Qs values:  [[0.648]
 [0.696]
 [0.663]
 [0.643]
 [0.68 ]
 [0.637]
 [0.658]] [[22.565]
 [28.884]
 [22.666]
 [21.983]
 [28.684]
 [22.622]
 [22.11 ]] [[1.581]
 [2.14 ]
 [1.605]
 [1.53 ]
 [2.108]
 [1.576]
 [1.556]]
maxi score, test score, baseline:  0.1773533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  55 total reward:  0.41333333333333344  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  36.91713893824815
Printing some Q and Qe and total Qs values:  [[0.24]
 [0.24]
 [0.24]
 [0.24]
 [0.24]
 [0.24]
 [0.24]] [[56.313]
 [56.313]
 [56.313]
 [56.313]
 [56.313]
 [56.313]
 [56.313]] [[0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.34666666666666657  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  68.10852225246046
maxi score, test score, baseline:  0.1773533333333332 0.6990000000000001 0.6990000000000001
printing an ep nov before normalisation:  54.95372893478008
printing an ep nov before normalisation:  50.98284615410699
maxi score, test score, baseline:  0.1773533333333332 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.1773533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  21.674280166625977
actions average: 
K:  3  action  0 :  tensor([0.5366, 0.0430, 0.0620, 0.0667, 0.1348, 0.0713, 0.0856],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0106, 0.9306, 0.0037, 0.0171, 0.0034, 0.0016, 0.0331],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0839, 0.0142, 0.4169, 0.1058, 0.0954, 0.1354, 0.1484],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1176, 0.0070, 0.1242, 0.3576, 0.1675, 0.1191, 0.1070],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1292, 0.0247, 0.0730, 0.1013, 0.4785, 0.0962, 0.0970],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0480, 0.0344, 0.1152, 0.1029, 0.0598, 0.5694, 0.0703],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1866, 0.2914, 0.0685, 0.0827, 0.1134, 0.0844, 0.1729],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1773533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  35 total reward:  0.5466666666666666  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  43.10234523187461
maxi score, test score, baseline:  0.1773533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  53 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1773533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
Printing some Q and Qe and total Qs values:  [[-0.024]
 [-0.024]
 [ 0.202]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.024]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.024]
 [-0.024]
 [ 0.202]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.024]]
maxi score, test score, baseline:  0.1773533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  35.471539103836825
actor:  1 policy actor:  1  step number:  68 total reward:  0.16666666666666563  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1773533333333332 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.1773533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
siam score:  -0.7716035
actor:  1 policy actor:  1  step number:  69 total reward:  0.09333333333333327  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1773533333333332 0.6990000000000001 0.6990000000000001
actor:  1 policy actor:  1  step number:  50 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1773533333333332 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  0 policy actor:  1  step number:  39 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17768666666666652 0.6990000000000001 0.6990000000000001
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.439]
 [0.368]
 [0.334]
 [0.465]
 [0.465]
 [0.465]] [[51.392]
 [47.167]
 [46.42 ]
 [45.903]
 [51.392]
 [51.392]
 [51.392]] [[1.798]
 [1.596]
 [1.494]
 [1.439]
 [1.798]
 [1.798]
 [1.798]]
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.649]
 [0.552]
 [0.555]
 [0.554]
 [0.555]
 [0.561]] [[32.322]
 [36.398]
 [30.131]
 [30.436]
 [32.113]
 [31.543]
 [30.953]] [[0.552]
 [0.649]
 [0.552]
 [0.555]
 [0.554]
 [0.555]
 [0.561]]
maxi score, test score, baseline:  0.17768666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
siam score:  -0.76989657
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.553]
 [0.459]
 [0.45 ]
 [0.457]
 [0.455]
 [0.459]] [[29.959]
 [31.821]
 [27.919]
 [27.545]
 [28.518]
 [28.766]
 [28.435]] [[1.289]
 [1.433]
 [1.147]
 [1.12 ]
 [1.175]
 [1.186]
 [1.173]]
Printing some Q and Qe and total Qs values:  [[0.784]
 [0.903]
 [0.763]
 [0.738]
 [0.738]
 [0.784]
 [0.784]] [[28.547]
 [27.091]
 [28.64 ]
 [30.61 ]
 [31.683]
 [28.547]
 [28.547]] [[0.784]
 [0.903]
 [0.763]
 [0.738]
 [0.738]
 [0.784]
 [0.784]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  38 total reward:  0.6200000000000002  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17768666666666652 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  34.925689697265625
Printing some Q and Qe and total Qs values:  [[-0.007]
 [ 0.199]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.007]
 [ 0.199]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.638]
 [0.741]
 [0.629]
 [0.639]
 [0.642]
 [0.547]
 [0.604]] [[38.241]
 [39.553]
 [42.922]
 [42.586]
 [35.289]
 [43.385]
 [40.816]] [[0.638]
 [0.741]
 [0.629]
 [0.639]
 [0.642]
 [0.547]
 [0.604]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  33.45533847808838
actor:  0 policy actor:  0  step number:  49 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17771333333333317 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
Printing some Q and Qe and total Qs values:  [[-0.07 ]
 [ 0.388]
 [ 0.184]
 [ 0.314]
 [-0.033]
 [ 0.229]
 [ 0.343]] [[41.628]
 [37.492]
 [33.139]
 [30.918]
 [32.75 ]
 [31.426]
 [30.954]] [[1.228]
 [1.484]
 [1.067]
 [1.088]
 [0.83 ]
 [1.028]
 [1.119]]
maxi score, test score, baseline:  0.17771333333333317 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.17771333333333317 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17771333333333317 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  63.179827287391234
printing an ep nov before normalisation:  37.043752150537244
maxi score, test score, baseline:  0.17771333333333317 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  42.63181827021436
printing an ep nov before normalisation:  39.9352468701096
maxi score, test score, baseline:  0.17771333333333317 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  39.543387677936785
actor:  1 policy actor:  1  step number:  58 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  1.667
using another actor
maxi score, test score, baseline:  0.17771333333333322 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.17771333333333322 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actions average: 
K:  4  action  0 :  tensor([0.4046, 0.0050, 0.0759, 0.0969, 0.2203, 0.0833, 0.1140],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0124, 0.8882, 0.0100, 0.0300, 0.0095, 0.0129, 0.0369],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0968, 0.1309, 0.1716, 0.1131, 0.1174, 0.2348, 0.1353],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1623, 0.0553, 0.1505, 0.1486, 0.1747, 0.1589, 0.1498],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2323, 0.0178, 0.0861, 0.1015, 0.3362, 0.0974, 0.1287],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1482, 0.0049, 0.1042, 0.1192, 0.1285, 0.3656, 0.1295],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0980, 0.1460, 0.1232, 0.0793, 0.0678, 0.1087, 0.3771],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  40.79444043756671
Printing some Q and Qe and total Qs values:  [[0.637]
 [0.739]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]] [[44.074]
 [47.622]
 [44.074]
 [44.074]
 [44.074]
 [44.074]
 [44.074]] [[0.637]
 [0.739]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]]
maxi score, test score, baseline:  0.17771333333333322 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.17771333333333322 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  30.784726568258733
actor:  0 policy actor:  1  step number:  33 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.009]
 [ 0.174]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.009]
 [ 0.174]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]]
maxi score, test score, baseline:  0.17816666666666653 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  20.225944617699817
printing an ep nov before normalisation:  52.065729546299885
actor:  1 policy actor:  1  step number:  63 total reward:  0.3333333333333326  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.17816666666666653 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  39.362822010416245
maxi score, test score, baseline:  0.17816666666666653 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  25.507662747656696
actor:  0 policy actor:  1  step number:  48 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  44 total reward:  0.44666666666666666  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18108666666666653 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  42.47284063986719
printing an ep nov before normalisation:  27.860636806220366
maxi score, test score, baseline:  0.18108666666666653 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  0 policy actor:  0  step number:  47 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.18085999999999985 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.18085999999999985 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  45.245570986469176
printing an ep nov before normalisation:  46.052419081835225
maxi score, test score, baseline:  0.18085999999999985 0.6990000000000001 0.6990000000000001
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  43 total reward:  0.5200000000000002  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  56 total reward:  0.24666666666666603  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18085999999999985 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.18085999999999985 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
Printing some Q and Qe and total Qs values:  [[0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]] [[27.708]
 [27.708]
 [27.708]
 [27.708]
 [27.708]
 [27.708]
 [27.708]] [[0.86]
 [0.86]
 [0.86]
 [0.86]
 [0.86]
 [0.86]
 [0.86]]
maxi score, test score, baseline:  0.18085999999999985 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  32.067199348338995
printing an ep nov before normalisation:  32.04840499171849
maxi score, test score, baseline:  0.17803333333333318 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  1 policy actor:  1  step number:  54 total reward:  0.36666666666666614  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  37.62216758395662
printing an ep nov before normalisation:  28.996925867927423
Printing some Q and Qe and total Qs values:  [[0.869]
 [0.958]
 [0.869]
 [0.869]
 [0.869]
 [0.869]
 [0.869]] [[37.672]
 [35.304]
 [37.672]
 [37.672]
 [37.672]
 [37.672]
 [37.672]] [[0.869]
 [0.958]
 [0.869]
 [0.869]
 [0.869]
 [0.869]
 [0.869]]
Printing some Q and Qe and total Qs values:  [[0.921]
 [0.973]
 [0.941]
 [0.944]
 [0.918]
 [0.936]
 [0.944]] [[20.993]
 [26.67 ]
 [23.822]
 [24.01 ]
 [22.374]
 [22.002]
 [22.426]] [[0.921]
 [0.973]
 [0.941]
 [0.944]
 [0.918]
 [0.936]
 [0.944]]
maxi score, test score, baseline:  0.17803333333333318 0.6990000000000001 0.6990000000000001
actor:  0 policy actor:  1  step number:  62 total reward:  0.13999999999999957  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  27.433427015209933
maxi score, test score, baseline:  0.1768999999999999 0.6990000000000001 0.6990000000000001
printing an ep nov before normalisation:  27.02484122057273
maxi score, test score, baseline:  0.1768999999999999 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  35.25176155659402
printing an ep nov before normalisation:  18.330377939279686
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.493]
 [0.429]
 [0.436]
 [0.421]
 [0.429]
 [0.451]] [[33.77 ]
 [35.012]
 [34.517]
 [33.212]
 [33.77 ]
 [38.05 ]
 [34.121]] [[0.88 ]
 [0.985]
 [0.907]
 [0.88 ]
 [0.88 ]
 [1.001]
 [0.919]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.4266666666666662  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1768999999999999 0.6990000000000001 0.6990000000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1768999999999999 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.1768999999999999 0.6990000000000001 0.6990000000000001
actor:  1 policy actor:  1  step number:  49 total reward:  0.3999999999999996  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.   ]
 [ 0.087]
 [ 0.021]
 [ 0.005]
 [-0.007]
 [ 0.021]
 [ 0.078]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.   ]
 [ 0.087]
 [ 0.021]
 [ 0.005]
 [-0.007]
 [ 0.021]
 [ 0.078]]
printing an ep nov before normalisation:  48.6959257236826
maxi score, test score, baseline:  0.1768999999999999 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.1768999999999999 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.1768999999999999 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
siam score:  -0.7636812
maxi score, test score, baseline:  0.1768999999999999 0.6990000000000001 0.6990000000000001
maxi score, test score, baseline:  0.1768999999999999 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.1768999999999999 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.1768999999999999 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
actor:  0 policy actor:  1  step number:  52 total reward:  0.21999999999999964  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.723]
 [0.001]
 [0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]] [[28.004]
 [ 0.049]
 [28.004]
 [28.004]
 [28.004]
 [28.004]
 [28.004]] [[2.244]
 [0.003]
 [2.244]
 [2.244]
 [2.244]
 [2.244]
 [2.244]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.29333333333333267  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17595333333333318 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
printing an ep nov before normalisation:  48.44471465263192
actor:  0 policy actor:  1  step number:  54 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17513999999999985 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
siam score:  -0.76668113
maxi score, test score, baseline:  0.17513999999999985 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17513999999999985 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
Printing some Q and Qe and total Qs values:  [[0.368]
 [0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]] [[43.537]
 [41.347]
 [41.347]
 [41.347]
 [41.347]
 [41.347]
 [41.347]] [[1.988]
 [1.802]
 [1.802]
 [1.802]
 [1.802]
 [1.802]
 [1.802]]
maxi score, test score, baseline:  0.17513999999999985 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
Starting evaluation
maxi score, test score, baseline:  0.17513999999999985 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
Printing some Q and Qe and total Qs values:  [[0.691]
 [0.811]
 [0.686]
 [0.742]
 [0.672]
 [0.68 ]
 [0.823]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.691]
 [0.811]
 [0.686]
 [0.742]
 [0.672]
 [0.68 ]
 [0.823]]
Printing some Q and Qe and total Qs values:  [[0.73 ]
 [0.807]
 [0.725]
 [0.752]
 [0.714]
 [0.673]
 [0.799]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.73 ]
 [0.807]
 [0.725]
 [0.752]
 [0.714]
 [0.673]
 [0.799]]
Printing some Q and Qe and total Qs values:  [[0.191]
 [0.248]
 [0.163]
 [0.176]
 [0.175]
 [0.172]
 [0.193]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.191]
 [0.248]
 [0.163]
 [0.176]
 [0.175]
 [0.172]
 [0.193]]
actor:  0 policy actor:  0  step number:  47 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  2.0
from probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
maxi score, test score, baseline:  0.17457999999999987 0.6990000000000001 0.6990000000000001
probs:  [0.1659779637092141, 0.1659779637092141, 0.1659779637092141, 0.17011018145392945, 0.1659779637092141, 0.1659779637092141]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  37.37506199953726
Printing some Q and Qe and total Qs values:  [[0.738]
 [0.86 ]
 [0.799]
 [0.796]
 [0.787]
 [0.795]
 [0.822]] [[37.032]
 [37.403]
 [37.076]
 [35.988]
 [32.216]
 [39.48 ]
 [35.597]] [[0.738]
 [0.86 ]
 [0.799]
 [0.796]
 [0.787]
 [0.795]
 [0.822]]
printing an ep nov before normalisation:  37.497554300483515
printing an ep nov before normalisation:  33.46749592438736
Printing some Q and Qe and total Qs values:  [[0.953]
 [0.977]
 [0.953]
 [0.904]
 [0.953]
 [0.953]
 [0.901]] [[32.028]
 [33.252]
 [32.028]
 [40.741]
 [32.028]
 [32.028]
 [31.41 ]] [[0.953]
 [0.977]
 [0.953]
 [0.904]
 [0.953]
 [0.953]
 [0.901]]
Printing some Q and Qe and total Qs values:  [[0.904]
 [0.957]
 [0.904]
 [0.871]
 [0.904]
 [0.904]
 [0.862]] [[25.602]
 [29.301]
 [25.602]
 [33.098]
 [25.602]
 [25.602]
 [25.626]] [[0.904]
 [0.957]
 [0.904]
 [0.871]
 [0.904]
 [0.904]
 [0.862]]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333333  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333333  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333333  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  0.18051333333333316 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
actor:  1 policy actor:  1  step number:  54 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18051333333333316 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
maxi score, test score, baseline:  0.18051333333333316 0.6900000000000001 0.6900000000000001
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.733]
 [0.612]
 [0.616]
 [0.628]
 [0.62 ]
 [0.628]] [[47.444]
 [47.933]
 [43.488]
 [41.384]
 [46.365]
 [40.359]
 [44.402]] [[0.635]
 [0.733]
 [0.612]
 [0.616]
 [0.628]
 [0.62 ]
 [0.628]]
maxi score, test score, baseline:  0.1775533333333332 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  48.992732732673865
maxi score, test score, baseline:  0.1775533333333332 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
actor:  0 policy actor:  0  step number:  50 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]] [[37.647]
 [37.647]
 [37.647]
 [37.647]
 [37.647]
 [37.647]
 [37.647]] [[1.187]
 [1.187]
 [1.187]
 [1.187]
 [1.187]
 [1.187]
 [1.187]]
maxi score, test score, baseline:  0.1777533333333332 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  42.47614174546093
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  53 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  47 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7627016
printing an ep nov before normalisation:  41.902519654460534
actions average: 
K:  3  action  0 :  tensor([0.5721, 0.0064, 0.0801, 0.0757, 0.0852, 0.0743, 0.1063],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0127, 0.9287, 0.0046, 0.0232, 0.0013, 0.0016, 0.0279],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0912, 0.0817, 0.3572, 0.0971, 0.0920, 0.1979, 0.0829],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0930, 0.1446, 0.1112, 0.3503, 0.0961, 0.1008, 0.1040],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2479, 0.0019, 0.0843, 0.0869, 0.4157, 0.0723, 0.0910],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0864, 0.0115, 0.1691, 0.1155, 0.0866, 0.4209, 0.1099],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2060, 0.2000, 0.0918, 0.1058, 0.0634, 0.0715, 0.2615],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1805799999999999 0.6900000000000001 0.6900000000000001
printing an ep nov before normalisation:  31.429376986237212
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1805799999999999 0.6900000000000001 0.6900000000000001
using another actor
maxi score, test score, baseline:  0.1805799999999999 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.1805799999999999 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
actor:  1 policy actor:  1  step number:  54 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17753999999999986 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  36.306947608909745
printing an ep nov before normalisation:  40.52813557027548
actor:  1 policy actor:  1  step number:  41 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17753999999999986 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
maxi score, test score, baseline:  0.17753999999999986 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  50.01051566623248
Printing some Q and Qe and total Qs values:  [[0.746]
 [0.763]
 [0.746]
 [0.746]
 [0.746]
 [0.746]
 [0.746]] [[23.309]
 [23.784]
 [23.309]
 [23.309]
 [23.309]
 [23.309]
 [23.309]] [[1.915]
 [1.956]
 [1.915]
 [1.915]
 [1.915]
 [1.915]
 [1.915]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.5600000000000003  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17753999999999986 0.6900000000000001 0.6900000000000001
maxi score, test score, baseline:  0.17753999999999986 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  39.36854362226934
Printing some Q and Qe and total Qs values:  [[-0.001]
 [ 0.15 ]
 [-0.003]
 [ 0.026]
 [ 0.028]
 [ 0.027]
 [-0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.001]
 [ 0.15 ]
 [-0.003]
 [ 0.026]
 [ 0.028]
 [ 0.027]
 [-0.001]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17753999999999986 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  3.7524523179286007
actor:  1 policy actor:  1  step number:  64 total reward:  0.05999999999999983  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  50 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 35.027731397625416
maxi score, test score, baseline:  0.17753999999999986 0.6900000000000001 0.6900000000000001
actor:  1 policy actor:  1  step number:  58 total reward:  0.2199999999999993  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17753999999999986 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  46.1154062290175
maxi score, test score, baseline:  0.17753999999999986 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
actor:  1 policy actor:  1  step number:  47 total reward:  0.3999999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17753999999999986 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  47.657875911651374
actor:  1 policy actor:  1  step number:  31 total reward:  0.5866666666666667  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.58 ]
 [0.578]
 [0.485]
 [0.511]
 [0.585]
 [0.492]] [[31.603]
 [36.791]
 [35.659]
 [31.659]
 [32.466]
 [35.344]
 [31.624]] [[0.87 ]
 [1.074]
 [1.043]
 [0.852]
 [0.898]
 [1.043]
 [0.858]]
maxi score, test score, baseline:  0.17753999999999986 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17753999999999986 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
using explorer policy with actor:  0
printing an ep nov before normalisation:  33.27166924562046
using explorer policy with actor:  1
printing an ep nov before normalisation:  29.951401804380545
printing an ep nov before normalisation:  54.260158347040225
printing an ep nov before normalisation:  38.735187911691376
printing an ep nov before normalisation:  43.592590408466535
printing an ep nov before normalisation:  35.38752126367537
printing an ep nov before normalisation:  39.41622643831623
printing an ep nov before normalisation:  38.57813837118596
actor:  1 policy actor:  1  step number:  64 total reward:  0.2733333333333332  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17453999999999986 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
actor:  0 policy actor:  1  step number:  54 total reward:  0.2199999999999993  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  57.32292040066186
maxi score, test score, baseline:  0.17115333333333319 0.6900000000000001 0.6900000000000001
maxi score, test score, baseline:  0.17115333333333319 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
Printing some Q and Qe and total Qs values:  [[0.722]
 [0.75 ]
 [0.664]
 [0.664]
 [0.679]
 [0.664]
 [0.647]] [[39.466]
 [38.673]
 [37.12 ]
 [37.12 ]
 [35.935]
 [37.12 ]
 [31.367]] [[0.722]
 [0.75 ]
 [0.664]
 [0.664]
 [0.679]
 [0.664]
 [0.647]]
maxi score, test score, baseline:  0.17115333333333319 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
maxi score, test score, baseline:  0.17115333333333319 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
actor:  1 policy actor:  1  step number:  39 total reward:  0.5733333333333334  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  43 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.16861999999999985 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
maxi score, test score, baseline:  0.16861999999999985 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  48.724050559673486
printing an ep nov before normalisation:  12.819306350329592
maxi score, test score, baseline:  0.16861999999999985 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  37.843303765217094
siam score:  -0.7631538
maxi score, test score, baseline:  0.16861999999999985 0.6900000000000001 0.6900000000000001
siam score:  -0.7613863
maxi score, test score, baseline:  0.16861999999999985 0.6900000000000001 0.6900000000000001
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.528]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]] [[27.797]
 [34.3  ]
 [27.797]
 [27.797]
 [27.797]
 [27.797]
 [27.797]] [[1.384]
 [1.809]
 [1.384]
 [1.384]
 [1.384]
 [1.384]
 [1.384]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.449258741380426
printing an ep nov before normalisation:  39.82745519785063
maxi score, test score, baseline:  0.16861999999999985 0.6900000000000001 0.6900000000000001
actor:  1 policy actor:  1  step number:  63 total reward:  0.2266666666666659  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  58 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.69651949334188
printing an ep nov before normalisation:  46.81642888901343
using explorer policy with actor:  1
maxi score, test score, baseline:  0.16861999999999985 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  40.52514013445047
printing an ep nov before normalisation:  0.00585706378700479
maxi score, test score, baseline:  0.1662733333333332 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  30.70766499642677
maxi score, test score, baseline:  0.1662733333333332 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
maxi score, test score, baseline:  0.1662733333333332 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  18.343500736979966
actor:  1 policy actor:  1  step number:  63 total reward:  0.3466666666666661  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  31.93485072152224
actor:  1 policy actor:  1  step number:  44 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  0.333
from probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
maxi score, test score, baseline:  0.1662733333333332 0.6900000000000001 0.6900000000000001
maxi score, test score, baseline:  0.1662733333333332 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.665001186111084
maxi score, test score, baseline:  0.1632466666666665 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
maxi score, test score, baseline:  0.1632466666666665 0.6900000000000001 0.6900000000000001
maxi score, test score, baseline:  0.1632466666666665 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
actor:  1 policy actor:  1  step number:  58 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.37 ]
 [0.462]
 [0.347]
 [0.359]
 [0.321]
 [0.359]
 [0.352]] [[37.56 ]
 [36.107]
 [37.686]
 [37.881]
 [38.263]
 [38.136]
 [38.624]] [[1.466]
 [1.483]
 [1.449]
 [1.471]
 [1.453]
 [1.484]
 [1.502]]
actor:  1 policy actor:  1  step number:  61 total reward:  0.1999999999999994  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  54 total reward:  0.5666666666666669  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.56227533590841
Printing some Q and Qe and total Qs values:  [[ 0.586]
 [ 0.49 ]
 [ 0.142]
 [ 0.559]
 [ 0.62 ]
 [ 0.566]
 [-0.003]] [[31.493]
 [36.54 ]
 [33.67 ]
 [36.394]
 [33.146]
 [31.404]
 [41.611]] [[1.295]
 [1.45 ]
 [0.959]
 [1.512]
 [1.412]
 [1.271]
 [1.21 ]]
actor:  1 policy actor:  1  step number:  43 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 44.131437536390465
maxi score, test score, baseline:  0.1632466666666665 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  75.10047866436616
actor:  1 policy actor:  1  step number:  46 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1632466666666665 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
using explorer policy with actor:  0
printing an ep nov before normalisation:  41.485452221912944
actions average: 
K:  4  action  0 :  tensor([0.4733, 0.0405, 0.0740, 0.0771, 0.1448, 0.1259, 0.0644],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0041, 0.9528, 0.0029, 0.0029, 0.0018, 0.0015, 0.0340],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0870, 0.0676, 0.4934, 0.0749, 0.0941, 0.0919, 0.0910],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.1421,     0.0002,     0.1086,     0.4023,     0.1409,     0.1038,
            0.1021], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2072, 0.0044, 0.0799, 0.0929, 0.4376, 0.1021, 0.0760],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1878, 0.0081, 0.1168, 0.1243, 0.1366, 0.3137, 0.1128],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1220, 0.1886, 0.0885, 0.0779, 0.0945, 0.0852, 0.3433],
       grad_fn=<DivBackward0>)
siam score:  -0.7651214
maxi score, test score, baseline:  0.16040666666666656 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
maxi score, test score, baseline:  0.16040666666666656 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
Printing some Q and Qe and total Qs values:  [[ 0.022]
 [ 0.147]
 [ 0.026]
 [ 0.019]
 [-0.003]
 [ 0.026]
 [ 0.053]] [[46.108]
 [49.591]
 [47.417]
 [31.842]
 [35.048]
 [47.417]
 [42.308]] [[0.815]
 [1.06 ]
 [0.864]
 [0.32 ]
 [0.408]
 [0.864]
 [0.715]]
maxi score, test score, baseline:  0.16040666666666656 0.6900000000000001 0.6900000000000001
actor:  1 policy actor:  1  step number:  56 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  1.667
from probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
maxi score, test score, baseline:  0.16040666666666656 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
maxi score, test score, baseline:  0.16040666666666656 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
actor:  0 policy actor:  1  step number:  52 total reward:  0.39333333333333276  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.16061999999999982 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  57.04301540116028
actions average: 
K:  4  action  0 :  tensor([0.6226, 0.0060, 0.0657, 0.0550, 0.0870, 0.0608, 0.1029],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0047, 0.9448, 0.0163, 0.0087, 0.0019, 0.0016, 0.0220],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1298, 0.0061, 0.5635, 0.0683, 0.0686, 0.0837, 0.0800],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1660, 0.0786, 0.1236, 0.1517, 0.1630, 0.1213, 0.1959],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1568, 0.0080, 0.0944, 0.0978, 0.4406, 0.1086, 0.0938],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1638, 0.0045, 0.2170, 0.1219, 0.1073, 0.2937, 0.0918],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1486, 0.1027, 0.1397, 0.1054, 0.1124, 0.0854, 0.3059],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.16061999999999982 0.6900000000000001 0.6900000000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.16061999999999982 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  47 total reward:  0.4800000000000001  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  54.86510948174562
printing an ep nov before normalisation:  38.71486875591947
printing an ep nov before normalisation:  40.93919072011199
maxi score, test score, baseline:  0.16061999999999982 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  36.85518120836428
maxi score, test score, baseline:  0.16061999999999982 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  27.520360166842757
maxi score, test score, baseline:  0.15777999999999986 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
actor:  0 policy actor:  0  step number:  43 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.15827333333333315 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
maxi score, test score, baseline:  0.15827333333333315 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.599]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.56 ]] [[44.588]
 [45.973]
 [44.588]
 [44.588]
 [44.588]
 [44.588]
 [44.588]] [[0.803]
 [0.856]
 [0.803]
 [0.803]
 [0.803]
 [0.803]
 [0.803]]
maxi score, test score, baseline:  0.15827333333333315 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
maxi score, test score, baseline:  0.15827333333333315 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
from probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
maxi score, test score, baseline:  0.15827333333333315 0.6900000000000001 0.6900000000000001
Printing some Q and Qe and total Qs values:  [[0.556]
 [0.588]
 [0.526]
 [0.526]
 [0.521]
 [0.437]
 [0.588]] [[40.509]
 [41.802]
 [25.945]
 [25.945]
 [33.208]
 [36.556]
 [40.189]] [[0.743]
 [0.786]
 [0.586]
 [0.586]
 [0.644]
 [0.589]
 [0.773]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  59 total reward:  0.0799999999999994  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  0.15277999999999983 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
maxi score, test score, baseline:  0.15277999999999983 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  37.69999622046204
maxi score, test score, baseline:  0.15277999999999983 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.415]
 [0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]] [[37.788]
 [31.674]
 [33.022]
 [33.022]
 [33.022]
 [33.022]
 [33.022]] [[0.962]
 [0.775]
 [0.77 ]
 [0.77 ]
 [0.77 ]
 [0.77 ]
 [0.77 ]]
printing an ep nov before normalisation:  32.67609816798093
actor:  0 policy actor:  0  step number:  54 total reward:  0.2866666666666665  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1523533333333332 0.6900000000000001 0.6900000000000001
actor:  1 policy actor:  1  step number:  53 total reward:  0.2799999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1523533333333332 0.6900000000000001 0.6900000000000001
maxi score, test score, baseline:  0.1523533333333332 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  35.40899550433543
maxi score, test score, baseline:  0.1523533333333332 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  39.0449105807502
actor:  0 policy actor:  1  step number:  37 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.15541999999999984 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  62.35842415564169
maxi score, test score, baseline:  0.15541999999999984 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
maxi score, test score, baseline:  0.15541999999999984 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
actor:  0 policy actor:  1  step number:  49 total reward:  0.22666666666666613  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]]
printing an ep nov before normalisation:  52.24098203444816
maxi score, test score, baseline:  0.1549133333333332 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.666]
 [0.6  ]
 [0.581]
 [0.577]
 [0.574]
 [0.6  ]] [[23.461]
 [27.06 ]
 [25.32 ]
 [21.507]
 [21.625]
 [21.767]
 [25.32 ]] [[0.744]
 [0.89 ]
 [0.794]
 [0.708]
 [0.707]
 [0.707]
 [0.794]]
maxi score, test score, baseline:  0.1549133333333332 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1549133333333332 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
actor:  1 policy actor:  1  step number:  58 total reward:  0.12666666666666615  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1549133333333332 0.6900000000000001 0.6900000000000001
maxi score, test score, baseline:  0.1549133333333332 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
actions average: 
K:  0  action  0 :  tensor([0.3408, 0.0034, 0.1285, 0.1079, 0.1961, 0.1091, 0.1141],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0052,     0.9738,     0.0032,     0.0015,     0.0005,     0.0006,
            0.0153], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0706, 0.0278, 0.5360, 0.0877, 0.0815, 0.0821, 0.1142],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1972, 0.0451, 0.0749, 0.3080, 0.1527, 0.0934, 0.1286],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1496, 0.0106, 0.0833, 0.0973, 0.4558, 0.1064, 0.0969],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1076, 0.0435, 0.2393, 0.0734, 0.0891, 0.3620, 0.0851],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1755, 0.1830, 0.1027, 0.1021, 0.1121, 0.0971, 0.2275],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1549133333333332 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  24.529527104614935
maxi score, test score, baseline:  0.1549133333333332 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  46 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[ 0.115]
 [ 0.105]
 [ 0.005]
 [ 0.095]
 [ 0.121]
 [-0.004]
 [-0.012]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.115]
 [ 0.105]
 [ 0.005]
 [ 0.095]
 [ 0.121]
 [-0.004]
 [-0.012]]
maxi score, test score, baseline:  0.1549133333333332 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.361]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]] [[52.386]
 [51.474]
 [52.386]
 [52.386]
 [52.386]
 [52.386]
 [52.386]] [[1.245]
 [1.292]
 [1.245]
 [1.245]
 [1.245]
 [1.245]
 [1.245]]
printing an ep nov before normalisation:  59.28579097105246
printing an ep nov before normalisation:  57.79878511125604
actor:  1 policy actor:  1  step number:  53 total reward:  0.626666666666667  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  27.963526529568504
maxi score, test score, baseline:  0.1549133333333332 0.6900000000000001 0.6900000000000001
actor:  0 policy actor:  1  step number:  47 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.778]
 [0.602]
 [0.601]
 [0.603]
 [0.61 ]
 [0.607]] [[49.644]
 [44.288]
 [47.945]
 [49.422]
 [49.563]
 [47.25 ]
 [49.681]] [[0.606]
 [0.778]
 [0.602]
 [0.601]
 [0.603]
 [0.61 ]
 [0.607]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.35333333333333317  reward:  1.0 rdn_beta:  1.333
from probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
maxi score, test score, baseline:  0.15517999999999985 0.6900000000000001 0.6900000000000001
printing an ep nov before normalisation:  39.95738744735718
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  35.70401151938504
maxi score, test score, baseline:  0.15517999999999985 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  19.45422688059766
actor:  0 policy actor:  0  step number:  54 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  0.667
using another actor
siam score:  -0.78056204
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.32 ]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]] [[38.866]
 [35.495]
 [38.866]
 [38.866]
 [38.866]
 [38.866]
 [38.866]] [[2.399]
 [1.987]
 [2.399]
 [2.399]
 [2.399]
 [2.399]
 [2.399]]
maxi score, test score, baseline:  0.15764666666666652 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
actor:  1 policy actor:  1  step number:  42 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.15764666666666652 0.6900000000000001 0.6900000000000001
maxi score, test score, baseline:  0.15512666666666652 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  48.191114572794014
printing an ep nov before normalisation:  43.48544284955532
maxi score, test score, baseline:  0.15512666666666652 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
Printing some Q and Qe and total Qs values:  [[0.57 ]
 [0.666]
 [0.467]
 [0.61 ]
 [0.57 ]
 [0.458]
 [0.588]] [[30.243]
 [38.035]
 [35.932]
 [38.243]
 [30.243]
 [33.67 ]
 [37.9  ]] [[0.57 ]
 [0.666]
 [0.467]
 [0.61 ]
 [0.57 ]
 [0.458]
 [0.588]]
siam score:  -0.7767596
siam score:  -0.77555484
maxi score, test score, baseline:  0.15512666666666652 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  35.785468558587056
printing an ep nov before normalisation:  28.99948938636369
actor:  1 policy actor:  1  step number:  42 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.15512666666666652 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
actor:  0 policy actor:  0  step number:  39 total reward:  0.5466666666666669  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.15583333333333318 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  38.20207118988037
printing an ep nov before normalisation:  41.95749921465412
printing an ep nov before normalisation:  61.8416615296553
actions average: 
K:  3  action  0 :  tensor([0.6508, 0.0090, 0.0408, 0.0384, 0.1590, 0.0478, 0.0542],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0302, 0.9044, 0.0156, 0.0060, 0.0048, 0.0038, 0.0354],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0713, 0.0088, 0.6933, 0.0434, 0.0591, 0.0598, 0.0643],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1882, 0.0003, 0.1811, 0.1689, 0.1700, 0.1716, 0.1200],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1450, 0.0907, 0.0928, 0.0862, 0.3823, 0.0860, 0.1171],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1725, 0.0048, 0.1792, 0.1139, 0.1278, 0.2708, 0.1309],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1172, 0.2800, 0.1011, 0.1105, 0.1408, 0.0902, 0.1602],
       grad_fn=<DivBackward0>)
actions average: 
K:  4  action  0 :  tensor([0.5185, 0.0325, 0.0898, 0.0647, 0.1374, 0.0682, 0.0889],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0289, 0.8628, 0.0188, 0.0187, 0.0201, 0.0171, 0.0336],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1519, 0.0075, 0.2940, 0.1083, 0.1241, 0.1301, 0.1841],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1794, 0.0161, 0.2982, 0.1011, 0.1307, 0.1205, 0.1539],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1368, 0.0362, 0.0405, 0.0520, 0.6160, 0.0632, 0.0553],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1299, 0.0292, 0.1765, 0.0712, 0.0990, 0.3865, 0.1077],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1998, 0.0036, 0.2707, 0.0980, 0.1479, 0.1405, 0.1395],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  2.41271322920511e-05
Printing some Q and Qe and total Qs values:  [[0.198]
 [0.319]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]] [[47.966]
 [49.674]
 [47.966]
 [47.966]
 [47.966]
 [47.966]
 [47.966]] [[0.666]
 [0.822]
 [0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]]
siam score:  -0.775826
printing an ep nov before normalisation:  45.39784022918106
Printing some Q and Qe and total Qs values:  [[0.248]
 [0.308]
 [0.228]
 [0.228]
 [0.292]
 [0.228]
 [0.214]] [[45.861]
 [42.927]
 [34.252]
 [34.252]
 [42.894]
 [34.252]
 [39.391]] [[0.678]
 [0.671]
 [0.397]
 [0.397]
 [0.654]
 [0.397]
 [0.498]]
maxi score, test score, baseline:  0.15297999999999987 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.15297999999999987 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
siam score:  -0.77173823
actor:  1 policy actor:  1  step number:  43 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  34.48328429293253
maxi score, test score, baseline:  0.15297999999999987 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
maxi score, test score, baseline:  0.15297999999999987 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  31.642422676082656
actions average: 
K:  2  action  0 :  tensor([0.4478, 0.0204, 0.0805, 0.1291, 0.1551, 0.0816, 0.0855],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0137, 0.8805, 0.0060, 0.0345, 0.0128, 0.0073, 0.0452],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0450, 0.0598, 0.6237, 0.0661, 0.0592, 0.0634, 0.0828],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1487, 0.1145, 0.1243, 0.1778, 0.1271, 0.1214, 0.1861],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1879, 0.0344, 0.0994, 0.1025, 0.3822, 0.0926, 0.1010],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1368, 0.0021, 0.3636, 0.0724, 0.0730, 0.2753, 0.0767],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0704, 0.2819, 0.1057, 0.0586, 0.0339, 0.0472, 0.4023],
       grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 20.72955232858658
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.724]
 [0.695]
 [0.696]
 [0.695]
 [0.7  ]
 [0.705]] [[26.321]
 [27.388]
 [26.321]
 [22.772]
 [26.321]
 [23.416]
 [23.452]] [[1.253]
 [1.322]
 [1.253]
 [1.124]
 [1.253]
 [1.152]
 [1.158]]
actor:  1 policy actor:  1  step number:  59 total reward:  0.2933333333333332  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  56 total reward:  0.1933333333333327  reward:  1.0 rdn_beta:  1.0
from probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
actions average: 
K:  1  action  0 :  tensor([0.3328, 0.0757, 0.1198, 0.1163, 0.1076, 0.1141, 0.1337],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0021,     0.9684,     0.0022,     0.0115,     0.0009,     0.0008,
            0.0142], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0556, 0.0110, 0.6494, 0.0884, 0.0524, 0.0722, 0.0709],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1720, 0.0053, 0.1398, 0.1804, 0.1958, 0.1380, 0.1687],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1527, 0.0074, 0.1066, 0.0955, 0.4328, 0.1202, 0.0849],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1233, 0.0291, 0.2495, 0.1008, 0.1050, 0.2966, 0.0958],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2032, 0.0141, 0.1325, 0.1033, 0.0918, 0.0863, 0.3687],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  49 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.15297999999999987 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  34.41952699079925
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.861]
 [0.937]
 [0.84 ]
 [0.835]
 [0.849]
 [0.871]
 [0.928]] [[30.071]
 [27.105]
 [31.341]
 [29.81 ]
 [29.747]
 [34.177]
 [26.399]] [[0.861]
 [0.937]
 [0.84 ]
 [0.835]
 [0.849]
 [0.871]
 [0.928]]
Printing some Q and Qe and total Qs values:  [[0.31]
 [0.31]
 [0.31]
 [0.31]
 [0.31]
 [0.31]
 [0.31]] [[34.326]
 [34.326]
 [34.326]
 [34.326]
 [34.326]
 [34.326]
 [34.326]] [[0.941]
 [0.941]
 [0.941]
 [0.941]
 [0.941]
 [0.941]
 [0.941]]
printing an ep nov before normalisation:  35.52535738162629
siam score:  -0.76974684
actor:  0 policy actor:  0  step number:  50 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  2.0
from probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
actor:  1 policy actor:  1  step number:  51 total reward:  0.41333333333333355  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1557133333333332 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
maxi score, test score, baseline:  0.1557133333333332 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
actor:  1 policy actor:  1  step number:  53 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  1.0
line 256 mcts: sample exp_bonus 33.870899046075365
printing an ep nov before normalisation:  30.55162238861715
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.15328666666666654 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
actor:  0 policy actor:  0  step number:  43 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7687195
printing an ep nov before normalisation:  44.96943350993593
printing an ep nov before normalisation:  41.758500913189266
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.982487784491646
printing an ep nov before normalisation:  33.22985109547002
printing an ep nov before normalisation:  23.47149502313405
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  66 total reward:  0.0733333333333327  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  37.017645280482384
printing an ep nov before normalisation:  35.00528251223821
maxi score, test score, baseline:  0.1484733333333332 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
actor:  1 policy actor:  1  step number:  65 total reward:  0.13333333333333264  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1484733333333332 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
maxi score, test score, baseline:  0.1484733333333332 0.6900000000000001 0.6900000000000001
maxi score, test score, baseline:  0.1484733333333332 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
maxi score, test score, baseline:  0.1484733333333332 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
maxi score, test score, baseline:  0.1484733333333332 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
actor:  1 policy actor:  1  step number:  45 total reward:  0.5733333333333337  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  42.79308266791969
maxi score, test score, baseline:  0.1484733333333332 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
maxi score, test score, baseline:  0.1484733333333332 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
maxi score, test score, baseline:  0.1484733333333332 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
maxi score, test score, baseline:  0.1484733333333332 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]] [[37.508]
 [37.508]
 [37.508]
 [37.508]
 [37.508]
 [37.508]
 [37.508]] [[1.48]
 [1.48]
 [1.48]
 [1.48]
 [1.48]
 [1.48]
 [1.48]]
maxi score, test score, baseline:  0.14847333333333318 0.6900000000000001 0.6900000000000001
maxi score, test score, baseline:  0.14847333333333318 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
actor:  0 policy actor:  0  step number:  44 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  0.333
from probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  52.748975329449
maxi score, test score, baseline:  0.15117999999999987 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  51.93340686483354
maxi score, test score, baseline:  0.15117999999999987 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
actor:  1 policy actor:  1  step number:  57 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 32.91496956411431
printing an ep nov before normalisation:  30.638321030509506
printing an ep nov before normalisation:  28.67846478798374
printing an ep nov before normalisation:  29.85898767629764
maxi score, test score, baseline:  0.15117999999999987 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
maxi score, test score, baseline:  0.15117999999999987 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
Printing some Q and Qe and total Qs values:  [[0.887]
 [0.925]
 [0.911]
 [0.911]
 [0.911]
 [0.911]
 [0.911]] [[26.337]
 [26.117]
 [22.967]
 [22.967]
 [22.967]
 [22.967]
 [22.967]] [[0.887]
 [0.925]
 [0.911]
 [0.911]
 [0.911]
 [0.911]
 [0.911]]
printing an ep nov before normalisation:  18.99192452430725
maxi score, test score, baseline:  0.15117999999999987 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
actor:  0 policy actor:  0  step number:  59 total reward:  0.1999999999999994  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10933
maxi score, test score, baseline:  0.15113999999999983 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
maxi score, test score, baseline:  0.15113999999999983 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
maxi score, test score, baseline:  0.15113999999999983 0.6900000000000001 0.6900000000000001
Printing some Q and Qe and total Qs values:  [[0.858]
 [0.858]
 [0.858]
 [0.858]
 [0.858]
 [0.858]
 [0.858]] [[52.248]
 [52.248]
 [52.248]
 [52.248]
 [52.248]
 [52.248]
 [52.248]] [[0.858]
 [0.858]
 [0.858]
 [0.858]
 [0.858]
 [0.858]
 [0.858]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  0.13859255977521912
actor:  1 policy actor:  1  step number:  68 total reward:  0.15333333333333232  reward:  1.0 rdn_beta:  0.667
siam score:  -0.77161795
maxi score, test score, baseline:  0.15113999999999983 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.15113999999999983 0.6900000000000001 0.6900000000000001
maxi score, test score, baseline:  0.15113999999999983 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
maxi score, test score, baseline:  0.15113999999999983 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
maxi score, test score, baseline:  0.15113999999999983 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
actor:  0 policy actor:  1  step number:  56 total reward:  0.43333333333333346  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.15137999999999985 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
Printing some Q and Qe and total Qs values:  [[-0.003]
 [ 0.2  ]
 [-0.001]
 [-0.001]
 [-0.001]
 [ 0.019]
 [-0.001]] [[45.06 ]
 [45.411]
 [45.456]
 [47.091]
 [45.889]
 [46.383]
 [47.091]] [[0.512]
 [0.724]
 [0.524]
 [0.562]
 [0.534]
 [0.565]
 [0.562]]
actions average: 
K:  4  action  0 :  tensor([0.5532, 0.0151, 0.0782, 0.0832, 0.0906, 0.0862, 0.0934],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0157, 0.8894, 0.0112, 0.0092, 0.0040, 0.0044, 0.0660],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1094, 0.0846, 0.5240, 0.0476, 0.0552, 0.0788, 0.1004],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1725, 0.0081, 0.1718, 0.2572, 0.1363, 0.1413, 0.1127],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1710, 0.0793, 0.1191, 0.1164, 0.2265, 0.1262, 0.1615],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1806, 0.0307, 0.1347, 0.1103, 0.1365, 0.2689, 0.1384],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1487, 0.0081, 0.1225, 0.1351, 0.1408, 0.1358, 0.3089],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.15137999999999985 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  30.66239350405568
maxi score, test score, baseline:  0.15137999999999985 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  46.710493277861396
printing an ep nov before normalisation:  36.825013693618104
maxi score, test score, baseline:  0.15137999999999985 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  1.3562049616666627e-05
printing an ep nov before normalisation:  58.85108358797436
from probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
maxi score, test score, baseline:  0.15137999999999985 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  42.92176670053965
maxi score, test score, baseline:  0.15137999999999985 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
maxi score, test score, baseline:  0.15137999999999985 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
maxi score, test score, baseline:  0.15137999999999985 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.15137999999999985 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  30.586361162090537
maxi score, test score, baseline:  0.15137999999999985 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.15137999999999985 0.6900000000000001 0.6900000000000001
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.64 ]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]] [[34.071]
 [34.988]
 [34.071]
 [34.071]
 [34.071]
 [34.071]
 [34.071]] [[0.885]
 [1.051]
 [0.885]
 [0.885]
 [0.885]
 [0.885]
 [0.885]]
printing an ep nov before normalisation:  36.79430886644494
actor:  1 policy actor:  1  step number:  50 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.15137999999999985 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
actor:  1 policy actor:  1  step number:  36 total reward:  0.4999999999999999  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  28.92076629587213
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  61 total reward:  0.14666666666666628  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  52.483709728613306
deleting a thread, now have 2 threads
Frames:  254978 train batches done:  29879 episodes:  6562
using explorer policy with actor:  0
maxi score, test score, baseline:  0.15137999999999985 0.6900000000000001 0.6900000000000001
probs:  [0.15024954032046983, 0.12562385080115152, 0.12562385080115152, 0.34725505647492405, 0.12562385080115152, 0.12562385080115152]
printing an ep nov before normalisation:  37.17472551303004
actor:  0 policy actor:  1  step number:  69 total reward:  0.11999999999999922  reward:  1.0 rdn_beta:  1.0
