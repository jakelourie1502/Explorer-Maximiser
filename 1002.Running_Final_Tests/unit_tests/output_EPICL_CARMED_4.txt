append_mcts_svs:False
dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 50}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:1
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.Car_Driving_Env.RaceWorld'>
same_env_each_time:True
env_size:[7, 42]
observable_size:[7, 9]
game_modes:1
env_map:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
max_steps:150
actions_size:7
optimal_score:0.86
total_frames:255000
exp_gamma:0.975
atari_env:False
memory_size:30
reward_clipping:False
image_size:[48, 48]
deque_length:3
PRESET_CONFIG:7
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:episodic
rdn_beta:[0.3333333333333333, 2, 6]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
contrast_vector:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 294)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:False
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
printing an ep nov before normalisation:  9.150487785403811
using explorer policy with actor:  1
printing an ep nov before normalisation:  12.641220092773438
printing an ep nov before normalisation:  8.942414239273617
Starting evaluation
printing an ep nov before normalisation:  9.62253042629787
siam score:  0.00019294516691430047
maxi score, test score, baseline:  -0.9472684210526315 0.0 0.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  8.860305436917315
maxi score, test score, baseline:  -0.9564217391304348 0.0 0.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
probs:  [0.20748122637675592, 0.11329378089193455, 0.3958561173463987, 0.11329378089193455, 0.05678131360104174, 0.11329378089193455]
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
probs:  [0.20748122637675592, 0.11329378089193455, 0.3958561173463987, 0.11329378089193455, 0.05678131360104174, 0.11329378089193455]
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
probs:  [0.20748122637675592, 0.11329378089193455, 0.3958561173463987, 0.11329378089193455, 0.05678131360104174, 0.11329378089193455]
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
probs:  [0.20748122637675592, 0.11329378089193455, 0.3958561173463987, 0.11329378089193455, 0.05678131360104174, 0.11329378089193455]
deleting a thread, now have 2 threads
Frames:  699 train batches done:  21 episodes:  20
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
probs:  [0.20748122637675592, 0.11329378089193455, 0.3958561173463987, 0.11329378089193455, 0.05678131360104174, 0.11329378089193455]
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
probs:  [0.20748122637675592, 0.11329378089193455, 0.3958561173463987, 0.11329378089193455, 0.05678131360104174, 0.11329378089193455]
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
probs:  [0.20748122637675592, 0.11329378089193455, 0.3958561173463987, 0.11329378089193455, 0.05678131360104174, 0.11329378089193455]
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
probs:  [0.20748122637675592, 0.11329378089193455, 0.3958561173463987, 0.11329378089193455, 0.05678131360104174, 0.11329378089193455]
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
probs:  [0.20748122637675592, 0.11329378089193455, 0.3958561173463987, 0.11329378089193455, 0.05678131360104174, 0.11329378089193455]
actions average: 
K:  0  action  0 :  tensor([0.4435, 0.1034, 0.0769, 0.0615, 0.1160, 0.1119, 0.0867],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0895, 0.5733, 0.0831, 0.0516, 0.0730, 0.0627, 0.0669],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0823, 0.1951, 0.2972, 0.0628, 0.0912, 0.1955, 0.0760],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2113, 0.1610, 0.1198, 0.1811, 0.1179, 0.1162, 0.0927],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1464, 0.1392, 0.1012, 0.0678, 0.2031, 0.1399, 0.2023],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1721, 0.0679, 0.1303, 0.0938, 0.1052, 0.3116, 0.1191],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1898, 0.1278, 0.1191, 0.1021, 0.1286, 0.1272, 0.2054],
       grad_fn=<DivBackward0>)
actions average: 
K:  3  action  0 :  tensor([0.4052, 0.1316, 0.0831, 0.0610, 0.1442, 0.0764, 0.0985],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.1517, 0.2352, 0.1105, 0.1105, 0.1371, 0.1641, 0.0907],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0802, 0.1170, 0.2851, 0.0638, 0.0857, 0.3037, 0.0646],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1522, 0.1426, 0.1500, 0.1241, 0.1905, 0.1286, 0.1120],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2736, 0.1163, 0.0815, 0.1060, 0.2524, 0.1049, 0.0653],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0845, 0.0772, 0.1229, 0.0687, 0.0690, 0.5079, 0.0698],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1535, 0.1008, 0.0936, 0.0779, 0.0873, 0.1614, 0.3255],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
probs:  [0.20748122637675592, 0.11329378089193455, 0.3958561173463987, 0.11329378089193455, 0.05678131360104174, 0.11329378089193455]
actions average: 
K:  4  action  0 :  tensor([0.2976, 0.1063, 0.1352, 0.1089, 0.1002, 0.1473, 0.1046],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.1502, 0.3792, 0.1189, 0.0705, 0.0702, 0.1278, 0.0833],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1837, 0.0806, 0.1470, 0.1039, 0.1319, 0.2317, 0.1212],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1495, 0.1477, 0.1093, 0.1254, 0.1091, 0.1902, 0.1689],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1466, 0.0922, 0.1092, 0.1076, 0.3381, 0.1237, 0.0826],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1811, 0.1246, 0.0952, 0.1473, 0.0908, 0.1815, 0.1794],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1507, 0.3110, 0.0987, 0.1052, 0.1206, 0.1278, 0.0860],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
probs:  [0.20748122637675592, 0.11329378089193455, 0.3958561173463987, 0.11329378089193455, 0.05678131360104174, 0.11329378089193455]
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
probs:  [0.20748122637675592, 0.11329378089193455, 0.3958561173463987, 0.11329378089193455, 0.05678131360104174, 0.11329378089193455]
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
probs:  [0.20748122637675592, 0.11329378089193455, 0.3958561173463987, 0.11329378089193455, 0.05678131360104174, 0.11329378089193455]
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
probs:  [0.20748122637675592, 0.11329378089193455, 0.3958561173463987, 0.11329378089193455, 0.05678131360104174, 0.11329378089193455]
deleting a thread, now have 1 threads
Frames:  699 train batches done:  53 episodes:  20
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
probs:  [0.20748122637675592, 0.11329378089193455, 0.3958561173463987, 0.11329378089193455, 0.05678131360104174, 0.11329378089193455]
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
probs:  [0.20748122637675592, 0.11329378089193455, 0.3958561173463987, 0.11329378089193455, 0.05678131360104174, 0.11329378089193455]
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
probs:  [0.20748122637675592, 0.11329378089193455, 0.3958561173463987, 0.11329378089193455, 0.05678131360104174, 0.11329378089193455]
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
probs:  [0.20748122637675592, 0.11329378089193455, 0.3958561173463987, 0.11329378089193455, 0.05678131360104174, 0.11329378089193455]
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
probs:  [0.20748122637675592, 0.11329378089193455, 0.3958561173463987, 0.11329378089193455, 0.05678131360104174, 0.11329378089193455]
siam score:  -0.385601
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
probs:  [0.20748122637675592, 0.11329378089193455, 0.3958561173463987, 0.11329378089193455, 0.05678131360104174, 0.11329378089193455]
maxi score, test score, baseline:  -0.9599 -1.0 -0.9599
main train batch thing paused
add a thread
from probs:  [0.20748122637675592, 0.11329378089193455, 0.3958561173463987, 0.11329378089193455, 0.05678131360104174, 0.11329378089193455]
Adding thread: now have 2 threads
deleting a thread, now have 1 threads
Frames:  910 train batches done:  90 episodes:  23
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.2825556593135811, 0.13139610368717092, 0.2825556593135811, 0.13139610368717092, 0.04070037031132488, 0.13139610368717092]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.2825556593135811, 0.13139610368717092, 0.2825556593135811, 0.13139610368717092, 0.04070037031132488, 0.13139610368717092]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.2825556593135811, 0.13139610368717092, 0.2825556593135811, 0.13139610368717092, 0.04070037031132488, 0.13139610368717092]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.2825556593135811, 0.13139610368717092, 0.2825556593135811, 0.13139610368717092, 0.04070037031132488, 0.13139610368717092]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.2825556593135811, 0.13139610368717092, 0.2825556593135811, 0.13139610368717092, 0.04070037031132488, 0.13139610368717092]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.2825556593135811, 0.13139610368717092, 0.2825556593135811, 0.13139610368717092, 0.04070037031132488, 0.13139610368717092]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.2825556593135811, 0.13139610368717092, 0.2825556593135811, 0.13139610368717092, 0.04070037031132488, 0.13139610368717092]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.2825556593135811, 0.13139610368717092, 0.2825556593135811, 0.13139610368717092, 0.04070037031132488, 0.13139610368717092]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.2825556593135811, 0.13139610368717092, 0.2825556593135811, 0.13139610368717092, 0.04070037031132488, 0.13139610368717092]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
from probs:  [0.2825556593135811, 0.13139610368717092, 0.2825556593135811, 0.13139610368717092, 0.04070037031132488, 0.13139610368717092]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.2825556593135811, 0.13139610368717092, 0.2825556593135811, 0.13139610368717092, 0.04070037031132488, 0.13139610368717092]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.15479312830900332, 0.15479312830900332, 0.33289620367395334, 0.15479312830900332, 0.0479312830900333, 0.15479312830900332]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.15479312830900332, 0.15479312830900332, 0.33289620367395334, 0.15479312830900332, 0.0479312830900333, 0.15479312830900332]
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.1547931282779388, 0.1547931282779388, 0.3328962041088567, 0.1547931282779388, 0.04793128277938816, 0.1547931282779388]
actions average: 
K:  3  action  0 :  tensor([0.3363, 0.0432, 0.0937, 0.1155, 0.1519, 0.1311, 0.1282],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.1270, 0.3898, 0.0654, 0.1120, 0.1421, 0.0552, 0.1085],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1732, 0.0300, 0.3476, 0.1272, 0.1137, 0.1056, 0.1026],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1181, 0.1571, 0.1259, 0.1992, 0.1469, 0.1289, 0.1239],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2340, 0.0512, 0.0915, 0.1222, 0.2875, 0.1201, 0.0935],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1126, 0.0773, 0.1099, 0.0774, 0.1106, 0.3910, 0.1212],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2288, 0.0544, 0.0941, 0.0982, 0.1197, 0.1140, 0.2909],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.17331439241327468, 0.17331439241327468, 0.37274616481151507, 0.05365532897433052, 0.05365532897433052, 0.17331439241327468]
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.19687518436739546, 0.06093685471411599, 0.42343906712286117, 0.06093685471411599, 0.06093685471411599, 0.19687518436739546]
printing an ep nov before normalisation:  29.70850944519043
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
siam score:  -0.45372087
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.5792058222319802, 0.0841588355536039, 0.0841588355536039, 0.0841588355536039, 0.0841588355536039, 0.0841588355536039]
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.274140372763416, 0.039652286734144816, 0.13735565591300775, 0.274140372763416, 0.13735565591300775, 0.13735565591300775]
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.3175996699520901, 0.04592026403832816, 0.15912001650239546, 0.15912001650239546, 0.15912001650239546, 0.15912001650239546]
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.3581607160099576, 0.05177023706069228, 0.17943293662288595, 0.17943293662288595, 0.05177023706069228, 0.17943293662288595]
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.3581607160099576, 0.05177023706069228, 0.17943293662288595, 0.17943293662288595, 0.05177023706069228, 0.17943293662288595]
siam score:  -0.4221901
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.3581607160099576, 0.05177023706069228, 0.17943293662288595, 0.17943293662288595, 0.05177023706069228, 0.17943293662288595]
using explorer policy with actor:  1
printing an ep nov before normalisation:  27.323296070098877
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.41060450956416966, 0.05933401579176539, 0.20569672153026708, 0.20569672153026708, 0.05933401579176539, 0.05933401579176539]
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.4810489459604177, 0.06949396215768915, 0.06949396215768915, 0.24097520540882586, 0.06949396215768915, 0.06949396215768915]
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.3866580603838792, 0.05667096980806038, 0.05667096980806038, 0.3866580603838792, 0.05667096980806038, 0.05667096980806038]
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.3866580603838792, 0.05667096980806038, 0.05667096980806038, 0.3866580603838792, 0.05667096980806038, 0.05667096980806038]
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.03430825289421207, 0.03430825289421207, 0.23284587355289396, 0.23284587355289396, 0.23284587355289396, 0.23284587355289396]
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.03430825289421207, 0.03430825289421207, 0.23284587355289396, 0.23284587355289396, 0.23284587355289396, 0.23284587355289396]
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.042782515120227925, 0.042782515120227925, 0.042782515120227925, 0.2905508182131054, 0.2905508182131054, 0.2905508182131054]
siam score:  -0.42270288
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.12410550208315979, 0.12410550208315979, 0.03472705645779532, 0.2390206464586284, 0.2390206464586284, 0.2390206464586284]
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.12410550208315979, 0.12410550208315979, 0.03472705645779532, 0.2390206464586284, 0.2390206464586284, 0.2390206464586284]
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.12410550208315979, 0.12410550208315979, 0.03472705645779532, 0.2390206464586284, 0.2390206464586284, 0.2390206464586284]
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.12410550208315979, 0.12410550208315979, 0.03472705645779532, 0.2390206464586284, 0.2390206464586284, 0.2390206464586284]
from probs:  [0.12410550208315979, 0.12410550208315979, 0.03472705645779532, 0.2390206464586284, 0.2390206464586284, 0.2390206464586284]
maxi score, test score, baseline:  -0.9665666666666667 -1.0 -0.9665666666666667
probs:  [0.12410550208315979, 0.12410550208315979, 0.03472705645779532, 0.2390206464586284, 0.2390206464586284, 0.2390206464586284]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.12410550177058967, 0.12410550177058967, 0.03472705548882806, 0.23902064698999756, 0.23902064698999756, 0.23902064698999756]
printing an ep nov before normalisation:  25.16595702416164
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.20986968825159885, 0.05865911270433643, 0.05865911270433643, 0.20986968825159885, 0.404283285383793, 0.05865911270433643]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.247269148935388, 0.06909524076242506, 0.06909524076242506, 0.06909524076242506, 0.4763498880149118, 0.06909524076242506]
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
probs:  [0.2687153340599934, 0.14115449981833494, 0.14115449981833494, 0.03910583242500823, 0.2687153340599934, 0.14115449981833494]
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
siam score:  -0.4521096
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
probs:  [0.3080212003873123, 0.1617923724004375, 0.1617923724004375, 0.044809310010937776, 0.1617923724004375, 0.1617923724004375]
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
probs:  [0.3080212003873123, 0.1617923724004375, 0.1617923724004375, 0.044809310010937776, 0.1617923724004375, 0.1617923724004375]
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
probs:  [0.3488474987560267, 0.18322856049297204, 0.18322856049297204, 0.05073340988252858, 0.18322856049297204, 0.05073340988252858]
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
probs:  [0.2880040065032851, 0.18764981566096903, 0.18764981566096903, 0.1073664629871163, 0.18764981566096903, 0.04168008352669136]
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
probs:  [0.21384924005919986, 0.21384924005919986, 0.21384924005919986, 0.11339601928800012, 0.21384924005919986, 0.031207020475200402]
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
probs:  [0.21384924005919986, 0.21384924005919986, 0.21384924005919986, 0.11339601928800012, 0.21384924005919986, 0.031207020475200402]
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
probs:  [0.21384924005919986, 0.21384924005919986, 0.21384924005919986, 0.11339601928800012, 0.21384924005919986, 0.031207020475200402]
maxi score, test score, baseline:  -0.96865 -1.0 -0.96865
probs:  [0.21384924005919986, 0.21384924005919986, 0.21384924005919986, 0.11339601928800012, 0.21384924005919986, 0.031207020475200402]
using another actor
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.14190082895962816, 0.14190082895962816, 0.2676350819338235, 0.14190082895962816, 0.2676350819338235, 0.039027349253468506]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
printing an ep nov before normalisation:  20.997905731201172
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.1581718148262703, 0.1581718148262703, 0.2983368701928111, 0.04349131498091869, 0.2983368701928111, 0.04349131498091869]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.1581718148262703, 0.1581718148262703, 0.2983368701928111, 0.04349131498091869, 0.2983368701928111, 0.04349131498091869]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.290109556205272, 0.290109556205272, 0.043223777128061365, 0.043223777128061365, 0.290109556205272, 0.043223777128061365]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.290109556205272, 0.290109556205272, 0.043223777128061365, 0.043223777128061365, 0.290109556205272, 0.043223777128061365]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.290109556205272, 0.290109556205272, 0.043223777128061365, 0.043223777128061365, 0.290109556205272, 0.043223777128061365]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.290109556205272, 0.290109556205272, 0.043223777128061365, 0.043223777128061365, 0.290109556205272, 0.043223777128061365]
maxi score, test score, baseline:  -0.9713285714285714 -1.0 -0.9713285714285714
probs:  [0.29010955916018366, 0.29010955916018366, 0.04322377417314968, 0.04322377417314968, 0.29010955916018366, 0.04322377417314968]
printing an ep nov before normalisation:  36.03074743518533
maxi score, test score, baseline:  -0.9713285714285714 -1.0 -0.9713285714285714
probs:  [0.29010955916018366, 0.29010955916018366, 0.04322377417314968, 0.04322377417314968, 0.29010955916018366, 0.04322377417314968]
from probs:  [0.38527055993787257, 0.38527055993787257, 0.05736472003106367, 0.05736472003106367, 0.05736472003106367, 0.05736472003106367]
maxi score, test score, baseline:  -0.9713285714285714 -1.0 -0.9713285714285714
probs:  [0.2667482453457046, 0.2667482453457046, 0.14250904422689886, 0.14250904422689886, 0.03897637662789414, 0.14250904422689886]
maxi score, test score, baseline:  -0.9713285714285714 -1.0 -0.9713285714285714
probs:  [0.2667482453457046, 0.2667482453457046, 0.14250904422689886, 0.14250904422689886, 0.03897637662789414, 0.14250904422689886]
maxi score, test score, baseline:  -0.9713285714285714 -1.0 -0.9713285714285714
probs:  [0.2667482453457046, 0.2667482453457046, 0.14250904422689886, 0.14250904422689886, 0.03897637662789414, 0.14250904422689886]
actions average: 
K:  3  action  0 :  tensor([0.3908, 0.0916, 0.0933, 0.0924, 0.1128, 0.1141, 0.1049],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0407, 0.4529, 0.0432, 0.0673, 0.0556, 0.0411, 0.2992],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.2010, 0.0804, 0.2846, 0.1186, 0.1642, 0.0860, 0.0652],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1905, 0.0716, 0.1146, 0.2269, 0.0976, 0.1537, 0.1452],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1150, 0.0587, 0.0671, 0.1318, 0.3632, 0.1036, 0.1606],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0655, 0.0535, 0.1566, 0.0899, 0.1108, 0.4027, 0.1210],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1884, 0.0753, 0.1436, 0.0922, 0.1004, 0.1025, 0.2976],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.26674824648143614, 0.26674824648143614, 0.14250904395275682, 0.14250904395275682, 0.038976375178857436, 0.14250904395275682]
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.26674824648143614, 0.26674824648143614, 0.14250904395275682, 0.14250904395275682, 0.038976375178857436, 0.14250904395275682]
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.2975669934170679, 0.2975669934170679, 0.15896664744605482, 0.043466359136877235, 0.043466359136877235, 0.15896664744605482]
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.2975669934170679, 0.2975669934170679, 0.15896664744605482, 0.043466359136877235, 0.043466359136877235, 0.15896664744605482]
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.38492623752078464, 0.05753688123960765, 0.05753688123960765, 0.05753688123960765, 0.05753688123960765, 0.38492623752078464]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
siam score:  -0.47967216
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.16307040610612134, 0.16307040610612134, 0.04439380760812601, 0.16307040610612134, 0.16307040610612134, 0.30332456796738866]
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.18503099924661404, 0.050359226993666455, 0.050359226993666455, 0.18503099924661404, 0.18503099924661404, 0.34418854827282497]
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.18135077508225034, 0.04919379934199873, 0.04919379934199873, 0.33553391344587674, 0.04919379934199873, 0.33553391344587674]
using another actor
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
printing an ep nov before normalisation:  58.07915210723877
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.15878834932504224, 0.04499043216824628, 0.15878834932504224, 0.3105189055341038, 0.09809612684141776, 0.22881783680614762]
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.15878834932504224, 0.04499043216824628, 0.15878834932504224, 0.3105189055341038, 0.09809612684141776, 0.22881783680614762]
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.12130951197978447, 0.05561983967464464, 0.19638342318565843, 0.3840682012003436, 0.12130951197978447, 0.12130951197978447]
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.05952434379762573, 0.05952434379762573, 0.21019323533221457, 0.41108509071166627, 0.12983649318043383, 0.12983649318043383]
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.05952434379762573, 0.05952434379762573, 0.21019323533221457, 0.41108509071166627, 0.12983649318043383, 0.12983649318043383]
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.05952434379762573, 0.05952434379762573, 0.21019323533221457, 0.41108509071166627, 0.12983649318043383, 0.12983649318043383]
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.05952434379762573, 0.05952434379762573, 0.21019323533221457, 0.41108509071166627, 0.12983649318043383, 0.12983649318043383]
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.06471834540375343, 0.06471834540375343, 0.14117958635093825, 0.4470245501396784, 0.14117958635093825, 0.14117958635093825]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.06471834540375343, 0.06471834540375343, 0.14117958635093825, 0.4470245501396784, 0.14117958635093825, 0.14117958635093825]
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.06471834540375343, 0.06471834540375343, 0.14117958635093825, 0.4470245501396784, 0.14117958635093825, 0.14117958635093825]
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.06471834540375343, 0.06471834540375343, 0.14117958635093825, 0.4470245501396784, 0.14117958635093825, 0.14117958635093825]
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.06471834540375343, 0.06471834540375343, 0.14117958635093825, 0.4470245501396784, 0.14117958635093825, 0.14117958635093825]
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.06471834540375343, 0.06471834540375343, 0.14117958635093825, 0.4470245501396784, 0.14117958635093825, 0.14117958635093825]
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.06471834540375343, 0.06471834540375343, 0.14117958635093825, 0.4470245501396784, 0.14117958635093825, 0.14117958635093825]
printing an ep nov before normalisation:  107.06965497931817
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.07007008212015074, 0.07007008212015074, 0.07007008212015074, 0.4840554444623618, 0.15286715458859298, 0.15286715458859298]
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
probs:  [0.0700700806983804, 0.0700700806983804, 0.0700700806983804, 0.48405544913389303, 0.15286715438548282, 0.15286715438548282]
using explorer policy with actor:  1
siam score:  -0.5035167
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
probs:  [0.07638887633137746, 0.07638887633137746, 0.07638887633137746, 0.5277778280078236, 0.07638887633137746, 0.16666666666666657]
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
Printing some Q and Qe and total Qs values:  [[0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]] [[71.35]
 [71.35]
 [71.35]
 [71.35]
 [71.35]
 [71.35]
 [71.35]] [[1.616]
 [1.616]
 [1.616]
 [1.616]
 [1.616]
 [1.616]
 [1.616]]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.12201504947795928, 0.12201504947795928, 0.06460582737819265, 0.4473339747099703, 0.12201504947795928, 0.12201504947795928]
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.12944465703395522, 0.12944465703395522, 0.06853591399860956, 0.47459420090091514, 0.12944465703395522, 0.06853591399860956]
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.12944465703395522, 0.12944465703395522, 0.06853591399860956, 0.47459420090091514, 0.12944465703395522, 0.06853591399860956]
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.12944465703395522, 0.12944465703395522, 0.06853591399860956, 0.47459420090091514, 0.12944465703395522, 0.06853591399860956]
using another actor
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.10419403291251746, 0.15238720752286108, 0.061355655481100765, 0.42548186364814217, 0.15238720752286108, 0.10419403291251746]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.10840333724164647, 0.16633656281723305, 0.05690713673001371, 0.3936130631522272, 0.16633656281723305, 0.10840333724164647]
printing an ep nov before normalisation:  9.609960610937378
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.10840333724164647, 0.16633656281723305, 0.05690713673001371, 0.3936130631522272, 0.16633656281723305, 0.10840333724164647]
siam score:  -0.49774688
actions average: 
K:  4  action  0 :  tensor([0.1839, 0.0632, 0.1256, 0.1546, 0.1524, 0.1807, 0.1396],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.1291, 0.5324, 0.0340, 0.0784, 0.0712, 0.1193, 0.0356],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1063, 0.0029, 0.2448, 0.1698, 0.1688, 0.1337, 0.1736],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1559, 0.1049, 0.1441, 0.1712, 0.1858, 0.1275, 0.1106],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1974, 0.0776, 0.0720, 0.2828, 0.1281, 0.1727, 0.0693],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1254, 0.0108, 0.2410, 0.1298, 0.1450, 0.2233, 0.1248],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1234, 0.1107, 0.1370, 0.1270, 0.1456, 0.1632, 0.1932],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.1309820209246449, 0.21175325499940076, 0.05918536841375061, 0.4079119663238083, 0.1309820209246449, 0.05918536841375061]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.1309820209246449, 0.21175325499940076, 0.05918536841375061, 0.4079119663238083, 0.1309820209246449, 0.05918536841375061]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.1309820209246449, 0.21175325499940076, 0.05918536841375061, 0.4079119663238083, 0.1309820209246449, 0.05918536841375061]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.1309820209246449, 0.21175325499940076, 0.05918536841375061, 0.4079119663238083, 0.1309820209246449, 0.05918536841375061]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.1309820209246449, 0.21175325499940076, 0.05918536841375061, 0.4079119663238083, 0.1309820209246449, 0.05918536841375061]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.1309820204298253, 0.21175325562459213, 0.05918536692336581, 0.4079119696690257, 0.1309820204298253, 0.05918536692336581]
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.14358740810310963, 0.24574150338442755, 0.052783767853049085, 0.3615161447032549, 0.14358740810310963, 0.052783767853049085]
printing an ep nov before normalisation:  11.798347155587852
actions average: 
K:  1  action  0 :  tensor([0.4621, 0.0373, 0.0659, 0.0834, 0.1832, 0.1117, 0.0565],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0169, 0.9058, 0.0099, 0.0275, 0.0164, 0.0129, 0.0104],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1202, 0.0848, 0.2962, 0.1264, 0.1272, 0.1199, 0.1253],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1460, 0.0316, 0.1077, 0.2743, 0.1465, 0.1998, 0.0940],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1075, 0.0172, 0.0799, 0.1127, 0.4531, 0.1561, 0.0735],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0852, 0.0238, 0.0691, 0.0970, 0.1302, 0.5405, 0.0542],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1023, 0.0132, 0.1383, 0.1702, 0.1392, 0.2168, 0.2200],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.1599237199755385, 0.1599237199755385, 0.05877951960861781, 0.4026698008561489, 0.1599237199755385, 0.05877951960861781]
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.20611230362562105, 0.20611230362562105, 0.12066237273116401, 0.30224347588188555, 0.04420717140454438, 0.12066237273116401]
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.33930011735263227, 0.18674032372317437, 0.18674032372317437, 0.18674032372317437, 0.050239455738922356, 0.050239455738922356]
siam score:  -0.50423884
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.3929648759253571, 0.2162662741754208, 0.058167525241267136, 0.2162662741754208, 0.058167525241267136, 0.058167525241267136]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.46680395888727166, 0.2568919876409591, 0.06907601336794233, 0.06907601336794233, 0.06907601336794233, 0.06907601336794233]
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.38254407163016113, 0.38254407163016113, 0.05872796418491943, 0.05872796418491943, 0.05872796418491943, 0.05872796418491943]
printing an ep nov before normalisation:  44.98557770754147
printing an ep nov before normalisation:  32.77670383453369
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.26288713636508426, 0.26288713636508426, 0.14506615306089948, 0.039027268087132916, 0.14506615306089948, 0.14506615306089948]
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.26288713636508426, 0.26288713636508426, 0.14506615306089948, 0.039027268087132916, 0.14506615306089948, 0.14506615306089948]
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.338750624755728, 0.18691183820655619, 0.18691183820655619, 0.050256930312301766, 0.18691183820655619, 0.050256930312301766]
actions average: 
K:  3  action  0 :  tensor([0.2904, 0.0491, 0.1065, 0.1204, 0.2018, 0.1213, 0.1105],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0792, 0.4935, 0.0610, 0.0915, 0.1105, 0.0798, 0.0846],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0897, 0.0107, 0.4911, 0.0699, 0.1002, 0.1834, 0.0550],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1407, 0.0439, 0.1524, 0.1746, 0.1794, 0.1567, 0.1524],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1813, 0.0189, 0.1043, 0.1367, 0.2861, 0.1703, 0.1024],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0856, 0.0683, 0.1134, 0.1047, 0.0962, 0.4289, 0.1030],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1401, 0.0461, 0.1258, 0.1342, 0.1468, 0.1395, 0.2675],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  61.04044437408447
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.2350155749599368, 0.2350155749599368, 0.12999164270442426, 0.12999164270442426, 0.2350155749599368, 0.03496998971134108]
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.2350155749599368, 0.2350155749599368, 0.12999164270442426, 0.12999164270442426, 0.2350155749599368, 0.03496998971134108]
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
maxi score, test score, baseline:  -0.9760904761904762 -1.0 -0.9760904761904762
probs:  [0.2938116491444042, 0.16249797871657684, 0.04369037213901891, 0.16249797871657684, 0.2938116491444042, 0.04369037213901891]
maxi score, test score, baseline:  -0.9760904761904762 -1.0 -0.9760904761904762
probs:  [0.2547431781688538, 0.1750603812093033, 0.10296642205542429, 0.1750603812093033, 0.2547431781688538, 0.0374264591882616]
using another actor
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.25474318114336425, 0.17506038149277486, 0.10296641990414653, 0.17506038149277486, 0.25474318114336425, 0.03742645482357529]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.21260980601950016, 0.21260980601950016, 0.11785208110428125, 0.21260980601950016, 0.21260980601950016, 0.03170869481771813]
Printing some Q and Qe and total Qs values:  [[0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]] [[29.92]
 [29.92]
 [29.92]
 [29.92]
 [29.92]
 [29.92]
 [29.92]] [[0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.553]]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.1301853866543818, 0.2348707988635473, 0.1301853866543818, 0.2348707988635473, 0.2348707988635473, 0.03501683010059445]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.1301853866543818, 0.2348707988635473, 0.1301853866543818, 0.2348707988635473, 0.2348707988635473, 0.03501683010059445]
siam score:  -0.50434846
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.1301853866543818, 0.2348707988635473, 0.1301853866543818, 0.2348707988635473, 0.2348707988635473, 0.03501683010059445]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.1301853866543818, 0.2348707988635473, 0.1301853866543818, 0.2348707988635473, 0.2348707988635473, 0.03501683010059445]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.1301853866543818, 0.2348707988635473, 0.1301853866543818, 0.2348707988635473, 0.2348707988635473, 0.03501683010059445]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.1301853866543818, 0.2348707988635473, 0.1301853866543818, 0.2348707988635473, 0.2348707988635473, 0.03501683010059445]
from probs:  [0.1301853866543818, 0.2348707988635473, 0.1301853866543818, 0.2348707988635473, 0.2348707988635473, 0.03501683010059445]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.14540543384071097, 0.262342214383467, 0.14540543384071097, 0.262342214383467, 0.14540543384071097, 0.03909926971093305]
printing an ep nov before normalisation:  74.26928884068161
maxi score, test score, baseline:  -0.9776777777777778 -1.0 -0.9776777777777778
probs:  [0.14540543329852992, 0.2623422168232816, 0.14540543329852992, 0.2623422168232816, 0.14540543329852992, 0.03909926645784691]
maxi score, test score, baseline:  -0.9776777777777778 -1.0 -0.9776777777777778
probs:  [0.14540543329852992, 0.2623422168232816, 0.14540543329852992, 0.2623422168232816, 0.14540543329852992, 0.03909926645784691]
maxi score, test score, baseline:  -0.9776777777777778 -1.0 -0.9776777777777778
probs:  [0.17522649579153698, 0.25445710927661713, 0.17522649579153698, 0.25445710927661713, 0.03743412451313675, 0.10319866535055505]
maxi score, test score, baseline:  -0.9776777777777778 -1.0 -0.9776777777777778
maxi score, test score, baseline:  -0.9776777777777778 -1.0 -0.9776777777777778
probs:  [0.1112053496096997, 0.2742143727649637, 0.18882869396934898, 0.2742143727649637, 0.04033186128132433, 0.1112053496096997]
siam score:  -0.5182975
maxi score, test score, baseline:  -0.9776777777777778 -1.0 -0.9776777777777778
from probs:  [0.13179519834470788, 0.22380786810436973, 0.22380786810436973, 0.3250218048399982, 0.0477836303032772, 0.0477836303032772]
maxi score, test score, baseline:  -0.9776777777777778 -1.0 -0.9776777777777778
probs:  [0.1451491778024539, 0.2464943471970395, 0.1451491778024539, 0.35797403353108387, 0.05261663183348437, 0.05261663183348437]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.1451491773028469, 0.2464943490505302, 0.1451491773028469, 0.357974037972982, 0.052616629185396946, 0.052616629185396946]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.1451491768251931, 0.24649435082257753, 0.1451491768251931, 0.35797404221970036, 0.052616626653667946, 0.052616626653667946]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.1451491768251931, 0.24649435082257753, 0.1451491768251931, 0.35797404221970036, 0.052616626653667946, 0.052616626653667946]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.05797296696364692, 0.2716374250544677, 0.1599491855979021, 0.39449448845668944, 0.05797296696364692, 0.05797296696364692]
printing an ep nov before normalisation:  70.57215902540419
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.06525175963701098, 0.18006108834982887, 0.18006108834982887, 0.4441225443893093, 0.06525175963701098, 0.06525175963701098]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([0.3834, 0.0683, 0.1268, 0.0530, 0.1685, 0.1015, 0.0985],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0375, 0.7490, 0.0345, 0.0416, 0.0498, 0.0481, 0.0396],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1124, 0.0508, 0.3833, 0.0763, 0.1353, 0.1212, 0.1206],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2127, 0.0661, 0.1546, 0.0983, 0.2072, 0.1390, 0.1221],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1562, 0.0573, 0.0961, 0.0685, 0.3263, 0.1839, 0.1117],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0971, 0.0123, 0.1173, 0.0444, 0.0964, 0.5134, 0.1192],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1354, 0.0941, 0.1154, 0.1085, 0.1674, 0.1903, 0.1890],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.06931020238311081, 0.25803196391738814, 0.06931020238311081, 0.4647272265501687, 0.06931020238311081, 0.06931020238311081]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.06931020238311081, 0.25803196391738814, 0.06931020238311081, 0.4647272265501687, 0.06931020238311081, 0.06931020238311081]
actions average: 
K:  1  action  0 :  tensor([0.3630, 0.0825, 0.0783, 0.0747, 0.1714, 0.0971, 0.1329],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0442, 0.7424, 0.0320, 0.0508, 0.0424, 0.0312, 0.0570],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1564, 0.0227, 0.3297, 0.1304, 0.1365, 0.1086, 0.1157],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1815, 0.0985, 0.1239, 0.1551, 0.1819, 0.1286, 0.1305],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2810, 0.0105, 0.0926, 0.1038, 0.2695, 0.1186, 0.1238],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1237, 0.1334, 0.1549, 0.0770, 0.1119, 0.3103, 0.0888],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1968, 0.1085, 0.0861, 0.0999, 0.1364, 0.1069, 0.2653],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.06931020238311081, 0.25803196391738814, 0.06931020238311081, 0.4647272265501687, 0.06931020238311081, 0.06931020238311081]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.06931020238311081, 0.25803196391738814, 0.06931020238311081, 0.4647272265501687, 0.06931020238311081, 0.06931020238311081]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.06931020238311081, 0.25803196391738814, 0.06931020238311081, 0.4647272265501687, 0.06931020238311081, 0.06931020238311081]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.06931020238311081, 0.25803196391738814, 0.06931020238311081, 0.4647272265501687, 0.06931020238311081, 0.06931020238311081]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.06931020238311081, 0.25803196391738814, 0.06931020238311081, 0.4647272265501687, 0.06931020238311081, 0.06931020238311081]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
actions average: 
K:  0  action  0 :  tensor([0.2265, 0.0108, 0.1180, 0.1642, 0.1599, 0.1084, 0.2123],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0274, 0.8357, 0.0222, 0.0355, 0.0237, 0.0203, 0.0352],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1472, 0.0200, 0.3288, 0.1195, 0.1213, 0.1486, 0.1147],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1427, 0.0502, 0.1127, 0.2178, 0.1501, 0.1118, 0.2147],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1659, 0.0162, 0.1057, 0.1333, 0.3174, 0.0982, 0.1632],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1228, 0.0176, 0.1167, 0.1029, 0.1178, 0.3921, 0.1301],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1975, 0.0471, 0.1106, 0.1384, 0.0998, 0.0921, 0.3146],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  68.52517126271198
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.06931020238311081, 0.25803196391738814, 0.06931020238311081, 0.4647272265501687, 0.06931020238311081, 0.06931020238311081]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.059397701290984456, 0.38120459741803114, 0.059397701290984456, 0.38120459741803114, 0.059397701290984456, 0.059397701290984456]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.059397701290984456, 0.38120459741803114, 0.059397701290984456, 0.38120459741803114, 0.059397701290984456, 0.059397701290984456]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.059397701290984456, 0.38120459741803114, 0.059397701290984456, 0.38120459741803114, 0.059397701290984456, 0.059397701290984456]
printing an ep nov before normalisation:  46.61931607293576
siam score:  -0.53139204
main train batch thing paused
add a thread
Adding thread: now have 2 threads
Printing some Q and Qe and total Qs values:  [[1.34 ]
 [1.34 ]
 [1.34 ]
 [1.34 ]
 [1.345]
 [1.34 ]
 [1.34 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.34 ]
 [1.34 ]
 [1.34 ]
 [1.34 ]
 [1.345]
 [1.34 ]
 [1.34 ]]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
printing an ep nov before normalisation:  62.62294323082813
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.04560661315918
main train batch thing paused
add a thread
Adding thread: now have 4 threads
printing an ep nov before normalisation:  37.72182944707872
printing an ep nov before normalisation:  48.66099646356038
printing an ep nov before normalisation:  63.548148402250874
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.18743193061360164, 0.18743193061360164, 0.050381188563831, 0.33694183103153313, 0.18743193061360164, 0.050381188563831]
printing an ep nov before normalisation:  52.49773979187012
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.18743193061360164, 0.18743193061360164, 0.050381188563831, 0.33694183103153313, 0.18743193061360164, 0.050381188563831]
main train batch thing paused
add a thread
Adding thread: now have 5 threads
Printing some Q and Qe and total Qs values:  [[1.166]
 [1.166]
 [1.166]
 [1.166]
 [1.107]
 [1.166]
 [1.166]] [[21.025]
 [21.025]
 [21.025]
 [21.025]
 [29.277]
 [21.025]
 [21.025]] [[1.774]
 [1.774]
 [1.774]
 [1.774]
 [2.224]
 [1.774]
 [1.774]]
printing an ep nov before normalisation:  21.22087637182977
printing an ep nov before normalisation:  29.101791381835938
from probs:  [0.18743193061360164, 0.18743193061360164, 0.050381188563831, 0.33694183103153313, 0.18743193061360164, 0.050381188563831]
maxi score, test score, baseline:  -0.9790666666666666 -1.0 -0.9790666666666666
probs:  [0.23184879202994474, 0.23184879202994474, 0.03630241594011045, 0.23184879202994474, 0.23184879202994474, 0.03630241594011045]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  -0.9790666666666666 -1.0 -0.9790666666666666
printing an ep nov before normalisation:  74.18070855786252
main train batch thing paused
add a thread
using another actor
Adding thread: now have 6 threads
printing an ep nov before normalisation:  39.66397762298584
printing an ep nov before normalisation:  45.337878044897565
from probs:  [0.23184879202994474, 0.23184879202994474, 0.03630241594011045, 0.23184879202994474, 0.23184879202994474, 0.03630241594011045]
Printing some Q and Qe and total Qs values:  [[0.708]
 [0.708]
 [0.708]
 [0.73 ]
 [0.727]
 [0.708]
 [0.726]] [[48.415]
 [48.415]
 [48.415]
 [61.592]
 [57.814]
 [48.415]
 [47.985]] [[1.432]
 [1.432]
 [1.432]
 [1.941]
 [1.798]
 [1.432]
 [1.434]]
actions average: 
K:  0  action  0 :  tensor([0.3723, 0.0644, 0.0944, 0.1013, 0.1872, 0.0748, 0.1057],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0277, 0.7907, 0.0272, 0.0373, 0.0183, 0.0345, 0.0643],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1107, 0.0085, 0.3643, 0.1342, 0.0970, 0.1380, 0.1473],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1250, 0.0568, 0.1447, 0.2337, 0.1404, 0.1352, 0.1641],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2524, 0.0211, 0.1132, 0.1196, 0.2967, 0.0862, 0.1107],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0887, 0.0423, 0.1191, 0.1275, 0.1082, 0.3950, 0.1192],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1115, 0.0943, 0.1516, 0.1846, 0.1113, 0.1076, 0.2392],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9794918367346939 -1.0 -0.9794918367346939
probs:  [0.2882301824310701, 0.2882301824310701, 0.04510315090226323, 0.2882301824310701, 0.04510315090226323, 0.04510315090226323]
Printing some Q and Qe and total Qs values:  [[0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]] [[46.052]
 [46.052]
 [46.052]
 [46.052]
 [46.052]
 [46.052]
 [46.052]] [[0.741]
 [0.741]
 [0.741]
 [0.741]
 [0.741]
 [0.741]
 [0.741]]
maxi score, test score, baseline:  -0.9794918367346939 -1.0 -0.9794918367346939
probs:  [0.2882301824310701, 0.2882301824310701, 0.04510315090226323, 0.2882301824310701, 0.04510315090226323, 0.04510315090226323]
maxi score, test score, baseline:  -0.9794918367346939 -1.0 -0.9794918367346939
probs:  [0.2882301824310701, 0.2882301824310701, 0.04510315090226323, 0.2882301824310701, 0.04510315090226323, 0.04510315090226323]
printing an ep nov before normalisation:  54.03699318300582
Printing some Q and Qe and total Qs values:  [[0.166]
 [0.2  ]
 [0.21 ]
 [0.286]
 [0.306]
 [0.2  ]
 [0.2  ]] [[33.396]
 [32.22 ]
 [23.645]
 [20.862]
 [18.631]
 [32.22 ]
 [32.22 ]] [[0.833]
 [0.833]
 [0.597]
 [0.593]
 [0.549]
 [0.833]
 [0.833]]
printing an ep nov before normalisation:  23.164149684584974
maxi score, test score, baseline:  -0.9794918367346939 -1.0 -0.9794918367346939
probs:  [0.2882301824310701, 0.2882301824310701, 0.04510315090226323, 0.2882301824310701, 0.04510315090226323, 0.04510315090226323]
main train batch thing paused
printing an ep nov before normalisation:  26.05184942921028
from probs:  [0.2882301824310701, 0.2882301824310701, 0.04510315090226323, 0.2882301824310701, 0.04510315090226323, 0.04510315090226323]
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.5613353043764346, 0.08773293912471308, 0.08773293912471308, 0.08773293912471308, 0.08773293912471308, 0.08773293912471308]
printing an ep nov before normalisation:  84.9072158417301
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.5613353043764346, 0.08773293912471308, 0.08773293912471308, 0.08773293912471308, 0.08773293912471308, 0.08773293912471308]
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.5613353043764346, 0.08773293912471308, 0.08773293912471308, 0.08773293912471308, 0.08773293912471308, 0.08773293912471308]
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.5613353043764346, 0.08773293912471308, 0.08773293912471308, 0.08773293912471308, 0.08773293912471308, 0.08773293912471308]
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.5613353043764346, 0.08773293912471308, 0.08773293912471308, 0.08773293912471308, 0.08773293912471308, 0.08773293912471308]
printing an ep nov before normalisation:  32.849484741396026
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.5613353043764346, 0.08773293912471308, 0.08773293912471308, 0.08773293912471308, 0.08773293912471308, 0.08773293912471308]
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.5613353043764346, 0.08773293912471308, 0.08773293912471308, 0.08773293912471308, 0.08773293912471308, 0.08773293912471308]
main train batch thing paused
line 256 mcts: sample exp_bonus 77.68470811846778
printing an ep nov before normalisation:  45.090545912125535
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.452]
 [0.497]
 [0.516]
 [0.516]
 [0.516]
 [0.515]] [[43.114]
 [16.736]
 [15.646]
 [12.166]
 [12.398]
 [12.797]
 [12.96 ]] [[1.261]
 [0.698]
 [0.719]
 [0.657]
 [0.663]
 [0.672]
 [0.675]]
printing an ep nov before normalisation:  16.762754211306582
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]] [[23.107]
 [23.107]
 [23.107]
 [23.107]
 [23.107]
 [23.107]
 [23.107]] [[1.063]
 [1.063]
 [1.063]
 [1.063]
 [1.063]
 [1.063]
 [1.063]]
printing an ep nov before normalisation:  28.449263639482915
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
probs:  [0.33656226108043746, 0.18753103791046336, 0.18753103791046336, 0.05042231259408623, 0.18753103791046336, 0.05042231259408623]
printing an ep nov before normalisation:  38.89354759578552
printing an ep nov before normalisation:  22.0396847854599
printing an ep nov before normalisation:  40.52499033882384
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
probs:  [0.35926145332702597, 0.0530153261285749, 0.1469308051360998, 0.1469308051360998, 0.1469308051360998, 0.1469308051360998]
printing an ep nov before normalisation:  59.8751610499277
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
printing an ep nov before normalisation:  24.79928970336914
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.17518184134885
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
printing an ep nov before normalisation:  32.21260504931513
Printing some Q and Qe and total Qs values:  [[1.385]
 [1.384]
 [1.384]
 [1.384]
 [1.384]
 [1.385]
 [1.384]] [[0.005]
 [0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.005]
 [0.006]] [[1.385]
 [1.384]
 [1.384]
 [1.384]
 [1.384]
 [1.385]
 [1.384]]
printing an ep nov before normalisation:  18.112876462782655
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.2612860480392166, 0.14603537298393027, 0.03932178496977594, 0.14603537298393027, 0.2612860480392166, 0.14603537298393027]
Printing some Q and Qe and total Qs values:  [[1.146]
 [1.146]
 [1.146]
 [1.146]
 [1.146]
 [1.146]
 [1.146]] [[19.88]
 [19.88]
 [19.88]
 [19.88]
 [19.88]
 [19.88]
 [19.88]] [[2.146]
 [2.146]
 [2.146]
 [2.146]
 [2.146]
 [2.146]
 [2.146]]
printing an ep nov before normalisation:  36.92354145194279
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.23636677307002515, 0.16408518124432014, 0.03501091012698927, 0.16408518124432014, 0.23636677307002515, 0.16408518124432014]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.23636677307002515, 0.16408518124432014, 0.03501091012698927, 0.16408518124432014, 0.23636677307002515, 0.16408518124432014]
Printing some Q and Qe and total Qs values:  [[0.511]
 [0.511]
 [0.511]
 [0.507]
 [0.506]
 [0.506]
 [0.507]] [[18.006]
 [18.006]
 [18.006]
 [22.777]
 [23.205]
 [23.179]
 [22.689]] [[0.511]
 [0.511]
 [0.511]
 [0.507]
 [0.506]
 [0.506]
 [0.507]]
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
Printing some Q and Qe and total Qs values:  [[1.177]
 [1.171]
 [1.171]
 [1.207]
 [1.214]
 [1.218]
 [1.239]] [[31.082]
 [32.123]
 [35.233]
 [32.104]
 [31.473]
 [30.913]
 [30.821]] [[2.208]
 [2.27 ]
 [2.472]
 [2.305]
 [2.27 ]
 [2.238]
 [2.253]]
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.17687023327613227, 0.17687023327613227, 0.03773068860160255, 0.17687023327613227, 0.25478837829386836, 0.17687023327613227]
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.17687023327613227, 0.17687023327613227, 0.03773068860160255, 0.17687023327613227, 0.25478837829386836, 0.17687023327613227]
printing an ep nov before normalisation:  56.939423052120915
printing an ep nov before normalisation:  49.92180048663521
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.17687023327613227, 0.17687023327613227, 0.03773068860160255, 0.17687023327613227, 0.25478837829386836, 0.17687023327613227]
siam score:  -0.55027163
printing an ep nov before normalisation:  37.3694771636623
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.17687023327613227, 0.17687023327613227, 0.03773068860160255, 0.17687023327613227, 0.25478837829386836, 0.17687023327613227]
actions average: 
K:  1  action  0 :  tensor([0.4632, 0.0055, 0.0860, 0.1024, 0.1710, 0.0600, 0.1119],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0529, 0.7106, 0.0389, 0.0739, 0.0358, 0.0322, 0.0557],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0603, 0.0199, 0.5050, 0.1229, 0.0765, 0.1387, 0.0767],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1238, 0.1444, 0.1161, 0.1980, 0.1556, 0.1149, 0.1472],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1849, 0.0089, 0.0978, 0.1119, 0.3538, 0.1428, 0.0999],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1648, 0.0226, 0.1123, 0.1395, 0.1298, 0.3168, 0.1143],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2297, 0.0229, 0.0712, 0.1175, 0.1147, 0.0767, 0.3672],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.17687023327613227, 0.17687023327613227, 0.03773068860160255, 0.17687023327613227, 0.25478837829386836, 0.17687023327613227]
printing an ep nov before normalisation:  13.438271486006506
printing an ep nov before normalisation:  26.694315255609684
printing an ep nov before normalisation:  58.079170788762724
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.12237678508544346, 0.2067002223661042, 0.04407645046768759, 0.12237678508544346, 0.29776953462921707, 0.2067002223661042]
printing an ep nov before normalisation:  66.31844308697455
printing an ep nov before normalisation:  28.601285473378464
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.13106863348584966, 0.23418017787166429, 0.035322199413307866, 0.13106863348584966, 0.23418017787166429, 0.23418017787166429]
printing an ep nov before normalisation:  57.256652858813055
Printing some Q and Qe and total Qs values:  [[1.075]
 [1.075]
 [1.075]
 [1.075]
 [1.075]
 [1.075]
 [1.075]] [[71.675]
 [71.675]
 [71.675]
 [71.675]
 [71.675]
 [71.675]
 [71.675]] [[2.742]
 [2.742]
 [2.742]
 [2.742]
 [2.742]
 [2.742]
 [2.742]]
siam score:  -0.5591768
printing an ep nov before normalisation:  58.48367953938534
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.13106863348584966, 0.23418017787166429, 0.035322199413307866, 0.13106863348584966, 0.23418017787166429, 0.23418017787166429]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.13106863348584966, 0.23418017787166429, 0.035322199413307866, 0.13106863348584966, 0.23418017787166429, 0.23418017787166429]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.13106863348584966, 0.23418017787166429, 0.035322199413307866, 0.13106863348584966, 0.23418017787166429, 0.23418017787166429]
Printing some Q and Qe and total Qs values:  [[0.71 ]
 [0.71 ]
 [0.677]
 [0.68 ]
 [0.681]
 [0.71 ]
 [0.71 ]] [[35.851]
 [35.851]
 [43.023]
 [43.161]
 [42.672]
 [35.851]
 [35.851]] [[1.794]
 [1.794]
 [2.144]
 [2.155]
 [2.13 ]
 [1.794]
 [1.794]]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.1461351621121947, 0.26111158761723663, 0.03937133842894259, 0.1461351621121947, 0.26111158761723663, 0.1461351621121947]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.1461351621121947, 0.26111158761723663, 0.03937133842894259, 0.1461351621121947, 0.26111158761723663, 0.1461351621121947]
printing an ep nov before normalisation:  31.78785800933838
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.16360163661006016, 0.29233289898753145, 0.04406546440240837, 0.16360163661006016, 0.29233289898753145, 0.04406546440240837]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.16360163661006016, 0.29233289898753145, 0.04406546440240837, 0.16360163661006016, 0.29233289898753145, 0.04406546440240837]
printing an ep nov before normalisation:  31.794464607243892
printing an ep nov before normalisation:  51.386860991726024
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.1858150926698642, 0.33203943669428443, 0.050035344647189, 0.050035344647189, 0.33203943669428443, 0.050035344647189]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.22575118712859854, 0.3248082358865247, 0.13376964185338175, 0.04813165142473157, 0.13376964185338175, 0.13376964185338175]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.22575118712859854, 0.3248082358865247, 0.13376964185338175, 0.04813165142473157, 0.13376964185338175, 0.13376964185338175]
printing an ep nov before normalisation:  85.22228894026307
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.22575118712859854, 0.3248082358865247, 0.13376964185338175, 0.04813165142473157, 0.13376964185338175, 0.13376964185338175]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.22575118712859854, 0.3248082358865247, 0.13376964185338175, 0.04813165142473157, 0.13376964185338175, 0.13376964185338175]
Printing some Q and Qe and total Qs values:  [[1.028]
 [0.979]
 [1.01 ]
 [1.031]
 [1.037]
 [1.033]
 [1.037]] [[30.611]
 [24.268]
 [17.038]
 [20.254]
 [17.197]
 [18.763]
 [13.974]] [[1.262]
 [1.149]
 [1.107]
 [1.161]
 [1.136]
 [1.147]
 [1.104]]
siam score:  -0.5395824
Printing some Q and Qe and total Qs values:  [[0.71 ]
 [0.608]
 [0.699]
 [0.718]
 [0.716]
 [0.702]
 [0.697]] [[24.107]
 [25.255]
 [24.454]
 [23.304]
 [22.387]
 [22.021]
 [20.84 ]] [[0.71 ]
 [0.608]
 [0.699]
 [0.718]
 [0.716]
 [0.702]
 [0.697]]
actions average: 
K:  1  action  0 :  tensor([0.3256, 0.0421, 0.1298, 0.0990, 0.1235, 0.1690, 0.1110],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0462, 0.6195, 0.0363, 0.1122, 0.0435, 0.0674, 0.0749],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1483, 0.0139, 0.3855, 0.0856, 0.1086, 0.1628, 0.0953],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2270, 0.1563, 0.0761, 0.1260, 0.1273, 0.1338, 0.1535],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2108, 0.0168, 0.1133, 0.0894, 0.2439, 0.2008, 0.1250],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1471, 0.0219, 0.1086, 0.0896, 0.1024, 0.4713, 0.0591],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1826, 0.0867, 0.1240, 0.1304, 0.1112, 0.1285, 0.2366],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  53.315730785753814
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.24690076975564015, 0.3552434943258957, 0.146296811226118, 0.05263105673311412, 0.146296811226118, 0.05263105673311412]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.24690076975564015, 0.3552434943258957, 0.146296811226118, 0.05263105673311412, 0.146296811226118, 0.05263105673311412]
Printing some Q and Qe and total Qs values:  [[1.24 ]
 [1.263]
 [1.263]
 [1.263]
 [1.263]
 [1.263]
 [1.263]] [[41.373]
 [33.225]
 [33.225]
 [33.225]
 [33.225]
 [33.225]
 [33.225]] [[1.843]
 [1.725]
 [1.725]
 [1.725]
 [1.725]
 [1.725]
 [1.725]]
Printing some Q and Qe and total Qs values:  [[0.784]
 [0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]] [[20.017]
 [19.595]
 [19.595]
 [19.595]
 [19.595]
 [19.595]
 [19.595]] [[1.941]
 [1.897]
 [1.897]
 [1.897]
 [1.897]
 [1.897]
 [1.897]]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.24690076975564015, 0.3552434943258957, 0.146296811226118, 0.05263105673311412, 0.146296811226118, 0.05263105673311412]
printing an ep nov before normalisation:  57.74592799652549
printing an ep nov before normalisation:  13.686711905195258
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.37060036304091
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.22885553225120986, 0.2999251872352347, 0.1628622811946154, 0.04407442929274532, 0.1628622811946154, 0.10142028883157933]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.22885553225120986, 0.2999251872352347, 0.1628622811946154, 0.04407442929274532, 0.1628622811946154, 0.10142028883157933]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.127]
 [1.127]
 [1.127]
 [1.127]
 [1.127]
 [1.127]
 [1.127]] [[20.569]
 [20.569]
 [20.569]
 [20.569]
 [20.569]
 [20.569]
 [20.569]] [[1.823]
 [1.823]
 [1.823]
 [1.823]
 [1.823]
 [1.823]
 [1.823]]
Printing some Q and Qe and total Qs values:  [[0.711]
 [0.563]
 [0.705]
 [0.715]
 [0.716]
 [0.718]
 [0.708]] [[27.985]
 [29.711]
 [29.688]
 [29.485]
 [29.389]
 [29.234]
 [28.653]] [[1.268]
 [1.19 ]
 [1.33 ]
 [1.333]
 [1.329]
 [1.325]
 [1.292]]
printing an ep nov before normalisation:  56.15657549431036
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.22885553225120986, 0.2999251872352347, 0.1628622811946154, 0.04407442929274532, 0.1628622811946154, 0.10142028883157933]
actions average: 
K:  2  action  0 :  tensor([0.3031, 0.0325, 0.0734, 0.1158, 0.2851, 0.0847, 0.1055],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.1059, 0.6526, 0.0357, 0.0564, 0.0451, 0.0406, 0.0638],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1249, 0.0248, 0.4614, 0.0629, 0.1227, 0.1068, 0.0965],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1591, 0.1747, 0.1197, 0.1490, 0.1436, 0.1077, 0.1462],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2111, 0.0664, 0.0780, 0.0908, 0.3464, 0.1082, 0.0991],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1525, 0.0091, 0.1190, 0.1344, 0.1720, 0.3088, 0.1042],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1471, 0.0614, 0.1244, 0.1228, 0.1537, 0.1119, 0.2787],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  30.142158681636744
siam score:  -0.5521887
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.17436992620167474, 0.32112545456934727, 0.17436992620167474, 0.04718180161635878, 0.17436992620167474, 0.10858296520926979]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.2121986742496467, 0.2121986742496467, 0.2121986742496467, 0.032140280626043766, 0.2121986742496467, 0.1190650223753695]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.2121986742496467, 0.2121986742496467, 0.2121986742496467, 0.032140280626043766, 0.2121986742496467, 0.1190650223753695]
printing an ep nov before normalisation:  31.996666599153457
Printing some Q and Qe and total Qs values:  [[1.22 ]
 [1.217]
 [1.217]
 [1.217]
 [1.217]
 [1.217]
 [1.217]] [[94.296]
 [95.667]
 [95.667]
 [95.667]
 [95.667]
 [95.667]
 [95.667]] [[2.715]
 [2.737]
 [2.737]
 [2.737]
 [2.737]
 [2.737]
 [2.737]]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.2121986742496467, 0.2121986742496467, 0.2121986742496467, 0.032140280626043766, 0.2121986742496467, 0.1190650223753695]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.2121986742496467, 0.2121986742496467, 0.2121986742496467, 0.032140280626043766, 0.2121986742496467, 0.1190650223753695]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.2121986742496467, 0.2121986742496467, 0.2121986742496467, 0.032140280626043766, 0.2121986742496467, 0.1190650223753695]
printing an ep nov before normalisation:  17.705197958822744
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.2121986742496467, 0.2121986742496467, 0.2121986742496467, 0.032140280626043766, 0.2121986742496467, 0.1190650223753695]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.23399665368362, 0.23399665368362, 0.23399665368362, 0.03543025129463883, 0.13128989382725065, 0.13128989382725065]
printing an ep nov before normalisation:  22.548489829622856
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.23399665368362, 0.23399665368362, 0.23399665368362, 0.03543025129463883, 0.13128989382725065, 0.13128989382725065]
printing an ep nov before normalisation:  26.96441637464197
printing an ep nov before normalisation:  6.790788751676047
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.23399665368362, 0.23399665368362, 0.23399665368362, 0.03543025129463883, 0.13128989382725065, 0.13128989382725065]
printing an ep nov before normalisation:  43.35656908393899
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.23399665368362, 0.23399665368362, 0.23399665368362, 0.03543025129463883, 0.13128989382725065, 0.13128989382725065]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.2919974450367594, 0.1638182398855281, 0.2919974450367594, 0.044184315077712526, 0.044184315077712526, 0.1638182398855281]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.291997449177143, 0.1638182397914287, 0.291997449177143, 0.0441843110314283, 0.0441843110314283, 0.1638182397914287]
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.3349480861942241, 0.18790606913131006, 0.18790606913131006, 0.05066685320592291, 0.05066685320592291, 0.18790606913131006]
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.290182328977835
printing an ep nov before normalisation:  20.95719541592055
printing an ep nov before normalisation:  40.817509940947005
actions average: 
K:  2  action  0 :  tensor([0.2727, 0.1126, 0.0706, 0.0897, 0.2415, 0.1229, 0.0899],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0266, 0.7948, 0.0125, 0.0286, 0.0306, 0.0304, 0.0764],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1248, 0.0460, 0.3037, 0.1158, 0.1494, 0.1249, 0.1354],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1402, 0.1357, 0.1064, 0.1230, 0.1843, 0.1405, 0.1700],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1534, 0.0171, 0.1170, 0.1041, 0.2873, 0.1651, 0.1559],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1353, 0.0864, 0.0941, 0.1067, 0.1311, 0.3130, 0.1334],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1249, 0.1005, 0.1115, 0.1219, 0.1557, 0.1309, 0.2546],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.2738173295543251, 0.19075249225047217, 0.19075249225047217, 0.11322531076687598, 0.04069988292738252, 0.19075249225047217]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.2738173295543251, 0.19075249225047217, 0.19075249225047217, 0.11322531076687598, 0.04069988292738252, 0.19075249225047217]
printing an ep nov before normalisation:  65.51516625914704
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.2738173295543251, 0.19075249225047217, 0.19075249225047217, 0.11322531076687598, 0.04069988292738252, 0.19075249225047217]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.2738173295543251, 0.19075249225047217, 0.19075249225047217, 0.11322531076687598, 0.04069988292738252, 0.19075249225047217]
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.21215693882648565, 0.21215693882648565, 0.21215693882648565, 0.11917682210421854, 0.03219542258983884, 0.21215693882648565]
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.556]
 [0.56 ]
 [0.549]
 [0.549]
 [0.561]
 [0.567]] [[49.896]
 [49.897]
 [52.578]
 [51.891]
 [51.819]
 [50.942]
 [50.453]] [[1.704]
 [1.718]
 [1.844]
 [1.802]
 [1.799]
 [1.771]
 [1.755]]
printing an ep nov before normalisation:  60.955784910161945
printing an ep nov before normalisation:  53.63040416061209
printing an ep nov before normalisation:  57.03341007232666
Printing some Q and Qe and total Qs values:  [[0.801]
 [0.801]
 [0.801]
 [0.801]
 [0.801]
 [0.801]
 [0.801]] [[36.508]
 [36.508]
 [36.508]
 [36.508]
 [36.508]
 [36.508]
 [36.508]] [[0.801]
 [0.801]
 [0.801]
 [0.801]
 [0.801]
 [0.801]
 [0.801]]
printing an ep nov before normalisation:  55.768079324187056
actions average: 
K:  4  action  0 :  tensor([0.3030, 0.0187, 0.1513, 0.1394, 0.1145, 0.1478, 0.1252],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0497, 0.6115, 0.0376, 0.0859, 0.1062, 0.0547, 0.0544],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1801, 0.0279, 0.1477, 0.1604, 0.1901, 0.1719, 0.1219],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1546, 0.0125, 0.1158, 0.2235, 0.1614, 0.1749, 0.1572],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2039, 0.0088, 0.1176, 0.1683, 0.1882, 0.1998, 0.1135],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1571, 0.0759, 0.1235, 0.1656, 0.1978, 0.1544, 0.1257],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1102, 0.0431, 0.1697, 0.1503, 0.1748, 0.1097, 0.2422],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.21215693882648565, 0.21215693882648565, 0.21215693882648565, 0.11917682210421854, 0.03219542258983884, 0.21215693882648565]
printing an ep nov before normalisation:  41.10532426715211
printing an ep nov before normalisation:  44.32483207242761
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.23391101413301882, 0.1313909434056295, 0.23391101413301882, 0.1313909434056295, 0.03548507078968447, 0.23391101413301882]
printing an ep nov before normalisation:  34.74471060957512
printing an ep nov before normalisation:  38.28727734817743
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.2606395015473472, 0.14639801600612773, 0.14639801600612773, 0.14639801600612773, 0.039526948886922444, 0.2606395015473472]
printing an ep nov before normalisation:  38.01988291207005
UNIT TEST: sample policy line 217 mcts : [0.041 0.49  0.    0.    0.143 0.204 0.122]
printing an ep nov before normalisation:  47.2865758127943
from probs:  [0.3879333501860078, 0.2178688744232089, 0.058776300322524795, 0.058776300322524795, 0.058776300322524795, 0.2178688744232089]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.225]
 [1.179]
 [1.23 ]
 [1.234]
 [1.229]
 [1.227]
 [1.228]] [[40.354]
 [33.322]
 [30.732]
 [29.853]
 [30.276]
 [31.114]
 [31.461]] [[1.671]
 [1.502]
 [1.507]
 [1.496]
 [1.499]
 [1.511]
 [1.518]]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.23382892826104512, 0.23382892826104512, 0.13148643440294427, 0.13148643440294427, 0.03554034641097599, 0.23382892826104512]
Printing some Q and Qe and total Qs values:  [[1.222]
 [1.263]
 [1.263]
 [1.286]
 [1.263]
 [1.263]
 [1.263]] [[31.556]
 [39.385]
 [39.385]
 [27.173]
 [39.385]
 [39.385]
 [39.385]] [[2.616]
 [3.263]
 [3.263]
 [2.341]
 [3.263]
 [3.263]
 [3.263]]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.23382892826104512, 0.23382892826104512, 0.13148643440294427, 0.13148643440294427, 0.03554034641097599, 0.23382892826104512]
printing an ep nov before normalisation:  55.27674703040913
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.23382892826104512, 0.23382892826104512, 0.13148643440294427, 0.13148643440294427, 0.03554034641097599, 0.23382892826104512]
printing an ep nov before normalisation:  48.73432895991159
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.23382892826104512, 0.23382892826104512, 0.13148643440294427, 0.13148643440294427, 0.03554034641097599, 0.23382892826104512]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.23382892826104512, 0.23382892826104512, 0.13148643440294427, 0.13148643440294427, 0.03554034641097599, 0.23382892826104512]
printing an ep nov before normalisation:  24.49272632598877
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.23382892826104512, 0.23382892826104512, 0.13148643440294427, 0.13148643440294427, 0.03554034641097599, 0.23382892826104512]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.26049646873318105, 0.26049646873318105, 0.14647544343716332, 0.14647544343716332, 0.03958073222214804, 0.14647544343716332]
Printing some Q and Qe and total Qs values:  [[0.234]
 [0.273]
 [0.273]
 [0.272]
 [0.27 ]
 [0.271]
 [0.271]] [[23.176]
 [16.656]
 [16.37 ]
 [16.591]
 [16.861]
 [16.995]
 [17.131]] [[0.871]
 [0.631]
 [0.619]
 [0.628]
 [0.638]
 [0.644]
 [0.65 ]]
siam score:  -0.5927893
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.26049646873318105, 0.26049646873318105, 0.14647544343716332, 0.14647544343716332, 0.03958073222214804, 0.14647544343716332]
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.2604964716735254, 0.2604964716735254, 0.14647544280443112, 0.14647544280443112, 0.03958072823965594, 0.14647544280443112]
actions average: 
K:  0  action  0 :  tensor([0.3920, 0.0130, 0.0853, 0.1029, 0.1986, 0.1200, 0.0883],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0364, 0.6963, 0.0309, 0.0909, 0.0516, 0.0416, 0.0523],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1554, 0.0069, 0.2099, 0.1598, 0.1786, 0.1674, 0.1221],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1468, 0.0621, 0.0955, 0.2320, 0.1965, 0.1407, 0.1264],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2201, 0.0135, 0.1310, 0.1792, 0.1752, 0.1299, 0.1511],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1193, 0.0844, 0.0848, 0.0959, 0.1286, 0.4169, 0.0702],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1938, 0.0766, 0.1024, 0.1546, 0.1590, 0.1413, 0.1723],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.2604964716735254, 0.2604964716735254, 0.14647544280443112, 0.14647544280443112, 0.03958072823965594, 0.14647544280443112]
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.2604964716735254, 0.2604964716735254, 0.14647544280443112, 0.14647544280443112, 0.03958072823965594, 0.14647544280443112]
Printing some Q and Qe and total Qs values:  [[0.733]
 [0.733]
 [0.733]
 [0.733]
 [0.733]
 [0.733]
 [0.733]] [[67.426]
 [67.426]
 [67.426]
 [67.426]
 [67.426]
 [67.426]
 [67.426]] [[2.4]
 [2.4]
 [2.4]
 [2.4]
 [2.4]
 [2.4]
 [2.4]]
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.2604964716735254, 0.2604964716735254, 0.14647544280443112, 0.14647544280443112, 0.03958072823965594, 0.14647544280443112]
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
printing an ep nov before normalisation:  26.06650023345129
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.25251557833021115, 0.25251557833021115, 0.10478902386123566, 0.17626961473332037, 0.03764059001170146, 0.17626961473332037]
printing an ep nov before normalisation:  123.349482423782
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.25251557833021115, 0.25251557833021115, 0.10478902386123566, 0.17626961473332037, 0.03764059001170146, 0.17626961473332037]
printing an ep nov before normalisation:  42.28687650923942
printing an ep nov before normalisation:  20.637570665975193
printing an ep nov before normalisation:  23.36682259646622
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.25251557833021115, 0.25251557833021115, 0.10478902386123566, 0.17626961473332037, 0.03764059001170146, 0.17626961473332037]
printing an ep nov before normalisation:  16.269484758377075
printing an ep nov before normalisation:  55.934963297157516
Printing some Q and Qe and total Qs values:  [[0.817]
 [1.042]
 [1.042]
 [1.042]
 [1.042]
 [1.042]
 [1.042]] [[59.939]
 [53.713]
 [53.713]
 [53.713]
 [53.713]
 [53.713]
 [53.713]] [[1.8  ]
 [1.904]
 [1.904]
 [1.904]
 [1.904]
 [1.904]
 [1.904]]
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.2525155800676915, 0.2525155800676915, 0.10478902260890528, 0.17626961492767296, 0.0376405874003658, 0.17626961492767296]
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.2525155800676915, 0.2525155800676915, 0.10478902260890528, 0.17626961492767296, 0.0376405874003658, 0.17626961492767296]
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.2525155800676915, 0.2525155800676915, 0.10478902260890528, 0.17626961492767296, 0.0376405874003658, 0.17626961492767296]
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.2525155800676915, 0.2525155800676915, 0.10478902260890528, 0.17626961492767296, 0.0376405874003658, 0.17626961492767296]
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
Printing some Q and Qe and total Qs values:  [[0.18 ]
 [0.203]
 [0.203]
 [0.208]
 [0.208]
 [0.21 ]
 [0.203]] [[29.692]
 [24.819]
 [24.819]
 [29.029]
 [28.968]
 [28.393]
 [24.819]] [[1.118]
 [0.881]
 [0.881]
 [1.11 ]
 [1.107]
 [1.078]
 [0.881]]
printing an ep nov before normalisation:  39.68601589658088
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.27196157582994146, 0.27196157582994146, 0.1128527249081834, 0.1128527249081834, 0.04053051994374813, 0.18984087858000223]
siam score:  -0.57914555
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.27196157582994146, 0.27196157582994146, 0.1128527249081834, 0.1128527249081834, 0.04053051994374813, 0.18984087858000223]
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.27196157582994146, 0.27196157582994146, 0.1128527249081834, 0.1128527249081834, 0.04053051994374813, 0.18984087858000223]
printing an ep nov before normalisation:  36.56925507648099
siam score:  -0.57893693
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.29465435328962186, 0.29465435328962186, 0.12226277538931483, 0.12226277538931483, 0.043902967252811786, 0.12226277538931483]
siam score:  -0.5772782
printing an ep nov before normalisation:  32.707097356394236
Starting evaluation
using explorer policy with actor:  0
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.25496257215232
printing an ep nov before normalisation:  24.997014935421497
printing an ep nov before normalisation:  30.33216972968623
printing an ep nov before normalisation:  40.02491181539107
printing an ep nov before normalisation:  57.14873764420702
printing an ep nov before normalisation:  42.85156964086818
printing an ep nov before normalisation:  67.77229505921389
Printing some Q and Qe and total Qs values:  [[0.79 ]
 [0.774]
 [0.77 ]
 [0.774]
 [0.786]
 [0.775]
 [0.774]] [[47.049]
 [34.026]
 [43.162]
 [34.026]
 [38.819]
 [39.49 ]
 [34.026]] [[0.79 ]
 [0.774]
 [0.77 ]
 [0.774]
 [0.786]
 [0.775]
 [0.774]]
Printing some Q and Qe and total Qs values:  [[0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]] [[28.748]
 [28.748]
 [28.748]
 [28.748]
 [28.748]
 [28.748]
 [28.748]] [[0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.3906891096100364, 0.27270023193013687, 0.0581749997848653, 0.0581749997848653, 0.0581749997848653, 0.1620856591052309]
printing an ep nov before normalisation:  61.53128836556827
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]] [[37.215]
 [37.215]
 [37.215]
 [37.215]
 [37.215]
 [37.215]
 [37.215]] [[0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]]
actions average: 
K:  1  action  0 :  tensor([0.3614, 0.0497, 0.0998, 0.1029, 0.1825, 0.1002, 0.1035],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0297, 0.8186, 0.0194, 0.0493, 0.0250, 0.0290, 0.0289],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1470, 0.0327, 0.3183, 0.1297, 0.1059, 0.1232, 0.1432],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1458, 0.0192, 0.1196, 0.2118, 0.1758, 0.1867, 0.1411],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1799, 0.0703, 0.1159, 0.1280, 0.2465, 0.1293, 0.1301],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1205, 0.0761, 0.1569, 0.1328, 0.0883, 0.2940, 0.1314],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2024, 0.0403, 0.1312, 0.1338, 0.1486, 0.1311, 0.2126],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  0
printing an ep nov before normalisation:  79.35228743884788
printing an ep nov before normalisation:  21.74251079559326
using explorer policy with actor:  0
printing an ep nov before normalisation:  22.38511562347412
using explorer policy with actor:  0
printing an ep nov before normalisation:  30.445233825938384
printing an ep nov before normalisation:  34.698824882507324
printing an ep nov before normalisation:  32.57158417121487
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
probs:  [0.33120888415944577, 0.33120888415944577, 0.05039016630510265, 0.05039016630510265, 0.05039016630510265, 0.1864117327658004]
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
line 256 mcts: sample exp_bonus 65.00964432320501
printing an ep nov before normalisation:  27.03785474798994
printing an ep nov before normalisation:  18.555896715859312
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
probs:  [0.3312088902581355, 0.3312088902581355, 0.05039016199536195, 0.05039016199536195, 0.05039016199536195, 0.18641173349764337]
using explorer policy with actor:  1
printing an ep nov before normalisation:  62.6499306233958
printing an ep nov before normalisation:  35.04083648280559
Printing some Q and Qe and total Qs values:  [[0.721]
 [0.723]
 [0.715]
 [0.724]
 [0.723]
 [0.727]
 [0.723]] [[61.983]
 [62.628]
 [65.627]
 [66.514]
 [62.628]
 [65.469]
 [62.628]] [[0.721]
 [0.723]
 [0.715]
 [0.724]
 [0.723]
 [0.727]
 [0.723]]
printing an ep nov before normalisation:  19.118046864509093
line 256 mcts: sample exp_bonus 36.37389755531715
siam score:  -0.6090147
printing an ep nov before normalisation:  73.41211467996786
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]] [[29.301]
 [23.475]
 [23.475]
 [23.475]
 [23.475]
 [23.475]
 [23.475]] [[1.401]
 [1.087]
 [1.087]
 [1.087]
 [1.087]
 [1.087]
 [1.087]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  36.29737799042389
Printing some Q and Qe and total Qs values:  [[-0.013]
 [-0.015]
 [-0.017]
 [-0.012]
 [-0.012]
 [-0.01 ]
 [-0.009]] [[56.617]
 [54.165]
 [66.852]
 [58.409]
 [63.009]
 [51.951]
 [47.771]] [[0.205]
 [0.183]
 [0.282]
 [0.22 ]
 [0.257]
 [0.171]
 [0.139]]
printing an ep nov before normalisation:  46.84175913214027
Printing some Q and Qe and total Qs values:  [[0.541]
 [0.545]
 [0.547]
 [0.547]
 [0.546]
 [0.546]
 [0.541]] [[64.246]
 [62.324]
 [62.306]
 [62.618]
 [62.918]
 [64.216]
 [63.631]] [[0.541]
 [0.545]
 [0.547]
 [0.547]
 [0.546]
 [0.546]
 [0.541]]
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
Printing some Q and Qe and total Qs values:  [[0.783]
 [0.783]
 [0.783]
 [0.783]
 [0.783]
 [0.783]
 [0.783]] [[48.098]
 [46.842]
 [46.842]
 [46.842]
 [46.842]
 [46.842]
 [46.842]] [[0.783]
 [0.783]
 [0.783]
 [0.783]
 [0.783]
 [0.783]
 [0.783]]
printing an ep nov before normalisation:  49.40039757798891
printing an ep nov before normalisation:  49.33033886699192
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
probs:  [0.38731682487847907, 0.21798065694848368, 0.05890728707485118, 0.05890728707485118, 0.05890728707485118, 0.21798065694848368]
printing an ep nov before normalisation:  25.432444928295777
printing an ep nov before normalisation:  12.798892666247948
printing an ep nov before normalisation:  49.98616204845035
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
probs:  [0.3873168341367783, 0.21798065910157594, 0.05890728255335663, 0.05890728255335663, 0.05890728255335663, 0.21798065910157594]
printing an ep nov before normalisation:  13.022743173711573
printing an ep nov before normalisation:  75.20265228380488
Printing some Q and Qe and total Qs values:  [[0.074]
 [0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]] [[23.492]
 [18.318]
 [18.318]
 [18.318]
 [18.318]
 [18.318]
 [18.318]] [[0.846]
 [0.569]
 [0.569]
 [0.569]
 [0.569]
 [0.569]
 [0.569]]
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.409]
 [0.457]
 [0.453]
 [0.461]
 [0.451]
 [0.451]] [[33.508]
 [27.711]
 [27.485]
 [25.456]
 [24.805]
 [21.795]
 [20.984]] [[0.449]
 [0.409]
 [0.457]
 [0.453]
 [0.461]
 [0.451]
 [0.451]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  16.76048334199546
printing an ep nov before normalisation:  22.621590214801138
printing an ep nov before normalisation:  84.03800938769999
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
probs:  [0.5681853527707943, 0.08636292944584116, 0.08636292944584116, 0.08636292944584116, 0.08636292944584116, 0.08636292944584116]
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
probs:  [0.5681853527707943, 0.08636292944584116, 0.08636292944584116, 0.08636292944584116, 0.08636292944584116, 0.08636292944584116]
Printing some Q and Qe and total Qs values:  [[1.038]
 [1.038]
 [1.02 ]
 [1.038]
 [1.038]
 [1.038]
 [1.038]] [[25.135]
 [25.135]
 [57.2  ]
 [25.135]
 [25.135]
 [25.135]
 [25.135]] [[1.459]
 [1.459]
 [2.17 ]
 [1.459]
 [1.459]
 [1.459]
 [1.459]]
line 256 mcts: sample exp_bonus 62.98769359773968
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.564]
 [0.561]
 [0.566]
 [0.564]
 [0.564]
 [0.564]] [[75.755]
 [75.761]
 [77.258]
 [75.405]
 [75.761]
 [75.761]
 [75.761]] [[0.56 ]
 [0.564]
 [0.561]
 [0.566]
 [0.564]
 [0.564]
 [0.564]]
printing an ep nov before normalisation:  57.91708349526481
printing an ep nov before normalisation:  63.08501228013785
printing an ep nov before normalisation:  43.91944408416748
Printing some Q and Qe and total Qs values:  [[0.57 ]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]] [[24.337]
 [20.064]
 [20.064]
 [20.064]
 [20.064]
 [20.064]
 [20.064]] [[0.57 ]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]]
Printing some Q and Qe and total Qs values:  [[0.786]
 [0.747]
 [0.747]
 [0.747]
 [0.747]
 [0.747]
 [0.747]] [[77.899]
 [70.326]
 [70.326]
 [70.326]
 [70.326]
 [70.326]
 [70.326]] [[2.651]
 [2.341]
 [2.341]
 [2.341]
 [2.341]
 [2.341]
 [2.341]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
printing an ep nov before normalisation:  40.18199913244274
from probs:  [0.39295593478791147, 0.16328921490366308, 0.16328921490366308, 0.058588210250549576, 0.058588210250549576, 0.16328921490366308]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.33389350335299006, 0.18810600470337496, 0.18810600470337496, 0.05089424126844243, 0.05089424126844243, 0.18810600470337496]
printing an ep nov before normalisation:  12.367334365844727
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.33389350335299006, 0.18810600470337496, 0.18810600470337496, 0.05089424126844243, 0.05089424126844243, 0.18810600470337496]
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.465]
 [0.474]
 [0.473]
 [0.468]
 [0.473]
 [0.475]] [[24.478]
 [26.954]
 [29.997]
 [27.227]
 [26.147]
 [24.272]
 [23.549]] [[0.477]
 [0.465]
 [0.474]
 [0.473]
 [0.468]
 [0.473]
 [0.475]]
Printing some Q and Qe and total Qs values:  [[0.692]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]] [[29.033]
 [30.127]
 [30.127]
 [30.127]
 [30.127]
 [30.127]
 [30.127]] [[0.692]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]]
actions average: 
K:  2  action  0 :  tensor([0.2409, 0.0982, 0.1286, 0.1381, 0.1402, 0.1270, 0.1270],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0929, 0.4436, 0.0776, 0.1013, 0.0927, 0.0844, 0.1074],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1762, 0.0172, 0.1604, 0.1514, 0.1831, 0.1680, 0.1437],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1751, 0.0591, 0.1475, 0.1675, 0.1608, 0.1439, 0.1460],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1647, 0.0553, 0.1390, 0.1646, 0.2032, 0.1508, 0.1222],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1230, 0.1259, 0.1727, 0.1367, 0.1366, 0.1989, 0.1061],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1568, 0.0807, 0.1538, 0.1551, 0.1321, 0.1519, 0.1696],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.686]
 [0.688]
 [0.659]
 [0.688]
 [0.688]
 [0.688]
 [0.688]] [[35.578]
 [19.166]
 [29.252]
 [19.166]
 [19.166]
 [19.166]
 [19.166]] [[0.686]
 [0.688]
 [0.659]
 [0.688]
 [0.688]
 [0.688]
 [0.688]]
printing an ep nov before normalisation:  14.22917596127543
printing an ep nov before normalisation:  53.79108701433454
printing an ep nov before normalisation:  65.31560761587961
siam score:  -0.7014011
printing an ep nov before normalisation:  48.70530128479004
printing an ep nov before normalisation:  52.39805072795633
line 256 mcts: sample exp_bonus 34.23768917631229
actions average: 
K:  3  action  0 :  tensor([0.2552, 0.0848, 0.1041, 0.1219, 0.1410, 0.1110, 0.1820],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0845, 0.4656, 0.0760, 0.1057, 0.0878, 0.0931, 0.0872],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1643, 0.0550, 0.1385, 0.1853, 0.1664, 0.1590, 0.1315],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2041, 0.0653, 0.1242, 0.1708, 0.1584, 0.1535, 0.1237],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1913, 0.1006, 0.1266, 0.1639, 0.1476, 0.1379, 0.1321],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1387, 0.0592, 0.1928, 0.1166, 0.1109, 0.2784, 0.1034],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1786, 0.1040, 0.1068, 0.1426, 0.1346, 0.1222, 0.2112],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  59.92533833018074
printing an ep nov before normalisation:  1.4178481928902897
siam score:  -0.7158201
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.3338935244428971, 0.18810600740720904, 0.18810600740720904, 0.05089422666773794, 0.05089422666773794, 0.18810600740720904]
printing an ep nov before normalisation:  70.12360257465811
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.3338935244428971, 0.18810600740720904, 0.18810600740720904, 0.05089422666773794, 0.05089422666773794, 0.18810600740720904]
siam score:  -0.7243616
printing an ep nov before normalisation:  47.822358885149676
siam score:  -0.7386922
using explorer policy with actor:  0
printing an ep nov before normalisation:  26.68173206133573
maxi score, test score, baseline:  -0.9870794871794872 -1.0 -0.9870794871794872
printing an ep nov before normalisation:  15.105011463165283
maxi score, test score, baseline:  -0.9870794871794872 -1.0 -0.9870794871794872
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
probs:  [0.3926002035080233, 0.16339226758200948, 0.16339226758200948, 0.058611496872973984, 0.16339226758200948, 0.058611496872973984]
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
probs:  [0.4385774830137545, 0.18251796986178856, 0.18251796986178856, 0.06546219242088946, 0.06546219242088946, 0.06546219242088946]
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.467]
 [0.467]
 [0.587]
 [0.467]
 [0.467]
 [0.467]] [[38.328]
 [31.91 ]
 [31.91 ]
 [37.012]
 [31.91 ]
 [31.91 ]
 [31.91 ]] [[0.47 ]
 [0.467]
 [0.467]
 [0.587]
 [0.467]
 [0.467]
 [0.467]]
maxi score, test score, baseline:  -0.9875543209876543 -1.0 -0.9875543209876543
probs:  [0.43857748884124215, 0.1825179702015076, 0.1825179702015076, 0.06546219025191423, 0.06546219025191423, 0.06546219025191423]
printing an ep nov before normalisation:  76.52995109558105
from probs:  [0.43857748884124215, 0.1825179702015076, 0.1825179702015076, 0.06546219025191423, 0.06546219025191423, 0.06546219025191423]
Printing some Q and Qe and total Qs values:  [[0.253]
 [0.253]
 [0.257]
 [0.258]
 [0.253]
 [0.253]
 [0.253]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.253]
 [0.253]
 [0.257]
 [0.258]
 [0.253]
 [0.253]
 [0.253]]
maxi score, test score, baseline:  -0.9877048780487805 -1.0 -0.9877048780487805
probs:  [0.36707410137765345, 0.054238709196532855, 0.2014553643405894, 0.1257439416950748, 0.1257439416950748, 0.1257439416950748]
Printing some Q and Qe and total Qs values:  [[0.831]
 [0.849]
 [0.849]
 [0.849]
 [0.849]
 [0.849]
 [0.849]] [[92.768]
 [92.37 ]
 [92.37 ]
 [92.37 ]
 [92.37 ]
 [92.37 ]
 [92.37 ]] [[2.654]
 [2.661]
 [2.661]
 [2.661]
 [2.661]
 [2.661]
 [2.661]]
maxi score, test score, baseline:  -0.9879952380952381 -1.0 -0.9879952380952381
probs:  [0.36707410659869655, 0.054238706267544075, 0.2014553652469097, 0.12574394062894992, 0.12574394062894992, 0.12574394062894992]
using explorer policy with actor:  1
printing an ep nov before normalisation:  75.1855240567366
printing an ep nov before normalisation:  37.564264234854384
printing an ep nov before normalisation:  77.12861406615569
maxi score, test score, baseline:  -0.9881352941176471 -1.0 -0.9881352941176471
probs:  [0.3225926081032301, 0.048242873786555805, 0.22576329010910995, 0.13446707600036803, 0.13446707600036803, 0.13446707600036803]
using explorer policy with actor:  1
printing an ep nov before normalisation:  57.096859485333425
printing an ep nov before normalisation:  37.121148109436035
maxi score, test score, baseline:  -0.9881352941176471 -1.0 -0.9881352941176471
probs:  [0.3225926081032301, 0.048242873786555805, 0.22576329010910995, 0.13446707600036803, 0.13446707600036803, 0.13446707600036803]
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.548565298294534
printing an ep nov before normalisation:  13.671655211433608
printing an ep nov before normalisation:  28.961455006733203
printing an ep nov before normalisation:  15.249288082122803
printing an ep nov before normalisation:  26.945338147336226
maxi score, test score, baseline:  -0.9881352941176471 -1.0 -0.9881352941176471
probs:  [0.2599795919459964, 0.03980302668016243, 0.2599795919459964, 0.14674592980928164, 0.14674592980928164, 0.14674592980928164]
maxi score, test score, baseline:  -0.9881352941176471 -1.0 -0.9881352941176471
probs:  [0.2599795919459964, 0.03980302668016243, 0.2599795919459964, 0.14674592980928164, 0.14674592980928164, 0.14674592980928164]
printing an ep nov before normalisation:  60.9231102962971
maxi score, test score, baseline:  -0.9881352941176471 -1.0 -0.9881352941176471
probs:  [0.2599795919459964, 0.03980302668016243, 0.2599795919459964, 0.14674592980928164, 0.14674592980928164, 0.14674592980928164]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.17725333388648928, 0.03791913305785635, 0.25306753139618643, 0.17725333388648928, 0.17725333388648928, 0.17725333388648928]
printing an ep nov before normalisation:  75.59460231237492
printing an ep nov before normalisation:  66.71348286471144
printing an ep nov before normalisation:  63.62409607703822
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
printing an ep nov before normalisation:  33.15056127728382
printing an ep nov before normalisation:  41.70588954950837
Printing some Q and Qe and total Qs values:  [[0.642]
 [0.69 ]
 [0.69 ]
 [0.681]
 [0.656]
 [0.654]
 [0.731]] [[106.443]
 [ 85.962]
 [ 85.962]
 [111.904]
 [110.674]
 [109.671]
 [103.107]] [[1.781]
 [1.39 ]
 [1.39 ]
 [1.938]
 [1.887]
 [1.863]
 [1.799]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.16760852681006
STARTED EXPV TRAINING ON FRAME NO.  11563
deleting a thread, now have 5 threads
Frames:  11573 train batches done:  1340 episodes:  254
printing an ep nov before normalisation:  83.72837349356388
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.14725067648454293, 0.05281595584396923, 0.35278506846696883, 0.24708166687600683, 0.14725067648454293, 0.05281595584396923]
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.14725067648454293, 0.05281595584396923, 0.35278506846696883, 0.24708166687600683, 0.14725067648454293, 0.05281595584396923]
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.14725067648454293, 0.05281595584396923, 0.35278506846696883, 0.24708166687600683, 0.14725067648454293, 0.05281595584396923]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.14725067648454293, 0.05281595584396923, 0.35278506846696883, 0.24708166687600683, 0.14725067648454293, 0.05281595584396923]
printing an ep nov before normalisation:  28.667038050205154
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.14725067648454293, 0.05281595584396923, 0.35278506846696883, 0.24708166687600683, 0.14725067648454293, 0.05281595584396923]
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.14725067648454293, 0.05281595584396923, 0.35278506846696883, 0.24708166687600683, 0.14725067648454293, 0.05281595584396923]
printing an ep nov before normalisation:  19.667999501451508
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]] [[15.753]
 [15.753]
 [15.753]
 [15.753]
 [15.753]
 [15.753]
 [15.753]] [[0.814]
 [0.814]
 [0.814]
 [0.814]
 [0.814]
 [0.814]
 [0.814]]
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.14725067648454293, 0.05281595584396923, 0.35278506846696883, 0.24708166687600683, 0.14725067648454293, 0.05281595584396923]
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.346]
 [0.367]
 [0.382]
 [0.387]
 [0.392]
 [0.387]] [[16.049]
 [16.757]
 [16.393]
 [21.507]
 [17.152]
 [16.951]
 [18.926]] [[0.744]
 [0.735]
 [0.739]
 [0.993]
 [0.795]
 [0.79 ]
 [0.878]]
printing an ep nov before normalisation:  22.41194565352153
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.14725067648454293, 0.05281595584396923, 0.35278506846696883, 0.24708166687600683, 0.14725067648454293, 0.05281595584396923]
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.16373492977917328, 0.044064619065634525, 0.29748410057665775, 0.2286988127379513, 0.16373492977917328, 0.10228260806140992]
printing an ep nov before normalisation:  37.478346234866166
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.16373492977917328, 0.044064619065634525, 0.29748410057665775, 0.2286988127379513, 0.16373492977917328, 0.10228260806140992]
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.16373492977917328, 0.044064619065634525, 0.29748410057665775, 0.2286988127379513, 0.16373492977917328, 0.10228260806140992]
line 256 mcts: sample exp_bonus 28.79223978737171
Printing some Q and Qe and total Qs values:  [[-0.071]
 [-0.073]
 [-0.082]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.086]] [[46.347]
 [47.389]
 [47.97 ]
 [47.766]
 [47.155]
 [46.752]
 [46.81 ]] [[0.234]
 [0.245]
 [0.243]
 [0.236]
 [0.228]
 [0.223]
 [0.224]]
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
printing an ep nov before normalisation:  56.81622929433143
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.10897663941773782, 0.04694352164835422, 0.316970034291555, 0.2436771237169718, 0.17445604150764324, 0.10897663941773782]
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.10897663941773782, 0.04694352164835422, 0.316970034291555, 0.2436771237169718, 0.17445604150764324, 0.10897663941773782]
printing an ep nov before normalisation:  60.49295212490607
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.10897663941773782, 0.04694352164835422, 0.316970034291555, 0.2436771237169718, 0.17445604150764324, 0.10897663941773782]
printing an ep nov before normalisation:  52.941357282577826
printing an ep nov before normalisation:  67.09866523742676
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.10897663941773782, 0.04694352164835422, 0.316970034291555, 0.2436771237169718, 0.17445604150764324, 0.10897663941773782]
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.371]
 [0.395]] [[60.697]
 [62.114]
 [62.114]
 [62.114]
 [62.114]
 [59.827]
 [62.114]] [[1.461]
 [1.507]
 [1.507]
 [1.507]
 [1.507]
 [1.44 ]
 [1.507]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.10897663941773782, 0.04694352164835422, 0.316970034291555, 0.2436771237169718, 0.17445604150764324, 0.10897663941773782]
Printing some Q and Qe and total Qs values:  [[-0.088]
 [-0.088]
 [-0.088]
 [-0.088]
 [-0.088]
 [-0.088]
 [-0.088]] [[27.661]
 [27.661]
 [27.661]
 [27.661]
 [27.661]
 [27.661]
 [27.661]] [[1.245]
 [1.245]
 [1.245]
 [1.245]
 [1.245]
 [1.245]
 [1.245]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  11563
printing an ep nov before normalisation:  43.85352190689416
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.10897663941773782, 0.04694352164835422, 0.316970034291555, 0.2436771237169718, 0.17445604150764324, 0.10897663941773782]
using explorer policy with actor:  1
deleting a thread, now have 4 threads
Frames:  12161 train batches done:  1393 episodes:  260
printing an ep nov before normalisation:  89.91449268703445
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.14686213398238587, 0.03991765748727176, 0.2597479702827853, 0.14686213398238587, 0.2597479702827853, 0.14686213398238587]
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.581]
 [0.581]
 [0.575]
 [0.572]
 [0.581]
 [0.581]] [[42.607]
 [42.607]
 [42.607]
 [47.082]
 [46.165]
 [42.607]
 [42.607]] [[1.36 ]
 [1.36 ]
 [1.36 ]
 [1.514]
 [1.478]
 [1.36 ]
 [1.36 ]]
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.18824870012159806, 0.05113931111379908, 0.33297527740760763, 0.05113931111379908, 0.18824870012159806, 0.18824870012159806]
actions average: 
K:  3  action  0 :  tensor([0.2663, 0.0207, 0.1417, 0.1552, 0.1532, 0.1275, 0.1354],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0486, 0.6684, 0.0426, 0.0739, 0.0589, 0.0439, 0.0637],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1590, 0.0980, 0.2206, 0.1535, 0.1420, 0.1005, 0.1264],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1841, 0.0405, 0.1503, 0.1883, 0.1594, 0.1390, 0.1383],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2033, 0.0165, 0.1324, 0.1615, 0.2153, 0.1259, 0.1452],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1845, 0.0247, 0.1791, 0.1521, 0.1375, 0.2001, 0.1220],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1719, 0.0995, 0.1376, 0.1576, 0.1633, 0.1250, 0.1450],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.548]
 [0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.534]] [[50.856]
 [47.558]
 [47.558]
 [47.558]
 [47.558]
 [47.558]
 [47.558]] [[1.355]
 [1.252]
 [1.252]
 [1.252]
 [1.252]
 [1.252]
 [1.252]]
siam score:  -0.8451663
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.21186589379439968, 0.11989529250840393, 0.21186589379439968, 0.0326411323139973, 0.21186589379439968, 0.21186589379439968]
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.21186589379439968, 0.11989529250840393, 0.21186589379439968, 0.0326411323139973, 0.21186589379439968, 0.21186589379439968]
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
deleting a thread, now have 3 threads
Frames:  12279 train batches done:  1418 episodes:  264
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.21186589379439968, 0.11989529250840393, 0.21186589379439968, 0.0326411323139973, 0.21186589379439968, 0.21186589379439968]
siam score:  -0.85921997
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.21186589379439968, 0.11989529250840393, 0.21186589379439968, 0.0326411323139973, 0.21186589379439968, 0.21186589379439968]
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.21186589379439968, 0.11989529250840393, 0.21186589379439968, 0.0326411323139973, 0.21186589379439968, 0.21186589379439968]
siam score:  -0.86208093
deleting a thread, now have 2 threads
Frames:  12279 train batches done:  1440 episodes:  264
from probs:  [0.21186589379439968, 0.11989529250840393, 0.21186589379439968, 0.0326411323139973, 0.21186589379439968, 0.21186589379439968]
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.21186589379439968, 0.11989529250840393, 0.21186589379439968, 0.0326411323139973, 0.21186589379439968, 0.21186589379439968]
printing an ep nov before normalisation:  33.17970887701106
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  58.68846757912715
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.346]
 [0.346]
 [0.301]
 [0.346]
 [0.346]
 [0.316]] [[34.211]
 [34.211]
 [34.211]
 [34.653]
 [34.211]
 [34.211]
 [35.389]] [[0.346]
 [0.346]
 [0.346]
 [0.301]
 [0.346]
 [0.346]
 [0.316]]
printing an ep nov before normalisation:  67.21991546947655
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.13203619253933807, 0.13203619253933807, 0.23333032936177495, 0.035936626835999054, 0.23333032936177495, 0.23333032936177495]
actions average: 
K:  1  action  0 :  tensor([0.2271, 0.0253, 0.1502, 0.1802, 0.1596, 0.1186, 0.1390],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0191, 0.8230, 0.0314, 0.0433, 0.0242, 0.0309, 0.0282],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1810, 0.0095, 0.1875, 0.1745, 0.1673, 0.1409, 0.1393],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1850, 0.0204, 0.1355, 0.2692, 0.1391, 0.0943, 0.1565],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1760, 0.0240, 0.1185, 0.1633, 0.2602, 0.1177, 0.1402],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1978, 0.0103, 0.1506, 0.2043, 0.1569, 0.1315, 0.1487],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1986, 0.0205, 0.1436, 0.1656, 0.1543, 0.1231, 0.1943],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.1469163577058497, 0.1469163577058497, 0.1469163577058497, 0.039975660405815135, 0.25963763323831784, 0.25963763323831784]
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.1469163577058497, 0.1469163577058497, 0.1469163577058497, 0.039975660405815135, 0.25963763323831784, 0.25963763323831784]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.16558073097895068, 0.16558073097895068, 0.16558073097895068, 0.045041869642476765, 0.16558073097895068, 0.2926352064417204]
from probs:  [0.16558073095316883, 0.16558073095316883, 0.16558073095316883, 0.04504186675490178, 0.16558073095316883, 0.29263520943242277]
Printing some Q and Qe and total Qs values:  [[0.657]
 [0.569]
 [0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]] [[55.4  ]
 [96.907]
 [55.4  ]
 [55.4  ]
 [55.4  ]
 [55.4  ]
 [55.4  ]] [[1.054]
 [2.232]
 [1.054]
 [1.054]
 [1.054]
 [1.054]
 [1.054]]
maxi score, test score, baseline:  -0.9885363636363637 -1.0 -0.9885363636363637
probs:  [0.1882775812117182, 0.051202637525962824, 0.1882775812117182, 0.051202637525962824, 0.1882775812117182, 0.3327619813129197]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9885363636363637 -1.0 -0.9885363636363637
probs:  [0.1882775812117182, 0.051202637525962824, 0.1882775812117182, 0.051202637525962824, 0.1882775812117182, 0.3327619813129197]
maxi score, test score, baseline:  -0.9885363636363637 -1.0 -0.9885363636363637
probs:  [0.1882775812117182, 0.051202637525962824, 0.1882775812117182, 0.051202637525962824, 0.1882775812117182, 0.3327619813129197]
siam score:  -0.8740856
siam score:  -0.8741249
maxi score, test score, baseline:  -0.9885363636363637 -1.0 -0.9885363636363637
maxi score, test score, baseline:  -0.9885363636363637 -1.0 -0.9885363636363637
probs:  [0.05932249364438839, 0.05932249364438839, 0.21819186971736007, 0.05932249364438839, 0.21819186971736007, 0.3856487796321148]
maxi score, test score, baseline:  -0.9885363636363637 -1.0 -0.9885363636363637
maxi score, test score, baseline:  -0.9885363636363637 -1.0 -0.9885363636363637
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9885363636363637 -1.0 -0.9885363636363637
maxi score, test score, baseline:  -0.9885363636363637 -1.0 -0.9885363636363637
probs:  [0.13210170082783504, 0.13210170082783504, 0.23326745450246428, 0.03599423483693716, 0.23326745450246428, 0.23326745450246428]
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.696]
 [0.579]
 [0.583]
 [0.578]
 [0.578]
 [0.583]] [[55.885]
 [74.121]
 [56.731]
 [58.757]
 [58.008]
 [57.126]
 [58.01 ]] [[1.201]
 [1.725]
 [1.219]
 [1.268]
 [1.246]
 [1.226]
 [1.251]]
maxi score, test score, baseline:  -0.9885363636363637 -1.0 -0.9885363636363637
probs:  [0.13210170082783504, 0.13210170082783504, 0.23326745450246428, 0.03599423483693716, 0.23326745450246428, 0.23326745450246428]
printing an ep nov before normalisation:  51.563084479744354
siam score:  -0.87896925
maxi score, test score, baseline:  -0.9885363636363637 -1.0 -0.9885363636363637
probs:  [0.13210170082783504, 0.13210170082783504, 0.23326745450246428, 0.03599423483693716, 0.23326745450246428, 0.23326745450246428]
maxi score, test score, baseline:  -0.9885363636363637 -1.0 -0.9885363636363637
probs:  [0.13210170082783504, 0.13210170082783504, 0.23326745450246428, 0.03599423483693716, 0.23326745450246428, 0.23326745450246428]
printing an ep nov before normalisation:  50.42672139327099
actions average: 
K:  4  action  0 :  tensor([0.3248, 0.0388, 0.1125, 0.1565, 0.1349, 0.1041, 0.1284],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0844, 0.4769, 0.0747, 0.1412, 0.0702, 0.0854, 0.0671],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2130, 0.0386, 0.1216, 0.1859, 0.1681, 0.1566, 0.1163],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1616, 0.1453, 0.1145, 0.1836, 0.1467, 0.1256, 0.1226],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1994, 0.0580, 0.1217, 0.1862, 0.1652, 0.1446, 0.1250],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1756, 0.0359, 0.1297, 0.2052, 0.1576, 0.1433, 0.1528],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1950, 0.1509, 0.1139, 0.1555, 0.1308, 0.1192, 0.1348],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9885363636363637 -1.0 -0.9885363636363637
maxi score, test score, baseline:  -0.9885363636363637 -1.0 -0.9885363636363637
probs:  [0.13210170082783504, 0.13210170082783504, 0.23326745450246428, 0.03599423483693716, 0.23326745450246428, 0.23326745450246428]
Printing some Q and Qe and total Qs values:  [[ 0.445]
 [ 0.308]
 [ 0.203]
 [ 0.169]
 [ 0.099]
 [ 0.036]
 [-0.009]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.445]
 [ 0.308]
 [ 0.203]
 [ 0.169]
 [ 0.099]
 [ 0.036]
 [-0.009]]
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
probs:  [0.1469682611115266, 0.1469682611115266, 0.25953057856946965, 0.0400340595264809, 0.1469682611115266, 0.25953057856946965]
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.562]
 [0.587]
 [0.57 ]
 [0.576]
 [0.564]
 [0.555]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.566]
 [0.562]
 [0.587]
 [0.57 ]
 [0.576]
 [0.564]
 [0.555]]
Printing some Q and Qe and total Qs values:  [[0.888]
 [0.58 ]
 [0.58 ]
 [0.797]
 [0.58 ]
 [0.58 ]
 [0.58 ]] [[52.945]
 [64.486]
 [64.486]
 [59.087]
 [64.486]
 [64.486]
 [64.486]] [[1.101]
 [0.908]
 [0.908]
 [1.071]
 [0.908]
 [0.908]
 [0.908]]
printing an ep nov before normalisation:  48.19482666974947
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
probs:  [0.19358326549606325, 0.19358326549606325, 0.19358326549606325, 0.032083672519683776, 0.19358326549606325, 0.19358326549606325]
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
probs:  [0.19358326549606325, 0.19358326549606325, 0.19358326549606325, 0.032083672519683776, 0.19358326549606325, 0.19358326549606325]
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
probs:  [0.19358326549606325, 0.19358326549606325, 0.19358326549606325, 0.032083672519683776, 0.19358326549606325, 0.19358326549606325]
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
probs:  [0.19358326549606325, 0.19358326549606325, 0.19358326549606325, 0.032083672519683776, 0.19358326549606325, 0.19358326549606325]
printing an ep nov before normalisation:  41.86327790441112
printing an ep nov before normalisation:  33.07836145412453
actions average: 
K:  0  action  0 :  tensor([0.2550, 0.0125, 0.1487, 0.1565, 0.1644, 0.1230, 0.1398],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0291, 0.8436, 0.0272, 0.0245, 0.0172, 0.0131, 0.0454],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2630, 0.0125, 0.1457, 0.1661, 0.1596, 0.1133, 0.1398],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2273, 0.0107, 0.1669, 0.1726, 0.1628, 0.1333, 0.1265],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2415, 0.0134, 0.1468, 0.1533, 0.1613, 0.1184, 0.1654],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1710, 0.0162, 0.1660, 0.1620, 0.1655, 0.1844, 0.1348],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2388, 0.0359, 0.1445, 0.1563, 0.1577, 0.1304, 0.1366],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
probs:  [0.0905202620944646, 0.0905202620944646, 0.0905202620944646, 0.0905202620944646, 0.0905202620944646, 0.547398689527677]
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
probs:  [0.0905202620944646, 0.0905202620944646, 0.0905202620944646, 0.0905202620944646, 0.0905202620944646, 0.547398689527677]
actions average: 
K:  3  action  0 :  tensor([0.3636, 0.0692, 0.1027, 0.1545, 0.1148, 0.0973, 0.0979],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.1719, 0.5303, 0.0571, 0.0690, 0.0608, 0.0488, 0.0621],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.2267, 0.0452, 0.1308, 0.1734, 0.1739, 0.1229, 0.1271],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1934, 0.0988, 0.1086, 0.1885, 0.1664, 0.1122, 0.1320],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2258, 0.0322, 0.1259, 0.1763, 0.1861, 0.1208, 0.1329],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.2047, 0.0262, 0.1189, 0.1731, 0.1710, 0.1713, 0.1347],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1842, 0.0406, 0.1194, 0.1941, 0.1868, 0.1189, 0.1559],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
siam score:  -0.87157136
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
probs:  [0.0905202620944646, 0.0905202620944646, 0.0905202620944646, 0.0905202620944646, 0.0905202620944646, 0.547398689527677]
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
probs:  [0.17738352107828442, 0.03806441372725273, 0.17738352107828442, 0.17738352107828442, 0.17738352107828442, 0.25240150195960964]
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
probs:  [0.17738352107828442, 0.03806441372725273, 0.17738352107828442, 0.17738352107828442, 0.17738352107828442, 0.25240150195960964]
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
probs:  [0.17738352107828442, 0.03806441372725273, 0.17738352107828442, 0.17738352107828442, 0.17738352107828442, 0.25240150195960964]
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
probs:  [0.17738352107828442, 0.03806441372725273, 0.17738352107828442, 0.17738352107828442, 0.17738352107828442, 0.25240150195960964]
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
probs:  [0.17738352107828442, 0.03806441372725273, 0.17738352107828442, 0.17738352107828442, 0.17738352107828442, 0.25240150195960964]
Printing some Q and Qe and total Qs values:  [[0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.394]
 [0.387]
 [0.387]] [[29.122]
 [29.122]
 [29.122]
 [29.122]
 [33.895]
 [29.122]
 [29.122]] [[1.688]
 [1.688]
 [1.688]
 [1.688]
 [2.049]
 [1.688]
 [1.688]]
actions average: 
K:  4  action  0 :  tensor([0.3490, 0.0434, 0.1078, 0.1363, 0.1298, 0.1075, 0.1261],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0656, 0.6195, 0.0395, 0.0901, 0.0564, 0.0423, 0.0866],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2452, 0.0520, 0.1069, 0.2043, 0.1406, 0.1203, 0.1307],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1866, 0.0698, 0.1130, 0.1968, 0.1641, 0.1275, 0.1423],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2215, 0.0186, 0.1181, 0.1749, 0.1896, 0.1358, 0.1415],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1502, 0.0365, 0.1709, 0.1444, 0.1458, 0.2086, 0.1436],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1793, 0.0696, 0.1179, 0.1670, 0.1546, 0.1468, 0.1647],
       grad_fn=<DivBackward0>)
siam score:  -0.886191
Printing some Q and Qe and total Qs values:  [[0.29 ]
 [0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.277]
 [0.277]] [[50.497]
 [47.956]
 [47.956]
 [47.956]
 [47.956]
 [47.956]
 [47.956]] [[2.29 ]
 [2.148]
 [2.148]
 [2.148]
 [2.148]
 [2.148]
 [2.148]]
printing an ep nov before normalisation:  46.55207668007604
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
probs:  [0.21177067478858283, 0.03280961030485126, 0.21177067478858283, 0.12010769054081746, 0.21177067478858283, 0.21177067478858283]
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
probs:  [0.21177067478858283, 0.03280961030485126, 0.21177067478858283, 0.12010769054081746, 0.21177067478858283, 0.21177067478858283]
Printing some Q and Qe and total Qs values:  [[0.167]
 [0.161]
 [0.161]
 [0.161]
 [0.161]
 [0.161]
 [0.161]] [[27.466]
 [19.343]
 [19.343]
 [19.343]
 [19.343]
 [19.343]
 [19.343]] [[1.507]
 [0.96 ]
 [0.96 ]
 [0.96 ]
 [0.96 ]
 [0.96 ]
 [0.96 ]]
actions average: 
K:  0  action  0 :  tensor([0.3277, 0.0224, 0.1153, 0.1708, 0.1370, 0.1187, 0.1080],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0136, 0.9033, 0.0145, 0.0223, 0.0146, 0.0143, 0.0173],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1631, 0.0165, 0.1423, 0.1906, 0.1729, 0.1712, 0.1434],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1891, 0.0191, 0.1372, 0.2053, 0.1598, 0.1528, 0.1368],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2278, 0.0056, 0.1275, 0.2011, 0.1728, 0.1424, 0.1228],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1553, 0.0052, 0.1561, 0.2147, 0.1544, 0.1809, 0.1333],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1929, 0.0098, 0.1448, 0.1913, 0.1629, 0.1538, 0.1446],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
probs:  [0.21177067478858283, 0.03280961030485126, 0.21177067478858283, 0.12010769054081746, 0.21177067478858283, 0.21177067478858283]
printing an ep nov before normalisation:  35.720410149716685
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
probs:  [0.21177067478858283, 0.03280961030485126, 0.21177067478858283, 0.12010769054081746, 0.21177067478858283, 0.21177067478858283]
Printing some Q and Qe and total Qs values:  [[0.45]
 [0.45]
 [0.45]
 [0.45]
 [0.45]
 [0.45]
 [0.45]] [[13.663]
 [13.663]
 [13.663]
 [13.663]
 [13.663]
 [13.663]
 [13.663]] [[0.58]
 [0.58]
 [0.58]
 [0.58]
 [0.58]
 [0.58]
 [0.58]]
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
probs:  [0.21177067478858283, 0.03280961030485126, 0.21177067478858283, 0.12010769054081746, 0.21177067478858283, 0.21177067478858283]
printing an ep nov before normalisation:  31.901695052547133
using explorer policy with actor:  1
siam score:  -0.8833242
Printing some Q and Qe and total Qs values:  [[0.52 ]
 [0.52 ]
 [0.521]
 [0.52 ]
 [0.522]
 [0.52 ]
 [0.52 ]] [[81.683]
 [81.683]
 [86.24 ]
 [86.797]
 [87.482]
 [81.683]
 [81.683]] [[2.085]
 [2.085]
 [2.249]
 [2.268]
 [2.295]
 [2.085]
 [2.085]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
probs:  [0.18835115727656748, 0.051396479740352115, 0.18835115727656748, 0.051396479740352115, 0.3321535686895934, 0.18835115727656748]
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.48 ]
 [0.45 ]
 [0.459]
 [0.454]
 [0.454]
 [0.454]] [[47.868]
 [48.021]
 [49.13 ]
 [49.848]
 [47.868]
 [47.868]
 [47.868]] [[1.935]
 [1.97 ]
 [2.007]
 [2.06 ]
 [1.935]
 [1.935]
 [1.935]]
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.449]
 [0.441]
 [0.443]
 [0.449]
 [0.45 ]
 [0.409]] [[42.135]
 [40.79 ]
 [50.994]
 [50.474]
 [45.996]
 [45.141]
 [43.652]] [[1.475]
 [1.402]
 [1.891]
 [1.868]
 [1.655]
 [1.615]
 [1.501]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
probs:  [0.18835115727656748, 0.051396479740352115, 0.18835115727656748, 0.051396479740352115, 0.3321535686895934, 0.18835115727656748]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
siam score:  -0.8739136
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
probs:  [0.21824671738237889, 0.05953886902634084, 0.21824671738237889, 0.05953886902634084, 0.3848899581562197, 0.05953886902634084]
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
probs:  [0.2857129925529381, 0.04762034078039523, 0.2857129925529381, 0.04762034078039523, 0.2857129925529381, 0.04762034078039523]
printing an ep nov before normalisation:  29.932762201173357
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
probs:  [0.25922672519565815, 0.14711172472392223, 0.14711172472392223, 0.14711172472392223, 0.25922672519565815, 0.04021137543691702]
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
printing an ep nov before normalisation:  42.17835474044261
printing an ep nov before normalisation:  19.771647674340468
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
probs:  [0.29026693193522496, 0.16472020579629543, 0.045012862268479614, 0.16472020579629543, 0.29026693193522496, 0.045012862268479614]
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
probs:  [0.29026693193522496, 0.16472020579629543, 0.045012862268479614, 0.16472020579629543, 0.29026693193522496, 0.045012862268479614]
printing an ep nov before normalisation:  29.301507472991943
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
probs:  [0.29026693193522496, 0.16472020579629543, 0.045012862268479614, 0.16472020579629543, 0.29026693193522496, 0.045012862268479614]
using explorer policy with actor:  1
main train batch thing paused
add a thread
Adding thread: now have 4 threads
printing an ep nov before normalisation:  33.79390798552008
printing an ep nov before normalisation:  22.633974716663648
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  63.49787700972335
main train batch thing paused
add a thread
Adding thread: now have 5 threads
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  -0.9890304347826087 -1.0 -0.9890304347826087
maxi score, test score, baseline:  -0.9890304347826087 -1.0 -0.9890304347826087
probs:  [0.17684483268044726, 0.038031173688073655, 0.17684483268044726, 0.2512092928549333, 0.2512092928549333, 0.10586057524116535]
maxi score, test score, baseline:  -0.9892617021276596 -1.0 -0.9892617021276596
probs:  [0.19156617906597873, 0.037040548384875374, 0.1377163380710488, 0.24798029820352419, 0.24798029820352419, 0.1377163380710488]
printing an ep nov before normalisation:  41.12046810237538
printing an ep nov before normalisation:  66.28541840714838
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]] [[55.513]
 [50.176]
 [50.176]
 [50.176]
 [50.176]
 [50.176]
 [50.176]] [[0.625]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]]
maxi score, test score, baseline:  -0.9892617021276596 -1.0 -0.9892617021276596
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.462]
 [0.491]
 [0.498]
 [0.495]
 [0.488]
 [0.477]] [[12.316]
 [14.925]
 [12.958]
 [15.5  ]
 [15.864]
 [18.075]
 [14.137]] [[0.981]
 [1.175]
 [1.03 ]
 [1.263]
 [1.292]
 [1.481]
 [1.12 ]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.022781550009369766
maxi score, test score, baseline:  -0.9892617021276596 -1.0 -0.9892617021276596
probs:  [0.20195977821676334, 0.03904438263673708, 0.14518623127220875, 0.2614368273967727, 0.2614368273967727, 0.09093595308074544]
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]] [[37.169]
 [37.169]
 [37.169]
 [37.169]
 [37.169]
 [37.169]
 [37.169]] [[0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]]
siam score:  -0.8646724
maxi score, test score, baseline:  -0.9892617021276596 -1.0 -0.9892617021276596
maxi score, test score, baseline:  -0.9892617021276596 -1.0 -0.9892617021276596
probs:  [0.20195977821676334, 0.03904438263673708, 0.14518623127220875, 0.2614368273967727, 0.2614368273967727, 0.09093595308074544]
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.4  ]
 [0.4  ]
 [0.404]
 [0.4  ]
 [0.386]
 [0.4  ]] [[76.59 ]
 [69.017]
 [69.017]
 [78.276]
 [80.165]
 [82.008]
 [69.017]] [[1.619]
 [1.494]
 [1.494]
 [1.646]
 [1.672]
 [1.688]
 [1.494]]
maxi score, test score, baseline:  -0.9892617021276596 -1.0 -0.9892617021276596
probs:  [0.20195977821676334, 0.03904438263673708, 0.14518623127220875, 0.2614368273967727, 0.2614368273967727, 0.09093595308074544]
printing an ep nov before normalisation:  91.49718467851173
using explorer policy with actor:  1
printing an ep nov before normalisation:  78.96205368063093
siam score:  -0.862269
maxi score, test score, baseline:  -0.9893736842105263 -1.0 -0.9893736842105263
probs:  [0.11401589061153002, 0.040966317184750585, 0.19038589919407198, 0.2703080011990586, 0.2703080011990586, 0.11401589061153002]
maxi score, test score, baseline:  -0.9893736842105263 -1.0 -0.9893736842105263
probs:  [0.11401589061153002, 0.040966317184750585, 0.19038589919407198, 0.2703080011990586, 0.2703080011990586, 0.11401589061153002]
maxi score, test score, baseline:  -0.9893736842105263 -1.0 -0.9893736842105263
probs:  [0.11401589061153002, 0.040966317184750585, 0.19038589919407198, 0.2703080011990586, 0.2703080011990586, 0.11401589061153002]
maxi score, test score, baseline:  -0.9893736842105263 -1.0 -0.9893736842105263
probs:  [0.11401589061153002, 0.040966317184750585, 0.19038589919407198, 0.2703080011990586, 0.2703080011990586, 0.11401589061153002]
printing an ep nov before normalisation:  65.28335094451904
maxi score, test score, baseline:  -0.9893736842105263 -1.0 -0.9893736842105263
probs:  [0.11401589061153002, 0.040966317184750585, 0.19038589919407198, 0.2703080011990586, 0.2703080011990586, 0.11401589061153002]
printing an ep nov before normalisation:  49.06858481583055
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.311]
 [0.317]
 [0.325]
 [0.328]
 [0.328]
 [0.325]] [[38.144]
 [37.333]
 [41.632]
 [41.579]
 [40.326]
 [39.803]
 [39.754]] [[1.175]
 [1.117]
 [1.317]
 [1.323]
 [1.269]
 [1.246]
 [1.241]]
printing an ep nov before normalisation:  36.00311451808295
printing an ep nov before normalisation:  42.438718816938426
Printing some Q and Qe and total Qs values:  [[0.4  ]
 [0.431]
 [0.395]
 [0.391]
 [0.391]
 [0.388]
 [0.386]] [[36.732]
 [39.661]
 [39.291]
 [48.402]
 [48.431]
 [48.618]
 [48.655]] [[1.15 ]
 [1.28 ]
 [1.231]
 [1.534]
 [1.535]
 [1.538]
 [1.537]]
printing an ep nov before normalisation:  50.874324457735604
printing an ep nov before normalisation:  57.25264923282603
siam score:  -0.8620136
maxi score, test score, baseline:  -0.9893736842105263 -1.0 -0.9893736842105263
printing an ep nov before normalisation:  48.60531261989049
using explorer policy with actor:  1
printing an ep nov before normalisation:  57.041096687316895
printing an ep nov before normalisation:  46.90392874591929
maxi score, test score, baseline:  -0.9893736842105263 -1.0 -0.9893736842105263
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.62534849907362
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.608]
 [0.638]
 [0.666]
 [0.643]
 [0.608]
 [0.608]] [[63.87 ]
 [62.432]
 [67.314]
 [66.791]
 [66.132]
 [62.432]
 [62.432]] [[1.951]
 [1.91 ]
 [2.136]
 [2.143]
 [2.094]
 [1.91 ]
 [1.91 ]]
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
probs:  [0.08756135531732584, 0.08756135531732584, 0.08756135531732584, 0.5621932234133707, 0.08756135531732584, 0.08756135531732584]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  80.17373571095638
printing an ep nov before normalisation:  93.8068015671016
Printing some Q and Qe and total Qs values:  [[0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]] [[68.261]
 [68.261]
 [68.261]
 [68.261]
 [68.261]
 [68.261]
 [68.261]] [[1.917]
 [1.917]
 [1.917]
 [1.917]
 [1.917]
 [1.917]
 [1.917]]
actions average: 
K:  4  action  0 :  tensor([0.3238, 0.0366, 0.1345, 0.1458, 0.1360, 0.1147, 0.1088],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.1102, 0.4761, 0.0977, 0.0821, 0.0731, 0.0649, 0.0959],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1783, 0.0725, 0.1459, 0.1806, 0.1530, 0.1305, 0.1391],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1486, 0.0295, 0.1453, 0.2211, 0.1786, 0.1375, 0.1394],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1486, 0.0363, 0.1355, 0.2053, 0.1921, 0.1387, 0.1434],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1088, 0.1143, 0.1362, 0.2069, 0.1337, 0.1924, 0.1077],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1617, 0.1379, 0.1281, 0.1600, 0.1538, 0.1059, 0.1526],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
probs:  [0.2185657642065087, 0.2185657642065087, 0.2185657642065087, 0.09246382472953713, 0.03327311844442815, 0.2185657642065087]
printing an ep nov before normalisation:  75.23176159725395
printing an ep nov before normalisation:  25.439786944580668
siam score:  -0.8719949
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
probs:  [0.2185657642065087, 0.2185657642065087, 0.2185657642065087, 0.09246382472953713, 0.03327311844442815, 0.2185657642065087]
printing an ep nov before normalisation:  38.09706687927246
printing an ep nov before normalisation:  56.41691919017726
printing an ep nov before normalisation:  71.46074353237806
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
probs:  [0.2185657642065087, 0.2185657642065087, 0.2185657642065087, 0.09246382472953713, 0.03327311844442815, 0.2185657642065087]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
probs:  [0.2185657642065087, 0.2185657642065087, 0.2185657642065087, 0.09246382472953713, 0.03327311844442815, 0.2185657642065087]
printing an ep nov before normalisation:  47.27829290097213
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]] [[52.493]
 [52.493]
 [52.493]
 [52.493]
 [52.493]
 [52.493]
 [52.493]] [[1.66]
 [1.66]
 [1.66]
 [1.66]
 [1.66]
 [1.66]
 [1.66]]
printing an ep nov before normalisation:  61.7164421081543
maxi score, test score, baseline:  -0.9896959183673469 -1.0 -0.9896959183673469
probs:  [0.22224693732875694, 0.22224693732875694, 0.22224693732875694, 0.03333827172581438, 0.07767397895915794, 0.22224693732875694]
Printing some Q and Qe and total Qs values:  [[-0.102]
 [-0.102]
 [-0.102]
 [-0.054]
 [-0.102]
 [-0.102]
 [-0.102]] [[34.844]
 [34.844]
 [34.844]
 [69.3  ]
 [34.844]
 [34.844]
 [34.844]] [[0.529]
 [0.529]
 [0.529]
 [1.467]
 [0.529]
 [0.529]
 [0.529]]
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
probs:  [0.24650703415670702, 0.24650703415670702, 0.13737058240818606, 0.03696504679954703, 0.08614326832214589, 0.24650703415670702]
printing an ep nov before normalisation:  37.53086839403425
printing an ep nov before normalisation:  50.48096054280044
using explorer policy with actor:  1
printing an ep nov before normalisation:  59.763689041137695
printing an ep nov before normalisation:  63.33610239884777
printing an ep nov before normalisation:  56.00934878066005
printing an ep nov before normalisation:  67.3437868856128
UNIT TEST: sample policy line 217 mcts : [0.184 0.061 0.245 0.204 0.061 0.143 0.102]
siam score:  -0.87381303
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
probs:  [0.29289214508596917, 0.29289214508596917, 0.1632084343812063, 0.04389942053282458, 0.04389942053282458, 0.1632084343812063]
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
probs:  [0.29289214508596917, 0.29289214508596917, 0.1632084343812063, 0.04389942053282458, 0.04389942053282458, 0.1632084343812063]
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
probs:  [0.31188421592633836, 0.31188421592633836, 0.1089666933484558, 0.04673865309123851, 0.04673865309123851, 0.17378756861639053]
printing an ep nov before normalisation:  72.70106143300013
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
probs:  [0.31188421592633836, 0.31188421592633836, 0.1089666933484558, 0.04673865309123851, 0.04673865309123851, 0.17378756861639053]
printing an ep nov before normalisation:  35.30764661939962
printing an ep nov before normalisation:  53.47965174494145
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
probs:  [0.31188421592633836, 0.31188421592633836, 0.1089666933484558, 0.04673865309123851, 0.04673865309123851, 0.17378756861639053]
UNIT TEST: sample policy line 217 mcts : [0.143 0.082 0.082 0.143 0.449 0.061 0.041]
Printing some Q and Qe and total Qs values:  [[0.178]
 [0.119]
 [0.168]
 [0.156]
 [0.119]
 [0.119]
 [0.163]] [[37.854]
 [38.651]
 [38.39 ]
 [46.791]
 [38.651]
 [38.651]
 [38.147]] [[1.251]
 [1.238]
 [1.272]
 [1.744]
 [1.238]
 [1.238]
 [1.253]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
printing an ep nov before normalisation:  55.47124761430291
printing an ep nov before normalisation:  45.751906876968086
printing an ep nov before normalisation:  2.3570853084805776
maxi score, test score, baseline:  -0.9900960784313726 -1.0 -0.9900960784313726
maxi score, test score, baseline:  -0.9900960784313726 -1.0 -0.9900960784313726
probs:  [0.19637882206035798, 0.35855482779956177, 0.09886122383809137, 0.052915432367984884, 0.14664484696700195, 0.14664484696700195]
using explorer policy with actor:  0
printing an ep nov before normalisation:  69.30585934291831
maxi score, test score, baseline:  -0.9900960784313726 -1.0 -0.9900960784313726
probs:  [0.20583763490323254, 0.37583033830375817, 0.05545947420276707, 0.05545947420276707, 0.1537065391937376, 0.1537065391937376]
maxi score, test score, baseline:  -0.9900960784313726 -1.0 -0.9900960784313726
probs:  [0.20583763490323254, 0.37583033830375817, 0.05545947420276707, 0.05545947420276707, 0.1537065391937376, 0.1537065391937376]
printing an ep nov before normalisation:  0.006671436580063528
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.23564153516667177, 0.3690076095984777, 0.05482791502355046, 0.05482791502355046, 0.17295948018372298, 0.11273554500402667]
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.23564153516667177, 0.3690076095984777, 0.05482791502355046, 0.05482791502355046, 0.17295948018372298, 0.11273554500402667]
printing an ep nov before normalisation:  19.789748961510597
siam score:  -0.87332445
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.24721385742541796, 0.35688413734043484, 0.09852626638678992, 0.09852626638678992, 0.14614516809197137, 0.052704304368596105]
printing an ep nov before normalisation:  32.67885642419941
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.24721385742541796, 0.35688413734043484, 0.09852626638678992, 0.09852626638678992, 0.14614516809197137, 0.052704304368596105]
actions average: 
K:  4  action  0 :  tensor([0.3764, 0.0162, 0.1175, 0.1212, 0.1469, 0.1189, 0.1030],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0725, 0.6267, 0.0671, 0.0563, 0.0585, 0.0664, 0.0525],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1921, 0.0355, 0.1999, 0.1360, 0.1774, 0.1455, 0.1136],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1631, 0.0299, 0.1585, 0.1697, 0.1782, 0.1460, 0.1547],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2126, 0.0048, 0.1260, 0.1408, 0.2581, 0.1456, 0.1122],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1334, 0.0361, 0.1385, 0.1681, 0.1779, 0.2171, 0.1290],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1879, 0.0178, 0.1413, 0.1673, 0.1911, 0.1539, 0.1406],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  59.824799630400186
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.24721385742541796, 0.35688413734043484, 0.09852626638678992, 0.09852626638678992, 0.14614516809197137, 0.052704304368596105]
printing an ep nov before normalisation:  92.6050597942789
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.24721385742541796, 0.35688413734043484, 0.09852626638678992, 0.09852626638678992, 0.14614516809197137, 0.052704304368596105]
line 256 mcts: sample exp_bonus 52.78585875034332
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.22276639084966038, 0.3485866051829122, 0.10662465454204377, 0.10662465454204377, 0.16355687822224785, 0.05184081666109198]
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.23568092090470977, 0.36879972862643967, 0.11280202146926785, 0.054840276452549186, 0.17303677609448442, 0.054840276452549186]
printing an ep nov before normalisation:  72.73073680955594
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.23568092090470977, 0.36879972862643967, 0.11280202146926785, 0.054840276452549186, 0.17303677609448442, 0.054840276452549186]
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.18460190328623902, 0.39345899676598867, 0.1203381822155468, 0.05849950722299322, 0.18460190328623902, 0.05849950722299322]
Printing some Q and Qe and total Qs values:  [[0.614]
 [0.572]
 [0.599]
 [0.6  ]
 [0.614]
 [0.592]
 [0.591]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.614]
 [0.572]
 [0.599]
 [0.6  ]
 [0.614]
 [0.592]
 [0.591]]
printing an ep nov before normalisation:  14.410651499511689
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.18460190328623902, 0.39345899676598867, 0.1203381822155468, 0.05849950722299322, 0.18460190328623902, 0.05849950722299322]
printing an ep nov before normalisation:  54.06007144218231
printing an ep nov before normalisation:  30.35933994797425
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
probs:  [0.18460190356290426, 0.3934590002644422, 0.12033818150089245, 0.05849950555442852, 0.18460190356290426, 0.05849950555442852]
printing an ep nov before normalisation:  54.78124743876053
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
probs:  [0.18460190356290426, 0.3934590002644422, 0.12033818150089245, 0.05849950555442852, 0.18460190356290426, 0.05849950555442852]
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
probs:  [0.18460190356290426, 0.3934590002644422, 0.12033818150089245, 0.05849950555442852, 0.18460190356290426, 0.05849950555442852]
printing an ep nov before normalisation:  29.45636749267578
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.20176757766384426, 0.36129173988710606, 0.12660715507788523, 0.05428297485366002, 0.20176757766384426, 0.05428297485366002]
using another actor
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.23668292606225402, 0.4238305645275314, 0.06365963766982824, 0.06365963766982824, 0.14850759640072977, 0.06365963766982824]
printing an ep nov before normalisation:  24.982036187960283
printing an ep nov before normalisation:  31.450706648911204
using explorer policy with actor:  1
printing an ep nov before normalisation:  18.38809956399625
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.17390986280083334, 0.3702995954575357, 0.05510619736653176, 0.11338724078713291, 0.17390986280083334, 0.11338724078713291]
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.17390986280083334, 0.3702995954575357, 0.05510619736653176, 0.11338724078713291, 0.17390986280083334, 0.11338724078713291]
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.18818848438655042, 0.3367305478454942, 0.05064953673938109, 0.11812147332101193, 0.18818848438655042, 0.11812147332101193]
actions average: 
K:  0  action  0 :  tensor([0.2921, 0.0165, 0.1427, 0.1743, 0.1354, 0.1269, 0.1122],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0243, 0.8396, 0.0242, 0.0338, 0.0171, 0.0216, 0.0394],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1995, 0.0586, 0.1540, 0.1760, 0.1469, 0.1429, 0.1220],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1748, 0.0155, 0.1406, 0.2255, 0.1608, 0.1615, 0.1212],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2112, 0.0140, 0.1493, 0.1630, 0.1956, 0.1424, 0.1246],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.2061, 0.0388, 0.1580, 0.1853, 0.1338, 0.1616, 0.1164],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2181, 0.0205, 0.1534, 0.1794, 0.1471, 0.1473, 0.1342],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  45.57543601942904
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.14752513660432878, 0.2582725605364252, 0.040879469114163264, 0.14752513660432878, 0.2582725605364252, 0.14752513660432878]
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.374]
 [0.409]
 [0.74 ]
 [0.488]
 [0.418]
 [0.309]] [[42.227]
 [42.877]
 [43.312]
 [44.606]
 [44.342]
 [43.392]
 [37.445]] [[1.691]
 [1.587]
 [1.64 ]
 [2.023]
 [1.76 ]
 [1.652]
 [1.302]]
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
maxi score, test score, baseline:  -0.9907256880733946 -1.0 -0.9907256880733946
probs:  [0.16542103715551543, 0.23393066026884182, 0.03587556799576987, 0.16542103715551543, 0.23393066026884182, 0.16542103715551543]
Printing some Q and Qe and total Qs values:  [[0.677]
 [0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.545]] [[33.479]
 [35.356]
 [35.356]
 [35.356]
 [35.356]
 [35.356]
 [35.356]] [[2.06]
 [2.03]
 [2.03]
 [2.03]
 [2.03]
 [2.03]
 [2.03]]
printing an ep nov before normalisation:  21.334678784266302
maxi score, test score, baseline:  -0.9907256880733946 -1.0 -0.9907256880733946
printing an ep nov before normalisation:  54.76027750738552
maxi score, test score, baseline:  -0.9907256880733946 -1.0 -0.9907256880733946
probs:  [0.1775881317086369, 0.2511408554606817, 0.03850661770477069, 0.1775881317086369, 0.1775881317086369, 0.1775881317086369]
printing an ep nov before normalisation:  57.166214739481084
maxi score, test score, baseline:  -0.9907256880733946 -1.0 -0.9907256880733946
printing an ep nov before normalisation:  34.6326208114624
maxi score, test score, baseline:  -0.9907256880733946 -1.0 -0.9907256880733946
probs:  [0.15109499779629268, 0.23577908718855156, 0.035211507048991196, 0.19263813598872154, 0.19263813598872154, 0.19263813598872154]
printing an ep nov before normalisation:  29.345809535543037
maxi score, test score, baseline:  -0.9907256880733946 -1.0 -0.9907256880733946
probs:  [0.15109499779629268, 0.23577908718855156, 0.035211507048991196, 0.19263813598872154, 0.19263813598872154, 0.19263813598872154]
printing an ep nov before normalisation:  41.783151712142335
printing an ep nov before normalisation:  64.71894264221191
maxi score, test score, baseline:  -0.9907256880733946 -1.0 -0.9907256880733946
probs:  [0.16499525063962409, 0.21263060741033676, 0.032117676489741565, 0.21263060741033676, 0.21263060741033676, 0.16499525063962409]
Printing some Q and Qe and total Qs values:  [[0.587]
 [0.538]
 [0.589]
 [0.592]
 [0.587]
 [0.584]
 [0.587]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.587]
 [0.538]
 [0.589]
 [0.592]
 [0.587]
 [0.584]
 [0.587]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  65.34184217453003
actions average: 
K:  1  action  0 :  tensor([0.3239, 0.0153, 0.1307, 0.1631, 0.1458, 0.0967, 0.1245],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0365, 0.7815, 0.0334, 0.0420, 0.0209, 0.0219, 0.0638],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2042, 0.0422, 0.2043, 0.1613, 0.1414, 0.1163, 0.1303],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1876, 0.0758, 0.1375, 0.1921, 0.1628, 0.1163, 0.1279],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2001, 0.0064, 0.1234, 0.1991, 0.2391, 0.1079, 0.1240],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1945, 0.0420, 0.1488, 0.1843, 0.1643, 0.1194, 0.1466],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1992, 0.0484, 0.1378, 0.1788, 0.1595, 0.1113, 0.1650],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9907256880733946 -1.0 -0.9907256880733946
printing an ep nov before normalisation:  27.682378271263858
printing an ep nov before normalisation:  32.76669776114201
maxi score, test score, baseline:  -0.9907256880733946 -1.0 -0.9907256880733946
probs:  [0.17324824333330746, 0.22326822599977852, 0.033718818000520616, 0.22326822599977852, 0.17324824333330746, 0.17324824333330746]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]] [[66.062]
 [66.062]
 [66.062]
 [66.062]
 [66.062]
 [66.062]
 [66.062]] [[1.971]
 [1.971]
 [1.971]
 [1.971]
 [1.971]
 [1.971]
 [1.971]]
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
probs:  [0.182371069690343, 0.182371069690343, 0.03548871199831127, 0.23502700924031666, 0.182371069690343, 0.182371069690343]
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
probs:  [0.19212052470007587, 0.19212052470007587, 0.037380178157035225, 0.24759347912116614, 0.1386647686215711, 0.19212052470007587]
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.564]
 [0.573]
 [0.569]
 [0.571]
 [0.574]
 [0.57 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.571]
 [0.564]
 [0.573]
 [0.569]
 [0.571]
 [0.574]
 [0.57 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
probs:  [0.19212052470007587, 0.19212052470007587, 0.037380178157035225, 0.24759347912116614, 0.1386647686215711, 0.19212052470007587]
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.393]
 [0.409]
 [0.41 ]
 [0.412]
 [0.41 ]
 [0.399]] [[49.716]
 [54.522]
 [51.916]
 [51.7  ]
 [52.666]
 [51.625]
 [52.773]] [[0.862]
 [1.008]
 [0.971]
 [0.968]
 [0.988]
 [0.966]
 [0.978]]
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
probs:  [0.19978079526380368, 0.19978079526380368, 0.03509525466896533, 0.23644663260378657, 0.16444826109982036, 0.16444826109982036]
printing an ep nov before normalisation:  16.41513705253601
printing an ep nov before normalisation:  48.861286764239985
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.2172836995887568, 0.17764115610552672, 0.03250913250590613, 0.2172836995887568, 0.17764115610552672, 0.17764115610552672]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 71.51257483359178
printing an ep nov before normalisation:  92.09228734963212
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.2262549094588792, 0.1849746003621214, 0.03384668909263511, 0.1849746003621214, 0.1849746003621214, 0.1849746003621214]
Printing some Q and Qe and total Qs values:  [[0.563]
 [0.55 ]
 [0.569]
 [0.566]
 [0.563]
 [0.56 ]
 [0.564]] [[85.482]
 [89.775]
 [85.106]
 [86.068]
 [86.78 ]
 [87.049]
 [88.054]] [[2.143]
 [2.297]
 [2.136]
 [2.169]
 [2.194]
 [2.201]
 [2.244]]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.2262549094588792, 0.1849746003621214, 0.03384668909263511, 0.1849746003621214, 0.1849746003621214, 0.1849746003621214]
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.044]
 [0.051]
 [0.05 ]
 [0.04 ]
 [0.047]
 [0.04 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.034]
 [0.044]
 [0.051]
 [0.05 ]
 [0.04 ]
 [0.047]
 [0.04 ]]
siam score:  -0.87256014
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.2454584515900223, 0.2006723833575797, 0.03670982847270709, 0.2006723833575797, 0.2006723833575797, 0.1158145698645316]
printing an ep nov before normalisation:  53.6360039821026
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.2454584515900223, 0.2006723833575797, 0.03670982847270709, 0.2006723833575797, 0.2006723833575797, 0.1158145698645316]
printing an ep nov before normalisation:  56.048808097839355
printing an ep nov before normalisation:  57.6613866161847
printing an ep nov before normalisation:  53.06536520249387
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.2454584515900223, 0.2006723833575797, 0.03670982847270709, 0.2006723833575797, 0.2006723833575797, 0.1158145698645316]
printing an ep nov before normalisation:  30.115951599941013
Printing some Q and Qe and total Qs values:  [[0.256]
 [0.256]
 [0.256]
 [0.256]
 [0.256]
 [0.256]
 [0.256]] [[45.214]
 [45.214]
 [45.214]
 [45.214]
 [45.214]
 [45.214]
 [45.214]] [[1.256]
 [1.256]
 [1.256]
 [1.256]
 [1.256]
 [1.256]
 [1.256]]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.25654066706710005, 0.20973145238141114, 0.03836212404058604, 0.20973145238141114, 0.16459399536306898, 0.12104030876642274]
printing an ep nov before normalisation:  40.91765770673419
printing an ep nov before normalisation:  28.464916356334093
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
probs:  [0.22278919321000645, 0.22278919321000645, 0.03371163863715074, 0.22278919321000645, 0.1729875158894771, 0.1249332658433528]
printing an ep nov before normalisation:  66.80930614471436
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
probs:  [0.22278919321000645, 0.22278919321000645, 0.03371163863715074, 0.22278919321000645, 0.1729875158894771, 0.1249332658433528]
printing an ep nov before normalisation:  83.69078145690035
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.2227891938809273, 0.2227891938809273, 0.033711637047729955, 0.2227891938809273, 0.17298751596504022, 0.12493326534444786]
siam score:  -0.8639322
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.2227891938809273, 0.2227891938809273, 0.033711637047729955, 0.2227891938809273, 0.17298751596504022, 0.12493326534444786]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.20067185770276566, 0.24538397286861882, 0.036727435427971085, 0.20067185770276566, 0.20067185770276566, 0.11587301859511324]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.20067185770276566, 0.24538397286861882, 0.036727435427971085, 0.20067185770276566, 0.20067185770276566, 0.11587301859511324]
deleting a thread, now have 4 threads
Frames:  17004 train batches done:  1994 episodes:  408
printing an ep nov before normalisation:  43.93351480989352
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.2090876456314295, 0.2556761457974879, 0.03826314502254954, 0.2090876456314295, 0.2090876456314295, 0.0787977722856739]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.569]
 [0.55 ]
 [0.577]
 [0.567]
 [0.574]
 [0.58 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.561]
 [0.569]
 [0.55 ]
 [0.577]
 [0.567]
 [0.574]
 [0.58 ]]
printing an ep nov before normalisation:  31.209311640452277
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.24041956811606377, 0.29399383715796523, 0.04398058162909191, 0.09059322249040766, 0.24041956811606377, 0.09059322249040766]
printing an ep nov before normalisation:  30.68274536836892
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.2535292190054478, 0.310026417248427, 0.046372825447859034, 0.09552857985135499, 0.19901437859555615, 0.09552857985135499]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.2535292190054478, 0.310026417248427, 0.046372825447859034, 0.09552857985135499, 0.19901437859555615, 0.09552857985135499]
printing an ep nov before normalisation:  62.4771362292335
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.2358947021597499, 0.36749270463581585, 0.05494744875515992, 0.11321859815663748, 0.17349909753747697, 0.05494744875515992]
siam score:  -0.87065315
printing an ep nov before normalisation:  72.57083614440047
Printing some Q and Qe and total Qs values:  [[0.588]
 [0.532]
 [0.578]
 [0.581]
 [0.594]
 [0.592]
 [0.59 ]] [[42.962]
 [53.613]
 [44.141]
 [44.151]
 [43.614]
 [43.933]
 [43.836]] [[0.948]
 [1.067]
 [0.957]
 [0.96 ]
 [0.965]
 [0.968]
 [0.965]]
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.18504607681911256, 0.3919605670865789, 0.05859833276677239, 0.12075061374165119, 0.18504607681911256, 0.05859833276677239]
printing an ep nov before normalisation:  57.06943363199227
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.21838831176390253, 0.3892918185880017, 0.058878372061411686, 0.13728156276263617, 0.13728156276263617, 0.058878372061411686]
printing an ep nov before normalisation:  30.14059766999945
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.24682546612988865, 0.3484594740452927, 0.05372085109062238, 0.14863667882178697, 0.14863667882178697, 0.05372085109062238]
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.24682546612988865, 0.3484594740452927, 0.05372085109062238, 0.14863667882178697, 0.14863667882178697, 0.05372085109062238]
line 256 mcts: sample exp_bonus 50.135592738751264
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.2500854139908962, 0.2500854139908962, 0.10676896640402314, 0.17721264403146883, 0.17721264403146883, 0.038634917551246845]
printing an ep nov before normalisation:  76.61306103268082
printing an ep nov before normalisation:  54.24490796374565
siam score:  -0.8736772
deleting a thread, now have 3 threads
Frames:  17505 train batches done:  2050 episodes:  424
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.20686431100392838, 0.29193951046868094, 0.12462495152133456, 0.12462495152133456, 0.20686431100392838, 0.04508196448079324]
printing an ep nov before normalisation:  26.32453663953664
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.20686431100392838, 0.29193951046868094, 0.12462495152133456, 0.12462495152133456, 0.20686431100392838, 0.04508196448079324]
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.20686431100392838, 0.29193951046868094, 0.12462495152133456, 0.12462495152133456, 0.20686431100392838, 0.04508196448079324]
printing an ep nov before normalisation:  40.30463728154447
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.20686431100392838, 0.29193951046868094, 0.12462495152133456, 0.12462495152133456, 0.20686431100392838, 0.04508196448079324]
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.2568243179003617, 0.2568243179003617, 0.14719740643347654, 0.04116481993271911, 0.2568243179003617, 0.04116481993271911]
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.2568243179003617, 0.2568243179003617, 0.14719740643347654, 0.04116481993271911, 0.2568243179003617, 0.04116481993271911]
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.2568243179003617, 0.2568243179003617, 0.14719740643347654, 0.04116481993271911, 0.2568243179003617, 0.04116481993271911]
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.2568243179003617, 0.2568243179003617, 0.14719740643347654, 0.04116481993271911, 0.2568243179003617, 0.04116481993271911]
from probs:  [0.2568243179003617, 0.2568243179003617, 0.14719740643347654, 0.04116481993271911, 0.2568243179003617, 0.04116481993271911]
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.2872974413345538, 0.2872974413345538, 0.04603589199877951, 0.04603589199877951, 0.2872974413345538, 0.04603589199877951]
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.2872974413345538, 0.2872974413345538, 0.04603589199877951, 0.04603589199877951, 0.2872974413345538, 0.04603589199877951]
Printing some Q and Qe and total Qs values:  [[0.289]
 [0.303]
 [0.287]
 [0.246]
 [0.233]
 [0.243]
 [0.24 ]] [[46.629]
 [46.604]
 [47.511]
 [46.418]
 [46.895]
 [44.387]
 [44.806]] [[0.289]
 [0.303]
 [0.287]
 [0.246]
 [0.233]
 [0.243]
 [0.24 ]]
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.2872974413345538, 0.2872974413345538, 0.04603589199877951, 0.04603589199877951, 0.2872974413345538, 0.04603589199877951]
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.2872974452730205, 0.2872974452730205, 0.046035888060312835, 0.046035888060312835, 0.2872974452730205, 0.046035888060312835]
siam score:  -0.87320864
printing an ep nov before normalisation:  64.69723368557018
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.3274752047596393, 0.18767490798522474, 0.05245822749849887, 0.05245822749849887, 0.3274752047596393, 0.05245822749849887]
printing an ep nov before normalisation:  48.35542678833008
printing an ep nov before normalisation:  44.44416550293232
maxi score, test score, baseline:  -0.9913529914529915 -1.0 -0.9913529914529915
probs:  [0.316108269948275, 0.13502593633016957, 0.04886579372155541, 0.13502593633016957, 0.316108269948275, 0.04886579372155541]
maxi score, test score, baseline:  -0.9913529914529915 -1.0 -0.9913529914529915
probs:  [0.348170825255155, 0.1487137419163205, 0.05381077484381094, 0.1487137419163205, 0.24678014122458206, 0.05381077484381094]
maxi score, test score, baseline:  -0.9913529914529915 -1.0 -0.9913529914529915
probs:  [0.348170825255155, 0.1487137419163205, 0.05381077484381094, 0.1487137419163205, 0.24678014122458206, 0.05381077484381094]
maxi score, test score, baseline:  -0.9913529914529915 -1.0 -0.9913529914529915
probs:  [0.2883670251824506, 0.16534383668279906, 0.046289138134750295, 0.16534383668279906, 0.2883670251824506, 0.046289138134750295]
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.553]
 [0.581]
 [0.551]
 [0.581]
 [0.581]
 [0.581]] [[51.641]
 [55.724]
 [74.292]
 [53.312]
 [74.292]
 [74.292]
 [74.292]] [[2.02 ]
 [2.22 ]
 [3.262]
 [2.087]
 [3.262]
 [3.262]
 [3.262]]
printing an ep nov before normalisation:  55.71852046199037
maxi score, test score, baseline:  -0.9913529914529915 -1.0 -0.9913529914529915
probs:  [0.2883670251824506, 0.16534383668279906, 0.046289138134750295, 0.16534383668279906, 0.2883670251824506, 0.046289138134750295]
maxi score, test score, baseline:  -0.9913529914529915 -1.0 -0.9913529914529915
maxi score, test score, baseline:  -0.9913529914529915 -1.0 -0.9913529914529915
probs:  [0.05030769256446563, 0.2830256407688677, 0.05030769256446563, 0.2830256407688677, 0.2830256407688677, 0.05030769256446563]
printing an ep nov before normalisation:  38.55131061704387
maxi score, test score, baseline:  -0.9913529914529915 -1.0 -0.9913529914529915
from probs:  [0.05030769256446563, 0.2830256407688677, 0.05030769256446563, 0.2830256407688677, 0.2830256407688677, 0.05030769256446563]
maxi score, test score, baseline:  -0.9913529914529915 -1.0 -0.9913529914529915
probs:  [0.06554156394724198, 0.3689168721055161, 0.06554156394724198, 0.3689168721055161, 0.06554156394724198, 0.06554156394724198]
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]] [[52.038]
 [52.038]
 [52.038]
 [52.038]
 [52.038]
 [52.038]
 [52.038]] [[0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.718]]
maxi score, test score, baseline:  -0.9913529914529915 -1.0 -0.9913529914529915
probs:  [0.06554156394724198, 0.3689168721055161, 0.06554156394724198, 0.3689168721055161, 0.06554156394724198, 0.06554156394724198]
maxi score, test score, baseline:  -0.9913529914529915 -1.0 -0.9913529914529915
probs:  [0.04143744158190116, 0.2576103962525871, 0.14778058863764157, 0.2576103962525871, 0.14778058863764157, 0.14778058863764157]
maxi score, test score, baseline:  -0.9913529914529915 -1.0 -0.9913529914529915
probs:  [0.04143744158190116, 0.2576103962525871, 0.14778058863764157, 0.2576103962525871, 0.14778058863764157, 0.14778058863764157]
maxi score, test score, baseline:  -0.9913529914529915 -1.0 -0.9913529914529915
probs:  [0.0362128235031827, 0.23348448877479247, 0.16560606631574407, 0.23348448877479247, 0.16560606631574407, 0.16560606631574407]
maxi score, test score, baseline:  -0.9913529914529915 -1.0 -0.9913529914529915
probs:  [0.0362128235031827, 0.23348448877479247, 0.16560606631574407, 0.23348448877479247, 0.16560606631574407, 0.16560606631574407]
printing an ep nov before normalisation:  51.78724180149651
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  96.11025332947959
maxi score, test score, baseline:  -0.9913529914529915 -1.0 -0.9913529914529915
probs:  [0.0362128235031827, 0.23348448877479247, 0.16560606631574407, 0.23348448877479247, 0.16560606631574407, 0.16560606631574407]
maxi score, test score, baseline:  -0.9913529914529915 -1.0 -0.9913529914529915
probs:  [0.0362128235031827, 0.23348448877479247, 0.16560606631574407, 0.23348448877479247, 0.16560606631574407, 0.16560606631574407]
printing an ep nov before normalisation:  58.14935225982438
printing an ep nov before normalisation:  46.358832097427914
maxi score, test score, baseline:  -0.9913529914529915 -1.0 -0.9913529914529915
maxi score, test score, baseline:  -0.9913529914529915 -1.0 -0.9913529914529915
probs:  [0.038842446275680585, 0.25049204561743266, 0.1776663770267217, 0.1776663770267217, 0.1776663770267217, 0.1776663770267217]
maxi score, test score, baseline:  -0.9913529914529915 -1.0 -0.9913529914529915
probs:  [0.038842446275680585, 0.25049204561743266, 0.1776663770267217, 0.1776663770267217, 0.1776663770267217, 0.1776663770267217]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
Printing some Q and Qe and total Qs values:  [[0.567]
 [0.562]
 [0.577]
 [0.59 ]
 [0.57 ]
 [0.59 ]
 [0.59 ]] [[35.707]
 [37.792]
 [33.512]
 [60.488]
 [36.169]
 [60.488]
 [60.488]] [[1.166]
 [1.229]
 [1.104]
 [2.   ]
 [1.184]
 [2.   ]
 [2.   ]]
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.041781574089767605, 0.26950140107499926, 0.19114619178975947, 0.1152784494659546, 0.19114619178975947, 0.19114619178975947]
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.034050058500524054, 0.2111846093372799, 0.2111846093372799, 0.12121150415035654, 0.2111846093372799, 0.2111846093372799]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.03740631703361788, 0.23206931369215303, 0.23206931369215303, 0.13319287094496146, 0.23206931369215303, 0.13319287094496146]
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.03740631703361788, 0.23206931369215303, 0.23206931369215303, 0.13319287094496146, 0.23206931369215303, 0.13319287094496146]
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.03740631703361788, 0.23206931369215303, 0.23206931369215303, 0.13319287094496146, 0.23206931369215303, 0.13319287094496146]
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.046424778150534765, 0.1653874976398997, 0.2881877242095655, 0.1653874976398997, 0.2881877242095655, 0.046424778150534765]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.046424778150534765, 0.1653874976398997, 0.2881877242095655, 0.1653874976398997, 0.2881877242095655, 0.046424778150534765]
printing an ep nov before normalisation:  34.90963697433472
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.052681005834710797, 0.052681005834710797, 0.32711781981228594, 0.18772134287129577, 0.32711781981228594, 0.052681005834710797]
printing an ep nov before normalisation:  49.51170606547306
printing an ep nov before normalisation:  25.804419170697184
siam score:  -0.85264635
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
printing an ep nov before normalisation:  16.04137245756884
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.12444711321487477, 0.12444711321487477, 0.29076656620678193, 0.12444711321487477, 0.29076656620678193, 0.04512552794181194]
printing an ep nov before normalisation:  14.17563960417447
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.12444711321487477, 0.12444711321487477, 0.29076656620678193, 0.12444711321487477, 0.29076656620678193, 0.04512552794181194]
printing an ep nov before normalisation:  17.711716316249237
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.14783070388067834, 0.14783070388067834, 0.25747287532150437, 0.14783070388067834, 0.25747287532150437, 0.0415621377149562]
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.16540829974680313, 0.04649262581968447, 0.2880990744335124, 0.16540829974680313, 0.2880990744335124, 0.04649262581968447]
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
probs:  [0.16540829970313514, 0.046492621649469754, 0.28809907864739515, 0.16540829970313514, 0.28809907864739515, 0.046492621649469754]
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
printing an ep nov before normalisation:  55.13320624841837
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
probs:  [0.1885429252826621, 0.05298184730157191, 0.1885429252826621, 0.1885429252826621, 0.32840752954886987, 0.05298184730157191]
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
probs:  [0.1885429252826621, 0.05298184730157191, 0.1885429252826621, 0.1885429252826621, 0.32840752954886987, 0.05298184730157191]
printing an ep nov before normalisation:  35.89405805423166
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
probs:  [0.22941257733615797, 0.04117484532768406, 0.22941257733615797, 0.22941257733615797, 0.22941257733615797, 0.04117484532768406]
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
probs:  [0.2319854274512061, 0.13325936153258938, 0.2319854274512061, 0.037524994581203064, 0.2319854274512061, 0.13325936153258938]
printing an ep nov before normalisation:  35.60291411690397
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
probs:  [0.25740519645020166, 0.14785502024812883, 0.14785502024812883, 0.0416245463552101, 0.25740519645020166, 0.14785502024812883]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
probs:  [0.28908526719190525, 0.16604525245080223, 0.16604525245080223, 0.04673372300488587, 0.16604525245080223, 0.16604525245080223]
using explorer policy with actor:  1
printing an ep nov before normalisation:  60.99844101924154
siam score:  -0.85753906
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.3282666994743358, 0.18854265364063064, 0.05305266980188617, 0.05305266980188617, 0.18854265364063064, 0.18854265364063064]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.3797408808165458, 0.21809837353043024, 0.0613541240408646, 0.0613541240408646, 0.21809837353043024, 0.0613541240408646]
from probs:  [0.23194404200160426, 0.23194404200160426, 0.13329176799917994, 0.13329176799917994, 0.23194404200160426, 0.037584337996827505]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.2573381962071862, 0.2573381962071862, 0.1478788722573701, 0.1478788722573701, 0.1478788722573701, 0.04168699081351754]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.2573381962071862, 0.2573381962071862, 0.1478788722573701, 0.1478788722573701, 0.1478788722573701, 0.04168699081351754]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.16605509736069823, 0.28898052786034145, 0.16605509736069823, 0.16605509736069823, 0.16605509736069823, 0.04679908269686555]
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.05312355340089274, 0.32812705709047296, 0.18854194536924718, 0.18854194536924718, 0.18854194536924718, 0.05312355340089274]
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.045368872408903575, 0.2912512956692542, 0.20680682707478998, 0.20680682707478998, 0.12488308888613114, 0.12488308888613114]
from probs:  [0.045368872408903575, 0.2912512956692542, 0.20680682707478998, 0.20680682707478998, 0.12488308888613114, 0.12488308888613114]
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.049279854273220874, 0.31642077145267866, 0.22467540595670288, 0.22467540595670288, 0.049279854273220874, 0.13566870808747378]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.09787799742369759, 0.27558786775588057, 0.21455619511654506, 0.21455619511654506, 0.04207538113581385, 0.155346363451518]
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.09787799742369759, 0.27558786775588057, 0.21455619511654506, 0.21455619511654506, 0.04207538113581385, 0.155346363451518]
printing an ep nov before normalisation:  37.0464825630188
printing an ep nov before normalisation:  17.380564212799072
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.09787799624243428, 0.2755878696263138, 0.2145561959389211, 0.2145561959389211, 0.04207537899628812, 0.1553463632571216]
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]] [[70.596]
 [70.596]
 [70.596]
 [70.596]
 [70.596]
 [70.596]
 [70.596]] [[0.982]
 [0.982]
 [0.982]
 [0.982]
 [0.982]
 [0.982]
 [0.982]]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.10403491969864881, 0.29293870241948244, 0.1651232076431968, 0.22806265582848922, 0.044717306766985834, 0.1651232076431968]
printing an ep nov before normalisation:  38.434575924051394
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
printing an ep nov before normalisation:  34.945797068095224
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]] [[68.318]
 [68.318]
 [68.318]
 [68.318]
 [68.318]
 [68.318]
 [68.318]] [[68.748]
 [68.748]
 [68.748]
 [68.748]
 [68.748]
 [68.748]
 [68.748]]
printing an ep nov before normalisation:  35.86078968707095
printing an ep nov before normalisation:  13.550587572865425
siam score:  -0.86191595
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.11080082201963355, 0.3120056992241645, 0.1758670758419943, 0.24290503432563904, 0.0476205465689351, 0.11080082201963355]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
printing an ep nov before normalisation:  6.912664521445677
printing an ep nov before normalisation:  55.78446388244629
using explorer policy with actor:  1
printing an ep nov before normalisation:  85.35577564929555
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.401651573571634
from probs:  [0.13605573703822454, 0.22524458766389632, 0.13605573703822454, 0.3171361307327715, 0.049452070488658496, 0.13605573703822454]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.05413233222986392, 0.24660721521476084, 0.14895451722977707, 0.3472190858659572, 0.05413233222986392, 0.14895451722977707]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.06701585729940092, 0.18446145405367825, 0.06701585729940092, 0.4300295199944408, 0.06701585729940092, 0.18446145405367825]
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]] [[19.291]
 [19.291]
 [19.291]
 [19.291]
 [19.291]
 [19.291]
 [19.291]] [[0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.06701585729940092, 0.18446145405367825, 0.06701585729940092, 0.4300295199944408, 0.06701585729940092, 0.18446145405367825]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.06701585729940092, 0.18446145405367825, 0.06701585729940092, 0.4300295199944408, 0.06701585729940092, 0.18446145405367825]
actions average: 
K:  2  action  0 :  tensor([0.3236, 0.0099, 0.1137, 0.1535, 0.1677, 0.1228, 0.1089],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0392, 0.7888, 0.0268, 0.0305, 0.0394, 0.0293, 0.0461],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1931, 0.0098, 0.1989, 0.1472, 0.1692, 0.1572, 0.1246],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2121, 0.0177, 0.1368, 0.1868, 0.1744, 0.1454, 0.1268],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1803, 0.0159, 0.1300, 0.1843, 0.2168, 0.1476, 0.1250],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1382, 0.0611, 0.1647, 0.1530, 0.1630, 0.1993, 0.1206],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1812, 0.0497, 0.1208, 0.1474, 0.2080, 0.1306, 0.1623],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  68.1592740713044
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.06701585729940092, 0.18446145405367825, 0.06701585729940092, 0.4300295199944408, 0.06701585729940092, 0.18446145405367825]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.06701585729940092, 0.18446145405367825, 0.06701585729940092, 0.4300295199944408, 0.06701585729940092, 0.18446145405367825]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.06701585729940092, 0.18446145405367825, 0.06701585729940092, 0.4300295199944408, 0.06701585729940092, 0.18446145405367825]
printing an ep nov before normalisation:  43.00713848543573
printing an ep nov before normalisation:  64.60929412283744
printing an ep nov before normalisation:  63.72473040061951
printing an ep nov before normalisation:  28.171550109489857
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.06158495948665046, 0.21803994573245214, 0.06158495948665046, 0.3791652300751444, 0.06158495948665046, 0.21803994573245214]
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.06158495948665046, 0.21803994573245214, 0.06158495948665046, 0.3791652300751444, 0.06158495948665046, 0.21803994573245214]
siam score:  -0.8620877
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.1479478410387927, 0.1479478410387927, 0.04187449581416902, 0.25714099053472644, 0.1479478410387927, 0.25714099053472644]
printing an ep nov before normalisation:  46.89139525095622
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.188537390681928, 0.053336551314859655, 0.053336551314859655, 0.188537390681928, 0.188537390681928, 0.3277147253244967]
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.188537390681928, 0.053336551314859655, 0.053336551314859655, 0.188537390681928, 0.188537390681928, 0.3277147253244967]
actions average: 
K:  2  action  0 :  tensor([0.3433, 0.0125, 0.1184, 0.1402, 0.1396, 0.1147, 0.1313],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0265, 0.8801, 0.0205, 0.0210, 0.0154, 0.0185, 0.0180],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2228, 0.0123, 0.1384, 0.1715, 0.1615, 0.1459, 0.1476],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1610, 0.0399, 0.1101, 0.2146, 0.1740, 0.1573, 0.1430],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1621, 0.0206, 0.1195, 0.1723, 0.2282, 0.1441, 0.1532],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2111, 0.0197, 0.1328, 0.1486, 0.1689, 0.1941, 0.1249],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2153, 0.0262, 0.1208, 0.1732, 0.1685, 0.1371, 0.1590],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.12498462651672787, 0.04549453070985303, 0.12498462651672787, 0.20677878307162753, 0.20677878307162753, 0.2909786501134363]
Printing some Q and Qe and total Qs values:  [[0.518]
 [0.518]
 [0.538]
 [0.518]
 [0.518]
 [0.518]
 [0.518]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.518]
 [0.518]
 [0.538]
 [0.518]
 [0.518]
 [0.518]
 [0.518]]
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.13611613161118186, 0.04953885529932471, 0.13611613161118186, 0.13611613161118186, 0.22520289419294956, 0.31690985567418006]
printing an ep nov before normalisation:  78.17179758357837
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.13611613161118186, 0.04953885529932471, 0.13611613161118186, 0.13611613161118186, 0.22520289419294956, 0.31690985567418006]
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.13611613161118186, 0.04953885529932471, 0.13611613161118186, 0.13611613161118186, 0.22520289419294956, 0.31690985567418006]
printing an ep nov before normalisation:  50.17797009896291
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.13611613161118186, 0.04953885529932471, 0.13611613161118186, 0.13611613161118186, 0.22520289419294956, 0.31690985567418006]
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.14901633776710455, 0.054225788514819226, 0.054225788514819226, 0.14901633776710455, 0.24655443917162964, 0.34696130826452276]
Starting evaluation
printing an ep nov before normalisation:  42.903090165501474
printing an ep nov before normalisation:  70.9007978439331
using explorer policy with actor:  0
printing an ep nov before normalisation:  58.97030560911961
printing an ep nov before normalisation:  0.36266608596022826
printing an ep nov before normalisation:  43.672673064770386
using explorer policy with actor:  0
printing an ep nov before normalisation:  39.39927108002541
printing an ep nov before normalisation:  42.669655123441714
printing an ep nov before normalisation:  69.58226032409661
printing an ep nov before normalisation:  17.982165155702642
printing an ep nov before normalisation:  18.98255228996277
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9920875 -1.0 -0.9920875
probs:  [0.20676906138439383, 0.04553670051271042, 0.1250174417874844, 0.1250174417874844, 0.20676906138439383, 0.29089029314353315]
using explorer policy with actor:  0
printing an ep nov before normalisation:  21.627713378263493
printing an ep nov before normalisation:  17.118966579437256
printing an ep nov before normalisation:  16.108476688157108
printing an ep nov before normalisation:  13.693755229343132
printing an ep nov before normalisation:  38.63880581522412
printing an ep nov before normalisation:  58.602464045778056
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9920875 -1.0 -0.9920875
probs:  [0.22518187101381756, 0.04958249208781994, 0.1361455662062691, 0.1361455662062691, 0.1361455662062691, 0.3167989382795551]
printing an ep nov before normalisation:  14.441860272541767
printing an ep nov before normalisation:  31.460180232605232
printing an ep nov before normalisation:  13.523100129751732
Printing some Q and Qe and total Qs values:  [[0.26 ]
 [0.245]
 [0.258]
 [0.256]
 [0.254]
 [0.253]
 [0.248]] [[24.976]
 [15.366]
 [15.311]
 [15.655]
 [18.487]
 [14.919]
 [13.174]] [[0.26 ]
 [0.245]
 [0.258]
 [0.256]
 [0.254]
 [0.253]
 [0.248]]
printing an ep nov before normalisation:  17.066918616960713
maxi score, test score, baseline:  -0.9920875 -1.0 -0.9920875
probs:  [0.22518187101381756, 0.04958249208781994, 0.1361455662062691, 0.1361455662062691, 0.1361455662062691, 0.3167989382795551]
Printing some Q and Qe and total Qs values:  [[0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]] [[13.174]
 [13.174]
 [13.174]
 [13.174]
 [13.174]
 [13.174]
 [13.174]] [[0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]]
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]] [[14.296]
 [14.296]
 [14.296]
 [14.296]
 [14.296]
 [14.296]
 [14.296]] [[0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]]
printing an ep nov before normalisation:  15.303835053159807
using explorer policy with actor:  0
printing an ep nov before normalisation:  13.018278817561606
printing an ep nov before normalisation:  8.602339751591366
printing an ep nov before normalisation:  19.939029216766357
printing an ep nov before normalisation:  37.54417318487265
printing an ep nov before normalisation:  37.10535211959624
Printing some Q and Qe and total Qs values:  [[0.902]
 [0.666]
 [0.666]
 [0.669]
 [0.666]
 [0.666]
 [0.666]] [[11.649]
 [ 8.201]
 [ 8.201]
 [ 9.199]
 [ 8.201]
 [ 8.201]
 [ 8.201]] [[0.902]
 [0.666]
 [0.666]
 [0.669]
 [0.666]
 [0.666]
 [0.666]]
printing an ep nov before normalisation:  76.60433326955764
printing an ep nov before normalisation:  58.713008219736686
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.393]
 [0.411]
 [0.402]
 [0.389]
 [0.398]
 [0.366]] [[79.743]
 [92.253]
 [80.255]
 [79.816]
 [80.64 ]
 [80.316]
 [81.644]] [[0.468]
 [0.393]
 [0.411]
 [0.402]
 [0.389]
 [0.398]
 [0.366]]
printing an ep nov before normalisation:  13.291089534759521
line 256 mcts: sample exp_bonus 7.099302767433429
maxi score, test score, baseline:  -0.9920875 -1.0 -0.9920875
printing an ep nov before normalisation:  14.1080904006958
printing an ep nov before normalisation:  75.1439326255638
Printing some Q and Qe and total Qs values:  [[0.362]
 [0.296]
 [0.313]
 [0.309]
 [0.306]
 [0.306]
 [0.306]] [[47.776]
 [61.419]
 [47.236]
 [46.938]
 [47.628]
 [47.628]
 [47.628]] [[0.362]
 [0.296]
 [0.313]
 [0.309]
 [0.306]
 [0.306]
 [0.306]]
maxi score, test score, baseline:  -0.9920875 -1.0 -0.9920875
probs:  [0.2874949064067441, 0.046967662812010744, 0.16553743078124522, 0.046967662812010744, 0.16553743078124522, 0.2874949064067441]
printing an ep nov before normalisation:  14.694555482964818
maxi score, test score, baseline:  -0.9920875 -1.0 -0.9920875
probs:  [0.2874949064067441, 0.046967662812010744, 0.16553743078124522, 0.046967662812010744, 0.16553743078124522, 0.2874949064067441]
printing an ep nov before normalisation:  19.076703216576036
printing an ep nov before normalisation:  54.141027110375894
maxi score, test score, baseline:  -0.9920875 -1.0 -0.9920875
probs:  [0.2874949064067441, 0.046967662812010744, 0.16553743078124522, 0.046967662812010744, 0.16553743078124522, 0.2874949064067441]
line 256 mcts: sample exp_bonus 7.444311227051976
printing an ep nov before normalisation:  22.869755080656553
Printing some Q and Qe and total Qs values:  [[0.551]
 [0.6  ]
 [0.583]
 [0.588]
 [0.597]
 [0.598]
 [0.588]] [[30.706]
 [42.508]
 [31.803]
 [37.574]
 [37.997]
 [38.241]
 [35.327]] [[0.551]
 [0.6  ]
 [0.583]
 [0.588]
 [0.597]
 [0.598]
 [0.588]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  15.911808017389255
Printing some Q and Qe and total Qs values:  [[0.842]
 [0.46 ]
 [0.696]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]] [[12.73 ]
 [25.013]
 [17.026]
 [25.013]
 [25.013]
 [25.013]
 [25.013]] [[0.842]
 [0.46 ]
 [0.696]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]]
printing an ep nov before normalisation:  22.80773928285441
Printing some Q and Qe and total Qs values:  [[0.886]
 [0.886]
 [0.886]
 [0.886]
 [0.886]
 [0.886]
 [0.886]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.886]
 [0.886]
 [0.886]
 [0.886]
 [0.886]
 [0.886]
 [0.886]]
printing an ep nov before normalisation:  20.850219560546808
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.32744495934895673, 0.053478748618334446, 0.1885325144714581, 0.053478748618334446, 0.1885325144714581, 0.1885325144714581]
printing an ep nov before normalisation:  45.98189079586941
printing an ep nov before normalisation:  50.11655097429649
Printing some Q and Qe and total Qs values:  [[0.208]
 [0.208]
 [0.208]
 [0.208]
 [0.208]
 [0.208]
 [0.208]] [[29.261]
 [29.261]
 [29.261]
 [29.261]
 [29.261]
 [29.261]
 [29.261]] [[0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]]
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.326]
 [0.362]
 [0.355]
 [0.376]
 [0.385]
 [0.379]] [[33.107]
 [33.908]
 [33.911]
 [34.361]
 [35.021]
 [34.882]
 [34.549]] [[0.364]
 [0.326]
 [0.362]
 [0.355]
 [0.376]
 [0.385]
 [0.379]]
printing an ep nov before normalisation:  13.194900730919198
line 256 mcts: sample exp_bonus 25.486166091103975
printing an ep nov before normalisation:  26.979281513484143
line 256 mcts: sample exp_bonus 14.503013913603274
printing an ep nov before normalisation:  26.613835241586965
using explorer policy with actor:  0
using explorer policy with actor:  1
printing an ep nov before normalisation:  15.99099933156842
using explorer policy with actor:  0
printing an ep nov before normalisation:  18.305180445164225
printing an ep nov before normalisation:  28.605208046614187
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.4487044104443663, 0.07324166404030365, 0.07324166404030365, 0.07324166404030365, 0.2583289333944191, 0.07324166404030365]
printing an ep nov before normalisation:  13.948175637222501
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.4487044104443663, 0.07324166404030365, 0.07324166404030365, 0.07324166404030365, 0.2583289333944191, 0.07324166404030365]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.4487044104443663, 0.07324166404030365, 0.07324166404030365, 0.07324166404030365, 0.2583289333944191, 0.07324166404030365]
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]] [[20.432]
 [20.276]
 [20.276]
 [20.276]
 [20.276]
 [20.276]
 [20.276]] [[0.552]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]]
printing an ep nov before normalisation:  18.92011214112755
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.434]] [[24.328]
 [22.497]
 [22.497]
 [22.497]
 [22.497]
 [22.497]
 [22.497]] [[0.5  ]
 [0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.434]]
Printing some Q and Qe and total Qs values:  [[0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]] [[16.131]
 [16.131]
 [16.131]
 [16.131]
 [16.131]
 [16.131]
 [16.131]] [[0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]]
printing an ep nov before normalisation:  12.84229020534683
printing an ep nov before normalisation:  21.623032107446292
printing an ep nov before normalisation:  12.523698642786135
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.4487044104443663, 0.07324166404030365, 0.07324166404030365, 0.07324166404030365, 0.2583289333944191, 0.07324166404030365]
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]] [[12.5  ]
 [13.017]
 [13.017]
 [13.017]
 [13.017]
 [13.017]
 [13.017]] [[0.454]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]]
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
probs:  [0.3166894255686854, 0.04962627634972298, 0.13617451915216483, 0.13617451915216483, 0.2251607406250971, 0.13617451915216483]
printing an ep nov before normalisation:  14.663670511892839
printing an ep nov before normalisation:  12.92917596385271
siam score:  -0.872079
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.401]] [[15.242]
 [12.735]
 [12.735]
 [12.735]
 [12.735]
 [12.735]
 [12.735]] [[0.581]
 [0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.401]]
siam score:  -0.8709164
printing an ep nov before normalisation:  58.70015101942582
printing an ep nov before normalisation:  12.877414676638502
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]] [[11.121]
 [10.739]
 [10.739]
 [10.739]
 [10.739]
 [10.739]
 [10.739]] [[0.479]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]]
printing an ep nov before normalisation:  18.322026548948543
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.2874107065663229, 0.04703547506101724, 0.16555381837265984, 0.04703547506101724, 0.2874107065663229, 0.16555381837265984]
printing an ep nov before normalisation:  10.645829694149993
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.2874107065663229, 0.04703547506101724, 0.16555381837265984, 0.04703547506101724, 0.2874107065663229, 0.16555381837265984]
printing an ep nov before normalisation:  11.008361723843645
printing an ep nov before normalisation:  64.07788728233037
printing an ep nov before normalisation:  15.080296724089095
Printing some Q and Qe and total Qs values:  [[0.657]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]] [[16.535]
 [15.667]
 [15.667]
 [15.667]
 [15.667]
 [15.667]
 [15.667]] [[0.657]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]]
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.2874107065663229, 0.04703547506101724, 0.16555381837265984, 0.04703547506101724, 0.2874107065663229, 0.16555381837265984]
printing an ep nov before normalisation:  12.866145201872067
printing an ep nov before normalisation:  24.211032711599
printing an ep nov before normalisation:  22.19217504190765
printing an ep nov before normalisation:  14.005151121555619
printing an ep nov before normalisation:  13.709765025164046
printing an ep nov before normalisation:  11.598889827728271
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.366]
 [0.368]
 [0.373]
 [0.378]
 [0.385]
 [0.366]] [[11.275]
 [15.467]
 [12.678]
 [14.302]
 [13.691]
 [12.346]
 [11.599]] [[0.484]
 [0.366]
 [0.368]
 [0.373]
 [0.378]
 [0.385]
 [0.366]]
printing an ep nov before normalisation:  20.79286955356274
printing an ep nov before normalisation:  21.86042766447308
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9930506849315068 -1.0 -0.9930506849315068
probs:  [0.2874107358605665, 0.04703544603676563, 0.16555381810266795, 0.04703544603676563, 0.2874107358605665, 0.16555381810266795]
maxi score, test score, baseline:  -0.9930506849315068 -1.0 -0.9930506849315068
probs:  [0.2874107358605665, 0.04703544603676563, 0.16555381810266795, 0.04703544603676563, 0.2874107358605665, 0.16555381810266795]
from probs:  [0.2874107358605665, 0.04703544603676563, 0.16555381810266795, 0.04703544603676563, 0.2874107358605665, 0.16555381810266795]
maxi score, test score, baseline:  -0.9930506849315068 -1.0 -0.9930506849315068
probs:  [0.3273115762428234, 0.053549836846768374, 0.18852958335454661, 0.053549836846768374, 0.18852958335454661, 0.18852958335454661]
printing an ep nov before normalisation:  40.22367477992119
printing an ep nov before normalisation:  1.1772600166295888
Printing some Q and Qe and total Qs values:  [[-0.073]
 [-0.063]
 [-0.07 ]
 [-0.071]
 [-0.071]
 [-0.074]
 [-0.075]] [[44.007]
 [48.799]
 [43.63 ]
 [43.412]
 [44.01 ]
 [43.905]
 [44.271]] [[0.801]
 [0.908]
 [0.796]
 [0.791]
 [0.804]
 [0.798]
 [0.805]]
using another actor
maxi score, test score, baseline:  -0.9930972789115646 -1.0 -0.9930972789115646
probs:  [0.31658125438027007, 0.13620301131397153, 0.04967020551865414, 0.13620301131397153, 0.13620301131397153, 0.22513950615916126]
printing an ep nov before normalisation:  0.1750566186967717
using another actor
from probs:  [0.31658125438027007, 0.13620301131397153, 0.04967020551865414, 0.13620301131397153, 0.13620301131397153, 0.22513950615916126]
maxi score, test score, baseline:  -0.9930972789115646 -1.0 -0.9930972789115646
probs:  [0.2568860299945926, 0.1480344068489424, 0.04212471946398767, 0.1480344068489424, 0.1480344068489424, 0.2568860299945926]
printing an ep nov before normalisation:  68.0389237373848
actions average: 
K:  1  action  0 :  tensor([0.3687, 0.0238, 0.1044, 0.1338, 0.1405, 0.1127, 0.1162],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0311, 0.8270, 0.0252, 0.0275, 0.0357, 0.0252, 0.0284],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1998, 0.0145, 0.1383, 0.1724, 0.1768, 0.1650, 0.1332],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1854, 0.0301, 0.1243, 0.2175, 0.1775, 0.1363, 0.1288],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2051, 0.0071, 0.1091, 0.1529, 0.2726, 0.1319, 0.1214],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1998, 0.0436, 0.1318, 0.1621, 0.1599, 0.1667, 0.1360],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2252, 0.0628, 0.1300, 0.1542, 0.1547, 0.1405, 0.1327],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  35.081124686262356
maxi score, test score, baseline:  -0.9930972789115646 -1.0 -0.9930972789115646
probs:  [0.1661163985434432, 0.1661163985434432, 0.047258483927266816, 0.1661163985434432, 0.1661163985434432, 0.28827592189896034]
maxi score, test score, baseline:  -0.9930972789115646 -1.0 -0.9930972789115646
probs:  [0.1661163985434432, 0.1661163985434432, 0.047258483927266816, 0.1661163985434432, 0.1661163985434432, 0.28827592189896034]
maxi score, test score, baseline:  -0.9931432432432432 -1.0 -0.9931432432432432
probs:  [0.16611639852752738, 0.16611639852752738, 0.0472584804734167, 0.16611639852752738, 0.16611639852752738, 0.2882759254164736]
siam score:  -0.8705253
printing an ep nov before normalisation:  29.958139941613574
printing an ep nov before normalisation:  34.78294120502596
printing an ep nov before normalisation:  41.78735272253492
printing an ep nov before normalisation:  50.39560172792151
printing an ep nov before normalisation:  60.76432228088379
printing an ep nov before normalisation:  27.867202758789062
printing an ep nov before normalisation:  57.50403807536595
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]] [[46.249]
 [46.249]
 [46.249]
 [46.249]
 [46.249]
 [46.249]
 [46.249]] [[0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]]
maxi score, test score, baseline:  -0.9931885906040269 -1.0 -0.9931885906040269
probs:  [0.16611639851182694, 0.16611639851182694, 0.04725847706639702, 0.16611639851182694, 0.16611639851182694, 0.28827592888629516]
printing an ep nov before normalisation:  35.00096563098965
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  51.95634365164506
printing an ep nov before normalisation:  17.542879031196094
maxi score, test score, baseline:  -0.9931885906040269 -1.0 -0.9931885906040269
probs:  [0.16611639851182694, 0.16611639851182694, 0.04725847706639702, 0.16611639851182694, 0.16611639851182694, 0.28827592888629516]
maxi score, test score, baseline:  -0.9931885906040269 -1.0 -0.9931885906040269
maxi score, test score, baseline:  -0.9931885906040269 -1.0 -0.9931885906040269
probs:  [0.18852632408508227, 0.05362100973143083, 0.05362100973143083, 0.18852632408508227, 0.18852632408508227, 0.32717900828189167]
printing an ep nov before normalisation:  67.6587417271783
maxi score, test score, baseline:  -0.9931885906040269 -1.0 -0.9931885906040269
probs:  [0.18852632408508227, 0.05362100973143083, 0.05362100973143083, 0.18852632408508227, 0.18852632408508227, 0.32717900828189167]
maxi score, test score, baseline:  -0.9931885906040269 -1.0 -0.9931885906040269
probs:  [0.18852632408508227, 0.05362100973143083, 0.05362100973143083, 0.18852632408508227, 0.18852632408508227, 0.32717900828189167]
maxi score, test score, baseline:  -0.9932333333333333 -1.0 -0.9932333333333333
probs:  [0.06196975653687083, 0.06196975653687083, 0.06196975653687083, 0.21793205024746337, 0.21793205024746337, 0.37822662989446076]
maxi score, test score, baseline:  -0.9932333333333333 -1.0 -0.9932333333333333
probs:  [0.06196975653687083, 0.06196975653687083, 0.06196975653687083, 0.21793205024746337, 0.21793205024746337, 0.37822662989446076]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9932333333333333 -1.0 -0.9932333333333333
probs:  [0.04566396670906454, 0.12511308488235076, 0.12511308488235076, 0.20673889122476832, 0.20673889122476832, 0.29063208107669725]
maxi score, test score, baseline:  -0.9932333333333333 -1.0 -0.9932333333333333
probs:  [0.04566396670906454, 0.12511308488235076, 0.12511308488235076, 0.20673889122476832, 0.20673889122476832, 0.29063208107669725]
Printing some Q and Qe and total Qs values:  [[0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]] [[85.158]
 [85.158]
 [85.158]
 [85.158]
 [85.158]
 [85.158]
 [85.158]] [[114.125]
 [114.125]
 [114.125]
 [114.125]
 [114.125]
 [114.125]
 [114.125]]
maxi score, test score, baseline:  -0.9932333333333333 -1.0 -0.9932333333333333
probs:  [0.04566396670906454, 0.12511308488235076, 0.12511308488235076, 0.20673889122476832, 0.20673889122476832, 0.29063208107669725]
line 256 mcts: sample exp_bonus 14.57416164738638
from probs:  [0.042277301381017415, 0.09814882970412854, 0.1555303993332694, 0.21448406676046916, 0.21448406676046916, 0.2750753360606462]
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.19537507102641
maxi score, test score, baseline:  -0.9932774834437086 -1.0 -0.9932774834437086
probs:  [0.04491974124824014, 0.10429442609778002, 0.16527383215946997, 0.16527383215946997, 0.2279239068803838, 0.2923142614546562]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9932774834437086 -1.0 -0.9932774834437086
maxi score, test score, baseline:  -0.9932774834437086 -1.0 -0.9932774834437086
probs:  [0.04491974124824014, 0.10429442609778002, 0.16527383215946997, 0.16527383215946997, 0.2279239068803838, 0.2923142614546562]
printing an ep nov before normalisation:  55.127525329589844
maxi score, test score, baseline:  -0.9932774834437086 -1.0 -0.9932774834437086
probs:  [0.039234038432232526, 0.1073893195505862, 0.17738663529376061, 0.17738663529376061, 0.24930168571483005, 0.24930168571483005]
maxi score, test score, baseline:  -0.9932774834437086 -1.0 -0.9932774834437086
probs:  [0.039234038432232526, 0.1073893195505862, 0.17738663529376061, 0.17738663529376061, 0.24930168571483005, 0.24930168571483005]
line 256 mcts: sample exp_bonus 55.26622614096333
maxi score, test score, baseline:  -0.9932774834437086 -1.0 -0.9932774834437086
probs:  [0.039234038432232526, 0.1073893195505862, 0.17738663529376061, 0.17738663529376061, 0.24930168571483005, 0.24930168571483005]
maxi score, test score, baseline:  -0.9932774834437086 -1.0 -0.9932774834437086
probs:  [0.039234038432232526, 0.1073893195505862, 0.17738663529376061, 0.17738663529376061, 0.24930168571483005, 0.24930168571483005]
printing an ep nov before normalisation:  40.14796733856201
actions average: 
K:  1  action  0 :  tensor([0.4726, 0.0027, 0.1148, 0.1188, 0.1036, 0.1043, 0.0832],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0329, 0.8253, 0.0281, 0.0320, 0.0265, 0.0229, 0.0322],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1692, 0.0035, 0.1908, 0.1614, 0.1400, 0.2082, 0.1270],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2140, 0.0156, 0.1337, 0.2276, 0.1554, 0.1283, 0.1255],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1870, 0.0044, 0.1654, 0.1928, 0.1584, 0.1474, 0.1448],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1890, 0.0103, 0.1460, 0.1734, 0.1835, 0.1469, 0.1510],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2196, 0.0235, 0.1310, 0.1833, 0.1558, 0.1089, 0.1778],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9932774834437086 -1.0 -0.9932774834437086
probs:  [0.0400168689070602, 0.14711523424360326, 0.14711523424360326, 0.20283532972275092, 0.26008200316023145, 0.20283532972275092]
maxi score, test score, baseline:  -0.9932774834437086 -1.0 -0.9932774834437086
maxi score, test score, baseline:  -0.9932774834437086 -1.0 -0.9932774834437086
probs:  [0.0400168689070602, 0.14711523424360326, 0.14711523424360326, 0.20283532972275092, 0.26008200316023145, 0.20283532972275092]
maxi score, test score, baseline:  -0.9932774834437086 -1.0 -0.9932774834437086
probs:  [0.04230677357873807, 0.0981854143663245, 0.15555415224158015, 0.21447339654589656, 0.2750068667215642, 0.21447339654589656]
maxi score, test score, baseline:  -0.9932774834437086 -1.0 -0.9932774834437086
probs:  [0.04230677357873807, 0.0981854143663245, 0.15555415224158015, 0.21447339654589656, 0.2750068667215642, 0.21447339654589656]
maxi score, test score, baseline:  -0.9932774834437086 -1.0 -0.9932774834437086
probs:  [0.04230677357873807, 0.0981854143663245, 0.15555415224158015, 0.21447339654589656, 0.2750068667215642, 0.21447339654589656]
maxi score, test score, baseline:  -0.9932774834437086 -1.0 -0.9932774834437086
maxi score, test score, baseline:  -0.9932774834437086 -1.0 -0.9932774834437086
probs:  [0.03664848553349153, 0.10022752503149007, 0.16550200558276962, 0.23254066128408293, 0.23254066128408293, 0.23254066128408293]
printing an ep nov before normalisation:  49.16962583387267
maxi score, test score, baseline:  -0.9932774834437086 -1.0 -0.9932774834437086
probs:  [0.03927456759205833, 0.10742604558557532, 0.17739489632558608, 0.17739489632558608, 0.24925479708559709, 0.24925479708559709]
maxi score, test score, baseline:  -0.9932774834437086 -1.0 -0.9932774834437086
probs:  [0.03927456759205833, 0.10742604558557532, 0.17739489632558608, 0.17739489632558608, 0.24925479708559709, 0.24925479708559709]
printing an ep nov before normalisation:  44.99895148070823
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.516]
 [0.532]
 [0.526]
 [0.522]
 [0.52 ]
 [0.513]] [[47.988]
 [49.375]
 [53.732]
 [47.961]
 [48.515]
 [49.334]
 [49.064]] [[1.536]
 [1.63 ]
 [1.832]
 [1.579]
 [1.599]
 [1.632]
 [1.613]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9932774834437086 -1.0 -0.9932774834437086
probs:  [0.03927456759205833, 0.10742604558557532, 0.17739489632558608, 0.17739489632558608, 0.24925479708559709, 0.24925479708559709]
maxi score, test score, baseline:  -0.9932774834437086 -1.0 -0.9932774834437086
printing an ep nov before normalisation:  29.728772553601814
maxi score, test score, baseline:  -0.9932774834437086 -1.0 -0.9932774834437086
maxi score, test score, baseline:  -0.9932774834437086 -1.0 -0.9932774834437086
probs:  [0.04574943291149027, 0.12517471072527, 0.12517471072527, 0.20671799594741733, 0.29046515374313503, 0.20671799594741733]
printing an ep nov before normalisation:  73.05399437235648
maxi score, test score, baseline:  -0.9932774834437086 -1.0 -0.9932774834437086
probs:  [0.04574943291149027, 0.12517471072527, 0.12517471072527, 0.20671799594741733, 0.29046515374313503, 0.20671799594741733]
maxi score, test score, baseline:  -0.9932774834437086 -1.0 -0.9932774834437086
probs:  [0.04574943291149027, 0.12517471072527, 0.12517471072527, 0.20671799594741733, 0.29046515374313503, 0.20671799594741733]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.0457494305128693, 0.12517470990219898, 0.12517470990219898, 0.2067179967419108, 0.2904651561989111, 0.2067179967419108]
printing an ep nov before normalisation:  53.47630212043715
printing an ep nov before normalisation:  58.640217781066895
printing an ep nov before normalisation:  47.141158125641894
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.0457494305128693, 0.12517470990219898, 0.12517470990219898, 0.2067179967419108, 0.2904651561989111, 0.2067179967419108]
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.049688522475501214, 0.049688522475501214, 0.13597240591070647, 0.22455719290418563, 0.31553616332991985, 0.22455719290418563]
printing an ep nov before normalisation:  61.852994874555385
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.049688522475501214, 0.049688522475501214, 0.13597240591070647, 0.22455719290418563, 0.31553616332991985, 0.22455719290418563]
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.49 ]
 [0.461]
 [0.461]
 [0.48 ]
 [0.451]
 [0.503]] [[23.696]
 [27.182]
 [23.4  ]
 [26.3  ]
 [25.382]
 [27.592]
 [24.22 ]] [[1.594]
 [1.932]
 [1.547]
 [1.82 ]
 [1.752]
 [1.932]
 [1.666]]
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.64 ]
 [0.562]
 [0.569]
 [0.716]
 [0.716]
 [0.716]] [[52.533]
 [67.362]
 [56.938]
 [58.032]
 [54.699]
 [54.699]
 [54.699]] [[1.191]
 [1.65 ]
 [1.306]
 [1.341]
 [1.403]
 [1.403]
 [1.403]]
printing an ep nov before normalisation:  63.95930093745979
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.05450926582632675, 0.05450926582632675, 0.14918685272511273, 0.2463891752745333, 0.3462185876225877, 0.14918685272511273]
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.06036900767928918, 0.06036900767928918, 0.16524936454683523, 0.16524936454683523, 0.38351389100091593, 0.16524936454683523]
printing an ep nov before normalisation:  33.75059494341014
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.06036900767928918, 0.06036900767928918, 0.16524936454683523, 0.16524936454683523, 0.38351389100091593, 0.16524936454683523]
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.06036900767928918, 0.06036900767928918, 0.16524936454683523, 0.16524936454683523, 0.38351389100091593, 0.16524936454683523]
siam score:  -0.86344236
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.06743308787809961, 0.06743308787809961, 0.18461316495821628, 0.18461316495821628, 0.42847440644926854, 0.06743308787809961]
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.06743308787809961, 0.06743308787809961, 0.18461316495821628, 0.18461316495821628, 0.42847440644926854, 0.06743308787809961]
actions average: 
K:  0  action  0 :  tensor([0.3133, 0.0063, 0.1331, 0.1467, 0.1561, 0.1180, 0.1266],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0094, 0.9394, 0.0077, 0.0155, 0.0101, 0.0108, 0.0071],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1965, 0.0195, 0.1602, 0.1813, 0.1573, 0.1518, 0.1334],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1732, 0.0165, 0.1293, 0.2602, 0.1698, 0.1331, 0.1178],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1998, 0.0030, 0.1321, 0.1610, 0.2447, 0.1433, 0.1161],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.2039, 0.0516, 0.1416, 0.1616, 0.1654, 0.1486, 0.1274],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2077, 0.0032, 0.1666, 0.2163, 0.1387, 0.1355, 0.1321],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.08805491181531368, 0.08805491181531368, 0.08805491181531368, 0.08805491181531368, 0.5597254409234315, 0.08805491181531368]
Printing some Q and Qe and total Qs values:  [[-0.008]
 [ 0.039]
 [-0.009]
 [-0.004]
 [-0.011]
 [-0.003]
 [-0.002]] [[15.331]
 [14.381]
 [15.954]
 [14.702]
 [14.782]
 [14.478]
 [14.718]] [[0.801]
 [0.798]
 [0.832]
 [0.772]
 [0.769]
 [0.761]
 [0.774]]
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.08805491181531368, 0.08805491181531368, 0.08805491181531368, 0.08805491181531368, 0.5597254409234315, 0.08805491181531368]
Printing some Q and Qe and total Qs values:  [[0.54 ]
 [0.64 ]
 [0.54 ]
 [0.662]
 [0.623]
 [0.54 ]
 [0.48 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.54 ]
 [0.64 ]
 [0.54 ]
 [0.662]
 [0.623]
 [0.54 ]
 [0.48 ]]
printing an ep nov before normalisation:  25.929236897133514
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.09629706310676597, 0.09629706310676597, 0.09629706310676597, 0.09629706310676597, 0.5185146844661701, 0.09629706310676597]
printing an ep nov before normalisation:  56.17916970010562
Printing some Q and Qe and total Qs values:  [[0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.592]] [[59.666]
 [59.666]
 [59.666]
 [59.666]
 [59.666]
 [59.666]
 [60.19 ]] [[2.213]
 [2.213]
 [2.213]
 [2.213]
 [2.213]
 [2.213]
 [2.179]]
actions average: 
K:  0  action  0 :  tensor([0.3486, 0.0060, 0.1289, 0.1266, 0.1697, 0.1095, 0.1107],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0070, 0.9397, 0.0090, 0.0086, 0.0032, 0.0087, 0.0237],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2837, 0.0203, 0.1360, 0.1451, 0.1438, 0.1292, 0.1418],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1946, 0.0126, 0.1542, 0.1900, 0.1603, 0.1565, 0.1318],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2075, 0.0042, 0.1441, 0.1605, 0.2019, 0.1498, 0.1319],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.2068, 0.0065, 0.1486, 0.1626, 0.1730, 0.1553, 0.1472],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1847, 0.0112, 0.1428, 0.1697, 0.1654, 0.1432, 0.1831],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.19280421821229374, 0.19280421821229374, 0.19280421821229374, 0.19280421821229374, 0.03597890893853116, 0.19280421821229374]
printing an ep nov before normalisation:  55.06790832384398
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.19359059853014163, 0.19359059853014163, 0.19359059853014163, 0.19359059853014163, 0.032047007349291785, 0.19359059853014163]
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.19359059853014163, 0.19359059853014163, 0.19359059853014163, 0.19359059853014163, 0.032047007349291785, 0.19359059853014163]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.19359059853014163, 0.19359059853014163, 0.19359059853014163, 0.19359059853014163, 0.032047007349291785, 0.19359059853014163]
printing an ep nov before normalisation:  48.16454217734571
Printing some Q and Qe and total Qs values:  [[0.45]
 [0.45]
 [0.45]
 [0.45]
 [0.45]
 [0.45]
 [0.45]] [[65.198]
 [65.198]
 [65.198]
 [65.198]
 [65.198]
 [65.198]
 [65.198]] [[1.365]
 [1.365]
 [1.365]
 [1.365]
 [1.365]
 [1.365]
 [1.365]]
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.1938633470329166, 0.1938633470329166, 0.1938633470329166, 0.1938633470329166, 0.030683264835417015, 0.1938633470329166]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  67.51019119027556
printing an ep nov before normalisation:  47.79523381723142
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.17741779475955086, 0.17741779475955086, 0.10753357134889302, 0.24911719280425237, 0.03939645352350049, 0.24911719280425237]
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.17741779475955086, 0.17741779475955086, 0.10753357134889302, 0.24911719280425237, 0.03939645352350049, 0.24911719280425237]
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.657]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]] [[44.704]
 [57.174]
 [44.704]
 [44.704]
 [44.704]
 [44.704]
 [44.704]] [[1.193]
 [1.728]
 [1.193]
 [1.193]
 [1.193]
 [1.193]
 [1.193]]
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.17741779475955086, 0.17741779475955086, 0.10753357134889302, 0.24911719280425237, 0.03939645352350049, 0.24911719280425237]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
printing an ep nov before normalisation:  47.53329042186807
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.17741779475955086, 0.17741779475955086, 0.10753357134889302, 0.24911719280425237, 0.03939645352350049, 0.24911719280425237]
Printing some Q and Qe and total Qs values:  [[0.038]
 [0.038]
 [0.038]
 [0.035]
 [0.035]
 [0.034]
 [0.034]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.038]
 [0.038]
 [0.038]
 [0.035]
 [0.035]
 [0.034]
 [0.034]]
printing an ep nov before normalisation:  28.264853702366963
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.1822720877337292, 0.1822720877337292, 0.13230750293122992, 0.23353445395967012, 0.036079413681971345, 0.23353445395967012]
printing an ep nov before normalisation:  42.834418829856965
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.19212174699845153, 0.19212174699845153, 0.13945519001409512, 0.19212174699845153, 0.0380233024886676, 0.24615626650188277]
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.19212174699845153, 0.19212174699845153, 0.13945519001409512, 0.19212174699845153, 0.0380233024886676, 0.24615626650188277]
printing an ep nov before normalisation:  40.159598769850724
printing an ep nov before normalisation:  15.779299803420717
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.19212174699845153, 0.19212174699845153, 0.13945519001409512, 0.19212174699845153, 0.0380233024886676, 0.24615626650188277]
using explorer policy with actor:  1
printing an ep nov before normalisation:  88.2346288909876
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.21442996530787037, 0.21442996530787037, 0.09832726306298259, 0.15564378695602896, 0.042425961982111064, 0.27474305738313665]
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.22747057607120108, 0.22747057607120108, 0.10430266994710848, 0.10430266994710848, 0.04499960403550803, 0.29145390392787296]
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
probs:  [0.2677843458234141, 0.2677843458234141, 0.11564420264823744, 0.11564420264823744, 0.04239154111944821, 0.19075136193724881]
from probs:  [0.2677843458234141, 0.2677843458234141, 0.11564420264823744, 0.11564420264823744, 0.04239154111944821, 0.19075136193724881]
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
probs:  [0.2895381138833804, 0.2895381138833804, 0.12503253992381305, 0.12503253992381305, 0.04582615246180012, 0.12503253992381305]
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
probs:  [0.2895381138833804, 0.2895381138833804, 0.12503253992381305, 0.12503253992381305, 0.04582615246180012, 0.12503253992381305]
printing an ep nov before normalisation:  41.590210347384684
actions average: 
K:  0  action  0 :  tensor([0.2694, 0.0200, 0.1433, 0.1362, 0.1940, 0.1142, 0.1228],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0087, 0.9131, 0.0083, 0.0182, 0.0061, 0.0061, 0.0395],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1664, 0.0049, 0.2933, 0.1336, 0.1348, 0.1529, 0.1140],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2175, 0.0081, 0.1482, 0.1969, 0.1682, 0.1250, 0.1361],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2014, 0.0096, 0.1438, 0.1750, 0.2054, 0.1263, 0.1385],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1799, 0.0133, 0.1707, 0.1901, 0.1569, 0.1564, 0.1327],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1845, 0.0618, 0.1338, 0.1995, 0.1498, 0.1043, 0.1663],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
probs:  [0.3158570392777356, 0.22498819685648164, 0.13639107549575796, 0.13639107549575796, 0.049981537378508946, 0.13639107549575796]
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
probs:  [0.3158570392777356, 0.22498819685648164, 0.13639107549575796, 0.13639107549575796, 0.049981537378508946, 0.13639107549575796]
Printing some Q and Qe and total Qs values:  [[0.343]
 [0.295]
 [0.36 ]
 [0.36 ]
 [0.336]
 [0.331]
 [0.338]] [[58.83 ]
 [74.152]
 [70.892]
 [83.258]
 [79.14 ]
 [80.662]
 [80.964]] [[0.343]
 [0.295]
 [0.36 ]
 [0.36 ]
 [0.336]
 [0.331]
 [0.338]]
printing an ep nov before normalisation:  1.1655990642753977
printing an ep nov before normalisation:  56.08198335120264
maxi score, test score, baseline:  -0.9934064935064936 -1.0 -0.9934064935064936
probs:  [0.3158570427497662, 0.22498819821376795, 0.13639107479116958, 0.13639107479116958, 0.04998153466295698, 0.13639107479116958]
maxi score, test score, baseline:  -0.9934064935064936 -1.0 -0.9934064935064936
probs:  [0.3158570427497662, 0.22498819821376795, 0.13639107479116958, 0.13639107479116958, 0.04998153466295698, 0.13639107479116958]
maxi score, test score, baseline:  -0.9934483870967742 -1.0 -0.9934483870967742
probs:  [0.3158570461765587, 0.22498819955337096, 0.13639107409576165, 0.13639107409576165, 0.049981531982785356, 0.13639107409576165]
printing an ep nov before normalisation:  38.28205247935319
maxi score, test score, baseline:  -0.9934483870967742 -1.0 -0.9934483870967742
probs:  [0.3158570461765587, 0.22498819955337096, 0.13639107409576165, 0.13639107409576165, 0.049981531982785356, 0.13639107409576165]
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
printing an ep nov before normalisation:  57.545067260868706
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.3818835757733372, 0.27201155410696687, 0.06040617904581282, 0.16488633298225758, 0.06040617904581282, 0.06040617904581282]
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.3818835757733372, 0.27201155410696687, 0.06040617904581282, 0.16488633298225758, 0.06040617904581282, 0.06040617904581282]
printing an ep nov before normalisation:  56.49609965558794
printing an ep nov before normalisation:  57.39705731922874
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.3818835757733372, 0.27201155410696687, 0.06040617904581282, 0.16488633298225758, 0.06040617904581282, 0.06040617904581282]
printing an ep nov before normalisation:  52.16134372102534
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.42772621386369536, 0.1846707733699105, 0.06764407979882793, 0.1846707733699105, 0.06764407979882793, 0.06764407979882793]
Printing some Q and Qe and total Qs values:  [[0.02 ]
 [0.023]
 [0.021]
 [0.025]
 [0.015]
 [0.019]
 [0.019]] [[28.734]
 [43.513]
 [28.237]
 [30.25 ]
 [33.75 ]
 [29.145]
 [28.759]] [[0.02 ]
 [0.023]
 [0.021]
 [0.025]
 [0.015]
 [0.019]
 [0.019]]
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.42772621386369536, 0.1846707733699105, 0.06764407979882793, 0.1846707733699105, 0.06764407979882793, 0.06764407979882793]
printing an ep nov before normalisation:  45.59222391774048
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.48444948675954014, 0.07659987808419466, 0.07659987808419466, 0.20915100090368127, 0.07659987808419466, 0.07659987808419466]
using explorer policy with actor:  1
printing an ep nov before normalisation:  19.418804420422617
printing an ep nov before normalisation:  33.60452265094892
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.48444948675954014, 0.07659987808419466, 0.07659987808419466, 0.20915100090368127, 0.07659987808419466, 0.07659987808419466]
actions average: 
K:  0  action  0 :  tensor([0.2693, 0.0034, 0.1183, 0.1617, 0.2009, 0.1289, 0.1175],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0306, 0.8643, 0.0216, 0.0173, 0.0257, 0.0189, 0.0216],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1855, 0.0286, 0.1723, 0.1690, 0.1615, 0.1571, 0.1260],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2073, 0.0070, 0.1435, 0.1890, 0.1657, 0.1490, 0.1384],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1879, 0.0085, 0.1258, 0.1529, 0.2466, 0.1483, 0.1299],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1592, 0.0019, 0.1331, 0.1583, 0.1819, 0.2375, 0.1281],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1677, 0.0474, 0.1288, 0.1686, 0.1480, 0.1211, 0.2184],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  19.915246501768582
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.897307973623526
printing an ep nov before normalisation:  70.02609734360732
Printing some Q and Qe and total Qs values:  [[0.609]
 [0.607]
 [0.597]
 [0.605]
 [0.608]
 [0.612]
 [0.608]] [[44.568]
 [42.165]
 [49.865]
 [51.546]
 [55.28 ]
 [55.469]
 [57.966]] [[0.609]
 [0.607]
 [0.597]
 [0.605]
 [0.608]
 [0.612]
 [0.608]]
siam score:  -0.87157196
printing an ep nov before normalisation:  18.581436077753704
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
printing an ep nov before normalisation:  69.68945909192202
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.4844494976693929, 0.0765998749920973, 0.0765998749920973, 0.2091510023622181, 0.0765998749920973, 0.0765998749920973]
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.2830793118021741, 0.04268608666575387, 0.15843097284254834, 0.19894168300442702, 0.15843097284254834, 0.15843097284254834]
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.2830793118021741, 0.04268608666575387, 0.15843097284254834, 0.19894168300442702, 0.15843097284254834, 0.15843097284254834]
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.2830793118021741, 0.04268608666575387, 0.15843097284254834, 0.19894168300442702, 0.15843097284254834, 0.15843097284254834]
using explorer policy with actor:  1
printing an ep nov before normalisation:  23.123977184295654
printing an ep nov before normalisation:  9.454633150894551
printing an ep nov before normalisation:  26.260629232921982
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.2551174601711503, 0.038895511359828126, 0.16542539251608304, 0.20971085092077246, 0.16542539251608304, 0.16542539251608304]
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.2551174601711503, 0.038895511359828126, 0.16542539251608304, 0.20971085092077246, 0.16542539251608304, 0.16542539251608304]
printing an ep nov before normalisation:  59.6454695589866
printing an ep nov before normalisation:  47.38699579344207
siam score:  -0.8634787
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.2551174601711503, 0.038895511359828126, 0.16542539251608304, 0.20971085092077246, 0.16542539251608304, 0.16542539251608304]
maxi score, test score, baseline:  -0.9935708860759493 -1.0 -0.9935708860759493
probs:  [0.26664123828142944, 0.040647335520534594, 0.17289561935839157, 0.21918251870164127, 0.17289561935839157, 0.1277376687796116]
maxi score, test score, baseline:  -0.9935708860759493 -1.0 -0.9935708860759493
probs:  [0.26664123828142944, 0.040647335520534594, 0.17289561935839157, 0.21918251870164127, 0.17289561935839157, 0.1277376687796116]
maxi score, test score, baseline:  -0.9935708860759493 -1.0 -0.9935708860759493
actions average: 
K:  1  action  0 :  tensor([0.3404, 0.0061, 0.1295, 0.1350, 0.1321, 0.1335, 0.1233],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0269, 0.8468, 0.0241, 0.0357, 0.0193, 0.0180, 0.0292],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1776, 0.0042, 0.2149, 0.1451, 0.1529, 0.1671, 0.1383],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2604, 0.0056, 0.1302, 0.1833, 0.1438, 0.1436, 0.1331],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1917, 0.0052, 0.1217, 0.1630, 0.2246, 0.1535, 0.1404],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1555, 0.0288, 0.1365, 0.1661, 0.1704, 0.1787, 0.1639],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1861, 0.0293, 0.1580, 0.1411, 0.1615, 0.1765, 0.1474],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
siam score:  -0.8668025
maxi score, test score, baseline:  -0.9935708860759493 -1.0 -0.9935708860759493
maxi score, test score, baseline:  -0.9935708860759493 -1.0 -0.9935708860759493
probs:  [0.26664123828142944, 0.040647335520534594, 0.17289561935839157, 0.21918251870164127, 0.17289561935839157, 0.1277376687796116]
using explorer policy with actor:  1
siam score:  -0.8670907
using explorer policy with actor:  1
printing an ep nov before normalisation:  63.67395764800981
maxi score, test score, baseline:  -0.9935708860759493 -1.0 -0.9935708860759493
probs:  [0.26664123828142944, 0.040647335520534594, 0.17289561935839157, 0.21918251870164127, 0.17289561935839157, 0.1277376687796116]
Printing some Q and Qe and total Qs values:  [[0.076]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.205]] [[23.449]
 [13.438]
 [13.438]
 [13.438]
 [13.438]
 [13.438]
 [19.692]] [[1.243]
 [0.984]
 [0.984]
 [0.984]
 [0.984]
 [0.984]
 [1.177]]
maxi score, test score, baseline:  -0.9935708860759493 -1.0 -0.9935708860759493
probs:  [0.26664123828142944, 0.040647335520534594, 0.17289561935839157, 0.21918251870164127, 0.17289561935839157, 0.1277376687796116]
line 256 mcts: sample exp_bonus 12.704631334565251
maxi score, test score, baseline:  -0.9935708860759493 -1.0 -0.9935708860759493
printing an ep nov before normalisation:  13.455927258194151
printing an ep nov before normalisation:  30.461088461163826
printing an ep nov before normalisation:  27.64057599677688
maxi score, test score, baseline:  -0.9935708860759493 -1.0 -0.9935708860759493
probs:  [0.30729540678803446, 0.04682751522815814, 0.19924946658541917, 0.2525971495604604, 0.04682751522815814, 0.14720294660976965]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.9935708860759493 -1.0 -0.9935708860759493
probs:  [0.30729540678803446, 0.04682751522815814, 0.19924946658541917, 0.2525971495604604, 0.04682751522815814, 0.14720294660976965]
printing an ep nov before normalisation:  52.68352284384581
maxi score, test score, baseline:  -0.9935708860759493 -1.0 -0.9935708860759493
probs:  [0.30729540678803446, 0.04682751522815814, 0.19924946658541917, 0.2525971495604604, 0.04682751522815814, 0.14720294660976965]
printing an ep nov before normalisation:  26.40951156616211
maxi score, test score, baseline:  -0.9935708860759493 -1.0 -0.9935708860759493
probs:  [0.30729540678803446, 0.04682751522815814, 0.19924946658541917, 0.2525971495604604, 0.04682751522815814, 0.14720294660976965]
printing an ep nov before normalisation:  72.41923280771852
printing an ep nov before normalisation:  61.815080642700195
maxi score, test score, baseline:  -0.9936106918238994 -1.0 -0.9936106918238994
probs:  [0.2785476036325113, 0.04248985346403037, 0.22900461902924946, 0.22900461902924946, 0.08745323444850292, 0.13350007039645653]
maxi score, test score, baseline:  -0.9936106918238994 -1.0 -0.9936106918238994
probs:  [0.2785476036325113, 0.04248985346403037, 0.22900461902924946, 0.22900461902924946, 0.08745323444850292, 0.13350007039645653]
maxi score, test score, baseline:  -0.9936106918238994 -1.0 -0.9936106918238994
probs:  [0.2785476036325113, 0.04248985346403037, 0.22900461902924946, 0.22900461902924946, 0.08745323444850292, 0.13350007039645653]
maxi score, test score, baseline:  -0.9936106918238994 -1.0 -0.9936106918238994
probs:  [0.27334728043090883, 0.04236974540835676, 0.27334728043090883, 0.2134902972695772, 0.04236974540835676, 0.1550756510518915]
maxi score, test score, baseline:  -0.9936106918238994 -1.0 -0.9936106918238994
probs:  [0.29031150226405095, 0.044992266696332404, 0.29031150226405095, 0.1646962310396167, 0.044992266696332404, 0.1646962310396167]
printing an ep nov before normalisation:  68.59710967707323
printing an ep nov before normalisation:  58.94578504767316
Printing some Q and Qe and total Qs values:  [[0.548]
 [0.596]
 [0.569]
 [0.561]
 [0.562]
 [0.554]
 [0.562]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.548]
 [0.596]
 [0.569]
 [0.561]
 [0.562]
 [0.554]
 [0.562]]
maxi score, test score, baseline:  -0.9936106918238994 -1.0 -0.9936106918238994
maxi score, test score, baseline:  -0.9936106918238994 -1.0 -0.9936106918238994
probs:  [0.30903494919937036, 0.04788674889864765, 0.30903494919937036, 0.11084211861400034, 0.04788674889864765, 0.17531448518996365]
maxi score, test score, baseline:  -0.9936106918238994 -1.0 -0.9936106918238994
probs:  [0.30903494919937036, 0.04788674889864765, 0.30903494919937036, 0.11084211861400034, 0.04788674889864765, 0.17531448518996365]
maxi score, test score, baseline:  -0.9936106918238994 -1.0 -0.9936106918238994
probs:  [0.30903494919937036, 0.04788674889864765, 0.30903494919937036, 0.11084211861400034, 0.04788674889864765, 0.17531448518996365]
maxi score, test score, baseline:  -0.9936106918238994 -1.0 -0.9936106918238994
probs:  [0.30903494919937036, 0.04788674889864765, 0.30903494919937036, 0.11084211861400034, 0.04788674889864765, 0.17531448518996365]
Printing some Q and Qe and total Qs values:  [[ 0.004]
 [-0.01 ]
 [-0.01 ]
 [-0.006]
 [-0.009]
 [-0.013]
 [-0.014]] [[23.121]
 [25.81 ]
 [21.58 ]
 [21.787]
 [25.585]
 [25.541]
 [26.221]] [[1.141]
 [1.397]
 [0.971]
 [0.996]
 [1.376]
 [1.368]
 [1.435]]
Printing some Q and Qe and total Qs values:  [[-0.029]
 [-0.022]
 [-0.029]
 [-0.029]
 [-0.029]
 [-0.029]
 [-0.029]] [[34.126]
 [38.91 ]
 [34.126]
 [34.126]
 [34.126]
 [34.126]
 [34.126]] [[1.581]
 [1.978]
 [1.581]
 [1.581]
 [1.581]
 [1.581]
 [1.581]]
maxi score, test score, baseline:  -0.9936106918238994 -1.0 -0.9936106918238994
probs:  [0.3541873588276224, 0.05486691908376928, 0.3541873588276224, 0.1270245250934473, 0.05486691908376928, 0.05486691908376928]
printing an ep nov before normalisation:  44.43967342376709
Printing some Q and Qe and total Qs values:  [[0.248]
 [0.252]
 [0.257]
 [0.257]
 [0.254]
 [0.266]
 [0.253]] [[26.062]
 [36.371]
 [28.544]
 [29.271]
 [28.964]
 [26.641]
 [26.823]] [[0.248]
 [0.252]
 [0.257]
 [0.257]
 [0.254]
 [0.266]
 [0.253]]
maxi score, test score, baseline:  -0.9936106918238994 -1.0 -0.9936106918238994
probs:  [0.38174562489057784, 0.059127187554711046, 0.38174562489057784, 0.059127187554711046, 0.059127187554711046, 0.059127187554711046]
maxi score, test score, baseline:  -0.9936106918238994 -1.0 -0.9936106918238994
probs:  [0.38174562489057784, 0.059127187554711046, 0.38174562489057784, 0.059127187554711046, 0.059127187554711046, 0.059127187554711046]
printing an ep nov before normalisation:  36.577403652174056
printing an ep nov before normalisation:  18.357253074645996
maxi score, test score, baseline:  -0.9936106918238994 -1.0 -0.9936106918238994
maxi score, test score, baseline:  -0.9936106918238994 -1.0 -0.9936106918238994
probs:  [0.38174562489057784, 0.059127187554711046, 0.38174562489057784, 0.059127187554711046, 0.059127187554711046, 0.059127187554711046]
maxi score, test score, baseline:  -0.9936106918238994 -1.0 -0.9936106918238994
probs:  [0.38174562489057784, 0.059127187554711046, 0.38174562489057784, 0.059127187554711046, 0.059127187554711046, 0.059127187554711046]
printing an ep nov before normalisation:  19.783834102082164
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
probs:  [0.38174562948478596, 0.05912718525760704, 0.38174562948478596, 0.05912718525760704, 0.05912718525760704, 0.05912718525760704]
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
probs:  [0.38174562948478596, 0.05912718525760704, 0.38174562948478596, 0.05912718525760704, 0.05912718525760704, 0.05912718525760704]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
probs:  [0.32535203734675106, 0.06451407742390648, 0.4165916529576228, 0.06451407742390648, 0.06451407742390648, 0.06451407742390648]
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
printing an ep nov before normalisation:  51.01875336563053
printing an ep nov before normalisation:  14.631238469198422
printing an ep nov before normalisation:  60.43504563242195
siam score:  -0.86757195
printing an ep nov before normalisation:  28.699429035186768
Printing some Q and Qe and total Qs values:  [[0.426]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]] [[29.978]
 [30.67 ]
 [30.67 ]
 [30.67 ]
 [30.67 ]
 [30.67 ]
 [30.67 ]] [[1.229]
 [1.215]
 [1.215]
 [1.215]
 [1.215]
 [1.215]
 [1.215]]
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
probs:  [0.30354189063344844, 0.06766584793062319, 0.4257947176440588, 0.06766584793062319, 0.06766584793062319, 0.06766584793062319]
printing an ep nov before normalisation:  53.602671287022844
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
probs:  [0.30354189063344844, 0.06766584793062319, 0.4257947176440588, 0.06766584793062319, 0.06766584793062319, 0.06766584793062319]
printing an ep nov before normalisation:  23.100683858033022
actions average: 
K:  0  action  0 :  tensor([0.3232, 0.0031, 0.1160, 0.1439, 0.1826, 0.1205, 0.1107],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0282, 0.8162, 0.0245, 0.0441, 0.0191, 0.0157, 0.0522],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2075, 0.0018, 0.2994, 0.0865, 0.1025, 0.2200, 0.0823],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1906, 0.0089, 0.1395, 0.1820, 0.1932, 0.1396, 0.1462],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2053, 0.0048, 0.1297, 0.1729, 0.2156, 0.1322, 0.1395],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1763, 0.0039, 0.1407, 0.1904, 0.2037, 0.1477, 0.1373],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1962, 0.0177, 0.1222, 0.1787, 0.1928, 0.1359, 0.1565],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
probs:  [0.259274834654998, 0.11911040469641043, 0.3319210331091433, 0.05147291814662731, 0.11911040469641043, 0.11911040469641043]
using explorer policy with actor:  1
siam score:  -0.86118317
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
probs:  [0.259274834654998, 0.11911040469641043, 0.3319210331091433, 0.05147291814662731, 0.11911040469641043, 0.11911040469641043]
printing an ep nov before normalisation:  98.7140148443759
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
probs:  [0.38215073034508357, 0.1654065259434011, 0.1654065259434011, 0.060814845912356606, 0.1654065259434011, 0.060814845912356606]
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
probs:  [0.42681365494435164, 0.1847291717138995, 0.06790933387594986, 0.06790933387594986, 0.1847291717138995, 0.06790933387594986]
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.466]
 [0.533]
 [0.535]
 [0.533]
 [0.533]
 [0.533]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.533]
 [0.466]
 [0.533]
 [0.535]
 [0.533]
 [0.533]
 [0.533]]
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
probs:  [0.37605888973821533, 0.21763310219434567, 0.06289163529103109, 0.06289163529103109, 0.21763310219434567, 0.06289163529103109]
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
probs:  [0.23117466600351685, 0.23117466600351685, 0.13385334908491675, 0.13385334908491675, 0.23117466600351685, 0.038769303819616026]
line 256 mcts: sample exp_bonus 74.63682397834788
printing an ep nov before normalisation:  61.385710808520265
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
probs:  [0.23117466774668297, 0.23117466774668297, 0.1338533481982186, 0.1338533481982186, 0.23117466774668297, 0.038769300363514]
Printing some Q and Qe and total Qs values:  [[0.25 ]
 [0.346]
 [0.296]
 [0.223]
 [0.305]
 [0.193]
 [0.295]] [[48.83 ]
 [53.381]
 [52.439]
 [53.277]
 [55.422]
 [54.14 ]
 [56.187]] [[0.686]
 [0.869]
 [0.801]
 [0.743]
 [0.866]
 [0.729]
 [0.87 ]]
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
probs:  [0.23117466774668297, 0.23117466774668297, 0.1338533481982186, 0.1338533481982186, 0.23117466774668297, 0.038769300363514]
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.219]
 [0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]] [[47.72 ]
 [54.496]
 [47.72 ]
 [47.72 ]
 [47.72 ]
 [47.72 ]
 [47.72 ]] [[1.719]
 [1.886]
 [1.719]
 [1.719]
 [1.719]
 [1.719]
 [1.719]]
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
printing an ep nov before normalisation:  65.16405582427979
printing an ep nov before normalisation:  42.93277554486084
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.495]
 [0.476]
 [0.472]
 [0.469]
 [0.47 ]
 [0.473]] [[24.676]
 [47.746]
 [26.642]
 [28.639]
 [29.486]
 [28.887]
 [27.089]] [[0.817]
 [1.46 ]
 [0.871]
 [0.921]
 [0.94 ]
 [0.925]
 [0.88 ]]
Printing some Q and Qe and total Qs values:  [[0.398]
 [0.398]
 [0.398]
 [0.418]
 [0.398]
 [0.398]
 [0.398]] [[35.278]
 [35.278]
 [35.278]
 [36.405]
 [35.278]
 [35.278]
 [35.278]] [[1.651]
 [1.651]
 [1.651]
 [1.735]
 [1.651]
 [1.651]
 [1.651]]
printing an ep nov before normalisation:  27.795400485013317
Printing some Q and Qe and total Qs values:  [[0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]] [[16.909]
 [16.909]
 [16.909]
 [16.909]
 [16.909]
 [16.909]
 [16.909]] [[0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]]
actions average: 
K:  0  action  0 :  tensor([0.4079, 0.0011, 0.1171, 0.1374, 0.1302, 0.1114, 0.0949],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0023, 0.9642, 0.0047, 0.0116, 0.0014, 0.0033, 0.0125],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1795, 0.0228, 0.1819, 0.1631, 0.1633, 0.1585, 0.1310],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2468, 0.0249, 0.1324, 0.2279, 0.1435, 0.1198, 0.1047],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2152, 0.0147, 0.1372, 0.1494, 0.2644, 0.1199, 0.0991],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.2093, 0.0034, 0.1454, 0.1576, 0.1543, 0.2167, 0.1134],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1855, 0.0310, 0.1440, 0.1895, 0.1667, 0.1417, 0.1416],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
probs:  [0.5443220229522248, 0.09113559540955503, 0.09113559540955503, 0.09113559540955503, 0.09113559540955503, 0.09113559540955503]
siam score:  -0.86543673
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
probs:  [0.19260570508804667, 0.19260570508804667, 0.19260570508804667, 0.19260570508804667, 0.03697147455976665, 0.19260570508804667]
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.507]
 [0.549]
 [0.582]
 [0.556]
 [0.551]
 [0.551]] [[49.475]
 [54.425]
 [65.428]
 [49.475]
 [70.814]
 [72.426]
 [71.075]] [[1.305]
 [1.371]
 [1.726]
 [1.305]
 [1.887]
 [1.928]
 [1.889]]
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.0437679600552675, 0.22811601997236625, 0.22811601997236625, 0.22811601997236625, 0.0437679600552675, 0.22811601997236625]
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.043767953629423935, 0.228116023185288, 0.228116023185288, 0.228116023185288, 0.043767953629423935, 0.228116023185288]
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.0536396506238005, 0.2796936827095328, 0.2796936827095328, 0.2796936827095328, 0.0536396506238005, 0.0536396506238005]
printing an ep nov before normalisation:  34.412805441030194
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.0536396506238005, 0.2796936827095328, 0.2796936827095328, 0.2796936827095328, 0.0536396506238005, 0.0536396506238005]
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
Printing some Q and Qe and total Qs values:  [[-0.033]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.033]] [[39.495]
 [39.495]
 [39.495]
 [39.495]
 [39.495]
 [39.495]
 [39.495]] [[0.27]
 [0.27]
 [0.27]
 [0.27]
 [0.27]
 [0.27]
 [0.27]]
printing an ep nov before normalisation:  97.86779433420838
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
using explorer policy with actor:  1
printing an ep nov before normalisation:  57.94021814636308
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.16621468876376788, 0.16621468876376788, 0.16621468876376788, 0.28689278883773855, 0.04824845610718989, 0.16621468876376788]
actions average: 
K:  2  action  0 :  tensor([0.3171, 0.0033, 0.1157, 0.1586, 0.1584, 0.1184, 0.1286],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0142, 0.8894, 0.0134, 0.0298, 0.0136, 0.0128, 0.0269],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2106, 0.0460, 0.2635, 0.1506, 0.1091, 0.1100, 0.1101],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1889, 0.0672, 0.1205, 0.2394, 0.1269, 0.1287, 0.1285],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1989, 0.0567, 0.1042, 0.1328, 0.2666, 0.1090, 0.1316],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1578, 0.0104, 0.1349, 0.1801, 0.1645, 0.1975, 0.1547],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1954, 0.0438, 0.1330, 0.1629, 0.1684, 0.1481, 0.1484],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.16621468876376788, 0.16621468876376788, 0.16621468876376788, 0.28689278883773855, 0.04824845610718989, 0.16621468876376788]
using another actor
from probs:  [0.16621468876376788, 0.16621468876376788, 0.16621468876376788, 0.28689278883773855, 0.04824845610718989, 0.16621468876376788]
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.054689389752589565, 0.18844714386963377, 0.18844714386963377, 0.3252797888859197, 0.054689389752589565, 0.18844714386963377]
printing an ep nov before normalisation:  57.63242737577945
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.054689389752589565, 0.18844714386963377, 0.18844714386963377, 0.3252797888859197, 0.054689389752589565, 0.18844714386963377]
line 256 mcts: sample exp_bonus 52.508424564434755
Printing some Q and Qe and total Qs values:  [[0.151]
 [0.151]
 [0.151]
 [0.151]
 [0.151]
 [0.151]
 [0.151]] [[43.359]
 [43.359]
 [43.359]
 [43.359]
 [43.359]
 [43.359]
 [43.359]] [[0.944]
 [0.944]
 [0.944]
 [0.944]
 [0.944]
 [0.944]
 [0.944]]
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.054689389752589565, 0.18844714386963377, 0.18844714386963377, 0.3252797888859197, 0.054689389752589565, 0.18844714386963377]
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
probs:  [0.05468938546033248, 0.18844714470451296, 0.18844714470451296, 0.3252797949657963, 0.05468938546033248, 0.18844714470451296]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
probs:  [0.07463596994692658, 0.07463596994692658, 0.07463596994692658, 0.44415846323987557, 0.07463596994692658, 0.2572976569724181]
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
probs:  [0.13660900887230906, 0.13660900887230906, 0.0503889159260366, 0.3149954080714931, 0.13660900887230906, 0.22478864938554313]
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
probs:  [0.14833483560811012, 0.14833483560811012, 0.0431260660546616, 0.25593471356050396, 0.14833483560811012, 0.25593471356050396]
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
probs:  [0.14833483560811012, 0.14833483560811012, 0.0431260660546616, 0.25593471356050396, 0.14833483560811012, 0.25593471356050396]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
probs:  [0.18844025918873672, 0.054760528355560306, 0.054760528355560306, 0.3251581657226692, 0.18844025918873672, 0.18844025918873672]
printing an ep nov before normalisation:  0.0017688565333173756
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
probs:  [0.36087765069884475, 0.06956117465057765, 0.06956117465057765, 0.36087765069884475, 0.06956117465057765, 0.06956117465057765]
line 256 mcts: sample exp_bonus 60.91818547425317
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
probs:  [0.36087765069884475, 0.06956117465057765, 0.06956117465057765, 0.36087765069884475, 0.06956117465057765, 0.06956117465057765]
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
probs:  [0.36087765069884475, 0.06956117465057765, 0.06956117465057765, 0.36087765069884475, 0.06956117465057765, 0.06956117465057765]
printing an ep nov before normalisation:  49.593354675115464
Printing some Q and Qe and total Qs values:  [[0.57 ]
 [0.57 ]
 [0.57 ]
 [0.544]
 [0.57 ]
 [0.57 ]
 [0.57 ]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [27.229]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.373]
 [0.373]
 [0.373]
 [0.808]
 [0.373]
 [0.373]
 [0.373]]
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
probs:  [0.2867182272590988, 0.16622530063507673, 0.16622530063507673, 0.16622530063507673, 0.04838057020059408, 0.16622530063507673]
UNIT TEST: sample policy line 217 mcts : [0.122 0.143 0.163 0.122 0.143 0.122 0.184]
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
probs:  [0.32503706279919964, 0.18843321400241725, 0.18843321400241725, 0.05483164759677421, 0.05483164759677421, 0.18843321400241725]
printing an ep nov before normalisation:  21.4572395850294
printing an ep nov before normalisation:  43.70370805343372
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
printing an ep nov before normalisation:  38.9344782042808
printing an ep nov before normalisation:  50.908429503217135
printing an ep nov before normalisation:  49.36368109979579
printing an ep nov before normalisation:  37.42289859365034
printing an ep nov before normalisation:  45.71124745522964
maxi score, test score, baseline:  -0.9938393939393939 -1.0 -0.9938393939393939
probs:  [0.09827039756707569, 0.09827039756707569, 0.5086480121646217, 0.09827039756707569, 0.09827039756707569, 0.09827039756707569]
maxi score, test score, baseline:  -0.9938393939393939 -1.0 -0.9938393939393939
probs:  [0.09827039756707569, 0.09827039756707569, 0.5086480121646217, 0.09827039756707569, 0.09827039756707569, 0.09827039756707569]
maxi score, test score, baseline:  -0.9938393939393939 -1.0 -0.9938393939393939
probs:  [0.09827039756707569, 0.09827039756707569, 0.5086480121646217, 0.09827039756707569, 0.09827039756707569, 0.09827039756707569]
maxi score, test score, baseline:  -0.9938393939393939 -1.0 -0.9938393939393939
probs:  [0.09827039756707569, 0.09827039756707569, 0.5086480121646217, 0.09827039756707569, 0.09827039756707569, 0.09827039756707569]
Printing some Q and Qe and total Qs values:  [[0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]] [[53.642]
 [53.642]
 [53.642]
 [53.642]
 [53.642]
 [53.642]
 [53.642]] [[2.252]
 [2.252]
 [2.252]
 [2.252]
 [2.252]
 [2.252]
 [2.252]]
printing an ep nov before normalisation:  66.22232798026593
printing an ep nov before normalisation:  43.47963333129883
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.16623043029527734, 0.16623043029527734, 0.286631668798857, 0.16623043029527734, 0.16623043029527734, 0.04844661002003377]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.16623043029527734, 0.16623043029527734, 0.286631668798857, 0.16623043029527734, 0.16623043029527734, 0.04844661002003377]
printing an ep nov before normalisation:  34.689435020368755
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.18842601670107545, 0.18842601670107545, 0.32491648509872856, 0.18842601670107545, 0.054902732399022575, 0.054902732399022575]
printing an ep nov before normalisation:  60.49433508329636
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.18842601670107545, 0.18842601670107545, 0.32491648509872856, 0.18842601670107545, 0.054902732399022575, 0.054902732399022575]
maxi score, test score, baseline:  -0.9939119760479042 -1.0 -0.9939119760479042
probs:  [0.22789539065877587, 0.22789539065877587, 0.22789539065877587, 0.22789539065877587, 0.044209218682448245, 0.044209218682448245]
maxi score, test score, baseline:  -0.9939119760479042 -1.0 -0.9939119760479042
probs:  [0.22789539065877587, 0.22789539065877587, 0.22789539065877587, 0.22789539065877587, 0.044209218682448245, 0.044209218682448245]
printing an ep nov before normalisation:  37.57310930538669
printing an ep nov before normalisation:  77.22537678611548
Printing some Q and Qe and total Qs values:  [[-0.023]
 [-0.027]
 [-0.018]
 [-0.017]
 [-0.016]
 [-0.02 ]
 [-0.017]] [[24.273]
 [25.909]
 [24.214]
 [20.626]
 [23.341]
 [23.592]
 [22.297]] [[1.033]
 [1.153]
 [1.032]
 [0.761]
 [0.968]
 [0.984]
 [0.888]]
maxi score, test score, baseline:  -0.9939119760479042 -1.0 -0.9939119760479042
printing an ep nov before normalisation:  31.278158285399446
Printing some Q and Qe and total Qs values:  [[0.618]
 [0.618]
 [0.594]
 [0.618]
 [0.618]
 [0.618]
 [0.618]] [[54.875]
 [54.875]
 [33.373]
 [54.875]
 [54.875]
 [54.875]
 [54.875]] [[3.627]
 [3.627]
 [1.927]
 [3.627]
 [3.627]
 [3.627]
 [3.627]]
printing an ep nov before normalisation:  39.419027195305496
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.5  ]
 [0.49 ]
 [0.572]
 [0.506]
 [0.476]
 [0.47 ]] [[62.361]
 [61.611]
 [59.038]
 [61.5  ]
 [63.008]
 [65.891]
 [66.489]] [[1.687]
 [1.678]
 [1.586]
 [1.747]
 [1.729]
 [1.79 ]
 [1.803]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.488]
 [0.464]
 [0.464]
 [0.464]
 [0.464]
 [0.464]] [[62.104]
 [70.999]
 [62.104]
 [62.104]
 [62.104]
 [62.104]
 [62.104]] [[1.489]
 [1.755]
 [1.489]
 [1.489]
 [1.489]
 [1.489]
 [1.489]]
maxi score, test score, baseline:  -0.9939119760479042 -1.0 -0.9939119760479042
probs:  [0.13399889124570402, 0.23095962893132724, 0.23095962893132724, 0.23095962893132724, 0.03912333071461035, 0.13399889124570402]
printing an ep nov before normalisation:  67.79206275939941
printing an ep nov before normalisation:  69.80558032979306
maxi score, test score, baseline:  -0.9939119760479042 -1.0 -0.9939119760479042
probs:  [0.1483849913884232, 0.25576577881221, 0.1483849913884232, 0.25576577881221, 0.04331346821031036, 0.1483849913884232]
maxi score, test score, baseline:  -0.9939119760479042 -1.0 -0.9939119760479042
maxi score, test score, baseline:  -0.9939119760479042 -1.0 -0.9939119760479042
probs:  [0.1483849913884232, 0.25576577881221, 0.1483849913884232, 0.25576577881221, 0.04331346821031036, 0.1483849913884232]
maxi score, test score, baseline:  -0.9939119760479042 -1.0 -0.9939119760479042
probs:  [0.16623544760643416, 0.28654556541130355, 0.16623544760643416, 0.16623544760643416, 0.04851264416295983, 0.16623544760643416]
printing an ep nov before normalisation:  35.92915820470182
printing an ep nov before normalisation:  77.00065479360678
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
probs:  [0.16623544759129807, 0.28654556961907895, 0.16623544759129807, 0.16623544759129807, 0.04851264001572875, 0.16623544759129807]
Printing some Q and Qe and total Qs values:  [[0.56]
 [0.56]
 [0.56]
 [0.56]
 [0.56]
 [0.56]
 [0.56]] [[52.333]
 [52.333]
 [52.333]
 [52.333]
 [52.333]
 [52.333]
 [52.333]] [[1.817]
 [1.817]
 [1.817]
 [1.817]
 [1.817]
 [1.817]
 [1.817]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.5748],
        [-0.4834],
        [-0.0000],
        [-0.4527],
        [-0.5492],
        [-0.0000],
        [-0.1426],
        [-0.5984],
        [-0.5165]], dtype=torch.float64)
-0.6215761596000005 -0.6215761596000005
-0.032346567066 -0.6071124878470235
-0.032346567066 -0.515728610646756
-0.548922 -0.548922
-0.070771701198 -0.5234409097514825
-0.032346567066 -0.5815274068616427
-0.793138606986 -0.793138606986
-0.08397170119799999 -0.2265557254193486
-0.032346567066 -0.6307117074869257
-0.032346567066 -0.5488713893447718
printing an ep nov before normalisation:  39.78381731284515
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
probs:  [0.18841867112094104, 0.32479640691290995, 0.18841867112094104, 0.18841867112094104, 0.0549737898621335, 0.0549737898621335]
printing an ep nov before normalisation:  56.828622731825334
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.318]
 [0.321]
 [0.321]
 [0.322]
 [0.32 ]
 [0.319]] [[33.89 ]
 [46.015]
 [36.828]
 [38.988]
 [42.664]
 [42.888]
 [40.807]] [[0.315]
 [0.318]
 [0.321]
 [0.321]
 [0.322]
 [0.32 ]
 [0.319]]
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
printing an ep nov before normalisation:  33.38929102007245
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.519]
 [0.587]
 [0.584]
 [0.453]
 [0.57 ]
 [0.573]] [[55.726]
 [55.513]
 [55.085]
 [70.421]
 [73.686]
 [65.637]
 [59.705]] [[1.453]
 [1.422]
 [1.477]
 [1.97 ]
 [1.945]
 [1.801]
 [1.612]]
printing an ep nov before normalisation:  45.58288097381592
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
probs:  [0.25703906535977905, 0.4431195025212376, 0.07496035802974585, 0.07496035802974585, 0.07496035802974585, 0.07496035802974585]
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
probs:  [0.36005154510973564, 0.36005154510973564, 0.06997422744513213, 0.06997422744513213, 0.06997422744513213, 0.06997422744513213]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
probs:  [0.36005154510973564, 0.36005154510973564, 0.06997422744513213, 0.06997422744513213, 0.06997422744513213, 0.06997422744513213]
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
printing an ep nov before normalisation:  85.85242565535039
printing an ep nov before normalisation:  52.4640648935978
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.19248785857767625, 0.19248785857767625, 0.19248785857767625, 0.19248785857767625, 0.19248785857767625, 0.03756070711161883]
printing an ep nov before normalisation:  57.970360865868415
printing an ep nov before normalisation:  74.22125890021884
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.22778560936915423, 0.22778560936915423, 0.22778560936915423, 0.04442878126169162, 0.22778560936915423, 0.04442878126169162]
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.22778560936915423, 0.22778560936915423, 0.22778560936915423, 0.04442878126169162, 0.22778560936915423, 0.04442878126169162]
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.22778560936915423, 0.22778560936915423, 0.22778560936915423, 0.04442878126169162, 0.22778560936915423, 0.04442878126169162]
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.22778560936915423, 0.22778560936915423, 0.22778560936915423, 0.04442878126169162, 0.22778560936915423, 0.04442878126169162]
using another actor
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.23088914016696457, 0.23088914016696457, 0.23088914016696457, 0.039241124007345286, 0.13404572774588044, 0.13404572774588044]
siam score:  -0.8583597
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.2556544874386703, 0.2556544874386703, 0.14841757868593883, 0.04343828906484283, 0.14841757868593883, 0.14841757868593883]
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.2556544874386703, 0.2556544874386703, 0.14841757868593883, 0.04343828906484283, 0.14841757868593883, 0.14841757868593883]
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.387]
 [0.483]
 [0.502]
 [0.503]
 [0.499]
 [0.452]] [[36.935]
 [52.992]
 [48.985]
 [49.563]
 [50.144]
 [49.78 ]
 [48.784]] [[0.507]
 [0.387]
 [0.483]
 [0.502]
 [0.503]
 [0.499]
 [0.452]]
printing an ep nov before normalisation:  34.470831453978214
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.2556544874386703, 0.2556544874386703, 0.14841757868593883, 0.04343828906484283, 0.14841757868593883, 0.14841757868593883]
printing an ep nov before normalisation:  34.97904300689697
printing an ep nov before normalisation:  31.09016036152371
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.2556544874386703, 0.2556544874386703, 0.14841757868593883, 0.04343828906484283, 0.14841757868593883, 0.14841757868593883]
siam score:  -0.86182475
printing an ep nov before normalisation:  42.75646769791077
maxi score, test score, baseline:  -0.9940860465116279 -1.0 -0.9940860465116279
probs:  [0.18840355614408005, 0.32455769902457615, 0.18840355614408005, 0.055115816271591796, 0.18840355614408005, 0.055115816271591796]
printing an ep nov before normalisation:  25.78641578202533
printing an ep nov before normalisation:  22.129030728915197
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
probs:  [0.18840355699373237, 0.324557705196237, 0.18840355699373237, 0.055115811911282994, 0.18840355699373237, 0.055115811911282994]
printing an ep nov before normalisation:  33.134748357018196
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
probs:  [0.18840355699373237, 0.324557705196237, 0.18840355699373237, 0.055115811911282994, 0.18840355699373237, 0.055115811911282994]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.87068037227483
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.07024776967486684, 0.35950446065026637, 0.07024776967486684, 0.07024776967486684, 0.35950446065026637, 0.07024776967486684]
printing an ep nov before normalisation:  25.26075663293125
actions average: 
K:  1  action  0 :  tensor([0.3445, 0.0147, 0.1182, 0.1454, 0.1380, 0.1135, 0.1256],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0065, 0.9647, 0.0056, 0.0053, 0.0063, 0.0055, 0.0060],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2307, 0.0368, 0.1589, 0.1839, 0.1341, 0.1286, 0.1271],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2362, 0.0241, 0.1370, 0.1768, 0.1560, 0.1360, 0.1339],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2487, 0.0014, 0.1100, 0.1565, 0.2670, 0.1032, 0.1132],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1377, 0.1059, 0.1149, 0.1927, 0.1396, 0.1899, 0.1193],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1856, 0.0175, 0.1369, 0.1858, 0.1669, 0.1468, 0.1605],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.55 ]
 [0.518]
 [0.515]
 [0.514]
 [0.517]
 [0.518]] [[64.655]
 [65.041]
 [69.819]
 [72.76 ]
 [73.761]
 [73.106]
 [71.013]] [[0.753]
 [0.796]
 [0.804]
 [0.825]
 [0.832]
 [0.83 ]
 [0.813]]
printing an ep nov before normalisation:  31.339130401611328
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.07024776967486684, 0.35950446065026637, 0.07024776967486684, 0.07024776967486684, 0.35950446065026637, 0.07024776967486684]
printing an ep nov before normalisation:  31.885056495666504
printing an ep nov before normalisation:  38.164871193424204
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.04350066092010852, 0.2555992206649349, 0.14843363258334052, 0.14843363258334052, 0.2555992206649349, 0.14843363258334052]
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.04350066092010852, 0.2555992206649349, 0.14843363258334052, 0.14843363258334052, 0.2555992206649349, 0.14843363258334052]
printing an ep nov before normalisation:  31.997799363322954
printing an ep nov before normalisation:  35.36703712049389
siam score:  -0.8613328
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.04858937641816918, 0.28557548712818137, 0.04858937641816918, 0.16583513645364942, 0.28557548712818137, 0.16583513645364942]
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.583]
 [0.582]
 [0.572]
 [0.569]
 [0.583]
 [0.583]] [[60.117]
 [59.855]
 [64.717]
 [65.274]
 [66.269]
 [59.855]
 [59.855]] [[0.967]
 [0.954]
 [1.019]
 [1.016]
 [1.027]
 [0.954]
 [0.954]]
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.304]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]] [[48.061]
 [53.812]
 [48.061]
 [48.061]
 [48.061]
 [48.061]
 [48.061]] [[1.437]
 [1.637]
 [1.437]
 [1.437]
 [1.437]
 [1.437]
 [1.437]]
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.05518678215345047, 0.18839579669890413, 0.05518678215345047, 0.18839579669890413, 0.3244390455963867, 0.18839579669890413]
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.05518678215345047, 0.18839579669890413, 0.05518678215345047, 0.18839579669890413, 0.3244390455963867, 0.18839579669890413]
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.05518678215345047, 0.18839579669890413, 0.05518678215345047, 0.18839579669890413, 0.3244390455963867, 0.18839579669890413]
printing an ep nov before normalisation:  20.978768973531885
actions average: 
K:  0  action  0 :  tensor([0.2983, 0.0025, 0.1494, 0.1555, 0.1389, 0.1274, 0.1279],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0430, 0.7906, 0.0187, 0.0600, 0.0193, 0.0148, 0.0536],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2075, 0.0050, 0.2120, 0.1618, 0.1301, 0.1579, 0.1257],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1978, 0.0030, 0.1483, 0.1891, 0.1621, 0.1545, 0.1453],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1905, 0.0049, 0.1598, 0.1837, 0.1648, 0.1512, 0.1451],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1724, 0.0055, 0.1610, 0.1921, 0.1516, 0.1778, 0.1396],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1756, 0.0435, 0.1538, 0.1806, 0.1367, 0.1479, 0.1619],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.05518678215345047, 0.18839579669890413, 0.05518678215345047, 0.18839579669890413, 0.3244390455963867, 0.18839579669890413]
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.05518678215345047, 0.18839579669890413, 0.05518678215345047, 0.18839579669890413, 0.3244390455963867, 0.18839579669890413]
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.05518678215345047, 0.18839579669890413, 0.05518678215345047, 0.18839579669890413, 0.3244390455963867, 0.18839579669890413]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.05518678215345047, 0.18839579669890413, 0.05518678215345047, 0.18839579669890413, 0.3244390455963867, 0.18839579669890413]
printing an ep nov before normalisation:  43.434170013783024
Printing some Q and Qe and total Qs values:  [[0.05 ]
 [0.063]
 [0.051]
 [0.07 ]
 [0.062]
 [0.066]
 [0.054]] [[11.974]
 [14.265]
 [14.714]
 [19.232]
 [19.924]
 [17.728]
 [14.147]] [[0.193]
 [0.354]
 [0.372]
 [0.685]
 [0.722]
 [0.583]
 [0.338]]
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.05518678215345047, 0.18839579669890413, 0.05518678215345047, 0.18839579669890413, 0.3244390455963867, 0.18839579669890413]
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.06365525881192018, 0.06365525881192018, 0.06365525881192018, 0.2173548197380512, 0.3743245840881372, 0.2173548197380512]
siam score:  -0.85855216
printing an ep nov before normalisation:  79.9165790395441
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.07520262248099761, 0.07520262248099761, 0.07520262248099761, 0.07520262248099761, 0.44234702519811775, 0.25684248487789185]
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.13676431012539136, 0.13676431012539136, 0.13676431012539136, 0.05070951028214759, 0.3143667693763421, 0.2246307899653362]
printing an ep nov before normalisation:  54.02077787566528
siam score:  -0.857807
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.14964041383368576, 0.14964041383368576, 0.055475815692572594, 0.055475815692572594, 0.34398011638023884, 0.24578742456724442]
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.375]
 [0.45 ]
 [0.449]
 [0.456]
 [0.456]
 [0.455]] [[42.667]
 [42.618]
 [46.16 ]
 [46.894]
 [45.433]
 [44.976]
 [44.626]] [[0.711]
 [0.631]
 [0.748]
 [0.756]
 [0.746]
 [0.739]
 [0.734]]
printing an ep nov before normalisation:  39.71916198730469
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.16555826041929095, 0.16555826041929095, 0.06136807316595812, 0.06136807316595812, 0.38058907241021106, 0.16555826041929095]
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.16555826041929095, 0.16555826041929095, 0.06136807316595812, 0.06136807316595812, 0.38058907241021106, 0.16555826041929095]
printing an ep nov before normalisation:  28.228149127266416
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.16555826041929095, 0.16555826041929095, 0.06136807316595812, 0.06136807316595812, 0.38058907241021106, 0.16555826041929095]
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.282]
 [0.282]
 [0.195]
 [0.282]
 [0.282]
 [0.282]] [[22.593]
 [22.593]
 [22.593]
 [20.018]
 [22.593]
 [22.593]
 [22.593]] [[0.573]
 [0.573]
 [0.573]
 [0.434]
 [0.573]
 [0.573]
 [0.573]]
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.16555826041929095, 0.16555826041929095, 0.06136807316595812, 0.06136807316595812, 0.38058907241021106, 0.16555826041929095]
printing an ep nov before normalisation:  12.475818750186818
printing an ep nov before normalisation:  34.299724972556746
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
printing an ep nov before normalisation:  43.897107144017795
printing an ep nov before normalisation:  75.8044685025301
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.1883879080473298, 0.1883879080473298, 0.05525771894004065, 0.05525771894004065, 0.3243208379779294, 0.1883879080473298]
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.562]
 [0.493]
 [0.588]
 [0.493]
 [0.495]
 [0.493]] [[15.12 ]
 [12.637]
 [14.7  ]
 [24.979]
 [17.021]
 [16.679]
 [14.819]] [[1.395]
 [1.286]
 [1.339]
 [2.046]
 [1.477]
 [1.459]
 [1.347]]
actions average: 
K:  4  action  0 :  tensor([0.3208, 0.0330, 0.1243, 0.1476, 0.1480, 0.1041, 0.1222],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0178, 0.8949, 0.0131, 0.0220, 0.0199, 0.0118, 0.0205],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1983, 0.0527, 0.1896, 0.1481, 0.1341, 0.1495, 0.1278],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1556, 0.0788, 0.1251, 0.2182, 0.1584, 0.1302, 0.1337],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1746, 0.0105, 0.0931, 0.1371, 0.3934, 0.0984, 0.0930],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1457, 0.0076, 0.1744, 0.1882, 0.1457, 0.2248, 0.1136],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1647, 0.0054, 0.1564, 0.1920, 0.1833, 0.1523, 0.1458],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  29.056801795959473
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.2276215867569049, 0.2276215867569049, 0.0447568264861902, 0.0447568264861902, 0.2276215867569049, 0.2276215867569049]
using explorer policy with actor:  1
printing an ep nov before normalisation:  22.321931589408166
printing an ep nov before normalisation:  17.467253244462327
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.2276215867569049, 0.2276215867569049, 0.0447568264861902, 0.0447568264861902, 0.2276215867569049, 0.2276215867569049]
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.07051990746574936, 0.3589601850685013, 0.07051990746574936, 0.07051990746574936, 0.3589601850685013, 0.07051990746574936]
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.043625349964529506, 0.2554893923332973, 0.14846528845629195, 0.14846528845629195, 0.2554893923332973, 0.14846528845629195]
from probs:  [0.043625349964529506, 0.2554893923332973, 0.14846528845629195, 0.14846528845629195, 0.2554893923332973, 0.14846528845629195]
printing an ep nov before normalisation:  82.22911887316432
printing an ep nov before normalisation:  27.625616081047898
printing an ep nov before normalisation:  36.24616652920078
maxi score, test score, baseline:  -0.9942181818181818 -1.0 -0.9942181818181818
probs:  [0.04362534607522246, 0.255489395140963, 0.1484652878809505, 0.1484652878809505, 0.255489395140963, 0.1484652878809505]
printing an ep nov before normalisation:  53.09619397593547
maxi score, test score, baseline:  -0.9942181818181818 -1.0 -0.9942181818181818
probs:  [0.04362534607522246, 0.255489395140963, 0.1484652878809505, 0.1484652878809505, 0.255489395140963, 0.1484652878809505]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9942502824858758 -1.0 -0.9942502824858758
probs:  [0.055328616490806476, 0.18837989636901223, 0.18837989636901223, 0.18837989636901223, 0.32420307791135033, 0.055328616490806476]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9942502824858758 -1.0 -0.9942502824858758
probs:  [0.055328616490806476, 0.18837989636901223, 0.18837989636901223, 0.18837989636901223, 0.32420307791135033, 0.055328616490806476]
maxi score, test score, baseline:  -0.9942820224719101 -1.0 -0.9942820224719101
printing an ep nov before normalisation:  42.35915660858154
maxi score, test score, baseline:  -0.9942820224719101 -1.0 -0.9942820224719101
probs:  [0.0553286121253414, 0.18837989722036919, 0.18837989722036919, 0.18837989722036919, 0.3242030840882096, 0.0553286121253414]
line 256 mcts: sample exp_bonus 42.33376093233172
maxi score, test score, baseline:  -0.9942820224719101 -1.0 -0.9942820224719101
probs:  [0.0553286121253414, 0.18837989722036919, 0.18837989722036919, 0.18837989722036919, 0.3242030840882096, 0.0553286121253414]
maxi score, test score, baseline:  -0.9942820224719101 -1.0 -0.9942820224719101
probs:  [0.0553286121253414, 0.18837989722036919, 0.18837989722036919, 0.18837989722036919, 0.3242030840882096, 0.0553286121253414]
printing an ep nov before normalisation:  25.438793005165383
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.06380730339677515, 0.06380730339677515, 0.21729692319847912, 0.21729692319847912, 0.37398424341271636, 0.06380730339677515]
printing an ep nov before normalisation:  1.0845342330867425e-05
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.07536362652363018, 0.07536362652363018, 0.25671035453186886, 0.07536362652363018, 0.4418351393736104, 0.07536362652363018]
Printing some Q and Qe and total Qs values:  [[0.376]
 [0.119]
 [0.124]
 [0.158]
 [0.375]
 [0.439]
 [0.106]] [[20.901]
 [25.347]
 [28.493]
 [29.733]
 [24.362]
 [22.506]
 [29.2  ]] [[0.939]
 [0.83 ]
 [0.939]
 [1.014]
 [1.053]
 [1.055]
 [0.944]]
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.682]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]] [[46.615]
 [66.44 ]
 [46.615]
 [46.615]
 [46.615]
 [46.615]
 [46.615]] [[1.148]
 [1.55 ]
 [1.148]
 [1.148]
 [1.148]
 [1.148]
 [1.148]]
from probs:  [0.050801543896218194, 0.13680680680880666, 0.2245853741113445, 0.13680680680880666, 0.3141926615660174, 0.13680680680880666]
printing an ep nov before normalisation:  54.47148720328684
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.05568138214271764, 0.1499697654550989, 0.1499697654550989, 0.1499697654550989, 0.34443955603688675, 0.1499697654550989]
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.05568138214271764, 0.1499697654550989, 0.1499697654550989, 0.1499697654550989, 0.34443955603688675, 0.1499697654550989]
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.04890852616653584, 0.16626338536358415, 0.16626338536358415, 0.16626338536358415, 0.28603793237912756, 0.16626338536358415]
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.04890852616653584, 0.16626338536358415, 0.16626338536358415, 0.16626338536358415, 0.28603793237912756, 0.16626338536358415]
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.0553994778987124, 0.18837176509162712, 0.0553994778987124, 0.18837176509162712, 0.32408574892769393, 0.18837176509162712]
printing an ep nov before normalisation:  25.691207654816488
printing an ep nov before normalisation:  63.27815585666232
printing an ep nov before normalisation:  21.1685954104041
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.04497464477291627, 0.22751267761354188, 0.04497464477291627, 0.22751267761354188, 0.22751267761354188, 0.22751267761354188]
printing an ep nov before normalisation:  19.027233680428804
siam score:  -0.86753005
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9943444444444445 -1.0 -0.9943444444444445
maxi score, test score, baseline:  -0.9943444444444445 -1.0 -0.9943444444444445
probs:  [0.05499745980041783, 0.05499745980041783, 0.05499745980041783, 0.2783358735329155, 0.2783358735329155, 0.2783358735329155]
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.05499745254858723, 0.05499745254858723, 0.05499745254858723, 0.2783358807847461, 0.2783358807847461, 0.2783358807847461]
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.05499745254858723, 0.05499745254858723, 0.05499745254858723, 0.2783358807847461, 0.2783358807847461, 0.2783358807847461]
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.531]
 [0.531]
 [0.571]
 [0.531]
 [0.531]
 [0.531]] [[25.738]
 [28.888]
 [28.888]
 [40.095]
 [28.888]
 [28.888]
 [28.888]] [[1.364]
 [1.681]
 [1.681]
 [2.278]
 [1.681]
 [1.681]
 [1.681]]
printing an ep nov before normalisation:  39.62653148482478
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.16601178391195054, 0.16601178391195054, 0.16601178391195054, 0.23215494213828902, 0.23215494213828902, 0.03765476398757048]
Printing some Q and Qe and total Qs values:  [[0.624]
 [0.619]
 [0.608]
 [0.609]
 [0.606]
 [0.599]
 [0.589]] [[41.666]
 [46.694]
 [45.056]
 [46.568]
 [48.036]
 [49.548]
 [46.879]] [[1.897]
 [2.233]
 [2.111]
 [2.214]
 [2.31 ]
 [2.406]
 [2.215]]
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.16601178391195054, 0.16601178391195054, 0.16601178391195054, 0.23215494213828902, 0.23215494213828902, 0.03765476398757048]
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.16601178391195054, 0.16601178391195054, 0.16601178391195054, 0.23215494213828902, 0.23215494213828902, 0.03765476398757048]
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.16601178391195054, 0.16601178391195054, 0.16601178391195054, 0.23215494213828902, 0.23215494213828902, 0.03765476398757048]
printing an ep nov before normalisation:  31.28390758433053
printing an ep nov before normalisation:  58.18545966601816
from probs:  [0.17374751847922895, 0.17374751847922895, 0.17374751847922895, 0.2218973108046503, 0.2218973108046503, 0.034962822953012423]
siam score:  -0.8619883
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.498]
 [0.498]
 [0.548]
 [0.498]
 [0.498]
 [0.498]] [[64.672]
 [64.672]
 [64.672]
 [65.864]
 [64.672]
 [64.672]
 [64.672]] [[2.118]
 [2.118]
 [2.118]
 [2.198]
 [2.118]
 [2.118]
 [2.118]]
printing an ep nov before normalisation:  37.97950234830202
Printing some Q and Qe and total Qs values:  [[0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.308]] [[32.634]
 [32.634]
 [32.634]
 [32.634]
 [32.634]
 [32.634]
 [43.097]] [[1.095]
 [1.095]
 [1.095]
 [1.095]
 [1.095]
 [1.095]
 [1.594]]
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.20501530332659681, 0.20501530332659681, 0.20501530332659681, 0.20501530332659681, 0.1466337967696882, 0.03330498992392459]
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]] [[50.239]
 [50.239]
 [50.239]
 [50.239]
 [50.239]
 [50.239]
 [50.239]] [[0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]]
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.20501530332659681, 0.20501530332659681, 0.20501530332659681, 0.20501530332659681, 0.1466337967696882, 0.03330498992392459]
printing an ep nov before normalisation:  53.70103589696763
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.20501530332659681, 0.20501530332659681, 0.20501530332659681, 0.20501530332659681, 0.1466337967696882, 0.03330498992392459]
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
printing an ep nov before normalisation:  44.22857672554354
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.20501530332659681, 0.20501530332659681, 0.20501530332659681, 0.20501530332659681, 0.1466337967696882, 0.03330498992392459]
Printing some Q and Qe and total Qs values:  [[0.483]
 [0.483]
 [0.483]
 [0.486]
 [0.483]
 [0.483]
 [0.483]] [[14.527]
 [14.527]
 [14.527]
 [16.046]
 [14.527]
 [14.527]
 [14.527]] [[0.483]
 [0.483]
 [0.483]
 [0.486]
 [0.483]
 [0.483]
 [0.483]]
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.20501530332659681, 0.20501530332659681, 0.20501530332659681, 0.20501530332659681, 0.1466337967696882, 0.03330498992392459]
from probs:  [0.20501530332659681, 0.20501530332659681, 0.20501530332659681, 0.20501530332659681, 0.1466337967696882, 0.03330498992392459]
printing an ep nov before normalisation:  29.3320470229682
maxi score, test score, baseline:  -0.9944054945054945 -1.0 -0.9944054945054945
maxi score, test score, baseline:  -0.9944054945054945 -1.0 -0.9944054945054945
probs:  [0.2050153039835146, 0.2050153039835146, 0.2050153039835146, 0.2050153039835146, 0.14663379642652216, 0.03330498763941936]
maxi score, test score, baseline:  -0.9944054945054945 -1.0 -0.9944054945054945
probs:  [0.2050153039835146, 0.2050153039835146, 0.2050153039835146, 0.2050153039835146, 0.14663379642652216, 0.03330498763941936]
maxi score, test score, baseline:  -0.9944355191256831 -1.0 -0.9944355191256831
probs:  [0.1660185639700378, 0.23212503902615267, 0.23212503902615267, 0.1660185639700378, 0.1660185639700378, 0.037694230037581274]
from probs:  [0.1660185639700378, 0.23212503902615267, 0.23212503902615267, 0.1660185639700378, 0.1660185639700378, 0.037694230037581274]
maxi score, test score, baseline:  -0.9944355191256831 -1.0 -0.9944355191256831
probs:  [0.17752210384665304, 0.2482128205522307, 0.2482128205522307, 0.17752210384665304, 0.10823120331346439, 0.040298947888768026]
printing an ep nov before normalisation:  34.650261448489715
actions average: 
K:  3  action  0 :  tensor([0.3448, 0.0134, 0.1017, 0.1207, 0.2092, 0.1018, 0.1083],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0472, 0.7379, 0.0303, 0.0568, 0.0322, 0.0305, 0.0650],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1420, 0.0414, 0.2758, 0.1432, 0.1424, 0.1273, 0.1278],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1833, 0.0574, 0.1097, 0.2042, 0.1794, 0.1142, 0.1518],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1701, 0.0105, 0.1266, 0.2034, 0.1826, 0.1430, 0.1637],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1694, 0.0690, 0.1120, 0.1801, 0.2077, 0.1197, 0.1421],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2073, 0.0687, 0.1166, 0.1677, 0.1562, 0.1181, 0.1654],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
probs:  [0.1910273422205775, 0.1910273422205775, 0.2670999944049906, 0.1910273422205775, 0.11646107918832803, 0.04335689974494898]
actions average: 
K:  1  action  0 :  tensor([0.3745, 0.0030, 0.0962, 0.1552, 0.1696, 0.0816, 0.1199],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0032, 0.9450, 0.0085, 0.0174, 0.0027, 0.0047, 0.0185],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1544, 0.0123, 0.3030, 0.1354, 0.1228, 0.1377, 0.1344],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1731, 0.0078, 0.1347, 0.2186, 0.1810, 0.1395, 0.1453],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1743, 0.0301, 0.1243, 0.1840, 0.2023, 0.1269, 0.1581],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1578, 0.0137, 0.1609, 0.1718, 0.1773, 0.1601, 0.1583],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2028, 0.0061, 0.1357, 0.1685, 0.1917, 0.1361, 0.1592],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
printing an ep nov before normalisation:  45.95024566231743
printing an ep nov before normalisation:  40.845252884644644
Printing some Q and Qe and total Qs values:  [[1.101]
 [0.61 ]
 [1.001]
 [1.02 ]
 [0.989]
 [0.921]
 [0.778]] [[19.446]
 [48.711]
 [28.058]
 [24.692]
 [26.806]
 [30.55 ]
 [32.408]] [[1.178]
 [0.901]
 [1.141]
 [1.136]
 [1.12 ]
 [1.08 ]
 [0.951]]
siam score:  -0.8624567
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
probs:  [0.21035588600186567, 0.21035588600186567, 0.21035588600186567, 0.21035588600186567, 0.12240258918232017, 0.036173866810217094]
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
probs:  [0.134203988234173, 0.23064650862585181, 0.23064650862585181, 0.23064650862585181, 0.134203988234173, 0.03965249765409849]
printing an ep nov before normalisation:  42.533798735586544
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
probs:  [0.134203988234173, 0.23064650862585181, 0.23064650862585181, 0.23064650862585181, 0.134203988234173, 0.03965249765409849]
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
probs:  [0.04378294532538271, 0.25473913550333804, 0.25473913550333804, 0.25473913550333804, 0.14821670283922064, 0.04378294532538271]
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
probs:  [0.04887757300649144, 0.2844557603268419, 0.2844557603268419, 0.2844557603268419, 0.04887757300649144, 0.04887757300649144]
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
probs:  [0.04333493169439781, 0.2666500081143253, 0.19073762240062037, 0.2666500081143253, 0.11631371483816565, 0.11631371483816565]
printing an ep nov before normalisation:  56.73797637254618
printing an ep nov before normalisation:  56.31717251219904
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.909]
 [0.569]
 [0.569]
 [0.569]
 [0.569]
 [0.569]] [[40.004]
 [43.134]
 [40.004]
 [40.004]
 [40.004]
 [40.004]
 [40.004]] [[1.347]
 [1.797]
 [1.347]
 [1.347]
 [1.347]
 [1.347]
 [1.347]]
Printing some Q and Qe and total Qs values:  [[0.526]
 [0.481]
 [0.476]
 [0.523]
 [0.519]
 [0.515]
 [0.495]] [[58.757]
 [61.786]
 [64.101]
 [71.262]
 [72.937]
 [64.224]
 [61.341]] [[1.057]
 [1.076]
 [1.121]
 [1.319]
 [1.351]
 [1.162]
 [1.081]]
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
probs:  [0.04333493169439781, 0.2666500081143253, 0.19073762240062037, 0.2666500081143253, 0.11631371483816565, 0.11631371483816565]
printing an ep nov before normalisation:  52.714148136670005
printing an ep nov before normalisation:  43.27996310821681
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
probs:  [0.04333493169439781, 0.2666500081143253, 0.19073762240062037, 0.2666500081143253, 0.11631371483816565, 0.11631371483816565]
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
probs:  [0.04333493169439781, 0.2666500081143253, 0.19073762240062037, 0.2666500081143253, 0.11631371483816565, 0.11631371483816565]
printing an ep nov before normalisation:  21.332475333963536
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
printing an ep nov before normalisation:  38.309225771536504
siam score:  -0.8526717
printing an ep nov before normalisation:  58.98077713942589
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
probs:  [0.04688695835094232, 0.2885629097712484, 0.20640903849635886, 0.20640903849635886, 0.1258660274425458, 0.1258660274425458]
printing an ep nov before normalisation:  40.96922586470001
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
probs:  [0.04688695835094232, 0.2885629097712484, 0.20640903849635886, 0.20640903849635886, 0.1258660274425458, 0.1258660274425458]
Printing some Q and Qe and total Qs values:  [[-0.002]
 [ 0.034]
 [-0.   ]
 [ 0.002]
 [-0.   ]
 [-0.002]
 [ 0.003]] [[32.375]
 [32.642]
 [32.581]
 [32.108]
 [33.047]
 [33.3  ]
 [33.293]] [[0.872]
 [0.921]
 [0.884]
 [0.861]
 [0.909]
 [0.921]
 [0.925]]
siam score:  -0.8510377
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
probs:  [0.04688695835094232, 0.2885629097712484, 0.20640903849635886, 0.20640903849635886, 0.1258660274425458, 0.1258660274425458]
printing an ep nov before normalisation:  19.444701105128743
actor:  1 policy actor:  1  step number:  74 total reward:  0.059999999999999165  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
probs:  [0.08143711744006872, 0.09417333737501354, 0.08559928081750166, 0.5675917027324129, 0.08559928081750166, 0.08559928081750166]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
printing an ep nov before normalisation:  30.090686486826716
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
probs:  [0.08143711744006872, 0.09417333737501354, 0.08559928081750166, 0.5675917027324129, 0.08559928081750166, 0.08559928081750166]
printing an ep nov before normalisation:  80.09066122046261
maxi score, test score, baseline:  -0.9945236559139785 -1.0 -0.9945236559139785
Printing some Q and Qe and total Qs values:  [[0.54 ]
 [0.547]
 [0.547]
 [0.556]
 [0.567]
 [0.545]
 [0.542]] [[26.131]
 [33.8  ]
 [25.371]
 [27.7  ]
 [39.112]
 [32.202]
 [33.38 ]] [[0.889]
 [1.112]
 [0.874]
 [0.949]
 [1.282]
 [1.065]
 [1.095]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9945236559139785 -1.0 -0.9945236559139785
printing an ep nov before normalisation:  35.876169204711914
maxi score, test score, baseline:  -0.9945236559139785 -1.0 -0.9945236559139785
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
printing an ep nov before normalisation:  38.24342001815413
printing an ep nov before normalisation:  39.634434578293934
printing an ep nov before normalisation:  18.015666007995605
actions average: 
K:  2  action  0 :  tensor([0.2840, 0.0148, 0.1533, 0.1458, 0.1548, 0.1285, 0.1189],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0229, 0.8808, 0.0174, 0.0225, 0.0170, 0.0137, 0.0257],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1844, 0.0183, 0.2266, 0.1531, 0.1336, 0.1436, 0.1405],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1616, 0.0109, 0.1143, 0.3177, 0.1577, 0.1094, 0.1284],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2142, 0.0068, 0.1130, 0.1425, 0.2874, 0.1099, 0.1262],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1521, 0.0073, 0.1834, 0.1699, 0.1570, 0.1944, 0.1359],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2412, 0.0183, 0.1351, 0.1818, 0.1691, 0.1289, 0.1256],
       grad_fn=<DivBackward0>)
siam score:  -0.86733603
maxi score, test score, baseline:  -0.9945236559139785 -1.0 -0.9945236559139785
probs:  [0.08033937543808004, 0.09323986054646922, 0.08885538194753954, 0.559854618172832, 0.08885538194753954, 0.08885538194753954]
actions average: 
K:  0  action  0 :  tensor([0.3437, 0.0016, 0.1365, 0.1405, 0.1405, 0.1254, 0.1119],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0296, 0.8709, 0.0202, 0.0211, 0.0109, 0.0189, 0.0285],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1845, 0.0432, 0.2025, 0.1720, 0.1303, 0.1477, 0.1198],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2139, 0.0067, 0.1519, 0.2138, 0.1406, 0.1417, 0.1314],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1966, 0.0066, 0.1442, 0.1683, 0.2143, 0.1456, 0.1246],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1946, 0.0020, 0.1571, 0.1345, 0.1359, 0.2617, 0.1142],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1988, 0.0486, 0.1790, 0.1676, 0.1096, 0.1365, 0.1600],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9945236559139785 -1.0 -0.9945236559139785
probs:  [0.08028624140642165, 0.09341315263314819, 0.08895171875216921, 0.5594454497039227, 0.08895171875216921, 0.08895171875216921]
maxi score, test score, baseline:  -0.9945524064171123 -1.0 -0.9945524064171123
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.659]
 [0.658]
 [0.657]
 [0.659]
 [0.655]
 [0.654]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.651]
 [0.659]
 [0.658]
 [0.657]
 [0.659]
 [0.655]
 [0.654]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 54.70186061101566
maxi score, test score, baseline:  -0.9945524064171123 -1.0 -0.9945524064171123
printing an ep nov before normalisation:  43.83657805916834
actions average: 
K:  4  action  0 :  tensor([0.3108, 0.0118, 0.1296, 0.1553, 0.1449, 0.1365, 0.1111],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0481, 0.7279, 0.0579, 0.0392, 0.0235, 0.0323, 0.0711],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1685, 0.1428, 0.1570, 0.1399, 0.1183, 0.1520, 0.1215],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1689, 0.0313, 0.1310, 0.2448, 0.1482, 0.1332, 0.1425],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1976, 0.0285, 0.1548, 0.1753, 0.1639, 0.1428, 0.1371],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1673, 0.0113, 0.1556, 0.1987, 0.1539, 0.1768, 0.1364],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2007, 0.0119, 0.1992, 0.1670, 0.1354, 0.1251, 0.1606],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9945524064171123 -1.0 -0.9945524064171123
probs:  [0.07802086911528006, 0.09889309021379579, 0.09455494229920229, 0.5436751258060739, 0.0903010302664456, 0.09455494229920229]
using explorer policy with actor:  0
printing an ep nov before normalisation:  30.256043214239604
from probs:  [0.07802086911528006, 0.09889309021379579, 0.09455494229920229, 0.5436751258060739, 0.0903010302664456, 0.09455494229920229]
maxi score, test score, baseline:  -0.9945524064171123 -1.0 -0.9945524064171123
probs:  [0.07686017126625903, 0.10109975541312334, 0.09686178890378591, 0.5356103825067654, 0.09270611300628037, 0.09686178890378591]
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.07686015172170509, 0.10109980830478445, 0.09686182913094536, 0.5356102408967689, 0.09270614081485079, 0.09686182913094536]
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.07749748191522428, 0.10193848457110682, 0.08936546878178257, 0.5400581666957562, 0.09347509530191778, 0.0976653027342123]
maxi score, test score, baseline:  -0.9946089947089947 -1.0 -0.9946089947089947
probs:  [0.07749746431784509, 0.1019385398351823, 0.08936548656424441, 0.5400580386876382, 0.09347512533565125, 0.09766534525943865]
maxi score, test score, baseline:  -0.9946089947089947 -1.0 -0.9946089947089947
maxi score, test score, baseline:  -0.9946089947089947 -1.0 -0.9946089947089947
probs:  [0.07749746431784509, 0.1019385398351823, 0.08936548656424441, 0.5400580386876382, 0.09347512533565125, 0.09766534525943865]
maxi score, test score, baseline:  -0.9946089947089947 -1.0 -0.9946089947089947
probs:  [0.07749746431784509, 0.1019385398351823, 0.08936548656424441, 0.5400580386876382, 0.09347512533565125, 0.09766534525943865]
maxi score, test score, baseline:  -0.9946089947089947 -1.0 -0.9946089947089947
probs:  [0.07730290478666454, 0.1025541849468124, 0.08956434371058253, 0.5386290154639639, 0.0938102141470201, 0.0981393369449565]
maxi score, test score, baseline:  -0.9946089947089947 -1.0 -0.9946089947089947
probs:  [0.07730290478666454, 0.1025541849468124, 0.08956434371058253, 0.5386290154639639, 0.0938102141470201, 0.0981393369449565]
printing an ep nov before normalisation:  28.727207558112287
maxi score, test score, baseline:  -0.9946089947089947 -1.0 -0.9946089947089947
probs:  [0.07730290478666454, 0.1025541849468124, 0.08956434371058253, 0.5386290154639639, 0.0938102141470201, 0.0981393369449565]
actions average: 
K:  2  action  0 :  tensor([0.2354, 0.0066, 0.1329, 0.1797, 0.1690, 0.1529, 0.1234],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0338, 0.8346, 0.0232, 0.0263, 0.0281, 0.0264, 0.0276],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1483, 0.0054, 0.1628, 0.2262, 0.1530, 0.1622, 0.1421],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1694, 0.0716, 0.1107, 0.2228, 0.1707, 0.1261, 0.1287],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1856, 0.0172, 0.1291, 0.1950, 0.1993, 0.1338, 0.1400],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1928, 0.0180, 0.1437, 0.1789, 0.1681, 0.1765, 0.1220],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1804, 0.0374, 0.1162, 0.1775, 0.1482, 0.1357, 0.2046],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
using explorer policy with actor:  1
Starting evaluation
printing an ep nov before normalisation:  52.75664806365967
printing an ep nov before normalisation:  53.91487447000542
printing an ep nov before normalisation:  34.60269104998309
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
probs:  [0.07797761625284076, 0.09899629376478536, 0.09034629955794662, 0.5433371971016953, 0.09034629955794662, 0.09899629376478536]
using explorer policy with actor:  0
printing an ep nov before normalisation:  60.307722151591406
printing an ep nov before normalisation:  36.55593922270911
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.344]
 [0.344]
 [0.344]
 [0.344]
 [0.344]
 [0.344]] [[23.192]
 [22.476]
 [22.476]
 [22.476]
 [22.476]
 [22.476]
 [22.476]] [[0.345]
 [0.344]
 [0.344]
 [0.344]
 [0.344]
 [0.344]
 [0.344]]
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.358]] [[37.867]
 [37.867]
 [37.867]
 [37.867]
 [37.867]
 [37.867]
 [37.867]] [[0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.358]]
Printing some Q and Qe and total Qs values:  [[0.199]
 [0.185]
 [0.213]
 [0.201]
 [0.203]
 [0.213]
 [0.213]] [[13.381]
 [25.436]
 [17.024]
 [13.229]
 [13.261]
 [17.024]
 [17.024]] [[0.199]
 [0.185]
 [0.213]
 [0.201]
 [0.203]
 [0.213]
 [0.213]]
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
using explorer policy with actor:  0
printing an ep nov before normalisation:  12.553586240247439
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.07712543923096526, 0.10163404685687945, 0.09315029806329375, 0.5374265489141442, 0.08902962007783792, 0.10163404685687945]
printing an ep nov before normalisation:  13.460502624511719
printing an ep nov before normalisation:  44.04219768074407
printing an ep nov before normalisation:  23.468731557222668
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.429]
 [0.429]
 [0.394]
 [0.429]
 [0.429]
 [0.429]] [[28.689]
 [28.689]
 [28.689]
 [37.217]
 [28.689]
 [28.689]
 [28.689]] [[1.301]
 [1.301]
 [1.301]
 [1.909]
 [1.301]
 [1.301]
 [1.301]]
Printing some Q and Qe and total Qs values:  [[0.541]
 [0.628]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]] [[44.381]
 [48.672]
 [44.381]
 [44.381]
 [44.381]
 [44.381]
 [44.381]] [[2.065]
 [2.392]
 [2.065]
 [2.065]
 [2.065]
 [2.065]
 [2.065]]
Printing some Q and Qe and total Qs values:  [[0.206]
 [0.185]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]] [[26.769]
 [58.104]
 [26.769]
 [26.769]
 [26.769]
 [26.769]
 [26.769]] [[0.206]
 [0.185]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]]
printing an ep nov before normalisation:  27.24207621564084
printing an ep nov before normalisation:  18.240139484405518
Printing some Q and Qe and total Qs values:  [[0.366]
 [0.321]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]] [[10.618]
 [18.24 ]
 [10.729]
 [11.262]
 [11.279]
 [11.224]
 [11.347]] [[0.366]
 [0.321]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  34.65203609229886
printing an ep nov before normalisation:  32.40724083262871
printing an ep nov before normalisation:  13.497400541780928
printing an ep nov before normalisation:  58.018784953547275
printing an ep nov before normalisation:  7.633889197222743e-05
maxi score, test score, baseline:  -0.9948494949494949 -1.0 -0.9948494949494949
probs:  [0.07681761150043559, 0.1025350262734412, 0.09363284423663157, 0.5351705644687264, 0.08930892724732407, 0.1025350262734412]
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.07681759337215298, 0.10253507895088981, 0.09363287240440389, 0.5351704328112669, 0.08930894351039655, 0.10253507895088981]
printing an ep nov before normalisation:  37.4797186973584
printing an ep nov before normalisation:  14.57448963787243
printing an ep nov before normalisation:  21.658916300925895
printing an ep nov before normalisation:  13.988670653155154
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
printing an ep nov before normalisation:  12.645313084117697
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]] [[11.81]
 [13.2 ]
 [13.2 ]
 [13.2 ]
 [13.2 ]
 [13.2 ]
 [13.2 ]] [[0.326]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]]
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]] [[11.313]
 [11.313]
 [11.313]
 [11.313]
 [11.313]
 [11.313]
 [11.313]] [[0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]]
printing an ep nov before normalisation:  22.22103095238986
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.321]
 [0.309]
 [0.323]
 [0.32 ]
 [0.323]
 [0.324]
 [0.325]] [[11.081]
 [25.099]
 [10.168]
 [13.362]
 [12.301]
 [12.202]
 [11.037]] [[0.321]
 [0.309]
 [0.323]
 [0.32 ]
 [0.323]
 [0.324]
 [0.325]]
printing an ep nov before normalisation:  12.377310461201995
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.0770636401060492, 0.1033156178818842, 0.09422839480563364, 0.5368498528560277, 0.0898146007400262, 0.09872789361037908]
printing an ep nov before normalisation:  10.81591405932895
using explorer policy with actor:  0
printing an ep nov before normalisation:  11.80373132581394
printing an ep nov before normalisation:  0.0
printing an ep nov before normalisation:  11.037105321884155
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.07685843097797171, 0.10396510200259981, 0.09458202357099775, 0.5353419012532663, 0.09002452833279101, 0.09922801386237355]
printing an ep nov before normalisation:  36.6153397481683
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.0768584132085229, 0.10396515783229422, 0.09458205392406567, 0.5353417718935378, 0.09002454631149753, 0.09922805683008173]
printing an ep nov before normalisation:  47.3288251266421
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.0768584132085229, 0.10396515783229422, 0.09458205392406567, 0.5353417718935378, 0.09002454631149753, 0.09922805683008173]
printing an ep nov before normalisation:  13.417015075683594
printing an ep nov before normalisation:  48.918909336414295
printing an ep nov before normalisation:  30.16381025314331
printing an ep nov before normalisation:  49.246120441568976
printing an ep nov before normalisation:  41.61211836075232
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.0768584132085229, 0.10396515783229422, 0.09458205392406567, 0.5353417718935378, 0.09002454631149753, 0.09922805683008173]
printing an ep nov before normalisation:  33.04855074162213
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.0768584132085229, 0.10396515783229422, 0.09458205392406567, 0.5353417718935378, 0.09002454631149753, 0.09922805683008173]
from probs:  [0.0768584132085229, 0.10396515783229422, 0.09458205392406567, 0.5353417718935378, 0.09002454631149753, 0.09922805683008173]
printing an ep nov before normalisation:  35.68191016346184
line 256 mcts: sample exp_bonus 60.5998019549451
printing an ep nov before normalisation:  46.43572127208267
printing an ep nov before normalisation:  12.34954833984375
maxi score, test score, baseline:  -0.9950219512195122 -1.0 -0.9950219512195122
probs:  [0.07710351891941938, 0.1047729689663506, 0.09519508241164364, 0.5370122730702686, 0.08597862553824642, 0.09993753109407132]
printing an ep nov before normalisation:  26.944313049316406
printing an ep nov before normalisation:  60.94699196467994
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
probs:  [0.07700223532650147, 0.1051191379846862, 0.09538636398762233, 0.5362658937097359, 0.08602086448101359, 0.10020550451044041]
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
probs:  [0.07700223532650147, 0.1051191379846862, 0.09538636398762233, 0.5362658937097359, 0.08602086448101359, 0.10020550451044041]
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
using explorer policy with actor:  0
printing an ep nov before normalisation:  24.6171293580889
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
probs:  [0.07700223532650147, 0.1051191379846862, 0.09538636398762233, 0.5362658937097359, 0.08602086448101359, 0.10020550451044041]
from probs:  [0.07700223532650147, 0.1051191379846862, 0.09538636398762233, 0.5362658937097359, 0.08602086448101359, 0.10020550451044041]
maxi score, test score, baseline:  -0.9951380952380953 -1.0 -0.9951380952380953
maxi score, test score, baseline:  -0.9951380952380953 -1.0 -0.9951380952380953
probs:  [0.07679506887862, 0.10582719590518025, 0.09577761347290945, 0.5347392408686001, 0.08610726056638457, 0.10075362030830556]
printing an ep nov before normalisation:  37.857073005483926
siam score:  -0.86847365
using explorer policy with actor:  0
printing an ep nov before normalisation:  38.17926141002211
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.042]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]] [[46.717]
 [46.717]
 [46.717]
 [46.717]
 [46.717]
 [46.717]
 [46.717]] [[1.291]
 [1.291]
 [1.291]
 [1.291]
 [1.291]
 [1.291]
 [1.291]]
printing an ep nov before normalisation:  81.7659376384664
printing an ep nov before normalisation:  40.467571029496916
printing an ep nov before normalisation:  62.75714958493288
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9951830188679245 -1.0 -0.9951830188679245
probs:  [0.0765815279166295, 0.10655704120680118, 0.09618090199097248, 0.5331656116961712, 0.08619631519838267, 0.10131860199104308]
printing an ep nov before normalisation:  24.59468077412346
maxi score, test score, baseline:  -0.9951830188679245 -1.0 -0.9951830188679245
printing an ep nov before normalisation:  52.51251115589611
using explorer policy with actor:  1
siam score:  -0.86469734
using explorer policy with actor:  1
using explorer policy with actor:  1
from probs:  [0.07647228561150211, 0.10693041255514595, 0.09638721476696151, 0.532360580478038, 0.08624187349908598, 0.10160763308926642]
maxi score, test score, baseline:  -0.9951830188679245 -1.0 -0.9951830188679245
from probs:  [0.07636133203813725, 0.10730963272303419, 0.09659675940903133, 0.5315429385377844, 0.08628814546536825, 0.1019011918266444]
printing an ep nov before normalisation:  61.31149898512555
maxi score, test score, baseline:  -0.9951830188679245 -1.0 -0.9951830188679245
probs:  [0.07624862666887194, 0.10769484022702458, 0.09680961245689478, 0.530712387217909, 0.08633514799884537, 0.10219938543045422]
maxi score, test score, baseline:  -0.9951830188679245 -1.0 -0.9951830188679245
probs:  [0.07624862666887194, 0.10769484022702458, 0.09680961245689478, 0.530712387217909, 0.08633514799884537, 0.10219938543045422]
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
printing an ep nov before normalisation:  61.8295955657959
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.07624860769119823, 0.10769490463116117, 0.0968096479980971, 0.5307122486502504, 0.08633515576628073, 0.10219943526301227]
printing an ep nov before normalisation:  40.454564885202885
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.07665350359091845, 0.10826707916149787, 0.09200866886805696, 0.5335346646691049, 0.08679370707582125, 0.10274237663460054]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.07665350359091845, 0.10826707916149787, 0.09200866886805696, 0.5335346646691049, 0.08679370707582125, 0.10274237663460054]
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
probs:  [0.07524877065355337, 0.11074309668890825, 0.09495174347317895, 0.5237926938649027, 0.08988659244171861, 0.10537710287773805]
printing an ep nov before normalisation:  43.38317394256592
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
probs:  [0.07524877065355337, 0.11074309668890825, 0.09495174347317895, 0.5237926938649027, 0.08988659244171861, 0.10537710287773805]
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
probs:  [0.07512131309455225, 0.11117292978442155, 0.09513363909382666, 0.5228604717044167, 0.08998896094778681, 0.10572268537499613]
using explorer policy with actor:  1
printing an ep nov before normalisation:  10.520131003772804
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
probs:  [0.07499190975489378, 0.11160932475770555, 0.09531831155237297, 0.5219140181412395, 0.09009289222236072, 0.10607354357142752]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.533]
 [0.413]
 [0.413]
 [0.439]
 [0.413]
 [0.413]] [[33.559]
 [35.348]
 [33.559]
 [33.559]
 [27.331]
 [33.559]
 [33.559]] [[1.629]
 [1.827]
 [1.629]
 [1.629]
 [1.379]
 [1.629]
 [1.629]]
printing an ep nov before normalisation:  0.6898552322596174
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
probs:  [0.07486051573511619, 0.11205243302602283, 0.09550582492517051, 0.5209530047815618, 0.09019842232678389, 0.10642979920534482]
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
probs:  [0.07472708474362529, 0.11250241070141062, 0.09569624527529395, 0.519977093049545, 0.09030558844050178, 0.10679157778962341]
printing an ep nov before normalisation:  47.86754131317139
printing an ep nov before normalisation:  43.71389656500691
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.07514755517538757, 0.11313592407091162, 0.09623497627657646, 0.5229073407946158, 0.09081391755009166, 0.10176028613241676]
printing an ep nov before normalisation:  15.652231244927687
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.07514755517538757, 0.11313592407091162, 0.09623497627657646, 0.5229073407946158, 0.09081391755009166, 0.10176028613241676]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.07501789637659033, 0.11360554346315331, 0.09643797802464163, 0.5219567303633795, 0.09093140043115684, 0.10205045134107822]
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.07501789637659033, 0.11360554346315331, 0.09643797802464163, 0.5219567303633795, 0.09093140043115684, 0.10205045134107822]
siam score:  -0.8506104
printing an ep nov before normalisation:  27.882742106703823
printing an ep nov before normalisation:  61.479877346161544
printing an ep nov before normalisation:  29.524472613620024
printing an ep nov before normalisation:  42.347308502887856
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.0748861723713329, 0.11408264295710577, 0.09664421318629263, 0.5209909786010349, 0.09105075458056008, 0.10234523830367385]
siam score:  -0.85224265
printing an ep nov before normalisation:  34.23273801803589
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.07530699008986756, 0.11472411511527278, 0.09156256995209669, 0.5239231195220121, 0.09156256995209669, 0.10292063536865419]
siam score:  -0.85291016
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.07517906506779902, 0.11522182871272167, 0.09169265762217418, 0.5229827898373998, 0.09169265762217418, 0.10323100113773122]
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.07547859178800544, 0.11639047397754061, 0.08662148092908051, 0.5250194570354214, 0.09235060789312385, 0.10413938837682826]
printing an ep nov before normalisation:  28.60489136546849
Printing some Q and Qe and total Qs values:  [[0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.394]
 [0.407]
 [0.407]] [[19.904]
 [19.904]
 [19.904]
 [19.904]
 [26.382]
 [19.904]
 [19.904]] [[1.377]
 [1.377]
 [1.377]
 [1.377]
 [1.698]
 [1.377]
 [1.377]]
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
printing an ep nov before normalisation:  91.40081743264211
line 256 mcts: sample exp_bonus 16.38590099586613
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.07535250029259763, 0.11691836263358088, 0.08667350952832605, 0.5240899681212345, 0.09249421710707334, 0.10447144231718747]
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.07535250029259763, 0.11691836263358088, 0.08667350952832605, 0.5240899681212345, 0.09249421710707334, 0.10447144231718747]
using explorer policy with actor:  1
printing an ep nov before normalisation:  85.72904902409672
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.0756887689370212, 0.11818071185864287, 0.0872620083976498, 0.5263799772511393, 0.09321240038448252, 0.09927613317106428]
printing an ep nov before normalisation:  41.472314731706206
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.07556506610728311, 0.11874162126132506, 0.08732476871265635, 0.5254650530287568, 0.09337103090126814, 0.09953245998871048]
printing an ep nov before normalisation:  65.83328572210426
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.07556506610728311, 0.11874162126132506, 0.08732476871265635, 0.5254650530287568, 0.09337103090126814, 0.09953245998871048]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.07556506610728311, 0.11874162126132506, 0.08732476871265635, 0.5254650530287568, 0.09337103090126814, 0.09953245998871048]
printing an ep nov before normalisation:  51.380426369747155
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.07579290487743988, 0.12066023590075921, 0.0880131125259808, 0.5269414590618681, 0.09429614381697601, 0.09429614381697601]
printing an ep nov before normalisation:  78.39923866175246
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.07579290487743988, 0.12066023590075921, 0.0880131125259808, 0.5269414590618681, 0.09429614381697601, 0.09429614381697601]
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.07579290487743988, 0.12066023590075921, 0.0880131125259808, 0.5269414590618681, 0.09429614381697601, 0.09429614381697601]
printing an ep nov before normalisation:  71.07185086592416
printing an ep nov before normalisation:  73.37799832853213
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.07579290487743988, 0.12066023590075921, 0.0880131125259808, 0.5269414590618681, 0.09429614381697601, 0.09429614381697601]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
from probs:  [0.07579290487743988, 0.12066023590075921, 0.0880131125259808, 0.5269414590618681, 0.09429614381697601, 0.09429614381697601]
printing an ep nov before normalisation:  49.443265968356116
actions average: 
K:  3  action  0 :  tensor([0.3177, 0.0168, 0.1170, 0.1354, 0.1706, 0.1120, 0.1306],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0331, 0.8377, 0.0151, 0.0255, 0.0462, 0.0143, 0.0281],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1328, 0.0120, 0.2174, 0.1566, 0.1382, 0.2070, 0.1361],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1484, 0.0757, 0.1196, 0.2257, 0.1297, 0.1386, 0.1624],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1738, 0.0030, 0.1437, 0.1845, 0.1843, 0.1505, 0.1602],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1399, 0.0050, 0.1720, 0.1558, 0.1466, 0.2493, 0.1313],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1967, 0.0744, 0.1273, 0.1649, 0.1705, 0.1226, 0.1437],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.07566959900003593, 0.12126740128826832, 0.08808876010390693, 0.5260260726044848, 0.094474083501652, 0.094474083501652]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
probs:  [0.07554405161556177, 0.12188560323379004, 0.08816578263040634, 0.5250940474422777, 0.09465525753898209, 0.09465525753898209]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
probs:  [0.076027833810738, 0.12266667985069254, 0.08232037653041432, 0.5284618225418651, 0.09526164363314504, 0.09526164363314504]
printing an ep nov before normalisation:  55.472626671808435
printing an ep nov before normalisation:  55.180825895548985
siam score:  -0.8476903
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.3847],
        [-0.4661],
        [-0.0000],
        [-0.0000],
        [-0.3187],
        [-0.5449],
        [-0.6474],
        [-0.0000],
        [-0.5252],
        [-0.0000]], dtype=torch.float64)
-0.09703970119800001 -0.48173264158602425
-0.032346567066 -0.49842297256707835
-0.924 -0.924
-0.9605640000000001 -0.9605640000000001
-0.09703970119800001 -0.41578066091564947
-0.032346567066 -0.5772531935996468
-0.032346567066 -0.6797106923463928
-0.368363051363999 -0.368363051363999
-0.032346567066 -0.557569035864535
-0.836418 -0.836418
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
probs:  [0.076027833810738, 0.12266667985069254, 0.08232037653041432, 0.5284618225418651, 0.09526164363314504, 0.09526164363314504]
maxi score, test score, baseline:  -0.9953128440366973 -1.0 -0.9953128440366973
probs:  [0.07602781242691328, 0.12266679355584846, 0.0823203733728808, 0.5284616647246165, 0.09526167795987042, 0.09526167795987042]
printing an ep nov before normalisation:  24.689107572875507
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9953128440366973 -1.0 -0.9953128440366973
probs:  [0.07633980928110849, 0.11754242896102159, 0.08288899111294654, 0.5305126442805267, 0.09635806318219832, 0.09635806318219832]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9953128440366973 -1.0 -0.9953128440366973
probs:  [0.07633980928110849, 0.11754242896102159, 0.08288899111294654, 0.5305126442805267, 0.09635806318219832, 0.09635806318219832]
maxi score, test score, baseline:  -0.9953128440366973 -1.0 -0.9953128440366973
probs:  [0.0762228983022812, 0.11811103747822575, 0.08288104388117358, 0.5296365979307166, 0.09657421120380143, 0.09657421120380143]
printing an ep nov before normalisation:  58.81564140319824
from probs:  [0.0762228983022812, 0.11811103747822575, 0.08288104388117358, 0.5296365979307166, 0.09657421120380143, 0.09657421120380143]
maxi score, test score, baseline:  -0.9953128440366973 -1.0 -0.9953128440366973
probs:  [0.076103749871177, 0.11869052807369354, 0.08287294455460176, 0.5287437857312001, 0.09679449588466384, 0.09679449588466384]
maxi score, test score, baseline:  -0.9953128440366973 -1.0 -0.9953128440366973
probs:  [0.076103749871177, 0.11869052807369354, 0.08287294455460176, 0.5287437857312001, 0.09679449588466384, 0.09679449588466384]
maxi score, test score, baseline:  -0.9953128440366973 -1.0 -0.9953128440366973
probs:  [0.076103749871177, 0.11869052807369354, 0.08287294455460176, 0.5287437857312001, 0.09679449588466384, 0.09679449588466384]
using another actor
UNIT TEST: sample policy line 217 mcts : [0.143 0.204 0.102 0.102 0.082 0.286 0.082]
maxi score, test score, baseline:  -0.9953128440366973 -1.0 -0.9953128440366973
probs:  [0.07718735441032368, 0.1203815685534278, 0.08405310141146521, 0.5362836153602748, 0.09104718013225427, 0.09104718013225427]
printing an ep nov before normalisation:  64.05640389253739
printing an ep nov before normalisation:  90.42329591562677
printing an ep nov before normalisation:  67.7259409471411
printing an ep nov before normalisation:  40.43113087250705
maxi score, test score, baseline:  -0.9953545454545455 -1.0 -0.9953545454545455
probs:  [0.07718731669158117, 0.12038179258359777, 0.08405310529787396, 0.5362833326239858, 0.09104722640148066, 0.09104722640148066]
actions average: 
K:  4  action  0 :  tensor([0.2045, 0.0547, 0.1137, 0.1825, 0.1708, 0.1514, 0.1223],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0632, 0.5980, 0.0450, 0.0897, 0.0798, 0.0529, 0.0713],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1704, 0.0046, 0.1732, 0.1780, 0.1557, 0.2020, 0.1161],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2093, 0.0445, 0.1314, 0.1917, 0.1458, 0.1662, 0.1111],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2397, 0.0160, 0.1220, 0.1497, 0.2061, 0.1431, 0.1233],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1508, 0.0305, 0.1259, 0.1594, 0.1465, 0.2535, 0.1335],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2121, 0.0488, 0.1291, 0.1710, 0.1393, 0.1744, 0.1253],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.598]
 [0.53 ]
 [0.527]
 [0.535]
 [0.525]
 [0.527]] [[32.245]
 [64.341]
 [34.129]
 [38.849]
 [44.077]
 [42.036]
 [36.495]] [[0.615]
 [0.858]
 [0.623]
 [0.647]
 [0.684]
 [0.662]
 [0.634]]
siam score:  -0.8491619
maxi score, test score, baseline:  -0.9953545454545455 -1.0 -0.9953545454545455
probs:  [0.07762397997367035, 0.12186035374872166, 0.07762397997367035, 0.539255274650489, 0.0918182058267243, 0.0918182058267243]
siam score:  -0.8480664
printing an ep nov before normalisation:  57.62727874597634
siam score:  -0.8481518
printing an ep nov before normalisation:  70.70489025403162
printing an ep nov before normalisation:  37.18193054199219
maxi score, test score, baseline:  -0.9953545454545455 -1.0 -0.9953545454545455
probs:  [0.07818348773373215, 0.12273926212595536, 0.07818348773373215, 0.5431479064404588, 0.09248020038918081, 0.08526565557694052]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
printing an ep nov before normalisation:  34.9939533181179
maxi score, test score, baseline:  -0.9953545454545455 -1.0 -0.9953545454545455
probs:  [0.07818348773373215, 0.12273926212595536, 0.07818348773373215, 0.5431479064404588, 0.09248020038918081, 0.08526565557694052]
maxi score, test score, baseline:  -0.9953545454545455 -1.0 -0.9953545454545455
probs:  [0.07818348773373215, 0.12273926212595536, 0.07818348773373215, 0.5431479064404588, 0.09248020038918081, 0.08526565557694052]
printing an ep nov before normalisation:  43.743381200387255
printing an ep nov before normalisation:  44.539037231177794
maxi score, test score, baseline:  -0.9953545454545455 -1.0 -0.9953545454545455
maxi score, test score, baseline:  -0.9953545454545455 -1.0 -0.9953545454545455
probs:  [0.0780026806346066, 0.12411271760132361, 0.0780026806346066, 0.541751908599062, 0.092798113056949, 0.08533189947345214]
printing an ep nov before normalisation:  76.95082574819028
printing an ep nov before normalisation:  46.79360503888798
from probs:  [0.07646012787542594, 0.1279219931808848, 0.08340714245794513, 0.5311128701852346, 0.09769072384256443, 0.08340714245794513]
maxi score, test score, baseline:  -0.9953545454545455 -1.0 -0.9953545454545455
probs:  [0.0763442960425381, 0.12866924687687634, 0.08340782151558758, 0.5302398560471788, 0.09793095800223177, 0.08340782151558758]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9953545454545455 -1.0 -0.9953545454545455
probs:  [0.0763442960425381, 0.12866924687687634, 0.08340782151558758, 0.5302398560471788, 0.09793095800223177, 0.08340782151558758]
printing an ep nov before normalisation:  0.016387031168960675
maxi score, test score, baseline:  -0.9953545454545455 -1.0 -0.9953545454545455
probs:  [0.07622615854740135, 0.12943137484874043, 0.08340851409004596, 0.5293494643394973, 0.09817597408426895, 0.08340851409004596]
printing an ep nov before normalisation:  29.703362659060115
maxi score, test score, baseline:  -0.995375113122172 -1.0 -0.995375113122172
probs:  [0.07745855998269278, 0.11534231751635139, 0.08475726556257192, 0.5379206414433949, 0.09976394993241698, 0.08475726556257192]
maxi score, test score, baseline:  -0.995375113122172 -1.0 -0.995375113122172
probs:  [0.07804907451806907, 0.1162221409824961, 0.08540351851580284, 0.5420275915469273, 0.09289415592090182, 0.08540351851580284]
from probs:  [0.07804907451806907, 0.1162221409824961, 0.08540351851580284, 0.5420275915469273, 0.09289415592090182, 0.08540351851580284]
maxi score, test score, baseline:  -0.995375113122172 -1.0 -0.995375113122172
maxi score, test score, baseline:  -0.995375113122172 -1.0 -0.995375113122172
probs:  [0.07795714045228923, 0.11679016990396288, 0.08543873328242825, 0.5413163489509512, 0.0930588741279402, 0.08543873328242825]
printing an ep nov before normalisation:  37.54040668197682
maxi score, test score, baseline:  -0.995375113122172 -1.0 -0.995375113122172
maxi score, test score, baseline:  -0.995375113122172 -1.0 -0.995375113122172
probs:  [0.07795714045228923, 0.11679016990396288, 0.08543873328242825, 0.5413163489509512, 0.0930588741279402, 0.08543873328242825]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.995375113122172 -1.0 -0.995375113122172
probs:  [0.07795714045228923, 0.11679016990396288, 0.08543873328242825, 0.5413163489509512, 0.0930588741279402, 0.08543873328242825]
maxi score, test score, baseline:  -0.995375113122172 -1.0 -0.995375113122172
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.636]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.559]] [[59.509]
 [57.272]
 [42.675]
 [42.675]
 [42.675]
 [42.675]
 [58.167]] [[1.673]
 [1.69 ]
 [1.06 ]
 [1.06 ]
 [1.06 ]
 [1.06 ]
 [1.646]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.995375113122172 -1.0 -0.995375113122172
probs:  [0.07776737852695458, 0.11796264346823895, 0.08551142039637646, 0.5398482667634885, 0.09339887044856517, 0.08551142039637646]
maxi score, test score, baseline:  -0.995375113122172 -1.0 -0.995375113122172
probs:  [0.07776737852695458, 0.11796264346823895, 0.08551142039637646, 0.5398482667634885, 0.09339887044856517, 0.08551142039637646]
printing an ep nov before normalisation:  36.29901829269858
using another actor
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.07776736033216869, 0.11796275478594093, 0.08551142715353752, 0.5398481279550686, 0.09339890261974669, 0.08551142715353752]
printing an ep nov before normalisation:  73.24023731757343
printing an ep nov before normalisation:  60.966601568392036
maxi score, test score, baseline:  -0.9954156950672646 -1.0 -0.9954156950672646
maxi score, test score, baseline:  -0.9954156950672646 -1.0 -0.9954156950672646
probs:  [0.07756929089546952, 0.11918655643177704, 0.08558729618228102, 0.5383157772597136, 0.09375378304847781, 0.08558729618228102]
Printing some Q and Qe and total Qs values:  [[-0.032]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]] [[26.391]
 [27.32 ]
 [27.32 ]
 [27.32 ]
 [27.32 ]
 [27.32 ]
 [27.32 ]] [[1.128]
 [1.207]
 [1.207]
 [1.207]
 [1.207]
 [1.207]
 [1.207]]
maxi score, test score, baseline:  -0.9954156950672646 -1.0 -0.9954156950672646
probs:  [0.07746698583812762, 0.11981866409725893, 0.08562648348438241, 0.5375243001569477, 0.09393708293890084, 0.08562648348438241]
Printing some Q and Qe and total Qs values:  [[0.351]
 [0.441]
 [0.351]
 [0.425]
 [0.439]
 [0.351]
 [0.473]] [[43.977]
 [41.767]
 [43.977]
 [46.694]
 [46.83 ]
 [43.977]
 [46.37 ]] [[1.433]
 [1.458]
 [1.433]
 [1.588]
 [1.606]
 [1.433]
 [1.627]]
Printing some Q and Qe and total Qs values:  [[0.674]
 [0.674]
 [0.674]
 [0.517]
 [0.344]
 [0.674]
 [0.674]] [[57.373]
 [57.373]
 [57.373]
 [65.917]
 [67.394]
 [57.373]
 [57.373]] [[1.985]
 [1.985]
 [1.985]
 [2.208]
 [2.101]
 [1.985]
 [1.985]]
printing an ep nov before normalisation:  32.71092787898368
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
printing an ep nov before normalisation:  41.7646880654465
printing an ep nov before normalisation:  43.51114077644426
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.07800959334641944, 0.12147344196326454, 0.07800959334641944, 0.5412118078560421, 0.09491220114185928, 0.08638336234599515]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.07868004240556062, 0.12251807766831774, 0.07868004240556062, 0.5458700326999367, 0.08712590241031212, 0.08712590241031212]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.07868004240556062, 0.12251807766831774, 0.07868004240556062, 0.5458700326999367, 0.08712590241031212, 0.08712590241031212]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.07868004240556062, 0.12251807766831774, 0.07868004240556062, 0.5458700326999367, 0.08712590241031212, 0.08712590241031212]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.07868004240556062, 0.12251807766831774, 0.07868004240556062, 0.5458700326999367, 0.08712590241031212, 0.08712590241031212]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.07868004240556062, 0.12251807766831774, 0.07868004240556062, 0.5458700326999367, 0.08712590241031212, 0.08712590241031212]
printing an ep nov before normalisation:  56.21460437774658
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.08418093197210162, 0.12666145212644256, 0.07614407680776669, 0.52828303435607, 0.09236525236880952, 0.09236525236880952]
printing an ep nov before normalisation:  64.45862851417944
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
printing an ep nov before normalisation:  66.97026434165967
printing an ep nov before normalisation:  70.14919417380638
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
printing an ep nov before normalisation:  4.99984271655336
printing an ep nov before normalisation:  46.5982238908852
printing an ep nov before normalisation:  24.349543335534786
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.08575846290376565, 0.1218574041110707, 0.07714024721012969, 0.5349506118758184, 0.09453481099544993, 0.08575846290376565]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.08575846290376565, 0.1218574041110707, 0.07714024721012969, 0.5349506118758184, 0.09453481099544993, 0.08575846290376565]
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.0857584708080236, 0.12185753386685214, 0.0771402260237087, 0.5349504499692309, 0.09453484852416093, 0.0857584708080236]
printing an ep nov before normalisation:  42.800626100965424
printing an ep nov before normalisation:  40.8609676361084
printing an ep nov before normalisation:  48.37445608994345
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.0857584708080236, 0.12185753386685214, 0.0771402260237087, 0.5349504499692309, 0.09453484852416093, 0.0857584708080236]
printing an ep nov before normalisation:  61.42439364957841
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.0857584708080236, 0.12185753386685214, 0.0771402260237087, 0.5349504499692309, 0.09453484852416093, 0.0857584708080236]
Printing some Q and Qe and total Qs values:  [[0.304]
 [0.304]
 [0.304]
 [0.303]
 [0.305]
 [0.304]
 [0.304]] [[33.129]
 [34.467]
 [33.129]
 [41.829]
 [42.252]
 [33.129]
 [33.129]] [[0.527]
 [0.543]
 [0.527]
 [0.631]
 [0.638]
 [0.527]
 [0.527]]
printing an ep nov before normalisation:  46.4967833841187
using another actor
from probs:  [0.08580127075768165, 0.12254630204001189, 0.07702880833442274, 0.5340876523159905, 0.09473469579421157, 0.08580127075768165]
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.07770989113837315, 0.12363054431819748, 0.07770989113837315, 0.5388170160825521, 0.0955726039349286, 0.08656005338757558]
printing an ep nov before normalisation:  43.59179737038794
from probs:  [0.07770987165358093, 0.1236306816092253, 0.07770987165358093, 0.5388168655313611, 0.09557264543449213, 0.08656006411775977]
printing an ep nov before normalisation:  26.94211483001709
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.07845669509954036, 0.11520036510336205, 0.07845669509954036, 0.5440027249083068, 0.09649143221150783, 0.08739208757774253]
printing an ep nov before normalisation:  55.82696199417114
printing an ep nov before normalisation:  80.97220308831234
maxi score, test score, baseline:  -0.9954947136563876 -1.0 -0.9954947136563876
maxi score, test score, baseline:  -0.9954947136563876 -1.0 -0.9954947136563876
probs:  [0.07845667815262364, 0.11520047368861756, 0.07845667815262364, 0.5440025919691613, 0.09649147687900611, 0.0873921011579677]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9954947136563876 -1.0 -0.9954947136563876
probs:  [0.07836667722725948, 0.1157839946191342, 0.07836667722725948, 0.5432847046422579, 0.096732057781987, 0.08746588850210191]
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.396]
 [0.396]
 [0.394]
 [0.398]
 [0.396]
 [0.396]] [[17.97 ]
 [17.97 ]
 [17.97 ]
 [21.02 ]
 [21.461]
 [17.97 ]
 [17.97 ]] [[0.396]
 [0.396]
 [0.396]
 [0.394]
 [0.398]
 [0.396]
 [0.396]]
printing an ep nov before normalisation:  63.40966195235508
printing an ep nov before normalisation:  57.50401431806686
maxi score, test score, baseline:  -0.9954947136563876 -1.0 -0.9954947136563876
probs:  [0.07836667722725948, 0.1157839946191342, 0.07836667722725948, 0.5432847046422579, 0.096732057781987, 0.08746588850210191]
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
probs:  [0.07900597065096142, 0.11747004226749522, 0.07900597065096142, 0.5476269207113696, 0.09788512506825084, 0.07900597065096142]
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
probs:  [0.08503600631361083, 0.1214909359716802, 0.07632913695331317, 0.529178892201085, 0.10292902224669997, 0.08503600631361083]
printing an ep nov before normalisation:  41.59659385681152
printing an ep nov before normalisation:  23.55368649349765
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
probs:  [0.08506560518117796, 0.12216690780608408, 0.07620435656317581, 0.5282216476979175, 0.10327587757046675, 0.08506560518117796]
maxi score, test score, baseline:  -0.9955331877729258 -1.0 -0.9955331877729258
printing an ep nov before normalisation:  16.47811564283584
siam score:  -0.858997
maxi score, test score, baseline:  -0.9955331877729258 -1.0 -0.9955331877729258
probs:  [0.08509589760073395, 0.12285872261548739, 0.07607665144765677, 0.5272419685631926, 0.10363086217219543, 0.08509589760073395]
printing an ep nov before normalisation:  38.16634895467007
maxi score, test score, baseline:  -0.9955331877729258 -1.0 -0.9955331877729258
probs:  [0.08509589760073395, 0.12285872261548739, 0.07607665144765677, 0.5272419685631926, 0.10363086217219543, 0.08509589760073395]
maxi score, test score, baseline:  -0.9955331877729258 -1.0 -0.9955331877729258
probs:  [0.08509589760073395, 0.12285872261548739, 0.07607665144765677, 0.5272419685631926, 0.10363086217219543, 0.08509589760073395]
printing an ep nov before normalisation:  36.230841433945514
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.08509590319735637, 0.12285885419767413, 0.07607662695397695, 0.5272417828478668, 0.1036309296057693, 0.08509590319735637]
from probs:  [0.08509590319735637, 0.12285885419767413, 0.07607662695397695, 0.5272417828478668, 0.1036309296057693, 0.08509590319735637]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.08512690248073629, 0.12356680852802625, 0.07594594277747721, 0.5262392479171497, 0.10399419581587399, 0.08512690248073629]
printing an ep nov before normalisation:  46.36579582601851
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.08512690248073629, 0.12356680852802625, 0.07594594277747721, 0.5262392479171497, 0.10399419581587399, 0.08512690248073629]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.08515863936470622, 0.12429160800687476, 0.07581214908633103, 0.5252128585525285, 0.1043661056248532, 0.08515863936470622]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.08519114656363674, 0.1250340036980477, 0.07567510702483783, 0.524161551818397, 0.10474704433144404, 0.08519114656363674]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.08519114656363674, 0.1250340036980477, 0.07567510702483783, 0.524161551818397, 0.10474704433144404, 0.08519114656363674]
printing an ep nov before normalisation:  90.8707088772451
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.09125901319936454, 0.1303034821256896, 0.07277335755725484, 0.5039820923927383, 0.1104230415255883, 0.09125901319936454]
UNIT TEST: sample policy line 217 mcts : [0.02  0.    0.    0.898 0.02  0.041 0.02 ]
siam score:  -0.8596714
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.0922716439663362, 0.13237824643096893, 0.07328311979060298, 0.5074173834135096, 0.11195699471732568, 0.08269261168125654]
printing an ep nov before normalisation:  71.60646840134415
printing an ep nov before normalisation:  70.74922455655037
actions average: 
K:  0  action  0 :  tensor([0.3249, 0.0012, 0.1201, 0.1652, 0.1450, 0.1200, 0.1236],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0040, 0.9462, 0.0045, 0.0183, 0.0118, 0.0097, 0.0054],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1928, 0.0055, 0.2561, 0.1561, 0.1166, 0.1439, 0.1290],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1597, 0.0413, 0.1352, 0.2128, 0.1610, 0.1341, 0.1560],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1914, 0.0027, 0.1275, 0.1793, 0.2311, 0.1317, 0.1363],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1796, 0.0029, 0.1408, 0.1824, 0.1295, 0.2299, 0.1350],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1600, 0.0437, 0.1197, 0.2430, 0.1498, 0.1233, 0.1604],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.855]
 [0.741]
 [0.741]
 [0.741]
 [0.741]
 [0.741]
 [0.741]] [[13.943]
 [12.682]
 [12.682]
 [12.682]
 [12.682]
 [12.682]
 [12.682]] [[1.49 ]
 [1.318]
 [1.318]
 [1.318]
 [1.318]
 [1.318]
 [1.318]]
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.49 ]
 [0.558]
 [0.558]
 [0.555]
 [0.558]
 [0.535]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.558]
 [0.49 ]
 [0.558]
 [0.558]
 [0.555]
 [0.558]
 [0.535]]
printing an ep nov before normalisation:  16.460969393237896
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.09259097553229115, 0.13413579446515556, 0.07292152586053681, 0.5047010236178809, 0.11298223987089905, 0.08266844065323664]
Printing some Q and Qe and total Qs values:  [[0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]] [[54.294]
 [54.294]
 [54.294]
 [54.294]
 [54.294]
 [54.294]
 [54.294]] [[2.324]
 [2.324]
 [2.324]
 [2.324]
 [2.324]
 [2.324]
 [2.324]]
siam score:  -0.85650045
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.696]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]] [[53.033]
 [55.088]
 [53.033]
 [53.033]
 [53.033]
 [53.033]
 [53.033]] [[1.917]
 [2.029]
 [1.917]
 [1.917]
 [1.917]
 [1.917]
 [1.917]]
using explorer policy with actor:  1
actions average: 
K:  0  action  0 :  tensor([0.3769, 0.0025, 0.0992, 0.1394, 0.1636, 0.1101, 0.1083],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0229, 0.7417, 0.0257, 0.0563, 0.0198, 0.0241, 0.1094],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1877, 0.0014, 0.1652, 0.1657, 0.1602, 0.1968, 0.1230],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2566, 0.0119, 0.1187, 0.1830, 0.1550, 0.1372, 0.1376],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2209, 0.0017, 0.1077, 0.1219, 0.3089, 0.1271, 0.1118],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1904, 0.0014, 0.1186, 0.1711, 0.1732, 0.2115, 0.1338],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2262, 0.0020, 0.1273, 0.1603, 0.1829, 0.1571, 0.1443],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.09935720135122696, 0.1397104349282641, 0.07095064876739164, 0.4910920770236853, 0.10917037406200636, 0.08971926386742564]
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.09935720135122696, 0.1397104349282641, 0.07095064876739164, 0.4910920770236853, 0.10917037406200636, 0.08971926386742564]
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.09935720135122696, 0.1397104349282641, 0.07095064876739164, 0.4910920770236853, 0.10917037406200636, 0.08971926386742564]
printing an ep nov before normalisation:  76.71139716466233
actions average: 
K:  4  action  0 :  tensor([0.3375, 0.0223, 0.1167, 0.1495, 0.1483, 0.1072, 0.1185],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0285, 0.8534, 0.0187, 0.0254, 0.0186, 0.0172, 0.0381],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1555, 0.0616, 0.2261, 0.1500, 0.1284, 0.1448, 0.1336],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1546, 0.0325, 0.1144, 0.2582, 0.1577, 0.1276, 0.1550],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1445, 0.0270, 0.1172, 0.1572, 0.2750, 0.1244, 0.1548],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1910, 0.0153, 0.1442, 0.1760, 0.1805, 0.1507, 0.1422],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2249, 0.0032, 0.1560, 0.1780, 0.1797, 0.1175, 0.1407],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.416]
 [0.424]
 [0.424]] [[54.905]
 [54.905]
 [54.905]
 [54.905]
 [62.048]
 [54.905]
 [54.905]] [[0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.735]
 [0.687]
 [0.687]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.61844064558684
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.09963236718864348, 0.14068000043859805, 0.07073699378242558, 0.48950765353294234, 0.109614405274428, 0.08982857978296248]
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.09963236718864348, 0.14068000043859805, 0.07073699378242558, 0.48950765353294234, 0.109614405274428, 0.08982857978296248]
printing an ep nov before normalisation:  56.87040724698934
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.09963236718864348, 0.14068000043859805, 0.07073699378242558, 0.48950765353294234, 0.109614405274428, 0.08982857978296248]
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
printing an ep nov before normalisation:  46.68109933198187
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.09991385776024597, 0.1416718515792497, 0.07051842790081597, 0.48788681179525833, 0.11006864262077641, 0.08994040834365373]
Printing some Q and Qe and total Qs values:  [[0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.171]
 [0.163]
 [0.142]] [[31.649]
 [31.649]
 [31.649]
 [31.649]
 [33.271]
 [31.649]
 [31.877]] [[0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.46 ]
 [0.428]
 [0.41 ]]
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
line 256 mcts: sample exp_bonus 14.462153055218197
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.10020189366393224, 0.14268676564188937, 0.07029477983734433, 0.4862282815930263, 0.11053344207675361, 0.09005483718705425]
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.488]
 [0.425]
 [0.425]] [[55.137]
 [55.137]
 [55.137]
 [55.137]
 [59.717]
 [55.137]
 [55.137]] [[1.544]
 [1.544]
 [1.544]
 [1.544]
 [1.788]
 [1.544]
 [1.544]]
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.09097737533387415, 0.14414927367476127, 0.07101459244799724, 0.4912153056120738, 0.11166607759741934, 0.09097737533387415]
printing an ep nov before normalisation:  40.732054710388184
printing an ep nov before normalisation:  5.976178927431874
printing an ep nov before normalisation:  20.839582240543848
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
printing an ep nov before normalisation:  58.79227161407471
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.0920547996112051, 0.14672828208332195, 0.07152826408658586, 0.49466019313897236, 0.11332775460944683, 0.08170070647046801]
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
printing an ep nov before normalisation:  79.23295604282733
actions average: 
K:  2  action  0 :  tensor([0.4286, 0.0025, 0.1263, 0.1268, 0.1265, 0.0935, 0.0958],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0046, 0.9341, 0.0138, 0.0176, 0.0077, 0.0106, 0.0116],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1927, 0.0101, 0.1633, 0.1841, 0.1618, 0.1561, 0.1320],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1663, 0.0079, 0.1517, 0.2555, 0.1462, 0.1267, 0.1455],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1934, 0.0103, 0.1284, 0.1798, 0.2391, 0.1226, 0.1264],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1787, 0.0264, 0.1758, 0.1701, 0.1528, 0.1610, 0.1351],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1442, 0.0126, 0.2069, 0.1855, 0.1507, 0.1685, 0.1316],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.09515928774566225, 0.1284490388469842, 0.07393950487990741, 0.511361964568276, 0.1171506990792628, 0.07393950487990741]
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.09622627049157063, 0.12988980947454576, 0.07476822517493746, 0.5171022029487615, 0.10724526673524719, 0.07476822517493746]
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.0976038745296249, 0.12052795098707257, 0.07548415163208788, 0.5219372200664423, 0.10896265115268454, 0.07548415163208788]
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.0976038745296249, 0.12052795098707257, 0.07548415163208788, 0.5219372200664423, 0.10896265115268454, 0.07548415163208788]
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.0976038745296249, 0.12052795098707257, 0.07548415163208788, 0.5219372200664423, 0.10896265115268454, 0.07548415163208788]
printing an ep nov before normalisation:  37.2459410345875
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.09902294289024127, 0.12266098199678241, 0.07621430866463158, 0.5268645148934719, 0.09902294289024127, 0.07621430866463158]
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.09902294289024127, 0.12266098199678241, 0.07621430866463158, 0.5268645148934719, 0.09902294289024127, 0.07621430866463158]
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.09902294289024127, 0.12266098199678241, 0.07621430866463158, 0.5268645148934719, 0.09902294289024127, 0.07621430866463158]
printing an ep nov before normalisation:  37.379629022134964
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.10659359403709358, 0.11790470448246082, 0.07384055248659521, 0.5105008602080101, 0.10659359403709358, 0.08456669474874669]
printing an ep nov before normalisation:  100.41174198196182
printing an ep nov before normalisation:  44.10957277834477
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.09680413468881027, 0.11990684557523805, 0.07450499635495375, 0.5149749080675696, 0.1082523530298526, 0.08555676228357566]
from probs:  [0.09680413468881027, 0.11990684557523805, 0.07450499635495375, 0.5149749080675696, 0.1082523530298526, 0.08555676228357566]
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.09706015721838644, 0.12060304133672602, 0.07433615602590184, 0.5136756583524245, 0.10872649711631388, 0.08559848995024752]
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.09706015721838644, 0.12060304133672602, 0.07433615602590184, 0.5136756583524245, 0.10872649711631388, 0.08559848995024752]
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.82522347699294
printing an ep nov before normalisation:  30.98558697219552
printing an ep nov before normalisation:  32.886162081110996
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.10312408251469377, 0.12520105396283393, 0.0714360157378376, 0.4937987541970746, 0.11406401033051337, 0.09237608325704676]
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.10312408251469377, 0.12520105396283393, 0.0714360157378376, 0.4937987541970746, 0.11406401033051337, 0.09237608325704676]
printing an ep nov before normalisation:  33.718342499133314
printing an ep nov before normalisation:  32.474174588454176
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.68428824253764
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
probs:  [0.10312422319110688, 0.12520135228909912, 0.07143593013235107, 0.4937981180765102, 0.1140642291280586, 0.09237614718287417]
printing an ep nov before normalisation:  40.737160397963486
Printing some Q and Qe and total Qs values:  [[0.527]
 [0.588]
 [0.462]
 [0.507]
 [0.507]
 [0.505]
 [0.508]] [[36.473]
 [44.697]
 [39.973]
 [38.066]
 [38.426]
 [39.607]
 [40.088]] [[0.684]
 [0.816]
 [0.649]
 [0.678]
 [0.681]
 [0.689]
 [0.696]]
printing an ep nov before normalisation:  25.228262605920836
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
probs:  [0.1046656508099945, 0.11593023217196162, 0.0720372082442972, 0.4978379829193369, 0.11593023217196162, 0.0935986936824482]
printing an ep nov before normalisation:  76.16464304121196
printing an ep nov before normalisation:  0.00014340071402330068
maxi score, test score, baseline:  -0.9956627118644068 -1.0 -0.9956627118644068
probs:  [0.09569722774628313, 0.11853023871017178, 0.07365156198804601, 0.5090121886988509, 0.11853023871017178, 0.08457854414647636]
printing an ep nov before normalisation:  29.264955520629883
printing an ep nov before normalisation:  65.04342291090224
maxi score, test score, baseline:  -0.9956627118644068 -1.0 -0.9956627118644068
probs:  [0.09592834100208025, 0.11918931349712482, 0.07346947100686504, 0.5076223022531814, 0.11918931349712482, 0.08460125874362368]
maxi score, test score, baseline:  -0.9956627118644068 -1.0 -0.9956627118644068
probs:  [0.09592834100208025, 0.11918931349712482, 0.07346947100686504, 0.5076223022531814, 0.11918931349712482, 0.08460125874362368]
printing an ep nov before normalisation:  83.1682756067708
maxi score, test score, baseline:  -0.9956983193277311 -1.0 -0.9956983193277311
probs:  [0.09732865142563767, 0.10921629716630495, 0.07416823817226846, 0.5123228435905653, 0.12131622229519833, 0.08564774735002537]
actions average: 
K:  0  action  0 :  tensor([0.3476, 0.0015, 0.1227, 0.1413, 0.1380, 0.1321, 0.1168],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0022, 0.9774, 0.0027, 0.0071, 0.0029, 0.0038, 0.0038],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1610, 0.0012, 0.2577, 0.1536, 0.1296, 0.1609, 0.1359],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1379, 0.0051, 0.1135, 0.3320, 0.1320, 0.1245, 0.1549],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1563, 0.0053, 0.1261, 0.1665, 0.2624, 0.1412, 0.1421],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1809, 0.0008, 0.1412, 0.1545, 0.1489, 0.2507, 0.1230],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2018, 0.0131, 0.1439, 0.1636, 0.1757, 0.1601, 0.1418],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  74.40835270404463
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9956983193277311 -1.0 -0.9956983193277311
probs:  [0.09732865142563767, 0.10921629716630495, 0.07416823817226846, 0.5123228435905653, 0.12131622229519833, 0.08564774735002537]
maxi score, test score, baseline:  -0.9956983193277311 -1.0 -0.9956983193277311
probs:  [0.09759781962522868, 0.10971464289345738, 0.07399090532678272, 0.5109570704305832, 0.12204783800576173, 0.08569172371818624]
maxi score, test score, baseline:  -0.9956983193277311 -1.0 -0.9956983193277311
probs:  [0.09759781962522868, 0.10971464289345738, 0.07399090532678272, 0.5109570704305832, 0.12204783800576173, 0.08569172371818624]
printing an ep nov before normalisation:  41.502312246638915
printing an ep nov before normalisation:  9.344696288482446
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.042]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]] [[54.586]
 [50.562]
 [54.586]
 [54.586]
 [54.586]
 [54.586]
 [54.586]] [[1.322]
 [1.196]
 [1.322]
 [1.322]
 [1.322]
 [1.322]
 [1.322]]
Printing some Q and Qe and total Qs values:  [[0.383]
 [0.202]
 [0.375]
 [0.415]
 [0.411]
 [0.406]
 [0.481]] [[46.866]
 [50.778]
 [49.167]
 [50.133]
 [52.505]
 [49.864]
 [44.117]] [[1.302]
 [1.275]
 [1.384]
 [1.462]
 [1.552]
 [1.442]
 [1.291]]
maxi score, test score, baseline:  -0.9956983193277311 -1.0 -0.9956983193277311
probs:  [0.08678957126056121, 0.11158072010672052, 0.07471457203807831, 0.5158172464893747, 0.12430831884470414, 0.08678957126056121]
maxi score, test score, baseline:  -0.9956983193277311 -1.0 -0.9956983193277311
probs:  [0.08678957126056121, 0.11158072010672052, 0.07471457203807831, 0.5158172464893747, 0.12430831884470414, 0.08678957126056121]
printing an ep nov before normalisation:  26.03394308879814
siam score:  -0.8605001
maxi score, test score, baseline:  -0.9956983193277311 -1.0 -0.9956983193277311
probs:  [0.08797870732594232, 0.10067155847618195, 0.0755046984369138, 0.5211290934895957, 0.12673723494542388, 0.08797870732594232]
maxi score, test score, baseline:  -0.9956983193277311 -1.0 -0.9956983193277311
probs:  [0.08797870732594232, 0.10067155847618195, 0.0755046984369138, 0.5211290934895957, 0.12673723494542388, 0.08797870732594232]
printing an ep nov before normalisation:  46.82558908366008
maxi score, test score, baseline:  -0.9956983193277311 -1.0 -0.9956983193277311
probs:  [0.08816886347062684, 0.10138504406289876, 0.07518054806098055, 0.518571266012839, 0.128525414922028, 0.08816886347062684]
printing an ep nov before normalisation:  31.601855754852295
printing an ep nov before normalisation:  60.70662868978229
maxi score, test score, baseline:  -0.9956983193277311 -1.0 -0.9956983193277311
probs:  [0.08826832489515067, 0.10175823364848753, 0.07501100077549201, 0.5172333909473437, 0.12946072483837542, 0.08826832489515067]
maxi score, test score, baseline:  -0.9956983193277311 -1.0 -0.9956983193277311
probs:  [0.08951837639598165, 0.10319961733182513, 0.07607301892454942, 0.5245676068267219, 0.1171230041249402, 0.08951837639598165]
printing an ep nov before normalisation:  84.67704346589348
maxi score, test score, baseline:  -0.99571589958159 -1.0 -0.99571589958159
probs:  [0.08951840223608253, 0.10319970120758103, 0.07607298772960935, 0.5245673595271138, 0.11712314706353073, 0.08951840223608253]
printing an ep nov before normalisation:  20.749154943394206
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
probs:  [0.08964901734644071, 0.10362080085229491, 0.07591812665965353, 0.5233231652333783, 0.11783987256179189, 0.08964901734644071]
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
probs:  [0.09089627180238861, 0.10506272899787297, 0.07697406386889552, 0.5306129515129178, 0.11947991994902953, 0.07697406386889552]
siam score:  -0.85648346
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.268]
 [0.268]
 [0.477]
 [0.268]
 [0.268]
 [0.268]] [[40.263]
 [40.263]
 [40.263]
 [41.671]
 [40.263]
 [40.263]
 [40.263]] [[1.495]
 [1.495]
 [1.495]
 [1.77 ]
 [1.495]
 [1.495]
 [1.495]]
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.387]
 [0.637]
 [0.449]
 [0.637]
 [0.637]
 [0.456]] [[34.339]
 [31.659]
 [28.372]
 [35.779]
 [28.372]
 [28.372]
 [33.931]] [[0.465]
 [0.387]
 [0.637]
 [0.449]
 [0.637]
 [0.637]
 [0.456]]
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
probs:  [0.09106014840367808, 0.10553492810829047, 0.07683493386638708, 0.5294691571511224, 0.12026589860413475, 0.07683493386638708]
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
printing an ep nov before normalisation:  51.464906720810276
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
probs:  [0.09122951354589516, 0.10602294207038912, 0.07669114413389289, 0.5282870549276933, 0.12107820118823653, 0.07669114413389289]
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.619]
 [0.619]
 [0.614]
 [0.629]
 [0.636]
 [0.625]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.617]
 [0.619]
 [0.619]
 [0.614]
 [0.629]
 [0.636]
 [0.625]]
maxi score, test score, baseline:  -0.995750622406639 -1.0 -0.995750622406639
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.995750622406639 -1.0 -0.995750622406639
probs:  [0.09140468368353234, 0.10652768324830406, 0.07654242549056636, 0.5270644355648438, 0.12191834652218708, 0.07654242549056636]
printing an ep nov before normalisation:  29.537343978881836
maxi score, test score, baseline:  -0.995750622406639 -1.0 -0.995750622406639
probs:  [0.09140468368353234, 0.10652768324830406, 0.07654242549056636, 0.5270644355648438, 0.12191834652218708, 0.07654242549056636]
printing an ep nov before normalisation:  39.84123270245846
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
probs:  [0.09140471941069342, 0.10652778671168084, 0.07654239464937794, 0.5270641856582192, 0.12191851892065064, 0.07654239464937794]
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
probs:  [0.09140471941069342, 0.10652778671168084, 0.07654239464937794, 0.5270641856582192, 0.12191851892065064, 0.07654239464937794]
Printing some Q and Qe and total Qs values:  [[0.255]
 [0.31 ]
 [0.281]
 [0.262]
 [0.266]
 [0.264]
 [0.297]] [[24.229]
 [20.986]
 [22.858]
 [23.349]
 [25.225]
 [25.915]
 [24.629]] [[1.37 ]
 [1.276]
 [1.333]
 [1.337]
 [1.428]
 [1.458]
 [1.431]]
printing an ep nov before normalisation:  26.862834253879303
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
probs:  [0.09158592525470191, 0.10704991886180848, 0.0763885522270285, 0.5257994399586221, 0.12278761147081062, 0.0763885522270285]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
probs:  [0.09158592525470191, 0.10704991886180848, 0.0763885522270285, 0.5257994399586221, 0.12278761147081062, 0.0763885522270285]
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
probs:  [0.09158592525470191, 0.10704991886180848, 0.0763885522270285, 0.5257994399586221, 0.12278761147081062, 0.0763885522270285]
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
probs:  [0.09158592525470191, 0.10704991886180848, 0.0763885522270285, 0.5257994399586221, 0.12278761147081062, 0.0763885522270285]
printing an ep nov before normalisation:  23.43330811717232
Sims:  50 1 epoch:  36406 pick best:  False frame count:  36406
printing an ep nov before normalisation:  22.5356388092041
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
probs:  [0.10004566965662391, 0.11423785103722969, 0.0723891110687773, 0.4985479657101071, 0.1286812214688193, 0.08609818105844266]
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
probs:  [0.10004566965662391, 0.11423785103722969, 0.0723891110687773, 0.4985479657101071, 0.1286812214688193, 0.08609818105844266]
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.89247468201534
printing an ep nov before normalisation:  41.8468498683311
Printing some Q and Qe and total Qs values:  [[0.161]
 [0.233]
 [0.135]
 [0.137]
 [0.128]
 [0.141]
 [0.149]] [[51.031]
 [40.966]
 [49.179]
 [46.753]
 [47.186]
 [48.153]
 [48.82 ]] [[1.494]
 [1.067]
 [1.376]
 [1.258]
 [1.27 ]
 [1.331]
 [1.372]]
printing an ep nov before normalisation:  52.15569083934244
printing an ep nov before normalisation:  33.722905759349096
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
probs:  [0.10336232495315466, 0.08870172193573375, 0.07429172751690177, 0.5114805335722103, 0.13346197008626587, 0.08870172193573375]
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
probs:  [0.10336232495315466, 0.08870172193573375, 0.07429172751690177, 0.5114805335722103, 0.13346197008626587, 0.08870172193573375]
printing an ep nov before normalisation:  44.577422586078264
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]] [[54.029]
 [54.029]
 [54.029]
 [54.029]
 [54.029]
 [54.029]
 [54.029]] [[2.044]
 [2.044]
 [2.044]
 [2.044]
 [2.044]
 [2.044]
 [2.044]]
printing an ep nov before normalisation:  31.301381674843558
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
probs:  [0.10336232495315466, 0.08870172193573375, 0.07429172751690177, 0.5114805335722103, 0.13346197008626587, 0.08870172193573375]
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.315]
 [0.32 ]
 [0.326]
 [0.326]
 [0.326]
 [0.315]] [[32.518]
 [32.518]
 [31.938]
 [32.01 ]
 [32.461]
 [31.974]
 [32.518]] [[2.321]
 [2.321]
 [2.258]
 [2.272]
 [2.326]
 [2.268]
 [2.321]]
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
probs:  [0.10336232495315466, 0.08870172193573375, 0.07429172751690177, 0.5114805335722103, 0.13346197008626587, 0.08870172193573375]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
probs:  [0.103797690050215, 0.08881788220793511, 0.07409413945697652, 0.5099197123151396, 0.13455269376179865, 0.08881788220793511]
printing an ep nov before normalisation:  14.229741730167778
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
probs:  [0.103797690050215, 0.08881788220793511, 0.07409413945697652, 0.5099197123151396, 0.13455269376179865, 0.08881788220793511]
from probs:  [0.1053480858950322, 0.09014424147265628, 0.07520029182673149, 0.5175440304644252, 0.13656305851442332, 0.07520029182673149]
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.601]
 [0.534]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [67.738]
 [ 0.   ]] [[0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [1.965]
 [0.329]]
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
probs:  [0.10583971800356332, 0.09029582758682446, 0.07501764469857716, 0.516076326861254, 0.13775283815120373, 0.07501764469857716]
printing an ep nov before normalisation:  24.867642072030385
Printing some Q and Qe and total Qs values:  [[-0.035]
 [-0.031]
 [-0.035]
 [-0.035]
 [-0.044]
 [-0.035]
 [-0.035]] [[24.718]
 [22.228]
 [24.718]
 [24.718]
 [24.251]
 [24.718]
 [24.718]] [[1.69 ]
 [1.385]
 [1.69 ]
 [1.69 ]
 [1.623]
 [1.69 ]
 [1.69 ]]
printing an ep nov before normalisation:  34.038446903781065
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
probs:  [0.1148591234415771, 0.10038614596057842, 0.0861605698040416, 0.496832042877811, 0.12958601280820722, 0.07217610510778456]
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
probs:  [0.1148591234415771, 0.10038614596057842, 0.0861605698040416, 0.496832042877811, 0.12958601280820722, 0.07217610510778456]
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.12240353890001981, 0.1085066337421479, 0.09484728251859874, 0.4694810085718141, 0.13654424941153837, 0.06821728685588113]
using explorer policy with actor:  1
siam score:  -0.8543698
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.12488571115644544, 0.11051688265395714, 0.08250984404741278, 0.4737215831301389, 0.1395066243695033, 0.0688593546425425]
printing an ep nov before normalisation:  52.94170263172464
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]] [[55.171]
 [55.171]
 [55.171]
 [55.171]
 [55.171]
 [55.171]
 [55.171]] [[0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]]
line 256 mcts: sample exp_bonus 25.277491909057204
printing an ep nov before normalisation:  43.411661236729934
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.11212739503123922, 0.11212739503123922, 0.08371172996674435, 0.4806313009090166, 0.14154010097519001, 0.06986207808657063]
from probs:  [0.11212739503123922, 0.11212739503123922, 0.08371172996674435, 0.4806313009090166, 0.14154010097519001, 0.06986207808657063]
printing an ep nov before normalisation:  38.70392048339317
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.577]
 [0.558]
 [0.57 ]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [54.242]
 [ 0.   ]
 [34.638]] [[0.247]
 [0.247]
 [0.247]
 [0.247]
 [1.491]
 [0.247]
 [1.041]]
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.09955003174505495, 0.11439127488437616, 0.08496033509962031, 0.4856276356081418, 0.14485487922298323, 0.07061584343982356]
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.10110259162927261, 0.11617560563199691, 0.08628505244015347, 0.49320944376677434, 0.13151075900868175, 0.07171654752312084]
Printing some Q and Qe and total Qs values:  [[0.36 ]
 [0.36 ]
 [0.36 ]
 [0.361]
 [0.359]
 [0.36 ]
 [0.36 ]] [[18.89 ]
 [18.89 ]
 [18.89 ]
 [19.358]
 [19.421]
 [18.89 ]
 [18.89 ]] [[0.36 ]
 [0.36 ]
 [0.36 ]
 [0.361]
 [0.359]
 [0.36 ]
 [0.36 ]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.10110259162927261, 0.11617560563199691, 0.08628505244015347, 0.49320944376677434, 0.13151075900868175, 0.07171654752312084]
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.10110259162927261, 0.11617560563199691, 0.08628505244015347, 0.49320944376677434, 0.13151075900868175, 0.07171654752312084]
printing an ep nov before normalisation:  36.03549618115078
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
probs:  [0.1011026687956644, 0.11617574861626666, 0.0862850649042251, 0.49320905235604995, 0.1315109689554882, 0.07171649637230569]
printing an ep nov before normalisation:  34.11870352140011
actions average: 
K:  3  action  0 :  tensor([0.2465, 0.0012, 0.1647, 0.1669, 0.1485, 0.1467, 0.1255],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0487, 0.6715, 0.0390, 0.0590, 0.0521, 0.0366, 0.0931],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1375, 0.0263, 0.2676, 0.1435, 0.1362, 0.1734, 0.1154],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0803, 0.1051, 0.1086, 0.3349, 0.1187, 0.1075, 0.1450],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1492, 0.0299, 0.1261, 0.1404, 0.3088, 0.1221, 0.1234],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1721, 0.0404, 0.1753, 0.1430, 0.1338, 0.1965, 0.1389],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1636, 0.0295, 0.1564, 0.1805, 0.1794, 0.1467, 0.1438],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
probs:  [0.1011026687956644, 0.11617574861626666, 0.0862850649042251, 0.49320905235604995, 0.1315109689554882, 0.07171649637230569]
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
printing an ep nov before normalisation:  51.91570281982422
siam score:  -0.8537755
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.676]
 [0.654]
 [0.633]
 [0.643]
 [0.622]
 [0.644]] [[32.229]
 [34.53 ]
 [32.209]
 [31.843]
 [32.811]
 [32.296]
 [30.113]] [[1.528]
 [1.663]
 [1.52 ]
 [1.48 ]
 [1.541]
 [1.493]
 [1.401]]
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
probs:  [0.1014830128612866, 0.11687933081134978, 0.08634764945274975, 0.4912799341620765, 0.13254341081271875, 0.07146666189981875]
line 256 mcts: sample exp_bonus 24.569801979160168
printing an ep nov before normalisation:  38.55070916029465
printing an ep nov before normalisation:  29.250929996887667
maxi score, test score, baseline:  -0.9958183673469387 -1.0 -0.9958183673469387
probs:  [0.1014830923270099, 0.1168794780521558, 0.08634766229212007, 0.4912795310888026, 0.1325436270073049, 0.07146660923260674]
siam score:  -0.8521538
printing an ep nov before normalisation:  25.840182449336453
maxi score, test score, baseline:  -0.9958183673469387 -1.0 -0.9958183673469387
probs:  [0.1014830923270099, 0.1168794780521558, 0.08634766229212007, 0.4912795310888026, 0.1325436270073049, 0.07146660923260674]
printing an ep nov before normalisation:  17.34831690788269
siam score:  -0.85280615
line 256 mcts: sample exp_bonus 51.707695825885395
printing an ep nov before normalisation:  55.97372177919979
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
printing an ep nov before normalisation:  39.33874184860457
maxi score, test score, baseline:  -0.9958183673469387 -1.0 -0.9958183673469387
probs:  [0.10187615237269788, 0.11760658299929788, 0.08641233921434506, 0.489285917005751, 0.1336105863324478, 0.0712084220754603]
maxi score, test score, baseline:  -0.9958183673469387 -1.0 -0.9958183673469387
maxi score, test score, baseline:  -0.9958183673469387 -1.0 -0.9958183673469387
probs:  [0.10187615237269788, 0.11760658299929788, 0.08641233921434506, 0.489285917005751, 0.1336105863324478, 0.0712084220754603]
printing an ep nov before normalisation:  13.243818283081055
printing an ep nov before normalisation:  14.452394748023423
maxi score, test score, baseline:  -0.9958349593495935 -1.0 -0.9958349593495935
maxi score, test score, baseline:  -0.9958349593495935 -1.0 -0.9958349593495935
probs:  [0.10350360285387426, 0.10350360285387426, 0.08779237790255244, 0.4971094403966707, 0.13574576884093567, 0.07234520715209264]
using another actor
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.08919275738341667, 0.10515504140551017, 0.08919275738341667, 0.5050482725170489, 0.13791242426824016, 0.07349874704236765]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.08960380865684375, 0.10669228730901956, 0.08960380865684375, 0.49953674663812453, 0.14176081741261534, 0.07280253132655294]
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.08960380865684375, 0.10669228730901956, 0.08960380865684375, 0.49953674663812453, 0.14176081741261534, 0.07280253132655294]
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.08960380865684375, 0.10669228730901956, 0.08960380865684375, 0.49953674663812453, 0.14176081741261534, 0.07280253132655294]
printing an ep nov before normalisation:  27.621576429717663
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.0912159334774825, 0.10861232071810831, 0.0912159334774825, 0.5085352441641454, 0.1263086456697797, 0.07411192249300166]
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.0912159334774825, 0.10861232071810831, 0.0912159334774825, 0.5085352441641454, 0.1263086456697797, 0.07411192249300166]
printing an ep nov before normalisation:  41.540775299226304
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.0912159334774825, 0.10861232071810831, 0.0912159334774825, 0.5085352441641454, 0.1263086456697797, 0.07411192249300166]
printing an ep nov before normalisation:  61.44197490407547
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.0912159334774825, 0.10861232071810831, 0.0912159334774825, 0.5085352441641454, 0.1263086456697797, 0.07411192249300166]
siam score:  -0.860981
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.10067741968824712, 0.11647938528702474, 0.10067741968824712, 0.4797483899973101, 0.1325537985685402, 0.0698635867706307]
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.10067749432560243, 0.11647952831537779, 0.10067749432560243, 0.4797479438208306, 0.13255401116704568, 0.06986352804554101]
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.10843091022208749, 0.12292635482708933, 0.10843091022208749, 0.45615796267664055, 0.13767172089079804, 0.06638214116129702]
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.10843091022208749, 0.12292635482708933, 0.10843091022208749, 0.45615796267664055, 0.13767172089079804, 0.06638214116129702]
printing an ep nov before normalisation:  59.1763629146572
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.10893107047154453, 0.12371577341056968, 0.10893107047154453, 0.453623487439772, 0.13875538502095688, 0.06604321318561242]
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.11224966003263272, 0.12748533110815555, 0.09727005065585792, 0.4674560046452812, 0.12748533110815555, 0.06805362244991706]
printing an ep nov before normalisation:  39.168710522270445
printing an ep nov before normalisation:  71.1503075138518
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.415]
 [0.415]
 [0.463]
 [0.415]
 [0.489]
 [0.456]] [[52.402]
 [52.402]
 [52.402]
 [62.75 ]
 [52.402]
 [63.931]
 [59.308]] [[1.294]
 [1.294]
 [1.294]
 [1.642]
 [1.294]
 [1.702]
 [1.536]]
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.1228544794199361, 0.1228544794199361, 0.09415294539376837, 0.45617243567829086, 0.1375732148179709, 0.06639244527009769]
from probs:  [0.12285463432497255, 0.12285463432497255, 0.0941529875177969, 0.4561719379619795, 0.13757342755942165, 0.06639237831085688]
using explorer policy with actor:  1
printing an ep nov before normalisation:  54.19472994515499
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.12552335352804472, 0.12552335352804472, 0.09580470550610151, 0.4605647945232465, 0.12552335352804472, 0.06706043938651782]
printing an ep nov before normalisation:  58.529417775063315
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
printing an ep nov before normalisation:  31.75984701833371
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.12552335352804472, 0.12552335352804472, 0.09580470550610151, 0.4605647945232465, 0.12552335352804472, 0.06706043938651782]
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.12552335352804472, 0.12552335352804472, 0.09580470550610151, 0.4605647945232465, 0.12552335352804472, 0.06706043938651782]
siam score:  -0.8535489
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.12552335352804472, 0.12552335352804472, 0.09580470550610151, 0.4605647945232465, 0.12552335352804472, 0.06706043938651782]
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.12552335352804472, 0.12552335352804472, 0.09580470550610151, 0.4605647945232465, 0.12552335352804472, 0.06706043938651782]
printing an ep nov before normalisation:  37.2705522906639
printing an ep nov before normalisation:  52.81910492099192
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
printing an ep nov before normalisation:  32.97323226928711
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
probs:  [0.12638345862184142, 0.12638345862184142, 0.0960584735637695, 0.458063400259826, 0.12638345862184142, 0.06672775031088043]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
printing an ep nov before normalisation:  42.3682829082418
printing an ep nov before normalisation:  59.72586457895423
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
probs:  [0.12638345862184142, 0.12638345862184142, 0.0960584735637695, 0.458063400259826, 0.12638345862184142, 0.06672775031088043]
printing an ep nov before normalisation:  36.71504789778773
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
probs:  [0.1272706236840486, 0.1272706236840486, 0.09632022567216983, 0.4554833084674623, 0.1272706236840486, 0.06638459480822213]
printing an ep nov before normalisation:  31.009127651648534
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
probs:  [0.1323677473443874, 0.09974008867840657, 0.09974008867840657, 0.4676021386414624, 0.1323677473443874, 0.06818218931294964]
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
probs:  [0.1323677473443874, 0.09974008867840657, 0.09974008867840657, 0.4676021386414624, 0.1323677473443874, 0.06818218931294964]
printing an ep nov before normalisation:  29.05935511104227
printing an ep nov before normalisation:  16.204383373260498
printing an ep nov before normalisation:  40.2902461200095
printing an ep nov before normalisation:  63.759094626952106
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.561]
 [0.561]
 [0.562]
 [0.562]
 [0.561]
 [0.561]] [[25.783]
 [25.783]
 [25.783]
 [25.58 ]
 [25.818]
 [25.783]
 [25.783]] [[0.561]
 [0.561]
 [0.561]
 [0.562]
 [0.562]
 [0.561]
 [0.561]]
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
probs:  [0.13450726170852637, 0.08518493469621952, 0.10135169743914225, 0.4751656435968639, 0.13450726170852637, 0.06928320085072183]
printing an ep nov before normalisation:  45.46688013808955
printing an ep nov before normalisation:  55.39350341224274
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
probs:  [0.12358954384658992, 0.0882994455638982, 0.0882994455638982, 0.48704007801983723, 0.14168319593220693, 0.07108829107356952]
maxi score, test score, baseline:  -0.995978431372549 -1.0 -0.995978431372549
probs:  [0.1235897311386569, 0.0882994682541579, 0.0882994682541579, 0.48703963124873567, 0.14168346761757336, 0.07108823348671811]
printing an ep nov before normalisation:  60.25924001421247
maxi score, test score, baseline:  -0.995978431372549 -1.0 -0.995978431372549
probs:  [0.1235897311386569, 0.0882994682541579, 0.0882994682541579, 0.48703963124873567, 0.14168346761757336, 0.07108823348671811]
from probs:  [0.1235897311386569, 0.0882994682541579, 0.0882994682541579, 0.48703963124873567, 0.14168346761757336, 0.07108823348671811]
maxi score, test score, baseline:  -0.995978431372549 -1.0 -0.995978431372549
probs:  [0.13324812823507112, 0.09999660766601569, 0.08377967755241918, 0.46485257184673395, 0.15029657734039126, 0.06782643735936893]
printing an ep nov before normalisation:  45.28848292652781
maxi score, test score, baseline:  -0.995978431372549 -1.0 -0.995978431372549
probs:  [0.13324812823507112, 0.09999660766601569, 0.08377967755241918, 0.46485257184673395, 0.15029657734039126, 0.06782643735936893]
maxi score, test score, baseline:  -0.995978431372549 -1.0 -0.995978431372549
probs:  [0.13324812823507112, 0.09999660766601569, 0.08377967755241918, 0.46485257184673395, 0.15029657734039126, 0.06782643735936893]
printing an ep nov before normalisation:  31.442603335043934
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.995978431372549 -1.0 -0.995978431372549
probs:  [0.13324812823507112, 0.09999660766601569, 0.08377967755241918, 0.46485257184673395, 0.15029657734039126, 0.06782643735936893]
maxi score, test score, baseline:  -0.995978431372549 -1.0 -0.995978431372549
printing an ep nov before normalisation:  20.092828766752575
printing an ep nov before normalisation:  56.395006842891625
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
probs:  [0.13434473173348035, 0.10036220343167829, 0.08378875725170147, 0.4622515324053065, 0.1517679771763535, 0.0674847980014799]
line 256 mcts: sample exp_bonus 27.785382845198153
printing an ep nov before normalisation:  48.69562669419473
printing an ep nov before normalisation:  27.964127320667195
printing an ep nov before normalisation:  35.84631532918732
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
probs:  [0.1365709869972582, 0.10202476795583769, 0.06860200319218716, 0.4699169952923398, 0.1542832433701899, 0.06860200319218716]
siam score:  -0.8502573
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
probs:  [0.14393723928953295, 0.09639226040428331, 0.0659736986443654, 0.4523146778431253, 0.16032279698444965, 0.08105932683424348]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
probs:  [0.14393723928953295, 0.09639226040428331, 0.0659736986443654, 0.4523146778431253, 0.16032279698444965, 0.08105932683424348]
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.434]] [[56.964]
 [56.964]
 [56.964]
 [56.964]
 [56.964]
 [56.964]
 [56.964]] [[1.033]
 [1.033]
 [1.033]
 [1.033]
 [1.033]
 [1.033]
 [1.033]]
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
probs:  [0.14393723928953295, 0.09639226040428331, 0.0659736986443654, 0.4523146778431253, 0.16032279698444965, 0.08105932683424348]
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
probs:  [0.1463347492462336, 0.09799709734541896, 0.06707139532285478, 0.4598534767409859, 0.1463347492462336, 0.08240853209827288]
from probs:  [0.1463347492462336, 0.09799709734541896, 0.06707139532285478, 0.4598534767409859, 0.1463347492462336, 0.08240853209827288]
printing an ep nov before normalisation:  51.477010318496774
siam score:  -0.8442806
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
probs:  [0.1510045670216196, 0.08503632734421596, 0.06920926984096833, 0.4745359989306081, 0.1510045670216196, 0.06920926984096833]
printing an ep nov before normalisation:  42.71139901533811
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
printing an ep nov before normalisation:  21.837835889410684
actor:  1 policy actor:  1  step number:  80 total reward:  0.059999999999999165  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
probs:  [0.10292864845302088, 0.08384955047372801, 0.07927210559563157, 0.5517489414289672, 0.10292864845302088, 0.07927210559563157]
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
probs:  [0.10330243691026958, 0.0838587776192247, 0.07919386742641352, 0.5511486137074092, 0.10330243691026958, 0.07919386742641352]
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
printing an ep nov before normalisation:  13.603827357292175
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
probs:  [0.10330243691026958, 0.0838587776192247, 0.07919386742641352, 0.5511486137074092, 0.10330243691026958, 0.07919386742641352]
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
probs:  [0.10616147600066378, 0.0868868616153016, 0.0777121451678693, 0.540815532648301, 0.10616147600066378, 0.08226250856720058]
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
probs:  [0.10616147600066378, 0.0868868616153016, 0.0777121451678693, 0.540815532648301, 0.10616147600066378, 0.08226250856720058]
printing an ep nov before normalisation:  38.55735932513467
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.10664659475683237, 0.08728375487516901, 0.07806704309149716, 0.543288969428172, 0.10664659475683237, 0.07806704309149716]
using explorer policy with actor:  0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.381]
 [0.381]
 [0.381]
 [0.381]
 [0.381]
 [0.381]] [[33.575]
 [33.575]
 [33.575]
 [33.575]
 [33.575]
 [33.575]
 [33.575]] [[1.655]
 [1.655]
 [1.655]
 [1.655]
 [1.655]
 [1.655]
 [1.655]]
printing an ep nov before normalisation:  29.216898921137183
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
printing an ep nov before normalisation:  50.488541904923245
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.10686325274623186, 0.08732020985002949, 0.07801772143143729, 0.5429178417946323, 0.10686325274623186, 0.07801772143143729]
printing an ep nov before normalisation:  71.25845353155388
Printing some Q and Qe and total Qs values:  [[0.467]
 [0.522]
 [0.456]
 [0.458]
 [0.458]
 [0.458]
 [0.457]] [[62.081]
 [63.743]
 [63.148]
 [65.773]
 [64.553]
 [65.979]
 [63.535]] [[1.449]
 [1.563]
 [1.476]
 [1.57 ]
 [1.527]
 [1.578]
 [1.49 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.10686325274623186, 0.08732020985002949, 0.07801772143143729, 0.5429178417946323, 0.10686325274623186, 0.07801772143143729]
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.10686325274623186, 0.08732020985002949, 0.07801772143143729, 0.5429178417946323, 0.10686325274623186, 0.07801772143143729]
printing an ep nov before normalisation:  39.06487226486206
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.10686325274623186, 0.08732020985002949, 0.07801772143143729, 0.5429178417946323, 0.10686325274623186, 0.07801772143143729]
printing an ep nov before normalisation:  46.588410776055845
printing an ep nov before normalisation:  43.52572898061972
printing an ep nov before normalisation:  49.57594461517456
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.97785466194479
printing an ep nov before normalisation:  33.463110547311096
printing an ep nov before normalisation:  34.31927528080491
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.97200558179297
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.1118506347690857, 0.09298101815750395, 0.07530003881279354, 0.52401859284114, 0.1118506347690857, 0.08399908065039113]
actions average: 
K:  3  action  0 :  tensor([0.3794, 0.0107, 0.1181, 0.1532, 0.1272, 0.1059, 0.1055],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0114, 0.8883, 0.0264, 0.0159, 0.0059, 0.0106, 0.0414],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1533, 0.0169, 0.2355, 0.1688, 0.1370, 0.1607, 0.1277],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1932, 0.0427, 0.1571, 0.1676, 0.1286, 0.1419, 0.1689],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1971, 0.0195, 0.1024, 0.1291, 0.3294, 0.1065, 0.1160],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.2242, 0.0098, 0.1748, 0.1834, 0.1339, 0.1310, 0.1429],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1853, 0.0096, 0.1576, 0.1832, 0.1537, 0.1406, 0.1700],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  17.612200339431897
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.11234296037926528, 0.09339018756421408, 0.07563129020207959, 0.526327294654956, 0.11234296037926528, 0.07996530682021966]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
probs:  [0.11289109220418797, 0.09384570443223157, 0.07600002612622518, 0.5288972442226354, 0.10801071158762413, 0.08035522142709572]
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
probs:  [0.11289109220418797, 0.09384570443223157, 0.07600002612622518, 0.5288972442226354, 0.10801071158762413, 0.08035522142709572]
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
probs:  [0.11367801229133057, 0.08974243553095711, 0.07628574119481016, 0.5308623780227885, 0.10873132642752006, 0.08070010653259377]
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
probs:  [0.11367801229133057, 0.08974243553095711, 0.07628574119481016, 0.5308623780227885, 0.10873132642752006, 0.08070010653259377]
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
probs:  [0.11367801229133057, 0.08974243553095711, 0.07628574119481016, 0.5308623780227885, 0.10873132642752006, 0.08070010653259377]
Printing some Q and Qe and total Qs values:  [[0.526]
 [0.487]
 [0.521]
 [0.522]
 [0.519]
 [0.521]
 [0.521]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.526]
 [0.487]
 [0.521]
 [0.522]
 [0.519]
 [0.521]
 [0.521]]
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
probs:  [0.11418186741940738, 0.0901400765071739, 0.07662366807305213, 0.5332174892964352, 0.10921323063087907, 0.07662366807305213]
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]] [[46.682]
 [46.682]
 [46.682]
 [46.682]
 [46.682]
 [46.682]
 [46.682]] [[1.876]
 [1.876]
 [1.876]
 [1.876]
 [1.876]
 [1.876]
 [1.876]]
siam score:  -0.8503688
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
probs:  [0.11418186741940738, 0.0901400765071739, 0.07662366807305213, 0.5332174892964352, 0.10921323063087907, 0.07662366807305213]
actions average: 
K:  0  action  0 :  tensor([0.3397, 0.0036, 0.1305, 0.1388, 0.1539, 0.1250, 0.1084],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0437, 0.8272, 0.0227, 0.0195, 0.0097, 0.0397, 0.0375],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2051, 0.0278, 0.2186, 0.1313, 0.1239, 0.1728, 0.1204],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1685, 0.0130, 0.1364, 0.2550, 0.1524, 0.1468, 0.1278],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2181, 0.0018, 0.1305, 0.1477, 0.2335, 0.1381, 0.1303],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1916, 0.0092, 0.1392, 0.1474, 0.1355, 0.2645, 0.1125],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1638, 0.0060, 0.1591, 0.1694, 0.1531, 0.1624, 0.1862],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
probs:  [0.11418186741940738, 0.0901400765071739, 0.07662366807305213, 0.5332174892964352, 0.10921323063087907, 0.07662366807305213]
printing an ep nov before normalisation:  63.22895098655585
Starting evaluation
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
probs:  [0.11502056801105286, 0.09032442940519399, 0.07644014360630959, 0.5318580160052918, 0.10991669936584206, 0.07644014360630959]
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
Printing some Q and Qe and total Qs values:  [[0.201]
 [0.201]
 [0.201]
 [0.224]
 [0.201]
 [0.225]
 [0.201]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.201]
 [0.201]
 [0.201]
 [0.224]
 [0.201]
 [0.225]
 [0.201]]
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
probs:  [0.11502056801105286, 0.09032442940519399, 0.07644014360630959, 0.5318580160052918, 0.10991669936584206, 0.07644014360630959]
using another actor
from probs:  [0.11502056801105286, 0.09032442940519399, 0.07644014360630959, 0.5318580160052918, 0.10991669936584206, 0.07644014360630959]
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.352]
 [0.346]
 [0.349]
 [0.348]
 [0.349]
 [0.35 ]] [[59.728]
 [69.385]
 [62.78 ]
 [63.882]
 [64.987]
 [65.084]
 [65.932]] [[0.346]
 [0.352]
 [0.346]
 [0.349]
 [0.348]
 [0.349]
 [0.35 ]]
printing an ep nov before normalisation:  65.2880600605116
printing an ep nov before normalisation:  74.81388707383928
printing an ep nov before normalisation:  73.3021785040675
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.292]
 [0.287]
 [0.288]
 [0.286]
 [0.286]
 [0.286]] [[47.223]
 [58.066]
 [52.811]
 [53.585]
 [54.114]
 [54.115]
 [54.02 ]] [[0.282]
 [0.292]
 [0.287]
 [0.288]
 [0.286]
 [0.286]
 [0.286]]
printing an ep nov before normalisation:  28.84833726008357
line 256 mcts: sample exp_bonus 48.52389819202857
printing an ep nov before normalisation:  20.235929166149376
printing an ep nov before normalisation:  53.691528028868326
maxi score, test score, baseline:  -0.9960977186311787 -1.0 -0.9960977186311787
probs:  [0.11502064265289406, 0.09032444574527809, 0.07644012716887036, 0.5318578953054338, 0.1099167619586534, 0.07644012716887036]
printing an ep nov before normalisation:  36.01041198777952
printing an ep nov before normalisation:  34.4205413717486
Printing some Q and Qe and total Qs values:  [[0.426]
 [0.407]
 [0.426]
 [0.384]
 [0.381]
 [0.426]
 [0.382]] [[15.456]
 [31.684]
 [15.456]
 [16.376]
 [16.27 ]
 [15.456]
 [16.301]] [[0.426]
 [0.407]
 [0.426]
 [0.384]
 [0.381]
 [0.426]
 [0.382]]
printing an ep nov before normalisation:  62.71566396925341
maxi score, test score, baseline:  -0.9960977186311787 -1.0 -0.9960977186311787
probs:  [0.11502064265289406, 0.09032444574527809, 0.07644012716887036, 0.5318578953054338, 0.1099167619586534, 0.07644012716887036]
using explorer policy with actor:  0
printing an ep nov before normalisation:  44.720677026512426
using explorer policy with actor:  0
printing an ep nov before normalisation:  11.632158052335129
printing an ep nov before normalisation:  0.009621451201837772
printing an ep nov before normalisation:  10.927876234054565
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.404]
 [0.361]
 [0.352]
 [0.359]
 [0.398]
 [0.364]] [[14.109]
 [19.359]
 [10.862]
 [15.77 ]
 [11.517]
 [15.468]
 [18.546]] [[0.373]
 [0.404]
 [0.361]
 [0.352]
 [0.359]
 [0.398]
 [0.364]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  58.897637281463076
printing an ep nov before normalisation:  54.4510100332442
printing an ep nov before normalisation:  46.33028315618865
printing an ep nov before normalisation:  18.293349772061056
Printing some Q and Qe and total Qs values:  [[0.54 ]
 [0.563]
 [0.536]
 [0.55 ]
 [0.546]
 [0.546]
 [0.545]] [[46.331]
 [43.906]
 [42.958]
 [41.434]
 [41.387]
 [43.962]
 [44.172]] [[0.54 ]
 [0.563]
 [0.536]
 [0.55 ]
 [0.546]
 [0.546]
 [0.545]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
probs:  [0.1104807868286807, 0.09078760690613959, 0.07683181011063803, 0.534587199215223, 0.1104807868286807, 0.07683181011063803]
Printing some Q and Qe and total Qs values:  [[0.577]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]] [[14.49 ]
 [12.337]
 [12.337]
 [12.337]
 [12.337]
 [12.337]
 [12.337]] [[0.577]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]]
printing an ep nov before normalisation:  38.11322646790485
printing an ep nov before normalisation:  47.61347921074896
using explorer policy with actor:  0
printing an ep nov before normalisation:  32.06125020980835
printing an ep nov before normalisation:  35.8938883281297
printing an ep nov before normalisation:  22.287774085998535
printing an ep nov before normalisation:  48.4815588028092
maxi score, test score, baseline:  -0.9962099630996311 -1.0 -0.9962099630996311
probs:  [0.11048090827096423, 0.09078764002598645, 0.07683178063978184, 0.5345869821525214, 0.11048090827096423, 0.07683178063978184]
printing an ep nov before normalisation:  13.73902340279649
printing an ep nov before normalisation:  25.86790290566441
printing an ep nov before normalisation:  7.333857987800343
siam score:  -0.85841346
maxi score, test score, baseline:  -0.9962369963369964 -1.0 -0.9962369963369964
probs:  [0.11048102792821339, 0.09078767265902028, 0.0768317516021118, 0.5345867682803294, 0.11048102792821339, 0.0768317516021118]
using explorer policy with actor:  0
printing an ep nov before normalisation:  10.818327153403438
using explorer policy with actor:  1
printing an ep nov before normalisation:  54.54579090624968
line 256 mcts: sample exp_bonus 17.829540883329596
using explorer policy with actor:  0
printing an ep nov before normalisation:  14.592418670654297
maxi score, test score, baseline:  -0.9962898916967509 -1.0 -0.9962898916967509
probs:  [0.1059702357464134, 0.09124781827468495, 0.07722094808508534, 0.5372987591553888, 0.11104129065334206, 0.07722094808508534]
printing an ep nov before normalisation:  15.870489748695142
using explorer policy with actor:  1
printing an ep nov before normalisation:  22.019424438476562
maxi score, test score, baseline:  -0.9962898916967509 -1.0 -0.9962898916967509
probs:  [0.1059702357464134, 0.09124781827468495, 0.07722094808508534, 0.5372987591553888, 0.11104129065334206, 0.07722094808508534]
printing an ep nov before normalisation:  15.017300845154129
printing an ep nov before normalisation:  11.58576706449055
printing an ep nov before normalisation:  34.74823339586845
maxi score, test score, baseline:  -0.9962898916967509 -1.0 -0.9962898916967509
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.358]
 [0.374]
 [0.361]
 [0.239]
 [0.356]
 [0.233]] [[41.685]
 [48.163]
 [47.422]
 [43.981]
 [45.515]
 [47.111]
 [48.428]] [[0.329]
 [0.358]
 [0.374]
 [0.361]
 [0.239]
 [0.356]
 [0.233]]
maxi score, test score, baseline:  -0.9963028776978418 -1.0 -0.9963028776978418
probs:  [0.10617571796568342, 0.0913194634716094, 0.07716507926859409, 0.5368817877340988, 0.11129287229142006, 0.07716507926859409]
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]] [[10.172]
 [ 9.352]
 [ 9.352]
 [ 9.352]
 [ 9.352]
 [ 9.352]
 [ 9.352]] [[0.531]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]]
Printing some Q and Qe and total Qs values:  [[0.779]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]] [[11.039]
 [10.932]
 [10.932]
 [10.932]
 [10.932]
 [10.932]
 [10.932]] [[0.779]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]]
printing an ep nov before normalisation:  67.64830221190692
printing an ep nov before normalisation:  12.096455282544579
printing an ep nov before normalisation:  28.374545880404945
Printing some Q and Qe and total Qs values:  [[0.622]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]] [[10.039]
 [ 9.352]
 [ 9.352]
 [ 9.352]
 [ 9.352]
 [ 9.352]
 [ 9.352]] [[0.622]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]]
printing an ep nov before normalisation:  11.830086505409465
Printing some Q and Qe and total Qs values:  [[0.72 ]
 [0.378]
 [0.396]
 [0.396]
 [0.396]
 [0.395]
 [0.394]] [[ 9.332]
 [19.944]
 [10.802]
 [11.189]
 [11.623]
 [11.846]
 [11.329]] [[0.72 ]
 [0.378]
 [0.396]
 [0.396]
 [0.396]
 [0.395]
 [0.394]]
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9963028776978418 -1.0 -0.9963028776978418
probs:  [0.10659245970367183, 0.09146476833048427, 0.07705177103791973, 0.5360361209355681, 0.11180310895443642, 0.07705177103791973]
printing an ep nov before normalisation:  15.902511636532147
printing an ep nov before normalisation:  14.186500163247732
maxi score, test score, baseline:  -0.9963028776978418 -1.0 -0.9963028776978418
probs:  [0.10659245970367183, 0.09146476833048427, 0.07705177103791973, 0.5360361209355681, 0.11180310895443642, 0.07705177103791973]
printing an ep nov before normalisation:  0.00014230597457753902
using explorer policy with actor:  1
printing an ep nov before normalisation:  19.073523708310173
Printing some Q and Qe and total Qs values:  [[0.737]
 [0.469]
 [0.48 ]
 [0.477]
 [0.48 ]
 [0.48 ]
 [0.48 ]] [[14.106]
 [18.846]
 [12.814]
 [13.31 ]
 [12.814]
 [12.814]
 [12.814]] [[0.737]
 [0.469]
 [0.48 ]
 [0.477]
 [0.48 ]
 [0.48 ]
 [0.48 ]]
maxi score, test score, baseline:  -0.996315770609319 -1.0 -0.996315770609319
probs:  [0.10659250951786195, 0.09146478564776603, 0.07705175739357989, 0.5360360200852061, 0.11180316996200622, 0.07705175739357989]
printing an ep nov before normalisation:  19.820583134702474
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.325]
 [0.324]
 [0.326]
 [0.326]
 [0.326]
 [0.326]] [[15.123]
 [17.643]
 [12.174]
 [10.715]
 [10.715]
 [10.715]
 [10.715]] [[0.377]
 [0.325]
 [0.324]
 [0.326]
 [0.326]
 [0.326]
 [0.326]]
maxi score, test score, baseline:  -0.996315770609319 -1.0 -0.996315770609319
probs:  [0.10659250951786195, 0.09146478564776603, 0.07705175739357989, 0.5360360200852061, 0.11180316996200622, 0.07705175739357989]
Printing some Q and Qe and total Qs values:  [[0.757]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]] [[16.909]
 [15.092]
 [15.092]
 [15.092]
 [15.092]
 [15.092]
 [15.092]] [[0.757]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]]
maxi score, test score, baseline:  -0.996315770609319 -1.0 -0.996315770609319
probs:  [0.10680387411773819, 0.0915384818974134, 0.07699428930954481, 0.5356071117054643, 0.11206195366029455, 0.07699428930954481]
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]] [[15.99 ]
 [15.092]
 [15.092]
 [15.092]
 [15.092]
 [15.092]
 [15.092]] [[0.668]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]]
printing an ep nov before normalisation:  13.764092922210693
maxi score, test score, baseline:  -0.9963285714285715 -1.0 -0.9963285714285715
probs:  [0.10680392426452515, 0.09153849933056575, 0.07699427557458857, 0.5356070101806208, 0.11206201507511121, 0.07699427557458857]
using explorer policy with actor:  0
printing an ep nov before normalisation:  13.875056810825654
Printing some Q and Qe and total Qs values:  [[0.711]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]] [[16.998]
 [15.439]
 [15.439]
 [15.439]
 [15.439]
 [15.439]
 [15.439]] [[0.711]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]]
siam score:  -0.859308
Printing some Q and Qe and total Qs values:  [[0.302]
 [0.161]
 [0.293]
 [0.29 ]
 [0.29 ]
 [0.289]
 [0.28 ]] [[13.421]
 [20.85 ]
 [10.8  ]
 [11.778]
 [11.562]
 [11.747]
 [11.085]] [[0.302]
 [0.161]
 [0.293]
 [0.29 ]
 [0.29 ]
 [0.289]
 [0.28 ]]
maxi score, test score, baseline:  -0.9963285714285715 -1.0 -0.9963285714285715
probs:  [0.10680392426452515, 0.09153849933056575, 0.07699427557458857, 0.5356070101806208, 0.11206201507511121, 0.07699427557458857]
line 256 mcts: sample exp_bonus 11.941325664520264
Printing some Q and Qe and total Qs values:  [[0.75 ]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]] [[20.599]
 [19.001]
 [19.001]
 [19.001]
 [19.001]
 [19.001]
 [19.001]] [[0.75 ]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.417]
 [0.437]
 [0.445]
 [0.437]
 [0.434]
 [0.432]] [[11.13 ]
 [24.524]
 [10.986]
 [11.392]
 [12.27 ]
 [12.415]
 [11.941]] [[0.512]
 [0.417]
 [0.437]
 [0.445]
 [0.437]
 [0.434]
 [0.432]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.467]
 [0.37 ]
 [0.406]
 [0.409]
 [0.404]
 [0.402]
 [0.393]] [[14.203]
 [22.299]
 [15.029]
 [15.762]
 [16.76 ]
 [17.334]
 [16.169]] [[0.467]
 [0.37 ]
 [0.406]
 [0.409]
 [0.404]
 [0.402]
 [0.393]]
printing an ep nov before normalisation:  25.429428877590112
maxi score, test score, baseline:  -0.9963285714285715 -1.0 -0.9963285714285715
probs:  [0.10758793049414693, 0.09210127468527232, 0.07734627190673818, 0.5380303205129576, 0.10758793049414693, 0.07734627190673818]
printing an ep nov before normalisation:  26.145303287286026
printing an ep nov before normalisation:  62.243916698816946
maxi score, test score, baseline:  -0.9964034965034965 -1.0 -0.9964034965034965
probs:  [0.10781009071409103, 0.0921813892908943, 0.07729105171446281, 0.537616325851998, 0.10781009071409103, 0.07729105171446281]
using another actor
maxi score, test score, baseline:  -0.9964034965034965 -1.0 -0.9964034965034965
probs:  [0.10968160163247595, 0.0784125974736957, 0.0784125974736957, 0.5453990043139612, 0.10968160163247595, 0.0784125974736957]
printing an ep nov before normalisation:  41.05948182458819
using explorer policy with actor:  1
printing an ep nov before normalisation:  24.32096004486084
maxi score, test score, baseline:  -0.9964034965034965 -1.0 -0.9964034965034965
probs:  [0.10968160163247595, 0.0784125974736957, 0.0784125974736957, 0.5453990043139612, 0.10968160163247595, 0.0784125974736957]
maxi score, test score, baseline:  -0.9964277777777778 -1.0 -0.9964277777777778
probs:  [0.10992678402575647, 0.07836643921335068, 0.07836643921335068, 0.5450471143084349, 0.10992678402575647, 0.07836643921335068]
printing an ep nov before normalisation:  22.050752639770508
maxi score, test score, baseline:  -0.9964277777777778 -1.0 -0.9964277777777778
probs:  [0.11017435457972129, 0.07831983154430228, 0.07831983154430228, 0.5446917962076505, 0.11017435457972129, 0.07831983154430228]
printing an ep nov before normalisation:  35.78985066946917
actions average: 
K:  1  action  0 :  tensor([0.4215, 0.0049, 0.1143, 0.1225, 0.1315, 0.1096, 0.0957],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0121, 0.9059, 0.0162, 0.0166, 0.0141, 0.0125, 0.0227],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1182, 0.0463, 0.2279, 0.1246, 0.1321, 0.2356, 0.1153],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1568, 0.0115, 0.1563, 0.2160, 0.1710, 0.1489, 0.1396],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1732, 0.0030, 0.1091, 0.1440, 0.3275, 0.1347, 0.1086],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1475, 0.0015, 0.1545, 0.1508, 0.1626, 0.2367, 0.1463],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1362, 0.0528, 0.1358, 0.1718, 0.1357, 0.1270, 0.2408],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9964277777777778 -1.0 -0.9964277777777778
probs:  [0.11017435457972129, 0.07831983154430228, 0.07831983154430228, 0.5446917962076505, 0.11017435457972129, 0.07831983154430228]
printing an ep nov before normalisation:  49.722445004817786
maxi score, test score, baseline:  -0.9964397923875432 -1.0 -0.9964397923875432
probs:  [0.11017441090183534, 0.07831982084694673, 0.07831982084694673, 0.5446917156554891, 0.11017441090183534, 0.07831982084694673]
printing an ep nov before normalisation:  33.39296579360962
printing an ep nov before normalisation:  67.76000664531105
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.643]
 [0.459]
 [0.469]
 [0.413]
 [0.472]
 [0.469]] [[37.882]
 [33.945]
 [31.613]
 [28.863]
 [33.8  ]
 [35.25 ]
 [28.863]] [[1.576]
 [1.534]
 [1.237]
 [1.116]
 [1.296]
 [1.425]
 [1.116]]
maxi score, test score, baseline:  -0.9964397923875432 -1.0 -0.9964397923875432
printing an ep nov before normalisation:  38.13329985305993
printing an ep nov before normalisation:  34.00281421876757
maxi score, test score, baseline:  -0.9964397923875432 -1.0 -0.9964397923875432
probs:  [0.10360115059226152, 0.07805189630500287, 0.08300080697796794, 0.5428821750993408, 0.114412074720424, 0.07805189630500287]
maxi score, test score, baseline:  -0.9964397923875432 -1.0 -0.9964397923875432
probs:  [0.10360115059226152, 0.07805189630500287, 0.08300080697796794, 0.5428821750993408, 0.114412074720424, 0.07805189630500287]
printing an ep nov before normalisation:  43.58019061812115
actions average: 
K:  3  action  0 :  tensor([0.3471, 0.0039, 0.1082, 0.1327, 0.1745, 0.1148, 0.1188],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0268, 0.8699, 0.0184, 0.0133, 0.0195, 0.0125, 0.0395],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1295, 0.0089, 0.2307, 0.1262, 0.1407, 0.2375, 0.1265],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1778, 0.0146, 0.1147, 0.2311, 0.1842, 0.1539, 0.1237],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1501, 0.0183, 0.1186, 0.1510, 0.2691, 0.1505, 0.1424],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1505, 0.0174, 0.1383, 0.1529, 0.1627, 0.2282, 0.1501],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1695, 0.0032, 0.1401, 0.1741, 0.1907, 0.1611, 0.1614],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  34.57469340538589
maxi score, test score, baseline:  -0.996451724137931 -1.0 -0.996451724137931
probs:  [0.10360119220357109, 0.0780518852774857, 0.08300080614663284, 0.5428820924894718, 0.11441213860535289, 0.0780518852774857]
maxi score, test score, baseline:  -0.996451724137931 -1.0 -0.996451724137931
probs:  [0.09899076844702372, 0.07842006756013663, 0.08344126226480975, 0.5454165096453226, 0.11531132452257072, 0.07842006756013663]
maxi score, test score, baseline:  -0.996451724137931 -1.0 -0.996451724137931
probs:  [0.10174012912455808, 0.0817619049640613, 0.08663847936544229, 0.535307929158096, 0.11759062102048969, 0.07696093636735263]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9964635738831615 -1.0 -0.9964635738831615
probs:  [0.1019052907779647, 0.08174747958105821, 0.08666789018817717, 0.5348777200923742, 0.11789826495071686, 0.07690335440970868]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9964635738831615 -1.0 -0.9964635738831615
maxi score, test score, baseline:  -0.9964635738831615 -1.0 -0.9964635738831615
probs:  [0.10207200860992684, 0.08173291836320593, 0.08669757818720863, 0.5344434573469574, 0.1182088074833583, 0.07684523000934286]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9964635738831615 -1.0 -0.9964635738831615
probs:  [0.10263283376201669, 0.08218184622583354, 0.08717381956143724, 0.5373830119588028, 0.1133612206662439, 0.0772672678256656]
printing an ep nov before normalisation:  15.457219783622646
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
printing an ep nov before normalisation:  78.78954149607378
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9964635738831615 -1.0 -0.9964635738831615
probs:  [0.10522303400696105, 0.08534898892307793, 0.0902001337860729, 0.5277084825175682, 0.11564876257555548, 0.07587059819076443]
maxi score, test score, baseline:  -0.9964635738831615 -1.0 -0.9964635738831615
probs:  [0.10522303400696105, 0.08534898892307793, 0.0902001337860729, 0.5277084825175682, 0.11564876257555548, 0.07587059819076443]
maxi score, test score, baseline:  -0.9964635738831615 -1.0 -0.9964635738831615
printing an ep nov before normalisation:  42.60556976518116
printing an ep nov before normalisation:  7.497692606456781
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9964753424657534 -1.0 -0.9964753424657534
maxi score, test score, baseline:  -0.9964753424657534 -1.0 -0.9964753424657534
probs:  [0.10593341113137329, 0.08578592369545364, 0.08578592369545364, 0.5298150346142245, 0.11650258486824926, 0.07617712199524587]
printing an ep nov before normalisation:  53.48834947446401
maxi score, test score, baseline:  -0.9964870307167235 -1.0 -0.9964870307167235
probs:  [0.10613294610083225, 0.08580731478345291, 0.08580731478345291, 0.5293432998114109, 0.11679557236568687, 0.07611355215516437]
maxi score, test score, baseline:  -0.9964870307167235 -1.0 -0.9964870307167235
probs:  [0.10613294610083225, 0.08580731478345291, 0.08580731478345291, 0.5293432998114109, 0.11679557236568687, 0.07611355215516437]
maxi score, test score, baseline:  -0.9964870307167235 -1.0 -0.9964870307167235
probs:  [0.10613294610083225, 0.08580731478345291, 0.08580731478345291, 0.5293432998114109, 0.11679557236568687, 0.07611355215516437]
maxi score, test score, baseline:  -0.9964870307167235 -1.0 -0.9964870307167235
probs:  [0.10613294610083225, 0.08580731478345291, 0.08580731478345291, 0.5293432998114109, 0.11679557236568687, 0.07611355215516437]
printing an ep nov before normalisation:  36.50502668407373
printing an ep nov before normalisation:  25.903220176696777
using explorer policy with actor:  1
printing an ep nov before normalisation:  23.650533446262312
printing an ep nov before normalisation:  29.15960567235004
maxi score, test score, baseline:  -0.9964870307167235 -1.0 -0.9964870307167235
probs:  [0.10613294610083225, 0.08580731478345291, 0.08580731478345291, 0.5293432998114109, 0.11679557236568687, 0.07611355215516437]
Printing some Q and Qe and total Qs values:  [[0.331]
 [0.299]
 [0.289]
 [0.295]
 [0.294]
 [0.289]
 [0.301]] [[34.673]
 [31.23 ]
 [31.866]
 [32.578]
 [33.272]
 [31.968]
 [32.912]] [[1.433]
 [1.19 ]
 [1.219]
 [1.27 ]
 [1.311]
 [1.226]
 [1.295]]
maxi score, test score, baseline:  -0.9964870307167235 -1.0 -0.9964870307167235
probs:  [0.10613294610083225, 0.08580731478345291, 0.08580731478345291, 0.5293432998114109, 0.11679557236568687, 0.07611355215516437]
Printing some Q and Qe and total Qs values:  [[0.176]
 [0.178]
 [0.178]
 [0.174]
 [0.178]
 [0.178]
 [0.178]] [[25.51 ]
 [28.929]
 [28.929]
 [26.074]
 [28.929]
 [28.929]
 [28.929]] [[1.453]
 [1.796]
 [1.796]
 [1.507]
 [1.796]
 [1.796]
 [1.796]]
maxi score, test score, baseline:  -0.9964870307167235 -1.0 -0.9964870307167235
probs:  [0.10633432287905534, 0.08582890339006621, 0.08582890339006621, 0.5288672104567049, 0.11709126425032843, 0.07604939563377908]
maxi score, test score, baseline:  -0.9964870307167235 -1.0 -0.9964870307167235
probs:  [0.10633432287905534, 0.08582890339006621, 0.08582890339006621, 0.5288672104567049, 0.11709126425032843, 0.07604939563377908]
from probs:  [0.10653761278192254, 0.0858506970932934, 0.0858506970932934, 0.5283865981461484, 0.11738976527431812, 0.07598462961102415]
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.503]
 [0.512]
 [0.512]
 [0.516]
 [0.512]
 [0.512]] [[66.477]
 [64.329]
 [70.727]
 [70.727]
 [69.983]
 [70.727]
 [70.727]] [[1.684]
 [1.6  ]
 [1.845]
 [1.845]
 [1.821]
 [1.845]
 [1.845]]
maxi score, test score, baseline:  -0.9964870307167235 -1.0 -0.9964870307167235
probs:  [0.10900823808852968, 0.08890270456207162, 0.08890270456207162, 0.5190016390383255, 0.11955540321716343, 0.07462931053183801]
printing an ep nov before normalisation:  53.94655160812555
printing an ep nov before normalisation:  36.23148180319049
printing an ep nov before normalisation:  41.86582088470459
Printing some Q and Qe and total Qs values:  [[-0.043]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]] [[35.089]
 [29.648]
 [29.648]
 [29.648]
 [29.648]
 [29.648]
 [29.648]] [[1.032]
 [0.778]
 [0.778]
 [0.778]
 [0.778]
 [0.778]
 [0.778]]
printing an ep nov before normalisation:  36.11999516764084
printing an ep nov before normalisation:  43.36394958733081
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9964870307167235 -1.0 -0.9964870307167235
probs:  [0.10922891370367716, 0.08895034871203855, 0.08895034871203855, 0.5184494238852143, 0.11986684943699577, 0.07455411555003562]
maxi score, test score, baseline:  -0.9964870307167235 -1.0 -0.9964870307167235
probs:  [0.10922891370367716, 0.08895034871203855, 0.08895034871203855, 0.5184494238852143, 0.11986684943699577, 0.07455411555003562]
printing an ep nov before normalisation:  70.13802614002365
actions average: 
K:  3  action  0 :  tensor([0.3221, 0.0500, 0.0970, 0.1630, 0.1543, 0.1041, 0.1096],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0268, 0.8273, 0.0223, 0.0367, 0.0230, 0.0201, 0.0439],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1834, 0.0363, 0.1834, 0.1838, 0.1464, 0.1343, 0.1324],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1510, 0.0408, 0.1211, 0.2170, 0.1639, 0.1679, 0.1383],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1835, 0.0370, 0.1210, 0.1784, 0.2121, 0.1310, 0.1371],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1527, 0.0528, 0.1286, 0.1918, 0.1746, 0.1482, 0.1512],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2063, 0.0603, 0.1078, 0.1746, 0.1462, 0.1078, 0.1970],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  65.61235226469006
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.362]
 [0.341]
 [0.334]
 [0.341]
 [0.341]
 [0.341]] [[34.292]
 [56.12 ]
 [34.292]
 [30.296]
 [32.088]
 [34.292]
 [34.292]] [[0.983]
 [1.664]
 [0.983]
 [0.855]
 [0.916]
 [0.983]
 [0.983]]
printing an ep nov before normalisation:  32.28872075252076
Printing some Q and Qe and total Qs values:  [[0.274]
 [0.268]
 [0.266]
 [0.264]
 [0.259]
 [0.253]
 [0.258]] [[37.872]
 [45.955]
 [38.571]
 [39.042]
 [38.794]
 [37.788]
 [37.812]] [[1.285]
 [1.711]
 [1.314]
 [1.337]
 [1.319]
 [1.26 ]
 [1.267]]
siam score:  -0.8522961
printing an ep nov before normalisation:  57.526535556731595
maxi score, test score, baseline:  -0.9964870307167235 -1.0 -0.9964870307167235
probs:  [0.10945159450265213, 0.08899842578378052, 0.08899842578378052, 0.5178921909919527, 0.12018112563386354, 0.07447823730397078]
maxi score, test score, baseline:  -0.9964870307167235 -1.0 -0.9964870307167235
probs:  [0.1100465231087428, 0.08948204486882336, 0.08948204486882336, 0.5207099207589089, 0.11539663126872186, 0.07488283512597982]
maxi score, test score, baseline:  -0.9964870307167235 -1.0 -0.9964870307167235
maxi score, test score, baseline:  -0.9964870307167235 -1.0 -0.9964870307167235
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.364]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]] [[27.501]
 [26.759]
 [27.501]
 [27.501]
 [27.501]
 [27.501]
 [27.501]] [[1.417]
 [1.413]
 [1.417]
 [1.417]
 [1.417]
 [1.417]
 [1.417]]
printing an ep nov before normalisation:  34.02003862156489
maxi score, test score, baseline:  -0.9964986394557823 -1.0 -0.9964986394557823
probs:  [0.11004657530429261, 0.08948205677375118, 0.08948205677375118, 0.5207097987741786, 0.11539669394638465, 0.07488281842764179]
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.321]
 [0.329]
 [0.329]
 [0.329]
 [0.327]
 [0.326]] [[48.286]
 [49.284]
 [52.482]
 [52.439]
 [53.215]
 [53.903]
 [55.511]] [[0.33 ]
 [0.321]
 [0.329]
 [0.329]
 [0.329]
 [0.327]
 [0.326]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9964986394557823 -1.0 -0.9964986394557823
probs:  [0.11004657530429261, 0.08948205677375118, 0.08948205677375118, 0.5207097987741786, 0.11539669394638465, 0.07488281842764179]
maxi score, test score, baseline:  -0.9964986394557823 -1.0 -0.9964986394557823
probs:  [0.11004657530429261, 0.08948205677375118, 0.08948205677375118, 0.5207097987741786, 0.11539669394638465, 0.07488281842764179]
printing an ep nov before normalisation:  47.003417615633374
maxi score, test score, baseline:  -0.9964986394557823 -1.0 -0.9964986394557823
probs:  [0.11004657530429261, 0.08948205677375118, 0.08948205677375118, 0.5207097987741786, 0.11539669394638465, 0.07488281842764179]
Printing some Q and Qe and total Qs values:  [[ 0.005]
 [ 0.014]
 [-0.001]
 [-0.003]
 [-0.001]
 [-0.003]
 [-0.003]] [[53.56 ]
 [44.171]
 [51.758]
 [52.868]
 [53.26 ]
 [54.034]
 [54.508]] [[1.939]
 [1.289]
 [1.807]
 [1.882]
 [1.912]
 [1.964]
 [1.997]]
printing an ep nov before normalisation:  26.519594192504883
printing an ep nov before normalisation:  21.69661939875141
maxi score, test score, baseline:  -0.9964986394557823 -1.0 -0.9964986394557823
actor:  1 policy actor:  1  step number:  87 total reward:  0.09333333333333227  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  59.52976082739526
using explorer policy with actor:  0
printing an ep nov before normalisation:  43.155754866717835
printing an ep nov before normalisation:  36.790083396585466
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.436]
 [0.494]
 [0.44 ]
 [0.494]
 [0.494]
 [0.494]] [[11.713]
 [19.378]
 [11.351]
 [14.071]
 [11.351]
 [11.351]
 [11.351]] [[0.593]
 [0.436]
 [0.494]
 [0.44 ]
 [0.494]
 [0.494]
 [0.494]]
maxi score, test score, baseline:  -0.9964986394557823 -1.0 -0.9964986394557823
probs:  [0.09989931191986262, 0.08849980125807143, 0.08849980125807143, 0.5424448772627686, 0.10286503827089766, 0.07779117003032836]
maxi score, test score, baseline:  -0.9964986394557823 -1.0 -0.9964986394557823
probs:  [0.09989931191986262, 0.08849980125807143, 0.08849980125807143, 0.5424448772627686, 0.10286503827089766, 0.07779117003032836]
maxi score, test score, baseline:  -0.9964986394557823 -1.0 -0.9964986394557823
probs:  [0.10000682779290918, 0.08853318335916074, 0.08853318335916074, 0.5421800531945077, 0.10299184097892498, 0.07775491131533654]
maxi score, test score, baseline:  -0.9964986394557823 -1.0 -0.9964986394557823
maxi score, test score, baseline:  -0.9964986394557823 -1.0 -0.9964986394557823
probs:  [0.10028318541147918, 0.08601293070891637, 0.08877779255753784, 0.5436799367327778, 0.10327645834908994, 0.07796969624019906]
from probs:  [0.10028318541147918, 0.08601293070891637, 0.08877779255753784, 0.5436799367327778, 0.10327645834908994, 0.07796969624019906]
maxi score, test score, baseline:  -0.9965101694915255 -1.0 -0.9965101694915255
probs:  [0.10028321828968846, 0.08601293581621752, 0.0887778030454525, 0.5436798601014184, 0.1032764970524165, 0.07796968569480658]
printing an ep nov before normalisation:  27.611826505886853
maxi score, test score, baseline:  -0.9965101694915255 -1.0 -0.9965101694915255
probs:  [0.10028321828968846, 0.08601293581621752, 0.0887778030454525, 0.5436798601014184, 0.1032764970524165, 0.07796968569480658]
maxi score, test score, baseline:  -0.9965101694915255 -1.0 -0.9965101694915255
printing an ep nov before normalisation:  46.99476868122711
maxi score, test score, baseline:  -0.9965101694915255 -1.0 -0.9965101694915255
probs:  [0.10055682324951802, 0.08351778696354913, 0.0890199757642266, 0.5451648033600206, 0.10355827950585404, 0.07818233115683151]
maxi score, test score, baseline:  -0.9965216216216216 -1.0 -0.9965216216216216
probs:  [0.100556856524428, 0.08351778721410479, 0.08901998667889666, 0.5451647299180806, 0.10355831859806215, 0.07818232106642783]
maxi score, test score, baseline:  -0.9965216216216216 -1.0 -0.9965216216216216
probs:  [0.10066909674736403, 0.08351877401453346, 0.08905689906367663, 0.5449166029305018, 0.10369015630734778, 0.07814847093657643]
printing an ep nov before normalisation:  0.0009468876692153572
maxi score, test score, baseline:  -0.9965329966329967 -1.0 -0.9965329966329967
printing an ep nov before normalisation:  19.98769763848127
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]] [[46.14]
 [46.14]
 [46.14]
 [46.14]
 [46.14]
 [46.14]
 [46.14]] [[1.679]
 [1.679]
 [1.679]
 [1.679]
 [1.679]
 [1.679]
 [1.679]]
printing an ep nov before normalisation:  49.01919278504263
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]] [[60.226]
 [60.226]
 [60.226]
 [60.226]
 [60.226]
 [60.226]
 [60.226]] [[1.55]
 [1.55]
 [1.55]
 [1.55]
 [1.55]
 [1.55]
 [1.55]]
printing an ep nov before normalisation:  78.44142291606141
printing an ep nov before normalisation:  27.620725631713867
printing an ep nov before normalisation:  28.28756034432871
maxi score, test score, baseline:  -0.9965329966329967 -1.0 -0.9965329966329967
probs:  [0.10242732250664019, 0.08549241182544941, 0.09096097673291717, 0.5410940242437595, 0.10242732250664019, 0.07759794218459351]
maxi score, test score, baseline:  -0.9965329966329967 -1.0 -0.9965329966329967
maxi score, test score, baseline:  -0.9965329966329967 -1.0 -0.9965329966329967
probs:  [0.10255002176745988, 0.08550609597224988, 0.09100986367695307, 0.5408231895601915, 0.10255002176745988, 0.07756080725568577]
maxi score, test score, baseline:  -0.9965329966329967 -1.0 -0.9965329966329967
probs:  [0.10267332212884452, 0.08551984715733821, 0.09105899011688708, 0.540551028064382, 0.10267332212884452, 0.07752349040370372]
actions average: 
K:  4  action  0 :  tensor([0.3790, 0.0071, 0.1316, 0.1206, 0.1266, 0.1210, 0.1141],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0712, 0.6885, 0.0343, 0.0463, 0.0425, 0.0250, 0.0922],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2053, 0.0160, 0.1796, 0.1595, 0.1655, 0.1520, 0.1223],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1382, 0.0361, 0.1150, 0.3260, 0.1505, 0.1218, 0.1124],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2206, 0.0626, 0.1220, 0.1470, 0.2144, 0.1289, 0.1044],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1625, 0.0268, 0.1710, 0.1403, 0.1813, 0.1667, 0.1515],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1511, 0.1482, 0.1239, 0.1365, 0.1446, 0.1175, 0.1782],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.452]
 [0.388]
 [0.38 ]
 [0.379]
 [0.381]
 [0.386]] [[21.845]
 [34.616]
 [22.573]
 [23.068]
 [23.776]
 [23.72 ]
 [23.679]] [[0.891]
 [1.393]
 [0.854]
 [0.866]
 [0.893]
 [0.892]
 [0.896]]
maxi score, test score, baseline:  -0.9965329966329967 -1.0 -0.9965329966329967
probs:  [0.10267332212884452, 0.08551984715733821, 0.09105899011688708, 0.540551028064382, 0.10267332212884452, 0.07752349040370372]
printing an ep nov before normalisation:  40.00682830810547
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.46 ]
 [0.475]
 [0.444]
 [0.441]
 [0.475]
 [0.445]] [[40.232]
 [32.175]
 [40.232]
 [28.967]
 [28.956]
 [40.232]
 [28.687]] [[2.403]
 [1.793]
 [2.403]
 [1.54 ]
 [1.536]
 [2.403]
 [1.52 ]]
maxi score, test score, baseline:  -0.9965329966329967 -1.0 -0.9965329966329967
probs:  [0.10267332212884452, 0.08551984715733821, 0.09105899011688708, 0.540551028064382, 0.10267332212884452, 0.07752349040370372]
actions average: 
K:  1  action  0 :  tensor([0.3997, 0.0026, 0.1139, 0.1293, 0.1402, 0.1085, 0.1059],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0347, 0.7515, 0.0308, 0.0400, 0.0307, 0.0335, 0.0788],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2077, 0.0010, 0.2763, 0.1168, 0.1446, 0.1461, 0.1077],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1920, 0.0170, 0.1248, 0.2040, 0.1858, 0.1400, 0.1364],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2558, 0.0257, 0.1174, 0.1473, 0.2208, 0.1156, 0.1174],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1578, 0.0108, 0.1565, 0.1500, 0.1818, 0.2103, 0.1328],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1973, 0.0397, 0.1232, 0.1453, 0.1478, 0.1110, 0.2357],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.463]
 [0.326]
 [0.329]
 [0.316]
 [0.323]
 [0.321]] [[36.467]
 [23.144]
 [37.19 ]
 [36.084]
 [35.964]
 [37.048]
 [36.665]] [[1.545]
 [1.198]
 [1.522]
 [1.488]
 [1.472]
 [1.514]
 [1.5  ]]
printing an ep nov before normalisation:  0.7013909889890613
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9965442953020134 -1.0 -0.9965442953020134
probs:  [0.10334077737670525, 0.08580544868922746, 0.08861472615337117, 0.5412671854638132, 0.10334077737670525, 0.07763108494017763]
printing an ep nov before normalisation:  47.82110059328834
maxi score, test score, baseline:  -0.9965442953020134 -1.0 -0.9965442953020134
maxi score, test score, baseline:  -0.9965442953020134 -1.0 -0.9965442953020134
probs:  [0.10334077737670525, 0.08580544868922746, 0.08861472615337117, 0.5412671854638132, 0.10334077737670525, 0.07763108494017763]
siam score:  -0.8501135
printing an ep nov before normalisation:  46.733561882625175
maxi score, test score, baseline:  -0.9965442953020134 -1.0 -0.9965442953020134
probs:  [0.10334077737670525, 0.08580544868922746, 0.08861472615337117, 0.5412671854638132, 0.10334077737670525, 0.07763108494017763]
maxi score, test score, baseline:  -0.9965442953020134 -1.0 -0.9965442953020134
probs:  [0.10334077737670525, 0.08580544868922746, 0.08861472615337117, 0.5412671854638132, 0.10334077737670525, 0.07763108494017763]
maxi score, test score, baseline:  -0.9965442953020134 -1.0 -0.9965442953020134
probs:  [0.10334077737670525, 0.08580544868922746, 0.08861472615337117, 0.5412671854638132, 0.10334077737670525, 0.07763108494017763]
using another actor
printing an ep nov before normalisation:  48.26046787566849
maxi score, test score, baseline:  -0.9965442953020134 -1.0 -0.9965442953020134
maxi score, test score, baseline:  -0.9965442953020134 -1.0 -0.9965442953020134
probs:  [0.10346877165847529, 0.0858210765014251, 0.08864835582891115, 0.5409986929620935, 0.10346877165847529, 0.07759433139061978]
maxi score, test score, baseline:  -0.9965442953020134 -1.0 -0.9965442953020134
probs:  [0.10346877165847529, 0.0858210765014251, 0.08864835582891115, 0.5409986929620935, 0.10346877165847529, 0.07759433139061978]
maxi score, test score, baseline:  -0.9965442953020134 -1.0 -0.9965442953020134
using explorer policy with actor:  0
printing an ep nov before normalisation:  38.28515260253343
actions average: 
K:  2  action  0 :  tensor([0.3434, 0.0163, 0.1261, 0.1530, 0.1322, 0.1224, 0.1066],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0162, 0.9082, 0.0030, 0.0187, 0.0203, 0.0027, 0.0309],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1645, 0.0474, 0.2091, 0.1627, 0.1429, 0.1404, 0.1331],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1571, 0.0343, 0.1344, 0.2000, 0.1652, 0.1501, 0.1589],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1440, 0.0031, 0.1230, 0.1891, 0.2630, 0.1376, 0.1402],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1261, 0.0153, 0.1433, 0.1472, 0.1430, 0.3176, 0.1075],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1833, 0.0706, 0.1097, 0.1799, 0.1434, 0.1193, 0.1938],
       grad_fn=<DivBackward0>)
siam score:  -0.84846985
printing an ep nov before normalisation:  67.61152118832823
printing an ep nov before normalisation:  51.99656825385438
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9965442953020134 -1.0 -0.9965442953020134
probs:  [0.10389287636260117, 0.08608153287122965, 0.08608153287122965, 0.5422726809653572, 0.10389287636260117, 0.07777850056698132]
Printing some Q and Qe and total Qs values:  [[0.279]
 [0.462]
 [0.303]
 [0.422]
 [0.43 ]
 [0.361]
 [0.292]] [[41.481]
 [44.049]
 [41.981]
 [43.142]
 [43.187]
 [44.317]
 [45.606]] [[1.169]
 [1.469]
 [1.216]
 [1.387]
 [1.398]
 [1.381]
 [1.369]]
Printing some Q and Qe and total Qs values:  [[0.36 ]
 [0.36 ]
 [0.36 ]
 [0.364]
 [0.417]
 [0.36 ]
 [0.36 ]] [[49.84 ]
 [49.84 ]
 [49.84 ]
 [48.663]
 [52.408]
 [49.84 ]
 [49.84 ]] [[1.391]
 [1.391]
 [1.391]
 [1.348]
 [1.552]
 [1.391]
 [1.391]]
printing an ep nov before normalisation:  47.497999241616284
maxi score, test score, baseline:  -0.9965666666666667 -1.0 -0.9965666666666667
probs:  [0.10415670824731058, 0.0861164282449436, 0.0861164282449436, 0.5417470533581177, 0.10415670824731058, 0.07770667365737408]
maxi score, test score, baseline:  -0.9965666666666667 -1.0 -0.9965666666666667
probs:  [0.10442314339825572, 0.08615166801200717, 0.08615166801200717, 0.541216239046771, 0.10442314339825572, 0.07763413813270338]
printing an ep nov before normalisation:  47.19070477301311
actions average: 
K:  4  action  0 :  tensor([0.3962, 0.0396, 0.0984, 0.1251, 0.1474, 0.1004, 0.0927],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0442, 0.7704, 0.0368, 0.0364, 0.0328, 0.0402, 0.0392],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1362, 0.0840, 0.2159, 0.1305, 0.1342, 0.1890, 0.1102],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1372, 0.0715, 0.1221, 0.2647, 0.1451, 0.1377, 0.1218],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1728, 0.0123, 0.1381, 0.1694, 0.2344, 0.1541, 0.1190],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1377, 0.0207, 0.1396, 0.1598, 0.1630, 0.2576, 0.1215],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1580, 0.0355, 0.1302, 0.1721, 0.1763, 0.1326, 0.1952],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]] [[50.271]
 [50.271]
 [50.271]
 [50.271]
 [50.271]
 [50.271]
 [50.271]] [[1.359]
 [1.359]
 [1.359]
 [1.359]
 [1.359]
 [1.359]
 [1.359]]
printing an ep nov before normalisation:  52.94309809211942
Printing some Q and Qe and total Qs values:  [[0.243]
 [0.305]
 [0.243]
 [0.298]
 [0.243]
 [0.243]
 [0.243]] [[21.998]
 [28.289]
 [21.998]
 [29.624]
 [21.998]
 [21.998]
 [21.998]] [[0.919]
 [1.358]
 [0.919]
 [1.431]
 [0.919]
 [0.919]
 [0.919]]
printing an ep nov before normalisation:  23.955029222794057
printing an ep nov before normalisation:  0.0
printing an ep nov before normalisation:  29.02268925059829
maxi score, test score, baseline:  -0.9965777408637874 -1.0 -0.9965777408637874
probs:  [0.10475479176739168, 0.08642518390364345, 0.08642518390364345, 0.54293662601372, 0.10157765973767537, 0.07788055467392618]
printing an ep nov before normalisation:  50.85563359075903
maxi score, test score, baseline:  -0.9965777408637874 -1.0 -0.9965777408637874
printing an ep nov before normalisation:  65.72716797831113
maxi score, test score, baseline:  -0.9965777408637874 -1.0 -0.9965777408637874
probs:  [0.10538876798963724, 0.08694808803910663, 0.08403887135479898, 0.546225856569917, 0.09904673520241769, 0.07835168084412242]
maxi score, test score, baseline:  -0.9965777408637874 -1.0 -0.9965777408637874
probs:  [0.10538876798963724, 0.08694808803910663, 0.08403887135479898, 0.546225856569917, 0.09904673520241769, 0.07835168084412242]
UNIT TEST: sample policy line 217 mcts : [0.061 0.102 0.041 0.122 0.286 0.306 0.082]
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.631]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.635]
 [0.631]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]]
maxi score, test score, baseline:  -0.9965887417218543 -1.0 -0.9965887417218543
maxi score, test score, baseline:  -0.9965887417218543 -1.0 -0.9965887417218543
probs:  [0.10684972460290364, 0.088711507578061, 0.08585000769119011, 0.5404552849192618, 0.1006117134567937, 0.07752176175178978]
maxi score, test score, baseline:  -0.9965887417218543 -1.0 -0.9965887417218543
probs:  [0.10715155892965272, 0.08896204021637037, 0.08326633233645368, 0.5419835185845229, 0.1008959043457196, 0.07774064558728072]
printing an ep nov before normalisation:  45.898673398560135
maxi score, test score, baseline:  -0.9965887417218543 -1.0 -0.9965887417218543
probs:  [0.1059095361308206, 0.09097360886918315, 0.08531606066401747, 0.5378292283517974, 0.10282751939429224, 0.07714404658988916]
maxi score, test score, baseline:  -0.9965887417218543 -1.0 -0.9965887417218543
probs:  [0.1059095361308206, 0.09097360886918315, 0.08531606066401747, 0.5378292283517974, 0.10282751939429224, 0.07714404658988916]
Printing some Q and Qe and total Qs values:  [[-0.062]
 [-0.062]
 [-0.059]
 [-0.062]
 [-0.062]
 [-0.101]
 [-0.062]] [[37.592]
 [37.592]
 [38.645]
 [47.502]
 [47.694]
 [48.854]
 [37.592]] [[0.575]
 [0.575]
 [0.618]
 [0.947]
 [0.954]
 [0.958]
 [0.575]]
siam score:  -0.8403843
maxi score, test score, baseline:  -0.9965887417218543 -1.0 -0.9965887417218543
probs:  [0.1059095361308206, 0.09097360886918315, 0.08531606066401747, 0.5378292283517974, 0.10282751939429224, 0.07714404658988916]
maxi score, test score, baseline:  -0.9965887417218543 -1.0 -0.9965887417218543
maxi score, test score, baseline:  -0.9965887417218543 -1.0 -0.9965887417218543
probs:  [0.10605055125284737, 0.09102119354467826, 0.08532825501885653, 0.5375456233731198, 0.10294925521782829, 0.07710512159266973]
printing an ep nov before normalisation:  26.295830608717637
maxi score, test score, baseline:  -0.9965887417218543 -1.0 -0.9965887417218543
maxi score, test score, baseline:  -0.9965887417218543 -1.0 -0.9965887417218543
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.9965887417218543 -1.0 -0.9965887417218543
probs:  [0.10634630038802866, 0.09127497674496209, 0.08277611002894696, 0.5390462206765863, 0.10323634471564978, 0.07732004744582625]
maxi score, test score, baseline:  -0.9965887417218543 -1.0 -0.9965887417218543
probs:  [0.10634630038802866, 0.09127497674496209, 0.08277611002894696, 0.5390462206765863, 0.10323634471564978, 0.07732004744582625]
printing an ep nov before normalisation:  66.42452236117907
printing an ep nov before normalisation:  31.832383461185678
maxi score, test score, baseline:  -0.9965887417218543 -1.0 -0.9965887417218543
probs:  [0.10634630038802866, 0.09127497674496209, 0.08277611002894696, 0.5390462206765863, 0.10323634471564978, 0.07732004744582625]
printing an ep nov before normalisation:  34.74713543132502
maxi score, test score, baseline:  -0.9965996699669967 -1.0 -0.9965996699669967
probs:  [0.1063463434018606, 0.09127499151958053, 0.082776108879197, 0.5390461382239947, 0.10323638190234247, 0.07732003607302484]
printing an ep nov before normalisation:  52.619004249572754
printing an ep nov before normalisation:  40.00266481377523
printing an ep nov before normalisation:  81.51027899913394
siam score:  -0.85281676
Printing some Q and Qe and total Qs values:  [[0.056]
 [0.056]
 [0.056]
 [0.056]
 [0.056]
 [0.056]
 [0.056]] [[31.493]
 [31.493]
 [31.493]
 [31.493]
 [31.493]
 [31.493]
 [31.493]] [[1.337]
 [1.337]
 [1.337]
 [1.337]
 [1.337]
 [1.337]
 [1.337]]
maxi score, test score, baseline:  -0.9965996699669967 -1.0 -0.9965996699669967
probs:  [0.10398693812527772, 0.08896583006396713, 0.08327362069336526, 0.5420366036139235, 0.10398693812527772, 0.07775006937818861]
maxi score, test score, baseline:  -0.9965996699669967 -1.0 -0.9965996699669967
maxi score, test score, baseline:  -0.9965996699669967 -1.0 -0.9965996699669967
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
probs:  [0.10411681080477027, 0.08900109527884241, 0.08327303465849084, 0.54177755410031, 0.10411681080477027, 0.07771469435281626]
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
probs:  [0.10437836775153157, 0.08907211774857465, 0.08327185458955948, 0.5412558410791025, 0.10437836775153157, 0.07764345107970022]
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
probs:  [0.10437836775153157, 0.08907211774857465, 0.08327185458955948, 0.5412558410791025, 0.10437836775153157, 0.07764345107970022]
siam score:  -0.84975034
printing an ep nov before normalisation:  54.219858629410126
printing an ep nov before normalisation:  28.327808917244113
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
probs:  [0.10437836775153157, 0.08907211774857465, 0.08327185458955948, 0.5412558410791025, 0.10437836775153157, 0.07764345107970022]
printing an ep nov before normalisation:  46.616984103003375
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
probs:  [0.10437836775153157, 0.08907211774857465, 0.08327185458955948, 0.5412558410791025, 0.10437836775153157, 0.07764345107970022]
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
probs:  [0.10451013904522373, 0.08910789857119755, 0.08327126007577708, 0.5409930042453199, 0.10451013904522373, 0.07760755901725795]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
probs:  [0.10464257893577644, 0.08914386094281332, 0.08327066254547995, 0.5407288337987529, 0.10464257893577644, 0.0775714848414009]
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
probs:  [0.10490948496570499, 0.08921633588819981, 0.08326945834303989, 0.5401964512603546, 0.10490948496570499, 0.07749878457699588]
printing an ep nov before normalisation:  55.86772714503138
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.5133],
        [-0.5399],
        [-0.4205],
        [-0.4583],
        [-0.5287],
        [-0.4830],
        [-0.5261],
        [-0.5441],
        [-0.5237],
        [-0.0000]], dtype=torch.float64)
-0.032346567066 -0.5456271725254982
-0.032346567066 -0.572206566118994
-0.09703970119800001 -0.5175749954410132
-0.032346567066 -0.49067683073633245
-0.032346567066 -0.5610174153306169
-0.09703970119800001 -0.5800353130698014
-0.09703970119800001 -0.6231289958683648
-0.09703970119800001 -0.641161189814914
-0.032346567066 -0.5560135297002134
-0.8945775456419999 -0.8945775456419999
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
probs:  [0.10200206326548876, 0.08950591002106993, 0.08353970151715566, 0.5419519842839364, 0.10525007135084366, 0.07775026956150553]
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
probs:  [0.10211877131247292, 0.08954435836538599, 0.08354078526658887, 0.5416938685309547, 0.10538712070943401, 0.0777150958151635]
Printing some Q and Qe and total Qs values:  [[0.641]
 [0.524]
 [0.609]
 [0.613]
 [0.586]
 [0.604]
 [0.608]] [[58.312]
 [56.776]
 [69.349]
 [70.475]
 [68.333]
 [70.839]
 [69.843]] [[0.858]
 [0.73 ]
 [0.912]
 [0.925]
 [0.881]
 [0.918]
 [0.914]]
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
probs:  [0.10211877131247292, 0.08954435836538599, 0.08354078526658887, 0.5416938685309547, 0.10538712070943401, 0.0777150958151635]
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
printing an ep nov before normalisation:  44.7390882814048
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
probs:  [0.10211877131247292, 0.08954435836538599, 0.08354078526658887, 0.5416938685309547, 0.10538712070943401, 0.0777150958151635]
maxi score, test score, baseline:  -0.9966213114754099 -1.0 -0.9966213114754099
probs:  [0.10211877131247292, 0.08954435836538599, 0.08354078526658887, 0.5416938685309547, 0.10538712070943401, 0.0777150958151635]
printing an ep nov before normalisation:  30.198434609862915
printing an ep nov before normalisation:  29.020915170073657
maxi score, test score, baseline:  -0.9966320261437909 -1.0 -0.9966320261437909
probs:  [0.09932006949063416, 0.08987377692475897, 0.08381299773462353, 0.5431938588752081, 0.10586749978761639, 0.07793179718715872]
maxi score, test score, baseline:  -0.9966320261437909 -1.0 -0.9966320261437909
probs:  [0.09932006949063416, 0.08987377692475897, 0.08381299773462353, 0.5431938588752081, 0.10586749978761639, 0.07793179718715872]
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.619]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]] [[ 0.   ]
 [48.513]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.123]
 [ 1.902]
 [-0.123]
 [-0.123]
 [-0.123]
 [-0.123]
 [-0.123]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.996642671009772 -1.0 -0.996642671009772
probs:  [0.09975113667554468, 0.09021391048926958, 0.0840947879236395, 0.5447528517883782, 0.10303034042988068, 0.07815697269328731]
maxi score, test score, baseline:  -0.996642671009772 -1.0 -0.996642671009772
probs:  [0.09975113667554468, 0.09021391048926958, 0.0840947879236395, 0.5447528517883782, 0.10303034042988068, 0.07815697269328731]
printing an ep nov before normalisation:  25.048885345458984
maxi score, test score, baseline:  -0.996642671009772 -1.0 -0.996642671009772
probs:  [0.09985410020489854, 0.09025694008917357, 0.08409936367407193, 0.5445114509009666, 0.10315391116332354, 0.07812423396756583]
maxi score, test score, baseline:  -0.996642671009772 -1.0 -0.996642671009772
probs:  [0.09995760845893713, 0.09030019733555969, 0.08410396363234013, 0.544268772891227, 0.10327813564309049, 0.07809132203884556]
maxi score, test score, baseline:  -0.996642671009772 -1.0 -0.996642671009772
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.95450617652886
maxi score, test score, baseline:  -0.996642671009772 -1.0 -0.996642671009772
probs:  [0.09995760845893713, 0.09030019733555969, 0.08410396363234013, 0.544268772891227, 0.10327813564309049, 0.07809132203884556]
printing an ep nov before normalisation:  34.563809355244295
maxi score, test score, baseline:  -0.996642671009772 -1.0 -0.996642671009772
probs:  [0.10006166577189402, 0.09034368403975214, 0.08410858799105969, 0.5440248075974273, 0.10340301907087185, 0.07805823552899503]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.996642671009772 -1.0 -0.996642671009772
probs:  [0.10006166577189402, 0.09034368403975214, 0.08410858799105969, 0.5440248075974273, 0.10340301907087185, 0.07805823552899503]
maxi score, test score, baseline:  -0.996642671009772 -1.0 -0.996642671009772
probs:  [0.10006166577189402, 0.09034368403975214, 0.08410858799105969, 0.5440248075974273, 0.10340301907087185, 0.07805823552899503]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.996642671009772 -1.0 -0.996642671009772
printing an ep nov before normalisation:  35.01306416632204
maxi score, test score, baseline:  -0.996642671009772 -1.0 -0.996642671009772
probs:  [0.10016627652410631, 0.09038740203234233, 0.08411323694489478, 0.5437795447497439, 0.10352856670368918, 0.07802497304522345]
maxi score, test score, baseline:  -0.996642671009772 -1.0 -0.996642671009772
probs:  [0.10016627652410631, 0.09038740203234233, 0.08411323694489478, 0.5437795447497439, 0.10352856670368918, 0.07802497304522345]
maxi score, test score, baseline:  -0.996642671009772 -1.0 -0.996642671009772
probs:  [0.10182055989900625, 0.09221326695450639, 0.08604918927582983, 0.5376501255100765, 0.10512385484842736, 0.07714300351215378]
printing an ep nov before normalisation:  31.112394332885742
maxi score, test score, baseline:  -0.996642671009772 -1.0 -0.996642671009772
maxi score, test score, baseline:  -0.996642671009772 -1.0 -0.996642671009772
printing an ep nov before normalisation:  25.585968494415283
actions average: 
K:  4  action  0 :  tensor([0.3607, 0.0254, 0.0983, 0.1643, 0.1313, 0.1085, 0.1115],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0393, 0.8494, 0.0268, 0.0191, 0.0201, 0.0230, 0.0223],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2279, 0.0259, 0.1476, 0.1725, 0.1525, 0.1629, 0.1108],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2372, 0.0473, 0.0845, 0.2661, 0.1616, 0.1041, 0.0991],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1798, 0.0225, 0.0958, 0.2128, 0.2429, 0.1163, 0.1300],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1704, 0.0301, 0.1113, 0.1924, 0.1343, 0.2356, 0.1259],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1553, 0.0705, 0.1249, 0.2003, 0.1522, 0.1489, 0.1478],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  61.98484273909982
printing an ep nov before normalisation:  59.236443849217004
printing an ep nov before normalisation:  22.455476615705642
maxi score, test score, baseline:  -0.996642671009772 -1.0 -0.996642671009772
siam score:  -0.84043443
maxi score, test score, baseline:  -0.996642671009772 -1.0 -0.996642671009772
probs:  [0.09958665647982005, 0.0931323518065749, 0.0807974584310397, 0.5424121796740249, 0.10624424633962413, 0.07782710726891631]
Printing some Q and Qe and total Qs values:  [[0.356]
 [0.379]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]] [[44.167]
 [38.95 ]
 [44.167]
 [44.167]
 [44.167]
 [44.167]
 [44.167]] [[1.598]
 [1.398]
 [1.598]
 [1.598]
 [1.598]
 [1.598]
 [1.598]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
probs:  [0.09958668730510889, 0.09313237034724796, 0.08079745349444728, 0.5424121023379591, 0.10624428983644552, 0.07782709667879122]
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
probs:  [0.09958668730510889, 0.09313237034724796, 0.08079745349444728, 0.5424121023379591, 0.10624428983644552, 0.07782709667879122]
printing an ep nov before normalisation:  71.38217317580953
printing an ep nov before normalisation:  47.82439382982004
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.552]
 [0.501]
 [0.474]
 [0.474]
 [0.474]
 [0.474]] [[47.924]
 [51.36 ]
 [49.293]
 [45.607]
 [45.607]
 [45.607]
 [45.607]] [[1.689]
 [1.9  ]
 [1.748]
 [1.542]
 [1.542]
 [1.542]
 [1.542]]
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
probs:  [0.09958668730510889, 0.09313237034724796, 0.08079745349444728, 0.5424121023379591, 0.10624428983644552, 0.07782709667879122]
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
probs:  [0.09968727453548741, 0.09319293165239605, 0.0807815208091549, 0.542159365786191, 0.10638616365111686, 0.07779274356565383]
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
probs:  [0.10002473928923673, 0.09350838211470673, 0.08105489951449389, 0.543996706623729, 0.1033592814371408, 0.07805599102069274]
printing an ep nov before normalisation:  34.13177641568893
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
probs:  [0.10044996223494111, 0.09065777226291273, 0.08130079073408564, 0.5455019152260638, 0.1038160275378258, 0.07827353200417105]
printing an ep nov before normalisation:  73.94030888875325
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
printing an ep nov before normalisation:  28.316865048902844
printing an ep nov before normalisation:  42.27719526943741
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
probs:  [0.10233253436278639, 0.09259443304920231, 0.08328913623844418, 0.5387918783805273, 0.10568000668933092, 0.07731201127970905]
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]] [[59.635]
 [59.635]
 [59.635]
 [59.635]
 [59.635]
 [59.635]
 [59.635]] [[2.456]
 [2.456]
 [2.456]
 [2.456]
 [2.456]
 [2.456]
 [2.456]]
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
probs:  [0.10233253436278639, 0.09259443304920231, 0.08328913623844418, 0.5387918783805273, 0.10568000668933092, 0.07731201127970905]
printing an ep nov before normalisation:  42.9669001094038
actions average: 
K:  1  action  0 :  tensor([0.4063, 0.0032, 0.1150, 0.1390, 0.1315, 0.0947, 0.1104],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0147, 0.9518, 0.0054, 0.0074, 0.0052, 0.0027, 0.0128],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1181, 0.0607, 0.3455, 0.1129, 0.1159, 0.1370, 0.1099],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1669, 0.0199, 0.1341, 0.2344, 0.1591, 0.1377, 0.1479],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2123, 0.0021, 0.1226, 0.1517, 0.2442, 0.1252, 0.1417],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2030, 0.0143, 0.1375, 0.1668, 0.1389, 0.1958, 0.1437],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1544, 0.0722, 0.1117, 0.1773, 0.1209, 0.1056, 0.2579],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
probs:  [0.10424270915031364, 0.0915389840987414, 0.0854694265741013, 0.5345112224047651, 0.10754270022816344, 0.07669495754391507]
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
probs:  [0.10424270915031364, 0.0915389840987414, 0.0854694265741013, 0.5345112224047651, 0.10754270022816344, 0.07669495754391507]
line 256 mcts: sample exp_bonus 34.21927062427621
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
probs:  [0.10424270915031364, 0.0915389840987414, 0.0854694265741013, 0.5345112224047651, 0.10754270022816344, 0.07669495754391507]
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
probs:  [0.10436842626979952, 0.09158819846624035, 0.08548208962676197, 0.5342182153955441, 0.1076882901328334, 0.07665478010882058]
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
probs:  [0.10436842626979952, 0.09158819846624035, 0.08548208962676197, 0.5342182153955441, 0.1076882901328334, 0.07665478010882058]
Printing some Q and Qe and total Qs values:  [[0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]] [[16.773]
 [16.773]
 [16.773]
 [16.773]
 [16.773]
 [16.773]
 [16.773]] [[0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]]
printing an ep nov before normalisation:  37.053420051634895
printing an ep nov before normalisation:  49.72791382328968
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
probs:  [0.10436842626979952, 0.09158819846624035, 0.08548208962676197, 0.5342182153955441, 0.1076882901328334, 0.07665478010882058]
Printing some Q and Qe and total Qs values:  [[0.063]
 [0.052]
 [0.061]
 [0.05 ]
 [0.044]
 [0.042]
 [0.051]] [[24.099]
 [30.951]
 [21.3  ]
 [21.741]
 [22.569]
 [23.71 ]
 [24.459]] [[0.68 ]
 [0.977]
 [0.552]
 [0.561]
 [0.592]
 [0.641]
 [0.684]]
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
probs:  [0.10436842626979952, 0.09158819846624035, 0.08548208962676197, 0.5342182153955441, 0.1076882901328334, 0.07665478010882058]
actor:  1 policy actor:  1  step number:  79 total reward:  0.03999999999999959  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
probs:  [0.09617594226134349, 0.0894093267234598, 0.08506532662506536, 0.5497007577178901, 0.10086323323289835, 0.07878541343934288]
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
probs:  [0.09617594226134349, 0.0894093267234598, 0.08506532662506536, 0.5497007577178901, 0.10086323323289835, 0.07878541343934288]
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
probs:  [0.09617594226134349, 0.0894093267234598, 0.08506532662506536, 0.5497007577178901, 0.10086323323289835, 0.07878541343934288]
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
probs:  [0.09617594226134349, 0.0894093267234598, 0.08506532662506536, 0.5497007577178901, 0.10086323323289835, 0.07878541343934288]
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.465]
 [0.466]] [[13.454]
 [13.454]
 [13.454]
 [13.454]
 [14.668]
 [14.931]
 [15.361]] [[0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.465]
 [0.466]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  29.2208942349923
siam score:  -0.84703517
siam score:  -0.84482956
printing an ep nov before normalisation:  39.43371581536878
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.09637961646023328, 0.08950540828120292, 0.08509233636380067, 0.5491686317910738, 0.10114143775091572, 0.0787125693527735]
printing an ep nov before normalisation:  0.03674104624167285
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
Printing some Q and Qe and total Qs values:  [[0.774]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]] [[52.591]
 [53.008]
 [53.008]
 [53.008]
 [53.008]
 [53.008]
 [53.008]] [[1.816]
 [1.639]
 [1.639]
 [1.639]
 [1.639]
 [1.639]
 [1.639]]
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.09705079186321389, 0.09012864630742869, 0.08351188952616348, 0.552997337058301, 0.09705079186321389, 0.07926054338167905]
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.0971231923823658, 0.09016442289654773, 0.08351265794686868, 0.5528377157604775, 0.0971231923823658, 0.07923881863137436]
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.447]
 [0.441]
 [0.441]] [[52.067]
 [52.067]
 [52.067]
 [52.067]
 [60.98 ]
 [52.067]
 [52.067]] [[1.333]
 [1.333]
 [1.333]
 [1.333]
 [1.615]
 [1.333]
 [1.333]]
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.09719585754700427, 0.0902003302597681, 0.08351342917638065, 0.5526775109992135, 0.09719585754700427, 0.07921701447062925]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
printing an ep nov before normalisation:  60.27311952715747
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.0972687888108233, 0.0902363691154303, 0.08351420323012815, 0.5525167195695518, 0.0972687888108233, 0.07919513046324321]
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.09741545550420927, 0.09030884420384366, 0.08351575987261185, 0.5521933637674119, 0.09741545550420927, 0.07915112114771415]
maxi score, test score, baseline:  -0.9966741935483872 -1.0 -0.9966741935483872
probs:  [0.09741547777290353, 0.09030885519040935, 0.08351576007478988, 0.5521933147682009, 0.09741547777290353, 0.07915111442079284]
Printing some Q and Qe and total Qs values:  [[0.144]
 [0.03 ]
 [0.033]
 [0.059]
 [0.061]
 [0.042]
 [0.046]] [[24.35 ]
 [28.718]
 [25.449]
 [26.906]
 [27.492]
 [27.051]
 [28.135]] [[0.573]
 [0.614]
 [0.501]
 [0.579]
 [0.601]
 [0.567]
 [0.61 ]]
maxi score, test score, baseline:  -0.9966741935483872 -1.0 -0.9966741935483872
probs:  [0.09741547777290353, 0.09030885519040935, 0.08351576007478988, 0.5521933147682009, 0.09741547777290353, 0.07915111442079284]
maxi score, test score, baseline:  -0.9966741935483872 -1.0 -0.9966741935483872
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9966741935483872 -1.0 -0.9966741935483872
printing an ep nov before normalisation:  44.02035584212813
printing an ep nov before normalisation:  40.182765241915774
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9966741935483872 -1.0 -0.9966741935483872
maxi score, test score, baseline:  -0.9966741935483872 -1.0 -0.9966741935483872
maxi score, test score, baseline:  -0.9966741935483872 -1.0 -0.9966741935483872
probs:  [0.09786584862396798, 0.08829005514528705, 0.0837133891444469, 0.5529954783331918, 0.09786584862396798, 0.07926938012913835]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9966741935483872 -1.0 -0.9966741935483872
probs:  [0.09910930983537389, 0.08965680113824136, 0.08513905801093538, 0.5483793147417192, 0.09910930983537389, 0.07860620643835627]
maxi score, test score, baseline:  -0.9966741935483872 -1.0 -0.9966741935483872
probs:  [0.09910930983537389, 0.08965680113824136, 0.08513905801093538, 0.5483793147417192, 0.09910930983537389, 0.07860620643835627]
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.283]
 [0.257]
 [0.258]
 [0.262]
 [0.271]
 [0.264]] [[16.555]
 [34.914]
 [20.912]
 [23.362]
 [24.241]
 [22.86 ]
 [22.34 ]] [[0.305]
 [0.283]
 [0.257]
 [0.258]
 [0.262]
 [0.271]
 [0.264]]
printing an ep nov before normalisation:  13.52648932532734
line 256 mcts: sample exp_bonus 60.5407433925493
maxi score, test score, baseline:  -0.9966948717948718 -1.0 -0.9966948717948718
probs:  [0.0991093590179588, 0.0896568207483902, 0.08513906348719938, 0.5483792062520801, 0.0991093590179588, 0.0786061914764126]
printing an ep nov before normalisation:  49.803447072181726
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  62.20381528767337
maxi score, test score, baseline:  -0.9966948717948718 -1.0 -0.9966948717948718
probs:  [0.0991907347550287, 0.0896893352995536, 0.08514822526568684, 0.5481993857871247, 0.0991907347550287, 0.07858158413757736]
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
probs:  [0.09919075927990173, 0.08968934507818248, 0.08514822799647842, 0.5481993316886435, 0.09919075927990173, 0.07858157667689197]
printing an ep nov before normalisation:  30.62356623894012
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
probs:  [0.09919075927990173, 0.08968934507818248, 0.08514822799647842, 0.5481993316886435, 0.09919075927990173, 0.07858157667689197]
printing an ep nov before normalisation:  39.45122948452344
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
probs:  [0.09919075927990173, 0.08968934507818248, 0.08514822799647842, 0.5481993316886435, 0.09919075927990173, 0.07858157667689197]
printing an ep nov before normalisation:  36.104340214198594
Printing some Q and Qe and total Qs values:  [[0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]] [[87.676]
 [87.676]
 [87.676]
 [87.676]
 [87.676]
 [87.676]
 [87.676]] [[1.197]
 [1.197]
 [1.197]
 [1.197]
 [1.197]
 [1.197]
 [1.197]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.26565848681634
printing an ep nov before normalisation:  41.73872710470238
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
printing an ep nov before normalisation:  29.8245717872692
printing an ep nov before normalisation:  35.799549420833074
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
probs:  [0.09720886991392405, 0.09000954135322632, 0.08538644311081746, 0.5490114167598229, 0.09968248536811247, 0.07870124349409674]
siam score:  -0.84252274
printing an ep nov before normalisation:  63.85940671525844
actions average: 
K:  1  action  0 :  tensor([0.3486, 0.0018, 0.1222, 0.1419, 0.1491, 0.1192, 0.1172],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0062, 0.9335, 0.0048, 0.0060, 0.0013, 0.0010, 0.0471],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1550, 0.0107, 0.2671, 0.1420, 0.1279, 0.1692, 0.1281],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2052, 0.0046, 0.1459, 0.1759, 0.1677, 0.1544, 0.1463],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2199, 0.0158, 0.1352, 0.1532, 0.2136, 0.1277, 0.1346],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1575, 0.0039, 0.1602, 0.1315, 0.1166, 0.3251, 0.1051],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1998, 0.0295, 0.1342, 0.1858, 0.1578, 0.1467, 0.1463],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
probs:  [0.09758115726931799, 0.08793211793556677, 0.08560854412541713, 0.5499535653474544, 0.10008619632711876, 0.078838418995125]
printing an ep nov before normalisation:  32.687806554410386
printing an ep nov before normalisation:  46.85647029324225
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.318]
 [0.176]
 [0.323]
 [0.329]
 [0.312]
 [0.346]] [[35.413]
 [33.049]
 [34.797]
 [41.76 ]
 [51.994]
 [39.151]
 [34.53 ]] [[0.327]
 [0.318]
 [0.176]
 [0.323]
 [0.329]
 [0.312]
 [0.346]]
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
probs:  [0.09790068820362713, 0.08817748325328804, 0.08583604970826153, 0.5511712034102421, 0.09790068820362713, 0.07901388722095408]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
probs:  [0.09797557605648884, 0.08820227146427945, 0.0858487734834349, 0.5510063437166072, 0.09797557605648884, 0.07899145922270079]
line 256 mcts: sample exp_bonus 36.16911425638312
printing an ep nov before normalisation:  25.058298110961914
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
probs:  [0.09797557605648884, 0.08820227146427945, 0.0858487734834349, 0.5510063437166072, 0.09797557605648884, 0.07899145922270079]
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
probs:  [0.0982831123947016, 0.08606498802976185, 0.08606498802976185, 0.5521477648988959, 0.0982831123947016, 0.07915603425217721]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.542]
 [0.542]
 [0.527]
 [0.53 ]
 [0.536]
 [0.526]] [[23.81 ]
 [23.81 ]
 [23.81 ]
 [20.781]
 [21.414]
 [22.521]
 [33.837]] [[0.542]
 [0.542]
 [0.542]
 [0.527]
 [0.53 ]
 [0.536]
 [0.526]]
printing an ep nov before normalisation:  79.94264602661133
printing an ep nov before normalisation:  52.88950158290963
printing an ep nov before normalisation:  11.15964511721675
siam score:  -0.8393659
printing an ep nov before normalisation:  18.50646077550185
maxi score, test score, baseline:  -0.9967253968253968 -1.0 -0.9967253968253968
probs:  [0.09835998308353161, 0.0860788936067965, 0.0860788936067965, 0.5519879115224133, 0.09835998308353161, 0.07913433509693049]
printing an ep nov before normalisation:  45.37252301620455
Printing some Q and Qe and total Qs values:  [[0.399]
 [0.381]
 [0.382]
 [0.411]
 [0.394]
 [0.392]
 [0.379]] [[48.211]
 [49.665]
 [48.148]
 [48.412]
 [48.951]
 [47.107]
 [49.665]] [[2.122]
 [2.193]
 [2.1  ]
 [2.146]
 [2.162]
 [2.046]
 [2.191]]
from probs:  [0.09835998308353161, 0.0860788936067965, 0.0860788936067965, 0.5519879115224133, 0.09835998308353161, 0.07913433509693049]
printing an ep nov before normalisation:  46.409614079896905
printing an ep nov before normalisation:  31.53637148272125
printing an ep nov before normalisation:  54.003942857169704
printing an ep nov before normalisation:  55.094028788647954
actions average: 
K:  1  action  0 :  tensor([0.3238, 0.0454, 0.1161, 0.1434, 0.1373, 0.1226, 0.1114],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0115, 0.8818, 0.0477, 0.0074, 0.0093, 0.0296, 0.0126],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1701, 0.0149, 0.1738, 0.1766, 0.1569, 0.1699, 0.1377],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1731, 0.0358, 0.1051, 0.3062, 0.1202, 0.1224, 0.1371],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2294, 0.0160, 0.1220, 0.1406, 0.2453, 0.1203, 0.1263],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1433, 0.0290, 0.1477, 0.1578, 0.1401, 0.2445, 0.1375],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1469, 0.0179, 0.1293, 0.1977, 0.1880, 0.1652, 0.1551],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9967253968253968 -1.0 -0.9967253968253968
probs:  [0.09859141153053393, 0.08392720716282769, 0.08628138703068251, 0.5532881160434324, 0.09859141153053393, 0.07932046670198954]
printing an ep nov before normalisation:  67.704640648611
maxi score, test score, baseline:  -0.9967253968253968 -1.0 -0.9967253968253968
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
printing an ep nov before normalisation:  83.55057569786477
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]] [[37.475]
 [37.475]
 [37.475]
 [37.475]
 [37.475]
 [37.475]
 [37.475]] [[0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]]
maxi score, test score, baseline:  -0.9967454258675079 -1.0 -0.9967454258675079
probs:  [0.1001231134078118, 0.08543293461945138, 0.08779128440042594, 0.5479695246210305, 0.1001231134078118, 0.07856002954346854]
printing an ep nov before normalisation:  56.47161559567732
maxi score, test score, baseline:  -0.9967454258675079 -1.0 -0.9967454258675079
probs:  [0.1001231134078118, 0.08543293461945138, 0.08779128440042594, 0.5479695246210305, 0.1001231134078118, 0.07856002954346854]
printing an ep nov before normalisation:  46.354553610116426
maxi score, test score, baseline:  -0.9967454258675079 -1.0 -0.9967454258675079
probs:  [0.1001231134078118, 0.08543293461945138, 0.08779128440042594, 0.5479695246210305, 0.1001231134078118, 0.07856002954346854]
Printing some Q and Qe and total Qs values:  [[0.601]
 [0.57 ]
 [0.578]
 [0.577]
 [0.571]
 [0.573]
 [0.577]] [[43.252]
 [51.271]
 [41.343]
 [41.813]
 [49.903]
 [43.241]
 [43.74 ]] [[1.415]
 [1.676]
 [1.323]
 [1.338]
 [1.627]
 [1.386]
 [1.408]]
printing an ep nov before normalisation:  58.08648109436035
maxi score, test score, baseline:  -0.9967553459119497 -1.0 -0.9967553459119497
probs:  [0.10029318254575852, 0.08545390624188193, 0.08783619202105818, 0.5476122918172603, 0.10029318254575852, 0.07851124482828246]
printing an ep nov before normalisation:  55.977190556829385
maxi score, test score, baseline:  -0.9967553459119497 -1.0 -0.9967553459119497
probs:  [0.10029318254575852, 0.08545390624188193, 0.08783619202105818, 0.5476122918172603, 0.10029318254575852, 0.07851124482828246]
printing an ep nov before normalisation:  51.38161004132573
maxi score, test score, baseline:  -0.9967553459119497 -1.0 -0.9967553459119497
probs:  [0.10029318254575852, 0.08545390624188193, 0.08783619202105818, 0.5476122918172603, 0.10029318254575852, 0.07851124482828246]
printing an ep nov before normalisation:  29.485039848385007
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.018]
 [0.006]
 [0.037]
 [0.04 ]
 [0.006]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.018]
 [0.006]
 [0.037]
 [0.04 ]
 [0.006]
 [0.006]]
Printing some Q and Qe and total Qs values:  [[0.214]
 [0.191]
 [0.214]
 [0.188]
 [0.178]
 [0.214]
 [0.214]] [[29.396]
 [31.175]
 [29.396]
 [32.502]
 [33.261]
 [29.396]
 [29.396]] [[0.745]
 [0.785]
 [0.745]
 [0.828]
 [0.845]
 [0.745]
 [0.745]]
maxi score, test score, baseline:  -0.9967652037617555 -1.0 -0.9967652037617555
probs:  [0.10053258008040966, 0.0856578167230977, 0.0856578167230977, 0.5489206539549156, 0.10053258008040966, 0.07869855243806967]
maxi score, test score, baseline:  -0.9967652037617555 -1.0 -0.9967652037617555
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9967652037617555 -1.0 -0.9967652037617555
probs:  [0.10079178512754534, 0.08587861899562042, 0.08587861899562042, 0.5503374805202241, 0.09821210866280575, 0.07890138769818407]
Printing some Q and Qe and total Qs values:  [[ 0.198]
 [ 0.07 ]
 [-0.009]
 [ 0.038]
 [ 0.166]
 [ 0.026]
 [ 0.059]] [[25.207]
 [28.412]
 [23.48 ]
 [28.85 ]
 [27.459]
 [34.244]
 [33.208]] [[0.523]
 [0.471]
 [0.274]
 [0.449]
 [0.544]
 [0.565]
 [0.574]]
maxi score, test score, baseline:  -0.9967652037617555 -1.0 -0.9967652037617555
probs:  [0.10079178512754534, 0.08587861899562042, 0.08587861899562042, 0.5503374805202241, 0.09821210866280575, 0.07890138769818407]
maxi score, test score, baseline:  -0.9967652037617555 -1.0 -0.9967652037617555
probs:  [0.10079178512754534, 0.08587861899562042, 0.08587861899562042, 0.5503374805202241, 0.09821210866280575, 0.07890138769818407]
maxi score, test score, baseline:  -0.9967652037617555 -1.0 -0.9967652037617555
printing an ep nov before normalisation:  23.37516961262725
printing an ep nov before normalisation:  21.020727157592773
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
printing an ep nov before normalisation:  55.852896930156014
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
probs:  [0.10137931935431575, 0.08393300536599192, 0.08631618277211328, 0.552895440840409, 0.09620726493887283, 0.0792687867282972]
printing an ep nov before normalisation:  47.87103083520222
siam score:  -0.83969206
printing an ep nov before normalisation:  29.600047334372793
line 256 mcts: sample exp_bonus 12.337206241760104
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
probs:  [0.10171459851731436, 0.08413726802207233, 0.08413726802207233, 0.5540691393684911, 0.09650370354772594, 0.07943802252232397]
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.491]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.512]
 [0.491]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]]
Printing some Q and Qe and total Qs values:  [[0.37 ]
 [0.303]
 [0.34 ]
 [0.353]
 [0.349]
 [0.339]
 [0.346]] [[30.015]
 [30.694]
 [30.296]
 [29.941]
 [30.32 ]
 [30.32 ]
 [30.214]] [[0.641]
 [0.586]
 [0.616]
 [0.622]
 [0.626]
 [0.615]
 [0.62 ]]
printing an ep nov before normalisation:  24.46843131278639
printing an ep nov before normalisation:  26.04924567106938
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
probs:  [0.10190211133028448, 0.08414514357029285, 0.08414514357029285, 0.553771767012247, 0.09663796191915917, 0.07939787259772368]
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
probs:  [0.09951386642583959, 0.0843687160382341, 0.0843687160382341, 0.5552451095984068, 0.09689478026858456, 0.07960881163070098]
printing an ep nov before normalisation:  74.37243938446045
printing an ep nov before normalisation:  77.78275520225279
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
probs:  [0.09984524738026801, 0.08438928085595142, 0.08438928085595142, 0.554672088466916, 0.09717241106403281, 0.07953169137688053]
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
probs:  [0.09984524738026801, 0.08438928085595142, 0.08438928085595142, 0.554672088466916, 0.09717241106403281, 0.07953169137688053]
using explorer policy with actor:  1
printing an ep nov before normalisation:  70.33341442584177
siam score:  -0.84106386
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
probs:  [0.10019390169293525, 0.08461822433051855, 0.08461822433051855, 0.5559996108263463, 0.09484702737449367, 0.07972301144518759]
maxi score, test score, baseline:  -0.9967847352024922 -1.0 -0.9967847352024922
probs:  [0.10027947666710185, 0.08462458916041367, 0.08462458916041367, 0.5558614525471622, 0.09490541080659698, 0.07970448165831161]
siam score:  -0.84358144
maxi score, test score, baseline:  -0.9967847352024922 -1.0 -0.9967847352024922
printing an ep nov before normalisation:  33.336532779675665
printing an ep nov before normalisation:  31.83559296654322
maxi score, test score, baseline:  -0.9967847352024922 -1.0 -0.9967847352024922
probs:  [0.10036536178858774, 0.08463097709274012, 0.08463097709274012, 0.5557227934179503, 0.09496400584822216, 0.07968588475975948]
maxi score, test score, baseline:  -0.9967847352024922 -1.0 -0.9967847352024922
maxi score, test score, baseline:  -0.9967847352024922 -1.0 -0.9967847352024922
probs:  [0.10036536178858774, 0.08463097709274012, 0.08463097709274012, 0.5557227934179503, 0.09496400584822216, 0.07968588475975948]
printing an ep nov before normalisation:  41.553158890485264
Printing some Q and Qe and total Qs values:  [[0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]] [[58.663]
 [58.663]
 [58.663]
 [58.663]
 [58.663]
 [58.663]
 [58.663]] [[0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  21.353140135090598
printing an ep nov before normalisation:  30.118209042939625
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9967847352024922 -1.0 -0.9967847352024922
probs:  [0.10036536178858774, 0.08463097709274012, 0.08463097709274012, 0.5557227934179503, 0.09496400584822216, 0.07968588475975948]
actions average: 
K:  0  action  0 :  tensor([0.4298, 0.0082, 0.1162, 0.1083, 0.1598, 0.0801, 0.0976],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0092, 0.8398, 0.0203, 0.0309, 0.0116, 0.0218, 0.0663],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1561, 0.0228, 0.2038, 0.2008, 0.1557, 0.1339, 0.1268],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1536, 0.0179, 0.1305, 0.2609, 0.1475, 0.1130, 0.1766],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1789, 0.0065, 0.1333, 0.1723, 0.2591, 0.1089, 0.1409],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1315, 0.0098, 0.1873, 0.1363, 0.1377, 0.2706, 0.1267],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1886, 0.0143, 0.1427, 0.1992, 0.1511, 0.1165, 0.1876],
       grad_fn=<DivBackward0>)
from probs:  [0.10070351074085987, 0.0823403503143968, 0.08484960204968987, 0.5569784676873542, 0.09526112417523436, 0.07986694503246501]
siam score:  -0.83806604
maxi score, test score, baseline:  -0.9967944099378883 -1.0 -0.9967944099378883
probs:  [0.10079160022490741, 0.08233513557473882, 0.08485713695550719, 0.556845403864609, 0.09532156059511326, 0.07984916278512429]
maxi score, test score, baseline:  -0.9967944099378883 -1.0 -0.9967944099378883
probs:  [0.10079160022490741, 0.08233513557473882, 0.08485713695550719, 0.556845403864609, 0.09532156059511326, 0.07984916278512429]
maxi score, test score, baseline:  -0.9967944099378883 -1.0 -0.9967944099378883
probs:  [0.10079160022490741, 0.08233513557473882, 0.08485713695550719, 0.556845403864609, 0.09532156059511326, 0.07984916278512429]
Printing some Q and Qe and total Qs values:  [[0.577]
 [0.56 ]
 [0.574]
 [0.567]
 [0.558]
 [0.561]
 [0.562]] [[34.566]
 [37.038]
 [34.324]
 [34.77 ]
 [34.504]
 [34.548]
 [34.594]] [[0.577]
 [0.56 ]
 [0.574]
 [0.567]
 [0.558]
 [0.561]
 [0.562]]
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]] [[23.923]
 [23.923]
 [23.923]
 [23.923]
 [23.923]
 [23.923]
 [23.923]] [[0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]]
printing an ep nov before normalisation:  85.92236582702637
from probs:  [0.10088009480205151, 0.082329896775103, 0.08486470644338165, 0.556711728388218, 0.09538227491773198, 0.07983129867351399]
maxi score, test score, baseline:  -0.9968230769230769 -1.0 -0.9968230769230769
probs:  [0.10133297189698304, 0.08254329878696737, 0.08511083175852227, 0.5579587968734501, 0.09304165582621428, 0.08001244485786314]
from probs:  [0.10133297189698304, 0.08254329878696737, 0.08511083175852227, 0.5579587968734501, 0.09304165582621428, 0.08001244485786314]
Printing some Q and Qe and total Qs values:  [[-0.006]
 [ 0.007]
 [-0.004]
 [-0.005]
 [-0.004]
 [-0.003]
 [-0.004]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.006]
 [ 0.007]
 [-0.004]
 [-0.005]
 [-0.004]
 [-0.003]
 [-0.004]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
using explorer policy with actor:  1
siam score:  -0.83536893
using another actor
printing an ep nov before normalisation:  54.266790597773365
printing an ep nov before normalisation:  41.23622728648148
printing an ep nov before normalisation:  43.23223699281314
maxi score, test score, baseline:  -0.9968512195121951 -1.0 -0.9968512195121951
probs:  [0.10169974674637386, 0.08252649023010356, 0.08514643832549448, 0.5574441878465682, 0.09323916688681327, 0.07994396996464674]
printing an ep nov before normalisation:  59.28987726828779
printing an ep nov before normalisation:  70.54197322658926
siam score:  -0.8393768
printing an ep nov before normalisation:  32.258005142211914
maxi score, test score, baseline:  -0.9968512195121951 -1.0 -0.9968512195121951
probs:  [0.10206096066274432, 0.08273993889154171, 0.08273993889154171, 0.5587864695506469, 0.09353517645259458, 0.08013751555093084]
actions average: 
K:  3  action  0 :  tensor([0.4434, 0.0121, 0.0937, 0.1337, 0.1198, 0.0989, 0.0985],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0275, 0.7643, 0.0384, 0.0543, 0.0174, 0.0186, 0.0796],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.2228, 0.0237, 0.2182, 0.1485, 0.1316, 0.1294, 0.1258],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1930, 0.0437, 0.1292, 0.2182, 0.1575, 0.1321, 0.1263],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1723, 0.0096, 0.1154, 0.2143, 0.2308, 0.1242, 0.1334],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1203, 0.0139, 0.1436, 0.1909, 0.1425, 0.2564, 0.1323],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2372, 0.0057, 0.1116, 0.1679, 0.1444, 0.1157, 0.2176],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  62.0109889123855
siam score:  -0.8413803
printing an ep nov before normalisation:  27.637305184185408
maxi score, test score, baseline:  -0.9968512195121951 -1.0 -0.9968512195121951
printing an ep nov before normalisation:  31.530895233154297
printing an ep nov before normalisation:  35.5382442219776
printing an ep nov before normalisation:  46.71488306956199
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.572]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]]
Printing some Q and Qe and total Qs values:  [[0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.671]
 [0.62 ]
 [0.637]] [[54.445]
 [54.445]
 [54.445]
 [54.445]
 [56.213]
 [51.816]
 [54.445]] [[1.616]
 [1.616]
 [1.616]
 [1.616]
 [1.713]
 [1.506]
 [1.616]]
Printing some Q and Qe and total Qs values:  [[0.58 ]
 [0.58 ]
 [0.58 ]
 [0.58 ]
 [0.626]
 [0.58 ]
 [0.58 ]] [[ 81.365]
 [ 81.365]
 [ 81.365]
 [ 81.365]
 [101.229]
 [ 81.365]
 [ 81.365]] [[1.856]
 [1.856]
 [1.856]
 [1.856]
 [2.293]
 [1.856]
 [1.856]]
siam score:  -0.8409082
printing an ep nov before normalisation:  41.82164602497988
printing an ep nov before normalisation:  38.12162578329777
Printing some Q and Qe and total Qs values:  [[0.856]
 [0.854]
 [0.769]
 [0.854]
 [0.854]
 [0.854]
 [0.854]] [[34.232]
 [34.565]
 [34.871]
 [34.565]
 [34.565]
 [34.565]
 [34.565]] [[2.061]
 [2.082]
 [2.019]
 [2.082]
 [2.082]
 [2.082]
 [2.082]]
maxi score, test score, baseline:  -0.9968604863221885 -1.0 -0.9968604863221885
probs:  [0.10243942848228116, 0.08296658974103177, 0.08296658974103177, 0.5602170383434182, 0.09106663610819013, 0.08034371758404713]
printing an ep nov before normalisation:  31.99008518343598
printing an ep nov before normalisation:  42.246214369202036
printing an ep nov before normalisation:  47.771690650880615
maxi score, test score, baseline:  -0.9968604863221885 -1.0 -0.9968604863221885
actions average: 
K:  3  action  0 :  tensor([0.4546, 0.0161, 0.0823, 0.1037, 0.1472, 0.0810, 0.1151],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0042, 0.9393, 0.0112, 0.0132, 0.0021, 0.0037, 0.0264],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.2161, 0.0019, 0.2381, 0.1400, 0.1239, 0.1504, 0.1296],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2110, 0.0095, 0.1133, 0.2657, 0.1429, 0.1303, 0.1273],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1972, 0.0210, 0.1036, 0.1673, 0.2665, 0.1155, 0.1289],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.2710, 0.0012, 0.1249, 0.1699, 0.1625, 0.1332, 0.1373],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1780, 0.0152, 0.1251, 0.1841, 0.1528, 0.1403, 0.2045],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.497]
 [0.452]
 [0.473]
 [0.454]
 [0.454]
 [0.454]] [[40.286]
 [45.712]
 [41.274]
 [43.746]
 [41.954]
 [42.864]
 [44.071]] [[1.229]
 [1.483]
 [1.266]
 [1.383]
 [1.294]
 [1.33 ]
 [1.376]]
siam score:  -0.8278124
Printing some Q and Qe and total Qs values:  [[0.296]
 [0.303]
 [0.29 ]
 [0.288]
 [0.284]
 [0.285]
 [0.276]] [[41.751]
 [44.635]
 [43.072]
 [42.678]
 [43.012]
 [43.537]
 [45.456]] [[1.239]
 [1.378]
 [1.294]
 [1.273]
 [1.284]
 [1.31 ]
 [1.388]]
maxi score, test score, baseline:  -0.9968604863221885 -1.0 -0.9968604863221885
probs:  [0.10329274767110021, 0.08341424086337734, 0.08073672770152078, 0.5629330255252362, 0.0888865305372447, 0.08073672770152078]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.9968604863221885 -1.0 -0.9968604863221885
probs:  [0.10339403554935936, 0.08341448275922153, 0.08072335932218251, 0.5628301739931543, 0.08891458905389966, 0.08072335932218251]
maxi score, test score, baseline:  -0.9968604863221885 -1.0 -0.9968604863221885
probs:  [0.10339403554935936, 0.08341448275922153, 0.08072335932218251, 0.5628301739931543, 0.08891458905389966, 0.08072335932218251]
printing an ep nov before normalisation:  39.85537435821029
maxi score, test score, baseline:  -0.9968604863221885 -1.0 -0.9968604863221885
probs:  [0.10349575167650989, 0.08341472567781025, 0.08070993442084255, 0.5627268876009951, 0.08894276620299969, 0.08070993442084255]
from probs:  [0.10349575167650989, 0.08341472567781025, 0.08070993442084255, 0.5627268876009951, 0.08894276620299969, 0.08070993442084255]
maxi score, test score, baseline:  -0.9968604863221885 -1.0 -0.9968604863221885
probs:  [0.10378455294481072, 0.08364741932121088, 0.08093507071068516, 0.564298808980719, 0.08639907733188908, 0.08093507071068516]
maxi score, test score, baseline:  -0.9968604863221885 -1.0 -0.9968604863221885
probs:  [0.10378455294481072, 0.08364741932121088, 0.08093507071068516, 0.564298808980719, 0.08639907733188908, 0.08093507071068516]
Printing some Q and Qe and total Qs values:  [[0.521]
 [0.523]
 [0.501]
 [0.499]
 [0.497]
 [0.492]
 [0.499]] [[25.07 ]
 [30.072]
 [25.667]
 [25.773]
 [25.55 ]
 [25.594]
 [25.546]] [[1.437]
 [1.806]
 [1.461]
 [1.467]
 [1.448]
 [1.446]
 [1.45 ]]
maxi score, test score, baseline:  -0.9968604863221885 -1.0 -0.9968604863221885
probs:  [0.10108178786817175, 0.08389946489785176, 0.08117893042755112, 0.5660014589749204, 0.08665942740395394, 0.08117893042755112]
printing an ep nov before normalisation:  29.20627179019658
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.53428604849569
maxi score, test score, baseline:  -0.9968604863221885 -1.0 -0.9968604863221885
maxi score, test score, baseline:  -0.9968604863221885 -1.0 -0.9968604863221885
probs:  [0.10126302341299113, 0.08390491585240446, 0.08115654882197826, 0.565825848801579, 0.08669311428906881, 0.08115654882197826]
printing an ep nov before normalisation:  49.443874574257265
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9968604863221885 -1.0 -0.9968604863221885
maxi score, test score, baseline:  -0.9968604863221885 -1.0 -0.9968604863221885
probs:  [0.10126302341299113, 0.08390491585240446, 0.08115654882197826, 0.565825848801579, 0.08669311428906881, 0.08115654882197826]
printing an ep nov before normalisation:  60.176921317722254
maxi score, test score, baseline:  -0.996869696969697 -1.0 -0.996869696969697
probs:  [0.10275347124128525, 0.08567296553475053, 0.08030249920149023, 0.5598859385558203, 0.0884165733354379, 0.08296855213121583]
printing an ep nov before normalisation:  93.20826273858447
printing an ep nov before normalisation:  31.54769488276891
printing an ep nov before normalisation:  37.38992858498964
siam score:  -0.82721734
printing an ep nov before normalisation:  47.36691498617839
Printing some Q and Qe and total Qs values:  [[0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]] [[46.496]
 [46.496]
 [46.496]
 [46.496]
 [46.496]
 [46.496]
 [46.496]] [[62.404]
 [62.404]
 [62.404]
 [62.404]
 [62.404]
 [62.404]
 [62.404]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.096084117889404
maxi score, test score, baseline:  -0.996869696969697 -1.0 -0.996869696969697
probs:  [0.10285068310262008, 0.08568453014027412, 0.08028713452799983, 0.5597692010178471, 0.08844189529002294, 0.08296655592123595]
maxi score, test score, baseline:  -0.996869696969697 -1.0 -0.996869696969697
maxi score, test score, baseline:  -0.996869696969697 -1.0 -0.996869696969697
probs:  [0.10285068310262008, 0.08568453014027412, 0.08028713452799983, 0.5597692010178471, 0.08844189529002294, 0.08296655592123595]
printing an ep nov before normalisation:  21.198936869559493
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]] [[54.774]
 [54.774]
 [54.774]
 [54.774]
 [54.774]
 [54.774]
 [54.774]] [[2.145]
 [2.145]
 [2.145]
 [2.145]
 [2.145]
 [2.145]
 [2.145]]
maxi score, test score, baseline:  -0.996869696969697 -1.0 -0.996869696969697
probs:  [0.10294829936664611, 0.085696142854717, 0.08027170593725465, 0.5596519778500972, 0.08846732258429015, 0.08296455140699495]
printing an ep nov before normalisation:  30.109915486024423
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.602]
 [0.594]
 [0.594]
 [0.591]
 [0.591]
 [0.582]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.574]
 [0.602]
 [0.594]
 [0.594]
 [0.591]
 [0.591]
 [0.582]]
maxi score, test score, baseline:  -0.996869696969697 -1.0 -0.996869696969697
probs:  [0.10351362436278497, 0.08616660748587766, 0.08071234449621174, 0.5627284716730363, 0.08616660748587766, 0.08071234449621174]
line 256 mcts: sample exp_bonus 50.04968759101159
printing an ep nov before normalisation:  42.449399154693225
maxi score, test score, baseline:  -0.996869696969697 -1.0 -0.996869696969697
probs:  [0.10351362436278497, 0.08616660748587766, 0.08071234449621174, 0.5627284716730363, 0.08616660748587766, 0.08071234449621174]
printing an ep nov before normalisation:  28.475596146320623
maxi score, test score, baseline:  -0.996869696969697 -1.0 -0.996869696969697
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.996869696969697 -1.0 -0.996869696969697
probs:  [0.10351362436278497, 0.08616660748587766, 0.08071234449621174, 0.5627284716730363, 0.08616660748587766, 0.08071234449621174]
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.84850047652126
maxi score, test score, baseline:  -0.996869696969697 -1.0 -0.996869696969697
probs:  [0.10127502869414416, 0.08669764373625508, 0.08115616973807882, 0.5658078723348411, 0.083907115758602, 0.08115616973807882]
maxi score, test score, baseline:  -0.996869696969697 -1.0 -0.996869696969697
probs:  [0.10136555715216317, 0.08671448219414891, 0.08114499554344279, 0.5657201217217591, 0.08390984784504331, 0.08114499554344279]
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.448]
 [0.417]
 [0.415]
 [0.413]
 [0.404]
 [0.432]] [[35.137]
 [36.482]
 [35.067]
 [36.001]
 [36.209]
 [37.612]
 [37.471]] [[1.293]
 [1.384]
 [1.284]
 [1.328]
 [1.335]
 [1.396]
 [1.417]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.996869696969697 -1.0 -0.996869696969697
probs:  [0.10136555715216317, 0.08671448219414891, 0.08114499554344279, 0.5657201217217591, 0.08390984784504331, 0.08114499554344279]
printing an ep nov before normalisation:  34.808232455539496
maxi score, test score, baseline:  -0.996869696969697 -1.0 -0.996869696969697
probs:  [0.10173904578379013, 0.08697289795952168, 0.08135966729724661, 0.5672090543649483, 0.08135966729724661, 0.08135966729724661]
maxi score, test score, baseline:  -0.996869696969697 -1.0 -0.996869696969697
probs:  [0.10173904578379013, 0.08697289795952168, 0.08135966729724661, 0.5672090543649483, 0.08135966729724661, 0.08135966729724661]
maxi score, test score, baseline:  -0.996869696969697 -1.0 -0.996869696969697
maxi score, test score, baseline:  -0.996869696969697 -1.0 -0.996869696969697
probs:  [0.10183204335712209, 0.08699115208726162, 0.08134950831375437, 0.5671282796143531, 0.08134950831375437, 0.08134950831375437]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.996869696969697 -1.0 -0.996869696969697
probs:  [0.1019254461092302, 0.0870094857459492, 0.0813393050688438, 0.5670471529382893, 0.0813393050688438, 0.0813393050688438]
maxi score, test score, baseline:  -0.996869696969697 -1.0 -0.996869696969697
Printing some Q and Qe and total Qs values:  [[0.24 ]
 [0.961]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]] [[32.109]
 [ 0.916]
 [32.109]
 [32.109]
 [32.109]
 [32.109]
 [32.109]] [[1.456]
 [0.988]
 [1.456]
 [1.456]
 [1.456]
 [1.456]
 [1.456]]
printing an ep nov before normalisation:  22.861588512853025
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.539]
 [0.539]
 [0.587]
 [0.585]
 [0.544]
 [0.539]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.539]
 [0.539]
 [0.539]
 [0.587]
 [0.585]
 [0.544]
 [0.539]]
line 256 mcts: sample exp_bonus 0.019961863050404816
maxi score, test score, baseline:  -0.996869696969697 -1.0 -0.996869696969697
maxi score, test score, baseline:  -0.996869696969697 -1.0 -0.996869696969697
probs:  [0.10220811209209946, 0.08706496913948468, 0.0813084268539517, 0.5668016382065607, 0.0813084268539517, 0.0813084268539517]
printing an ep nov before normalisation:  37.2977666379835
maxi score, test score, baseline:  -0.9968788519637463 -1.0 -0.9968788519637463
Printing some Q and Qe and total Qs values:  [[0.55 ]
 [0.54 ]
 [0.55 ]
 [0.565]
 [0.564]
 [0.565]
 [0.566]] [[62.14 ]
 [75.571]
 [62.14 ]
 [60.183]
 [59.725]
 [60.66 ]
 [60.476]] [[1.236]
 [1.517]
 [1.236]
 [1.209]
 [1.198]
 [1.22 ]
 [1.217]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
rdn beta is 0 so we're just using the maxi policy
Starting evaluation
printing an ep nov before normalisation:  47.595212410365924
printing an ep nov before normalisation:  45.985752918642895
maxi score, test score, baseline:  -0.9968788519637463 -1.0 -0.9968788519637463
probs:  [0.10239866181238834, 0.08710237136045522, 0.08128761130213172, 0.5666361329207613, 0.08128761130213172, 0.08128761130213172]
printing an ep nov before normalisation:  64.2968359637958
printing an ep nov before normalisation:  23.082773511662474
printing an ep nov before normalisation:  33.520843519831075
Printing some Q and Qe and total Qs values:  [[0.753]
 [0.326]
 [0.358]
 [0.353]
 [0.351]
 [0.344]
 [0.351]] [[23.748]
 [39.776]
 [29.353]
 [30.486]
 [31.191]
 [30.454]
 [30.061]] [[0.753]
 [0.326]
 [0.358]
 [0.353]
 [0.351]
 [0.344]
 [0.351]]
Printing some Q and Qe and total Qs values:  [[0.828]
 [0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]] [[26.437]
 [33.499]
 [33.499]
 [33.499]
 [33.499]
 [33.499]
 [33.499]] [[0.828]
 [0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]]
siam score:  -0.8315451
printing an ep nov before normalisation:  25.87247133255005
maxi score, test score, baseline:  -0.9968788519637463 -1.0 -0.9968788519637463
Printing some Q and Qe and total Qs values:  [[0.882]
 [0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]] [[33.727]
 [31.618]
 [31.618]
 [31.618]
 [31.618]
 [31.618]
 [31.618]] [[0.882]
 [0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]]
printing an ep nov before normalisation:  29.007940317101166
maxi score, test score, baseline:  -0.9968879518072289 -1.0 -0.9968879518072289
probs:  [0.10239869218925014, 0.08710237728936991, 0.08128760793735872, 0.5666361067093038, 0.08128760793735872, 0.08128760793735872]
maxi score, test score, baseline:  -0.9968879518072289 -1.0 -0.9968879518072289
probs:  [0.10249458300710752, 0.08712119932035327, 0.08127713289758716, 0.5665528189797777, 0.08127713289758716, 0.08127713289758716]
printing an ep nov before normalisation:  35.195722575491075
printing an ep nov before normalisation:  11.7368483543396
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9968879518072289 -1.0 -0.9968879518072289
probs:  [0.10249458300710752, 0.08712119932035327, 0.08127713289758716, 0.5665528189797777, 0.08127713289758716, 0.08127713289758716]
printing an ep nov before normalisation:  25.96631423456781
printing an ep nov before normalisation:  15.206643930854234
Printing some Q and Qe and total Qs values:  [[0.873]
 [0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.867]] [[34.206]
 [34.414]
 [34.414]
 [34.414]
 [34.414]
 [34.414]
 [34.414]] [[0.873]
 [0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.867]]
printing an ep nov before normalisation:  11.403812431851698
Printing some Q and Qe and total Qs values:  [[0.887]
 [0.873]
 [0.873]
 [0.873]
 [0.873]
 [0.873]
 [0.873]] [[33.461]
 [31.666]
 [31.666]
 [31.666]
 [31.666]
 [31.666]
 [31.666]] [[0.887]
 [0.873]
 [0.873]
 [0.873]
 [0.873]
 [0.873]
 [0.873]]
printing an ep nov before normalisation:  10.672607421875
printing an ep nov before normalisation:  13.557801745878285
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]] [[12.613]
 [12.613]
 [12.613]
 [12.613]
 [12.613]
 [12.613]
 [12.613]] [[0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]]
printing an ep nov before normalisation:  11.633361253937844
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.336]
 [0.352]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]] [[10.823]
 [24.667]
 [10.991]
 [10.673]
 [11.007]
 [11.152]
 [11.763]] [[0.522]
 [0.336]
 [0.352]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]]
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.563]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]] [[34.454]
 [35.593]
 [34.454]
 [34.454]
 [34.454]
 [34.454]
 [34.454]] [[0.568]
 [0.563]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]]
printing an ep nov before normalisation:  14.791982825614664
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.316]
 [0.341]
 [0.345]
 [0.334]
 [0.335]
 [0.325]] [[18.275]
 [41.055]
 [12.915]
 [12.414]
 [14.143]
 [14.975]
 [31.348]] [[0.349]
 [0.316]
 [0.341]
 [0.345]
 [0.334]
 [0.335]
 [0.325]]
printing an ep nov before normalisation:  31.969955720262483
printing an ep nov before normalisation:  30.506870836225673
maxi score, test score, baseline:  -0.996896996996997 -1.0 -0.996896996996997
probs:  [0.09964411203329122, 0.08739768663545072, 0.08153503617903778, 0.5683530927941447, 0.08153503617903778, 0.08153503617903778]
printing an ep nov before normalisation:  11.070750591116445
printing an ep nov before normalisation:  27.680554084058784
printing an ep nov before normalisation:  18.385207130888833
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]] [[32.874]
 [31.716]
 [31.716]
 [31.716]
 [31.716]
 [31.716]
 [31.716]] [[0.481]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]]
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.555]
 [0.589]
 [0.574]
 [0.55 ]
 [0.548]
 [0.54 ]] [[39.316]
 [41.841]
 [33.593]
 [39.386]
 [44.178]
 [48.333]
 [41.989]] [[0.552]
 [0.555]
 [0.589]
 [0.574]
 [0.55 ]
 [0.548]
 [0.54 ]]
printing an ep nov before normalisation:  26.12514055169848
maxi score, test score, baseline:  -0.996896996996997 -1.0 -0.996896996996997
probs:  [0.09964411203329122, 0.08739768663545072, 0.08153503617903778, 0.5683530927941447, 0.08153503617903778, 0.08153503617903778]
printing an ep nov before normalisation:  27.144768875775604
Printing some Q and Qe and total Qs values:  [[0.357]
 [0.281]
 [0.282]
 [0.282]
 [0.28 ]
 [0.279]
 [0.276]] [[14.112]
 [25.766]
 [12.789]
 [13.615]
 [14.424]
 [14.296]
 [12.645]] [[0.357]
 [0.281]
 [0.282]
 [0.282]
 [0.28 ]
 [0.279]
 [0.276]]
maxi score, test score, baseline:  -0.996896996996997 -1.0 -0.996896996996997
probs:  [0.09964411203329122, 0.08739768663545072, 0.08153503617903778, 0.5683530927941447, 0.08153503617903778, 0.08153503617903778]
printing an ep nov before normalisation:  11.792452778312791
printing an ep nov before normalisation:  11.832177836047842
printing an ep nov before normalisation:  38.133934135525024
printing an ep nov before normalisation:  11.349195923411358
maxi score, test score, baseline:  -0.9969059880239521 -1.0 -0.9969059880239521
probs:  [0.09964413783439224, 0.08739769299359942, 0.08153503322939007, 0.5683530694838381, 0.08153503322939007, 0.08153503322939007]
printing an ep nov before normalisation:  37.18656740248561
printing an ep nov before normalisation:  10.390322208404541
printing an ep nov before normalisation:  11.036438383666976
printing an ep nov before normalisation:  12.761217041429203
Printing some Q and Qe and total Qs values:  [[0.458]
 [0.361]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]] [[14.721]
 [26.601]
 [11.397]
 [11.397]
 [11.397]
 [11.397]
 [11.397]] [[0.458]
 [0.361]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]]
printing an ep nov before normalisation:  29.389265909035814
printing an ep nov before normalisation:  12.789105232333018
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]] [[16.077]
 [16.077]
 [16.077]
 [16.077]
 [16.077]
 [16.077]
 [16.077]] [[0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]]
printing an ep nov before normalisation:  31.25287277506711
using explorer policy with actor:  0
printing an ep nov before normalisation:  11.456758975982666
maxi score, test score, baseline:  -0.9969149253731343 -1.0 -0.9969149253731343
probs:  [0.09964416348106206, 0.0873976993136919, 0.08153503029739773, 0.5683530463130528, 0.08153503029739773, 0.08153503029739773]
printing an ep nov before normalisation:  25.64414879986852
Printing some Q and Qe and total Qs values:  [[0.392]
 [0.323]
 [0.344]
 [0.343]
 [0.342]
 [0.351]
 [0.342]] [[19.196]
 [28.798]
 [11.792]
 [12.028]
 [12.439]
 [10.315]
 [12.469]] [[0.392]
 [0.323]
 [0.344]
 [0.343]
 [0.342]
 [0.351]
 [0.342]]
printing an ep nov before normalisation:  25.734306690899867
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
printing an ep nov before normalisation:  16.708144144158318
maxi score, test score, baseline:  -0.9969238095238095 -1.0 -0.9969238095238095
probs:  [0.09964418897468312, 0.08739770559606899, 0.08153502738290268, 0.5683530232805397, 0.08153502738290268, 0.08153502738290268]
printing an ep nov before normalisation:  33.750717040247494
printing an ep nov before normalisation:  15.428760849885334
printing an ep nov before normalisation:  14.175790345420786
using explorer policy with actor:  0
printing an ep nov before normalisation:  22.689460797957437
Printing some Q and Qe and total Qs values:  [[0.601]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]] [[13.298]
 [11.168]
 [11.168]
 [11.168]
 [11.168]
 [11.168]
 [11.168]] [[0.601]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]]
maxi score, test score, baseline:  -0.9969238095238095 -1.0 -0.9969238095238095
probs:  [0.09964418897468312, 0.08739770559606899, 0.08153502738290268, 0.5683530232805397, 0.08153502738290268, 0.08153502738290268]
printing an ep nov before normalisation:  19.70332055108651
maxi score, test score, baseline:  -0.9969238095238095 -1.0 -0.9969238095238095
probs:  [0.09964418897468312, 0.08739770559606899, 0.08153502738290268, 0.5683530232805397, 0.08153502738290268, 0.08153502738290268]
printing an ep nov before normalisation:  51.19171553871846
printing an ep nov before normalisation:  12.03498628511696
printing an ep nov before normalisation:  16.02645532657334
Printing some Q and Qe and total Qs values:  [[0.363]
 [0.319]
 [0.342]
 [0.348]
 [0.351]
 [0.354]
 [0.353]] [[35.758]
 [41.017]
 [35.856]
 [34.258]
 [33.671]
 [32.266]
 [32.651]] [[0.363]
 [0.319]
 [0.342]
 [0.348]
 [0.351]
 [0.354]
 [0.353]]
Printing some Q and Qe and total Qs values:  [[0.772]
 [0.641]
 [0.641]
 [0.609]
 [0.641]
 [0.641]
 [0.641]] [[19.84 ]
 [17.067]
 [17.067]
 [18.476]
 [17.067]
 [17.067]
 [17.067]] [[0.772]
 [0.641]
 [0.641]
 [0.609]
 [0.641]
 [0.641]
 [0.641]]
printing an ep nov before normalisation:  17.067342226848012
maxi score, test score, baseline:  -0.9969326409495549 -1.0 -0.9969326409495549
printing an ep nov before normalisation:  52.721049647980664
printing an ep nov before normalisation:  41.186670883156644
Printing some Q and Qe and total Qs values:  [[0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]] [[56.17]
 [56.17]
 [56.17]
 [56.17]
 [56.17]
 [56.17]
 [56.17]] [[0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]]
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  80 total reward:  0.1266666666666657  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  16.302999902557122
maxi score, test score, baseline:  -0.9969501474926253 -1.0 -0.9969501474926253
probs:  [0.09500199899050646, 0.08624808983711733, 0.0820573886466651, 0.5725777452323809, 0.0820573886466651, 0.0820573886466651]
printing an ep nov before normalisation:  43.40250711817968
printing an ep nov before normalisation:  71.71492592609746
printing an ep nov before normalisation:  34.457715822500276
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]] [[20.108]
 [17.822]
 [17.822]
 [17.822]
 [17.822]
 [17.822]
 [17.822]] [[0.402]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]]
printing an ep nov before normalisation:  21.52634611007719
printing an ep nov before normalisation:  21.033538703576284
printing an ep nov before normalisation:  16.65768157707924
printing an ep nov before normalisation:  53.527854880276536
printing an ep nov before normalisation:  11.118850708007812
using explorer policy with actor:  0
siam score:  -0.82173246
maxi score, test score, baseline:  -0.9969588235294118 -1.0 -0.9969588235294118
probs:  [0.09500201830540458, 0.08624809459872404, 0.08205738644127057, 0.5725777277720596, 0.08205738644127057, 0.08205738644127057]
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.351]
 [0.36 ]
 [0.36 ]
 [0.356]
 [0.357]
 [0.362]] [[20.266]
 [26.683]
 [17.806]
 [17.766]
 [18.307]
 [19.018]
 [19.713]] [[0.459]
 [0.351]
 [0.36 ]
 [0.36 ]
 [0.356]
 [0.357]
 [0.362]]
Printing some Q and Qe and total Qs values:  [[0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]] [[35.284]
 [35.284]
 [35.284]
 [35.284]
 [35.284]
 [35.284]
 [35.284]] [[0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]]
maxi score, test score, baseline:  -0.9969588235294118 -1.0 -0.9969588235294118
probs:  [0.09500201830540458, 0.08624809459872404, 0.08205738644127057, 0.5725777277720596, 0.08205738644127057, 0.08205738644127057]
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.515]
 [0.515]
 [0.516]
 [0.515]
 [0.515]
 [0.515]] [[10.863]
 [10.863]
 [10.863]
 [12.291]
 [10.863]
 [10.863]
 [10.863]] [[0.515]
 [0.515]
 [0.515]
 [0.516]
 [0.515]
 [0.515]
 [0.515]]
siam score:  -0.8234662
printing an ep nov before normalisation:  15.652558456319797
printing an ep nov before normalisation:  40.03527999205516
printing an ep nov before normalisation:  35.623674392700195
printing an ep nov before normalisation:  49.4033467229296
Printing some Q and Qe and total Qs values:  [[0.785]
 [0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]] [[19.004]
 [16.035]
 [16.035]
 [16.035]
 [16.035]
 [16.035]
 [16.035]] [[0.785]
 [0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]]
printing an ep nov before normalisation:  32.946109771728516
printing an ep nov before normalisation:  29.077426200064533
printing an ep nov before normalisation:  11.94070816040039
maxi score, test score, baseline:  -0.9969674486803519 -1.0 -0.9969674486803519
probs:  [0.09621083737343177, 0.08756330616836687, 0.08139737282672736, 0.5679814224485259, 0.08342353059147413, 0.08342353059147413]
maxi score, test score, baseline:  -0.9969674486803519 -1.0 -0.9969674486803519
probs:  [0.09621083737343177, 0.08756330616836687, 0.08139737282672736, 0.5679814224485259, 0.08342353059147413, 0.08342353059147413]
actions average: 
K:  0  action  0 :  tensor([0.5287, 0.0048, 0.0960, 0.1113, 0.1199, 0.0700, 0.0692],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0016, 0.9771, 0.0033, 0.0080, 0.0023, 0.0030, 0.0047],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1702, 0.0086, 0.2331, 0.2081, 0.1459, 0.1236, 0.1105],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1565, 0.0175, 0.1241, 0.3516, 0.1246, 0.1098, 0.1159],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1794, 0.0309, 0.1501, 0.1542, 0.2246, 0.1302, 0.1305],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1386, 0.0289, 0.1295, 0.2103, 0.1472, 0.2276, 0.1179],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2093, 0.0133, 0.1387, 0.1981, 0.1559, 0.1329, 0.1517],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  20.137242883356663
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]] [[16.447]
 [16.447]
 [16.447]
 [16.447]
 [16.447]
 [16.447]
 [16.447]] [[0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]]
maxi score, test score, baseline:  -0.9969674486803519 -1.0 -0.9969674486803519
probs:  [0.09737429863688986, 0.08882918338205517, 0.08076211653308546, 0.5635575280129561, 0.0847384367175067, 0.0847384367175067]
printing an ep nov before normalisation:  27.36699873156617
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.414]
 [0.405]
 [0.407]
 [0.414]
 [0.414]
 [0.414]] [[13.238]
 [13.238]
 [11.434]
 [11.471]
 [13.238]
 [13.238]
 [13.238]] [[0.414]
 [0.414]
 [0.405]
 [0.407]
 [0.414]
 [0.414]
 [0.414]]
printing an ep nov before normalisation:  16.341054164921577
printing an ep nov before normalisation:  11.782114157698151
using explorer policy with actor:  1
printing an ep nov before normalisation:  15.418150098064842
printing an ep nov before normalisation:  26.217402403981694
using explorer policy with actor:  0
printing an ep nov before normalisation:  21.896024114395885
maxi score, test score, baseline:  -0.996993023255814 -1.0 -0.996993023255814
probs:  [0.09756960589915449, 0.08900729284156444, 0.08092399030467873, 0.5646886753286815, 0.08290212243958563, 0.08490831318633517]
Printing some Q and Qe and total Qs values:  [[0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]] [[46.339]
 [46.339]
 [46.339]
 [46.339]
 [46.339]
 [46.339]
 [46.339]] [[0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]]
printing an ep nov before normalisation:  34.90971602949572
using explorer policy with actor:  1
printing an ep nov before normalisation:  12.562577792713814
printing an ep nov before normalisation:  35.49961922155591
maxi score, test score, baseline:  -0.9970671388101983 -1.0 -0.9970671388101983
probs:  [0.09559098780629886, 0.08920239479538614, 0.08110124188644312, 0.5659272698218969, 0.08308374233423026, 0.08509436335574483]
maxi score, test score, baseline:  -0.9970671388101983 -1.0 -0.9970671388101983
probs:  [0.0956435691941157, 0.08922749647969844, 0.08109149751316459, 0.5658531165076491, 0.08308252542927062, 0.0851017948761015]
printing an ep nov before normalisation:  25.702617132537895
printing an ep nov before normalisation:  80.61202824526355
printing an ep nov before normalisation:  65.44569122989684
printing an ep nov before normalisation:  47.760886154055974
maxi score, test score, baseline:  -0.9970671388101983 -1.0 -0.9970671388101983
maxi score, test score, baseline:  -0.9970671388101983 -1.0 -0.9970671388101983
probs:  [0.09376570823793173, 0.0894736111683308, 0.08124959300699754, 0.5669456510373242, 0.08326216083169001, 0.08530327571772567]
printing an ep nov before normalisation:  36.23085021972656
maxi score, test score, baseline:  -0.9970671388101983 -1.0 -0.9970671388101983
printing an ep nov before normalisation:  77.12064817728924
printing an ep nov before normalisation:  59.214137011088646
maxi score, test score, baseline:  -0.9970671388101983 -1.0 -0.9970671388101983
probs:  [0.09390008157695388, 0.08955252336975575, 0.08122223701470468, 0.5667360472792713, 0.0832608106121556, 0.08532830014715895]
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.466]
 [0.502]
 [0.522]
 [0.489]
 [0.522]
 [0.498]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.522]
 [0.466]
 [0.502]
 [0.522]
 [0.489]
 [0.522]
 [0.498]]
using another actor
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.3941203643892
actions average: 
K:  2  action  0 :  tensor([0.5139, 0.0210, 0.0865, 0.1269, 0.0985, 0.0739, 0.0793],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0085, 0.9103, 0.0068, 0.0258, 0.0055, 0.0048, 0.0383],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1541, 0.0135, 0.1890, 0.2014, 0.1423, 0.1565, 0.1432],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1028, 0.0308, 0.1038, 0.3594, 0.1409, 0.1109, 0.1515],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1528, 0.0754, 0.1302, 0.1717, 0.2197, 0.1183, 0.1319],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1237, 0.1277, 0.1224, 0.1404, 0.1007, 0.2769, 0.1081],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1308, 0.0371, 0.1375, 0.2562, 0.1492, 0.1370, 0.1524],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.396]
 [0.396]
 [0.396]
 [0.393]
 [0.396]
 [0.396]] [[10.882]
 [10.882]
 [10.882]
 [10.882]
 [11.511]
 [10.882]
 [10.882]] [[0.928]
 [0.928]
 [0.928]
 [0.928]
 [0.959]
 [0.928]
 [0.928]]
actions average: 
K:  2  action  0 :  tensor([0.5481, 0.0144, 0.0942, 0.1065, 0.0834, 0.0682, 0.0852],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0150, 0.9370, 0.0095, 0.0099, 0.0056, 0.0142, 0.0088],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1244, 0.0284, 0.1665, 0.2646, 0.1420, 0.1640, 0.1101],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0885, 0.0305, 0.1140, 0.4166, 0.1171, 0.1140, 0.1192],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1889, 0.0042, 0.1292, 0.1845, 0.2488, 0.1255, 0.1187],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1083, 0.0223, 0.2193, 0.1602, 0.1211, 0.2480, 0.1209],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1120, 0.0614, 0.1484, 0.1966, 0.1156, 0.1257, 0.2403],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  20.688649079674555
line 256 mcts: sample exp_bonus 28.547056820797906
maxi score, test score, baseline:  -0.9970830985915493 -1.0 -0.9970830985915493
probs:  [0.09399037751112861, 0.08960555061471365, 0.0812038543236808, 0.5665951983398254, 0.08325990323997226, 0.08534511597067924]
maxi score, test score, baseline:  -0.9970830985915493 -1.0 -0.9970830985915493
probs:  [0.09399037751112861, 0.08960555061471365, 0.0812038543236808, 0.5665951983398254, 0.08325990323997226, 0.08534511597067924]
printing an ep nov before normalisation:  37.4132186504072
maxi score, test score, baseline:  -0.9970830985915493 -1.0 -0.9970830985915493
probs:  [0.09399037751112861, 0.08960555061471365, 0.0812038543236808, 0.5665951983398254, 0.08325990323997226, 0.08534511597067924]
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.457]
 [0.457]
 [0.457]
 [0.453]
 [0.457]
 [0.457]] [[44.828]
 [44.828]
 [44.828]
 [44.828]
 [55.255]
 [44.828]
 [44.828]] [[1.333]
 [1.333]
 [1.333]
 [1.333]
 [1.692]
 [1.333]
 [1.333]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
printing an ep nov before normalisation:  51.70340253600367
printing an ep nov before normalisation:  39.00594711303711
actions average: 
K:  4  action  0 :  tensor([0.5321, 0.0051, 0.0907, 0.0915, 0.0933, 0.1034, 0.0838],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0337, 0.7453, 0.0425, 0.0384, 0.0356, 0.0485, 0.0560],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1981, 0.0488, 0.1887, 0.1428, 0.1369, 0.1489, 0.1358],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1793, 0.0442, 0.1600, 0.1937, 0.1470, 0.1456, 0.1301],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1449, 0.0666, 0.0982, 0.1553, 0.3072, 0.1098, 0.1180],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1002, 0.0264, 0.1426, 0.1166, 0.1030, 0.4085, 0.1028],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1752, 0.1526, 0.1369, 0.1428, 0.1093, 0.1222, 0.1610],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.519]
 [0.47 ]
 [0.47 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.47 ]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.519]
 [0.47 ]
 [0.47 ]]
maxi score, test score, baseline:  -0.9970830985915493 -1.0 -0.9970830985915493
maxi score, test score, baseline:  -0.9970830985915493 -1.0 -0.9970830985915493
probs:  [0.09437839239571658, 0.08772160672362177, 0.08134412674406248, 0.5675502354591196, 0.08344001218335427, 0.08556562649412537]
printing an ep nov before normalisation:  65.697065490288
printing an ep nov before normalisation:  33.326900005340576
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.4119],
        [-0.6360],
        [-0.0000],
        [-0.5785],
        [-0.5474],
        [-0.6271],
        [-0.5954],
        [-0.4032],
        [-0.5111]], dtype=torch.float64)
-0.8756220000000001 -0.8756220000000001
-0.071551887066 -0.4834756792580823
-0.032346567066 -0.6683731573606477
-0.7854000000000001 -0.7854000000000001
-0.032346567066 -0.6108681356309466
-0.032346567066 -0.579745357662556
-0.032346567066 -0.65949166480554
-0.032346567066 -0.6277591516565685
-0.045546567066 -0.4487574165059747
-0.032346567066 -0.5434729537058429
printing an ep nov before normalisation:  32.742878400106996
printing an ep nov before normalisation:  39.16450201023146
printing an ep nov before normalisation:  45.36575374092714
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9970830985915493 -1.0 -0.9970830985915493
printing an ep nov before normalisation:  61.819111423462914
maxi score, test score, baseline:  -0.9970830985915493 -1.0 -0.9970830985915493
probs:  [0.09560396167698226, 0.08902870265527331, 0.08068786482217954, 0.5629807983631245, 0.08479954544412716, 0.08689912703831323]
printing an ep nov before normalisation:  51.55887125719958
printing an ep nov before normalisation:  39.29475753470481
Printing some Q and Qe and total Qs values:  [[-0.018]
 [-0.017]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]] [[33.533]
 [34.82 ]
 [33.533]
 [33.533]
 [30.545]
 [33.533]
 [33.533]] [[1.239]
 [1.316]
 [1.239]
 [1.239]
 [1.062]
 [1.239]
 [1.239]]
printing an ep nov before normalisation:  42.02773829276936
maxi score, test score, baseline:  -0.9970830985915493 -1.0 -0.9970830985915493
probs:  [0.09560396167698226, 0.08902870265527331, 0.08068786482217954, 0.5629807983631245, 0.08479954544412716, 0.08689912703831323]
printing an ep nov before normalisation:  32.25437919055871
printing an ep nov before normalisation:  31.657130894082684
Printing some Q and Qe and total Qs values:  [[0.664]
 [0.422]
 [0.4  ]
 [0.435]
 [0.435]
 [0.435]
 [0.435]] [[14.249]
 [19.13 ]
 [13.577]
 [13.928]
 [13.928]
 [13.928]
 [13.928]] [[0.664]
 [0.422]
 [0.4  ]
 [0.435]
 [0.435]
 [0.435]
 [0.435]]
maxi score, test score, baseline:  -0.9970830985915493 -1.0 -0.9970830985915493
probs:  [0.09580217558806697, 0.08921326518085111, 0.08085511031243843, 0.5641493681725132, 0.08290081255295902, 0.08707926819317127]
maxi score, test score, baseline:  -0.9970830985915493 -1.0 -0.9970830985915493
probs:  [0.09580217558806697, 0.08921326518085111, 0.08085511031243843, 0.5641493681725132, 0.08290081255295902, 0.08707926819317127]
maxi score, test score, baseline:  -0.9970830985915493 -1.0 -0.9970830985915493
maxi score, test score, baseline:  -0.9970830985915493 -1.0 -0.9970830985915493
probs:  [0.09590711525642091, 0.08926260299677943, 0.08083391614890084, 0.5639888862924505, 0.08289688146131874, 0.08711059784412957]
printing an ep nov before normalisation:  37.164175190412784
maxi score, test score, baseline:  -0.9970830985915493 -1.0 -0.9970830985915493
probs:  [0.09590711525642091, 0.08926260299677943, 0.08083391614890084, 0.5639888862924505, 0.08289688146131874, 0.08711059784412957]
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]] [[32.599]
 [32.599]
 [32.599]
 [32.599]
 [32.599]
 [32.599]
 [32.599]] [[0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]]
actions average: 
K:  2  action  0 :  tensor([0.5135, 0.0124, 0.0929, 0.1099, 0.0979, 0.0860, 0.0875],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0222, 0.8633, 0.0198, 0.0248, 0.0194, 0.0103, 0.0403],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2302, 0.0141, 0.2442, 0.1608, 0.1219, 0.1054, 0.1234],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1346, 0.0179, 0.1221, 0.3343, 0.1481, 0.1133, 0.1296],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1811, 0.0090, 0.1286, 0.2631, 0.1534, 0.1343, 0.1305],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1019, 0.0331, 0.1382, 0.1442, 0.1262, 0.3488, 0.1076],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1185, 0.1719, 0.0903, 0.1629, 0.1296, 0.0898, 0.2369],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  15.915699330687211
maxi score, test score, baseline:  -0.9970830985915493 -1.0 -0.9970830985915493
probs:  [0.0961675226754236, 0.08731488796190362, 0.08099816423402736, 0.5651302469522237, 0.0830742902145182, 0.08731488796190362]
printing an ep nov before normalisation:  37.16167317474792
printing an ep nov before normalisation:  53.28446549431597
printing an ep nov before normalisation:  13.949071473928171
line 256 mcts: sample exp_bonus 62.72953077335042
maxi score, test score, baseline:  -0.9970830985915493 -1.0 -0.9970830985915493
probs:  [0.0961675226754236, 0.08731488796190362, 0.08099816423402736, 0.5651302469522237, 0.0830742902145182, 0.08731488796190362]
printing an ep nov before normalisation:  58.6763224686055
siam score:  -0.7995783
maxi score, test score, baseline:  -0.9970910112359551 -1.0 -0.9970910112359551
probs:  [0.09622139623296014, 0.08733150442731305, 0.08098819621182525, 0.5650543389494072, 0.08307305975118134, 0.08733150442731305]
printing an ep nov before normalisation:  40.622626446396524
printing an ep nov before normalisation:  83.89840902244669
from probs:  [0.09627543330971018, 0.0873481713276995, 0.08097819793428562, 0.5649782005475052, 0.08307182555309993, 0.0873481713276995]
maxi score, test score, baseline:  -0.9970988795518207 -1.0 -0.9970988795518207
probs:  [0.09632961517912439, 0.08736488290603889, 0.08096817289868102, 0.5649018580341922, 0.08307058807592449, 0.08736488290603889]
printing an ep nov before normalisation:  33.3406662940979
siam score:  -0.80248296
printing an ep nov before normalisation:  54.13001537322998
maxi score, test score, baseline:  -0.9971067039106145 -1.0 -0.9971067039106145
probs:  [0.09653246832197655, 0.08754882006693648, 0.08113861271828804, 0.5660926661075744, 0.08113861271828804, 0.08754882006693648]
line 256 mcts: sample exp_bonus 65.93506107820257
printing an ep nov before normalisation:  41.53216447975828
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.478]
 [0.457]
 [0.478]
 [0.463]
 [0.463]
 [0.478]] [[45.238]
 [55.266]
 [51.628]
 [45.238]
 [50.568]
 [52.313]
 [45.238]] [[0.478]
 [0.478]
 [0.457]
 [0.478]
 [0.463]
 [0.463]
 [0.478]]
Printing some Q and Qe and total Qs values:  [[-0.057]
 [-0.043]
 [-0.056]
 [-0.05 ]
 [-0.061]
 [-0.06 ]
 [-0.055]] [[28.723]
 [26.675]
 [27.907]
 [26.898]
 [27.845]
 [31.153]
 [31.24 ]] [[0.928]
 [0.872]
 [0.901]
 [0.873]
 [0.894]
 [1.009]
 [1.016]]
Printing some Q and Qe and total Qs values:  [[0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.465]] [[39.804]
 [39.804]
 [39.804]
 [39.804]
 [39.804]
 [39.804]
 [15.211]] [[6.102]
 [6.102]
 [6.102]
 [6.102]
 [6.102]
 [6.102]
 [1.798]]
printing an ep nov before normalisation:  45.10040166459077
printing an ep nov before normalisation:  34.69842613460855
maxi score, test score, baseline:  -0.9971144846796658 -1.0 -0.9971144846796658
probs:  [0.09702532105469576, 0.08783142040317256, 0.0812711892091169, 0.5669870026595611, 0.0812711892091169, 0.08561387746433681]
printing an ep nov before normalisation:  69.36304868034958
printing an ep nov before normalisation:  50.20072910112327
maxi score, test score, baseline:  -0.9971144846796658 -1.0 -0.9971144846796658
actor:  1 policy actor:  1  step number:  69 total reward:  0.19999999999999973  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9971144846796658 -1.0 -0.9971144846796658
probs:  [0.09385677986954861, 0.08520951246003486, 0.08191874680696988, 0.5718867015964417, 0.08191874680696988, 0.08520951246003486]
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.434]
 [0.385]
 [0.566]
 [0.408]
 [0.376]
 [0.394]] [[28.749]
 [39.631]
 [31.195]
 [29.701]
 [31.596]
 [32.477]
 [33.156]] [[0.414]
 [0.434]
 [0.385]
 [0.566]
 [0.408]
 [0.376]
 [0.394]]
printing an ep nov before normalisation:  36.94077271537883
siam score:  -0.790039
maxi score, test score, baseline:  -0.9971144846796658 -1.0 -0.9971144846796658
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.323]
 [0.323]
 [0.323]
 [0.323]
 [0.323]
 [0.323]] [[29.328]
 [26.588]
 [26.588]
 [26.588]
 [26.588]
 [26.588]
 [26.588]] [[0.352]
 [0.323]
 [0.323]
 [0.323]
 [0.323]
 [0.323]
 [0.323]]
maxi score, test score, baseline:  -0.9971144846796658 -1.0 -0.9971144846796658
probs:  [0.09385677986954861, 0.08520951246003486, 0.08191874680696988, 0.5718867015964417, 0.08191874680696988, 0.08520951246003486]
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.507]
 [0.496]
 [0.496]
 [0.496]
 [0.5  ]
 [0.496]] [[23.312]
 [32.916]
 [23.312]
 [23.312]
 [23.635]
 [24.369]
 [23.312]] [[0.496]
 [0.507]
 [0.496]
 [0.496]
 [0.496]
 [0.5  ]
 [0.496]]
printing an ep nov before normalisation:  24.247001176518136
printing an ep nov before normalisation:  26.521252470357396
actions average: 
K:  1  action  0 :  tensor([0.4767, 0.0124, 0.0983, 0.1298, 0.1203, 0.0858, 0.0767],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0125, 0.8114, 0.0299, 0.0400, 0.0103, 0.0267, 0.0692],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1078, 0.0254, 0.2174, 0.1718, 0.1370, 0.2140, 0.1265],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1082, 0.0382, 0.1174, 0.3088, 0.1412, 0.1305, 0.1557],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1126, 0.0046, 0.0571, 0.0974, 0.5777, 0.0858, 0.0648],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1338, 0.0221, 0.1477, 0.1545, 0.1393, 0.2667, 0.1360],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1201, 0.0626, 0.1273, 0.1674, 0.1574, 0.1113, 0.2539],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.503]
 [0.503]
 [0.486]
 [0.483]
 [0.491]
 [0.49 ]] [[16.377]
 [23.648]
 [18.709]
 [19.277]
 [19.658]
 [20.768]
 [18.71 ]] [[0.501]
 [0.503]
 [0.503]
 [0.486]
 [0.483]
 [0.491]
 [0.49 ]]
UNIT TEST: sample policy line 217 mcts : [0.143 0.265 0.082 0.204 0.184 0.061 0.061]
maxi score, test score, baseline:  -0.9971222222222222 -1.0 -0.9971222222222222
probs:  [0.09405282807558893, 0.0853583468484617, 0.08204961371480494, 0.5727971863616769, 0.08204961371480494, 0.08369241128466251]
maxi score, test score, baseline:  -0.9971222222222222 -1.0 -0.9971222222222222
probs:  [0.09405282807558893, 0.0853583468484617, 0.08204961371480494, 0.5727971863616769, 0.08204961371480494, 0.08369241128466251]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.997129916897507 -1.0 -0.997129916897507
probs:  [0.09405284711722933, 0.08535835038036736, 0.08204961134439488, 0.5727971679688407, 0.08204961134439488, 0.08369241184477282]
printing an ep nov before normalisation:  54.79160455586686
siam score:  -0.78439766
maxi score, test score, baseline:  -0.997129916897507 -1.0 -0.997129916897507
probs:  [0.09405284711722933, 0.08535835038036736, 0.08204961134439488, 0.5727971679688407, 0.08204961134439488, 0.08369241184477282]
printing an ep nov before normalisation:  26.561296607127247
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.542]
 [0.519]
 [0.523]
 [0.524]
 [0.52 ]
 [0.517]] [[43.04 ]
 [44.969]
 [41.805]
 [42.143]
 [42.868]
 [43.046]
 [42.581]] [[1.417]
 [1.534]
 [1.38 ]
 [1.398]
 [1.429]
 [1.433]
 [1.411]]
maxi score, test score, baseline:  -0.997129916897507 -1.0 -0.997129916897507
probs:  [0.0940932995734155, 0.08536588992522168, 0.08204462569799238, 0.5727579141057544, 0.08204462569799238, 0.08369364499962373]
maxi score, test score, baseline:  -0.997129916897507 -1.0 -0.997129916897507
probs:  [0.0940932995734155, 0.08536588992522168, 0.08204462569799238, 0.5727579141057544, 0.08204462569799238, 0.08369364499962373]
printing an ep nov before normalisation:  50.103741009325915
maxi score, test score, baseline:  -0.997129916897507 -1.0 -0.997129916897507
probs:  [0.0940932995734155, 0.08536588992522168, 0.08204462569799238, 0.5727579141057544, 0.08204462569799238, 0.08369364499962373]
from probs:  [0.0940932995734155, 0.08536588992522168, 0.08204462569799238, 0.5727579141057544, 0.08204462569799238, 0.08369364499962373]
actions average: 
K:  4  action  0 :  tensor([0.4421, 0.0568, 0.0955, 0.0959, 0.1110, 0.0783, 0.1205],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0476, 0.7583, 0.0472, 0.0343, 0.0365, 0.0301, 0.0460],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1907, 0.0385, 0.2326, 0.1539, 0.1257, 0.1073, 0.1513],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0929, 0.0776, 0.0824, 0.4131, 0.1186, 0.0855, 0.1299],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2011, 0.0182, 0.1177, 0.1445, 0.2370, 0.1362, 0.1452],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1493, 0.0101, 0.1237, 0.1855, 0.1802, 0.1785, 0.1729],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1458, 0.0287, 0.1296, 0.1756, 0.1751, 0.1500, 0.1951],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.571]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.552]] [[56.457]
 [55.046]
 [56.457]
 [56.457]
 [56.457]
 [56.457]
 [58.282]] [[1.704]
 [1.701]
 [1.704]
 [1.704]
 [1.704]
 [1.704]
 [1.8  ]]
maxi score, test score, baseline:  -0.997129916897507 -1.0 -0.997129916897507
probs:  [0.09413384746848964, 0.0853734472580139, 0.08203962828902726, 0.5727185676316031, 0.08203962828902726, 0.0836948810638388]
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.072907822146405
printing an ep nov before normalisation:  65.5447485658343
maxi score, test score, baseline:  -0.997129916897507 -1.0 -0.997129916897507
probs:  [0.09421523092950254, 0.08538861553980827, 0.08202959801650798, 0.5726395955360303, 0.08202959801650798, 0.0836973619616431]
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.606]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]] [[55.435]
 [67.666]
 [55.435]
 [55.435]
 [55.435]
 [55.435]
 [55.435]] [[1.48 ]
 [1.777]
 [1.48 ]
 [1.48 ]
 [1.48 ]
 [1.48 ]
 [1.48 ]]
Printing some Q and Qe and total Qs values:  [[0.495]
 [0.518]
 [0.498]
 [0.497]
 [0.497]
 [0.509]
 [0.509]] [[20.685]
 [54.697]
 [21.247]
 [22.047]
 [23.456]
 [48.882]
 [46.808]] [[0.53 ]
 [0.693]
 [0.535]
 [0.538]
 [0.543]
 [0.66 ]
 [0.652]]
maxi score, test score, baseline:  -0.9971375690607734 -1.0 -0.9971375690607734
probs:  [0.09519461106360858, 0.08645422364825187, 0.08149932816175308, 0.5689443243931954, 0.08312802065962999, 0.08477949207356111]
maxi score, test score, baseline:  -0.9971375690607734 -1.0 -0.9971375690607734
maxi score, test score, baseline:  -0.9971375690607734 -1.0 -0.9971375690607734
probs:  [0.09519461106360858, 0.08645422364825187, 0.08149932816175308, 0.5689443243931954, 0.08312802065962999, 0.08477949207356111]
printing an ep nov before normalisation:  42.35164364860793
siam score:  -0.78193694
maxi score, test score, baseline:  -0.9971375690607734 -1.0 -0.9971375690607734
probs:  [0.09519461106360858, 0.08645422364825187, 0.08149932816175308, 0.5689443243931954, 0.08312802065962999, 0.08477949207356111]
line 256 mcts: sample exp_bonus 11.773922196574517
printing an ep nov before normalisation:  24.877222280977662
maxi score, test score, baseline:  -0.9971375690607734 -1.0 -0.9971375690607734
probs:  [0.09360213495312493, 0.08662200807740825, 0.08163964165232768, 0.569920859914132, 0.08327736394946064, 0.08493799145354648]
maxi score, test score, baseline:  -0.9971375690607734 -1.0 -0.9971375690607734
probs:  [0.09379874949146962, 0.08508756340599041, 0.08177130497896662, 0.5708368993113092, 0.08341791940627358, 0.08508756340599041]
maxi score, test score, baseline:  -0.9971375690607734 -1.0 -0.9971375690607734
probs:  [0.09379874949146962, 0.08508756340599041, 0.08177130497896662, 0.5708368993113092, 0.08341791940627358, 0.08508756340599041]
maxi score, test score, baseline:  -0.9971375690607734 -1.0 -0.9971375690607734
probs:  [0.09383765924289657, 0.08509398207602249, 0.08176535463042633, 0.5707909113555939, 0.08341811061903832, 0.08509398207602249]
printing an ep nov before normalisation:  34.54122938843185
maxi score, test score, baseline:  -0.9971375690607734 -1.0 -0.9971375690607734
probs:  [0.09383765924289657, 0.08509398207602249, 0.08176535463042633, 0.5707909113555939, 0.08341811061903832, 0.08509398207602249]
siam score:  -0.7777214
Printing some Q and Qe and total Qs values:  [[0.603]
 [0.592]
 [0.603]
 [0.619]
 [0.64 ]
 [0.615]
 [0.623]] [[38.452]
 [38.487]
 [38.808]
 [39.003]
 [42.832]
 [39.862]
 [40.148]] [[1.306]
 [1.296]
 [1.318]
 [1.341]
 [1.498]
 [1.367]
 [1.386]]
printing an ep nov before normalisation:  33.47781418072168
printing an ep nov before normalisation:  40.91376304626465
maxi score, test score, baseline:  -0.9971375690607734 -1.0 -0.9971375690607734
probs:  [0.0949695596998016, 0.08629615598266972, 0.08137726675131005, 0.5680895103591131, 0.0846337536035528, 0.0846337536035528]
siam score:  -0.7766713
maxi score, test score, baseline:  -0.9971375690607734 -1.0 -0.9971375690607734
probs:  [0.09431135452923868, 0.08747575043216153, 0.08101215200023147, 0.565548370540343, 0.08582618624901273, 0.08582618624901273]
printing an ep nov before normalisation:  60.539865493774414
printing an ep nov before normalisation:  56.1202305748008
maxi score, test score, baseline:  -0.9971375690607734 -1.0 -0.9971375690607734
probs:  [0.09431135452923868, 0.08747575043216153, 0.08101215200023147, 0.565548370540343, 0.08582618624901273, 0.08582618624901273]
maxi score, test score, baseline:  -0.9971375690607734 -1.0 -0.9971375690607734
probs:  [0.09431135452923868, 0.08747575043216153, 0.08101215200023147, 0.565548370540343, 0.08582618624901273, 0.08582618624901273]
from probs:  [0.09431135452923868, 0.08747575043216153, 0.08101215200023147, 0.565548370540343, 0.08582618624901273, 0.08582618624901273]
maxi score, test score, baseline:  -0.9971375690607734 -1.0 -0.9971375690607734
probs:  [0.09450784963512257, 0.0859774410581811, 0.08113773986554901, 0.566422087324785, 0.0859774410581811, 0.0859774410581811]
maxi score, test score, baseline:  -0.9971375690607734 -1.0 -0.9971375690607734
probs:  [0.09450784963512257, 0.0859774410581811, 0.08113773986554901, 0.566422087324785, 0.0859774410581811, 0.0859774410581811]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
using explorer policy with actor:  1
printing an ep nov before normalisation:  52.98778612975788
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.408]
 [0.26 ]
 [0.219]
 [0.302]
 [0.29 ]
 [0.309]] [[29.631]
 [28.977]
 [32.425]
 [33.808]
 [30.794]
 [34.815]
 [31.143]] [[1.273]
 [1.217]
 [1.25 ]
 [1.283]
 [1.207]
 [1.407]
 [1.232]]
printing an ep nov before normalisation:  80.08291000699066
maxi score, test score, baseline:  -0.9971451790633609 -1.0 -0.9971451790633609
probs:  [0.09450786905047558, 0.08597744559185587, 0.0811377359561492, 0.5664220582178077, 0.08597744559185587, 0.08597744559185587]
actor:  1 policy actor:  1  step number:  69 total reward:  0.07999999999999952  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9971451790633609 -1.0 -0.9971451790633609
probs:  [0.09265705383695008, 0.08554373223897298, 0.08150801100583498, 0.569203738440296, 0.08554373223897298, 0.08554373223897298]
maxi score, test score, baseline:  -0.9971451790633609 -1.0 -0.9971451790633609
probs:  [0.09265705383695008, 0.08554373223897298, 0.08150801100583498, 0.569203738440296, 0.08554373223897298, 0.08554373223897298]
printing an ep nov before normalisation:  65.10556538899739
line 256 mcts: sample exp_bonus 98.50440393563423
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.506863594055176
maxi score, test score, baseline:  -0.9971527472527473 -1.0 -0.9971527472527473
printing an ep nov before normalisation:  24.79836294937943
maxi score, test score, baseline:  -0.9971527472527473 -1.0 -0.9971527472527473
probs:  [0.09272164765644643, 0.08555886890795038, 0.08149508831186489, 0.5691066573078375, 0.08555886890795038, 0.08555886890795038]
maxi score, test score, baseline:  -0.9971527472527473 -1.0 -0.9971527472527473
probs:  [0.09272164765644643, 0.08555886890795038, 0.08149508831186489, 0.5691066573078375, 0.08555886890795038, 0.08555886890795038]
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.403]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]] [[41.694]
 [47.367]
 [41.694]
 [41.694]
 [41.694]
 [41.694]
 [41.694]] [[1.317]
 [1.551]
 [1.317]
 [1.317]
 [1.317]
 [1.317]
 [1.317]]
siam score:  -0.77022475
maxi score, test score, baseline:  -0.9971602739726028 -1.0 -0.9971602739726028
probs:  [0.0927216628670764, 0.08555887245902662, 0.08149508524792899, 0.5691066345079147, 0.08555887245902662, 0.08555887245902662]
maxi score, test score, baseline:  -0.9971602739726028 -1.0 -0.9971602739726028
probs:  [0.0927216628670764, 0.08555887245902662, 0.08149508524792899, 0.5691066345079147, 0.08555887245902662, 0.08555887245902662]
using explorer policy with actor:  1
printing an ep nov before normalisation:  63.912193183967965
maxi score, test score, baseline:  -0.9971602739726028 -1.0 -0.9971602739726028
probs:  [0.09275404207610675, 0.08556646008584017, 0.08148860744646441, 0.5690579702199083, 0.08556646008584017, 0.08556646008584017]
maxi score, test score, baseline:  -0.9971602739726028 -1.0 -0.9971602739726028
probs:  [0.09275404207610675, 0.08556646008584017, 0.08148860744646441, 0.5690579702199083, 0.08556646008584017, 0.08556646008584017]
maxi score, test score, baseline:  -0.9971602739726028 -1.0 -0.9971602739726028
probs:  [0.09288195512953577, 0.08568444680083297, 0.08160096248373218, 0.5698436780638051, 0.08430451072126098, 0.08568444680083297]
maxi score, test score, baseline:  -0.9971602739726028 -1.0 -0.9971602739726028
probs:  [0.09288195512953577, 0.08568444680083297, 0.08160096248373218, 0.5698436780638051, 0.08430451072126098, 0.08568444680083297]
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.415]
 [0.386]
 [0.386]
 [0.411]
 [0.393]
 [0.387]] [[30.545]
 [29.746]
 [30.545]
 [30.545]
 [32.065]
 [31.855]
 [30.168]] [[0.953]
 [0.952]
 [0.953]
 [0.953]
 [1.037]
 [1.011]
 [0.94 ]]
printing an ep nov before normalisation:  39.279446601867676
siam score:  -0.76565856
maxi score, test score, baseline:  -0.9971602739726028 -1.0 -0.9971602739726028
probs:  [0.09301022197766334, 0.08442091539317885, 0.08171362828378352, 0.5706315590969293, 0.08442091539317885, 0.08580275985526606]
maxi score, test score, baseline:  -0.9971602739726028 -1.0 -0.9971602739726028
maxi score, test score, baseline:  -0.9971602739726028 -1.0 -0.9971602739726028
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.048]
 [0.036]
 [0.059]
 [0.043]
 [0.047]
 [0.035]] [[14.622]
 [10.42 ]
 [14.902]
 [ 7.79 ]
 [11.939]
 [16.047]
 [15.603]] [[0.421]
 [0.337]
 [0.449]
 [0.275]
 [0.374]
 [0.491]
 [0.467]]
maxi score, test score, baseline:  -0.9971602739726028 -1.0 -0.9971602739726028
probs:  [0.09172100221828439, 0.08455436722777093, 0.08182422056471819, 0.5713981631744183, 0.08455436722777093, 0.08594787958703744]
siam score:  -0.7581083
maxi score, test score, baseline:  -0.9971602739726028 -1.0 -0.9971602739726028
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9971602739726028 -1.0 -0.9971602739726028
probs:  [0.09052939884682638, 0.08480067175874326, 0.08205322101241765, 0.5729961546638168, 0.08341753730768206, 0.0862030164105136]
maxi score, test score, baseline:  -0.9971602739726028 -1.0 -0.9971602739726028
probs:  [0.09052939884682638, 0.08480067175874326, 0.08205322101241765, 0.5729961546638168, 0.08341753730768206, 0.0862030164105136]
maxi score, test score, baseline:  -0.9971602739726028 -1.0 -0.9971602739726028
probs:  [0.09052939884682638, 0.08480067175874326, 0.08205322101241765, 0.5729961546638168, 0.08341753730768206, 0.0862030164105136]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.105]
 [0.082]
 [0.046]
 [0.089]
 [0.139]
 [0.094]
 [0.089]] [[11.649]
 [ 8.944]
 [12.076]
 [ 9.889]
 [ 6.331]
 [10.054]
 [ 9.608]] [[0.411]
 [0.317]
 [0.364]
 [0.349]
 [0.306]
 [0.359]
 [0.342]]
maxi score, test score, baseline:  -0.9971602739726028 -1.0 -0.9971602739726028
probs:  [0.09055421062398128, 0.08480563920389882, 0.08204867127794088, 0.5729609237414653, 0.08341771356994052, 0.08621284158277316]
maxi score, test score, baseline:  -0.9971602739726028 -1.0 -0.9971602739726028
printing an ep nov before normalisation:  35.17049771481333
maxi score, test score, baseline:  -0.9971602739726028 -1.0 -0.9971602739726028
probs:  [0.09057906985402092, 0.0848106161493655, 0.08204411284203074, 0.5729256254394622, 0.08341789016930314, 0.08622268554581758]
maxi score, test score, baseline:  -0.9971602739726028 -1.0 -0.9971602739726028
probs:  [0.0907070687216257, 0.08493045150699896, 0.08216003304692288, 0.5737362406749449, 0.0835357545425086, 0.08493045150699896]
printing an ep nov before normalisation:  39.57966872296655
maxi score, test score, baseline:  -0.9971602739726028 -1.0 -0.9971602739726028
probs:  [0.0908336695535849, 0.08365233157155105, 0.08227468714234458, 0.5745380021680839, 0.08365233157155105, 0.08504897799288456]
maxi score, test score, baseline:  -0.9971602739726028 -1.0 -0.9971602739726028
probs:  [0.09096062479703196, 0.08376923495104219, 0.0823896622050768, 0.5753420081447645, 0.08376923495104219, 0.08376923495104219]
maxi score, test score, baseline:  -0.9971602739726028 -1.0 -0.9971602739726028
probs:  [0.09096062479703196, 0.08376923495104219, 0.0823896622050768, 0.5753420081447645, 0.08376923495104219, 0.08376923495104219]
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.479]
 [0.479]
 [0.543]
 [0.567]
 [0.526]
 [0.479]] [[56.181]
 [56.181]
 [56.181]
 [52.564]
 [51.172]
 [56.884]
 [56.181]] [[1.421]
 [1.421]
 [1.421]
 [1.392]
 [1.379]
 [1.486]
 [1.421]]
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([0.3577, 0.0108, 0.1051, 0.1399, 0.1635, 0.1147, 0.1083],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0268, 0.7626, 0.0336, 0.0497, 0.0216, 0.0304, 0.0751],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1029, 0.0426, 0.2332, 0.1472, 0.1531, 0.2048, 0.1161],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1016, 0.0521, 0.1102, 0.2797, 0.1508, 0.1612, 0.1444],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1206, 0.0143, 0.1103, 0.1621, 0.2938, 0.1493, 0.1497],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0929, 0.0650, 0.1640, 0.1615, 0.1302, 0.2649, 0.1216],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1071, 0.0401, 0.0969, 0.2100, 0.1583, 0.1242, 0.2634],
       grad_fn=<DivBackward0>)
using another actor
siam score:  -0.7391418
printing an ep nov before normalisation:  96.99463567003974
Printing some Q and Qe and total Qs values:  [[0.656]
 [0.677]
 [0.642]
 [0.656]
 [0.656]
 [0.626]
 [0.663]] [[38.19 ]
 [45.874]
 [41.567]
 [38.19 ]
 [38.19 ]
 [41.906]
 [43.228]] [[1.816]
 [2.293]
 [2.003]
 [1.816]
 [1.816]
 [2.007]
 [2.122]]
using another actor
maxi score, test score, baseline:  -0.9971677595628415 -1.0 -0.9971677595628415
probs:  [0.09068850195681417, 0.08353611462317038, 0.08216362948617388, 0.5737569520809873, 0.08492740092642712, 0.08492740092642712]
actions average: 
K:  2  action  0 :  tensor([0.3236, 0.0608, 0.0779, 0.1527, 0.1638, 0.0939, 0.1272],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0145, 0.8475, 0.0235, 0.0433, 0.0156, 0.0179, 0.0377],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1370, 0.0490, 0.2586, 0.1452, 0.1204, 0.1617, 0.1281],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1005, 0.0329, 0.1175, 0.2845, 0.1529, 0.1509, 0.1607],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1747, 0.1135, 0.0764, 0.1385, 0.2963, 0.0831, 0.1175],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0970, 0.0105, 0.1616, 0.1347, 0.1444, 0.3339, 0.1179],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1203, 0.2291, 0.1045, 0.1452, 0.1282, 0.1166, 0.1561],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9971677595628415 -1.0 -0.9971677595628415
printing an ep nov before normalisation:  66.03856095763629
actor:  1 policy actor:  1  step number:  65 total reward:  0.11999999999999933  reward:  1.0 rdn_beta:  2.0
from probs:  [0.09071360558702413, 0.08353669563674264, 0.08215950480844539, 0.5737246897821382, 0.08493275209282479, 0.08493275209282479]
siam score:  -0.74526954
printing an ep nov before normalisation:  49.823324754464906
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.997175204359673 -1.0 -0.997175204359673
probs:  [0.07226932534146874, 0.06651781947251802, 0.06541415213009773, 0.45662271430736584, 0.06763660554565636, 0.2715393832028933]
maxi score, test score, baseline:  -0.9971826086956522 -1.0 -0.9971826086956522
printing an ep nov before normalisation:  48.996787197746734
printing an ep nov before normalisation:  50.63654148176283
printing an ep nov before normalisation:  36.42690896987915
printing an ep nov before normalisation:  28.332884874137978
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9971826086956522 -1.0 -0.9971826086956522
probs:  [0.07226933453134521, 0.06651781931355444, 0.06541415017716756, 0.4566226995149471, 0.06763660720523426, 0.27153938925775145]
printing an ep nov before normalisation:  47.66946315765381
actor:  1 policy actor:  1  step number:  74 total reward:  0.03333333333333277  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  56.38616384168668
printing an ep nov before normalisation:  34.62648391723633
actions average: 
K:  1  action  0 :  tensor([0.4491, 0.0297, 0.0880, 0.1065, 0.1267, 0.0948, 0.1052],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0148, 0.8681, 0.0131, 0.0465, 0.0140, 0.0147, 0.0287],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0818, 0.0069, 0.4037, 0.0902, 0.0900, 0.2212, 0.1062],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1481, 0.0243, 0.1029, 0.2875, 0.1645, 0.1264, 0.1463],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1728, 0.0143, 0.0868, 0.1394, 0.3386, 0.1301, 0.1179],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1289, 0.0022, 0.1208, 0.1357, 0.1454, 0.3390, 0.1279],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1878, 0.1009, 0.1087, 0.1308, 0.1194, 0.1284, 0.2241],
       grad_fn=<DivBackward0>)
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
Printing some Q and Qe and total Qs values:  [[-0.035]
 [-0.021]
 [-0.035]
 [-0.035]
 [-0.032]
 [-0.033]
 [-0.036]] [[30.988]
 [29.179]
 [32.453]
 [33.338]
 [31.649]
 [31.677]
 [32.147]] [[0.431]
 [0.383]
 [0.482]
 [0.512]
 [0.457]
 [0.457]
 [0.47 ]]
printing an ep nov before normalisation:  42.778345928878174
maxi score, test score, baseline:  -0.9971826086956522 -1.0 -0.9971826086956522
maxi score, test score, baseline:  -0.9971826086956522 -1.0 -0.9971826086956522
probs:  [0.07342952627441103, 0.06827931490141899, 0.06729103109741241, 0.4698525026492479, 0.06928113683972703, 0.2518664882377827]
from probs:  [0.07342952627441103, 0.06827931490141899, 0.06729103109741241, 0.4698525026492479, 0.06928113683972703, 0.2518664882377827]
printing an ep nov before normalisation:  30.88209556945647
maxi score, test score, baseline:  -0.997189972899729 -1.0 -0.997189972899729
probs:  [0.0726525065538086, 0.06753736045432213, 0.06753736045432213, 0.4715730439320324, 0.0695413765974086, 0.251158352008106]
maxi score, test score, baseline:  -0.997189972899729 -1.0 -0.997189972899729
probs:  [0.0726525065538086, 0.06753736045432213, 0.06753736045432213, 0.4715730439320324, 0.0695413765974086, 0.251158352008106]
printing an ep nov before normalisation:  37.1937700189538
printing an ep nov before normalisation:  39.332986654975855
maxi score, test score, baseline:  -0.997189972899729 -1.0 -0.997189972899729
probs:  [0.0726525065538086, 0.06753736045432213, 0.06753736045432213, 0.4715730439320324, 0.0695413765974086, 0.251158352008106]
printing an ep nov before normalisation:  42.60361050970346
maxi score, test score, baseline:  -0.997189972899729 -1.0 -0.997189972899729
maxi score, test score, baseline:  -0.997189972899729 -1.0 -0.997189972899729
probs:  [0.07167634380881058, 0.06760836372150926, 0.06760836372150926, 0.4720696569060925, 0.06961449088784964, 0.2514227809542289]
actor:  1 policy actor:  1  step number:  69 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.997189972899729 -1.0 -0.997189972899729
probs:  [0.05909342937677527, 0.05573047856921916, 0.05573047856921916, 0.3889910892971766, 0.0573889200633564, 0.38306560412425344]
printing an ep nov before normalisation:  29.592939531184122
maxi score, test score, baseline:  -0.997189972899729 -1.0 -0.997189972899729
probs:  [0.05909342937677527, 0.05573047856921916, 0.05573047856921916, 0.3889910892971766, 0.0573889200633564, 0.38306560412425344]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
printing an ep nov before normalisation:  22.167179880343966
Printing some Q and Qe and total Qs values:  [[0.826]
 [0.46 ]
 [0.46 ]
 [0.435]
 [0.46 ]
 [0.46 ]
 [0.46 ]] [[25.612]
 [11.207]
 [11.207]
 [12.634]
 [11.207]
 [11.207]
 [11.207]] [[1.024]
 [0.51 ]
 [0.51 ]
 [0.5  ]
 [0.51 ]
 [0.51 ]
 [0.51 ]]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  19.390546886029192
Printing some Q and Qe and total Qs values:  [[0.677]
 [0.354]
 [0.354]
 [0.33 ]
 [0.354]
 [0.354]
 [0.354]] [[24.266]
 [13.104]
 [13.104]
 [12.443]
 [13.104]
 [13.104]
 [13.104]] [[0.954]
 [0.457]
 [0.457]
 [0.423]
 [0.457]
 [0.457]
 [0.457]]
siam score:  -0.7390055
printing an ep nov before normalisation:  23.081561460313505
maxi score, test score, baseline:  -0.9972045822102426 -1.0 -0.9972045822102426
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.551]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.566]
 [0.551]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]]
maxi score, test score, baseline:  -0.9972045822102426 -1.0 -0.9972045822102426
printing an ep nov before normalisation:  65.35534582782464
printing an ep nov before normalisation:  82.63118243578663
printing an ep nov before normalisation:  48.70215892791748
printing an ep nov before normalisation:  82.12053910760915
Printing some Q and Qe and total Qs values:  [[0.615]
 [0.489]
 [0.615]
 [0.626]
 [0.555]
 [0.615]
 [0.55 ]] [[67.93 ]
 [70.328]
 [67.93 ]
 [72.696]
 [72.515]
 [67.93 ]
 [74.92 ]] [[1.623]
 [1.563]
 [1.623]
 [1.764]
 [1.688]
 [1.623]
 [1.749]]
maxi score, test score, baseline:  -0.9972118279569893 -1.0 -0.9972118279569893
probs:  [0.060002337703850474, 0.05580395911428271, 0.05662094089387429, 0.38950649280295674, 0.05828847904673926, 0.37977779043829646]
printing an ep nov before normalisation:  28.693368816140243
maxi score, test score, baseline:  -0.9972118279569893 -1.0 -0.9972118279569893
Printing some Q and Qe and total Qs values:  [[ 0.218]
 [ 0.001]
 [-0.006]
 [ 0.001]
 [-0.003]
 [-0.001]
 [-0.003]] [[33.047]
 [32.697]
 [29.56 ]
 [31.331]
 [34.232]
 [32.984]
 [34.148]] [[0.435]
 [0.214]
 [0.17 ]
 [0.197]
 [0.227]
 [0.215]
 [0.226]]
siam score:  -0.74372745
maxi score, test score, baseline:  -0.9972118279569893 -1.0 -0.9972118279569893
probs:  [0.06022885875867537, 0.0560145951637761, 0.056834668079540296, 0.39097972735653685, 0.057665898449940724, 0.3782762521915306]
maxi score, test score, baseline:  -0.9972118279569893 -1.0 -0.9972118279569893
maxi score, test score, baseline:  -0.9972118279569893 -1.0 -0.9972118279569893
probs:  [0.06022885875867537, 0.0560145951637761, 0.056834668079540296, 0.39097972735653685, 0.057665898449940724, 0.3782762521915306]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9972118279569893 -1.0 -0.9972118279569893
probs:  [0.06022885875867537, 0.0560145951637761, 0.056834668079540296, 0.39097972735653685, 0.057665898449940724, 0.3782762521915306]
maxi score, test score, baseline:  -0.9972118279569893 -1.0 -0.9972118279569893
printing an ep nov before normalisation:  41.2018440818137
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.538]
 [0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]] [[37.064]
 [45.265]
 [37.064]
 [37.064]
 [37.064]
 [37.064]
 [37.064]] [[1.2  ]
 [1.595]
 [1.2  ]
 [1.2  ]
 [1.2  ]
 [1.2  ]
 [1.2  ]]
from probs:  [0.05936140700070874, 0.056002965038472956, 0.05682555639746653, 0.39089644474584634, 0.05765933947563008, 0.37925428734187544]
maxi score, test score, baseline:  -0.9972118279569893 -1.0 -0.9972118279569893
probs:  [0.05930560098644721, 0.055929583714402036, 0.05675647983677797, 0.3903793193221824, 0.05675647983677797, 0.38087253630341233]
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.568]
 [0.449]
 [0.501]
 [0.542]
 [0.472]
 [0.456]] [[38.485]
 [49.394]
 [38.813]
 [37.779]
 [40.845]
 [38.106]
 [37.488]] [[1.067]
 [1.509]
 [1.064]
 [1.084]
 [1.219]
 [1.066]
 [1.03 ]]
printing an ep nov before normalisation:  45.16141891479492
maxi score, test score, baseline:  -0.9972190348525469 -1.0 -0.9972190348525469
maxi score, test score, baseline:  -0.9972190348525469 -1.0 -0.9972190348525469
probs:  [0.05930561790402659, 0.055929595011808414, 0.056756492510747, 0.39037939759291107, 0.056756492510747, 0.38087240446975995]
line 256 mcts: sample exp_bonus 49.436787365149286
printing an ep nov before normalisation:  32.10340370876682
maxi score, test score, baseline:  -0.9972262032085562 -1.0 -0.9972262032085562
probs:  [0.0593546440299386, 0.05597581936622182, 0.05597581936622182, 0.3907026952646633, 0.056803403109868665, 0.3811876188630858]
maxi score, test score, baseline:  -0.9972262032085562 -1.0 -0.9972262032085562
probs:  [0.05924958729873754, 0.05585597968745866, 0.05585597968745866, 0.38986063151021094, 0.056687184254410425, 0.38249063756172375]
printing an ep nov before normalisation:  86.16772295773822
maxi score, test score, baseline:  -0.9972262032085562 -1.0 -0.9972262032085562
probs:  [0.05914461616741845, 0.055736237653674935, 0.055736237653674935, 0.38901925386655956, 0.05657106009369658, 0.38379259456497555]
maxi score, test score, baseline:  -0.9972262032085562 -1.0 -0.9972262032085562
probs:  [0.05914461616741845, 0.055736237653674935, 0.055736237653674935, 0.38901925386655956, 0.05657106009369658, 0.38379259456497555]
printing an ep nov before normalisation:  17.353007012152037
Printing some Q and Qe and total Qs values:  [[0.399]
 [0.378]
 [0.344]
 [0.418]
 [0.411]
 [0.204]
 [0.375]] [[43.001]
 [46.013]
 [45.066]
 [45.15 ]
 [50.454]
 [45.549]
 [45.746]] [[0.823]
 [0.861]
 [0.808]
 [0.884]
 [0.979]
 [0.678]
 [0.852]]
maxi score, test score, baseline:  -0.9972262032085562 -1.0 -0.9972262032085562
maxi score, test score, baseline:  -0.9972262032085562 -1.0 -0.9972262032085562
probs:  [0.05914461616741845, 0.055736237653674935, 0.055736237653674935, 0.38901925386655956, 0.05657106009369658, 0.38379259456497555]
printing an ep nov before normalisation:  45.15369764892796
Printing some Q and Qe and total Qs values:  [[0.709]
 [0.449]
 [0.469]
 [0.522]
 [0.522]
 [0.522]
 [0.522]] [[15.792]
 [24.579]
 [12.26 ]
 [11.679]
 [11.679]
 [11.679]
 [11.679]] [[0.709]
 [0.449]
 [0.469]
 [0.522]
 [0.522]
 [0.522]
 [0.522]]
maxi score, test score, baseline:  -0.9972262032085562 -1.0 -0.9972262032085562
probs:  [0.05914461616741845, 0.055736237653674935, 0.055736237653674935, 0.38901925386655956, 0.05657106009369658, 0.38379259456497555]
printing an ep nov before normalisation:  62.17182169998567
actions average: 
K:  2  action  0 :  tensor([0.4672, 0.0194, 0.0847, 0.1155, 0.1120, 0.0905, 0.1107],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0175, 0.8286, 0.0211, 0.0409, 0.0224, 0.0289, 0.0405],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1219, 0.0047, 0.3262, 0.1469, 0.1332, 0.1409, 0.1262],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1156, 0.0376, 0.0945, 0.3485, 0.1232, 0.1704, 0.1101],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2388, 0.0530, 0.0901, 0.1225, 0.2711, 0.1100, 0.1144],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1280, 0.0457, 0.1222, 0.1867, 0.1460, 0.2232, 0.1482],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1703, 0.0037, 0.1266, 0.1618, 0.1440, 0.1496, 0.2439],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.179]
 [0.226]
 [0.215]
 [0.184]
 [0.182]
 [0.185]
 [0.313]] [[30.667]
 [29.862]
 [30.958]
 [31.613]
 [31.894]
 [31.236]
 [32.304]] [[0.596]
 [0.622]
 [0.64 ]
 [0.627]
 [0.632]
 [0.618]
 [0.775]]
printing an ep nov before normalisation:  17.24385758353881
maxi score, test score, baseline:  -0.9972333333333333 -1.0 -0.9972333333333333
probs:  [0.05980994743212714, 0.05559868041447742, 0.05641832298838239, 0.38806258621498024, 0.0572490418132861, 0.38286142113674665]
actor:  1 policy actor:  1  step number:  62 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  4.0322132122128096e-05
siam score:  -0.7329903
maxi score, test score, baseline:  -0.9972333333333333 -1.0 -0.9972333333333333
probs:  [0.0625292180064438, 0.05876498241404748, 0.05949761887162798, 0.4103050638239154, 0.06024015582187846, 0.3486629610620869]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9972333333333333 -1.0 -0.9972333333333333
probs:  [0.06269145507754387, 0.058917432020517795, 0.059651973420878586, 0.4113715598624646, 0.06039644105637937, 0.34697113856221584]
maxi score, test score, baseline:  -0.9972333333333333 -1.0 -0.9972333333333333
probs:  [0.062807607210021, 0.059015605932018225, 0.05975364644921341, 0.41205670816227913, 0.060501660486911205, 0.34586477175955704]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.457]
 [0.457]
 [0.457]
 [0.457]
 [0.457]
 [0.457]] [[53.187]
 [53.187]
 [53.187]
 [53.187]
 [53.187]
 [53.187]
 [53.187]] [[1.533]
 [1.533]
 [1.533]
 [1.533]
 [1.533]
 [1.533]
 [1.533]]
siam score:  -0.72589755
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.44 ]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]] [[44.226]
 [59.523]
 [44.226]
 [44.226]
 [44.226]
 [44.226]
 [44.226]] [[1.189]
 [1.654]
 [1.189]
 [1.189]
 [1.189]
 [1.189]
 [1.189]]
maxi score, test score, baseline:  -0.9972333333333333 -1.0 -0.9972333333333333
probs:  [0.06297025979575778, 0.05915740614024233, 0.059899505173866156, 0.4130470438282098, 0.059899505173866156, 0.34502627988805773]
printing an ep nov before normalisation:  48.102020810355356
actions average: 
K:  3  action  0 :  tensor([0.4670, 0.0355, 0.0844, 0.0894, 0.1285, 0.0890, 0.1062],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0194, 0.8432, 0.0492, 0.0311, 0.0134, 0.0183, 0.0253],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1216, 0.0033, 0.2225, 0.1628, 0.1645, 0.1893, 0.1361],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0949, 0.0425, 0.0868, 0.3490, 0.1578, 0.1392, 0.1298],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1671, 0.0021, 0.0796, 0.1092, 0.4546, 0.0934, 0.0940],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.2737, 0.0269, 0.1203, 0.1222, 0.1620, 0.1443, 0.1507],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1275, 0.0754, 0.1241, 0.1419, 0.1742, 0.1397, 0.2171],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  44.11158133321658
actions average: 
K:  0  action  0 :  tensor([0.4705, 0.0092, 0.0753, 0.1044, 0.1690, 0.0794, 0.0921],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0112, 0.8655, 0.0200, 0.0328, 0.0115, 0.0242, 0.0347],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1402, 0.0342, 0.2487, 0.1646, 0.1300, 0.1549, 0.1273],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1602, 0.0264, 0.0960, 0.3513, 0.1336, 0.1265, 0.1060],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1328, 0.0068, 0.1150, 0.1772, 0.3024, 0.1409, 0.1249],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1759, 0.0090, 0.1280, 0.1508, 0.1423, 0.2626, 0.1314],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1385, 0.0194, 0.1394, 0.1902, 0.1704, 0.1859, 0.1562],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9972404255319149 -1.0 -0.9972404255319149
probs:  [0.06297027409304268, 0.05915741382196326, 0.05989951414318005, 0.4130470968162188, 0.05989951414318005, 0.3450261869824153]
maxi score, test score, baseline:  -0.9972404255319149 -1.0 -0.9972404255319149
probs:  [0.06297027409304268, 0.05915741382196326, 0.05989951414318005, 0.4130470968162188, 0.05989951414318005, 0.3450261869824153]
maxi score, test score, baseline:  -0.9972404255319149 -1.0 -0.9972404255319149
probs:  [0.06292578783648353, 0.059104611389371065, 0.0598483302951849, 0.412676051264284, 0.0598483302951849, 0.3455968889194916]
printing an ep nov before normalisation:  24.94339665306459
maxi score, test score, baseline:  -0.9972404255319149 -1.0 -0.9972404255319149
probs:  [0.06292578783648353, 0.059104611389371065, 0.0598483302951849, 0.412676051264284, 0.0598483302951849, 0.3455968889194916]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9972404255319149 -1.0 -0.9972404255319149
probs:  [0.06288131634178153, 0.05905182647818947, 0.05979716343150605, 0.41230512883624987, 0.05979716343150605, 0.34616740148076697]
printing an ep nov before normalisation:  40.90260028839111
Printing some Q and Qe and total Qs values:  [[0.374]
 [0.385]
 [0.342]
 [0.374]
 [0.336]
 [0.324]
 [0.364]] [[36.52 ]
 [44.752]
 [40.034]
 [36.52 ]
 [41.664]
 [42.842]
 [40.032]] [[0.902]
 [1.182]
 [0.984]
 [0.902]
 [1.033]
 [1.059]
 [1.007]]
maxi score, test score, baseline:  -0.9972404255319149 -1.0 -0.9972404255319149
printing an ep nov before normalisation:  42.334279105277744
printing an ep nov before normalisation:  45.80144646247117
printing an ep nov before normalisation:  19.78520024148365
maxi score, test score, baseline:  -0.9972404255319149 -1.0 -0.9972404255319149
probs:  [0.06304098557675193, 0.05920175116602156, 0.05994898470904961, 0.41335394438923745, 0.05994898470904961, 0.3445053494498898]
printing an ep nov before normalisation:  30.512032825432005
maxi score, test score, baseline:  -0.9972404255319149 -1.0 -0.9972404255319149
probs:  [0.06304098557675193, 0.05920175116602156, 0.05994898470904961, 0.41335394438923745, 0.05994898470904961, 0.3445053494498898]
UNIT TEST: sample policy line 217 mcts : [0.061 0.143 0.041 0.061 0.163 0.449 0.082]
UNIT TEST: sample policy line 217 mcts : [0.02  0.204 0.02  0.041 0.184 0.449 0.082]
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.441]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]] [[21.348]
 [29.151]
 [21.348]
 [21.348]
 [21.348]
 [21.348]
 [21.348]] [[1.242]
 [1.774]
 [1.242]
 [1.242]
 [1.242]
 [1.242]
 [1.242]]
line 256 mcts: sample exp_bonus 28.370560747265785
printing an ep nov before normalisation:  35.68791433663257
printing an ep nov before normalisation:  54.1066267949377
maxi score, test score, baseline:  -0.9972404255319149 -1.0 -0.9972404255319149
probs:  [0.06358651807442951, 0.059733586626436665, 0.05899361965960361, 0.41189764126902095, 0.059733586626436665, 0.34605504774407264]
maxi score, test score, baseline:  -0.9972404255319149 -1.0 -0.9972404255319149
probs:  [0.06358651807442951, 0.059733586626436665, 0.05899361965960361, 0.41189764126902095, 0.059733586626436665, 0.34605504774407264]
using another actor
printing an ep nov before normalisation:  0.16856349755414612
printing an ep nov before normalisation:  36.589743256571566
printing an ep nov before normalisation:  17.62334942817688
maxi score, test score, baseline:  -0.9972404255319149 -1.0 -0.9972404255319149
probs:  [0.0637045595115809, 0.05983355083949849, 0.059090112087906495, 0.4125710137913395, 0.05983355083949849, 0.344967212930176]
printing an ep nov before normalisation:  38.158864221403554
from probs:  [0.0637045595115809, 0.05983355083949849, 0.059090112087906495, 0.4125710137913395, 0.05983355083949849, 0.344967212930176]
maxi score, test score, baseline:  -0.9972404255319149 -1.0 -0.9972404255319149
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.022]
 [0.018]
 [0.018]
 [0.019]
 [0.018]
 [0.018]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.018]
 [0.022]
 [0.018]
 [0.018]
 [0.019]
 [0.018]
 [0.018]]
maxi score, test score, baseline:  -0.9972404255319149 -1.0 -0.9972404255319149
probs:  [0.06366222158537471, 0.05978287756096698, 0.05903783798012047, 0.4122036740233854, 0.05978287756096698, 0.34553051128918544]
printing an ep nov before normalisation:  54.55877403097862
maxi score, test score, baseline:  -0.9972544973544973 -1.0 -0.9972544973544973
probs:  [0.06393849491996406, 0.060031296974343826, 0.05928090796492007, 0.41390242081453366, 0.060031296974343826, 0.34281558235189447]
printing an ep nov before normalisation:  28.127424600336656
maxi score, test score, baseline:  -0.9972544973544973 -1.0 -0.9972544973544973
probs:  [0.06398644800721469, 0.05932536062532436, 0.05932536062532436, 0.41421339059546547, 0.060076313592406706, 0.3430731265542644]
printing an ep nov before normalisation:  62.64904696427559
printing an ep nov before normalisation:  30.17981767654419
maxi score, test score, baseline:  -0.9972544973544973 -1.0 -0.9972544973544973
probs:  [0.06398644800721469, 0.05932536062532436, 0.05932536062532436, 0.41421339059546547, 0.060076313592406706, 0.3430731265542644]
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]] [[52.18]
 [52.18]
 [52.18]
 [52.18]
 [52.18]
 [52.18]
 [52.18]] [[2.462]
 [2.462]
 [2.462]
 [2.462]
 [2.462]
 [2.462]
 [2.462]]
maxi score, test score, baseline:  -0.9972544973544973 -1.0 -0.9972544973544973
probs:  [0.06323373453545618, 0.059372995029097896, 0.059372995029097896, 0.4145466183476752, 0.060124552319668984, 0.34334910473900393]
printing an ep nov before normalisation:  49.29289214180284
maxi score, test score, baseline:  -0.9972544973544973 -1.0 -0.9972544973544973
probs:  [0.06323373453545618, 0.059372995029097896, 0.059372995029097896, 0.4145466183476752, 0.060124552319668984, 0.34334910473900393]
printing an ep nov before normalisation:  35.18080047551518
using explorer policy with actor:  1
printing an ep nov before normalisation:  29.631510835030355
Printing some Q and Qe and total Qs values:  [[-0.004]
 [ 0.091]
 [-0.004]
 [-0.004]
 [-0.004]
 [ 0.059]
 [-0.004]] [[25.706]
 [25.241]
 [25.706]
 [25.706]
 [25.706]
 [29.456]
 [25.706]] [[1.363]
 [1.41 ]
 [1.363]
 [1.363]
 [1.363]
 [1.816]
 [1.363]]
Printing some Q and Qe and total Qs values:  [[-0.009]
 [ 0.027]
 [ 0.001]
 [-0.015]
 [ 0.039]
 [-0.009]
 [ 0.001]] [[16.075]
 [20.544]
 [19.533]
 [19.713]
 [19.75 ]
 [19.412]
 [19.533]] [[0.886]
 [1.429]
 [1.288]
 [1.293]
 [1.351]
 [1.265]
 [1.288]]
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.484]
 [0.484]
 [0.52 ]
 [0.562]
 [0.484]
 [0.484]] [[22.758]
 [22.758]
 [22.758]
 [21.596]
 [36.752]
 [22.758]
 [22.758]] [[0.961]
 [0.961]
 [0.961]
 [0.954]
 [1.555]
 [0.961]
 [0.961]]
printing an ep nov before normalisation:  46.25061527143777
printing an ep nov before normalisation:  27.96341520008383
maxi score, test score, baseline:  -0.9972544973544973 -1.0 -0.9972544973544973
actor:  1 policy actor:  1  step number:  94 total reward:  0.05999999999999894  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 41.66051866364401
maxi score, test score, baseline:  -0.9972544973544973 -1.0 -0.9972544973544973
probs:  [0.06131936919164777, 0.05799043935650642, 0.05735093441449242, 0.3642395580482102, 0.0586384710310806, 0.40046122795806277]
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.539]
 [0.545]
 [0.555]
 [0.546]
 [0.534]
 [0.63 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.539]
 [0.539]
 [0.545]
 [0.555]
 [0.546]
 [0.534]
 [0.63 ]]
maxi score, test score, baseline:  -0.9972544973544973 -1.0 -0.9972544973544973
maxi score, test score, baseline:  -0.9972544973544973 -1.0 -0.9972544973544973
probs:  [0.06135907722308814, 0.05802798700905565, 0.05738806704688625, 0.3644758524945384, 0.05802798700905565, 0.40072102921737596]
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.263]
 [0.608]
 [0.551]
 [0.338]
 [0.53 ]
 [0.24 ]] [[51.625]
 [54.495]
 [53.201]
 [55.303]
 [60.021]
 [55.566]
 [58.512]] [[1.392]
 [1.229]
 [1.525]
 [1.548]
 [1.516]
 [1.537]
 [1.36 ]]
using another actor
from probs:  [0.06112026119374712, 0.057753023530576465, 0.05710615945317789, 0.36752631940652725, 0.057753023530576465, 0.3987412128853948]
maxi score, test score, baseline:  -0.9972544973544973 -1.0 -0.9972544973544973
probs:  [0.06117757595956627, 0.0578071738133233, 0.057159701822071356, 0.3669325554709468, 0.0578071738133233, 0.399115819120769]
line 256 mcts: sample exp_bonus 53.63898135011739
maxi score, test score, baseline:  -0.9972544973544973 -1.0 -0.9972544973544973
probs:  [0.06117757595956627, 0.0578071738133233, 0.057159701822071356, 0.3669325554709468, 0.0578071738133233, 0.399115819120769]
maxi score, test score, baseline:  -0.9972544973544973 -1.0 -0.9972544973544973
probs:  [0.06117757595956627, 0.0578071738133233, 0.057159701822071356, 0.3669325554709468, 0.0578071738133233, 0.399115819120769]
Printing some Q and Qe and total Qs values:  [[0.259]
 [0.272]
 [0.239]
 [0.255]
 [0.251]
 [0.257]
 [0.247]] [[43.743]
 [44.83 ]
 [35.86 ]
 [43.33 ]
 [44.86 ]
 [47.157]
 [44.77 ]] [[0.259]
 [0.272]
 [0.239]
 [0.255]
 [0.251]
 [0.257]
 [0.247]]
siam score:  -0.731387
maxi score, test score, baseline:  -0.9972544973544973 -1.0 -0.9972544973544973
probs:  [0.06117757595956627, 0.0578071738133233, 0.057159701822071356, 0.3669325554709468, 0.0578071738133233, 0.399115819120769]
printing an ep nov before normalisation:  0.3138090141021621
Printing some Q and Qe and total Qs values:  [[0.318]
 [0.305]
 [0.316]
 [0.315]
 [0.321]
 [0.321]
 [0.316]] [[72.799]
 [62.75 ]
 [58.226]
 [58.353]
 [62.799]
 [60.634]
 [66.737]] [[0.318]
 [0.305]
 [0.316]
 [0.315]
 [0.321]
 [0.321]
 [0.316]]
printing an ep nov before normalisation:  50.78599913544876
maxi score, test score, baseline:  -0.9972614775725593 -1.0 -0.9972614775725593
probs:  [0.06111598464690627, 0.05772431931549637, 0.057072762554462336, 0.3678587930185876, 0.05772431931549637, 0.3985038211490511]
maxi score, test score, baseline:  -0.9972614775725593 -1.0 -0.9972614775725593
printing an ep nov before normalisation:  25.480108317238763
siam score:  -0.73144156
maxi score, test score, baseline:  -0.9972614775725593 -1.0 -0.9972614775725593
probs:  [0.06111598464690627, 0.05772431931549637, 0.057072762554462336, 0.3678587930185876, 0.05772431931549637, 0.3985038211490511]
printing an ep nov before normalisation:  25.739361865006405
printing an ep nov before normalisation:  80.57095677097008
printing an ep nov before normalisation:  40.17771731391385
maxi score, test score, baseline:  -0.9972684210526316 -1.0 -0.9972684210526316
probs:  [0.06105479885060943, 0.057641837264969874, 0.056986189381412804, 0.36878095431564334, 0.057641837264969874, 0.39789438292239465]
maxi score, test score, baseline:  -0.9972821989528796 -1.0 -0.9972821989528796
from probs:  [0.061112044635737016, 0.057695865292980916, 0.057039599261346184, 0.3681885717479326, 0.057695865292980916, 0.39826805376902236]
maxi score, test score, baseline:  -0.9972821989528796 -1.0 -0.9972821989528796
maxi score, test score, baseline:  -0.9972821989528796 -1.0 -0.9972821989528796
probs:  [0.06116912826650419, 0.05774975096211151, 0.05709287058521499, 0.3675977425682019, 0.05774975096211151, 0.3986407566558559]
actions average: 
K:  0  action  0 :  tensor([0.5138, 0.0225, 0.0650, 0.0963, 0.1346, 0.0778, 0.0900],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0199, 0.9307, 0.0055, 0.0085, 0.0060, 0.0070, 0.0225],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0845, 0.0092, 0.3498, 0.1529, 0.1002, 0.2010, 0.1023],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0804, 0.0335, 0.0760, 0.3771, 0.1141, 0.1392, 0.1797],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1290, 0.0315, 0.1028, 0.1983, 0.2355, 0.1538, 0.1490],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0958, 0.0201, 0.1325, 0.1960, 0.1263, 0.3087, 0.1206],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2011, 0.0291, 0.0961, 0.1637, 0.1281, 0.1329, 0.2489],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9972821989528796 -1.0 -0.9972821989528796
probs:  [0.06122605867216225, 0.057803491990170375, 0.05714599891705086, 0.3670084993048593, 0.057803491990170375, 0.39901245912558686]
printing an ep nov before normalisation:  40.190222658836504
maxi score, test score, baseline:  -0.9972821989528796 -1.0 -0.9972821989528796
probs:  [0.06122605867216225, 0.057803491990170375, 0.05714599891705086, 0.3670084993048593, 0.057803491990170375, 0.39901245912558686]
maxi score, test score, baseline:  -0.9972821989528796 -1.0 -0.9972821989528796
maxi score, test score, baseline:  -0.9972821989528796 -1.0 -0.9972821989528796
probs:  [0.06110841672814469, 0.05766766456360078, 0.05700667796356996, 0.3685156032768533, 0.05766766456360078, 0.39803397290423065]
printing an ep nov before normalisation:  48.558238520367354
maxi score, test score, baseline:  -0.9972821989528796 -1.0 -0.9972821989528796
probs:  [0.06110841672814469, 0.05766766456360078, 0.05700667796356996, 0.3685156032768533, 0.05766766456360078, 0.39803397290423065]
maxi score, test score, baseline:  -0.9972821989528796 -1.0 -0.9972821989528796
probs:  [0.06110841672814469, 0.05766766456360078, 0.05700667796356996, 0.3685156032768533, 0.05766766456360078, 0.39803397290423065]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9972890339425587 -1.0 -0.9972890339425587
probs:  [0.06110842092156792, 0.05766766315703536, 0.05700667548121725, 0.3685156224585841, 0.05766766315703536, 0.39803395482456]
printing an ep nov before normalisation:  25.911893860913057
printing an ep nov before normalisation:  35.81222722272997
maxi score, test score, baseline:  -0.9972890339425587 -1.0 -0.9972890339425587
maxi score, test score, baseline:  -0.9972890339425587 -1.0 -0.9972890339425587
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.9972890339425587 -1.0 -0.9972890339425587
probs:  [0.06037971665951926, 0.057626865669580155, 0.05696129149931528, 0.3696925864893573, 0.057626865669580155, 0.3977126740126479]
maxi score, test score, baseline:  -0.9972890339425587 -1.0 -0.9972890339425587
UNIT TEST: sample policy line 217 mcts : [0.143 0.204 0.245 0.102 0.122 0.102 0.082]
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.95073431797827
maxi score, test score, baseline:  -0.9972890339425587 -1.0 -0.9972890339425587
probs:  [0.06025900185515217, 0.05749169439920629, 0.05682262499785095, 0.3711962135053426, 0.05749169439920629, 0.3967387708432417]
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.495]
 [0.428]
 [0.451]
 [0.478]
 [0.428]
 [0.431]] [[44.787]
 [49.42 ]
 [39.37 ]
 [42.516]
 [43.993]
 [40.208]
 [40.922]] [[0.492]
 [0.495]
 [0.428]
 [0.451]
 [0.478]
 [0.428]
 [0.431]]
printing an ep nov before normalisation:  49.13064135486796
printing an ep nov before normalisation:  30.03944791596637
siam score:  -0.7405292
maxi score, test score, baseline:  -0.9972890339425587 -1.0 -0.9972890339425587
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.384]
 [0.413]
 [0.414]
 [0.444]
 [0.405]
 [0.434]] [[13.46 ]
 [22.813]
 [12.265]
 [12.106]
 [11.691]
 [12.269]
 [13.266]] [[0.653]
 [0.384]
 [0.413]
 [0.414]
 [0.444]
 [0.405]
 [0.434]]
maxi score, test score, baseline:  -0.9972890339425587 -1.0 -0.9972890339425587
maxi score, test score, baseline:  -0.9972890339425587 -1.0 -0.9972890339425587
probs:  [0.060315608067372545, 0.0575456952929445, 0.05687599598728509, 0.3706048449414732, 0.0575456952929445, 0.39711216041798014]
printing an ep nov before normalisation:  19.426711212083525
printing an ep nov before normalisation:  17.354437965847893
maxi score, test score, baseline:  -0.9972890339425587 -1.0 -0.9972890339425587
probs:  [0.060315608067372545, 0.0575456952929445, 0.05687599598728509, 0.3706048449414732, 0.0575456952929445, 0.39711216041798014]
maxi score, test score, baseline:  -0.9972890339425587 -1.0 -0.9972890339425587
probs:  [0.060315608067372545, 0.0575456952929445, 0.05687599598728509, 0.3706048449414732, 0.0575456952929445, 0.39711216041798014]
printing an ep nov before normalisation:  18.995384316925197
maxi score, test score, baseline:  -0.9972890339425587 -1.0 -0.9972890339425587
probs:  [0.060315608067372545, 0.0575456952929445, 0.05687599598728509, 0.3706048449414732, 0.0575456952929445, 0.39711216041798014]
maxi score, test score, baseline:  -0.9972890339425587 -1.0 -0.9972890339425587
probs:  [0.060315608067372545, 0.0575456952929445, 0.05687599598728509, 0.3706048449414732, 0.0575456952929445, 0.39711216041798014]
printing an ep nov before normalisation:  33.87081610851851
printing an ep nov before normalisation:  19.75055708487119
maxi score, test score, baseline:  -0.9972958333333334 -1.0 -0.9972958333333334
maxi score, test score, baseline:  -0.9973025974025974 -1.0 -0.9973025974025974
probs:  [0.06037207093866845, 0.05759955074380914, 0.05692922102564413, 0.37001507908544895, 0.05759955074380914, 0.39748452746262014]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.9973025974025974 -1.0 -0.9973025974025974
probs:  [0.06037207093866845, 0.05759955074380914, 0.05692922102564413, 0.37001507908544895, 0.05759955074380914, 0.39748452746262014]
printing an ep nov before normalisation:  37.864736098382046
maxi score, test score, baseline:  -0.9973025974025974 -1.0 -0.9973025974025974
printing an ep nov before normalisation:  33.62692375288873
maxi score, test score, baseline:  -0.9973025974025974 -1.0 -0.9973025974025974
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.482]
 [0.493]
 [0.493]
 [0.506]
 [0.493]
 [0.493]] [[64.606]
 [61.406]
 [64.606]
 [64.606]
 [66.087]
 [64.606]
 [64.606]] [[1.675]
 [1.555]
 [1.675]
 [1.675]
 [1.737]
 [1.675]
 [1.675]]
printing an ep nov before normalisation:  39.85877999022686
printing an ep nov before normalisation:  28.179444044473172
maxi score, test score, baseline:  -0.9973025974025974 -1.0 -0.9973025974025974
probs:  [0.06025188726143362, 0.05746487618926982, 0.05679104292017758, 0.37151327815167046, 0.05746487618926982, 0.3965140392881787]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  36.34747572972928
maxi score, test score, baseline:  -0.9973093264248705 -1.0 -0.9973093264248705
probs:  [0.06025189033979825, 0.05746487478092521, 0.056791040427052956, 0.3715132985445043, 0.05746487478092521, 0.3965140211267941]
printing an ep nov before normalisation:  23.215687902467234
printing an ep nov before normalisation:  24.961910247802734
siam score:  -0.734232
siam score:  -0.73975337
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9973226804123712 -1.0 -0.9973226804123712
actions average: 
K:  0  action  0 :  tensor([0.3740, 0.0347, 0.0973, 0.1428, 0.1107, 0.1075, 0.1331],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0207, 0.9078, 0.0111, 0.0169, 0.0095, 0.0093, 0.0246],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1554, 0.0046, 0.1898, 0.1681, 0.1585, 0.1747, 0.1489],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1211, 0.0063, 0.1089, 0.3102, 0.1550, 0.1431, 0.1554],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1717, 0.0041, 0.0989, 0.1328, 0.3628, 0.1084, 0.1212],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0815, 0.0028, 0.1231, 0.1947, 0.1223, 0.3773, 0.0983],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1390, 0.0385, 0.1050, 0.1924, 0.1251, 0.1349, 0.2651],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9973226804123712 -1.0 -0.9973226804123712
probs:  [0.05957173704272931, 0.0574793758881974, 0.056800276215235285, 0.37209439341286127, 0.0574793758881974, 0.39657484155277933]
maxi score, test score, baseline:  -0.9973226804123712 -1.0 -0.9973226804123712
probs:  [0.058961693612274796, 0.05757386501032938, 0.056893646386349535, 0.3717688724627278, 0.05757386501032938, 0.3972280575179892]
maxi score, test score, baseline:  -0.9973226804123712 -1.0 -0.9973226804123712
probs:  [0.058961693612274796, 0.05757386501032938, 0.056893646386349535, 0.3717688724627278, 0.05757386501032938, 0.3972280575179892]
Printing some Q and Qe and total Qs values:  [[0.304]
 [0.311]
 [0.297]
 [0.322]
 [0.342]
 [0.306]
 [0.314]] [[30.836]
 [37.27 ]
 [33.245]
 [35.202]
 [41.288]
 [41.491]
 [36.833]] [[0.304]
 [0.311]
 [0.297]
 [0.322]
 [0.342]
 [0.306]
 [0.314]]
maxi score, test score, baseline:  -0.9973293059125964 -1.0 -0.9973293059125964
probs:  [0.05896169457675603, 0.057573863767788135, 0.056893644062076885, 0.3717688932845839, 0.057573863767788135, 0.39722804054100697]
siam score:  -0.7473961
printing an ep nov before normalisation:  26.09510280761242
maxi score, test score, baseline:  -0.9973293059125964 -1.0 -0.9973293059125964
probs:  [0.05830382015122145, 0.05761404862992872, 0.05693335304970565, 0.37202888566631914, 0.05761404862992872, 0.3975058438728964]
using explorer policy with actor:  1
printing an ep nov before normalisation:  24.754319190979004
maxi score, test score, baseline:  -0.9973293059125964 -1.0 -0.9973293059125964
probs:  [0.05835829574325826, 0.05766787826479378, 0.05698654522683539, 0.3714414271549395, 0.05766787826479378, 0.3978779753453794]
Printing some Q and Qe and total Qs values:  [[0.463]
 [0.507]
 [0.406]
 [0.468]
 [0.467]
 [0.464]
 [0.47 ]] [[47.576]
 [45.7  ]
 [46.178]
 [47.464]
 [47.789]
 [48.778]
 [47.932]] [[2.012]
 [1.939]
 [1.868]
 [2.01 ]
 [2.029]
 [2.087]
 [2.04 ]]
Printing some Q and Qe and total Qs values:  [[0.589]
 [0.53 ]
 [0.559]
 [0.566]
 [0.597]
 [0.596]
 [0.621]] [[50.264]
 [49.42 ]
 [51.093]
 [51.248]
 [51.94 ]
 [51.841]
 [50.932]] [[1.689]
 [1.59 ]
 [1.698]
 [1.712]
 [1.775]
 [1.769]
 [1.752]]
maxi score, test score, baseline:  -0.9973293059125964 -1.0 -0.9973293059125964
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.402]
 [0.417]
 [0.418]
 [0.41 ]
 [0.404]
 [0.403]] [[42.124]
 [41.615]
 [45.271]
 [45.785]
 [47.385]
 [49.564]
 [49.671]] [[1.465]
 [1.463]
 [1.663]
 [1.689]
 [1.761]
 [1.865]
 [1.87 ]]
maxi score, test score, baseline:  -0.9973293059125964 -1.0 -0.9973293059125964
probs:  [0.058412629583634086, 0.057721567828851544, 0.0570395989918951, 0.37085549727714856, 0.057721567828851544, 0.3982491384896192]
maxi score, test score, baseline:  -0.9973293059125964 -1.0 -0.9973293059125964
probs:  [0.058412629583634086, 0.057721567828851544, 0.0570395989918951, 0.37085549727714856, 0.057721567828851544, 0.3982491384896192]
printing an ep nov before normalisation:  25.39251655835116
Printing some Q and Qe and total Qs values:  [[0.541]
 [0.36 ]
 [0.6  ]
 [0.647]
 [0.664]
 [0.565]
 [0.652]] [[35.596]
 [40.121]
 [38.015]
 [37.822]
 [37.625]
 [38.519]
 [37.349]] [[1.536]
 [1.604]
 [1.728]
 [1.765]
 [1.771]
 [1.721]
 [1.743]]
maxi score, test score, baseline:  -0.9973293059125964 -1.0 -0.9973293059125964
probs:  [0.058412629583634086, 0.057721567828851544, 0.0570395989918951, 0.37085549727714856, 0.057721567828851544, 0.3982491384896192]
maxi score, test score, baseline:  -0.9973293059125964 -1.0 -0.9973293059125964
probs:  [0.058888094765932604, 0.058195607696262176, 0.05683778991259471, 0.371045191408914, 0.058195607696262176, 0.3968377085200342]
using another actor
from probs:  [0.058888094765932604, 0.058195607696262176, 0.05683778991259471, 0.371045191408914, 0.058195607696262176, 0.3968377085200342]
maxi score, test score, baseline:  -0.9973293059125964 -1.0 -0.9973293059125964
probs:  [0.05894259877208244, 0.05824946931615912, 0.056890391951603565, 0.3704623588547038, 0.05824946931615912, 0.3972057117892919]
maxi score, test score, baseline:  -0.9973293059125964 -1.0 -0.9973293059125964
probs:  [0.05894259877208244, 0.05824946931615912, 0.056890391951603565, 0.3704623588547038, 0.05824946931615912, 0.3972057117892919]
printing an ep nov before normalisation:  46.73544154707598
maxi score, test score, baseline:  -0.9973358974358975 -1.0 -0.9973358974358975
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9973424552429667 -1.0 -0.9973424552429667
probs:  [0.05894260045282562, 0.05824946881196412, 0.05689038716321608, 0.37046239788913615, 0.05824946881196412, 0.3972056768708938]
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]] [[46.841]
 [46.841]
 [46.841]
 [46.841]
 [46.841]
 [46.841]
 [46.841]] [[2.215]
 [2.215]
 [2.215]
 [2.215]
 [2.215]
 [2.215]
 [2.215]]
maxi score, test score, baseline:  -0.9973424552429667 -1.0 -0.9973424552429667
probs:  [0.05869124991589111, 0.05799098518328143, 0.05661791708012521, 0.37341693086045236, 0.05799098518328143, 0.3952919317769685]
printing an ep nov before normalisation:  46.02859577319263
maxi score, test score, baseline:  -0.9973489795918368 -1.0 -0.9973489795918368
probs:  [0.058620672979222854, 0.057916187004529225, 0.056534841956110374, 0.3743051495098068, 0.057916187004529225, 0.3947069615458015]
printing an ep nov before normalisation:  55.135031256605444
maxi score, test score, baseline:  -0.9973489795918368 -1.0 -0.9973489795918368
probs:  [0.058620672979222854, 0.057916187004529225, 0.056534841956110374, 0.3743051495098068, 0.057916187004529225, 0.3947069615458015]
printing an ep nov before normalisation:  21.316521085403828
maxi score, test score, baseline:  -0.9973489795918368 -1.0 -0.9973489795918368
probs:  [0.05866139527494792, 0.0579564187969404, 0.05657411197731783, 0.3745656693422396, 0.057260718325222455, 0.39498168628333186]
printing an ep nov before normalisation:  23.504635840729136
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]] [[30.297]
 [30.297]
 [30.297]
 [30.297]
 [30.297]
 [30.297]
 [30.297]] [[1.134]
 [1.134]
 [1.134]
 [1.134]
 [1.134]
 [1.134]
 [1.134]]
maxi score, test score, baseline:  -0.9973489795918368 -1.0 -0.9973489795918368
probs:  [0.05853641201601005, 0.05782786908030883, 0.05643856920638492, 0.3760388376016343, 0.057128649077972125, 0.39402966301768966]
printing an ep nov before normalisation:  19.45481538772583
maxi score, test score, baseline:  -0.9973489795918368 -1.0 -0.9973489795918368
probs:  [0.058591356450346174, 0.05788214693300854, 0.056491540036268134, 0.37545245532439686, 0.05718226911984644, 0.3944002321361338]
printing an ep nov before normalisation:  35.348287246440485
printing an ep nov before normalisation:  38.746460382909724
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]] [[37.297]
 [37.297]
 [37.297]
 [37.297]
 [37.297]
 [37.297]
 [37.297]] [[1.73]
 [1.73]
 [1.73]
 [1.73]
 [1.73]
 [1.73]
 [1.73]]
printing an ep nov before normalisation:  42.19868033321574
maxi score, test score, baseline:  -0.9973489795918368 -1.0 -0.9973489795918368
probs:  [0.058591356450346174, 0.05788214693300854, 0.056491540036268134, 0.37545245532439686, 0.05718226911984644, 0.3944002321361338]
siam score:  -0.7486302
siam score:  -0.7496695
maxi score, test score, baseline:  -0.9973489795918368 -1.0 -0.9973489795918368
probs:  [0.05864616109807711, 0.057936286694983394, 0.05654437610068198, 0.3748675648886693, 0.05723575274456195, 0.3947698584730263]
using another actor
printing an ep nov before normalisation:  12.921191044033268
actions average: 
K:  1  action  0 :  tensor([0.4052, 0.0062, 0.0918, 0.1307, 0.1517, 0.1043, 0.1101],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0096, 0.9071, 0.0090, 0.0156, 0.0078, 0.0080, 0.0428],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1318, 0.0164, 0.3043, 0.1153, 0.1229, 0.1838, 0.1254],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0945, 0.0166, 0.0854, 0.4126, 0.1313, 0.1185, 0.1410],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1734, 0.0215, 0.0998, 0.1401, 0.3122, 0.1135, 0.1394],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0939, 0.0029, 0.1575, 0.1314, 0.1167, 0.3703, 0.1273],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1396, 0.0377, 0.1209, 0.1785, 0.1364, 0.1335, 0.2534],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9973554707379135 -1.0 -0.9973554707379135
probs:  [0.05870082733594621, 0.057990288624131726, 0.056597075463711136, 0.3742841817088274, 0.057289099105893734, 0.39513852776148967]
maxi score, test score, baseline:  -0.9973554707379135 -1.0 -0.9973554707379135
printing an ep nov before normalisation:  31.666984502964436
printing an ep nov before normalisation:  30.71204354596834
Printing some Q and Qe and total Qs values:  [[0.204]
 [0.236]
 [0.214]
 [0.205]
 [0.2  ]
 [0.202]
 [0.203]] [[34.157]
 [34.745]
 [33.713]
 [34.023]
 [34.15 ]
 [35.092]
 [35.226]] [[1.245]
 [1.313]
 [1.229]
 [1.238]
 [1.242]
 [1.3  ]
 [1.309]]
maxi score, test score, baseline:  -0.9973554707379135 -1.0 -0.9973554707379135
maxi score, test score, baseline:  -0.9973554707379135 -1.0 -0.9973554707379135
printing an ep nov before normalisation:  51.24800180135619
maxi score, test score, baseline:  -0.9973554707379135 -1.0 -0.9973554707379135
probs:  [0.058796020373440123, 0.05808432678525096, 0.05668884916135068, 0.37396140362394636, 0.05668884916135068, 0.3957805508946613]
Printing some Q and Qe and total Qs values:  [[-0.004]
 [ 0.049]
 [ 0.014]
 [-0.007]
 [ 0.014]
 [-0.01 ]
 [-0.011]] [[24.195]
 [25.397]
 [25.017]
 [27.215]
 [25.017]
 [26.823]
 [26.232]] [[0.911]
 [1.073]
 [1.004]
 [1.181]
 [1.004]
 [1.143]
 [1.088]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9973554707379135 -1.0 -0.9973554707379135
probs:  [0.05867195307130922, 0.05795666584743494, 0.05655414187905396, 0.37542873687223177, 0.05655414187905396, 0.39483436045091613]
printing an ep nov before normalisation:  37.527832984924316
printing an ep nov before normalisation:  36.198225021362305
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.366]
 [0.383]
 [0.388]
 [0.64 ]
 [0.406]
 [0.624]] [[25.42 ]
 [28.526]
 [25.587]
 [25.588]
 [21.989]
 [25.623]
 [20.504]] [[1.358]
 [1.366]
 [1.251]
 [1.255]
 [1.345]
 [1.275]
 [1.262]]
maxi score, test score, baseline:  -0.9973554707379135 -1.0 -0.9973554707379135
probs:  [0.058726576588925515, 0.05801062191793151, 0.05660678922970794, 0.3748465599960388, 0.05660678922970794, 0.3952026630376883]
maxi score, test score, baseline:  -0.9973554707379135 -1.0 -0.9973554707379135
probs:  [0.058726576588925515, 0.05801062191793151, 0.05660678922970794, 0.3748465599960388, 0.05660678922970794, 0.3952026630376883]
printing an ep nov before normalisation:  32.70587205886841
printing an ep nov before normalisation:  22.980908889287864
maxi score, test score, baseline:  -0.9973554707379135 -1.0 -0.9973554707379135
probs:  [0.05878106242023936, 0.058064441984520515, 0.05665930387526783, 0.3742658505790088, 0.05665930387526783, 0.3955700372656956]
printing an ep nov before normalisation:  43.743496260846904
printing an ep nov before normalisation:  37.20343859797249
printing an ep nov before normalisation:  24.51907399288574
maxi score, test score, baseline:  -0.9973554707379135 -1.0 -0.9973554707379135
probs:  [0.059273946215860115, 0.05855603707769603, 0.05645825063500872, 0.3743993116811346, 0.057148372100903684, 0.39416408228939676]
maxi score, test score, baseline:  -0.9973554707379135 -1.0 -0.9973554707379135
probs:  [0.059273946215860115, 0.05855603707769603, 0.05645825063500872, 0.3743993116811346, 0.057148372100903684, 0.39416408228939676]
printing an ep nov before normalisation:  16.775958486031183
maxi score, test score, baseline:  -0.9973554707379135 -1.0 -0.9973554707379135
probs:  [0.059153972590059306, 0.05843248850442824, 0.05632425578667511, 0.37584855700959957, 0.05701781382672026, 0.3932229122825175]
maxi score, test score, baseline:  -0.9973554707379135 -1.0 -0.9973554707379135
probs:  [0.05903415117317897, 0.05830909667556856, 0.056190430935797855, 0.3772959637000681, 0.05688742119005794, 0.3922829363253285]
maxi score, test score, baseline:  -0.9973554707379135 -1.0 -0.9973554707379135
probs:  [0.05903415117317897, 0.05830909667556856, 0.056190430935797855, 0.3772959637000681, 0.05688742119005794, 0.3922829363253285]
siam score:  -0.7516197
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9973554707379135 -1.0 -0.9973554707379135
probs:  [0.05908913651644072, 0.058363405164080964, 0.05624276160199079, 0.37671527726892856, 0.05694040251239521, 0.39264901693616366]
printing an ep nov before normalisation:  60.117569961110696
maxi score, test score, baseline:  -0.9973554707379135 -1.0 -0.9973554707379135
probs:  [0.05908913651644072, 0.058363405164080964, 0.05624276160199079, 0.37671527726892856, 0.05694040251239521, 0.39264901693616366]
printing an ep nov before normalisation:  62.70793157479234
printing an ep nov before normalisation:  42.19852841074857
actions average: 
K:  4  action  0 :  tensor([0.5679, 0.0595, 0.0671, 0.0807, 0.0781, 0.0688, 0.0780],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0181, 0.7867, 0.0345, 0.0488, 0.0250, 0.0435, 0.0434],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0890, 0.0651, 0.1788, 0.2577, 0.1442, 0.1357, 0.1295],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1759, 0.0137, 0.1076, 0.2481, 0.1596, 0.1450, 0.1502],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1501, 0.0988, 0.0719, 0.1205, 0.3531, 0.0953, 0.1104],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0462, 0.1310, 0.1924, 0.1860, 0.0834, 0.2779, 0.0832],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1006, 0.1321, 0.1149, 0.1505, 0.1346, 0.1161, 0.2512],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9973554707379135 -1.0 -0.9973554707379135
from probs:  [0.05908913651644072, 0.058363405164080964, 0.05624276160199079, 0.37671527726892856, 0.05694040251239521, 0.39264901693616366]
printing an ep nov before normalisation:  31.87077311077754
maxi score, test score, baseline:  -0.9973619289340102 -1.0 -0.9973619289340102
probs:  [0.05914398666977749, 0.058417579014639905, 0.056294959243133935, 0.3761360585617124, 0.05699325027907599, 0.3930141662316603]
maxi score, test score, baseline:  -0.9973683544303797 -1.0 -0.9973683544303797
probs:  [0.05914398845102857, 0.05841757966742735, 0.05629495659846274, 0.3761360795643263, 0.05699324871918964, 0.39301414699956533]
printing an ep nov before normalisation:  60.75519767163005
maxi score, test score, baseline:  -0.9973747474747475 -1.0 -0.9973747474747475
printing an ep nov before normalisation:  60.40654158234859
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9973747474747475 -1.0 -0.9973747474747475
probs:  [0.0590660257935512, 0.05833552360954144, 0.05620093930561685, 0.37784393178966097, 0.05620093930561685, 0.39235264019601274]
maxi score, test score, baseline:  -0.9973747474747475 -1.0 -0.9973747474747475
probs:  [0.0590660257935512, 0.05833552360954144, 0.05620093930561685, 0.37784393178966097, 0.05620093930561685, 0.39235264019601274]
printing an ep nov before normalisation:  73.64249943823005
siam score:  -0.75468534
maxi score, test score, baseline:  -0.9973747474747475 -1.0 -0.9973747474747475
probs:  [0.059002046552900395, 0.05826727674942861, 0.05612022212889417, 0.378706059394002, 0.05612022212889417, 0.39178417304588065]
printing an ep nov before normalisation:  50.90492405993708
maxi score, test score, baseline:  -0.9973747474747475 -1.0 -0.9973747474747475
probs:  [0.059002046552900395, 0.05826727674942861, 0.05612022212889417, 0.378706059394002, 0.05612022212889417, 0.39178417304588065]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
printing an ep nov before normalisation:  48.78164113853991
maxi score, test score, baseline:  -0.9973747474747475 -1.0 -0.9973747474747475
probs:  [0.059002046552900395, 0.05826727674942861, 0.05612022212889417, 0.378706059394002, 0.05612022212889417, 0.39178417304588065]
maxi score, test score, baseline:  -0.9973747474747475 -1.0 -0.9973747474747475
Printing some Q and Qe and total Qs values:  [[0.235]
 [0.271]
 [0.235]
 [0.257]
 [0.246]
 [0.237]
 [0.25 ]] [[31.124]
 [31.243]
 [31.124]
 [31.669]
 [31.497]
 [31.658]
 [31.83 ]] [[1.831]
 [1.879]
 [1.831]
 [1.908]
 [1.879]
 [1.887]
 [1.917]]
maxi score, test score, baseline:  -0.9973747474747475 -1.0 -0.9973747474747475
probs:  [0.05904479761128582, 0.0575838658563972, 0.05616088038085636, 0.37898098233837446, 0.05616088038085636, 0.39206859343222983]
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.539]
 [0.534]
 [0.534]
 [0.547]
 [0.547]
 [0.547]] [[61.113]
 [59.33 ]
 [61.113]
 [61.113]
 [64.06 ]
 [64.13 ]
 [63.554]] [[1.721]
 [1.66 ]
 [1.721]
 [1.721]
 [1.842]
 [1.844]
 [1.823]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9973747474747475 -1.0 -0.9973747474747475
probs:  [0.05909984738968033, 0.05763755047056026, 0.05621323528959915, 0.378401295049663, 0.05621323528959915, 0.3924348365108982]
maxi score, test score, baseline:  -0.9973747474747475 -1.0 -0.9973747474747475
probs:  [0.05966454817459163, 0.058208352257186104, 0.05678997961036255, 0.377633804913458, 0.056094519473855506, 0.39160879557054623]
siam score:  -0.75614953
maxi score, test score, baseline:  -0.9973747474747475 -1.0 -0.9973747474747475
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.446]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]] [[34.001]
 [48.986]
 [34.001]
 [34.001]
 [34.001]
 [34.001]
 [34.001]] [[1.394]
 [2.225]
 [1.394]
 [1.394]
 [1.394]
 [1.394]
 [1.394]]
printing an ep nov before normalisation:  49.10297793926544
maxi score, test score, baseline:  -0.9973747474747475 -1.0 -0.9973747474747475
probs:  [0.05966454817459163, 0.058208352257186104, 0.05678997961036255, 0.377633804913458, 0.056094519473855506, 0.39160879557054623]
maxi score, test score, baseline:  -0.9973747474747475 -1.0 -0.9973747474747475
probs:  [0.05971975683966105, 0.05826221046285434, 0.05684252243349717, 0.37705724740324015, 0.05614641733523172, 0.3919718455255156]
using explorer policy with actor:  1
printing an ep nov before normalisation:  52.91811064685352
printing an ep nov before normalisation:  30.07341838543187
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.9973747474747475 -1.0 -0.9973747474747475
probs:  [0.059774829369027956, 0.0583159358628361, 0.05689493569446742, 0.37648211159125716, 0.05619818722481568, 0.39233400025759557]
printing an ep nov before normalisation:  22.51068405474939
printing an ep nov before normalisation:  19.42296960641272
maxi score, test score, baseline:  -0.9973747474747475 -1.0 -0.9973747474747475
probs:  [0.059816446828634094, 0.058356535317578194, 0.056237308930561586, 0.3767447249991022, 0.056237308930561586, 0.39260767499356236]
Printing some Q and Qe and total Qs values:  [[0.249]
 [0.426]
 [0.227]
 [0.146]
 [0.182]
 [0.303]
 [0.249]] [[23.885]
 [27.074]
 [27.102]
 [29.685]
 [29.83 ]
 [38.377]
 [28.482]] [[0.942]
 [1.3  ]
 [1.102]
 [1.168]
 [1.211]
 [1.816]
 [1.202]]
printing an ep nov before normalisation:  30.232300038958613
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  53.436580783357066
printing an ep nov before normalisation:  70.4965137182626
siam score:  -0.75046843
maxi score, test score, baseline:  -0.9973811083123426 -1.0 -0.9973811083123426
maxi score, test score, baseline:  -0.9973811083123426 -1.0 -0.9973811083123426
printing an ep nov before normalisation:  53.9186326193801
printing an ep nov before normalisation:  41.97418571111391
printing an ep nov before normalisation:  47.32244390668597
maxi score, test score, baseline:  -0.9973811083123426 -1.0 -0.9973811083123426
probs:  [0.05870785433075062, 0.057967366694323036, 0.05580323186011856, 0.38216235513315566, 0.05580323186011856, 0.3895559601215335]
maxi score, test score, baseline:  -0.9973811083123426 -1.0 -0.9973811083123426
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.390021070683154
maxi score, test score, baseline:  -0.9973811083123426 -1.0 -0.9973811083123426
Printing some Q and Qe and total Qs values:  [[0.217]
 [0.239]
 [0.233]
 [0.233]
 [0.197]
 [0.146]
 [0.211]] [[28.493]
 [29.441]
 [29.256]
 [29.256]
 [34.032]
 [32.127]
 [30.989]] [[1.257]
 [1.353]
 [1.333]
 [1.333]
 [1.673]
 [1.472]
 [1.447]]
printing an ep nov before normalisation:  30.298396853834628
maxi score, test score, baseline:  -0.9973874371859297 -1.0 -0.9973874371859297
maxi score, test score, baseline:  -0.9973874371859297 -1.0 -0.9973874371859297
probs:  [0.05862645324032737, 0.057137227940106936, 0.05568643426053735, 0.38413214768202025, 0.05568643426053735, 0.38873130261647076]
printing an ep nov before normalisation:  55.69091798230322
Printing some Q and Qe and total Qs values:  [[0.374]
 [0.339]
 [0.366]
 [0.345]
 [0.366]
 [0.366]
 [0.366]] [[14.161]
 [26.663]
 [12.861]
 [14.626]
 [12.861]
 [12.861]
 [12.861]] [[0.448]
 [0.547]
 [0.428]
 [0.425]
 [0.428]
 [0.428]
 [0.428]]
Starting evaluation
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.9973874371859297 -1.0 -0.9973874371859297
probs:  [0.05862645324032737, 0.057137227940106936, 0.05568643426053735, 0.38413214768202025, 0.05568643426053735, 0.38873130261647076]
Printing some Q and Qe and total Qs values:  [[0.27]
 [0.27]
 [0.27]
 [0.27]
 [0.27]
 [0.27]
 [0.27]] [[63.459]
 [63.459]
 [63.459]
 [63.459]
 [63.459]
 [63.459]
 [63.459]] [[0.27]
 [0.27]
 [0.27]
 [0.27]
 [0.27]
 [0.27]
 [0.27]]
printing an ep nov before normalisation:  41.12184746679037
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9973874371859297 -1.0 -0.9973874371859297
probs:  [0.05862645324032737, 0.057137227940106936, 0.05568643426053735, 0.38413214768202025, 0.05568643426053735, 0.38873130261647076]
printing an ep nov before normalisation:  40.61578342762786
printing an ep nov before normalisation:  39.493096074014886
Printing some Q and Qe and total Qs values:  [[0.239]
 [0.253]
 [0.254]
 [0.252]
 [0.25 ]
 [0.25 ]
 [0.248]] [[59.014]
 [58.383]
 [52.146]
 [55.506]
 [57.873]
 [56.396]
 [57.141]] [[0.239]
 [0.253]
 [0.254]
 [0.252]
 [0.25 ]
 [0.25 ]
 [0.248]]
printing an ep nov before normalisation:  51.37557045618495
printing an ep nov before normalisation:  36.316948025370856
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.275]
 [0.263]
 [0.264]
 [0.264]
 [0.259]
 [0.262]] [[27.577]
 [35.19 ]
 [22.367]
 [22.28 ]
 [25.779]
 [25.025]
 [21.042]] [[0.282]
 [0.275]
 [0.263]
 [0.264]
 [0.264]
 [0.259]
 [0.262]]
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.411]
 [0.417]
 [0.419]
 [0.418]
 [0.419]
 [0.423]] [[40.756]
 [40.238]
 [45.319]
 [44.961]
 [45.456]
 [44.197]
 [43.202]] [[0.48 ]
 [0.411]
 [0.417]
 [0.419]
 [0.418]
 [0.419]
 [0.423]]
actions average: 
K:  2  action  0 :  tensor([0.5499, 0.0036, 0.0678, 0.1018, 0.1141, 0.0822, 0.0807],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0146, 0.8862, 0.0082, 0.0213, 0.0267, 0.0108, 0.0323],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1154, 0.0077, 0.3549, 0.1569, 0.1342, 0.1257, 0.1052],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1236, 0.0446, 0.0961, 0.3382, 0.1560, 0.1178, 0.1238],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1282, 0.0255, 0.1019, 0.1675, 0.3030, 0.1415, 0.1324],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0778, 0.0029, 0.1192, 0.1338, 0.1327, 0.4246, 0.1091],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1040, 0.0226, 0.1033, 0.2428, 0.1700, 0.1249, 0.2325],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  50.49313514923074
printing an ep nov before normalisation:  26.694236659395898
Printing some Q and Qe and total Qs values:  [[0.261]
 [0.261]
 [0.261]
 [0.262]
 [0.261]
 [0.261]
 [0.261]] [[13.339]
 [13.339]
 [13.339]
 [13.64 ]
 [13.339]
 [13.339]
 [13.339]] [[0.261]
 [0.261]
 [0.261]
 [0.262]
 [0.261]
 [0.261]
 [0.261]]
printing an ep nov before normalisation:  33.55947256088257
printing an ep nov before normalisation:  0.0
using explorer policy with actor:  0
printing an ep nov before normalisation:  26.600697564175313
Printing some Q and Qe and total Qs values:  [[0.407]
 [0.407]
 [0.407]
 [0.391]
 [0.407]
 [0.451]
 [0.407]] [[43.074]
 [43.074]
 [43.074]
 [42.194]
 [43.074]
 [47.152]
 [43.074]] [[0.407]
 [0.407]
 [0.407]
 [0.391]
 [0.407]
 [0.451]
 [0.407]]
printing an ep nov before normalisation:  17.14551091194153
printing an ep nov before normalisation:  44.489315065885364
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.432]
 [0.358]
 [0.442]
 [0.358]
 [0.448]
 [0.422]] [[41.769]
 [41.38 ]
 [41.769]
 [44.706]
 [41.769]
 [49.781]
 [43.734]] [[0.358]
 [0.432]
 [0.358]
 [0.442]
 [0.358]
 [0.448]
 [0.422]]
printing an ep nov before normalisation:  39.65350668812496
maxi score, test score, baseline:  -0.9974609756097561 -1.0 -0.9974609756097561
printing an ep nov before normalisation:  39.67547840392468
maxi score, test score, baseline:  -0.9974669099756691 -1.0 -0.9974669099756691
probs:  [0.05732973665319377, 0.05732973665319377, 0.05587401506637163, 0.38354906431089697, 0.05587401506637163, 0.3900434322499722]
printing an ep nov before normalisation:  16.378783341606002
using explorer policy with actor:  0
printing an ep nov before normalisation:  16.537437468621462
printing an ep nov before normalisation:  11.537361239373816
Printing some Q and Qe and total Qs values:  [[0.65 ]
 [0.391]
 [0.4  ]
 [0.493]
 [0.442]
 [0.475]
 [0.483]] [[12.989]
 [17.781]
 [15.77 ]
 [16.118]
 [18.017]
 [18.45 ]
 [17.511]] [[0.65 ]
 [0.391]
 [0.4  ]
 [0.493]
 [0.442]
 [0.475]
 [0.483]]
printing an ep nov before normalisation:  12.924023401990521
printing an ep nov before normalisation:  14.677356510067057
Printing some Q and Qe and total Qs values:  [[0.642]
 [0.588]
 [0.588]
 [0.588]
 [0.588]
 [0.588]
 [0.588]] [[13.496]
 [10.802]
 [10.802]
 [10.802]
 [10.802]
 [10.802]
 [10.802]] [[0.642]
 [0.588]
 [0.588]
 [0.588]
 [0.588]
 [0.588]
 [0.588]]
actor:  0 policy actor:  1  step number:  68 total reward:  0.19333333333333313  reward:  1.0 rdn_beta:  2
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9945763754045308 -1.0 -0.9945763754045308
probs:  [0.0574900560437307, 0.0574900560437307, 0.05603129308770064, 0.3818133511685342, 0.05603129308770064, 0.391143950568603]
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.466]
 [0.427]
 [0.427]
 [0.429]
 [0.431]
 [0.432]] [[44.639]
 [48.802]
 [44.639]
 [44.639]
 [48.438]
 [49.178]
 [48.122]] [[1.487]
 [1.719]
 [1.487]
 [1.487]
 [1.665]
 [1.701]
 [1.653]]
Printing some Q and Qe and total Qs values:  [[0.619]
 [0.383]
 [0.391]
 [0.478]
 [0.434]
 [0.467]
 [0.475]] [[14.321]
 [21.387]
 [13.84 ]
 [17.146]
 [15.228]
 [16.073]
 [15.656]] [[0.619]
 [0.383]
 [0.391]
 [0.478]
 [0.434]
 [0.467]
 [0.475]]
printing an ep nov before normalisation:  15.65582275390625
maxi score, test score, baseline:  -0.9945763754045308 -1.0 -0.9945763754045308
probs:  [0.0574900560437307, 0.0574900560437307, 0.05603129308770064, 0.3818133511685342, 0.05603129308770064, 0.391143950568603]
printing an ep nov before normalisation:  15.313304631537381
from probs:  [0.0574900560437307, 0.0574900560437307, 0.05603129308770064, 0.3818133511685342, 0.05603129308770064, 0.391143950568603]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9945763754045308 -1.0 -0.9945763754045308
printing an ep nov before normalisation:  42.57148769385781
Printing some Q and Qe and total Qs values:  [[0.843]
 [0.495]
 [0.447]
 [0.488]
 [0.5  ]
 [0.468]
 [0.498]] [[39.19 ]
 [50.269]
 [47.283]
 [46.389]
 [47.937]
 [49.775]
 [50.018]] [[1.786]
 [2.011]
 [1.809]
 [1.803]
 [1.895]
 [1.959]
 [2.001]]
maxi score, test score, baseline:  -0.9945892655367232 -1.0 -0.9945892655367232
probs:  [0.05736710451494696, 0.05736710451494696, 0.05590140569455159, 0.38323145076261106, 0.05590140569455159, 0.39023152881839185]
maxi score, test score, baseline:  -0.9946020933977456 -1.0 -0.9946020933977456
probs:  [0.057367103809045125, 0.057367103809045125, 0.05590140035574694, 0.3832315017596082, 0.05590140035574694, 0.39023148991080775]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9946020933977456 -1.0 -0.9946020933977456
line 256 mcts: sample exp_bonus 15.30524082728361
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
printing an ep nov before normalisation:  22.84429222389253
printing an ep nov before normalisation:  17.905873508843868
printing an ep nov before normalisation:  13.714376311665166
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.58 ]
 [0.58 ]
 [0.58 ]
 [0.58 ]
 [0.58 ]
 [0.58 ]] [[16.387]
 [14.866]
 [14.866]
 [14.866]
 [14.866]
 [14.866]
 [14.866]] [[0.606]
 [0.58 ]
 [0.58 ]
 [0.58 ]
 [0.58 ]
 [0.58 ]
 [0.58 ]]
printing an ep nov before normalisation:  20.011129556498467
maxi score, test score, baseline:  -0.9946148594377511 -1.0 -0.9946148594377511
probs:  [0.05811602671095208, 0.05811602671095208, 0.056652563967675304, 0.38071001888213923, 0.055934904353183795, 0.3904704593750976]
printing an ep nov before normalisation:  19.3156351933359
maxi score, test score, baseline:  -0.9946148594377511 -1.0 -0.9946148594377511
probs:  [0.0579969356200618, 0.0579969356200618, 0.056526570657004734, 0.38211239777331085, 0.05580552630012098, 0.3895616340294398]
printing an ep nov before normalisation:  12.686306238174438
printing an ep nov before normalisation:  19.59135315895363
maxi score, test score, baseline:  -0.9946148594377511 -1.0 -0.9946148594377511
probs:  [0.0579969356200618, 0.0579969356200618, 0.056526570657004734, 0.38211239777331085, 0.05580552630012098, 0.3895616340294398]
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]] [[13.908]
 [13.908]
 [13.908]
 [13.908]
 [13.908]
 [13.908]
 [13.908]] [[0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]]
printing an ep nov before normalisation:  15.137324333190918
maxi score, test score, baseline:  -0.9946148594377511 -1.0 -0.9946148594377511
probs:  [0.0579969356200618, 0.0579969356200618, 0.056526570657004734, 0.38211239777331085, 0.05580552630012098, 0.3895616340294398]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9946148594377511 -1.0 -0.9946148594377511
maxi score, test score, baseline:  -0.9946148594377511 -1.0 -0.9946148594377511
probs:  [0.05787798060086285, 0.05787798060086285, 0.056400721304398146, 0.3835131743274296, 0.05567629607247794, 0.3886538470939687]
printing an ep nov before normalisation:  22.333521831029373
maxi score, test score, baseline:  -0.9946148594377511 -1.0 -0.9946148594377511
probs:  [0.057176996400805626, 0.057920975881578296, 0.05644261665526875, 0.38379862932321696, 0.055717652034674665, 0.3889431297044557]
maxi score, test score, baseline:  -0.9946275641025641 -1.0 -0.9946275641025641
probs:  [0.057176995340712765, 0.0579209771489502, 0.05644261329774294, 0.38379867900119935, 0.05571764640917016, 0.3889430888022247]
using explorer policy with actor:  1
printing an ep nov before normalisation:  18.015366925415222
maxi score, test score, baseline:  -0.9946901821060966 -0.9403333333333332 -0.9403333333333332
probs:  [0.05711289804242536, 0.05785056092488297, 0.05638475339070911, 0.3844012690625554, 0.05566594392683539, 0.3885845746525917]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9946901821060966 -0.9403333333333332 -0.9403333333333332
from probs:  [0.05704420438904222, 0.05778596910159765, 0.05631201083406812, 0.38522460808121084, 0.05558920437595268, 0.3880440032181285]
maxi score, test score, baseline:  -0.9947025276461295 -0.9403333333333332 -0.9403333333333332
probs:  [0.05704420438904222, 0.05778596910159765, 0.05631201083406812, 0.38522460808121084, 0.05558920437595268, 0.3880440032181285]
printing an ep nov before normalisation:  27.63411250476222
printing an ep nov before normalisation:  34.17116731407349
maxi score, test score, baseline:  -0.9947025276461295 -0.9403333333333332 -0.9403333333333332
probs:  [0.056975787683907264, 0.05772166136825476, 0.05623953817613203, 0.38604501665350277, 0.05551272776461032, 0.38750526835359284]
Printing some Q and Qe and total Qs values:  [[0.472]
 [0.466]
 [0.472]
 [0.472]
 [0.471]
 [0.472]
 [0.472]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.472]
 [0.466]
 [0.472]
 [0.472]
 [0.471]
 [0.472]
 [0.472]]
maxi score, test score, baseline:  -0.9947025276461295 -0.9403333333333332 -0.9403333333333332
probs:  [0.05711601560609846, 0.057866941897551795, 0.055643044803632274, 0.3884143854604153, 0.055643044803632274, 0.3853165674286699]
Printing some Q and Qe and total Qs values:  [[0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]] [[51.874]
 [51.874]
 [51.874]
 [51.874]
 [51.874]
 [51.874]
 [51.874]] [[1.619]
 [1.619]
 [1.619]
 [1.619]
 [1.619]
 [1.619]
 [1.619]]
maxi score, test score, baseline:  -0.9947148148148148 -0.9403333333333332 -0.9403333333333332
maxi score, test score, baseline:  -0.9947148148148148 -0.9403333333333332 -0.9403333333333332
probs:  [0.05724457317390615, 0.05799719361300607, 0.0557682792356717, 0.3892903777098797, 0.0557682792356717, 0.3839312970318646]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.9947148148148148 -0.9403333333333332 -0.9403333333333332
probs:  [0.057402285885334726, 0.058161107614383645, 0.055913827878354166, 0.3903053416843316, 0.055913827878354166, 0.38230360905924177]
printing an ep nov before normalisation:  32.44202067028596
Printing some Q and Qe and total Qs values:  [[0.443]
 [0.47 ]
 [0.443]
 [0.454]
 [0.45 ]
 [0.452]
 [0.456]] [[60.319]
 [57.75 ]
 [60.319]
 [65.962]
 [70.989]
 [66.883]
 [64.459]] [[0.938]
 [0.924]
 [0.938]
 [1.04 ]
 [1.117]
 [1.053]
 [1.018]]
Printing some Q and Qe and total Qs values:  [[0.074]
 [0.093]
 [0.074]
 [0.078]
 [0.074]
 [0.101]
 [0.106]] [[40.399]
 [36.063]
 [40.399]
 [42.17 ]
 [40.399]
 [41.161]
 [41.795]] [[1.683]
 [1.365]
 [1.683]
 [1.825]
 [1.683]
 [1.77 ]
 [1.824]]
maxi score, test score, baseline:  -0.9947148148148148 -0.9403333333333332 -0.9403333333333332
probs:  [0.057528944991012146, 0.05828944496801946, 0.056037195036113194, 0.391168265781312, 0.056037195036113194, 0.38093895418742996]
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.5  ]
 [0.507]
 [0.479]
 [0.479]
 [0.482]
 [0.487]] [[34.608]
 [33.399]
 [37.933]
 [38.921]
 [38.502]
 [38.451]
 [39.011]] [[0.544]
 [0.5  ]
 [0.507]
 [0.479]
 [0.479]
 [0.482]
 [0.487]]
printing an ep nov before normalisation:  39.52089984134498
Printing some Q and Qe and total Qs values:  [[ 0.038]
 [-0.006]
 [-0.005]
 [ 0.011]
 [-0.006]
 [-0.009]
 [-0.005]] [[34.697]
 [34.437]
 [35.091]
 [33.087]
 [33.983]
 [34.695]
 [34.394]] [[1.738]
 [1.67 ]
 [1.732]
 [1.563]
 [1.629]
 [1.691]
 [1.668]]
maxi score, test score, baseline:  -0.9947270440251572 -0.9403333333333332 -0.9403333333333332
probs:  [0.057528944991012146, 0.05828944496801946, 0.056037195036113194, 0.391168265781312, 0.056037195036113194, 0.38093895418742996]
printing an ep nov before normalisation:  39.947384320148416
actions average: 
K:  1  action  0 :  tensor([0.5609, 0.0751, 0.0533, 0.0755, 0.0866, 0.0705, 0.0781],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0133, 0.9134, 0.0134, 0.0141, 0.0127, 0.0140, 0.0191],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1053, 0.0017, 0.1851, 0.1528, 0.1723, 0.2274, 0.1555],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1147, 0.0034, 0.0892, 0.3730, 0.1485, 0.1326, 0.1386],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1512, 0.0020, 0.1027, 0.1530, 0.3131, 0.1355, 0.1425],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1156, 0.0029, 0.1314, 0.1837, 0.1705, 0.2446, 0.1514],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1966, 0.0315, 0.1004, 0.1581, 0.1407, 0.1177, 0.2550],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9947270440251572 -0.9403333333333332 -0.9403333333333332
printing an ep nov before normalisation:  28.553507872970034
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.10355782427821
printing an ep nov before normalisation:  45.21624565124512
maxi score, test score, baseline:  -0.9947392156862745 -0.9403333333333332 -0.9403333333333332
probs:  [0.057948995047924674, 0.057948995047924674, 0.05644632773415855, 0.3940300522601375, 0.05644632773415855, 0.37717930217569606]
siam score:  -0.73999506
maxi score, test score, baseline:  -0.9947392156862745 -0.9403333333333332 -0.9403333333333332
probs:  [0.05807305088819477, 0.05807305088819477, 0.05656715928697646, 0.39487524040012684, 0.05656715928697646, 0.37584433924953065]
printing an ep nov before normalisation:  54.53559325502989
printing an ep nov before normalisation:  36.51753805888531
printing an ep nov before normalisation:  30.188850154520374
using another actor
maxi score, test score, baseline:  -0.9947513302034429 -0.9403333333333332 -0.9403333333333332
probs:  [0.05814866935026973, 0.05814866935026973, 0.05663670820266798, 0.39536013376284884, 0.05663670820266798, 0.3750691111312756]
using explorer policy with actor:  0
printing an ep nov before normalisation:  23.095063189775473
printing an ep nov before normalisation:  44.89723468384277
printing an ep nov before normalisation:  49.26483864301274
printing an ep nov before normalisation:  37.86243093801901
printing an ep nov before normalisation:  41.9430725204503
printing an ep nov before normalisation:  44.8315019687834
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.472956949593762
printing an ep nov before normalisation:  23.523927205959538
maxi score, test score, baseline:  -0.9947753894080997 -0.9403333333333332 -0.9403333333333332
probs:  [0.058223890528832305, 0.058223890528832305, 0.056705854277293646, 0.3958422032141619, 0.056705854277293646, 0.3742983071735863]
printing an ep nov before normalisation:  41.447746317085674
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.994787334887335 -0.9403333333333332 -0.9403333333333332
probs:  [0.05808118427852718, 0.05808118427852718, 0.05655453189904253, 0.3947789795150852, 0.05655453189904253, 0.37594958812977536]
maxi score, test score, baseline:  -0.994787334887335 -0.9403333333333332 -0.9403333333333332
probs:  [0.05808118427852718, 0.05808118427852718, 0.05655453189904253, 0.3947789795150852, 0.05655453189904253, 0.37594958812977536]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.994787334887335 -0.9403333333333332 -0.9403333333333332
printing an ep nov before normalisation:  56.141061782836914
printing an ep nov before normalisation:  56.31749629974365
maxi score, test score, baseline:  -0.994787334887335 -0.9403333333333332 -0.9403333333333332
probs:  [0.05808118427852718, 0.05808118427852718, 0.05655453189904253, 0.3947789795150852, 0.05655453189904253, 0.37594958812977536]
printing an ep nov before normalisation:  48.36601219963131
printing an ep nov before normalisation:  41.66267954460963
printing an ep nov before normalisation:  27.851238250732422
maxi score, test score, baseline:  -0.994787334887335 -0.9403333333333332 -0.9403333333333332
maxi score, test score, baseline:  -0.994787334887335 -0.9403333333333332 -0.9403333333333332
probs:  [0.05808118427852718, 0.05808118427852718, 0.05655453189904253, 0.3947789795150852, 0.05655453189904253, 0.37594958812977536]
Printing some Q and Qe and total Qs values:  [[0.323]
 [0.467]
 [0.324]
 [0.326]
 [0.322]
 [0.376]
 [0.32 ]] [[20.575]
 [20.845]
 [16.795]
 [16.638]
 [16.638]
 [18.806]
 [16.389]] [[1.255]
 [1.411]
 [1.084]
 [1.079]
 [1.075]
 [1.227]
 [1.062]]
maxi score, test score, baseline:  -0.994787334887335 -0.9403333333333332 -0.9403333333333332
using explorer policy with actor:  1
printing an ep nov before normalisation:  16.114021116281023
maxi score, test score, baseline:  -0.9947992248062015 -0.9403333333333332 -0.9403333333333332
probs:  [0.05808118427852718, 0.05808118427852718, 0.05655453189904253, 0.3947789795150852, 0.05655453189904253, 0.37594958812977536]
printing an ep nov before normalisation:  35.05186132764454
maxi score, test score, baseline:  -0.9947992248062015 -0.9403333333333332 -0.9403333333333332
probs:  [0.05808118427852718, 0.05808118427852718, 0.05655453189904253, 0.3947789795150852, 0.05655453189904253, 0.37594958812977536]
printing an ep nov before normalisation:  23.65675687789917
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  76 total reward:  0.13999999999999968  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9948110595514308 -0.9403333333333332 -0.9403333333333332
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.403]
 [0.463]
 [0.464]
 [0.466]
 [0.463]
 [0.458]] [[59.389]
 [49.229]
 [63.814]
 [67.944]
 [67.933]
 [66.117]
 [65.154]] [[1.857]
 [1.324]
 [2.045]
 [2.234]
 [2.236]
 [2.15 ]
 [2.101]]
Printing some Q and Qe and total Qs values:  [[0.239]
 [0.31 ]
 [0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]] [[47.277]
 [53.077]
 [47.277]
 [47.277]
 [47.277]
 [47.277]
 [47.277]] [[1.141]
 [1.428]
 [1.141]
 [1.141]
 [1.141]
 [1.141]
 [1.141]]
printing an ep nov before normalisation:  66.74562554174229
siam score:  -0.72333014
maxi score, test score, baseline:  -0.9948110595514308 -0.9403333333333332 -0.9403333333333332
maxi score, test score, baseline:  -0.9948110595514308 -0.9403333333333332 -0.9403333333333332
probs:  [0.060954174126888444, 0.060954174126888444, 0.059563880441534746, 0.4111371050363947, 0.058882016659546, 0.3485086496087477]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9948110595514308 -0.9403333333333332 -0.9403333333333332
probs:  [0.060954174126888444, 0.060954174126888444, 0.059563880441534746, 0.4111371050363947, 0.058882016659546, 0.3485086496087477]
printing an ep nov before normalisation:  22.999796867370605
maxi score, test score, baseline:  -0.9948110595514308 -0.9403333333333332 -0.9403333333333332
probs:  [0.061070280947641935, 0.061070280947641935, 0.05967733320005006, 0.4119217089546924, 0.05899416774422472, 0.34726622820574893]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.20796742792062
actor:  0 policy actor:  1  step number:  53 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.991582098765432 -0.9403333333333332 -0.9403333333333332
probs:  [0.061070280947641935, 0.061070280947641935, 0.05967733320005006, 0.4119217089546924, 0.05899416774422472, 0.34726622820574893]
maxi score, test score, baseline:  -0.991582098765432 -0.9403333333333332 -0.9403333333333332
probs:  [0.06102819628762487, 0.06102819628762487, 0.059632619335095456, 0.41159848199637467, 0.05894816439659378, 0.34776434169668635]
maxi score, test score, baseline:  -0.991582098765432 -0.9403333333333332 -0.9403333333333332
probs:  [0.06114376824128066, 0.06114376824128066, 0.05974554263721056, 0.412379397789135, 0.05905978867852652, 0.3465277344125666]
siam score:  -0.7203617
maxi score, test score, baseline:  -0.991582098765432 -0.9403333333333332 -0.9403333333333332
probs:  [0.06114376824128066, 0.06114376824128066, 0.05974554263721056, 0.412379397789135, 0.05905978867852652, 0.3465277344125666]
printing an ep nov before normalisation:  44.51977725504927
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.53 ]
 [0.478]
 [0.478]
 [0.491]
 [0.469]
 [0.496]] [[35.59 ]
 [38.803]
 [39.345]
 [39.695]
 [41.241]
 [40.814]
 [38.495]] [[1.211]
 [1.424]
 [1.396]
 [1.413]
 [1.498]
 [1.456]
 [1.375]]
maxi score, test score, baseline:  -0.9916013086989992 -0.9403333333333332 -0.9403333333333332
maxi score, test score, baseline:  -0.9916013086989992 -0.9403333333333332 -0.9403333333333332
probs:  [0.06148650250309968, 0.06148650250309968, 0.060080422193784094, 0.4146952416362646, 0.05939081592743185, 0.34286051523632016]
printing an ep nov before normalisation:  33.592736248659534
maxi score, test score, baseline:  -0.9916013086989992 -0.9403333333333332 -0.9403333333333332
probs:  [0.061444984192596516, 0.061444984192596516, 0.0600362357993489, 0.4143755614039541, 0.05934532098227842, 0.3433529134292254]
printing an ep nov before normalisation:  33.38596690897688
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.9916013086989992 -0.9403333333333332 -0.9403333333333332
maxi score, test score, baseline:  -0.9916013086989992 -0.9403333333333332 -0.9403333333333332
probs:  [0.061444984192596516, 0.061444984192596516, 0.0600362357993489, 0.4143755614039541, 0.05934532098227842, 0.3433529134292254]
maxi score, test score, baseline:  -0.9916013086989992 -0.9403333333333332 -0.9403333333333332
probs:  [0.06140347844671398, 0.06140347844671398, 0.05999206277697239, 0.41405597791596643, 0.05929983980518826, 0.343845162608445]
maxi score, test score, baseline:  -0.9916013086989992 -0.9403333333333332 -0.9403333333333332
probs:  [0.06140347844671398, 0.06140347844671398, 0.05999206277697239, 0.41405597791596643, 0.05929983980518826, 0.343845162608445]
maxi score, test score, baseline:  -0.9916013086989992 -0.9403333333333332 -0.9403333333333332
probs:  [0.060736296077069984, 0.061447062098812684, 0.06003464244022143, 0.41435041550490176, 0.05934192706626266, 0.3440896568127315]
actions average: 
K:  0  action  0 :  tensor([0.5814, 0.0040, 0.0653, 0.0877, 0.0910, 0.0962, 0.0745],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0120, 0.9011, 0.0214, 0.0162, 0.0075, 0.0096, 0.0322],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1369, 0.0042, 0.1620, 0.1853, 0.1496, 0.2282, 0.1339],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1212, 0.0032, 0.1150, 0.2706, 0.1702, 0.1896, 0.1302],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1511, 0.0056, 0.1052, 0.1507, 0.3110, 0.1533, 0.1231],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.2367, 0.0087, 0.1122, 0.2171, 0.1436, 0.1638, 0.1179],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2187, 0.0433, 0.0916, 0.1437, 0.1338, 0.1146, 0.2542],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  28.395776748657227
maxi score, test score, baseline:  -0.9916013086989992 -0.9403333333333332 -0.9403333333333332
probs:  [0.060736296077069984, 0.061447062098812684, 0.06003464244022143, 0.41435041550490176, 0.05934192706626266, 0.3440896568127315]
maxi score, test score, baseline:  -0.9916013086989992 -0.9403333333333332 -0.9403333333333332
printing an ep nov before normalisation:  35.3347888893827
siam score:  -0.7294526
maxi score, test score, baseline:  -0.9916013086989992 -0.9403333333333332 -0.9403333333333332
probs:  [0.06069351196868903, 0.06140562173813118, 0.05999053181167566, 0.4140312576783676, 0.05929650681589811, 0.34458256998723846]
printing an ep nov before normalisation:  61.07903833841496
Printing some Q and Qe and total Qs values:  [[0.733]
 [0.733]
 [0.733]
 [0.733]
 [0.715]
 [0.715]
 [0.733]] [[26.854]
 [26.854]
 [26.854]
 [26.854]
 [32.099]
 [26.295]
 [26.854]] [[0.733]
 [0.733]
 [0.733]
 [0.733]
 [0.715]
 [0.715]
 [0.733]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.765771194147945
printing an ep nov before normalisation:  55.81963464551844
maxi score, test score, baseline:  -0.9916204301075269 -0.9403333333333332 -0.9403333333333332
probs:  [0.06124111158162067, 0.06195030593038053, 0.060541009468101356, 0.4131322710824243, 0.05916739139727229, 0.3439679105402009]
maxi score, test score, baseline:  -0.9916204301075269 -0.9403333333333332 -0.9403333333333332
probs:  [0.06124111158162067, 0.06195030593038053, 0.060541009468101356, 0.4131322710824243, 0.05916739139727229, 0.3439679105402009]
maxi score, test score, baseline:  -0.9916204301075269 -0.9403333333333332 -0.9403333333333332
probs:  [0.06124111158162067, 0.06195030593038053, 0.060541009468101356, 0.4131322710824243, 0.05916739139727229, 0.3439679105402009]
maxi score, test score, baseline:  -0.9916204301075269 -0.9403333333333332 -0.9403333333333332
probs:  [0.060582454678819134, 0.061996371208922506, 0.059889005170424475, 0.4133893270628348, 0.059204333503908226, 0.3449385083750908]
maxi score, test score, baseline:  -0.9916204301075269 -0.9403333333333332 -0.9403333333333332
probs:  [0.060582454678819134, 0.061996371208922506, 0.059889005170424475, 0.4133893270628348, 0.059204333503908226, 0.3449385083750908]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9916394636015325 -0.9403333333333332 -0.9403333333333332
maxi score, test score, baseline:  -0.9916394636015325 -0.9403333333333332 -0.9403333333333332
probs:  [0.0606090824764766, 0.06203094351053777, 0.0599117366190071, 0.41351865772670665, 0.05922321792429038, 0.3447063617429814]
maxi score, test score, baseline:  -0.9916394636015325 -0.9403333333333332 -0.9403333333333332
probs:  [0.0606090824764766, 0.06203094351053777, 0.0599117366190071, 0.41351865772670665, 0.05922321792429038, 0.3447063617429814]
printing an ep nov before normalisation:  53.70411794560014
maxi score, test score, baseline:  -0.9916394636015325 -0.9403333333333332 -0.9403333333333332
maxi score, test score, baseline:  -0.9916394636015325 -0.9403333333333332 -0.9403333333333332
probs:  [0.0606090824764766, 0.06203094351053777, 0.0599117366190071, 0.41351865772670665, 0.05922321792429038, 0.3447063617429814]
maxi score, test score, baseline:  -0.9916394636015325 -0.9403333333333332 -0.9403333333333332
probs:  [0.0606090824764766, 0.06203094351053777, 0.0599117366190071, 0.41351865772670665, 0.05922321792429038, 0.3447063617429814]
using explorer policy with actor:  0
siam score:  -0.7171942
maxi score, test score, baseline:  -0.9916394636015325 -0.9403333333333332 -0.9403333333333332
probs:  [0.0606090824764766, 0.06203094351053777, 0.0599117366190071, 0.41351865772670665, 0.05922321792429038, 0.3447063617429814]
printing an ep nov before normalisation:  36.09901761908807
Printing some Q and Qe and total Qs values:  [[-0.003]
 [ 0.01 ]
 [-0.007]
 [-0.008]
 [-0.009]
 [-0.009]
 [-0.008]] [[38.768]
 [49.536]
 [33.942]
 [34.135]
 [34.195]
 [34.645]
 [35.845]] [[0.344]
 [0.521]
 [0.266]
 [0.268]
 [0.269]
 [0.275]
 [0.294]]
printing an ep nov before normalisation:  57.07219205482144
maxi score, test score, baseline:  -0.9916584097859327 -0.9403333333333332 -0.9403333333333332
probs:  [0.0606090824764766, 0.06203094351053777, 0.0599117366190071, 0.41351865772670665, 0.05922321792429038, 0.3447063617429814]
maxi score, test score, baseline:  -0.9916584097859327 -0.9403333333333332 -0.9403333333333332
probs:  [0.06056646890117633, 0.06199099237151458, 0.05986781726285757, 0.4132009915273108, 0.05917800931616311, 0.34519572062097753]
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.42 ]
 [0.42 ]
 [0.429]
 [0.447]
 [0.351]
 [0.42 ]] [[47.61 ]
 [47.61 ]
 [47.61 ]
 [43.151]
 [62.002]
 [42.738]
 [47.61 ]] [[1.055]
 [1.055]
 [1.055]
 [0.963]
 [1.411]
 [0.875]
 [1.055]]
printing an ep nov before normalisation:  70.57478593964206
printing an ep nov before normalisation:  32.2345495223999
maxi score, test score, baseline:  -0.9916584097859327 -0.9403333333333332 -0.9403333333333332
probs:  [0.06052386823445345, 0.061951053334559, 0.05982391121083482, 0.41288342155590546, 0.059132814402705, 0.34568493126154226]
maxi score, test score, baseline:  -0.9916584097859327 -0.9403333333333332 -0.9403333333333332
probs:  [0.06056730649415676, 0.061276805188342255, 0.0598668459998972, 0.41318030779049997, 0.05917525209417256, 0.34593348243293126]
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.39619419337965
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]] [[61.002]
 [61.002]
 [61.002]
 [61.002]
 [61.002]
 [61.002]
 [61.002]] [[2.105]
 [2.105]
 [2.105]
 [2.105]
 [2.105]
 [2.105]
 [2.105]]
printing an ep nov before normalisation:  61.80033766845006
printing an ep nov before normalisation:  39.72841831972085
maxi score, test score, baseline:  -0.9916584097859327 -0.9403333333333332 -0.9403333333333332
printing an ep nov before normalisation:  35.8996801500741
maxi score, test score, baseline:  -0.9916584097859327 -0.9403333333333332 -0.9403333333333332
probs:  [0.06103087045554788, 0.061740099791123354, 0.06033067588857846, 0.4116529365146289, 0.05895670919112903, 0.3462887081589924]
printing an ep nov before normalisation:  51.86960580896084
printing an ep nov before normalisation:  19.487654409532524
maxi score, test score, baseline:  -0.9916584097859327 -0.9403333333333332 -0.9403333333333332
probs:  [0.06107412610596497, 0.06107412610596497, 0.06037343419017266, 0.41194523933001964, 0.05899849156295755, 0.34653458270492016]
maxi score, test score, baseline:  -0.9916584097859327 -0.9403333333333332 -0.9403333333333332
probs:  [0.06107412610596497, 0.06107412610596497, 0.06037343419017266, 0.41194523933001964, 0.05899849156295755, 0.34653458270492016]
printing an ep nov before normalisation:  1.5043114645123978
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.83963341388235
maxi score, test score, baseline:  -0.9916584097859327 -0.9403333333333332 -0.9403333333333332
probs:  [0.06165467641066413, 0.060956320979303556, 0.060956320979303556, 0.4113557962679895, 0.058913631342573915, 0.3461632540201653]
maxi score, test score, baseline:  -0.9916584097859327 -0.9403333333333332 -0.9403333333333332
probs:  [0.06165467641066413, 0.060956320979303556, 0.060956320979303556, 0.4113557962679895, 0.058913631342573915, 0.3461632540201653]
maxi score, test score, baseline:  -0.9916584097859327 -0.9403333333333332 -0.9403333333333332
probs:  [0.061614613716835805, 0.06091496968794574, 0.06091496968794574, 0.41103877267360356, 0.0588685109034423, 0.3466481633302268]
printing an ep nov before normalisation:  47.065700935434485
printing an ep nov before normalisation:  47.65869589369183
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.289]
 [0.303]
 [0.287]
 [0.286]
 [0.287]
 [0.287]
 [0.287]] [[48.533]
 [45.779]
 [45.642]
 [45.067]
 [45.642]
 [45.642]
 [45.642]] [[0.289]
 [0.303]
 [0.287]
 [0.286]
 [0.287]
 [0.287]
 [0.287]]
printing an ep nov before normalisation:  0.010534028092479275
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9916960426179604 -0.9403333333333332 -0.9403333333333332
maxi score, test score, baseline:  -0.9916960426179604 -0.9403333333333332 -0.9403333333333332
probs:  [0.06008619976638035, 0.05933206607119615, 0.05933206607119615, 0.398793647176897, 0.057126225012782354, 0.3653297959015479]
line 256 mcts: sample exp_bonus 0.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.65469168736627
line 256 mcts: sample exp_bonus 19.245109418393554
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.502]
 [0.51 ]
 [0.508]
 [0.507]
 [0.503]
 [0.506]] [[23.506]
 [40.375]
 [26.901]
 [24.944]
 [25.693]
 [25.558]
 [29.101]] [[0.505]
 [0.502]
 [0.51 ]
 [0.508]
 [0.507]
 [0.503]
 [0.506]]
siam score:  -0.7260974
maxi score, test score, baseline:  -0.9916960426179604 -0.9403333333333332 -0.9403333333333332
probs:  [0.06008619976638035, 0.05933206607119615, 0.05933206607119615, 0.398793647176897, 0.057126225012782354, 0.3653297959015479]
maxi score, test score, baseline:  -0.9916960426179604 -0.9403333333333332 -0.9403333333333332
probs:  [0.06066168245107272, 0.05991077876920073, 0.05991077876920073, 0.3979184195020554, 0.05700044400219372, 0.3645978965062767]
printing an ep nov before normalisation:  14.417414665222168
printing an ep nov before normalisation:  38.02943265593253
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  48.70934866707016
printing an ep nov before normalisation:  42.679806891661244
printing an ep nov before normalisation:  64.13100479743316
siam score:  -0.72276443
printing an ep nov before normalisation:  42.99419071475359
maxi score, test score, baseline:  -0.9917147304479877 -0.9403333333333332 -0.9403333333333332
maxi score, test score, baseline:  -0.9917147304479877 -0.9403333333333332 -0.9403333333333332
probs:  [0.06078076757994595, 0.06002838655224224, 0.06002838655224224, 0.39870103158748443, 0.0571123259231297, 0.3633491018049554]
maxi score, test score, baseline:  -0.9917147304479877 -0.9403333333333332 -0.9403333333333332
probs:  [0.06078076757994595, 0.06002838655224224, 0.06002838655224224, 0.39870103158748443, 0.0571123259231297, 0.3633491018049554]
maxi score, test score, baseline:  -0.9917147304479877 -0.9403333333333332 -0.9403333333333332
probs:  [0.06078076757994595, 0.06002838655224224, 0.06002838655224224, 0.39870103158748443, 0.0571123259231297, 0.3633491018049554]
siam score:  -0.71645075
maxi score, test score, baseline:  -0.9917147304479877 -0.9403333333333332 -0.9403333333333332
probs:  [0.06078076757994595, 0.06002838655224224, 0.06002838655224224, 0.39870103158748443, 0.0571123259231297, 0.3633491018049554]
maxi score, test score, baseline:  -0.9917147304479877 -0.9403333333333332 -0.9403333333333332
maxi score, test score, baseline:  -0.9917147304479877 -0.9403333333333332 -0.9403333333333332
probs:  [0.06078076757994595, 0.06002838655224224, 0.06002838655224224, 0.39870103158748443, 0.0571123259231297, 0.3633491018049554]
printing an ep nov before normalisation:  80.4897111675496
maxi score, test score, baseline:  -0.9917147304479877 -0.9403333333333332 -0.9403333333333332
probs:  [0.06078076757994595, 0.06002838655224224, 0.06002838655224224, 0.39870103158748443, 0.0571123259231297, 0.3633491018049554]
printing an ep nov before normalisation:  41.596791183385875
maxi score, test score, baseline:  -0.9917147304479877 -0.9403333333333332 -0.9403333333333332
probs:  [0.06078076757994595, 0.06002838655224224, 0.06002838655224224, 0.39870103158748443, 0.0571123259231297, 0.3633491018049554]
printing an ep nov before normalisation:  44.14172056197539
printing an ep nov before normalisation:  45.56684990670556
maxi score, test score, baseline:  -0.9917147304479877 -0.9403333333333332 -0.9403333333333332
probs:  [0.06078076757994595, 0.06002838655224224, 0.06002838655224224, 0.39870103158748443, 0.0571123259231297, 0.3633491018049554]
maxi score, test score, baseline:  -0.9917147304479877 -0.9403333333333332 -0.9403333333333332
probs:  [0.06078076757994595, 0.06002838655224224, 0.06002838655224224, 0.39870103158748443, 0.0571123259231297, 0.3633491018049554]
maxi score, test score, baseline:  -0.9917147304479877 -0.9403333333333332 -0.9403333333333332
probs:  [0.06078076757994595, 0.06002838655224224, 0.06002838655224224, 0.39870103158748443, 0.0571123259231297, 0.3633491018049554]
maxi score, test score, baseline:  -0.9917147304479877 -0.9403333333333332 -0.9403333333333332
probs:  [0.06082588963138477, 0.06007294882870965, 0.05932953892227089, 0.3989975678813903, 0.05715471863697495, 0.3636193360992694]
maxi score, test score, baseline:  -0.9917147304479877 -0.9403333333333332 -0.9403333333333332
probs:  [0.06082588963138477, 0.06007294882870965, 0.05932953892227089, 0.3989975678813903, 0.05715471863697495, 0.3636193360992694]
actor:  1 policy actor:  1  step number:  93 total reward:  0.039999999999999813  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.9  ]
 [0.621]
 [0.621]
 [0.621]
 [0.675]
 [0.66 ]] [[32.151]
 [46.02 ]
 [32.151]
 [32.151]
 [32.151]
 [47.711]
 [47.554]] [[1.654]
 [2.395]
 [1.654]
 [1.654]
 [1.654]
 [2.226]
 [2.205]]
maxi score, test score, baseline:  -0.9917147304479877 -0.9403333333333332 -0.9403333333333332
probs:  [0.06113343138666914, 0.060476634052084685, 0.05982815060781139, 0.356123762787788, 0.057931034444751055, 0.4045069867208957]
printing an ep nov before normalisation:  34.4388327172708
maxi score, test score, baseline:  -0.9917147304479877 -0.9403333333333332 -0.9403333333333332
probs:  [0.06094970730712117, 0.0602874825321951, 0.05963364034935669, 0.35837768932087916, 0.057720847379686566, 0.4030306331107613]
maxi score, test score, baseline:  -0.9917147304479877 -0.9403333333333332 -0.9403333333333332
probs:  [0.06094970730712117, 0.0602874825321951, 0.05963364034935669, 0.35837768932087916, 0.057720847379686566, 0.4030306331107613]
printing an ep nov before normalisation:  26.27598370402639
maxi score, test score, baseline:  -0.9917147304479877 -0.9403333333333332 -0.9403333333333332
probs:  [0.06094970730712117, 0.0602874825321951, 0.05963364034935669, 0.35837768932087916, 0.057720847379686566, 0.4030306331107613]
maxi score, test score, baseline:  -0.9917147304479877 -0.9403333333333332 -0.9403333333333332
maxi score, test score, baseline:  -0.9917147304479877 -0.9403333333333332 -0.9403333333333332
probs:  [0.06094970730712117, 0.0602874825321951, 0.05963364034935669, 0.35837768932087916, 0.057720847379686566, 0.4030306331107613]
Printing some Q and Qe and total Qs values:  [[ 0.046]
 [ 0.172]
 [ 0.038]
 [ 0.042]
 [ 0.055]
 [-0.   ]
 [ 0.002]] [[21.788]
 [27.942]
 [21.018]
 [20.807]
 [21.024]
 [20.977]
 [21.28 ]] [[0.644]
 [1.111]
 [0.592]
 [0.585]
 [0.61 ]
 [0.552]
 [0.571]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9917147304479877 -0.9403333333333332 -0.9403333333333332
maxi score, test score, baseline:  -0.9917147304479877 -0.9403333333333332 -0.9403333333333332
probs:  [0.06085798115115453, 0.06019304667023621, 0.059536529081481375, 0.3595029855652274, 0.057615909303198654, 0.40229354822870184]
Printing some Q and Qe and total Qs values:  [[0.29]
 [0.29]
 [0.29]
 [0.29]
 [0.29]
 [0.29]
 [0.29]] [[38.132]
 [38.132]
 [38.132]
 [38.132]
 [38.132]
 [38.132]
 [38.132]] [[0.29]
 [0.29]
 [0.29]
 [0.29]
 [0.29]
 [0.29]
 [0.29]]
printing an ep nov before normalisation:  6.041839114786569
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
printing an ep nov before normalisation:  45.578089436732995
maxi score, test score, baseline:  -0.9917333333333332 -0.9403333333333332 -0.9403333333333332
maxi score, test score, baseline:  -0.9917333333333332 -0.9403333333333332 -0.9403333333333332
probs:  [0.06085798115115453, 0.06019304667023621, 0.059536529081481375, 0.3595029855652274, 0.057615909303198654, 0.40229354822870184]
maxi score, test score, baseline:  -0.9917333333333332 -0.9403333333333332 -0.9403333333333332
maxi score, test score, baseline:  -0.9917333333333332 -0.9403333333333332 -0.9403333333333332
probs:  [0.06023304107903014, 0.06023304107903014, 0.05957608630893078, 0.35974229332641805, 0.05765418757155936, 0.4025613506350315]
maxi score, test score, baseline:  -0.9917333333333332 -0.9403333333333332 -0.9403333333333332
probs:  [0.06023304107903014, 0.06023304107903014, 0.05957608630893078, 0.35974229332641805, 0.05765418757155936, 0.4025613506350315]
Printing some Q and Qe and total Qs values:  [[-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [ 1.13 ]
 [-0.009]] [[48.665]
 [48.665]
 [48.665]
 [48.665]
 [48.665]
 [ 0.   ]
 [48.665]] [[0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [1.13 ]
 [0.658]]
maxi score, test score, baseline:  -0.9917333333333332 -0.9403333333333332 -0.9403333333333332
probs:  [0.060178437525182565, 0.0595183706691652, 0.0595183706691652, 0.3611065066662906, 0.05758736763075419, 0.4020909468394423]
maxi score, test score, baseline:  -0.9917333333333332 -0.9403333333333332 -0.9403333333333332
probs:  [0.06008438526274294, 0.05942164115110276, 0.05942164115110276, 0.3622330298013706, 0.057482805892826214, 0.40135649674085483]
maxi score, test score, baseline:  -0.9917333333333332 -0.9403333333333332 -0.9403333333333332
probs:  [0.06008438526274294, 0.05942164115110276, 0.05942164115110276, 0.3622330298013706, 0.057482805892826214, 0.40135649674085483]
printing an ep nov before normalisation:  34.54253334343531
maxi score, test score, baseline:  -0.9917518518518518 -0.9403333333333332 -0.9403333333333332
actions average: 
K:  1  action  0 :  tensor([0.3762, 0.0230, 0.0799, 0.1246, 0.1381, 0.1385, 0.1197],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0155, 0.9268, 0.0095, 0.0113, 0.0095, 0.0076, 0.0199],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1364, 0.0165, 0.3059, 0.1331, 0.1290, 0.1513, 0.1278],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1786, 0.0783, 0.0857, 0.2683, 0.1393, 0.1218, 0.1280],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1269, 0.0451, 0.0861, 0.1508, 0.3571, 0.1316, 0.1025],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1514, 0.0153, 0.1030, 0.1620, 0.1328, 0.3309, 0.1046],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1406, 0.0296, 0.1223, 0.1466, 0.1181, 0.1737, 0.2691],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.419]] [[41.687]
 [41.687]
 [41.687]
 [41.687]
 [41.687]
 [41.687]
 [44.48 ]] [[2.147]
 [2.147]
 [2.147]
 [2.147]
 [2.147]
 [2.147]
 [2.316]]
maxi score, test score, baseline:  -0.9917518518518518 -0.9403333333333332 -0.9403333333333332
probs:  [0.05936445093954717, 0.05936445093954717, 0.05936445093954717, 0.36360050721357695, 0.057416493845290205, 0.4008896461224913]
line 256 mcts: sample exp_bonus 37.50062626847067
maxi score, test score, baseline:  -0.9917518518518518 -0.9403333333333332 -0.9403333333333332
probs:  [0.05936445093954717, 0.05936445093954717, 0.05936445093954717, 0.36360050721357695, 0.057416493845290205, 0.4008896461224913]
siam score:  -0.71216726
printing an ep nov before normalisation:  40.48274775065342
printing an ep nov before normalisation:  35.318638174134364
printing an ep nov before normalisation:  30.19711071003731
using another actor
maxi score, test score, baseline:  -0.9917518518518518 -0.9403333333333332 -0.9403333333333332
probs:  [0.0592680036077062, 0.0592680036077062, 0.0592680036077062, 0.3647265625250156, 0.05731221909573322, 0.40015720755613254]
maxi score, test score, baseline:  -0.9917702865761688 -0.9403333333333332 -0.9403333333333332
maxi score, test score, baseline:  -0.9917702865761688 -0.9403333333333332 -0.9403333333333332
probs:  [0.0592680036077062, 0.0592680036077062, 0.0592680036077062, 0.3647265625250156, 0.05731221909573322, 0.40015720755613254]
maxi score, test score, baseline:  -0.9917702865761688 -0.9403333333333332 -0.9403333333333332
Printing some Q and Qe and total Qs values:  [[0.664]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]] [[28.037]
 [30.965]
 [30.965]
 [30.965]
 [30.965]
 [30.965]
 [30.965]] [[0.664]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]]
printing an ep nov before normalisation:  42.33054212273773
printing an ep nov before normalisation:  60.96559669360045
maxi score, test score, baseline:  -0.9917702865761688 -0.9403333333333332 -0.9403333333333332
printing an ep nov before normalisation:  36.323708237690695
printing an ep nov before normalisation:  63.85007363200541
printing an ep nov before normalisation:  58.94689142279938
maxi score, test score, baseline:  -0.9917702865761688 -0.9403333333333332 -0.9403333333333332
probs:  [0.0592680036077062, 0.0592680036077062, 0.0592680036077062, 0.3647265625250156, 0.05731221909573322, 0.40015720755613254]
printing an ep nov before normalisation:  36.37899708931846
printing an ep nov before normalisation:  38.63790679494848
actor:  1 policy actor:  1  step number:  64 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9917702865761688 -0.9403333333333332 -0.9403333333333332
probs:  [0.06347208939260474, 0.06347208939260474, 0.06347208939260474, 0.3156424350798535, 0.061857497684249606, 0.4320837990580827]
maxi score, test score, baseline:  -0.9917702865761688 -0.9403333333333332 -0.9403333333333332
probs:  [0.06347208939260474, 0.06347208939260474, 0.06347208939260474, 0.3156424350798535, 0.061857497684249606, 0.4320837990580827]
Printing some Q and Qe and total Qs values:  [[0.369]
 [0.369]
 [0.369]
 [0.394]
 [0.422]
 [0.396]
 [0.369]] [[48.741]
 [48.741]
 [48.741]
 [44.639]
 [49.474]
 [45.715]
 [48.741]] [[1.498]
 [1.498]
 [1.498]
 [1.356]
 [1.581]
 [1.401]
 [1.498]]
maxi score, test score, baseline:  -0.9917886380737396 -0.9403333333333332 -0.9403333333333332
probs:  [0.06347208939260474, 0.06347208939260474, 0.06347208939260474, 0.3156424350798535, 0.061857497684249606, 0.4320837990580827]
maxi score, test score, baseline:  -0.9917886380737396 -0.9403333333333332 -0.9403333333333332
probs:  [0.06347208939260474, 0.06347208939260474, 0.06347208939260474, 0.3156424350798535, 0.061857497684249606, 0.4320837990580827]
maxi score, test score, baseline:  -0.9917886380737396 -0.9403333333333332 -0.9403333333333332
probs:  [0.06347208939260474, 0.06347208939260474, 0.06347208939260474, 0.3156424350798535, 0.061857497684249606, 0.4320837990580827]
actor:  1 policy actor:  1  step number:  75 total reward:  0.09333333333333282  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9917886380737396 -0.9403333333333332 -0.9403333333333332
probs:  [0.06144197039559397, 0.06144197039559397, 0.06144197039559397, 0.337555916071239, 0.059879129116010274, 0.41823904362596875]
printing an ep nov before normalisation:  37.026751236568856
printing an ep nov before normalisation:  35.08329886428529
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.445]
 [0.445]
 [0.46 ]
 [0.392]
 [0.451]
 [0.445]] [[60.977]
 [60.977]
 [60.977]
 [56.048]
 [57.707]
 [51.342]
 [60.977]] [[2.361]
 [2.361]
 [2.361]
 [2.094]
 [2.121]
 [1.816]
 [2.361]]
maxi score, test score, baseline:  -0.9917886380737396 -0.9403333333333332 -0.9403333333333332
probs:  [0.06184615436788123, 0.06184615436788123, 0.06184615436788123, 0.33712103502535623, 0.05978152208934932, 0.4175589797816507]
maxi score, test score, baseline:  -0.9917886380737396 -0.9403333333333332 -0.9403333333333332
probs:  [0.06184615436788123, 0.06184615436788123, 0.06184615436788123, 0.33712103502535623, 0.05978152208934932, 0.4175589797816507]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9917886380737396 -0.9403333333333332 -0.9403333333333332
probs:  [0.06184615436788123, 0.06184615436788123, 0.06184615436788123, 0.33712103502535623, 0.05978152208934932, 0.4175589797816507]
printing an ep nov before normalisation:  50.179163419321185
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.515]
 [0.498]
 [0.499]
 [0.493]
 [0.488]
 [0.499]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.509]
 [0.515]
 [0.498]
 [0.499]
 [0.493]
 [0.488]
 [0.499]]
maxi score, test score, baseline:  -0.9917886380737396 -0.9403333333333332 -0.9403333333333332
probs:  [0.061266092033572486, 0.0617943306370831, 0.0617943306370831, 0.3382949493440763, 0.05972050500848592, 0.41712979233969916]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  -0.9917886380737396 -0.9403333333333332 -0.9403333333333332
maxi score, test score, baseline:  -0.9917886380737396 -0.9403333333333332 -0.9403333333333332
probs:  [0.06109359806072292, 0.061625959544827996, 0.061625959544827996, 0.3402846542191111, 0.05953594779241544, 0.41583388083809464]
siam score:  -0.72150284
printing an ep nov before normalisation:  35.63683480713106
printing an ep nov before normalisation:  0.0
Printing some Q and Qe and total Qs values:  [[0.74 ]
 [0.775]
 [0.751]
 [0.751]
 [0.739]
 [0.75 ]
 [0.743]] [[27.361]
 [35.917]
 [32.867]
 [35.49 ]
 [35.532]
 [36.747]
 [32.928]] [[0.74 ]
 [0.775]
 [0.751]
 [0.751]
 [0.739]
 [0.75 ]
 [0.743]]
maxi score, test score, baseline:  -0.9917886380737396 -0.9403333333333332 -0.9403333333333332
probs:  [0.0610074899927202, 0.06154190959675106, 0.06154190959675106, 0.34127790424318144, 0.059443817817963235, 0.41518696875263306]
maxi score, test score, baseline:  -0.9917886380737396 -0.9403333333333332 -0.9403333333333332
probs:  [0.0610074899927202, 0.06154190959675106, 0.06154190959675106, 0.34127790424318144, 0.059443817817963235, 0.41518696875263306]
siam score:  -0.728432
printing an ep nov before normalisation:  29.11656141281128
Printing some Q and Qe and total Qs values:  [[0.711]
 [0.721]
 [0.711]
 [0.711]
 [0.711]
 [0.711]
 [0.711]] [[30.163]
 [25.522]
 [30.163]
 [30.163]
 [30.163]
 [30.163]
 [30.163]] [[0.711]
 [0.721]
 [0.711]
 [0.711]
 [0.711]
 [0.711]
 [0.711]]
maxi score, test score, baseline:  -0.9917886380737396 -0.9403333333333332 -0.9403333333333332
probs:  [0.0610074899927202, 0.06154190959675106, 0.06154190959675106, 0.34127790424318144, 0.059443817817963235, 0.41518696875263306]
maxi score, test score, baseline:  -0.9917886380737396 -0.9403333333333332 -0.9403333333333332
probs:  [0.060921474338210865, 0.061457949853337863, 0.061457949853337863, 0.3422700882846248, 0.059351786719876315, 0.4145407509506123]
maxi score, test score, baseline:  -0.9917886380737396 -0.9403333333333332 -0.9403333333333332
probs:  [0.060921474338210865, 0.061457949853337863, 0.061457949853337863, 0.3422700882846248, 0.059351786719876315, 0.4145407509506123]
printing an ep nov before normalisation:  52.33918392936471
maxi score, test score, baseline:  -0.9918069069069069 -0.9403333333333332 -0.9403333333333332
siam score:  -0.7259526
maxi score, test score, baseline:  -0.9918069069069069 -0.9403333333333332 -0.9403333333333332
probs:  [0.060749719675226514, 0.06129030040026509, 0.06129030040026509, 0.3442512652765185, 0.05916802051678027, 0.41325039373094447]
Printing some Q and Qe and total Qs values:  [[0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.537]
 [0.544]] [[41.981]
 [41.981]
 [41.981]
 [41.981]
 [41.981]
 [49.095]
 [50.709]] [[1.637]
 [1.637]
 [1.637]
 [1.637]
 [1.637]
 [2.07 ]
 [2.179]]
maxi score, test score, baseline:  -0.9918069069069069 -0.9403333333333332 -0.9403333333333332
probs:  [0.060749719675226514, 0.06129030040026509, 0.06129030040026509, 0.3442512652765185, 0.05916802051678027, 0.41325039373094447]
maxi score, test score, baseline:  -0.9918069069069069 -0.9403333333333332 -0.9403333333333332
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.35716329254243
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.561]
 [0.542]
 [0.542]] [[36.563]
 [36.563]
 [36.563]
 [36.563]
 [40.928]
 [36.563]
 [36.563]] [[2.126]
 [2.126]
 [2.126]
 [2.126]
 [2.498]
 [2.126]
 [2.126]]
siam score:  -0.72789407
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9918069069069069 -0.9403333333333332 -0.9403333333333332
probs:  [0.06078253174917122, 0.06132340509382316, 0.06078253174917122, 0.3444375384503899, 0.05919997640741179, 0.41347401655003263]
printing an ep nov before normalisation:  35.22928290169869
Printing some Q and Qe and total Qs values:  [[0.65 ]
 [0.649]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.672]] [[38.782]
 [38.575]
 [38.782]
 [38.782]
 [38.782]
 [38.782]
 [34.269]] [[2.231]
 [2.214]
 [2.231]
 [2.231]
 [2.231]
 [2.231]
 [1.897]]
from probs:  [0.06078253174917122, 0.06132340509382316, 0.06078253174917122, 0.3444375384503899, 0.05919997640741179, 0.41347401655003263]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  40.33804299066948
maxi score, test score, baseline:  -0.9918250936329588 -0.9403333333333332 -0.9403333333333332
probs:  [0.06078253174917122, 0.06132340509382316, 0.06078253174917122, 0.3444375384503899, 0.05919997640741179, 0.41347401655003263]
actions average: 
K:  1  action  0 :  tensor([0.5432, 0.0338, 0.0664, 0.0635, 0.1261, 0.0679, 0.0992],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0253, 0.8591, 0.0181, 0.0226, 0.0169, 0.0227, 0.0353],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2642, 0.0117, 0.1475, 0.1404, 0.0916, 0.2040, 0.1406],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1320, 0.0926, 0.1020, 0.2270, 0.1382, 0.1484, 0.1599],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1828, 0.0077, 0.0663, 0.1295, 0.3949, 0.1141, 0.1046],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0952, 0.0234, 0.1087, 0.1302, 0.1115, 0.3964, 0.1346],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1238, 0.1599, 0.1052, 0.1171, 0.0627, 0.1232, 0.3081],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  44.17202852649526
printing an ep nov before normalisation:  33.2009301574901
maxi score, test score, baseline:  -0.9918250936329588 -0.9403333333333332 -0.9403333333333332
probs:  [0.06069687031302549, 0.061239795186991616, 0.06069687031302549, 0.3454277787334086, 0.05910831234845791, 0.4128303731050909]
maxi score, test score, baseline:  -0.9918250936329588 -0.9403333333333332 -0.9403333333333332
probs:  [0.06069687031302549, 0.061239795186991616, 0.06069687031302549, 0.3454277787334086, 0.05910831234845791, 0.4128303731050909]
printing an ep nov before normalisation:  48.24141183006789
maxi score, test score, baseline:  -0.9918250936329588 -0.9403333333333332 -0.9403333333333332
printing an ep nov before normalisation:  36.81683409222457
printing an ep nov before normalisation:  27.25109100341797
maxi score, test score, baseline:  -0.9918250936329588 -0.9403333333333332 -0.9403333333333332
probs:  [0.060192958083280075, 0.06127260057106002, 0.06072938422500721, 0.34561315193770686, 0.059139973434704576, 0.4130519317482413]
from probs:  [0.060192958083280075, 0.06127260057106002, 0.06072938422500721, 0.34561315193770686, 0.059139973434704576, 0.4130519317482413]
siam score:  -0.72753096
printing an ep nov before normalisation:  0.0
using explorer policy with actor:  1
siam score:  -0.72446895
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.137525376535386
printing an ep nov before normalisation:  43.31664741039276
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.596]
 [0.484]
 [0.484]
 [0.485]
 [0.484]
 [0.484]] [[43.713]
 [54.293]
 [43.713]
 [43.713]
 [63.342]
 [43.713]
 [43.713]] [[1.183]
 [1.556]
 [1.183]
 [1.183]
 [1.67 ]
 [1.183]
 [1.183]]
maxi score, test score, baseline:  -0.9918431988041853 -0.9403333333333332 -0.9403333333333332
probs:  [0.060105438813780866, 0.06118915874889105, 0.06064389085701171, 0.34660356894859695, 0.05904847739558698, 0.4124094652361325]
printing an ep nov before normalisation:  56.61649691486359
printing an ep nov before normalisation:  31.325798167941777
printing an ep nov before normalisation:  39.72505540164093
siam score:  -0.72442156
maxi score, test score, baseline:  -0.9918431988041853 -0.9403333333333332 -0.9403333333333332
probs:  [0.060105438813780866, 0.06118915874889105, 0.06064389085701171, 0.34660356894859695, 0.05904847739558698, 0.4124094652361325]
siam score:  -0.72470397
maxi score, test score, baseline:  -0.9918431988041853 -0.9403333333333332 -0.9403333333333332
probs:  [0.06052223092368259, 0.06160258866728364, 0.06105901244408814, 0.34613151371952183, 0.05895140402040987, 0.41173325022501395]
maxi score, test score, baseline:  -0.9918431988041853 -0.9403333333333332 -0.9403333333333332
probs:  [0.060436638488454396, 0.06152104406694979, 0.06097543119726659, 0.3471160290591453, 0.05885992608291201, 0.4110909311052719]
maxi score, test score, baseline:  -0.9918431988041853 -0.9403333333333332 -0.9403333333333332
probs:  [0.060436638488454396, 0.06152104406694979, 0.06097543119726659, 0.3471160290591453, 0.05885992608291201, 0.4110909311052719]
using explorer policy with actor:  1
siam score:  -0.7272209
maxi score, test score, baseline:  -0.9918431988041853 -0.9403333333333332 -0.9403333333333332
printing an ep nov before normalisation:  21.3182459135528
printing an ep nov before normalisation:  51.976395402842364
printing an ep nov before normalisation:  27.128254223469384
maxi score, test score, baseline:  -0.9918431988041853 -0.9403333333333332 -0.9403333333333332
probs:  [0.06035113891544303, 0.06143958793719559, 0.060891940630653424, 0.34809947626375576, 0.05876854739301753, 0.4104493088599346]
maxi score, test score, baseline:  -0.9918431988041853 -0.9403333333333332 -0.9403333333333332
probs:  [0.06035113891544303, 0.06143958793719559, 0.060891940630653424, 0.34809947626375576, 0.05876854739301753, 0.4104493088599346]
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.368]
 [0.305]
 [0.32 ]
 [0.321]
 [0.324]
 [0.315]] [[31.275]
 [28.92 ]
 [31.275]
 [31.789]
 [31.662]
 [31.494]
 [31.739]] [[1.267]
 [1.192]
 [1.267]
 [1.313]
 [1.307]
 [1.3  ]
 [1.305]]
printing an ep nov before normalisation:  21.055612513671775
maxi score, test score, baseline:  -0.9918431988041853 -0.9403333333333332 -0.9403333333333332
probs:  [0.06035113891544303, 0.06143958793719559, 0.060891940630653424, 0.34809947626375576, 0.05876854739301753, 0.4104493088599346]
maxi score, test score, baseline:  -0.9918431988041853 -0.9403333333333332 -0.9403333333333332
probs:  [0.06035113891544303, 0.06143958793719559, 0.060891940630653424, 0.34809947626375576, 0.05876854739301753, 0.4104493088599346]
printing an ep nov before normalisation:  47.879695892333984
printing an ep nov before normalisation:  36.31100666755078
using explorer policy with actor:  1
siam score:  -0.7250499
maxi score, test score, baseline:  -0.9918431988041853 -0.9403333333333332 -0.9403333333333332
probs:  [0.05988132897441099, 0.061505608989244585, 0.06041598781262705, 0.34847420243288674, 0.05883169199141014, 0.4108911797994206]
siam score:  -0.7246571
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9918431988041853 -0.9403333333333332 -0.9403333333333332
probs:  [0.05970693392653947, 0.06134327536264833, 0.06024556298259197, 0.350442812403876, 0.058649502650853585, 0.4096119126734907]
maxi score, test score, baseline:  -0.9918431988041853 -0.9403333333333332 -0.9403333333333332
using another actor
Printing some Q and Qe and total Qs values:  [[0.866]
 [0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.515]] [[15.911]
 [11.857]
 [11.857]
 [11.857]
 [11.857]
 [11.857]
 [11.857]] [[0.997]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]]
maxi score, test score, baseline:  -0.9918431988041853 -0.9403333333333332 -0.9403333333333332
probs:  [0.05965207932038985, 0.0612953308112547, 0.05965207932038985, 0.3516156971896879, 0.058590182651446505, 0.40919463070683115]
printing an ep nov before normalisation:  15.638508501694997
maxi score, test score, baseline:  -0.9918431988041853 -0.9403333333333332 -0.9403333333333332
probs:  [0.05965207932038985, 0.0612953308112547, 0.05965207932038985, 0.3516156971896879, 0.058590182651446505, 0.40919463070683115]
maxi score, test score, baseline:  -0.9918431988041853 -0.9403333333333332 -0.9403333333333332
probs:  [0.059149406714995724, 0.06132804881710184, 0.05968391828663045, 0.3518037189345218, 0.058621453567552835, 0.40941345367919735]
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.576]] [[45.849]
 [45.849]
 [45.849]
 [45.849]
 [45.849]
 [45.849]
 [57.807]] [[1.155]
 [1.155]
 [1.155]
 [1.155]
 [1.155]
 [1.155]
 [1.518]]
printing an ep nov before normalisation:  29.498448974738245
maxi score, test score, baseline:  -0.9918431988041853 -0.9403333333333332 -0.9403333333333332
probs:  [0.059149406714995724, 0.06132804881710184, 0.05968391828663045, 0.3518037189345218, 0.058621453567552835, 0.40941345367919735]
printing an ep nov before normalisation:  23.849289591996158
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.481]
 [0.476]
 [0.476]
 [0.477]
 [0.485]
 [0.477]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.469]
 [0.481]
 [0.476]
 [0.476]
 [0.477]
 [0.485]
 [0.477]]
actions average: 
K:  1  action  0 :  tensor([0.6104, 0.0009, 0.0586, 0.0709, 0.1120, 0.0768, 0.0703],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0174, 0.8782, 0.0143, 0.0204, 0.0145, 0.0162, 0.0390],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0793, 0.0018, 0.4934, 0.0950, 0.1025, 0.1390, 0.0890],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1083, 0.0110, 0.0807, 0.3960, 0.1619, 0.1327, 0.1094],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1681, 0.0120, 0.0582, 0.1051, 0.4726, 0.0909, 0.0932],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1801, 0.0205, 0.1336, 0.1672, 0.1641, 0.1701, 0.1644],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1273, 0.0445, 0.1107, 0.1530, 0.1712, 0.1514, 0.2418],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9918612229679343 -0.9403333333333332 -0.9403333333333332
printing an ep nov before normalisation:  55.57365785183689
maxi score, test score, baseline:  -0.9918612229679343 -0.9403333333333332 -0.9403333333333332
printing an ep nov before normalisation:  41.87945689531961
Printing some Q and Qe and total Qs values:  [[ 0.266]
 [-0.015]
 [ 0.124]
 [ 0.143]
 [ 0.138]
 [ 0.121]
 [ 0.275]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.266]
 [-0.015]
 [ 0.124]
 [ 0.143]
 [ 0.138]
 [ 0.121]
 [ 0.275]]
printing an ep nov before normalisation:  45.3643798828125
using explorer policy with actor:  1
using explorer policy with actor:  0
using explorer policy with actor:  1
siam score:  -0.719771
maxi score, test score, baseline:  -0.9918791666666666 -0.9403333333333332 -0.9403333333333332
probs:  [0.05900487301742524, 0.060641392412777384, 0.059543603004715076, 0.35396884062649897, 0.05847275321402853, 0.4083685377245548]
printing an ep nov before normalisation:  19.543396291343342
printing an ep nov before normalisation:  48.1268835067749
printing an ep nov before normalisation:  50.60743290221118
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.492]
 [0.492]
 [0.488]
 [0.488]
 [0.485]
 [0.484]] [[29.483]
 [37.751]
 [37.751]
 [31.873]
 [32.345]
 [32.283]
 [32.167]] [[2.255]
 [3.094]
 [3.094]
 [2.436]
 [2.488]
 [2.478]
 [2.464]]
maxi score, test score, baseline:  -0.9918791666666666 -0.9403333333333332 -0.9403333333333332
probs:  [0.05900487301742524, 0.060641392412777384, 0.059543603004715076, 0.35396884062649897, 0.05847275321402853, 0.4083685377245548]
maxi score, test score, baseline:  -0.9918791666666666 -0.9403333333333332 -0.9403333333333332
probs:  [0.05891635000791596, 0.06055881599748604, 0.05945703756969989, 0.3549521234442013, 0.0583822966493441, 0.4077333763313527]
printing an ep nov before normalisation:  65.19865192925782
printing an ep nov before normalisation:  43.83646670318697
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9918791666666666 -0.9403333333333332 -0.9403333333333332
printing an ep nov before normalisation:  47.935758119461454
printing an ep nov before normalisation:  49.4673388092889
maxi score, test score, baseline:  -0.9918791666666666 -0.9403333333333332 -0.9403333333333332
probs:  [0.05882792091948337, 0.06047632719405827, 0.05937056397881547, 0.3559343630192321, 0.05829193605719827, 0.4070988888312124]
maxi score, test score, baseline:  -0.9918791666666666 -0.9403333333333332 -0.9403333333333332
printing an ep nov before normalisation:  41.70567989349365
printing an ep nov before normalisation:  0.0004988829377339243
printing an ep nov before normalisation:  20.3228964763764
maxi score, test score, baseline:  -0.9918791666666666 -0.9403333333333332 -0.9403333333333332
probs:  [0.058739585602734255, 0.06039392586313647, 0.05928418208597224, 0.35691556101099936, 0.05820167128493475, 0.40646507415222294]
UNIT TEST: sample policy line 217 mcts : [0.143 0.102 0.143 0.082 0.082 0.082 0.367]
printing an ep nov before normalisation:  30.5522619543767
printing an ep nov before normalisation:  33.17108666323683
maxi score, test score, baseline:  -0.9918970304380104 -0.9403333333333332 -0.9403333333333332
probs:  [0.05865134390859207, 0.060311611865658435, 0.05919789174539031, 0.3578957190753935, 0.05811150218022081, 0.405831931224745]
maxi score, test score, baseline:  -0.9918970304380104 -0.9403333333333332 -0.9403333333333332
probs:  [0.05865134390859207, 0.060311611865658435, 0.05919789174539031, 0.3578957190753935, 0.05811150218022081, 0.405831931224745]
maxi score, test score, baseline:  -0.9918970304380104 -0.9403333333333332 -0.9403333333333332
probs:  [0.05865134390859207, 0.060311611865658435, 0.05919789174539031, 0.3578957190753935, 0.05811150218022081, 0.405831931224745]
printing an ep nov before normalisation:  30.281741919601107
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9918970304380104 -0.9403333333333332 -0.9403333333333332
probs:  [0.05865134390859207, 0.060311611865658435, 0.05919789174539031, 0.3578957190753935, 0.05811150218022081, 0.405831931224745]
maxi score, test score, baseline:  -0.9918970304380104 -0.9403333333333332 -0.9403333333333332
probs:  [0.05865134390859207, 0.060311611865658435, 0.05919789174539031, 0.3578957190753935, 0.05811150218022081, 0.405831931224745]
printing an ep nov before normalisation:  36.0004368956055
printing an ep nov before normalisation:  51.50564349836012
printing an ep nov before normalisation:  30.93496848057045
printing an ep nov before normalisation:  40.269025435687766
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9919148148148148 -0.9403333333333332 -0.9403333333333332
printing an ep nov before normalisation:  39.13521026970329
printing an ep nov before normalisation:  66.13416532751808
printing an ep nov before normalisation:  41.08440761250677
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9919148148148148 -0.9403333333333332 -0.9403333333333332
maxi score, test score, baseline:  -0.9919148148148148 -0.9403333333333332 -0.9403333333333332
maxi score, test score, baseline:  -0.9919148148148148 -0.9403333333333332 -0.9403333333333332
probs:  [0.058313502966000956, 0.06053116863846677, 0.059408561481286856, 0.3604898768785731, 0.057775989487461224, 0.40348090054821106]
maxi score, test score, baseline:  -0.9919148148148148 -0.9403333333333332 -0.9403333333333332
probs:  [0.058313502966000956, 0.06053116863846677, 0.059408561481286856, 0.3604898768785731, 0.057775989487461224, 0.40348090054821106]
maxi score, test score, baseline:  -0.9919148148148148 -0.9403333333333332 -0.9403333333333332
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9919148148148148 -0.9403333333333332 -0.9403333333333332
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9919148148148148 -0.9403333333333332 -0.9403333333333332
probs:  [0.058258583541292645, 0.05991817738776099, 0.05935810741472922, 0.36166715255152676, 0.057718878225367996, 0.4030791008793224]
maxi score, test score, baseline:  -0.9919148148148148 -0.9403333333333332 -0.9403333333333332
probs:  [0.058258583541292645, 0.05991817738776099, 0.05935810741472922, 0.36166715255152676, 0.057718878225367996, 0.4030791008793224]
maxi score, test score, baseline:  -0.9919148148148148 -0.9403333333333332 -0.9403333333333332
probs:  [0.058258583541292645, 0.05991817738776099, 0.05935810741472922, 0.36166715255152676, 0.057718878225367996, 0.4030791008793224]
actor:  1 policy actor:  1  step number:  62 total reward:  0.30000000000000016  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9919148148148148 -0.9403333333333332 -0.9403333333333332
probs:  [0.061989603351093624, 0.06382590624434423, 0.06335827631500711, 0.31577094655696836, 0.061989603351093624, 0.43306566418149306]
printing an ep nov before normalisation:  52.20978400650967
siam score:  -0.7175266
printing an ep nov before normalisation:  42.731033949028685
printing an ep nov before normalisation:  42.230632770242146
actions average: 
K:  2  action  0 :  tensor([0.5062, 0.0296, 0.0682, 0.0825, 0.1204, 0.1040, 0.0892],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0400, 0.7807, 0.0311, 0.0264, 0.0311, 0.0364, 0.0544],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1274, 0.0735, 0.3555, 0.0774, 0.0849, 0.1861, 0.0952],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1214, 0.1335, 0.0967, 0.1920, 0.1813, 0.1756, 0.0995],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1510, 0.0935, 0.0576, 0.0971, 0.3834, 0.1215, 0.0960],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0824, 0.0546, 0.1243, 0.1051, 0.1290, 0.4010, 0.1037],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1181, 0.0370, 0.0903, 0.1790, 0.1872, 0.1600, 0.2284],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.515]
 [0.516]
 [0.516]
 [0.528]
 [0.52 ]
 [0.53 ]] [[36.936]
 [34.106]
 [36.936]
 [36.936]
 [37.452]
 [35.667]
 [37.188]] [[2.418]
 [2.135]
 [2.418]
 [2.418]
 [2.481]
 [2.295]
 [2.457]]
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]] [[35.296]
 [35.296]
 [35.296]
 [35.296]
 [35.296]
 [35.296]
 [35.296]] [[2.499]
 [2.499]
 [2.499]
 [2.499]
 [2.499]
 [2.499]
 [2.499]]
maxi score, test score, baseline:  -0.9919148148148148 -0.9403333333333332 -0.9403333333333332
maxi score, test score, baseline:  -0.9919148148148148 -0.9403333333333332 -0.9403333333333332
probs:  [0.061989603351093624, 0.06382590624434423, 0.06335827631500711, 0.31577094655696836, 0.061989603351093624, 0.43306566418149306]
maxi score, test score, baseline:  -0.9919148148148148 -0.9403333333333332 -0.9403333333333332
probs:  [0.061989603351093624, 0.06382590624434423, 0.06335827631500711, 0.31577094655696836, 0.061989603351093624, 0.43306566418149306]
actions average: 
K:  0  action  0 :  tensor([0.4486, 0.0039, 0.0845, 0.1080, 0.1461, 0.1046, 0.1043],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0063, 0.9465, 0.0055, 0.0150, 0.0048, 0.0065, 0.0155],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  printing an ep nov before normalisation:  24.478130340576172
tensor([0.2529, 0.0072, 0.1195, 0.1487, 0.1492, 0.1594, 0.1632],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0927, 0.0270, 0.0882, 0.3455, 0.1411, 0.1771, 0.1284],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1593, 0.0449, 0.1101, 0.1486, 0.2548, 0.1433, 0.1391],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.2314, 0.0242, 0.1051, 0.1109, 0.1205, 0.2945, 0.1133],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2673, 0.1227, 0.0728, 0.1287, 0.1109, 0.0936, 0.2039],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  34.66349326700644
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.542]
 [0.494]
 [0.498]
 [0.519]
 [0.469]
 [0.485]] [[38.957]
 [41.07 ]
 [41.932]
 [38.957]
 [40.812]
 [45.101]
 [36.523]] [[1.724]
 [1.887]
 [1.887]
 [1.724]
 [1.849]
 [2.041]
 [1.575]]
printing an ep nov before normalisation:  22.254559728693362
printing an ep nov before normalisation:  49.20601800930966
maxi score, test score, baseline:  -0.9919325203252032 -0.9403333333333332 -0.9403333333333332
probs:  [0.06191327980172521, 0.06375613893892013, 0.06328683940708786, 0.31660071121379396, 0.06191327980172521, 0.43252975083674766]
printing an ep nov before normalisation:  20.610973534222737
printing an ep nov before normalisation:  15.928707015167323
printing an ep nov before normalisation:  28.740344623401324
maxi score, test score, baseline:  -0.9919325203252032 -0.9403333333333332 -0.9403333333333332
probs:  [0.06191327980172521, 0.06375613893892013, 0.06328683940708786, 0.31660071121379396, 0.06191327980172521, 0.43252975083674766]
maxi score, test score, baseline:  -0.9919325203252032 -0.9403333333333332 -0.9403333333333332
probs:  [0.06176083581719405, 0.06361678999463147, 0.06314415570099831, 0.318258032336257, 0.06176083581719405, 0.43145935033372523]
printing an ep nov before normalisation:  51.10310870050707
maxi score, test score, baseline:  -0.9919325203252032 -0.9403333333333332 -0.9403333333333332
probs:  [0.06176083581719405, 0.06361678999463147, 0.06314415570099831, 0.318258032336257, 0.06176083581719405, 0.43145935033372523]
printing an ep nov before normalisation:  37.41034620861494
maxi score, test score, baseline:  -0.9919325203252032 -0.9403333333333332 -0.9403333333333332
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.98373794555664
printing an ep nov before normalisation:  35.306286797152126
printing an ep nov before normalisation:  37.181382179260254
maxi score, test score, baseline:  -0.9919325203252032 -0.9403333333333332 -0.9403333333333332
probs:  [0.06176083581719405, 0.06361678999463147, 0.06314415570099831, 0.318258032336257, 0.06176083581719405, 0.43145935033372523]
printing an ep nov before normalisation:  32.79942512512207
maxi score, test score, baseline:  -0.9919325203252032 -0.9403333333333332 -0.9403333333333332
probs:  [0.06176083581719405, 0.06361678999463147, 0.06314415570099831, 0.318258032336257, 0.06176083581719405, 0.43145935033372523]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.9919501474926253 -0.9403333333333332 -0.9403333333333332
probs:  [0.06181883170650086, 0.06320345339104144, 0.0627362147979043, 0.3185574100224964, 0.06181883170650086, 0.431865258375556]
maxi score, test score, baseline:  -0.9919501474926253 -0.9403333333333332 -0.9403333333333332
probs:  [0.06181883170650086, 0.06320345339104144, 0.0627362147979043, 0.3185574100224964, 0.06181883170650086, 0.431865258375556]
maxi score, test score, baseline:  -0.9919501474926253 -0.9403333333333332 -0.9403333333333332
probs:  [0.06181883170650086, 0.06320345339104144, 0.0627362147979043, 0.3185574100224964, 0.06181883170650086, 0.431865258375556]
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.187]
 [0.466]
 [0.469]
 [0.457]
 [0.456]
 [0.457]] [[30.475]
 [29.954]
 [30.667]
 [30.756]
 [31.808]
 [31.915]
 [31.959]] [[1.279]
 [0.939]
 [1.237]
 [1.242]
 [1.258]
 [1.26 ]
 [1.262]]
printing an ep nov before normalisation:  47.841524976665795
maxi score, test score, baseline:  -0.9919501474926253 -0.9403333333333332 -0.9403333333333332
probs:  [0.06181883170650086, 0.06320345339104144, 0.0627362147979043, 0.3185574100224964, 0.06181883170650086, 0.431865258375556]
siam score:  -0.7152739
Printing some Q and Qe and total Qs values:  [[-0.022]
 [-0.032]
 [-0.026]
 [-0.024]
 [-0.018]
 [-0.024]
 [-0.027]] [[ 9.43 ]
 [15.968]
 [ 9.301]
 [14.523]
 [14.391]
 [14.043]
 [13.561]] [[0.569]
 [0.968]
 [0.557]
 [0.885]
 [0.883]
 [0.855]
 [0.822]]
maxi score, test score, baseline:  -0.9919501474926253 -0.9403333333333332 -0.9403333333333332
probs:  [0.06174284372519566, 0.06313234825165295, 0.06266346195054392, 0.31938680573251904, 0.06174284372519566, 0.43133169661489273]
maxi score, test score, baseline:  -0.9919676968359087 -0.9403333333333332 -0.9403333333333332
probs:  [0.06174284372519566, 0.06313234825165295, 0.06266346195054392, 0.31938680573251904, 0.06174284372519566, 0.43133169661489273]
maxi score, test score, baseline:  -0.9919851688693098 -0.9403333333333332 -0.9403333333333332
probs:  [0.06174284372519566, 0.06313234825165295, 0.06266346195054392, 0.31938680573251904, 0.06174284372519566, 0.43133169661489273]
Printing some Q and Qe and total Qs values:  [[0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]] [[46.253]
 [46.253]
 [46.253]
 [46.253]
 [46.253]
 [46.253]
 [46.253]] [[0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]]
maxi score, test score, baseline:  -0.9919851688693098 -0.9403333333333332 -0.9403333333333332
probs:  [0.06166692258799428, 0.06306130566109441, 0.06259077310144746, 0.32021547185067734, 0.06166692258799428, 0.4307986042107923]
actions average: 
K:  2  action  0 :  tensor([0.5425, 0.0428, 0.0655, 0.0739, 0.1072, 0.0911, 0.0770],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0261, 0.8234, 0.0215, 0.0282, 0.0190, 0.0273, 0.0545],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1075, 0.0099, 0.3528, 0.1170, 0.1301, 0.1453, 0.1375],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1237, 0.0050, 0.0913, 0.2322, 0.1974, 0.2049, 0.1455],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1408, 0.0166, 0.1010, 0.1176, 0.3347, 0.1509, 0.1384],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0970, 0.0101, 0.1018, 0.1257, 0.1310, 0.3964, 0.1380],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1341, 0.0073, 0.1040, 0.1717, 0.1646, 0.1896, 0.2286],
       grad_fn=<DivBackward0>)
actions average: 
K:  1  action  0 :  tensor([0.6353, 0.0020, 0.0480, 0.0776, 0.0805, 0.0759, 0.0807],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0127, 0.8923, 0.0129, 0.0204, 0.0094, 0.0213, 0.0309],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1068, 0.0330, 0.3533, 0.1371, 0.0797, 0.1778, 0.1123],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1508, 0.0395, 0.0769, 0.3537, 0.1193, 0.1326, 0.1271],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1356, 0.0182, 0.0569, 0.1142, 0.4508, 0.0986, 0.1258],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1411, 0.0126, 0.1104, 0.1214, 0.1385, 0.3339, 0.1422],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1290, 0.0592, 0.1009, 0.1500, 0.1345, 0.1717, 0.2548],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9919851688693098 -0.9403333333333332 -0.9403333333333332
probs:  [0.06159106820673468, 0.0629903255368689, 0.06251814816620632, 0.3210434093392446, 0.06159106820673468, 0.43026598054421095]
printing an ep nov before normalisation:  56.97318720552924
maxi score, test score, baseline:  -0.9919851688693098 -0.9403333333333332 -0.9403333333333332
probs:  [0.06159106820673468, 0.0629903255368689, 0.06251814816620632, 0.3210434093392446, 0.06159106820673468, 0.43026598054421095]
maxi score, test score, baseline:  -0.992002564102564 -0.9403333333333332 -0.9403333333333332
probs:  [0.06159106820673468, 0.0629903255368689, 0.06251814816620632, 0.3210434093392446, 0.06159106820673468, 0.43026598054421095]
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]] [[33.558]
 [33.558]
 [33.558]
 [33.558]
 [33.558]
 [33.558]
 [33.558]] [[2.068]
 [2.068]
 [2.068]
 [2.068]
 [2.068]
 [2.068]
 [2.068]]
maxi score, test score, baseline:  -0.992002564102564 -0.9403333333333332 -0.9403333333333332
maxi score, test score, baseline:  -0.992002564102564 -0.9403333333333332 -0.9403333333333332
probs:  [0.061878496199284115, 0.0632787869713483, 0.06280626086719497, 0.32152245964627896, 0.06142304808958243, 0.4290909482263112]
maxi score, test score, baseline:  -0.992002564102564 -0.9403333333333332 -0.9403333333333332
printing an ep nov before normalisation:  31.1833037509525
maxi score, test score, baseline:  -0.992002564102564 -0.9403333333333332 -0.9403333333333332
probs:  [0.061878496199284115, 0.0632787869713483, 0.06280626086719497, 0.32152245964627896, 0.06142304808958243, 0.4290909482263112]
maxi score, test score, baseline:  -0.992002564102564 -0.9403333333333332 -0.9403333333333332
Printing some Q and Qe and total Qs values:  [[0.031]
 [0.031]
 [0.031]
 [0.031]
 [0.031]
 [0.045]
 [0.031]] [[33.166]
 [33.166]
 [33.166]
 [33.166]
 [33.166]
 [33.424]
 [33.166]] [[1.671]
 [1.671]
 [1.671]
 [1.671]
 [1.671]
 [1.706]
 [1.671]]
Printing some Q and Qe and total Qs values:  [[-0.001]
 [-0.001]
 [-0.001]
 [ 0.013]
 [ 0.013]
 [ 0.005]
 [ 0.01 ]] [[35.407]
 [35.407]
 [35.407]
 [37.779]
 [38.164]
 [35.059]
 [36.493]] [[1.703]
 [1.703]
 [1.703]
 [1.897]
 [1.927]
 [1.683]
 [1.796]]
maxi score, test score, baseline:  -0.992002564102564 -0.9403333333333332 -0.9403333333333332
probs:  [0.061878496199284115, 0.0632787869713483, 0.06280626086719497, 0.32152245964627896, 0.06142304808958243, 0.4290909482263112]
using explorer policy with actor:  1
siam score:  -0.7163791
maxi score, test score, baseline:  -0.992002564102564 -0.9403333333333332 -0.9403333333333332
maxi score, test score, baseline:  -0.992002564102564 -0.9403333333333332 -0.9403333333333332
maxi score, test score, baseline:  -0.992002564102564 -0.9403333333333332 -0.9403333333333332
probs:  [0.061404315954266035, 0.06279360352331775, 0.06279360352331775, 0.3226460316482848, 0.061404315954266035, 0.42895812939654765]
maxi score, test score, baseline:  -0.992002564102564 -0.9403333333333332 -0.9403333333333332
maxi score, test score, baseline:  -0.992002564102564 -0.9403333333333332 -0.9403333333333332
probs:  [0.06132867745734885, 0.06272274974588055, 0.06272274974588055, 0.3234701120820676, 0.06132867745734885, 0.42842703351147365]
maxi score, test score, baseline:  -0.992002564102564 -0.9403333333333332 -0.9403333333333332
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.992002564102564 -0.9403333333333332 -0.9403333333333332
probs:  [0.06128199194065296, 0.06268150579594918, 0.06220927719446886, 0.3244466590978564, 0.06128199194065296, 0.4280985740304196]
using explorer policy with actor:  1
printing an ep nov before normalisation:  59.965545009277676
printing an ep nov before normalisation:  24.120898246765137
maxi score, test score, baseline:  -0.992002564102564 -0.9403333333333332 -0.9403333333333332
actor:  1 policy actor:  1  step number:  67 total reward:  0.21333333333333315  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  25.919941907608855
printing an ep nov before normalisation:  49.01461601257324
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.44 ]
 [0.436]
 [0.434]
 [0.423]
 [0.431]
 [0.421]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.425]
 [0.44 ]
 [0.436]
 [0.434]
 [0.423]
 [0.431]
 [0.421]]
printing an ep nov before normalisation:  37.296220660209656
siam score:  -0.72568816
printing an ep nov before normalisation:  53.45971010963316
printing an ep nov before normalisation:  40.656647018995926
maxi score, test score, baseline:  -0.992002564102564 -0.9403333333333332 -0.9403333333333332
printing an ep nov before normalisation:  32.40047379556871
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.4699],
        [-0.5911],
        [-0.5412],
        [-0.5858],
        [-0.0000],
        [-0.5132],
        [-0.5459],
        [-0.0000],
        [-0.6064],
        [-0.0000]], dtype=torch.float64)
-0.09703970119800001 -0.5669341563309735
-0.032346567066 -0.6234733888772626
-0.09703970119800001 -0.6382413944124076
-0.032346567066 -0.6181806425064508
-0.34300331669999895 -0.34300331669999895
-0.032346567066 -0.545578139576234
-0.057834381198 -0.6037171967658006
-0.9438 -0.9438
-0.032346567066 -0.6387795147912537
-0.620796 -0.620796
printing an ep nov before normalisation:  50.362831480668774
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.55011544872161
maxi score, test score, baseline:  -0.9920198830409356 -0.9403333333333332 -0.9403333333333332
probs:  [0.05487105533377526, 0.056544761737796094, 0.05569770300893189, 0.2927553246684486, 0.156901863668405, 0.3832292915826432]
printing an ep nov before normalisation:  33.150887057846006
actions average: 
K:  4  action  0 :  tensor([0.4384, 0.0336, 0.0677, 0.0898, 0.1092, 0.1074, 0.1539],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0069, 0.9094, 0.0049, 0.0211, 0.0334, 0.0156, 0.0087],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1015, 0.0081, 0.2211, 0.1551, 0.1825, 0.1893, 0.1424],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1248, 0.0765, 0.0945, 0.2324, 0.1400, 0.1464, 0.1854],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1148, 0.0783, 0.0626, 0.1415, 0.3448, 0.0955, 0.1625],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1512, 0.0345, 0.1327, 0.1554, 0.1202, 0.2386, 0.1674],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0853, 0.1128, 0.0750, 0.2635, 0.1332, 0.1321, 0.1981],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.319]
 [0.386]
 [0.328]
 [0.333]
 [0.357]
 [0.331]
 [0.306]] [[37.868]
 [40.504]
 [37.896]
 [38.014]
 [40.163]
 [40.996]
 [41.446]] [[1.617]
 [1.858]
 [1.628]
 [1.641]
 [1.806]
 [1.835]
 [1.84 ]]
maxi score, test score, baseline:  -0.9920198830409356 -0.9403333333333332 -0.9403333333333332
probs:  [0.054893844594286874, 0.0565682478204552, 0.05530483447707365, 0.2928771534182117, 0.15696713190061845, 0.38338878778935415]
printing an ep nov before normalisation:  28.679260008712646
printing an ep nov before normalisation:  13.46801649734811
printing an ep nov before normalisation:  24.413266195242514
printing an ep nov before normalisation:  36.16665690624453
printing an ep nov before normalisation:  27.05463409423828
maxi score, test score, baseline:  -0.9920198830409356 -0.9403333333333332 -0.9403333333333332
probs:  [0.054634954489289025, 0.05632458229097863, 0.05504968131334013, 0.2947821356716556, 0.15763634707337718, 0.3815722991613594]
printing an ep nov before normalisation:  34.691035608850804
maxi score, test score, baseline:  -0.9920198830409356 -0.9403333333333332 -0.9403333333333332
maxi score, test score, baseline:  -0.9920371261852662 -0.9403333333333332 -0.9403333333333332
probs:  [0.05460506029888225, 0.05630149496589155, 0.05502145789896637, 0.2957197027869742, 0.15699073580957942, 0.38136154823970625]
printing an ep nov before normalisation:  25.694671665554633
siam score:  -0.7352214
printing an ep nov before normalisation:  35.30117901826363
printing an ep nov before normalisation:  31.517648696899414
maxi score, test score, baseline:  -0.9920371261852662 -0.9403333333333332 -0.9403333333333332
probs:  [0.05460506029888225, 0.05630149496589155, 0.05502145789896637, 0.2957197027869742, 0.15699073580957942, 0.38136154823970625]
printing an ep nov before normalisation:  29.053910597191255
printing an ep nov before normalisation:  27.25496939788335
printing an ep nov before normalisation:  29.47509765625
maxi score, test score, baseline:  -0.9920371261852662 -0.9403333333333332 -0.9403333333333332
probs:  [0.054660635462315095, 0.05635880092536582, 0.05507745789415483, 0.29602127638280384, 0.15613133097423676, 0.3817504983611236]
printing an ep nov before normalisation:  22.69484281539917
printing an ep nov before normalisation:  26.079799604952743
printing an ep nov before normalisation:  22.29055404663086
maxi score, test score, baseline:  -0.9920371261852662 -0.9403333333333332 -0.9403333333333332
probs:  [0.054660635462315095, 0.05635880092536582, 0.05507745789415483, 0.29602127638280384, 0.15613133097423676, 0.3817504983611236]
printing an ep nov before normalisation:  0.3935334075713115
actor:  1 policy actor:  1  step number:  62 total reward:  0.21999999999999942  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9920371261852662 -0.9403333333333332 -0.9403333333333332
probs:  [0.04952502158959724, 0.051070270358900674, 0.04990430992388082, 0.26915157828016517, 0.23454198445780483, 0.34580683538965135]
maxi score, test score, baseline:  -0.9920371261852662 -0.9403333333333332 -0.9403333333333332
probs:  [0.04943376934539303, 0.05098318479965712, 0.04981408041143967, 0.2696525379107742, 0.23494962107341555, 0.3451668064593204]
maxi score, test score, baseline:  -0.9920371261852662 -0.9403333333333332 -0.9403333333333332
probs:  [0.0492940353762673, 0.051220020209068386, 0.050050422146967366, 0.2699817101122265, 0.2352641393877564, 0.34418967276771395]
maxi score, test score, baseline:  -0.9920371261852662 -0.9403333333333332 -0.9403333333333332
probs:  [0.0492940353762673, 0.051220020209068386, 0.050050422146967366, 0.2699817101122265, 0.2352641393877564, 0.34418967276771395]
maxi score, test score, baseline:  -0.9920371261852662 -0.9403333333333332 -0.9403333333333332
line 256 mcts: sample exp_bonus 43.455957701792926
from probs:  [0.0492940353762673, 0.051220020209068386, 0.050050422146967366, 0.2699817101122265, 0.2352641393877564, 0.34418967276771395]
maxi score, test score, baseline:  -0.9920371261852662 -0.9403333333333332 -0.9403333333333332
probs:  [0.049366022617641905, 0.05129482771192613, 0.050123516981942624, 0.2703768551159361, 0.23414529546730178, 0.34469348210525136]
maxi score, test score, baseline:  -0.9920371261852662 -0.9403333333333332 -0.9403333333333332
probs:  [0.04943737534388744, 0.05136897584110284, 0.0501959675391575, 0.2707685172040867, 0.2330363133415355, 0.34519285073023004]
maxi score, test score, baseline:  -0.9920371261852662 -0.9403333333333332 -0.9403333333333332
probs:  [0.049346702013850934, 0.05128346456165313, 0.050107321487169613, 0.271269333963416, 0.23343629375019562, 0.3445568842237147]
printing an ep nov before normalisation:  43.221004827546984
maxi score, test score, baseline:  -0.9920371261852662 -0.9403333333333332 -0.9403333333333332
probs:  [0.049346702013850934, 0.05128346456165313, 0.050107321487169613, 0.271269333963416, 0.23343629375019562, 0.3445568842237147]
printing an ep nov before normalisation:  21.647411738596034
printing an ep nov before normalisation:  11.212169593482528
printing an ep nov before normalisation:  35.9934069600864
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.74198892514069
printing an ep nov before normalisation:  37.89082732568107
maxi score, test score, baseline:  -0.9920542940320233 -0.9403333333333332 -0.9403333333333332
Printing some Q and Qe and total Qs values:  [[0.343]
 [0.501]
 [0.343]
 [0.343]
 [0.505]
 [0.343]
 [0.343]] [[35.397]
 [43.606]
 [35.397]
 [35.397]
 [42.878]
 [35.397]
 [35.397]] [[1.422]
 [2.068]
 [1.422]
 [1.422]
 [2.029]
 [1.422]
 [1.422]]
printing an ep nov before normalisation:  31.371274763346157
maxi score, test score, baseline:  -0.9920542940320233 -0.9403333333333332 -0.9403333333333332
probs:  [0.04927508660107203, 0.05121774615521886, 0.04965425629718261, 0.2718734230504524, 0.23392518973912757, 0.34405429815694655]
printing an ep nov before normalisation:  0.0
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.1334],
        [-0.6293],
        [-0.4251],
        [-0.0000],
        [-0.0000],
        [-0.4064],
        [-0.5545],
        [-0.3596],
        [-0.0000]], dtype=torch.float64)
-0.8969470157999999 -0.8969470157999999
-0.09703970119800001 -0.23042396032086218
-0.032346567066 -0.6616817894801679
-0.09703970119800001 -0.5221486562584896
-0.8628778773119999 -0.8628778773119999
-0.8777350956 -0.8777350956
-0.032346567066 -0.4387151216851941
-0.032346567066 -0.5868204738343371
-0.083839701198 -0.44342479025671994
-0.96402702 -0.96402702
actions average: 
K:  0  action  0 :  tensor([0.5437, 0.0024, 0.0630, 0.0893, 0.1252, 0.0875, 0.0888],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0289, 0.9013, 0.0111, 0.0083, 0.0134, 0.0070, 0.0301],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1122, 0.0040, 0.2915, 0.1648, 0.0972, 0.2216, 0.1088],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0886, 0.0087, 0.1015, 0.3794, 0.1506, 0.1462, 0.1250],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1130, 0.0011, 0.0906, 0.1248, 0.4013, 0.1263, 0.1429],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1093, 0.0064, 0.1179, 0.1718, 0.1501, 0.2732, 0.1714],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1331, 0.0452, 0.1111, 0.1891, 0.1359, 0.1508, 0.2348],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  32.06505901108868
from probs:  [0.04911340826815068, 0.05106709086253031, 0.04911340826815068, 0.27297481235755894, 0.2348112531536138, 0.3429200270899956]
printing an ep nov before normalisation:  21.589727346496908
printing an ep nov before normalisation:  32.96729564666748
maxi score, test score, baseline:  -0.9920542940320233 -0.9403333333333332 -0.9403333333333332
probs:  [0.04893383040111663, 0.05089775547478364, 0.04893383040111663, 0.27396886208040333, 0.23560522459055666, 0.34166049705202306]
maxi score, test score, baseline:  -0.9920542940320233 -0.9403333333333332 -0.9403333333333332
probs:  [0.049005007833812786, 0.05097179736271801, 0.049005007833812786, 0.274368261187093, 0.23449129894365417, 0.3421586268389092]
printing an ep nov before normalisation:  20.401640236680024
maxi score, test score, baseline:  -0.9920713870733479 -0.9403333333333332 -0.9403333333333332
probs:  [0.049005007833812786, 0.05097179736271801, 0.049005007833812786, 0.274368261187093, 0.23449129894365417, 0.3421586268389092]
printing an ep nov before normalisation:  51.99244970086055
maxi score, test score, baseline:  -0.9920713870733479 -0.9403333333333332 -0.9403333333333332
printing an ep nov before normalisation:  51.217570266259855
siam score:  -0.7257266
printing an ep nov before normalisation:  30.622757104808898
maxi score, test score, baseline:  -0.9920713870733479 -0.9403333333333332 -0.9403333333333332
siam score:  -0.7252823
printing an ep nov before normalisation:  44.19349354643766
Printing some Q and Qe and total Qs values:  [[0.777]
 [0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.554]] [[21.031]
 [22.141]
 [22.141]
 [22.141]
 [22.141]
 [22.141]
 [22.141]] [[0.777]
 [0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.554]]
Printing some Q and Qe and total Qs values:  [[0.81 ]
 [0.421]
 [0.448]
 [0.455]
 [0.44 ]
 [0.44 ]
 [0.451]] [[10.409]
 [21.578]
 [13.162]
 [12.964]
 [14.968]
 [15.099]
 [13.64 ]] [[0.81 ]
 [0.421]
 [0.448]
 [0.455]
 [0.44 ]
 [0.44 ]
 [0.451]]
printing an ep nov before normalisation:  32.10697232047736
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 14.953201879506373
printing an ep nov before normalisation:  35.84416118719023
maxi score, test score, baseline:  -0.9920713870733479 -0.9403333333333332 -0.9403333333333332
probs:  [0.04891555515630095, 0.05088746477147905, 0.04891555515630095, 0.27486549014630257, 0.23488471736086833, 0.34153121740874803]
maxi score, test score, baseline:  -0.9920713870733479 -0.9403333333333332 -0.9403333333333332
probs:  [0.04891555515630095, 0.05088746477147905, 0.04891555515630095, 0.27486549014630257, 0.23488471736086833, 0.34153121740874803]
printing an ep nov before normalisation:  13.437705267930534
maxi score, test score, baseline:  -0.9920713870733479 -0.9403333333333332 -0.9403333333333332
probs:  [0.04891555515630095, 0.05088746477147905, 0.04891555515630095, 0.27486549014630257, 0.23488471736086833, 0.34153121740874803]
printing an ep nov before normalisation:  15.597281692510645
maxi score, test score, baseline:  -0.9920713870733479 -0.9403333333333332 -0.9403333333333332
probs:  [0.04891555515630095, 0.05088746477147905, 0.04891555515630095, 0.27486549014630257, 0.23488471736086833, 0.34153121740874803]
printing an ep nov before normalisation:  13.989575330742966
printing an ep nov before normalisation:  13.750607464265702
printing an ep nov before normalisation:  19.66903554090342
printing an ep nov before normalisation:  28.735307434569446
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.653]
 [0.546]] [[47.511]
 [47.511]
 [47.511]
 [47.511]
 [47.511]
 [48.084]
 [47.511]] [[2.159]
 [2.159]
 [2.159]
 [2.159]
 [2.159]
 [2.297]
 [2.159]]
maxi score, test score, baseline:  -0.9921053506869124 -0.9403333333333332 -0.9403333333333332
probs:  [0.04898616853148754, 0.05096093252442999, 0.04898616853148754, 0.27526317047492727, 0.23377816225522766, 0.34202539768243995]
maxi score, test score, baseline:  -0.9921053506869124 -0.9403333333333332 -0.9403333333333332
probs:  [0.04898616853148754, 0.05096093252442999, 0.04898616853148754, 0.27526317047492727, 0.23377816225522766, 0.34202539768243995]
maxi score, test score, baseline:  -0.9921053506869124 -0.9403333333333332 -0.9403333333333332
probs:  [0.04905617146394503, 0.051033765158956204, 0.04905617146394503, 0.27565741291286233, 0.23268117316334583, 0.3425153058369455]
printing an ep nov before normalisation:  31.932701831327496
maxi score, test score, baseline:  -0.9921222222222221 -0.9403333333333332 -0.9403333333333332
probs:  [0.049125571835392956, 0.05110597087537703, 0.049125571835392956, 0.2760482618483673, 0.23159362657382032, 0.34300099703164955]
printing an ep nov before normalisation:  40.94071069164163
maxi score, test score, baseline:  -0.9921222222222221 -0.9403333333333332 -0.9403333333333332
probs:  [0.04903646669903527, 0.051022019306569, 0.04903646669903527, 0.27654967477827824, 0.231979355764668, 0.3423760167524143]
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.446]
 [0.509]
 [0.505]
 [0.501]
 [0.5  ]
 [0.453]] [[39.559]
 [48.514]
 [36.72 ]
 [37.007]
 [36.712]
 [37.216]
 [44.042]] [[1.075]
 [1.317]
 [0.929]
 [0.935]
 [0.92 ]
 [0.939]
 [1.152]]
printing an ep nov before normalisation:  0.009551416789008726
printing an ep nov before normalisation:  30.082263212497384
printing an ep nov before normalisation:  86.32348824701674
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.475]
 [0.447]
 [0.45 ]
 [0.447]
 [0.475]
 [0.475]] [[ 0.   ]
 [ 0.   ]
 [18.643]
 [56.989]
 [17.048]
 [ 0.   ]
 [ 0.   ]] [[0.371]
 [0.371]
 [0.573]
 [1.051]
 [0.553]
 [0.371]
 [0.371]]
maxi score, test score, baseline:  -0.9921222222222221 -0.9403333333333332 -0.9403333333333332
Printing some Q and Qe and total Qs values:  [[0.642]
 [0.359]
 [0.449]
 [0.385]
 [0.342]
 [0.444]
 [0.459]] [[27.039]
 [36.324]
 [30.624]
 [32.217]
 [33.01 ]
 [30.167]
 [30.818]] [[1.019]
 [1.036]
 [0.942]
 [0.93 ]
 [0.912]
 [0.922]
 [0.958]]
printing an ep nov before normalisation:  37.20958785677052
printing an ep nov before normalisation:  43.453743587925146
maxi score, test score, baseline:  -0.9921222222222221 -0.9403333333333332 -0.9403333333333332
maxi score, test score, baseline:  -0.9921222222222221 -0.9403333333333332 -0.9403333333333332
probs:  [0.04885880613947481, 0.050854634088386495, 0.04885880613947481, 0.2775494072891094, 0.23274843448095772, 0.3411299118625966]
printing an ep nov before normalisation:  15.203969478607178
line 256 mcts: sample exp_bonus 44.47986765003199
maxi score, test score, baseline:  -0.9921222222222221 -0.9403333333333332 -0.9403333333333332
probs:  [0.04919421926899609, 0.051185449169130134, 0.04881019635968452, 0.27735795609666053, 0.23266019712433555, 0.3407919819811932]
printing an ep nov before normalisation:  20.045134258585364
actions average: 
K:  4  action  0 :  tensor([0.6044, 0.0329, 0.0645, 0.0689, 0.0740, 0.0614, 0.0940],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0311, 0.8188, 0.0209, 0.0478, 0.0262, 0.0264, 0.0288],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2449, 0.0124, 0.2697, 0.1189, 0.1076, 0.1150, 0.1314],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0450, 0.0244, 0.0471, 0.5424, 0.1561, 0.1013, 0.0836],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1220, 0.0251, 0.0821, 0.1300, 0.4235, 0.1100, 0.1073],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1055, 0.0611, 0.1308, 0.1169, 0.1081, 0.3367, 0.1409],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1317, 0.1998, 0.0938, 0.1517, 0.1497, 0.1100, 0.1634],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9921390208783297 -0.9403333333333332 -0.9403333333333332
probs:  [0.04910672603162827, 0.05110305416825174, 0.04872171989099375, 0.2778546408532948, 0.2330424401754829, 0.3401714188803485]
maxi score, test score, baseline:  -0.9921390208783297 -0.9403333333333332 -0.9403333333333332
probs:  [0.04917605912073286, 0.0511752135123435, 0.048790507916636514, 0.27824781846635793, 0.23195758096581107, 0.3406528200181181]
printing an ep nov before normalisation:  47.95131134499231
maxi score, test score, baseline:  -0.9921390208783297 -0.9403333333333332 -0.9403333333333332
maxi score, test score, baseline:  -0.9921390208783297 -0.9403333333333332 -0.9403333333333332
probs:  [0.04917605912073286, 0.0511752135123435, 0.048790507916636514, 0.27824781846635793, 0.23195758096581107, 0.3406528200181181]
maxi score, test score, baseline:  -0.9921390208783297 -0.9403333333333332 -0.9403333333333332
probs:  [0.04917605912073286, 0.0511752135123435, 0.048790507916636514, 0.27824781846635793, 0.23195758096581107, 0.3406528200181181]
maxi score, test score, baseline:  -0.9921390208783297 -0.9403333333333332 -0.9403333333333332
probs:  [0.04917605912073286, 0.0511752135123435, 0.048790507916636514, 0.27824781846635793, 0.23195758096581107, 0.3406528200181181]
printing an ep nov before normalisation:  22.045816062803322
maxi score, test score, baseline:  -0.9921557471264367 -0.9403333333333332 -0.9403333333333332
probs:  [0.0490887994908035, 0.05109305603042806, 0.04870226430101876, 0.27874518502983014, 0.23233680775762874, 0.3400338873902908]
maxi score, test score, baseline:  -0.9921557471264367 -0.9403333333333332 -0.9403333333333332
maxi score, test score, baseline:  -0.9921557471264367 -0.9403333333333332 -0.9403333333333332
probs:  [0.0490887994908035, 0.05109305603042806, 0.04870226430101876, 0.27874518502983014, 0.23233680775762874, 0.3400338873902908]
maxi score, test score, baseline:  -0.9921557471264367 -0.9403333333333332 -0.9403333333333332
siam score:  -0.725835
Starting evaluation
siam score:  -0.7289107
maxi score, test score, baseline:  -0.9921557471264367 -0.9403333333333332 -0.9403333333333332
probs:  [0.0490887994908035, 0.05109305603042806, 0.04870226430101876, 0.27874518502983014, 0.23233680775762874, 0.3400338873902908]
printing an ep nov before normalisation:  42.059379153781464
printing an ep nov before normalisation:  44.556707492442754
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
printing an ep nov before normalisation:  47.464933677443575
printing an ep nov before normalisation:  20.70488452911377
siam score:  -0.7314958
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.355]
 [0.354]] [[21.945]
 [33.007]
 [21.945]
 [23.762]
 [27.245]
 [23.921]
 [21.945]] [[0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.355]
 [0.354]]
maxi score, test score, baseline:  -0.9921557471264367 -0.9403333333333332 -0.9403333333333332
probs:  [0.0490887994908035, 0.05109305603042806, 0.04870226430101876, 0.27874518502983014, 0.23233680775762874, 0.3400338873902908]
Printing some Q and Qe and total Qs values:  [[0.298]
 [0.345]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.34 ]] [[36.623]
 [43.113]
 [36.623]
 [36.623]
 [36.623]
 [36.623]
 [43.943]] [[1.687]
 [2.069]
 [1.687]
 [1.687]
 [1.687]
 [1.687]
 [2.106]]
printing an ep nov before normalisation:  30.282649005500975
printing an ep nov before normalisation:  25.672647434001906
printing an ep nov before normalisation:  27.986850415644476
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
printing an ep nov before normalisation:  31.727617438783717
printing an ep nov before normalisation:  55.195546215125404
using explorer policy with actor:  0
printing an ep nov before normalisation:  28.603700292425923
printing an ep nov before normalisation:  38.52042901422351
using explorer policy with actor:  0
printing an ep nov before normalisation:  23.29366644289461
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
printing an ep nov before normalisation:  30.388741493225098
printing an ep nov before normalisation:  31.19698289775652
printing an ep nov before normalisation:  28.17214690545704
printing an ep nov before normalisation:  25.1685831997072
printing an ep nov before normalisation:  16.936947368995448
printing an ep nov before normalisation:  16.281037930731284
line 256 mcts: sample exp_bonus 35.110084877116
using explorer policy with actor:  0
printing an ep nov before normalisation:  24.83028613249941
printing an ep nov before normalisation:  25.73167749874642
printing an ep nov before normalisation:  26.923933061456218
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.447]
 [0.452]
 [0.453]
 [0.453]
 [0.451]
 [0.453]] [[15.616]
 [16.21 ]
 [14.381]
 [15.167]
 [15.862]
 [16.554]
 [15.83 ]] [[0.571]
 [0.447]
 [0.452]
 [0.453]
 [0.453]
 [0.451]
 [0.453]]
printing an ep nov before normalisation:  14.380562800255348
using explorer policy with actor:  0
printing an ep nov before normalisation:  11.948153209916477
printing an ep nov before normalisation:  20.56741331989187
line 256 mcts: sample exp_bonus 22.06180329660677
printing an ep nov before normalisation:  16.859455104212756
using explorer policy with actor:  0
using explorer policy with actor:  0
printing an ep nov before normalisation:  18.6136242158263
using explorer policy with actor:  0
printing an ep nov before normalisation:  15.60483694076538
Printing some Q and Qe and total Qs values:  [[0.518]
 [0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.476]] [[19.51 ]
 [15.785]
 [15.785]
 [15.785]
 [15.785]
 [15.785]
 [15.785]] [[0.518]
 [0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.476]]
maxi score, test score, baseline:  -0.9921724014336917 -0.9403333333333332 -0.9403333333333332
probs:  [0.0490887994908035, 0.05109305603042806, 0.04870226430101876, 0.27874518502983014, 0.23233680775762874, 0.3400338873902908]
printing an ep nov before normalisation:  17.524190521421765
printing an ep nov before normalisation:  12.788649624484307
maxi score, test score, baseline:  -0.9921724014336917 -0.9403333333333332 -0.9403333333333332
probs:  [0.0490887994908035, 0.05109305603042806, 0.04870226430101876, 0.27874518502983014, 0.23233680775762874, 0.3400338873902908]
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.375]
 [0.432]
 [0.423]
 [0.422]
 [0.414]
 [0.429]] [[14.084]
 [23.077]
 [16.32 ]
 [19.508]
 [20.079]
 [20.386]
 [19.73 ]] [[0.456]
 [0.375]
 [0.432]
 [0.423]
 [0.422]
 [0.414]
 [0.429]]
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]] [[13.035]
 [13.035]
 [13.035]
 [13.035]
 [13.035]
 [13.035]
 [13.035]] [[0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]]
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  81 total reward:  0.1599999999999987  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.689]
 [0.772]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]] [[32.803]
 [32.829]
 [32.803]
 [32.803]
 [32.803]
 [32.803]
 [32.803]] [[0.689]
 [0.772]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]]
maxi score, test score, baseline:  -0.9921724014336917 -0.9403333333333332 -0.9403333333333332
probs:  [0.05229523664655029, 0.054112009959633764, 0.05194485893616991, 0.2604689829598193, 0.21840176268898562, 0.36277714880884104]
printing an ep nov before normalisation:  23.276152074664193
printing an ep nov before normalisation:  19.99798059463501
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.356]
 [0.36 ]
 [0.355]
 [0.356]
 [0.357]
 [0.353]] [[13.757]
 [19.998]
 [15.233]
 [17.144]
 [17.423]
 [17.091]
 [20.747]] [[0.434]
 [0.356]
 [0.36 ]
 [0.355]
 [0.356]
 [0.357]
 [0.353]]
printing an ep nov before normalisation:  32.706971469597676
siam score:  -0.7276407
printing an ep nov before normalisation:  23.81404874192155
actor:  0 policy actor:  1  step number:  54 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  22.02534873379447
maxi score, test score, baseline:  -0.9891131616595137 -0.9403333333333332 -0.9403333333333332
probs:  [0.05221359949324586, 0.05403514620314446, 0.05186230119919399, 0.26093430226524783, 0.21875655442676664, 0.36219809641240114]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  57 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  19.538260757645176
printing an ep nov before normalisation:  16.75655029625153
printing an ep nov before normalisation:  12.477101188673343
using explorer policy with actor:  0
printing an ep nov before normalisation:  11.528384765320698
printing an ep nov before normalisation:  19.02965288251999
printing an ep nov before normalisation:  46.593551629864315
printing an ep nov before normalisation:  11.248162984848022
maxi score, test score, baseline:  -0.9861108262108262 -0.9403333333333332 -0.9403333333333332
probs:  [0.05213211372008646, 0.053958424975468745, 0.051779896549405596, 0.26139875872696994, 0.2191106882726131, 0.3616201177554562]
printing an ep nov before normalisation:  16.75655029625153
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  15.197387933731079
maxi score, test score, baseline:  -0.9861108262108262 -0.9403333333333332 -0.9403333333333332
printing an ep nov before normalisation:  16.768343164435873
printing an ep nov before normalisation:  23.075539488742614
printing an ep nov before normalisation:  14.929551022551308
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9861108262108262 -0.9403333333333332 -0.9403333333333332
probs:  [0.05213211372008646, 0.053958424975468745, 0.051779896549405596, 0.26139875872696994, 0.2191106882726131, 0.3616201177554562]
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.29 ]
 [0.424]
 [0.421]
 [0.421]
 [0.418]
 [0.429]] [[14.28 ]
 [26.705]
 [17.465]
 [17.636]
 [17.493]
 [19.861]
 [19.042]] [[0.496]
 [0.29 ]
 [0.424]
 [0.421]
 [0.421]
 [0.418]
 [0.429]]
maxi score, test score, baseline:  -0.9861108262108262 -0.9403333333333332 -0.9403333333333332
probs:  [0.05213211372008646, 0.053958424975468745, 0.051779896549405596, 0.26139875872696994, 0.2191106882726131, 0.3616201177554562]
line 256 mcts: sample exp_bonus 16.29181841498208
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.346]
 [0.374]
 [0.376]
 [0.377]
 [0.375]
 [0.378]] [[15.368]
 [18.668]
 [15.933]
 [16.619]
 [16.628]
 [16.896]
 [16.054]] [[0.471]
 [0.346]
 [0.374]
 [0.376]
 [0.377]
 [0.375]
 [0.378]]
printing an ep nov before normalisation:  16.11731712986712
printing an ep nov before normalisation:  39.700114727020264
printing an ep nov before normalisation:  15.458788098484728
maxi score, test score, baseline:  -0.9861108262108262 -0.9403333333333332 -0.9403333333333332
probs:  [0.05213211372008646, 0.053958424975468745, 0.051779896549405596, 0.26139875872696994, 0.2191106882726131, 0.3616201177554562]
using explorer policy with actor:  0
using explorer policy with actor:  1
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]] [[15.427]
 [15.427]
 [15.427]
 [15.427]
 [15.427]
 [15.427]
 [15.427]] [[0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]]
printing an ep nov before normalisation:  11.133041699572548
printing an ep nov before normalisation:  16.93509820947329
printing an ep nov before normalisation:  18.498249458405095
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.469]
 [0.47 ]] [[12.954]
 [11.6  ]
 [11.6  ]
 [11.6  ]
 [11.6  ]
 [16.263]
 [15.795]] [[0.568]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.469]
 [0.47 ]]
printing an ep nov before normalisation:  16.793889874207625
line 256 mcts: sample exp_bonus 32.19647215391796
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9861108262108262 -0.9403333333333332 -0.9403333333333332
maxi score, test score, baseline:  -0.9861108262108262 -0.9403333333333332 -0.9403333333333332
probs:  [0.05213211372008646, 0.053958424975468745, 0.051779896549405596, 0.26139875872696994, 0.2191106882726131, 0.3616201177554562]
using explorer policy with actor:  0
printing an ep nov before normalisation:  18.236010039699835
maxi score, test score, baseline:  -0.9861108262108262 -0.9403333333333332 -0.9403333333333332
probs:  [0.05213211372008646, 0.053958424975468745, 0.051779896549405596, 0.26139875872696994, 0.2191106882726131, 0.3616201177554562]
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.397]
 [0.432]
 [0.424]
 [0.425]
 [0.418]
 [0.419]] [[14.536]
 [20.188]
 [14.687]
 [15.687]
 [15.484]
 [16.882]
 [16.829]] [[0.529]
 [0.397]
 [0.432]
 [0.424]
 [0.425]
 [0.418]
 [0.419]]
printing an ep nov before normalisation:  19.8514768815855
maxi score, test score, baseline:  -0.9861402274342573 -0.9403333333333332 -0.9403333333333332
maxi score, test score, baseline:  -0.9861402274342573 -0.9403333333333332 -0.9403333333333332
probs:  [0.05213211372008646, 0.053958424975468745, 0.051779896549405596, 0.26139875872696994, 0.2191106882726131, 0.3616201177554562]
using explorer policy with actor:  0
siam score:  -0.7330104
maxi score, test score, baseline:  -0.9861402274342573 -0.9403333333333332 -0.9403333333333332
printing an ep nov before normalisation:  25.944921131208368
printing an ep nov before normalisation:  14.77634394481897
maxi score, test score, baseline:  -0.9861402274342573 -0.9403333333333332 -0.9403333333333332
probs:  [0.05213211372008646, 0.053958424975468745, 0.051779896549405596, 0.26139875872696994, 0.2191106882726131, 0.3616201177554562]
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]] [[21.246]
 [21.246]
 [21.246]
 [21.246]
 [21.246]
 [21.246]
 [21.246]] [[0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]]
maxi score, test score, baseline:  -0.9861402274342573 -0.9403333333333332 -0.9403333333333332
using explorer policy with actor:  0
printing an ep nov before normalisation:  20.65582456226891
printing an ep nov before normalisation:  22.425176022502455
maxi score, test score, baseline:  -0.9861402274342573 -0.9403333333333332 -0.9403333333333332
probs:  [0.05213211372008646, 0.053958424975468745, 0.051779896549405596, 0.26139875872696994, 0.2191106882726131, 0.3616201177554562]
printing an ep nov before normalisation:  20.201212615539337
using explorer policy with actor:  0
printing an ep nov before normalisation:  41.55990604945239
maxi score, test score, baseline:  -0.9861402274342573 -0.9403333333333332 -0.9403333333333332
probs:  [0.05213211372008646, 0.053958424975468745, 0.051779896549405596, 0.26139875872696994, 0.2191106882726131, 0.3616201177554562]
printing an ep nov before normalisation:  20.302880115971774
maxi score, test score, baseline:  -0.9861402274342573 -0.9403333333333332 -0.9403333333333332
probs:  [0.05213211372008646, 0.053958424975468745, 0.051779896549405596, 0.26139875872696994, 0.2191106882726131, 0.3616201177554562]
printing an ep nov before normalisation:  17.11578130722046
printing an ep nov before normalisation:  25.46020226160671
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]] [[15.1]
 [15.1]
 [15.1]
 [15.1]
 [15.1]
 [15.1]
 [15.1]] [[1.145]
 [1.145]
 [1.145]
 [1.145]
 [1.145]
 [1.145]
 [1.145]]
maxi score, test score, baseline:  -0.9861402274342573 -0.9403333333333332 -0.9403333333333332
probs:  [0.05213211372008646, 0.053958424975468745, 0.051779896549405596, 0.26139875872696994, 0.2191106882726131, 0.3616201177554562]
maxi score, test score, baseline:  -0.9861402274342573 -0.9403333333333332 -0.9403333333333332
probs:  [0.05213211372008646, 0.053958424975468745, 0.051779896549405596, 0.26139875872696994, 0.2191106882726131, 0.3616201177554562]
printing an ep nov before normalisation:  20.302880115971774
printing an ep nov before normalisation:  19.908886050243858
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.397]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.404]] [[11.222]
 [21.246]
 [18.424]
 [18.424]
 [18.424]
 [18.424]
 [18.431]] [[0.49 ]
 [0.397]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.404]]
maxi score, test score, baseline:  -0.9861402274342573 -0.9403333333333332 -0.9403333333333332
probs:  [0.05213211372008646, 0.053958424975468745, 0.051779896549405596, 0.26139875872696994, 0.2191106882726131, 0.3616201177554562]
printing an ep nov before normalisation:  25.433561842615802
printing an ep nov before normalisation:  24.53879080701185
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]] [[15.231]
 [13.857]
 [13.857]
 [13.857]
 [13.857]
 [13.857]
 [13.857]] [[0.502]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]]
printing an ep nov before normalisation:  21.06884704437352
printing an ep nov before normalisation:  14.403655529022217
printing an ep nov before normalisation:  21.949635721321847
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.421]
 [0.412]
 [0.415]
 [0.403]
 [0.386]
 [0.407]] [[16.437]
 [17.34 ]
 [14.416]
 [14.054]
 [15.81 ]
 [17.258]
 [17.351]] [[0.583]
 [0.421]
 [0.412]
 [0.415]
 [0.403]
 [0.386]
 [0.407]]
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]] [[14.259]
 [11.091]
 [11.091]
 [11.091]
 [11.091]
 [11.091]
 [11.091]] [[0.573]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]]
UNIT TEST: sample policy line 217 mcts : [0.02  0.388 0.041 0.102 0.347 0.02  0.082]
printing an ep nov before normalisation:  16.61385822806274
printing an ep nov before normalisation:  16.703511476516724
Printing some Q and Qe and total Qs values:  [[0.612]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.456]
 [0.481]] [[19.45 ]
 [13.931]
 [13.931]
 [13.931]
 [13.931]
 [20.96 ]
 [13.931]] [[0.612]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.456]
 [0.481]]
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.428]
 [0.468]
 [0.468]
 [0.468]
 [0.44 ]
 [0.468]] [[18.213]
 [17.742]
 [11.886]
 [11.886]
 [11.886]
 [18.707]
 [11.886]] [[0.621]
 [0.428]
 [0.468]
 [0.468]
 [0.468]
 [0.44 ]
 [0.468]]
maxi score, test score, baseline:  -0.9861402274342573 -0.9403333333333332 -0.9403333333333332
probs:  [0.05205077890640478, 0.053881845880536086, 0.05169764456139374, 0.2618623547427249, 0.2194641660547279, 0.36104320985421257]
Printing some Q and Qe and total Qs values:  [[0.62 ]
 [0.274]
 [0.412]
 [0.415]
 [0.412]
 [0.403]
 [0.409]] [[16.875]
 [17.748]
 [14.482]
 [14.189]
 [14.154]
 [14.433]
 [15.192]] [[0.62 ]
 [0.274]
 [0.412]
 [0.415]
 [0.412]
 [0.403]
 [0.409]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  28.153373729901627
maxi score, test score, baseline:  -0.9861402274342573 -0.9403333333333332 -0.9403333333333332
probs:  [0.05205077890640478, 0.053881845880536086, 0.05169764456139374, 0.2618623547427249, 0.2194641660547279, 0.36104320985421257]
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]] [[10.833]
 [10.833]
 [10.833]
 [10.833]
 [10.833]
 [10.833]
 [10.833]] [[0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]]
Printing some Q and Qe and total Qs values:  [[0.597]
 [0.427]
 [0.44 ]
 [0.46 ]
 [0.433]
 [0.432]
 [0.46 ]] [[19.005]
 [16.603]
 [16.2  ]
 [10.833]
 [17.402]
 [17.958]
 [10.833]] [[0.597]
 [0.427]
 [0.44 ]
 [0.46 ]
 [0.433]
 [0.432]
 [0.46 ]]
printing an ep nov before normalisation:  19.79550992405968
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]] [[11.317]
 [16.807]
 [16.807]
 [16.807]
 [16.807]
 [16.807]
 [16.807]] [[0.469]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]]
printing an ep nov before normalisation:  18.9864665535665
using explorer policy with actor:  0
printing an ep nov before normalisation:  21.43911912147752
printing an ep nov before normalisation:  24.282942136943664
Printing some Q and Qe and total Qs values:  [[0.554]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]] [[19.463]
 [16.161]
 [16.161]
 [16.161]
 [16.161]
 [16.161]
 [16.161]] [[0.554]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]] [[25.498]
 [21.439]
 [21.439]
 [21.439]
 [21.439]
 [21.439]
 [21.439]] [[0.493]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]]
printing an ep nov before normalisation:  20.25546857486397
maxi score, test score, baseline:  -0.9861402274342573 -0.9403333333333332 -0.9403333333333332
probs:  [0.05205077890640478, 0.053881845880536086, 0.05169764456139374, 0.2618623547427249, 0.2194641660547279, 0.36104320985421257]
printing an ep nov before normalisation:  16.160515653372745
maxi score, test score, baseline:  -0.9861402274342573 -0.9403333333333332 -0.9403333333333332
probs:  [0.05196959463309069, 0.05380540852374212, 0.05161554481132221, 0.2623250927013766, 0.21981698959454662, 0.36046736973592175]
Printing some Q and Qe and total Qs values:  [[0.169]
 [0.229]
 [0.191]
 [0.212]
 [0.186]
 [0.178]
 [0.175]] [[28.473]
 [39.939]
 [29.324]
 [37.463]
 [28.306]
 [27.997]
 [27.684]] [[0.754]
 [1.281]
 [0.811]
 [1.164]
 [0.764]
 [0.743]
 [0.728]]
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.239]
 [0.379]
 [0.365]
 [0.358]
 [0.327]
 [0.342]] [[19.119]
 [23.948]
 [18.55 ]
 [22.062]
 [21.213]
 [21.925]
 [22.762]] [[0.471]
 [0.239]
 [0.379]
 [0.365]
 [0.358]
 [0.327]
 [0.342]]
printing an ep nov before normalisation:  23.10711741524255
maxi score, test score, baseline:  -0.986621536351166 -0.857 -0.857
probs:  [0.05196840613322224, 0.05374932481031509, 0.05162494324549719, 0.2612270903210527, 0.2208894506409833, 0.3605407848489296]
Printing some Q and Qe and total Qs values:  [[0.181]
 [0.181]
 [0.181]
 [0.181]
 [0.181]
 [0.181]
 [0.181]] [[38.859]
 [38.859]
 [38.859]
 [38.859]
 [38.859]
 [38.859]
 [38.859]] [[1.385]
 [1.385]
 [1.385]
 [1.385]
 [1.385]
 [1.385]
 [1.385]]
printing an ep nov before normalisation:  49.788776266136274
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.361]
 [0.366]
 [0.363]
 [0.362]
 [0.358]
 [0.359]] [[41.582]
 [50.681]
 [50.609]
 [51.355]
 [50.268]
 [49.315]
 [49.916]] [[1.196]
 [1.582]
 [1.583]
 [1.615]
 [1.564]
 [1.517]
 [1.545]]
maxi score, test score, baseline:  -0.986621536351166 -0.857 -0.857
probs:  [0.05196840613322224, 0.05374932481031509, 0.05162494324549719, 0.2612270903210526, 0.2208894506409833, 0.3605407848489296]
maxi score, test score, baseline:  -0.986621536351166 -0.857 -0.857
probs:  [0.05203491186989648, 0.05381811550693333, 0.05169100831132507, 0.2615620798001101, 0.2198906884115994, 0.3610031961001357]
maxi score, test score, baseline:  -0.986621536351166 -0.857 -0.857
probs:  [0.05210083835557177, 0.05388630705103141, 0.05175649796430456, 0.26189415159115925, 0.2189006251914737, 0.3614615798464594]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.986621536351166 -0.857 -0.857
printing an ep nov before normalisation:  16.58696123187724
using another actor
printing an ep nov before normalisation:  41.64210840376802
maxi score, test score, baseline:  -0.986621536351166 -0.857 -0.857
maxi score, test score, baseline:  -0.986621536351166 -0.857 -0.857
probs:  [0.05210083835557177, 0.05388630705103141, 0.05175649796430456, 0.26189415159115914, 0.2189006251914737, 0.3614615798464594]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.986621536351166 -0.857 -0.857
printing an ep nov before normalisation:  50.48516612036407
maxi score, test score, baseline:  -0.986621536351166 -0.857 -0.857
probs:  [0.05195900859241938, 0.05338645161457137, 0.05161276869270095, 0.26290961903043336, 0.21967892450056573, 0.3604532275693092]
Printing some Q and Qe and total Qs values:  [[0.221]
 [0.236]
 [0.221]
 [0.221]
 [0.221]
 [0.221]
 [0.221]] [[69.894]
 [71.52 ]
 [69.894]
 [69.894]
 [69.894]
 [69.894]
 [69.894]] [[2.14 ]
 [2.236]
 [2.14 ]
 [2.14 ]
 [2.14 ]
 [2.14 ]
 [2.14 ]]
maxi score, test score, baseline:  -0.986621536351166 -0.857 -0.857
probs:  [0.05195900859241938, 0.05338645161457137, 0.05161276869270095, 0.26290961903043336, 0.21967892450056573, 0.3604532275693092]
siam score:  -0.7330685
maxi score, test score, baseline:  -0.9866488021902806 -0.857 -0.857
probs:  [0.05195900859241938, 0.05338645161457137, 0.05161276869270095, 0.26290961903043336, 0.21967892450056573, 0.3604532275693092]
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.447]
 [0.383]
 [0.293]
 [0.267]
 [0.383]
 [0.299]] [[39.239]
 [43.202]
 [36.706]
 [37.435]
 [37.595]
 [36.706]
 [36.533]] [[1.606]
 [1.997]
 [1.493]
 [1.453]
 [1.438]
 [1.493]
 [1.398]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.986675956284153 -0.857 -0.857
probs:  [0.051718828324109956, 0.053157187276151846, 0.051369940661635506, 0.2642826184393774, 0.22072133051750994, 0.3587500947812154]
printing an ep nov before normalisation:  25.66428962276767
maxi score, test score, baseline:  -0.9867029993183368 -0.857 -0.857
probs:  [0.051639064369195915, 0.05308104850554505, 0.05128929738374218, 0.2647385920450481, 0.22106751393683455, 0.3581844837596342]
maxi score, test score, baseline:  -0.9867029993183368 -0.857 -0.857
probs:  [0.051639064369195915, 0.05308104850554505, 0.05128929738374218, 0.2647385920450481, 0.22106751393683455, 0.3581844837596342]
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.286]
 [0.283]
 [0.288]
 [0.284]
 [0.284]
 [0.284]] [[24.895]
 [29.158]
 [24.507]
 [25.015]
 [24.655]
 [24.655]
 [24.655]] [[0.286]
 [0.286]
 [0.283]
 [0.288]
 [0.284]
 [0.284]
 [0.284]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  22.44660225252736
printing an ep nov before normalisation:  14.652524703159438
siam score:  -0.7335012
maxi score, test score, baseline:  -0.9867029993183368 -0.857 -0.857
Printing some Q and Qe and total Qs values:  [[0.348]
 [0.448]
 [0.453]
 [0.45 ]
 [0.498]
 [0.458]
 [0.494]] [[28.577]
 [29.034]
 [28.714]
 [30.612]
 [31.813]
 [30.757]
 [29.949]] [[1.308]
 [1.44 ]
 [1.422]
 [1.552]
 [1.684]
 [1.57 ]
 [1.55 ]]
maxi score, test score, baseline:  -0.9867029993183368 -0.857 -0.857
probs:  [0.051639064369195915, 0.05308104850554505, 0.05128929738374218, 0.2647385920450482, 0.22106751393683455, 0.3581844837596342]
printing an ep nov before normalisation:  37.478248745331214
printing an ep nov before normalisation:  22.45998535416532
printing an ep nov before normalisation:  14.077978594097932
maxi score, test score, baseline:  -0.9867029993183368 -0.857 -0.857
line 256 mcts: sample exp_bonus 24.706508749444772
maxi score, test score, baseline:  -0.9867029993183368 -0.857 -0.857
probs:  [0.05170486051246357, 0.05314868671175573, 0.051354646717099536, 0.26507661222547096, 0.22007331741881903, 0.3586418764143911]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9867299319727891 -0.857 -0.857
probs:  [0.05170486051246357, 0.05314868671175573, 0.051354646717099536, 0.26507661222547096, 0.22007331741881903, 0.3586418764143911]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  43.36620781395325
printing an ep nov before normalisation:  32.316144072154486
actions average: 
K:  1  action  0 :  tensor([0.7411, 0.0463, 0.0303, 0.0361, 0.0608, 0.0394, 0.0460],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0126, 0.8694, 0.0170, 0.0132, 0.0089, 0.0129, 0.0660],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0855, 0.0746, 0.3229, 0.1099, 0.1046, 0.1698, 0.1327],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1225, 0.0372, 0.1011, 0.2716, 0.1515, 0.1709, 0.1451],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2153, 0.0072, 0.0788, 0.1062, 0.3704, 0.1120, 0.1101],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1110, 0.0023, 0.0974, 0.1294, 0.1180, 0.4161, 0.1257],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2370, 0.0359, 0.0928, 0.1154, 0.1231, 0.1257, 0.2701],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  37.344607445466536
maxi score, test score, baseline:  -0.986756754921928 -0.857 -0.857
maxi score, test score, baseline:  -0.986756754921928 -0.857 -0.857
probs:  [0.05170486051246357, 0.05314868671175573, 0.051354646717099536, 0.26507661222547096, 0.22007331741881903, 0.3586418764143911]
maxi score, test score, baseline:  -0.986756754921928 -0.857 -0.857
probs:  [0.051625307298043356, 0.053072761245639234, 0.051274213557778274, 0.2655331754845688, 0.2204168056925229, 0.3580777367214475]
actions average: 
K:  2  action  0 :  tensor([0.7072, 0.0400, 0.0425, 0.0451, 0.0529, 0.0509, 0.0613],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0128, 0.8960, 0.0103, 0.0326, 0.0133, 0.0141, 0.0209],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0647, 0.2145, 0.2232, 0.1037, 0.1077, 0.1728, 0.1133],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1886, 0.0138, 0.0746, 0.2950, 0.1549, 0.1665, 0.1065],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1650, 0.0094, 0.0520, 0.0774, 0.5357, 0.0837, 0.0768],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0937, 0.0881, 0.1038, 0.1124, 0.1171, 0.3557, 0.1293],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0818, 0.0851, 0.0693, 0.1811, 0.1182, 0.1055, 0.3589],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  26.648266026892767
maxi score, test score, baseline:  -0.986756754921928 -0.857 -0.857
probs:  [0.051644303435832595, 0.05272367248976749, 0.051293080172250614, 0.26563108518139816, 0.22049807133908222, 0.3582097873816688]
maxi score, test score, baseline:  -0.9867834688346884 -0.857 -0.857
probs:  [0.051644303435832595, 0.05272367248976749, 0.051293080172250614, 0.26563108518139816, 0.22049807133908222, 0.3582097873816688]
maxi score, test score, baseline:  -0.9867834688346884 -0.857 -0.857
probs:  [0.051709645475671265, 0.052790383714260466, 0.05135797668374938, 0.26596787044700304, 0.21951011451863955, 0.3586640091606764]
Printing some Q and Qe and total Qs values:  [[0.311]
 [0.276]
 [0.311]
 [0.308]
 [0.29 ]
 [0.311]
 [0.311]] [[37.307]
 [47.137]
 [37.307]
 [48.522]
 [50.479]
 [37.307]
 [37.307]] [[1.277]
 [1.7  ]
 [1.277]
 [1.796]
 [1.869]
 [1.277]
 [1.277]]
printing an ep nov before normalisation:  32.12039147955123
printing an ep nov before normalisation:  37.840037232090665
maxi score, test score, baseline:  -0.9867834688346884 -0.857 -0.857
probs:  [0.051709645475671265, 0.052790383714260466, 0.05135797668374938, 0.26596787044700304, 0.21951011451863955, 0.3586640091606764]
printing an ep nov before normalisation:  33.04445822694761
maxi score, test score, baseline:  -0.9867834688346884 -0.857 -0.857
probs:  [0.05163031955954979, 0.05271376590470531, 0.051277769558348396, 0.2664254313414091, 0.2198512618825593, 0.3581014517534281]
printing an ep nov before normalisation:  60.73163401979742
maxi score, test score, baseline:  -0.9867834688346884 -0.857 -0.857
maxi score, test score, baseline:  -0.9867834688346884 -0.857 -0.857
probs:  [0.05163031955954979, 0.05271376590470531, 0.051277769558348396, 0.2664254313414091, 0.2198512618825593, 0.3581014517534281]
printing an ep nov before normalisation:  22.899908898480206
siam score:  -0.7369022
maxi score, test score, baseline:  -0.9867834688346884 -0.857 -0.857
probs:  [0.05163031955954979, 0.05271376590470531, 0.051277769558348396, 0.2664254313414091, 0.2198512618825593, 0.3581014517534281]
Printing some Q and Qe and total Qs values:  [[0.663]
 [0.663]
 [0.663]
 [0.663]
 [0.663]
 [0.663]
 [0.663]] [[28.192]
 [28.192]
 [28.192]
 [28.192]
 [28.192]
 [28.192]
 [28.192]] [[2.109]
 [2.109]
 [2.109]
 [2.109]
 [2.109]
 [2.109]
 [2.109]]
printing an ep nov before normalisation:  29.211502075195312
printing an ep nov before normalisation:  11.74065977901904
maxi score, test score, baseline:  -0.9867834688346884 -0.857 -0.857
probs:  [0.05163031955954979, 0.05271376590470531, 0.051277769558348396, 0.26642543134140906, 0.2198512618825593, 0.3581014517534281]
actor:  1 policy actor:  1  step number:  91 total reward:  0.01333333333333242  reward:  1.0 rdn_beta:  2.0
from probs:  [0.05163031955954979, 0.05271376590470531, 0.051277769558348396, 0.26642543134140906, 0.2198512618825593, 0.3581014517534281]
maxi score, test score, baseline:  -0.9868365721997301 -0.857 -0.857
probs:  [0.05395998803015484, 0.05497467126700196, 0.05330354601499536, 0.25512269466190124, 0.2103275769535575, 0.3723115230723892]
Printing some Q and Qe and total Qs values:  [[0.37 ]
 [0.362]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]] [[29.378]
 [52.507]
 [29.378]
 [29.378]
 [29.378]
 [29.378]
 [29.378]] [[0.741]
 [1.301]
 [0.741]
 [0.741]
 [0.741]
 [0.741]
 [0.741]]
maxi score, test score, baseline:  -0.9868365721997301 -0.857 -0.857
printing an ep nov before normalisation:  33.078608590658355
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.278]
 [0.242]
 [0.3  ]
 [0.324]
 [0.25 ]
 [0.268]] [[32.573]
 [32.857]
 [30.5  ]
 [31.858]
 [40.599]
 [32.826]
 [30.943]] [[1.183]
 [1.148]
 [0.981]
 [1.114]
 [1.627]
 [1.118]
 [1.031]]
maxi score, test score, baseline:  -0.9868365721997301 -0.857 -0.857
probs:  [0.053885241074628235, 0.05490250347106324, 0.053227130490741285, 0.2555592705554112, 0.21065029095092838, 0.37177556345722756]
maxi score, test score, baseline:  -0.9868365721997301 -0.857 -0.857
probs:  [0.053885241074628235, 0.05490250347106324, 0.053227130490741285, 0.2555592705554112, 0.21065029095092838, 0.37177556345722756]
maxi score, test score, baseline:  -0.9868365721997301 -0.857 -0.857
probs:  [0.05388524107462824, 0.05490250347106325, 0.05322713049074129, 0.2555592705554112, 0.2106502909509284, 0.3717755634572276]
maxi score, test score, baseline:  -0.9868365721997301 -0.857 -0.857
probs:  [0.05388524107462824, 0.05490250347106325, 0.05322713049074129, 0.2555592705554112, 0.2106502909509284, 0.3717755634572276]
printing an ep nov before normalisation:  28.089377657345178
maxi score, test score, baseline:  -0.9868365721997301 -0.857 -0.857
probs:  [0.05388524107462824, 0.05490250347106325, 0.05322713049074129, 0.2555592705554112, 0.2106502909509284, 0.3717755634572276]
printing an ep nov before normalisation:  32.121553830714326
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.264]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]] [[37.585]
 [46.865]
 [37.585]
 [37.585]
 [37.585]
 [37.585]
 [37.585]] [[1.25 ]
 [1.638]
 [1.25 ]
 [1.25 ]
 [1.25 ]
 [1.25 ]
 [1.25 ]]
maxi score, test score, baseline:  -0.9868365721997301 -0.857 -0.857
probs:  [0.05394808775309852, 0.05496653952938383, 0.053289207708480016, 0.2558579138462743, 0.2097281647069995, 0.3722100864557638]
printing an ep nov before normalisation:  30.140732915914423
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.0
siam score:  -0.73248655
maxi score, test score, baseline:  -0.9868365721997301 -0.857 -0.857
maxi score, test score, baseline:  -0.9868365721997301 -0.857 -0.857
probs:  [0.05407216172932515, 0.0550929616183646, 0.05341176259006493, 0.2564475052371435, 0.20790767279792735, 0.3730679360271746]
maxi score, test score, baseline:  -0.9868365721997301 -0.857 -0.857
probs:  [0.05407216172932515, 0.0550929616183646, 0.05341176259006493, 0.2564475052371435, 0.20790767279792735, 0.3730679360271746]
maxi score, test score, baseline:  -0.9868365721997301 -0.857 -0.857
maxi score, test score, baseline:  -0.9868365721997301 -0.857 -0.857
probs:  [0.053997757333627536, 0.0550211499284864, 0.053335680862002875, 0.2568871092543105, 0.20822399162728278, 0.37253431099428974]
maxi score, test score, baseline:  -0.9868365721997301 -0.857 -0.857
probs:  [0.053997757333627536, 0.0550211499284864, 0.053335680862002875, 0.2568871092543105, 0.20822399162728278, 0.37253431099428974]
maxi score, test score, baseline:  -0.9868365721997301 -0.857 -0.857
probs:  [0.0539234788845474, 0.054949459796472484, 0.05325972791983058, 0.2573259691401873, 0.20853977501399112, 0.3720015892449712]
printing an ep nov before normalisation:  17.886115253450967
maxi score, test score, baseline:  -0.986862962962963 -0.857 -0.857
probs:  [0.0539234788845474, 0.054949459796472484, 0.05325972791983058, 0.2573259691401873, 0.20853977501399112, 0.3720015892449712]
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.49346387983237
maxi score, test score, baseline:  -0.986862962962963 -0.857 -0.857
probs:  [0.0539234788845474, 0.054949459796472484, 0.05325972791983058, 0.2573259691401873, 0.20853977501399112, 0.3720015892449712]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.986862962962963 -0.857 -0.857
maxi score, test score, baseline:  -0.986862962962963 -0.857 -0.857
probs:  [0.0539234788845474, 0.054949459796472484, 0.05325972791983058, 0.2573259691401873, 0.20853977501399112, 0.3720015892449712]
printing an ep nov before normalisation:  35.347017078370456
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.986862962962963 -0.857 -0.857
probs:  [0.0539234788845474, 0.054949459796472484, 0.05325972791983058, 0.2573259691401873, 0.20853977501399112, 0.3720015892449712]
maxi score, test score, baseline:  -0.986862962962963 -0.857 -0.857
probs:  [0.0539234788845474, 0.054949459796472484, 0.05325972791983058, 0.2573259691401873, 0.20853977501399112, 0.3720015892449712]
maxi score, test score, baseline:  -0.986889247311828 -0.857 -0.857
probs:  [0.05384932606256454, 0.054877890913936724, 0.05318390343682475, 0.25776408678259677, 0.2088550243164429, 0.37146976848763424]
maxi score, test score, baseline:  -0.986889247311828 -0.857 -0.857
printing an ep nov before normalisation:  42.65336513519287
printing an ep nov before normalisation:  35.04504456877214
printing an ep nov before normalisation:  31.076274149411244
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  57 total reward:  0.2666666666666664  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.986889247311828 -0.857 -0.857
probs:  [0.05684253977661778, 0.05777367345705329, 0.055944269637844705, 0.2414414023481575, 0.19716526819871122, 0.3908328465816155]
maxi score, test score, baseline:  -0.986889247311828 -0.857 -0.857
probs:  [0.05684253977661778, 0.05777367345705329, 0.055944269637844705, 0.2414414023481575, 0.19716526819871122, 0.3908328465816155]
Printing some Q and Qe and total Qs values:  [[0.363]
 [0.357]
 [0.363]
 [0.358]
 [0.34 ]
 [0.337]
 [0.337]] [[47.255]
 [52.371]
 [47.255]
 [52.183]
 [53.772]
 [54.312]
 [56.778]] [[1.139]
 [1.297]
 [1.139]
 [1.292]
 [1.324]
 [1.339]
 [1.418]]
printing an ep nov before normalisation:  44.78376593499586
actions average: 
K:  3  action  0 :  tensor([0.7465, 0.0592, 0.0362, 0.0434, 0.0419, 0.0430, 0.0298],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0047, 0.9314, 0.0087, 0.0074, 0.0018, 0.0025, 0.0435],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0594, 0.0225, 0.4863, 0.0958, 0.0747, 0.1859, 0.0754],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1931, 0.0732, 0.0874, 0.2645, 0.1505, 0.1267, 0.1046],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1620, 0.0132, 0.0631, 0.0978, 0.4966, 0.0932, 0.0742],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0906, 0.0606, 0.2051, 0.1592, 0.1164, 0.2703, 0.0979],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1102, 0.0771, 0.1205, 0.1433, 0.1399, 0.1380, 0.2711],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.986889247311828 -0.857 -0.857
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.41 ]
 [0.386]
 [0.39 ]
 [0.389]
 [0.387]
 [0.389]] [[12.012]
 [38.581]
 [12.012]
 [12.706]
 [19.022]
 [13.031]
 [12.573]] [[0.386]
 [0.41 ]
 [0.386]
 [0.39 ]
 [0.389]
 [0.387]
 [0.389]]
printing an ep nov before normalisation:  15.190667027072383
siam score:  -0.72821045
siam score:  -0.72545296
printing an ep nov before normalisation:  35.20731030501633
printing an ep nov before normalisation:  41.00383758544922
maxi score, test score, baseline:  -0.9869154258886653 -0.857 -0.857
maxi score, test score, baseline:  -0.9869154258886653 -0.857 -0.857
probs:  [0.056706574703906394, 0.05764248146217798, 0.05580369994886794, 0.24225170821163647, 0.19774861048111045, 0.3898469251923008]
printing an ep nov before normalisation:  29.59261527777082
maxi score, test score, baseline:  -0.9869154258886653 -0.857 -0.857
probs:  [0.056638748106002224, 0.05757703592894502, 0.055733576323869174, 0.2426559318029279, 0.19803961258657607, 0.3893550952516796]
printing an ep nov before normalisation:  43.36619478616487
maxi score, test score, baseline:  -0.9869154258886653 -0.857 -0.857
probs:  [0.05657102514936031, 0.05751169039862956, 0.05566355985006529, 0.24305953772733727, 0.198330170031297, 0.3888640168433106]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9869154258886653 -0.857 -0.857
probs:  [0.05650340559661117, 0.05744644464219497, 0.05559365028204797, 0.24346252739950683, 0.19862028383367944, 0.38837368824595964]
maxi score, test score, baseline:  -0.9869154258886653 -0.857 -0.857
probs:  [0.05650340559661117, 0.05744644464219497, 0.05559365028204797, 0.24346252739950683, 0.19862028383367944, 0.38837368824595964]
maxi score, test score, baseline:  -0.9869154258886653 -0.857 -0.857
probs:  [0.05650340559661117, 0.05744644464219497, 0.05559365028204797, 0.24346252739950683, 0.19862028383367944, 0.38837368824595964]
printing an ep nov before normalisation:  11.754551747470785
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9869154258886653 -0.857 -0.857
probs:  [0.05650340559661117, 0.05744644464219497, 0.05559365028204797, 0.24346252739950677, 0.19862028383367944, 0.38837368824595964]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9869154258886653 -0.857 -0.857
maxi score, test score, baseline:  -0.9869154258886653 -0.857 -0.857
probs:  [0.05650340559661117, 0.05744644464219497, 0.05559365028204797, 0.24346252739950677, 0.19862028383367944, 0.38837368824595964]
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.7712459564209
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9869154258886653 -0.857 -0.857
probs:  [0.05650340559661117, 0.05744644464219497, 0.05559365028204797, 0.24346252739950683, 0.19862028383367944, 0.38837368824595964]
maxi score, test score, baseline:  -0.9869154258886653 -0.857 -0.857
probs:  [0.05650340559661117, 0.05744644464219497, 0.05559365028204797, 0.24346252739950683, 0.19862028383367944, 0.38837368824595964]
printing an ep nov before normalisation:  12.098908424377441
printing an ep nov before normalisation:  18.257596415331534
maxi score, test score, baseline:  -0.9869154258886653 -0.857 -0.857
printing an ep nov before normalisation:  31.32621587432025
maxi score, test score, baseline:  -0.986941499330656 -0.857 -0.857
maxi score, test score, baseline:  -0.986941499330656 -0.857 -0.857
probs:  [0.056513104004406046, 0.05714041611495919, 0.05559981137286544, 0.24419916157238997, 0.19813162852905875, 0.38841587840632064]
printing an ep nov before normalisation:  21.137375919174133
printing an ep nov before normalisation:  27.37540228459568
printing an ep nov before normalisation:  26.514398372958166
printing an ep nov before normalisation:  17.45169235731551
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.80878084134677
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.9869674682698731 -0.857 -0.857
probs:  [0.05653091428054406, 0.05684277932786559, 0.05561733314191981, 0.24427626145105, 0.19819417577787135, 0.3885385360207493]
printing an ep nov before normalisation:  23.55426788330078
printing an ep nov before normalisation:  27.083659172058105
maxi score, test score, baseline:  -0.9869674682698731 -0.857 -0.857
probs:  [0.05653091428054406, 0.05684277932786559, 0.05561733314191981, 0.24427626145105, 0.19819417577787135, 0.3885385360207493]
maxi score, test score, baseline:  -0.9869674682698731 -0.857 -0.857
probs:  [0.05658967378809611, 0.05690186376073651, 0.05567514080942014, 0.24453062869764863, 0.19735948599261907, 0.3889432069514795]
maxi score, test score, baseline:  -0.9869674682698731 -0.857 -0.857
maxi score, test score, baseline:  -0.9869674682698731 -0.857 -0.857
maxi score, test score, baseline:  -0.9869933333333334 -0.857 -0.857
probs:  [0.05676897983995842, 0.05708139404498931, 0.055555862692937906, 0.24484492450986073, 0.19763990083805497, 0.38810893807419866]
printing an ep nov before normalisation:  33.18409303843997
maxi score, test score, baseline:  -0.9869933333333334 -0.857 -0.857
probs:  [0.056702604712652295, 0.057015798554768064, 0.05548646020291623, 0.2452478973791488, 0.19792507244717428, 0.38762216670334043]
printing an ep nov before normalisation:  33.903472168638444
using another actor
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.6233],
        [-0.5004],
        [-0.0000],
        [-0.0000],
        [-0.3730],
        [-0.4193],
        [-0.0000],
        [-0.0000],
        [-0.3250],
        [-0.5143]], dtype=torch.float64)
-0.032346567066 -0.6556072261628327
-0.032346567066 -0.5327704319811872
-0.8058184743179999 -0.8058184743179999
-0.6347761596 -0.6347761596
-0.032346567066 -0.4053706836410634
-0.09703970119800001 -0.5163734830306048
-0.06443047379999879 -0.06443047379999879
-0.8756220000000001 -0.8756220000000001
-0.032346567066 -0.3573260199264846
-0.032346567066 -0.5466879000901694
Printing some Q and Qe and total Qs values:  [[0.253]
 [0.202]
 [0.195]
 [0.253]
 [0.253]
 [0.253]
 [0.253]] [[39.443]
 [44.518]
 [40.199]
 [39.443]
 [39.443]
 [39.443]
 [39.443]] [[1.47 ]
 [1.778]
 [1.466]
 [1.47 ]
 [1.47 ]
 [1.47 ]
 [1.47 ]]
printing an ep nov before normalisation:  32.71055423645553
maxi score, test score, baseline:  -0.9869933333333334 -0.857 -0.857
probs:  [0.05681921334449627, 0.057133052785109814, 0.05560056194936535, 0.24575316166553654, 0.1962730977152471, 0.3884209125402449]
maxi score, test score, baseline:  -0.9869933333333334 -0.857 -0.857
maxi score, test score, baseline:  -0.9869933333333334 -0.857 -0.857
probs:  [0.05706301721560538, 0.0573763007028479, 0.05555124224809789, 0.24566227699251786, 0.1962698648947093, 0.3880772979462217]
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.38 ]
 [0.388]
 [0.388]] [[36.29 ]
 [36.29 ]
 [36.29 ]
 [36.29 ]
 [23.536]
 [36.29 ]
 [36.29 ]] [[4.502]
 [4.502]
 [4.502]
 [4.502]
 [2.38 ]
 [4.502]
 [4.502]]
maxi score, test score, baseline:  -0.9869933333333334 -0.857 -0.857
probs:  [0.05706301721560538, 0.0573763007028479, 0.05555124224809789, 0.24566227699251786, 0.1962698648947093, 0.3880772979462217]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.19143408707572
printing an ep nov before normalisation:  32.85610072421801
printing an ep nov before normalisation:  32.436447483087896
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05706301721560538, 0.0573763007028479, 0.05555124224809789, 0.24566227699251786, 0.1962698648947093, 0.3880772979462217]
from probs:  [0.05699761987110773, 0.057311682258576996, 0.0554820862571572, 0.24606578404990326, 0.1965505702004883, 0.3875922573627666]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.057055397578093914, 0.057369779072188674, 0.05553832408914831, 0.24631566659422488, 0.19573489319229823, 0.3879859394740461]
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.577]
 [0.515]
 [0.515]
 [0.519]
 [0.547]
 [0.555]] [[22.509]
 [27.406]
 [22.509]
 [22.509]
 [25.22 ]
 [25.516]
 [30.61 ]] [[1.372]
 [1.799]
 [1.372]
 [1.372]
 [1.578]
 [1.628]
 [2.015]]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.057055397578093914, 0.057369779072188674, 0.05553832408914831, 0.24631566659422488, 0.19573489319229823, 0.3879859394740461]
using another actor
printing an ep nov before normalisation:  43.280204015444006
printing an ep nov before normalisation:  35.67912044097744
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05699017601727344, 0.05730533681479349, 0.05546934193621741, 0.24671959216415573, 0.19601343679571248, 0.38750211627184755]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05699017601727344, 0.05730533681479349, 0.05546934193621741, 0.24671959216415573, 0.19601343679571248, 0.38750211627184755]
printing an ep nov before normalisation:  36.26957527560217
printing an ep nov before normalisation:  28.5514635372369
printing an ep nov before normalisation:  21.689023662515886
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05699017601727344, 0.05730533681479349, 0.05546934193621741, 0.24671959216415573, 0.19601343679571248, 0.38750211627184755]
Printing some Q and Qe and total Qs values:  [[0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.576]
 [0.322]] [[40.667]
 [40.667]
 [40.667]
 [40.667]
 [40.667]
 [45.4  ]
 [40.667]] [[1.592]
 [1.592]
 [1.592]
 [1.592]
 [1.592]
 [2.078]
 [1.592]]
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.545]
 [0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.503]] [[48.578]
 [51.919]
 [48.578]
 [48.578]
 [48.578]
 [48.578]
 [48.578]] [[2.203]
 [2.417]
 [2.203]
 [2.203]
 [2.203]
 [2.203]
 [2.203]]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05692505286102619, 0.057240991786179043, 0.05540046386174207, 0.24712290830182473, 0.19629156013987387, 0.3870190230493541]
printing an ep nov before normalisation:  44.56352233886719
printing an ep nov before normalisation:  62.07111815387085
printing an ep nov before normalisation:  35.10148143052806
printing an ep nov before normalisation:  44.26519684969137
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05673027159769921, 0.057048537877545574, 0.055194451758905735, 0.24832921388254758, 0.19712341810650574, 0.3855741067767961]
from probs:  [0.056600905382992, 0.056920717402206895, 0.05505762645305959, 0.2491303955983492, 0.19767590613579097, 0.3846144490276013]
printing an ep nov before normalisation:  61.17889194313643
using explorer policy with actor:  0
printing an ep nov before normalisation:  62.36195325413744
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
printing an ep nov before normalisation:  41.212109091030555
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05653636800427702, 0.056856951151929506, 0.05498936793130282, 0.24953008393992446, 0.19795152778292788, 0.3841357011896383]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05653636800427702, 0.056856951151929506, 0.05498936793130282, 0.24953008393992446, 0.19795152778292788, 0.3841357011896383]
Printing some Q and Qe and total Qs values:  [[0.229]
 [0.218]
 [0.192]
 [0.205]
 [0.196]
 [0.172]
 [0.157]] [[27.796]
 [26.942]
 [27.799]
 [28.06 ]
 [27.784]
 [28.414]
 [27.457]] [[1.742]
 [1.636]
 [1.706]
 [1.748]
 [1.708]
 [1.754]
 [1.633]]
printing an ep nov before normalisation:  24.786314835183564
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05653636800427702, 0.056856951151929506, 0.05498936793130282, 0.24953008393992446, 0.19795152778292788, 0.3841357011896383]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05647192748636573, 0.05679328060510913, 0.05492121185522028, 0.2499291724100027, 0.19822673576383312, 0.3836576718794691]
printing an ep nov before normalisation:  41.73709512184825
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
printing an ep nov before normalisation:  47.05659286975718
from probs:  [0.05647192748636573, 0.05679328060510913, 0.05492121185522028, 0.2499291724100027, 0.19822673576383312, 0.3836576718794691]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05652988410658661, 0.056851567806980224, 0.054977573226780235, 0.2501861419276964, 0.19740262265664385, 0.3840522102753128]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05652988410658661, 0.056851567806980224, 0.054977573226780235, 0.2501861419276964, 0.19740262265664385, 0.3840522102753128]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05652988410658661, 0.056851567806980224, 0.054977573226780235, 0.2501861419276964, 0.19740262265664385, 0.3840522102753128]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05652988410658661, 0.056851567806980224, 0.054977573226780235, 0.2501861419276964, 0.19740262265664385, 0.3840522102753128]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05652988410658661, 0.056851567806980224, 0.054977573226780235, 0.2501861419276964, 0.19740262265664385, 0.3840522102753128]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05652988410658661, 0.056851567806980224, 0.054977573226780235, 0.2501861419276964, 0.19740262265664385, 0.3840522102753128]
printing an ep nov before normalisation:  33.45932335807384
printing an ep nov before normalisation:  48.76232325840952
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05652988410658661, 0.056851567806980224, 0.054977573226780235, 0.2501861419276964, 0.19740262265664385, 0.3840522102753128]
actions average: 
K:  2  action  0 :  tensor([0.7335, 0.0042, 0.0404, 0.0477, 0.0571, 0.0525, 0.0646],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0403, 0.8223, 0.0276, 0.0145, 0.0151, 0.0362, 0.0440],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1238, 0.0055, 0.2877, 0.1574, 0.1344, 0.1553, 0.1360],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1498, 0.0085, 0.1080, 0.2536, 0.1612, 0.1593, 0.1596],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1315, 0.0410, 0.0719, 0.1207, 0.4047, 0.1131, 0.1171],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0973, 0.0105, 0.1552, 0.1052, 0.1222, 0.3869, 0.1227],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1511, 0.0292, 0.1221, 0.1720, 0.1544, 0.1715, 0.1996],
       grad_fn=<DivBackward0>)
using another actor
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05652988410658661, 0.056851567806980224, 0.054977573226780235, 0.2501861419276964, 0.19740262265664385, 0.3840522102753128]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05646561268178447, 0.056788066783025566, 0.05490958416998155, 0.2505856582079036, 0.19767572759574353, 0.3835753505615612]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05646561268178447, 0.056788066783025566, 0.05490958416998155, 0.2505856582079036, 0.19767572759574353, 0.3835753505615612]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
printing an ep nov before normalisation:  34.081687622482235
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05646561268178447, 0.056788066783025566, 0.05490958416998155, 0.2505856582079036, 0.19767572759574353, 0.3835753505615612]
Printing some Q and Qe and total Qs values:  [[0.062]
 [0.228]
 [0.18 ]
 [0.183]
 [0.196]
 [0.178]
 [0.159]] [[30.549]
 [31.119]
 [29.82 ]
 [29.287]
 [31.853]
 [29.681]
 [27.084]] [[1.404]
 [1.615]
 [1.464]
 [1.425]
 [1.64 ]
 [1.451]
 [1.229]]
Printing some Q and Qe and total Qs values:  [[0.161]
 [0.288]
 [0.161]
 [0.186]
 [0.18 ]
 [0.165]
 [0.167]] [[34.414]
 [34.655]
 [34.414]
 [34.966]
 [33.988]
 [34.132]
 [33.133]] [[1.77 ]
 [1.918]
 [1.77 ]
 [1.845]
 [1.749]
 [1.748]
 [1.659]]
siam score:  -0.7472645
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05646561268178447, 0.056788066783025566, 0.05490958416998155, 0.2505856582079036, 0.19767572759574353, 0.3835753505615612]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05646561268178447, 0.056788066783025566, 0.05490958416998155, 0.2505856582079036, 0.19767572759574353, 0.3835753505615612]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
Printing some Q and Qe and total Qs values:  [[0.219]
 [0.21 ]
 [0.219]
 [0.219]
 [0.224]
 [0.184]
 [0.166]] [[40.05 ]
 [41.746]
 [40.05 ]
 [40.05 ]
 [40.09 ]
 [42.835]
 [43.576]] [[1.548]
 [1.641]
 [1.548]
 [1.548]
 [1.557]
 [1.681]
 [1.707]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05646561268178447, 0.056788066783025566, 0.05490958416998155, 0.2505856582079036, 0.19767572759574353, 0.3835753505615612]
printing an ep nov before normalisation:  7.590749822495724
siam score:  -0.7473916
printing an ep nov before normalisation:  13.64256739616394
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05640143725841956, 0.056724660609769856, 0.05484169666760132, 0.25098457773555394, 0.1979484246013526, 0.3830992031273028]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.056516175516845714, 0.05684005796298652, 0.05495325441046857, 0.25149609652878774, 0.19631429461793817, 0.38388012096297325]
Printing some Q and Qe and total Qs values:  [[0.353]
 [0.437]
 [0.413]
 [0.368]
 [0.417]
 [0.413]
 [0.388]] [[22.431]
 [26.53 ]
 [ 0.   ]
 [24.937]
 [34.626]
 [ 0.   ]
 [29.034]] [[0.431]
 [0.557]
 [0.26 ]
 [0.472]
 [0.621]
 [0.26 ]
 [0.535]]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.056516175516845714, 0.05684005796298652, 0.05495325441046857, 0.25149609652878774, 0.19631429461793817, 0.38388012096297325]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.056516175516845714, 0.05684005796298652, 0.05495325441046857, 0.25149609652878774, 0.19631429461793817, 0.38388012096297325]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.056572824922714475, 0.05689703278191983, 0.0550083335091073, 0.25174864730064783, 0.1955074800998989, 0.3842656813857117]
Printing some Q and Qe and total Qs values:  [[0.274]
 [0.35 ]
 [0.277]
 [0.37 ]
 [0.363]
 [0.379]
 [0.362]] [[46.545]
 [50.617]
 [49.81 ]
 [51.563]
 [50.171]
 [47.496]
 [52.722]] [[1.369]
 [1.677]
 [1.558]
 [1.751]
 [1.664]
 [1.528]
 [1.809]]
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.548]] [[30.834]
 [30.834]
 [30.834]
 [30.834]
 [30.834]
 [30.834]
 [36.696]] [[1.559]
 [1.559]
 [1.559]
 [1.559]
 [1.559]
 [1.559]
 [1.913]]
Printing some Q and Qe and total Qs values:  [[0.262]
 [0.262]
 [0.262]
 [0.262]
 [0.312]
 [0.262]
 [0.262]] [[37.533]
 [37.533]
 [37.533]
 [37.533]
 [40.724]
 [37.533]
 [37.533]] [[2.006]
 [2.006]
 [2.006]
 [2.006]
 [2.312]
 [2.006]
 [2.006]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
printing an ep nov before normalisation:  43.579279549289545
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05627047453628464, 0.0569152373511969, 0.05502593235866331, 0.2518293422130387, 0.19557013805712167, 0.3843888754836949]
siam score:  -0.7439929
printing an ep nov before normalisation:  33.117581630823125
actions average: 
K:  3  action  0 :  tensor([0.7227, 0.0055, 0.0238, 0.0317, 0.1269, 0.0328, 0.0566],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0310, 0.6862, 0.0607, 0.0344, 0.0260, 0.0848, 0.0769],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1499, 0.0050, 0.1284, 0.1742, 0.1809, 0.2163, 0.1453],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0861, 0.1277, 0.0724, 0.3527, 0.1271, 0.1179, 0.1162],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1112, 0.1029, 0.0964, 0.1349, 0.2630, 0.1715, 0.1201],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0986, 0.0201, 0.1005, 0.1310, 0.1195, 0.4250, 0.1053],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1034, 0.1332, 0.0621, 0.1175, 0.1582, 0.1066, 0.3189],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  28.697600133964514
printing an ep nov before normalisation:  32.19413852334681
printing an ep nov before normalisation:  32.59138302972255
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]] [[33.637]
 [33.851]
 [33.851]
 [33.851]
 [33.851]
 [33.851]
 [33.851]] [[0.538]
 [0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]]
printing an ep nov before normalisation:  27.910208702087402
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
actor:  1 policy actor:  1  step number:  73 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05270650579044332, 0.05331033474038138, 0.05154097549172569, 0.23584998582565475, 0.24659844669971834, 0.3599937514520765]
Printing some Q and Qe and total Qs values:  [[0.217]
 [0.217]
 [0.217]
 [0.217]
 [0.217]
 [0.217]
 [0.217]] [[18.783]
 [18.783]
 [18.783]
 [18.783]
 [18.783]
 [18.783]
 [18.783]] [[0.217]
 [0.217]
 [0.217]
 [0.217]
 [0.217]
 [0.217]
 [0.217]]
actor:  1 policy actor:  1  step number:  83 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05475718590564031, 0.055320628226040114, 0.05366961119417093, 0.22565125676415088, 0.2356808151949451, 0.3749205027150527]
printing an ep nov before normalisation:  15.297491522527707
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05475718590564031, 0.055320628226040114, 0.05366961119417093, 0.22565125676415088, 0.2356808151949451, 0.3749205027150527]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05469180800614092, 0.055256537895960994, 0.05360174798625569, 0.22597640327464902, 0.2360288810926932, 0.37444462174430004]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05469180800614092, 0.055256537895960994, 0.05360174798625569, 0.22597640327464902, 0.2360288810926932, 0.37444462174430004]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05469180800614092, 0.055256537895960994, 0.05360174798625569, 0.22597640327464902, 0.2360288810926932, 0.37444462174430004]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05469180800614092, 0.055256537895960994, 0.05360174798625569, 0.22597640327464902, 0.2360288810926932, 0.37444462174430004]
printing an ep nov before normalisation:  28.78145694732666
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05469180800614092, 0.055256537895960994, 0.05360174798625569, 0.22597640327464902, 0.2360288810926932, 0.37444462174430004]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05469180800614092, 0.055256537895960994, 0.05360174798625569, 0.22597640327464902, 0.2360288810926932, 0.37444462174430004]
Printing some Q and Qe and total Qs values:  [[0.163]
 [0.163]
 [0.163]
 [0.181]
 [0.203]
 [0.163]
 [0.163]] [[26.863]
 [26.863]
 [26.863]
 [18.932]
 [32.08 ]
 [26.863]
 [26.863]] [[0.163]
 [0.163]
 [0.163]
 [0.181]
 [0.203]
 [0.163]
 [0.163]]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05469180800614092, 0.055256537895960994, 0.05360174798625569, 0.22597640327464902, 0.2360288810926932, 0.37444462174430004]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05469180800614092, 0.055256537895960994, 0.05360174798625569, 0.22597640327464902, 0.2360288810926932, 0.37444462174430004]
printing an ep nov before normalisation:  22.506155117809623
printing an ep nov before normalisation:  28.04604345608069
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05469180800614092, 0.055256537895960994, 0.05360174798625569, 0.22597640327464902, 0.2360288810926932, 0.37444462174430004]
printing an ep nov before normalisation:  19.57258368185304
Printing some Q and Qe and total Qs values:  [[0.302]
 [0.302]
 [0.302]
 [0.319]
 [0.331]
 [0.359]
 [0.288]] [[56.734]
 [56.734]
 [56.734]
 [53.756]
 [55.59 ]
 [52.058]
 [56.359]] [[1.966]
 [1.966]
 [1.966]
 [1.839]
 [1.94 ]
 [1.796]
 [1.934]]
printing an ep nov before normalisation:  40.76899849631278
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05475578424849813, 0.055321176349385034, 0.053664446007251315, 0.22624123050189338, 0.23513381777543563, 0.37488354511753647]
actor:  1 policy actor:  1  step number:  76 total reward:  0.16666666666666652  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  38.09708384579434
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.464]
 [0.486]
 [0.49 ]
 [0.488]
 [0.49 ]
 [0.478]] [[31.509]
 [33.877]
 [36.628]
 [31.509]
 [33.715]
 [31.509]
 [34.492]] [[1.213]
 [1.299]
 [1.451]
 [1.213]
 [1.315]
 [1.213]
 [1.342]]
siam score:  -0.74510074
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05348660056106447, 0.0540407754987651, 0.052416914053409785, 0.24362040654257996, 0.23028600284409548, 0.3661493005000851]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.053420274381251444, 0.053975679447521095, 0.05234821343938212, 0.2439761294500939, 0.23061212673905118, 0.36566757654270027]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.053420274381251444, 0.053975679447521095, 0.05234821343938212, 0.2439761294500939, 0.23061212673905118, 0.36566757654270027]
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.379]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.352]
 [0.379]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05354144276953297, 0.05409811076065429, 0.05246694408899646, 0.24453059911633163, 0.22886414522466372, 0.36649875803982096]
Printing some Q and Qe and total Qs values:  [[0.588]
 [0.624]
 [0.574]
 [0.566]
 [0.586]
 [0.573]
 [0.601]] [[33.429]
 [35.13 ]
 [32.301]
 [33.106]
 [32.239]
 [33.67 ]
 [33.718]] [[1.856]
 [2.017]
 [1.758]
 [1.81 ]
 [1.765]
 [1.858]
 [1.89 ]]
printing an ep nov before normalisation:  42.771100997924805
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.119]
 [0.463]
 [0.446]
 [0.445]
 [0.422]
 [0.454]] [[48.515]
 [47.609]
 [50.317]
 [51.528]
 [51.825]
 [51.842]
 [52.094]] [[1.975]
 [1.54 ]
 [2.039]
 [2.092]
 [2.107]
 [2.086]
 [2.132]]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05334350792670749, 0.053903870241832776, 0.05226187834169824, 0.24560016273945148, 0.22982973851431376, 0.3650608422359961]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05334350792670749, 0.053903870241832776, 0.05226187834169824, 0.24560016273945148, 0.22982973851431376, 0.3650608422359961]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05334350792670749, 0.053903870241832776, 0.05226187834169824, 0.24560016273945148, 0.22982973851431376, 0.3650608422359961]
Printing some Q and Qe and total Qs values:  [[ 0.156]
 [-0.022]
 [ 0.159]
 [ 0.264]
 [ 0.283]
 [ 0.245]
 [ 0.272]] [[20.44 ]
 [20.504]
 [22.994]
 [22.535]
 [21.913]
 [22.822]
 [21.517]] [[1.339]
 [1.165]
 [1.493]
 [1.571]
 [1.554]
 [1.569]
 [1.518]]
printing an ep nov before normalisation:  30.053779210548637
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05334350792670749, 0.053903870241832776, 0.05226187834169824, 0.24560016273945148, 0.22982973851431376, 0.3650608422359961]
from probs:  [0.05334350792670749, 0.053903870241832776, 0.05226187834169824, 0.24560016273945148, 0.22982973851431376, 0.3650608422359961]
Printing some Q and Qe and total Qs values:  [[0.366]
 [0.366]
 [0.366]
 [0.324]
 [0.366]
 [0.366]
 [0.366]] [[40.377]
 [40.377]
 [40.377]
 [43.302]
 [40.377]
 [40.377]
 [40.377]] [[2.014]
 [2.014]
 [2.014]
 [2.221]
 [2.014]
 [2.014]
 [2.014]]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.053277758058497916, 0.05383934755178404, 0.05219375973424799, 0.2459554496927642, 0.23015048866942683, 0.36458319629327907]
using explorer policy with actor:  0
printing an ep nov before normalisation:  47.29080874538239
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  73 total reward:  0.09333333333333305  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.0507981392184883, 0.0513353574439765, 0.04976118310882503, 0.23511420785652387, 0.2654379230159834, 0.34755318935620294]
printing an ep nov before normalisation:  80.68794144494515
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.0507981392184883, 0.0513353574439765, 0.04976118310882503, 0.23511420785652387, 0.2654379230159834, 0.34755318935620294]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
printing an ep nov before normalisation:  39.64695651887975
printing an ep nov before normalisation:  34.99127388000488
from probs:  [0.0507981392184883, 0.0513353574439765, 0.04976118310882503, 0.23511420785652387, 0.2654379230159834, 0.34755318935620294]
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.424]] [[40.649]
 [40.115]
 [40.115]
 [40.115]
 [40.115]
 [40.115]
 [43.83 ]] [[0.896]
 [0.874]
 [0.874]
 [0.874]
 [0.874]
 [0.874]
 [0.953]]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
printing an ep nov before normalisation:  34.90757020661639
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.0507981392184883, 0.0513353574439765, 0.04976118310882503, 0.23511420785652387, 0.2654379230159834, 0.34755318935620294]
printing an ep nov before normalisation:  0.00013102482171234442
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.0507443116165899, 0.051011938542213, 0.049704923324053664, 0.2354926940055911, 0.26588753350673544, 0.34715859900481705]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.0507443116165899, 0.051011938542213, 0.049704923324053664, 0.2354926940055911, 0.26588753350673544, 0.34715859900481705]
printing an ep nov before normalisation:  34.27344136702009
printing an ep nov before normalisation:  46.53281324798036
printing an ep nov before normalisation:  48.494562246098454
line 256 mcts: sample exp_bonus 19.023207440780304
printing an ep nov before normalisation:  23.861308015534124
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05067690863649971, 0.05094508852219211, 0.049635372801368746, 0.2358070107575161, 0.2662646508625899, 0.3466709684198334]
printing an ep nov before normalisation:  34.17517957589229
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.592]
 [0.56 ]
 [0.543]] [[33.131]
 [33.131]
 [33.131]
 [33.131]
 [30.423]
 [27.284]
 [33.131]] [[0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.592]
 [0.56 ]
 [0.543]]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.050690477921089445, 0.050690477921089445, 0.04964866246849434, 0.23587028126979925, 0.2663360982304385, 0.346764002189089]
using another actor
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.050756785125853195, 0.050756785125853195, 0.04971360330406389, 0.23617945680412603, 0.26537475179702236, 0.34721861784308145]
siam score:  -0.7325278
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.050756785125853195, 0.050756785125853195, 0.04971360330406389, 0.23617945680412603, 0.26537475179702236, 0.34721861784308145]
printing an ep nov before normalisation:  43.58920858219789
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.050756785125853195, 0.050756785125853195, 0.04971360330406389, 0.23617945680412603, 0.26537475179702236, 0.34721861784308145]
printing an ep nov before normalisation:  32.93717469409504
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.050756785125853195, 0.050756785125853195, 0.04971360330406389, 0.23617945680412603, 0.26537475179702236, 0.34721861784308145]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.050756785125853195, 0.050756785125853195, 0.04971360330406389, 0.23617945680412603, 0.26537475179702236, 0.34721861784308145]
printing an ep nov before normalisation:  0.00010161374120798428
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05082260080848853, 0.05082260080848853, 0.049778062746128234, 0.23648634048167463, 0.264420531633621, 0.34766986352159907]
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.476]] [[19.294]
 [16.373]
 [16.373]
 [16.373]
 [16.373]
 [16.373]
 [16.373]] [[0.566]
 [0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.476]]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05082260080848853, 0.05082260080848853, 0.049778062746128234, 0.23648634048167463, 0.264420531633621, 0.34766986352159907]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.050755433659088835, 0.050755433659088835, 0.04970873979133144, 0.2368023617544658, 0.26479420583665875, 0.3471838252993663]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.050755433659088835, 0.050755433659088835, 0.04970873979133144, 0.2368023617544658, 0.26479420583665875, 0.3471838252993663]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.423]
 [0.423]
 [0.349]
 [0.402]
 [0.319]
 [0.423]] [[44.251]
 [44.251]
 [44.251]
 [39.635]
 [47.887]
 [41.512]
 [44.251]] [[1.512]
 [1.512]
 [1.512]
 [1.214]
 [1.669]
 [1.274]
 [1.512]]
printing an ep nov before normalisation:  37.379487911029806
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.501]
 [0.196]
 [0.341]
 [0.501]
 [0.516]
 [0.501]] [[42.098]
 [42.098]
 [37.232]
 [44.401]
 [42.098]
 [53.428]
 [42.098]] [[1.556]
 [1.556]
 [1.036]
 [1.497]
 [1.556]
 [2.07 ]
 [1.556]]
siam score:  -0.72975135
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]] [[22.918]
 [20.384]
 [20.384]
 [20.384]
 [20.384]
 [20.384]
 [20.384]] [[2.039]
 [1.758]
 [1.758]
 [1.758]
 [1.758]
 [1.758]
 [1.758]]
printing an ep nov before normalisation:  33.70334204524327
siam score:  -0.73071814
printing an ep nov before normalisation:  13.85934829711914
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.431]
 [0.382]
 [0.388]
 [0.395]
 [0.382]
 [0.439]] [[30.602]
 [29.467]
 [29.528]
 [29.219]
 [30.379]
 [31.555]
 [29.192]] [[1.403]
 [1.377]
 [1.332]
 [1.316]
 [1.406]
 [1.475]
 [1.366]]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
printing an ep nov before normalisation:  35.39256446465794
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.0505000782436703, 0.05076736111558717, 0.04971687727014647, 0.2374879472185711, 0.26428769139400193, 0.34724004475802306]
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.472]
 [0.433]
 [0.454]
 [0.439]
 [0.454]
 [0.454]] [[23.794]
 [26.222]
 [18.297]
 [23.794]
 [22.002]
 [23.794]
 [23.794]] [[0.454]
 [0.472]
 [0.433]
 [0.454]
 [0.439]
 [0.454]
 [0.454]]
printing an ep nov before normalisation:  30.358710130853297
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.0505000782436703, 0.05076736111558717, 0.04971687727014647, 0.2374879472185711, 0.26428769139400193, 0.34724004475802306]
printing an ep nov before normalisation:  33.493269837084746
printing an ep nov before normalisation:  21.43181678136364
Printing some Q and Qe and total Qs values:  [[0.083]
 [0.144]
 [0.127]
 [0.139]
 [0.095]
 [0.028]
 [0.097]] [[18.219]
 [19.381]
 [18.649]
 [22.125]
 [21.892]
 [19.749]
 [18.069]] [[0.575]
 [0.71 ]
 [0.647]
 [0.878]
 [0.82 ]
 [0.617]
 [0.58 ]]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.0505000782436703, 0.05076736111558717, 0.04971687727014647, 0.2374879472185711, 0.26428769139400193, 0.34724004475802306]
Printing some Q and Qe and total Qs values:  [[0.343]
 [0.421]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]] [[44.649]
 [43.464]
 [44.649]
 [44.649]
 [44.649]
 [44.649]
 [44.649]] [[1.676]
 [1.685]
 [1.676]
 [1.676]
 [1.676]
 [1.676]
 [1.676]]
actor:  1 policy actor:  1  step number:  62 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.04920345468149096, 0.04946385673280399, 0.04844041611252719, 0.2571005960162793, 0.2574874072236492, 0.3383042692332493]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.04920345468149096, 0.04946385673280399, 0.04844041611252719, 0.2571005960162793, 0.2574874072236492, 0.3383042692332493]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
siam score:  -0.7334534
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.51 ]
 [0.51 ]
 [0.51 ]
 [0.51 ]
 [0.51 ]
 [0.51 ]] [[16.171]
 [16.976]
 [16.976]
 [16.976]
 [16.976]
 [16.976]
 [16.976]] [[0.905]
 [0.77 ]
 [0.77 ]
 [0.77 ]
 [0.77 ]
 [0.77 ]
 [0.77 ]]
printing an ep nov before normalisation:  35.511722977210994
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.04895869090705859, 0.0494765648316527, 0.048452860562106195, 0.25716679356394434, 0.2575537044180258, 0.33839138571721245]
printing an ep nov before normalisation:  35.282609709682326
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.04895869090705859, 0.0494765648316527, 0.048452860562106195, 0.25716679356394434, 0.2575537044180258, 0.33839138571721245]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.04895869090705859, 0.0494765648316527, 0.048452860562106195, 0.25716679356394434, 0.2575537044180258, 0.33839138571721245]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.035]
 [-0.03 ]
 [-0.035]
 [-0.04 ]
 [-0.04 ]
 [-0.035]
 [-0.039]] [[29.709]
 [35.019]
 [29.709]
 [29.461]
 [29.426]
 [29.709]
 [28.999]] [[1.451]
 [1.97 ]
 [1.451]
 [1.422]
 [1.418]
 [1.451]
 [1.378]]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.04895869090705859, 0.0494765648316527, 0.048452860562106195, 0.25716679356394434, 0.2575537044180258, 0.33839138571721245]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.04888993001846769, 0.04940883886256056, 0.048383088821911856, 0.2575141158473509, 0.2579017999042683, 0.3379022265454407]
actor:  1 policy actor:  1  step number:  64 total reward:  0.23333333333333306  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.271]
 [0.21 ]
 [0.261]
 [0.277]
 [0.27 ]
 [0.271]
 [0.28 ]] [[40.046]
 [43.691]
 [36.957]
 [29.755]
 [35.551]
 [36.269]
 [34.323]] [[0.271]
 [0.21 ]
 [0.261]
 [0.277]
 [0.27 ]
 [0.271]
 [0.28 ]]
printing an ep nov before normalisation:  40.55094923152991
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.259]
 [0.505]
 [0.377]
 [0.378]
 [0.372]
 [0.367]] [[25.487]
 [36.09 ]
 [14.893]
 [30.618]
 [30.755]
 [31.301]
 [31.471]] [[0.412]
 [0.259]
 [0.505]
 [0.377]
 [0.378]
 [0.372]
 [0.367]]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05113487801472771, 0.05161999816727444, 0.050661039726193687, 0.24617452323636693, 0.24653696328912902, 0.35387259756630823]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05113487801472771, 0.05161999816727444, 0.050661039726193687, 0.24617452323636693, 0.24653696328912902, 0.35387259756630823]
line 256 mcts: sample exp_bonus 27.0847665610138
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05113487801472771, 0.05161999816727444, 0.050661039726193687, 0.24617452323636693, 0.24653696328912902, 0.35387259756630823]
printing an ep nov before normalisation:  25.34295371837473
printing an ep nov before normalisation:  50.83457487914967
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05090868308506669, 0.05163228044850801, 0.05067309324580673, 0.24623321692244243, 0.24659574343606686, 0.35395698286210936]
printing an ep nov before normalisation:  40.49129641323296
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
printing an ep nov before normalisation:  43.25550359134325
printing an ep nov before normalisation:  4.391743573251006e-06
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.050901677300834315, 0.05162760937843907, 0.05066532732207927, 0.24685643310785543, 0.2460470073052953, 0.35390194558549665]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
printing an ep nov before normalisation:  45.26014404034523
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.14721608843338
printing an ep nov before normalisation:  31.925455613780503
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.050901677300834315, 0.05162760937843907, 0.05066532732207927, 0.24685643310785543, 0.2460470073052953, 0.35390194558549665]
Printing some Q and Qe and total Qs values:  [[0.361]
 [0.389]
 [0.361]
 [0.361]
 [0.361]
 [0.361]
 [0.361]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.361]
 [0.389]
 [0.361]
 [0.361]
 [0.361]
 [0.361]
 [0.361]]
printing an ep nov before normalisation:  29.899301986267314
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05076905331875006, 0.051497950930743094, 0.05053173781717094, 0.2475243117965013, 0.24671157937597196, 0.35296536676086265]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05076905331875006, 0.051497950930743094, 0.05053173781717094, 0.2475243117965013, 0.24671157937597196, 0.35296536676086265]
Printing some Q and Qe and total Qs values:  [[0.406]
 [0.406]
 [0.406]
 [0.384]
 [0.417]
 [0.363]
 [0.35 ]] [[25.661]
 [25.661]
 [25.661]
 [24.748]
 [34.207]
 [30.026]
 [28.38 ]] [[0.406]
 [0.406]
 [0.406]
 [0.384]
 [0.417]
 [0.363]
 [0.35 ]]
printing an ep nov before normalisation:  32.83990394262965
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.88444781683931
printing an ep nov before normalisation:  31.87363061519409
printing an ep nov before normalisation:  21.280844852527526
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.05076905331875006, 0.051497950930743094, 0.05053173781717094, 0.2475243117965013, 0.24671157937597196, 0.35296536676086265]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.050781514716114855, 0.051264690001118524, 0.05054414081156363, 0.24758519435385778, 0.2467722619211566, 0.3530521981961887]
printing an ep nov before normalisation:  19.23608891334814
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.050781514716114855, 0.051264690001118524, 0.05054414081156363, 0.24758519435385778, 0.2467722619211566, 0.3530521981961887]
actor:  1 policy actor:  1  step number:  59 total reward:  0.22666666666666602  reward:  1.0 rdn_beta:  1.333
from probs:  [0.04967455390655806, 0.05014716899049385, 0.04944236800776401, 0.2640152920306672, 0.2413817589641306, 0.34533885810038617]
siam score:  -0.7317644
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.0495409485332731, 0.05001543776324828, 0.049307841905756225, 0.2647316507382776, 0.24200836483101332, 0.3443957562284315]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.0495409485332731, 0.05001543776324828, 0.049307841905756225, 0.2647316507382776, 0.24200836483101332, 0.3443957562284315]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.0495409485332731, 0.05001543776324828, 0.049307841905756225, 0.2647316507382776, 0.24200836483101332, 0.3443957562284315]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.049474340073919765, 0.04994976365239618, 0.049240774420540354, 0.26508878869377367, 0.2423207568433151, 0.3439255763160549]
printing an ep nov before normalisation:  48.90254363911259
actions average: 
K:  3  action  0 :  tensor([0.6020, 0.0301, 0.0486, 0.0741, 0.0717, 0.0712, 0.1023],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0253, 0.8450, 0.0189, 0.0220, 0.0211, 0.0207, 0.0469],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1392, 0.0080, 0.2226, 0.1480, 0.1315, 0.1635, 0.1871],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0660, 0.2413, 0.0536, 0.3920, 0.0881, 0.0703, 0.0887],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0860, 0.0385, 0.0489, 0.0902, 0.5522, 0.1007, 0.0835],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1001, 0.0310, 0.0896, 0.0970, 0.0742, 0.4638, 0.1443],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1190, 0.1566, 0.0460, 0.0877, 0.1424, 0.0715, 0.3768],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.049474340073919765, 0.04994976365239618, 0.049240774420540354, 0.26508878869377367, 0.2423207568433151, 0.3439255763160549]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.04940786059794614, 0.049884216715612675, 0.049173836807580903, 0.2654452350725394, 0.24263254392678443, 0.3434563068795363]
printing an ep nov before normalisation:  48.22823340025423
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.04940786059794614, 0.049884216715612675, 0.049173836807580903, 0.2654452350725394, 0.24263254392678443, 0.3434563068795363]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.049608857677431444, 0.05008455365997457, 0.049144160445929785, 0.2653468471886147, 0.24256576990930556, 0.3432498111187439]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.193]
 [0.172]
 [0.168]
 [0.156]
 [0.187]
 [0.147]] [[28.02 ]
 [29.42 ]
 [27.48 ]
 [27.704]
 [27.481]
 [33.608]
 [30.425]] [[0.947]
 [1.229]
 [1.077]
 [1.088]
 [1.061]
 [1.504]
 [1.25 ]]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.049608857677431444, 0.05008455365997457, 0.049144160445929785, 0.2653468471886147, 0.24256576990930556, 0.3432498111187439]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.049608857677431444, 0.05008455365997457, 0.049144160445929785, 0.2653468471886147, 0.24256576990930556, 0.3432498111187439]
Printing some Q and Qe and total Qs values:  [[0.047]
 [0.047]
 [0.047]
 [0.047]
 [0.047]
 [0.047]
 [0.06 ]] [[19.577]
 [19.577]
 [19.577]
 [19.577]
 [19.577]
 [19.577]
 [19.53 ]] [[1.62 ]
 [1.62 ]
 [1.62 ]
 [1.62 ]
 [1.62 ]
 [1.62 ]
 [1.629]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  70.84729644204769
Printing some Q and Qe and total Qs values:  [[0.251]
 [0.354]
 [0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.251]] [[51.186]
 [49.239]
 [51.186]
 [51.186]
 [51.186]
 [51.186]
 [51.186]] [[1.723]
 [1.717]
 [1.723]
 [1.723]
 [1.723]
 [1.723]
 [1.723]]
siam score:  -0.72757506
printing an ep nov before normalisation:  43.78599173004272
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.04964698737825685, 0.05012618056508332, 0.0491788738026865, 0.26697103153081886, 0.24058539057641207, 0.3434915361467424]
printing an ep nov before normalisation:  34.09162828054795
printing an ep nov before normalisation:  47.95746790964059
siam score:  -0.7308275
printing an ep nov before normalisation:  33.566322326660156
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.04964698737825685, 0.05012618056508332, 0.0491788738026865, 0.26697103153081886, 0.24058539057641207, 0.3434915361467424]
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.03609718719647
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.12259158073334
printing an ep nov before normalisation:  53.823388224910104
printing an ep nov before normalisation:  54.16733697911894
siam score:  -0.7338437
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.04958147937378455, 0.050061603500717686, 0.04911245638273428, 0.2673277241635948, 0.24089082319114175, 0.3430259133880268]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
printing an ep nov before normalisation:  19.40909636221022
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  66 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.04857115266558456, 0.049041466896252967, 0.04811171275238824, 0.28228407381587256, 0.23597166392893815, 0.3360199299409636]
printing an ep nov before normalisation:  40.171937986428915
siam score:  -0.7330453
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.04857115266558456, 0.049041466896252967, 0.04811171275238824, 0.28228407381587256, 0.23597166392893815, 0.3360199299409636]
printing an ep nov before normalisation:  44.82025003738658
from probs:  [0.04857115266558456, 0.049041466896252967, 0.04811171275238824, 0.28228407381587256, 0.23597166392893815, 0.3360199299409636]
printing an ep nov before normalisation:  39.607956777702526
printing an ep nov before normalisation:  0.1847458762279075
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
from probs:  [0.04850519373496679, 0.04897639950112424, 0.04804488289981875, 0.2826611449912292, 0.23626094452874954, 0.33555143434411155]
printing an ep nov before normalisation:  18.606656876365477
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.04850519373496679, 0.04897639950112424, 0.04804488289981875, 0.2826611449912292, 0.23626094452874954, 0.33555143434411155]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.04850519373496679, 0.04897639950112424, 0.04804488289981875, 0.2826611449912292, 0.23626094452874954, 0.33555143434411155]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.04850519373496679, 0.04897639950112424, 0.04804488289981875, 0.2826611449912292, 0.23626094452874954, 0.33555143434411155]
Printing some Q and Qe and total Qs values:  [[0.35 ]
 [0.344]
 [0.332]
 [0.339]
 [0.329]
 [0.328]
 [0.327]] [[25.778]
 [33.513]
 [26.976]
 [27.525]
 [27.431]
 [26.079]
 [28.736]] [[0.35 ]
 [0.344]
 [0.332]
 [0.339]
 [0.329]
 [0.328]
 [0.327]]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.0484393665950868, 0.04891146211538059, 0.0479781865781524, 0.283037462750969, 0.2365496471248954, 0.3350838748355158]
printing an ep nov before normalisation:  0.0020969800112879966
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.04849293430365581, 0.04896555334073919, 0.048031242874366276, 0.2833511812910168, 0.23570377943844212, 0.33545530875177987]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.04849293430365581, 0.04896555334073919, 0.048031242874366276, 0.2833511812910168, 0.23570377943844212, 0.33545530875177987]
maxi score, test score, baseline:  -0.9889933333333334 -0.857 -0.857
probs:  [0.04849293430365581, 0.04896555334073919, 0.048031242874366276, 0.2833511812910168, 0.23570377943844212, 0.33545530875177987]
actor:  0 policy actor:  1  step number:  60 total reward:  0.19333333333333302  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.515]
 [0.517]
 [0.518]
 [0.519]
 [0.518]
 [0.517]] [[36.932]
 [35.955]
 [36.527]
 [37.114]
 [36.806]
 [37.414]
 [37.879]] [[0.517]
 [0.515]
 [0.517]
 [0.518]
 [0.519]
 [0.518]
 [0.517]]
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04849293430365581, 0.04896555334073919, 0.048031242874366276, 0.2833511812910168, 0.23570377943844212, 0.33545530875177987]
printing an ep nov before normalisation:  30.993862345705722
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04842726644031917, 0.04890077542786371, 0.04796470563745775, 0.28372775588685595, 0.23599063308797036, 0.33498886351953305]
printing an ep nov before normalisation:  31.174977555144476
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.39 ]
 [0.381]
 [0.38 ]
 [0.379]
 [0.381]
 [0.381]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.381]
 [0.39 ]
 [0.381]
 [0.38 ]
 [0.379]
 [0.381]
 [0.381]]
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04842726644031917, 0.04890077542786371, 0.04796470563745775, 0.28372775588685595, 0.23599063308797036, 0.33498886351953305]
actions average: 
K:  2  action  0 :  tensor([0.7289, 0.0025, 0.0330, 0.0367, 0.1002, 0.0392, 0.0595],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0094, 0.9131, 0.0121, 0.0130, 0.0088, 0.0119, 0.0317],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1104, 0.0260, 0.1577, 0.1525, 0.1622, 0.2407, 0.1506],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1838, 0.0100, 0.0903, 0.2819, 0.1534, 0.1584, 0.1221],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1792, 0.0691, 0.0861, 0.0987, 0.3228, 0.1071, 0.1369],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0640, 0.0168, 0.0768, 0.0817, 0.0969, 0.5671, 0.0966],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1543, 0.0939, 0.0860, 0.0865, 0.1158, 0.1180, 0.3456],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04842726644031917, 0.04890077542786371, 0.04796470563745775, 0.28372775588685595, 0.23599063308797036, 0.33498886351953305]
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.048467779046258715, 0.04894321936664096, 0.0480033315656541, 0.2847280043667714, 0.23459902776437533, 0.33525863789029947]
printing an ep nov before normalisation:  45.84678611559894
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.584]
 [0.364]
 [0.361]
 [0.427]
 [0.34 ]
 [0.338]] [[34.572]
 [41.756]
 [43.3  ]
 [42.276]
 [34.572]
 [43.99 ]
 [43.566]] [[1.055]
 [1.536]
 [1.385]
 [1.337]
 [1.055]
 [1.392]
 [1.372]]
printing an ep nov before normalisation:  57.517162055967
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.045]
 [0.42 ]
 [0.322]
 [0.42 ]
 [0.462]
 [0.257]] [[32.546]
 [35.654]
 [32.546]
 [37.789]
 [32.546]
 [38.962]
 [36.812]] [[1.148]
 [0.943]
 [1.148]
 [1.336]
 [1.148]
 [1.541]
 [1.218]]
printing an ep nov before normalisation:  19.55738398266647
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.048467779046258715, 0.04894321936664096, 0.0480033315656541, 0.2847280043667714, 0.23459902776437533, 0.33525863789029947]
Printing some Q and Qe and total Qs values:  [[0.367]
 [0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]] [[31.823]
 [29.226]
 [29.226]
 [29.226]
 [29.226]
 [29.226]
 [29.226]] [[1.525]
 [1.374]
 [1.374]
 [1.374]
 [1.374]
 [1.374]
 [1.374]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.30659914359825
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04852021456652677, 0.04899617066618848, 0.04805526323217516, 0.2850367457230362, 0.23376940863646029, 0.3356221971756132]
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04852021456652677, 0.04899617066618848, 0.04805526323217516, 0.2850367457230362, 0.23376940863646029, 0.3356221971756132]
printing an ep nov before normalisation:  12.635187881255538
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04852021456652677, 0.04899617066618848, 0.04805526323217516, 0.2850367457230362, 0.23376940863646029, 0.3356221971756132]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04852021456652677, 0.04899617066618848, 0.04805526323217516, 0.2850367457230362, 0.23376940863646029, 0.3356221971756132]
from probs:  [0.04845488942923506, 0.04893173778356182, 0.047989066470384055, 0.2854148080713521, 0.2340513622875385, 0.3351581359579284]
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04845488942923506, 0.04893173778356182, 0.047989066470384055, 0.2854148080713521, 0.2340513622875385, 0.3351581359579284]
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04845488942923506, 0.04893173778356182, 0.047989066470384055, 0.2854148080713521, 0.2340513622875385, 0.3351581359579284]
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04845488942923506, 0.04893173778356182, 0.047989066470384055, 0.2854148080713521, 0.2340513622875385, 0.3351581359579284]
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.048506965110953, 0.04898432735908541, 0.04804064014023407, 0.2857222526418751, 0.2332266264259789, 0.33551918832187344]
siam score:  -0.73105645
actions average: 
K:  3  action  0 :  tensor([0.6047, 0.0171, 0.0532, 0.0893, 0.0906, 0.0878, 0.0573],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0457, 0.7793, 0.0381, 0.0257, 0.0247, 0.0388, 0.0478],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1404, 0.0053, 0.1191, 0.2724, 0.1352, 0.1625, 0.1651],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0391, 0.1126, 0.0766, 0.4192, 0.0918, 0.1600, 0.1007],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0827, 0.0138, 0.1043, 0.1672, 0.3500, 0.1486, 0.1334],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0730, 0.0641, 0.0780, 0.1975, 0.0877, 0.4019, 0.0977],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1145, 0.0436, 0.0990, 0.1960, 0.1381, 0.1715, 0.2373],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04855865795441904, 0.0490365303184285, 0.04809183466240982, 0.28602743701074446, 0.23240795367151845, 0.33587758638247966]
printing an ep nov before normalisation:  35.980036609202884
from probs:  [0.04855865795441904, 0.0490365303184285, 0.04809183466240982, 0.28602743701074446, 0.23240795367151845, 0.33587758638247966]
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04833521463458766, 0.0490480231680573, 0.04810310549748484, 0.286094625501677, 0.2324625409552881, 0.33595649024290497]
printing an ep nov before normalisation:  24.005117194958594
actions average: 
K:  1  action  0 :  tensor([    0.5363,     0.0002,     0.0699,     0.0750,     0.0988,     0.0863,
            0.1335], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0258, 0.8389, 0.0133, 0.0281, 0.0265, 0.0186, 0.0487],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1717, 0.1274, 0.1944, 0.1409, 0.1210, 0.1361, 0.1086],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0574, 0.0022, 0.0621, 0.5956, 0.0993, 0.0954, 0.0880],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1866, 0.0093, 0.0565, 0.0754, 0.5103, 0.0772, 0.0848],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0638, 0.0044, 0.1253, 0.0619, 0.0876, 0.5694, 0.0876],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1441, 0.0151, 0.1081, 0.1476, 0.1953, 0.1409, 0.2490],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04833521463458766, 0.0490480231680573, 0.04810310549748484, 0.286094625501677, 0.2324625409552881, 0.33595649024290497]
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.048386304089904436, 0.04909986813162839, 0.04815394893951648, 0.2863977169870181, 0.2316497313590898, 0.33631243049284293]
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.048386304089904436, 0.04909986813162839, 0.04815394893951648, 0.2863977169870181, 0.2316497313590898, 0.33631243049284293]
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.0484370207234659, 0.04915133476017512, 0.04820442135505771, 0.2866985966832751, 0.23084285318452216, 0.336665773293504]
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.0484370207234659, 0.04915133476017512, 0.04820442135505771, 0.2866985966832751, 0.23084285318452216, 0.336665773293504]
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04842187106115904, 0.04913827240171093, 0.048188592011923465, 0.2873796734389073, 0.2303172735378727, 0.3365543175484265]
actions average: 
K:  3  action  0 :  tensor([0.6990, 0.0114, 0.0376, 0.0723, 0.0700, 0.0540, 0.0557],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0124, 0.8042, 0.0117, 0.0663, 0.0152, 0.0299, 0.0603],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1241, 0.0852, 0.2254, 0.0917, 0.1142, 0.2158, 0.1436],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1440, 0.0032, 0.0902, 0.3342, 0.1301, 0.1499, 0.1485],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1486, 0.0040, 0.0357, 0.0698, 0.5965, 0.0664, 0.0790],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1474, 0.0777, 0.1027, 0.1646, 0.1556, 0.1772, 0.1748],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1716, 0.0532, 0.0732, 0.1327, 0.1045, 0.1252, 0.3396],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
printing an ep nov before normalisation:  68.15289282494909
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
printing an ep nov before normalisation:  34.934459835643004
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04823328598249698, 0.04918693371143432, 0.04823328598249698, 0.28842360057450406, 0.229056327403803, 0.3368665663452647]
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04823328598249698, 0.04918693371143432, 0.04823328598249698, 0.28842360057450406, 0.229056327403803, 0.3368665663452647]
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04823328598249698, 0.04918693371143432, 0.04823328598249698, 0.28842360057450406, 0.229056327403803, 0.3368665663452647]
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04823328598249698, 0.04918693371143432, 0.04823328598249698, 0.28842360057450406, 0.229056327403803, 0.3368665663452647]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04828238737591175, 0.04923700860536171, 0.04828238737591175, 0.28871789250031854, 0.2282700144329252, 0.337210309709571]
from probs:  [0.04828238737591175, 0.04923700860536171, 0.04828238737591175, 0.28871789250031854, 0.2282700144329252, 0.337210309709571]
siam score:  -0.73554975
printing an ep nov before normalisation:  32.21871466876742
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04826556368606897, 0.049222939955462196, 0.04826556368606897, 0.2893949664527658, 0.22775907497551437, 0.3370918912441197]
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.048313990548582555, 0.04927233005250635, 0.048313990548582555, 0.2896859982079475, 0.22698277885703017, 0.3374309117853508]
siam score:  -0.73976976
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04836207338441078, 0.04932136928000219, 0.04836207338441078, 0.28997496245999393, 0.2262119975823053, 0.33776752390887693]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04829667527044374, 0.04925776339272062, 0.04829667527044374, 0.29036096319944327, 0.22647887169177092, 0.3373090511751777]
Printing some Q and Qe and total Qs values:  [[ 0.32 ]
 [-0.04 ]
 [ 0.302]
 [ 0.229]
 [ 0.295]
 [ 0.29 ]
 [ 0.291]] [[13.572]
 [19.787]
 [12.886]
 [16.517]
 [12.914]
 [12.407]
 [14.864]] [[0.938]
 [0.86 ]
 [0.888]
 [0.981]
 [0.882]
 [0.855]
 [0.967]]
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
printing an ep nov before normalisation:  19.091066035086286
printing an ep nov before normalisation:  17.86813815251941
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04823140284367383, 0.04919427974819514, 0.04823140284367383, 0.2907462220925595, 0.22674523290175397, 0.3368514595701436]
line 256 mcts: sample exp_bonus 21.90596917742192
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04823140284367383, 0.04919427974819514, 0.04823140284367383, 0.2907462220925595, 0.22674523290175397, 0.3368514595701436]
Printing some Q and Qe and total Qs values:  [[0.198]
 [0.216]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]] [[30.813]
 [40.5  ]
 [30.813]
 [30.813]
 [30.813]
 [30.813]
 [30.813]] [[1.205]
 [1.82 ]
 [1.205]
 [1.205]
 [1.205]
 [1.205]
 [1.205]]
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04823140284367383, 0.04919427974819514, 0.04823140284367383, 0.2907462220925595, 0.22674523290175397, 0.3368514595701436]
printing an ep nov before normalisation:  33.15532815634232
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.048279193867299126, 0.04924302749984282, 0.048279193867299126, 0.29103497924833294, 0.22597757797949478, 0.3371860275377312]
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
printing an ep nov before normalisation:  32.49183108666632
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.048279193867299126, 0.04924302749984282, 0.048279193867299126, 0.29103497924833294, 0.22597757797949478, 0.3371860275377312]
printing an ep nov before normalisation:  45.251736640930176
printing an ep nov before normalisation:  35.88840739871617
printing an ep nov before normalisation:  31.71214764668747
printing an ep nov before normalisation:  38.26568847307108
Printing some Q and Qe and total Qs values:  [[0.12 ]
 [0.165]
 [0.117]
 [0.102]
 [0.101]
 [0.098]
 [0.122]] [[24.651]
 [24.772]
 [26.3  ]
 [27.101]
 [27.397]
 [26.941]
 [26.709]] [[1.628]
 [1.689]
 [1.839]
 [1.928]
 [1.966]
 [1.904]
 [1.897]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04821407061592484, 0.049179693149143604, 0.04821407061592484, 0.2914204171453608, 0.22624226762172106, 0.33672948085192483]
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04821407061592484, 0.049179693149143604, 0.04821407061592484, 0.2914204171453608, 0.22624226762172106, 0.33672948085192483]
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04814907204414727, 0.04911648005315968, 0.04814907204414727, 0.2918051171148352, 0.22650645051102652, 0.3362738082326841]
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04814907204414727, 0.04911648005315968, 0.04814907204414727, 0.2918051171148352, 0.22650645051102652, 0.3362738082326841]
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04814907204414727, 0.04911648005315968, 0.04814907204414727, 0.2918051171148352, 0.22650645051102652, 0.3362738082326841]
printing an ep nov before normalisation:  47.47504156791553
printing an ep nov before normalisation:  14.329591989517212
printing an ep nov before normalisation:  66.15851911349218
Printing some Q and Qe and total Qs values:  [[ 0.164]
 [-0.058]
 [ 0.123]
 [ 0.118]
 [ 0.113]
 [ 0.112]
 [ 0.117]] [[13.673]
 [15.817]
 [14.857]
 [15.242]
 [15.473]
 [15.366]
 [15.945]] [[0.941]
 [0.841]
 [0.968]
 [0.985]
 [0.993]
 [0.986]
 [1.024]]
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04806699550095258, 0.04903892833675849, 0.04806699550095258, 0.29286268526226167, 0.2262664561859912, 0.3356979392130835]
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04806699550095258, 0.04903892833675849, 0.04806699550095258, 0.29286268526226167, 0.2262664561859912, 0.3356979392130835]
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04811421030218872, 0.049087100492629064, 0.04811421030218872, 0.29315102401717774, 0.22550498415780645, 0.33602847072800934]
printing an ep nov before normalisation:  18.264825378761827
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.04811421030218872, 0.049087100492629064, 0.04811421030218872, 0.29315102401717774, 0.22550498415780645, 0.33602847072800934]
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.376]
 [0.377]
 [0.377]
 [0.247]
 [0.359]
 [0.413]] [[25.443]
 [27.384]
 [25.443]
 [25.443]
 [27.978]
 [29.571]
 [26.989]] [[1.528]
 [1.718]
 [1.528]
 [1.528]
 [1.647]
 [1.916]
 [1.717]]
printing an ep nov before normalisation:  28.09685599513727
printing an ep nov before normalisation:  0.0018465288090396825
printing an ep nov before normalisation:  2.7594003881858953e-06
actor:  1 policy actor:  1  step number:  98 total reward:  0.21999999999999875  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.050065576017548366, 0.050984612789646305, 0.050065576017548366, 0.2815386081588845, 0.21763705099606978, 0.34970857602030264]
printing an ep nov before normalisation:  42.39099344500647
Starting evaluation
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.050065576017548366, 0.050984612789646305, 0.050065576017548366, 0.2815386081588845, 0.21763705099606978, 0.34970857602030264]
Printing some Q and Qe and total Qs values:  [[0.665]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]] [[25.308]
 [25.982]
 [25.982]
 [25.982]
 [25.982]
 [25.982]
 [25.982]] [[1.049]
 [1.02 ]
 [1.02 ]
 [1.02 ]
 [1.02 ]
 [1.02 ]
 [1.02 ]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  28.0271140930982
using explorer policy with actor:  0
printing an ep nov before normalisation:  46.75839928735126
printing an ep nov before normalisation:  48.01710107472667
printing an ep nov before normalisation:  43.190747377117155
printing an ep nov before normalisation:  43.47177049118271
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.05007726296287758, 0.050762646348229336, 0.05007726296287758, 0.2816044726606798, 0.21768795898137833, 0.34979039608395734]
printing an ep nov before normalisation:  32.44086767716402
printing an ep nov before normalisation:  25.764170218153453
printing an ep nov before normalisation:  39.3036570841621
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.05007726296287758, 0.050762646348229336, 0.05007726296287758, 0.2816044726606798, 0.21768795898137833, 0.34979039608395734]
printing an ep nov before normalisation:  38.54850001341188
printing an ep nov before normalisation:  12.558841705322266
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.05007726296287758, 0.050762646348229336, 0.05007726296287758, 0.2816044726606798, 0.21768795898137833, 0.34979039608395734]
printing an ep nov before normalisation:  13.557994365692139
printing an ep nov before normalisation:  39.110374450683594
printing an ep nov before normalisation:  35.90027522759501
printing an ep nov before normalisation:  13.929296030821092
printing an ep nov before normalisation:  12.690871707322495
printing an ep nov before normalisation:  17.086480030358313
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.487]
 [0.487]
 [0.487]
 [0.478]
 [0.476]
 [0.484]] [[13.497]
 [13.582]
 [13.582]
 [13.582]
 [14.14 ]
 [14.679]
 [15.334]] [[0.508]
 [0.487]
 [0.487]
 [0.487]
 [0.478]
 [0.476]
 [0.484]]
printing an ep nov before normalisation:  17.101703707645058
printing an ep nov before normalisation:  15.197247186386953
printing an ep nov before normalisation:  15.74921734084854
using explorer policy with actor:  0
printing an ep nov before normalisation:  18.14171655303509
printing an ep nov before normalisation:  13.108992723458858
printing an ep nov before normalisation:  13.980382073749018
line 256 mcts: sample exp_bonus 14.535112241825905
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.503]] [[36.028]
 [36.028]
 [36.028]
 [36.028]
 [36.028]
 [36.028]
 [36.028]] [[0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.503]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.05007726296287758, 0.050762646348229336, 0.05007726296287758, 0.2816044726606798, 0.21768795898137833, 0.34979039608395734]
siam score:  -0.7414438
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.265]
 [0.433]
 [0.436]
 [0.426]
 [0.409]
 [0.428]] [[16.738]
 [29.641]
 [17.539]
 [17.908]
 [18.238]
 [18.3  ]
 [20.266]] [[0.466]
 [0.265]
 [0.433]
 [0.436]
 [0.426]
 [0.409]
 [0.428]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.405]] [[15.798]
 [15.798]
 [15.798]
 [15.798]
 [15.798]
 [15.798]
 [18.403]] [[0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.405]]
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
printing an ep nov before normalisation:  17.22677580003351
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.0500148551249428, 0.05070152340301121, 0.0500148551249428, 0.2819761103975424, 0.21793977201186973, 0.349352883937691]
printing an ep nov before normalisation:  18.5153385183483
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.274]
 [0.48 ]
 [0.478]
 [0.463]
 [0.418]
 [0.462]] [[17.502]
 [27.293]
 [19.18 ]
 [19.755]
 [19.728]
 [20.235]
 [19.529]] [[0.533]
 [0.274]
 [0.48 ]
 [0.478]
 [0.463]
 [0.418]
 [0.462]]
printing an ep nov before normalisation:  17.908897399902344
printing an ep nov before normalisation:  21.051235619393328
printing an ep nov before normalisation:  21.484508514404297
printing an ep nov before normalisation:  16.499463976213733
printing an ep nov before normalisation:  16.14574471538539
printing an ep nov before normalisation:  21.262893782332018
siam score:  -0.7444893
printing an ep nov before normalisation:  20.797003949271065
using explorer policy with actor:  0
siam score:  -0.744608
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.0500148551249428, 0.05070152340301121, 0.0500148551249428, 0.2819761103975424, 0.21793977201186973, 0.349352883937691]
printing an ep nov before normalisation:  19.74060308522283
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.0500148551249428, 0.05070152340301121, 0.0500148551249428, 0.2819761103975424, 0.21793977201186973, 0.349352883937691]
Printing some Q and Qe and total Qs values:  [[0.5]
 [0.5]
 [0.5]
 [0.5]
 [0.5]
 [0.5]
 [0.5]] [[11.333]
 [11.333]
 [11.333]
 [11.333]
 [11.333]
 [11.333]
 [11.333]] [[0.5]
 [0.5]
 [0.5]
 [0.5]
 [0.5]
 [0.5]
 [0.5]]
printing an ep nov before normalisation:  19.16409339255212
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.0500148551249428, 0.05070152340301121, 0.0500148551249428, 0.2819761103975424, 0.21793977201186973, 0.349352883937691]
printing an ep nov before normalisation:  12.57914688789899
line 256 mcts: sample exp_bonus 15.251137956139038
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.259]
 [0.259]
 [0.259]
 [0.259]
 [0.259]
 [0.259]] [[21.648]
 [23.006]
 [23.006]
 [23.006]
 [23.006]
 [23.006]
 [23.006]] [[0.54 ]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]]
printing an ep nov before normalisation:  21.04609662236175
printing an ep nov before normalisation:  21.869065761566162
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.411]
 [0.43 ]
 [0.43 ]
 [0.424]
 [0.422]
 [0.431]] [[20.474]
 [17.789]
 [20.474]
 [16.671]
 [16.447]
 [16.444]
 [16.694]] [[0.43 ]
 [0.411]
 [0.43 ]
 [0.43 ]
 [0.424]
 [0.422]
 [0.431]]
printing an ep nov before normalisation:  13.504326343536377
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.05020836529970448, 0.05089411358699199, 0.04998503731342311, 0.2818588413748441, 0.2179082981872893, 0.34914534423774696]
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]] [[13.361]
 [13.128]
 [13.128]
 [13.128]
 [13.128]
 [13.128]
 [13.128]] [[0.516]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]]
printing an ep nov before normalisation:  19.344537258148193
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.05020836529970448, 0.05089411358699199, 0.04998503731342311, 0.2818588413748441, 0.2179082981872893, 0.34914534423774696]
printing an ep nov before normalisation:  22.371435256620522
printing an ep nov before normalisation:  16.518992117347935
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.39 ]
 [0.421]
 [0.412]
 [0.396]
 [0.349]
 [0.399]] [[13.924]
 [21.007]
 [16.662]
 [16.55 ]
 [16.907]
 [17.185]
 [18.154]] [[0.477]
 [0.39 ]
 [0.421]
 [0.412]
 [0.396]
 [0.349]
 [0.399]]
line 256 mcts: sample exp_bonus 49.807565216511804
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.475]
 [0.482]
 [0.483]
 [0.446]
 [0.612]
 [0.443]] [[14.924]
 [20.412]
 [17.815]
 [17.742]
 [18.134]
 [15.201]
 [18.119]] [[0.571]
 [0.475]
 [0.482]
 [0.483]
 [0.446]
 [0.612]
 [0.443]]
printing an ep nov before normalisation:  17.909453923566694
printing an ep nov before normalisation:  18.175280271772493
printing an ep nov before normalisation:  14.049863020027065
using explorer policy with actor:  0
actions average: 
K:  0  action  0 :  tensor([0.5626, 0.0428, 0.0580, 0.0759, 0.0865, 0.0802, 0.0941],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0052, 0.9502, 0.0049, 0.0088, 0.0051, 0.0060, 0.0199],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1604, 0.0278, 0.2192, 0.1560, 0.0987, 0.1740, 0.1639],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1097, 0.0231, 0.1011, 0.3140, 0.1401, 0.1691, 0.1430],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0997, 0.0062, 0.0775, 0.1358, 0.3751, 0.1447, 0.1609],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0594, 0.0039, 0.0950, 0.0795, 0.1003, 0.5760, 0.0858],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2023, 0.0135, 0.1098, 0.1471, 0.1234, 0.1701, 0.2339],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.0502199574352636, 0.05067455469007733, 0.049996577749708566, 0.28192405939739773, 0.21795871198988048, 0.34922613873767244]
Printing some Q and Qe and total Qs values:  [[0.589]
 [0.202]
 [0.202]
 [0.202]
 [0.202]
 [0.202]
 [0.202]] [[ 8.598]
 [11.814]
 [11.814]
 [11.814]
 [11.814]
 [11.814]
 [11.814]] [[0.666]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]]
maxi score, test score, baseline:  -0.9866066666666667 -0.857 -0.857
probs:  [0.0502199574352636, 0.05067455469007733, 0.049996577749708566, 0.28192405939739773, 0.21795871198988048, 0.34922613873767244]
printing an ep nov before normalisation:  27.415149211883545
actor:  0 policy actor:  1  step number:  98 total reward:  0.09999999999999998  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.9844066666666667 -0.857 -0.857
probs:  [0.0502199574352636, 0.05067455469007733, 0.049996577749708566, 0.28192405939739773, 0.21795871198988048, 0.34922613873767244]
printing an ep nov before normalisation:  20.979074108910638
printing an ep nov before normalisation:  18.96876335144043
printing an ep nov before normalisation:  14.496761258817017
printing an ep nov before normalisation:  16.399730443954468
printing an ep nov before normalisation:  14.17332850620725
printing an ep nov before normalisation:  9.383240467590914
maxi score, test score, baseline:  -0.9844066666666667 -0.857 -0.857
probs:  [0.050158112814138026, 0.05061355824987063, 0.04993431635002804, 0.2822945249057899, 0.21820983188726178, 0.3487896557929116]
maxi score, test score, baseline:  -0.9844066666666667 -0.857 -0.857
probs:  [0.050158112814138026, 0.05061355824987063, 0.04993431635002804, 0.2822945249057899, 0.21820983188726178, 0.3487896557929116]
printing an ep nov before normalisation:  16.657557681431882
printing an ep nov before normalisation:  11.598273847906828
Printing some Q and Qe and total Qs values:  [[0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]] [[16.4]
 [16.4]
 [16.4]
 [16.4]
 [16.4]
 [16.4]
 [16.4]] [[0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]]
printing an ep nov before normalisation:  15.405764359201068
printing an ep nov before normalisation:  16.52297913851614
printing an ep nov before normalisation:  11.252110004425049
maxi score, test score, baseline:  -0.9844066666666667 -0.857 -0.857
probs:  [0.050158112814138026, 0.05061355824987063, 0.04993431635002804, 0.2822945249057899, 0.21820983188726178, 0.3487896557929116]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9844066666666667 -0.857 -0.857
printing an ep nov before normalisation:  11.618432922708628
maxi score, test score, baseline:  -0.9844066666666667 -0.857 -0.857
probs:  [0.050204419255981944, 0.05066028628303125, 0.0499804156306215, 0.2825557126219861, 0.21748677027699304, 0.34911239593138615]
Printing some Q and Qe and total Qs values:  [[-0.013]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.013]] [[20.941]
 [20.941]
 [20.941]
 [20.941]
 [20.941]
 [20.941]
 [20.941]] [[0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]]
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.394]
 [0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]] [[18.909]
 [17.673]
 [18.909]
 [18.909]
 [18.909]
 [18.909]
 [18.909]] [[0.419]
 [0.394]
 [0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]]
maxi score, test score, baseline:  -0.9844066666666667 -0.857 -0.857
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.326]
 [0.44 ]
 [0.423]
 [0.41 ]
 [0.43 ]
 [0.416]] [[14.034]
 [25.119]
 [18.886]
 [20.149]
 [21.373]
 [20.007]
 [23.125]] [[0.572]
 [0.326]
 [0.44 ]
 [0.423]
 [0.41 ]
 [0.43 ]
 [0.416]]
Printing some Q and Qe and total Qs values:  [[0.527]
 [0.285]
 [0.477]
 [0.396]
 [0.477]
 [0.35 ]
 [0.477]] [[12.176]
 [22.585]
 [18.697]
 [20.947]
 [18.697]
 [23.289]
 [18.697]] [[0.527]
 [0.285]
 [0.477]
 [0.396]
 [0.477]
 [0.35 ]
 [0.477]]
printing an ep nov before normalisation:  22.67749776166017
printing an ep nov before normalisation:  22.294113636016846
maxi score, test score, baseline:  -0.9844066666666667 -0.857 -0.857
maxi score, test score, baseline:  -0.9844066666666667 -0.857 -0.857
Printing some Q and Qe and total Qs values:  [[ 0.027]
 [-0.039]
 [-0.03 ]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]] [[19.637]
 [23.049]
 [20.497]
 [15.813]
 [15.813]
 [15.813]
 [15.813]] [[0.227]
 [0.217]
 [0.185]
 [0.126]
 [0.126]
 [0.126]
 [0.126]]
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.283]
 [0.235]
 [0.307]
 [0.391]
 [0.513]
 [0.391]] [[43.368]
 [41.951]
 [40.955]
 [45.33 ]
 [43.368]
 [43.626]
 [43.368]] [[1.434]
 [1.253]
 [1.153]
 [1.451]
 [1.434]
 [1.568]
 [1.434]]
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.444]
 [0.518]
 [0.525]
 [0.524]
 [0.522]
 [0.515]] [[42.588]
 [39.774]
 [43.702]
 [44.888]
 [44.777]
 [45.352]
 [45.679]] [[1.506]
 [1.271]
 [1.539]
 [1.606]
 [1.599]
 [1.626]
 [1.635]]
printing an ep nov before normalisation:  17.06625461578369
printing an ep nov before normalisation:  50.549007957032494
Printing some Q and Qe and total Qs values:  [[0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]] [[12.372]
 [12.372]
 [12.372]
 [12.372]
 [12.372]
 [12.372]
 [12.372]] [[0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]]
printing an ep nov before normalisation:  17.21994184934261
printing an ep nov before normalisation:  17.365711020168597
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]] [[16.302]
 [16.302]
 [16.302]
 [16.302]
 [16.302]
 [16.302]
 [16.302]] [[0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]]
printing an ep nov before normalisation:  10.833364923595834
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.347]
 [0.327]
 [0.327]] [[54.149]
 [54.149]
 [54.149]
 [54.149]
 [47.925]
 [54.149]
 [54.149]] [[1.524]
 [1.524]
 [1.524]
 [1.524]
 [1.207]
 [1.524]
 [1.524]]
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]] [[ 9.671]
 [10.715]
 [10.715]
 [10.715]
 [10.715]
 [10.715]
 [10.715]] [[0.529]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]]
printing an ep nov before normalisation:  16.80102825164795
line 256 mcts: sample exp_bonus 11.149275127844177
printing an ep nov before normalisation:  10.302906784007071
printing an ep nov before normalisation:  11.43515397727608
using explorer policy with actor:  0
printing an ep nov before normalisation:  23.888319794557685
printing an ep nov before normalisation:  22.818964858009803
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]] [[14.223]
 [14.223]
 [14.223]
 [14.223]
 [14.223]
 [14.223]
 [14.223]] [[0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]]
maxi score, test score, baseline:  -0.9844066666666667 -0.857 -0.857
maxi score, test score, baseline:  -0.9844066666666667 -0.857 -0.857
probs:  [0.04999159302018398, 0.050671616122095996, 0.04999159302018398, 0.28261904109078806, 0.2175355081353039, 0.349190648611444]
using explorer policy with actor:  0
printing an ep nov before normalisation:  18.04286537276064
printing an ep nov before normalisation:  30.683025857538873
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.05000760974017545, 0.05071104740380195, 0.05000760974017545, 0.2834421330893703, 0.21653450582936687, 0.34929709419710997]
printing an ep nov before normalisation:  30.512615557978474
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.05000760974017545, 0.05071104740380195, 0.05000760974017545, 0.2834421330893703, 0.21653450582936687, 0.34929709419710997]
printing an ep nov before normalisation:  30.03671781235191
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.05020587577556806, 0.05090834786680212, 0.04997707058013753, 0.2833199756631773, 0.21650418903250992, 0.3490845410818049]
printing an ep nov before normalisation:  29.71192154271917
printing an ep nov before normalisation:  47.1011384796892
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.05020587577556806, 0.05090834786680212, 0.04997707058013753, 0.2833199756631773, 0.21650418903250992, 0.3490845410818049]
using explorer policy with actor:  0
printing an ep nov before normalisation:  31.0742858179599
printing an ep nov before normalisation:  23.48494529724121
printing an ep nov before normalisation:  22.68447096949366
printing an ep nov before normalisation:  42.823503729755885
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
from probs:  [0.05040135445662114, 0.05110287455011457, 0.049946960759699253, 0.28319953560643374, 0.2164742984491248, 0.3488749761780065]
Printing some Q and Qe and total Qs values:  [[0.337]
 [0.379]
 [0.337]
 [0.374]
 [0.329]
 [0.248]
 [0.37 ]] [[35.082]
 [37.637]
 [35.082]
 [35.666]
 [35.222]
 [34.925]
 [34.921]] [[1.613]
 [1.828]
 [1.613]
 [1.69 ]
 [1.615]
 [1.514]
 [1.635]]
printing an ep nov before normalisation:  56.00820647345649
printing an ep nov before normalisation:  34.2347095980132
printing an ep nov before normalisation:  41.07178989769149
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.05027857660580573, 0.05098270948099094, 0.049822490538924395, 0.28394380466023106, 0.21697005220073612, 0.34800236651331173]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.05027857660580573, 0.05098270948099094, 0.049822490538924395, 0.28394380466023106, 0.21697005220073612, 0.34800236651331173]
printing an ep nov before normalisation:  36.52894657393
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.533]
 [0.533]
 [0.528]
 [0.529]
 [0.533]
 [0.525]] [[28.509]
 [22.857]
 [22.857]
 [24.082]
 [23.647]
 [22.857]
 [24.182]] [[0.514]
 [0.533]
 [0.533]
 [0.528]
 [0.529]
 [0.533]
 [0.525]]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.05027857660580573, 0.05098270948099094, 0.049822490538924395, 0.28394380466023106, 0.21697005220073612, 0.34800236651331173]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.05027857660580573, 0.05098270948099094, 0.049822490538924395, 0.28394380466023106, 0.21697005220073612, 0.34800236651331173]
printing an ep nov before normalisation:  27.879521342000967
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.05027857660580573, 0.05098270948099094, 0.049822490538924395, 0.28394380466023106, 0.21697005220073612, 0.34800236651331173]
printing an ep nov before normalisation:  19.263008819690697
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.05027857660580573, 0.05098270948099094, 0.049822490538924395, 0.28394380466023106, 0.21697005220073612, 0.34800236651331173]
actions average: 
K:  0  action  0 :  tensor([0.5693, 0.0100, 0.0616, 0.0684, 0.1176, 0.0908, 0.0825],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0041, 0.9749, 0.0044, 0.0034, 0.0018, 0.0038, 0.0075],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1184, 0.0316, 0.2178, 0.1573, 0.1427, 0.1725, 0.1597],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1129, 0.0183, 0.1084, 0.2993, 0.1406, 0.1582, 0.1624],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0971, 0.0046, 0.0808, 0.1142, 0.4577, 0.1242, 0.1214],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1174, 0.0061, 0.1046, 0.1286, 0.1073, 0.4220, 0.1140],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1886, 0.0042, 0.1236, 0.1799, 0.1620, 0.1751, 0.1667],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.05027857660580573, 0.05098270948099094, 0.049822490538924395, 0.28394380466023106, 0.21697005220073612, 0.34800236651331173]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.05027857660580573, 0.05098270948099094, 0.049822490538924395, 0.28394380466023106, 0.21697005220073612, 0.34800236651331173]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.05021735102049273, 0.050922786810554795, 0.04976042102011163, 0.28431494903314897, 0.21721726954011722, 0.3475672225755746]
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.536]
 [0.481]
 [0.481]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.536]
 [0.481]
 [0.481]]
printing an ep nov before normalisation:  19.985660314559937
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.05021735102049273, 0.050922786810554795, 0.04976042102011163, 0.28431494903314897, 0.21721726954011722, 0.3475672225755746]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.05026334996158606, 0.05096943364755977, 0.0498060003013531, 0.2845759510671053, 0.21649894740141273, 0.34788631762098293]
printing an ep nov before normalisation:  42.30625196771895
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.05026334996158606, 0.05096943364755977, 0.0498060003013531, 0.2845759510671053, 0.21649894740141273, 0.34788631762098293]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.05107089195330147, 0.05178834988273564, 0.05060617488537253, 0.27306197516883374, 0.21998436482720854, 0.353488243282548]
Printing some Q and Qe and total Qs values:  [[0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.051118053237120605, 0.051836175434084586, 0.05065290590499621, 0.2733146697145222, 0.21926279419994982, 0.3538154015093265]
printing an ep nov before normalisation:  32.95500461868883
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.05105752551679034, 0.05177699556449401, 0.050591505144982285, 0.2736711850477858, 0.21951785902701562, 0.35338492969893187]
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.136]
 [0.53 ]
 [0.598]
 [0.619]
 [0.604]
 [0.721]] [[22.933]
 [29.795]
 [23.439]
 [20.818]
 [20.267]
 [21.135]
 [22.024]] [[0.704]
 [0.41 ]
 [0.688]
 [0.708]
 [0.719]
 [0.72 ]
 [0.853]]
printing an ep nov before normalisation:  29.073612689971924
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.05083507490525869, 0.05178911037476692, 0.05060334185797756, 0.2737353527207382, 0.21956932318731834, 0.35346779695394026]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.050847391501525785, 0.051558930871887676, 0.05061560216118063, 0.2738018167066897, 0.219622629051003, 0.35355362970771337]
siam score:  -0.74081975
Printing some Q and Qe and total Qs values:  [[ 0.27 ]
 [-0.036]
 [ 0.221]
 [ 0.216]
 [ 0.198]
 [ 0.181]
 [ 0.222]] [[12.284]
 [13.589]
 [13.707]
 [14.223]
 [15.006]
 [22.695]
 [25.517]] [[0.938]
 [0.703]
 [0.966]
 [0.99 ]
 [1.015]
 [1.419]
 [1.614]]
printing an ep nov before normalisation:  38.899478208571864
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.051043239258021074, 0.051515637645556486, 0.05058151654026046, 0.27436676892741824, 0.2191771977266723, 0.3533156399020713]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.05108978186646238, 0.05156261212686714, 0.050627637035671295, 0.27461747686769666, 0.21846396815633642, 0.35363852394696604]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.05108978186646238, 0.05156261212686714, 0.050627637035671295, 0.27461747686769666, 0.21846396815633642, 0.35363852394696604]
printing an ep nov before normalisation:  4.655129640296707
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.05108978186646238, 0.05156261212686714, 0.050627637035671295, 0.27461747686769666, 0.21846396815633642, 0.35363852394696604]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.05108978186646238, 0.05156261212686714, 0.050627637035671295, 0.27461747686769666, 0.21846396815633642, 0.35363852394696604]
using explorer policy with actor:  0
printing an ep nov before normalisation:  43.55677330318623
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.05108978186646238, 0.05156261212686714, 0.050627637035671295, 0.27461747686769666, 0.21846396815633642, 0.35363852394696604]
actor:  1 policy actor:  1  step number:  82 total reward:  0.20666666666666633  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.05279501029748705, 0.05324286033005946, 0.05235728116960554, 0.2645134510788449, 0.21132660410354748, 0.36576479302045556]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
printing an ep nov before normalisation:  55.108746130364345
printing an ep nov before normalisation:  32.46160708298059
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.05273675562714624, 0.053185459043881694, 0.052298192400619496, 0.26485862869888427, 0.21157043348290386, 0.3653505307465644]
Printing some Q and Qe and total Qs values:  [[0.647]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]] [[18.987]
 [20.043]
 [20.043]
 [20.043]
 [20.043]
 [20.043]
 [20.043]] [[0.647]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.05274863732328671, 0.05297175003769004, 0.05230997503734119, 0.2649184229152392, 0.21161819135574783, 0.365433023330695]
siam score:  -0.7399504
Printing some Q and Qe and total Qs values:  [[0.629]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]] [[15.886]
 [16.579]
 [16.579]
 [16.579]
 [16.579]
 [16.579]
 [16.579]] [[0.629]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]]
siam score:  -0.73861235
siam score:  -0.7350264
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.05287362898677134, 0.05287362898677134, 0.052216987271366554, 0.2658037918279293, 0.21144993490662248, 0.36478202802053905]
printing an ep nov before normalisation:  45.104512625812085
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.05281613020125402, 0.05281613020125402, 0.05215825029770759, 0.26614780210960315, 0.21169145352577412, 0.364370233664407]
siam score:  -0.7322069
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.05281613020125402, 0.05281613020125402, 0.05215825029770759, 0.26614780210960315, 0.21169145352577412, 0.364370233664407]
Printing some Q and Qe and total Qs values:  [[0.258]
 [0.118]
 [0.138]
 [0.181]
 [0.258]
 [0.11 ]
 [0.062]] [[36.849]
 [34.622]
 [33.626]
 [35.732]
 [36.849]
 [34.281]
 [40.018]] [[1.408]
 [1.139]
 [1.102]
 [1.266]
 [1.408]
 [1.112]
 [1.395]]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.05281613020125402, 0.05281613020125402, 0.05215825029770759, 0.26614780210960315, 0.21169145352577412, 0.364370233664407]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.052861341217349506, 0.052861341217349506, 0.05220289673939732, 0.2663760884883085, 0.2110155205101807, 0.36468281182741447]
actor:  1 policy actor:  1  step number:  75 total reward:  0.31999999999999995  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04902246262878383, 0.04902246262878383, 0.11259719745069989, 0.24991668312673002, 0.19702854620520005, 0.34241264795980236]
actions average: 
K:  1  action  0 :  tensor([0.4007, 0.0038, 0.1010, 0.1277, 0.1017, 0.1193, 0.1458],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0087, 0.9043, 0.0067, 0.0209, 0.0111, 0.0116, 0.0369],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1748, 0.0165, 0.3589, 0.1045, 0.0852, 0.1494, 0.1107],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1273, 0.0302, 0.0954, 0.3594, 0.1033, 0.1475, 0.1368],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1734, 0.0249, 0.0930, 0.1132, 0.3367, 0.1389, 0.1199],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0764, 0.0175, 0.1168, 0.1087, 0.0587, 0.5253, 0.0966],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1553, 0.0097, 0.1058, 0.1746, 0.1167, 0.1846, 0.2533],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04902246262878383, 0.04902246262878383, 0.11259719745069989, 0.24991668312673002, 0.19702854620520005, 0.34241264795980236]
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.585]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]] [[38.899]
 [44.101]
 [38.899]
 [38.899]
 [38.899]
 [38.899]
 [38.899]] [[1.596]
 [2.02 ]
 [1.596]
 [1.596]
 [1.596]
 [1.596]
 [1.596]]
actions average: 
K:  0  action  0 :  tensor([0.5048, 0.0024, 0.0753, 0.0867, 0.1148, 0.1090, 0.1071],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0122, 0.9059, 0.0059, 0.0108, 0.0111, 0.0116, 0.0425],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0875, 0.0033, 0.3954, 0.1514, 0.1148, 0.1333, 0.1144],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0823, 0.0345, 0.0760, 0.4068, 0.1261, 0.1464, 0.1280],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1234, 0.0241, 0.1152, 0.1364, 0.2662, 0.1714, 0.1633],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1943, 0.0034, 0.1757, 0.0944, 0.0992, 0.3036, 0.1295],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1596, 0.0216, 0.1168, 0.1488, 0.1381, 0.1586, 0.2565],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  57.64421051191168
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
actions average: 
K:  3  action  0 :  tensor([0.5520, 0.0248, 0.0662, 0.0667, 0.1144, 0.0854, 0.0906],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0114, 0.9219, 0.0075, 0.0133, 0.0171, 0.0132, 0.0155],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1168, 0.1039, 0.1034, 0.1168, 0.2307, 0.1614, 0.1671],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1423, 0.0455, 0.1061, 0.1502, 0.1727, 0.1996, 0.1837],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1007, 0.0217, 0.0677, 0.1146, 0.4921, 0.1033, 0.1000],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.2403, 0.0310, 0.0746, 0.0824, 0.0890, 0.3916, 0.0911],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1655, 0.1624, 0.0916, 0.1193, 0.1069, 0.1062, 0.2480],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  11.848849058151245
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.048901040469943896, 0.048901040469943896, 0.112700540569195, 0.2505055123800542, 0.19743039211056063, 0.34156147400030235]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.048901040469943896, 0.048901040469943896, 0.112700540569195, 0.2505055123800542, 0.19743039211056063, 0.34156147400030235]
printing an ep nov before normalisation:  33.369024244263656
line 256 mcts: sample exp_bonus 14.245009666381497
printing an ep nov before normalisation:  17.223785873440725
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.048901040469943896, 0.048901040469943896, 0.112700540569195, 0.2505055123800542, 0.19743039211056063, 0.34156147400030235]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.048901040469943896, 0.048901040469943896, 0.112700540569195, 0.2505055123800542, 0.19743039211056063, 0.34156147400030235]
printing an ep nov before normalisation:  44.69259803936332
line 256 mcts: sample exp_bonus 63.05086946912358
printing an ep nov before normalisation:  70.91344569914067
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04908382048555676, 0.04887449033370505, 0.11280320031467206, 0.2504351147888693, 0.19742664696911066, 0.34137672710808625]
printing an ep nov before normalisation:  30.84517363291983
printing an ep nov before normalisation:  38.38604012026973
printing an ep nov before normalisation:  25.22703722838022
printing an ep nov before normalisation:  0.004572220100271807
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.049122787808711835, 0.04891329101872458, 0.11289289171440431, 0.25063436864492133, 0.19678828540144963, 0.3416483754117883]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.049122787808711835, 0.04891329101872458, 0.11289289171440431, 0.25063436864492133, 0.19678828540144963, 0.3416483754117883]
actions average: 
K:  1  action  0 :  tensor([0.6354, 0.0065, 0.0505, 0.0615, 0.0612, 0.0799, 0.1050],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0155, 0.8956, 0.0093, 0.0118, 0.0129, 0.0166, 0.0383],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1420, 0.0016, 0.1199, 0.1610, 0.1664, 0.1919, 0.2172],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1386, 0.0173, 0.0850, 0.3285, 0.1094, 0.1561, 0.1652],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1578, 0.0019, 0.0525, 0.1051, 0.4875, 0.0901, 0.1051],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1110, 0.0073, 0.1043, 0.1436, 0.1234, 0.3528, 0.1575],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1180, 0.0413, 0.0946, 0.1546, 0.1348, 0.1188, 0.3379],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  24.53474459954102
Printing some Q and Qe and total Qs values:  [[0.521]
 [0.572]
 [0.531]
 [0.54 ]
 [0.542]
 [0.462]
 [0.539]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.521]
 [0.572]
 [0.531]
 [0.54 ]
 [0.542]
 [0.462]
 [0.539]]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04906265933348103, 0.048852794690088415, 0.11294473655333703, 0.2509280724228406, 0.19698744135795698, 0.34122429564229595]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04906265933348103, 0.048852794690088415, 0.11294473655333703, 0.2509280724228406, 0.19698744135795698, 0.34122429564229595]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04906265933348103, 0.048852794690088415, 0.11294473655333703, 0.2509280724228406, 0.19698744135795698, 0.34122429564229595]
using explorer policy with actor:  1
printing an ep nov before normalisation:  27.694127559661865
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.049002639994994414, 0.04879240816587216, 0.11299648729081613, 0.25122124311100563, 0.19718623583460634, 0.34080098560270516]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04904137619936275, 0.048830977730566376, 0.11308594804010252, 0.25142026732821393, 0.19655041574831994, 0.3410710149534345]
printing an ep nov before normalisation:  36.14233732223511
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04904137619936275, 0.048830977730566376, 0.11308594804010252, 0.25142026732821393, 0.19655041574831994, 0.3410710149534345]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04898148571664385, 0.048770720146197905, 0.1131378020396642, 0.2517134859968833, 0.19674789791690225, 0.3406486081837086]
printing an ep nov before normalisation:  25.88555038080758
printing an ep nov before normalisation:  35.20643949508667
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.049201694015566844, 0.04878273832647243, 0.11332832836352325, 0.25183990016107627, 0.19611346269642116, 0.3407338764369399]
printing an ep nov before normalisation:  25.469578923657885
printing an ep nov before normalisation:  35.97445277291793
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.0492400090703127, 0.048820726240162975, 0.11341671662500918, 0.2520364450199111, 0.19548626899237845, 0.3409998340522256]
printing an ep nov before normalisation:  42.74347125392651
printing an ep nov before normalisation:  39.925857231987024
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.0492400090703127, 0.048820726240162975, 0.11341671662500918, 0.2520364450199111, 0.19548626899237845, 0.3409998340522256]
printing an ep nov before normalisation:  46.33415550273772
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04921872851534877, 0.04879839047022495, 0.11355695048926168, 0.2525255450971048, 0.19505746095153811, 0.34084292447652176]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04921872851534877, 0.04879839047022495, 0.11355695048926168, 0.2525255450971048, 0.19505746095153811, 0.34084292447652176]
printing an ep nov before normalisation:  61.20881861138244
UNIT TEST: sample policy line 217 mcts : [0.143 0.041 0.102 0.122 0.082 0.102 0.408]
printing an ep nov before normalisation:  20.93906053072119
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.049159504121916636, 0.04873843706424264, 0.11360931097809168, 0.2528189255238768, 0.19525117170752224, 0.3404226506043499]
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.279]
 [0.53 ]
 [0.574]
 [0.574]
 [0.517]
 [0.518]] [[15.577]
 [18.615]
 [14.603]
 [14.342]
 [14.342]
 [15.356]
 [14.976]] [[1.349]
 [1.343]
 [1.21 ]
 [1.229]
 [1.229]
 [1.27 ]
 [1.234]]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.049159504121916636, 0.04873843706424264, 0.11360931097809168, 0.2528189255238768, 0.19525117170752224, 0.3404226506043499]
Printing some Q and Qe and total Qs values:  [[0.053]
 [0.052]
 [0.057]
 [0.089]
 [0.089]
 [0.089]
 [0.089]] [[19.363]
 [23.086]
 [21.046]
 [18.268]
 [18.268]
 [18.268]
 [18.268]] [[0.053]
 [0.052]
 [0.057]
 [0.089]
 [0.089]
 [0.089]
 [0.089]]
printing an ep nov before normalisation:  22.7825665473938
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.049159504121916636, 0.04873843706424264, 0.11360931097809168, 0.2528189255238768, 0.19525117170752224, 0.3404226506043499]
from probs:  [0.049159504121916636, 0.04873843706424264, 0.11360931097809168, 0.2528189255238768, 0.19525117170752224, 0.3404226506043499]
using explorer policy with actor:  0
printing an ep nov before normalisation:  13.43181848526001
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.049234904077055094, 0.048813189439369686, 0.11378383150298021, 0.2532075434523349, 0.19401453286101455, 0.3409459986672455]
printing an ep nov before normalisation:  1.7079779013329244e-05
printing an ep nov before normalisation:  29.49706322274685
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.0491758258353845, 0.048753381256813534, 0.11383648023741093, 0.25350151904306073, 0.19420605196882995, 0.34052674165850044]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.0491758258353845, 0.048753381256813534, 0.11383648023741093, 0.25350151904306073, 0.19420605196882995, 0.34052674165850044]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.0491758258353845, 0.048753381256813534, 0.11383648023741093, 0.25350151904306073, 0.19420605196882995, 0.34052674165850044]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
printing an ep nov before normalisation:  42.86497116088867
siam score:  -0.7433215
printing an ep nov before normalisation:  36.141136745636295
using explorer policy with actor:  1
printing an ep nov before normalisation:  29.295451925606088
printing an ep nov before normalisation:  36.129889409392725
printing an ep nov before normalisation:  47.959818475731346
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.049036618088158845, 0.048611666614003145, 0.11408098556700096, 0.2545748329001749, 0.19416291166695537, 0.3395329851637067]
printing an ep nov before normalisation:  33.18077984775147
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.53 ]
 [0.546]
 [0.57 ]
 [0.519]
 [0.53 ]
 [0.525]] [[38.332]
 [41.696]
 [38.332]
 [35.512]
 [39.584]
 [35.244]
 [42.756]] [[0.546]
 [0.53 ]
 [0.546]
 [0.57 ]
 [0.519]
 [0.53 ]
 [0.525]]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04907375616958817, 0.04864848197993115, 0.11416751946638774, 0.2547680602578274, 0.19355145091426695, 0.3397907312119986]
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.517]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [27.192]] [[0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [1.304]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.2898],
        [-0.5467],
        [-0.4431],
        [-0.4873],
        [-0.0000],
        [-0.5497],
        [-0.3554],
        [-0.4844],
        [-0.3822]], dtype=torch.float64)
-0.07776244753199966 -0.07776244753199966
-0.083839701198 -0.3736594533357041
-0.032346567066 -0.5790508901669692
-0.032346567066 -0.47549000451255197
-0.032346567066 -0.5196074009812388
-0.7854000000000001 -0.7854000000000001
-0.032346567066 -0.5820522066969769
-0.032346567066 -0.3877880016851867
-0.032346567066 -0.5167612277251543
-0.057834381198 -0.440079074728346
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04907375616958817, 0.04864848197993115, 0.11416751946638774, 0.2547680602578274, 0.19355145091426695, 0.3397907312119986]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
actor:  1 policy actor:  1  step number:  56 total reward:  0.21999999999999975  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  33.662584789600935
printing an ep nov before normalisation:  34.49034113016507
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04716230557437869, 0.04675364114788975, 0.10971372791004426, 0.244822889867204, 0.22502257098001519, 0.3265248645204681]
line 256 mcts: sample exp_bonus 32.61264604936997
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04716230557437869, 0.04675364114788975, 0.10971372791004426, 0.244822889867204, 0.22502257098001519, 0.3265248645204681]
printing an ep nov before normalisation:  36.021504402160645
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04716230557437869, 0.04675364114788975, 0.10971372791004426, 0.244822889867204, 0.22502257098001519, 0.3265248645204681]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04716230557437869, 0.04675364114788975, 0.10971372791004426, 0.244822889867204, 0.22502257098001519, 0.3265248645204681]
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.536]
 [0.495]
 [0.501]
 [0.508]
 [0.562]
 [0.504]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.501]
 [0.536]
 [0.495]
 [0.501]
 [0.508]
 [0.562]
 [0.504]]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.047102841292039795, 0.046693505649568805, 0.10975700199113855, 0.24508807567573773, 0.22525523550794546, 0.3261033398835696]
printing an ep nov before normalisation:  61.525062574282025
printing an ep nov before normalisation:  62.58865237903744
actor:  1 policy actor:  1  step number:  70 total reward:  0.21999999999999975  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  4  action  0 :  tensor([0.5314, 0.0663, 0.0415, 0.0653, 0.0971, 0.0919, 0.1067],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0210, 0.8629, 0.0092, 0.0146, 0.0172, 0.0139, 0.0612],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1566, 0.0074, 0.2357, 0.1349, 0.1202, 0.1450, 0.2002],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1125, 0.0279, 0.0675, 0.3663, 0.1455, 0.1255, 0.1549],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1534, 0.1270, 0.0380, 0.0703, 0.4635, 0.0560, 0.0917],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1080, 0.1340, 0.1069, 0.1688, 0.1567, 0.1446, 0.1811],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1276, 0.2611, 0.0838, 0.1335, 0.1082, 0.1106, 0.1752],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
Printing some Q and Qe and total Qs values:  [[0.463]
 [0.492]
 [0.474]
 [0.482]
 [0.459]
 [0.47 ]
 [0.467]] [[29.488]
 [33.447]
 [29.647]
 [32.582]
 [31.249]
 [31.612]
 [31.019]] [[1.841]
 [2.245]
 [1.867]
 [2.153]
 [2.004]
 [2.049]
 [1.989]]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04621415671626161, 0.045812565873813826, 0.1076828747250851, 0.2593587186644731, 0.22099583379029175, 0.31993585023007476]
printing an ep nov before normalisation:  50.333188798422746
printing an ep nov before normalisation:  30.395123468870224
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04621415671626161, 0.045812565873813826, 0.1076828747250851, 0.2593587186644731, 0.22099583379029175, 0.31993585023007476]
line 256 mcts: sample exp_bonus 38.56901400929554
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04621415671626161, 0.045812565873813826, 0.1076828747250851, 0.2593587186644731, 0.22099583379029175, 0.31993585023007476]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04621415671626161, 0.045812565873813826, 0.1076828747250851, 0.2593587186644731, 0.22099583379029175, 0.31993585023007476]
siam score:  -0.7449452
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04621415671626161, 0.045812565873813826, 0.1076828747250851, 0.2593587186644731, 0.22099583379029175, 0.31993585023007476]
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.630933167951696
actor:  1 policy actor:  1  step number:  60 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04515807157349375, 0.044763404819289414, 0.10556696753779506, 0.2758462805561428, 0.21607562288119822, 0.31258965263208077]
printing an ep nov before normalisation:  46.512610380919114
using explorer policy with actor:  1
from probs:  [0.04515807157349375, 0.044763404819289414, 0.10556696753779506, 0.2758462805561428, 0.21607562288119822, 0.31258965263208077]
siam score:  -0.7444028
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04512228929798868, 0.04472679625948382, 0.1051269005024676, 0.2762934729507017, 0.2163976779148258, 0.3123328630745323]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04512228929798868, 0.04472679625948382, 0.1051269005024676, 0.2762934729507017, 0.2163976779148258, 0.3123328630745323]
line 256 mcts: sample exp_bonus 42.45896689295721
printing an ep nov before normalisation:  39.801159568315114
Printing some Q and Qe and total Qs values:  [[0.285]
 [0.387]
 [0.34 ]
 [0.389]
 [0.35 ]
 [0.386]
 [0.368]] [[34.495]
 [33.761]
 [32.071]
 [34.487]
 [33.31 ]
 [34.186]
 [32.374]] [[1.389]
 [1.444]
 [1.288]
 [1.493]
 [1.377]
 [1.47 ]
 [1.335]]
printing an ep nov before normalisation:  21.490966202175432
printing an ep nov before normalisation:  39.150203805575224
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04523704918594353, 0.0446469134133424, 0.10526495616881754, 0.2764979811522878, 0.21657893258405922, 0.31177416749554937]
printing an ep nov before normalisation:  37.116095493887705
printing an ep nov before normalisation:  52.59942834005397
printing an ep nov before normalisation:  33.56452393967822
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04517796983129144, 0.04458691968149239, 0.1052988861722343, 0.27679722564915255, 0.2167853363919255, 0.31135366227390376]
printing an ep nov before normalisation:  34.23001860701378
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04517796983129144, 0.04458691968149239, 0.1052988861722343, 0.27679722564915255, 0.2167853363919255, 0.31135366227390376]
printing an ep nov before normalisation:  61.10797989903422
Printing some Q and Qe and total Qs values:  [[0.733]
 [0.733]
 [0.733]
 [0.733]
 [0.733]
 [0.733]
 [0.733]] [[39.801]
 [39.801]
 [39.801]
 [39.801]
 [39.801]
 [39.801]
 [39.801]] [[1.974]
 [1.974]
 [1.974]
 [1.974]
 [1.974]
 [1.974]
 [1.974]]
siam score:  -0.7416275
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.940840317468638
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04511901071536766, 0.044527048049317765, 0.10533274712106361, 0.2770958611214609, 0.21699132012527458, 0.31093401286751543]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  78 total reward:  0.11333333333333251  reward:  1.0 rdn_beta:  1.333
from probs:  [0.04443539084090081, 0.043852423848768335, 0.10373409802411397, 0.2880712451404625, 0.2136958678844178, 0.3062109742613366]
siam score:  -0.7345303
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.044376411435023584, 0.043792561269712635, 0.10376495394066317, 0.28838136493700145, 0.21389331176753862, 0.3057913966500606]
printing an ep nov before normalisation:  35.33853921549221
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.044376411435023584, 0.043792561269712635, 0.10376495394066317, 0.28838136493700145, 0.21389331176753862, 0.3057913966500606]
printing an ep nov before normalisation:  15.55296468730507
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04441349107573333, 0.04382915159315209, 0.10385180635053855, 0.2886229419725542, 0.21323504420006292, 0.3060475648079588]
printing an ep nov before normalisation:  55.275416824284385
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04441349107573333, 0.04382915159315209, 0.10385180635053855, 0.2886229419725542, 0.21323504420006292, 0.3060475648079588]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04445032946953473, 0.04386550385327064, 0.10393809368278739, 0.2888629472638333, 0.21258105944149497, 0.306302066289079]
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.6  ]
 [0.482]
 [0.482]
 [0.545]
 [0.482]
 [0.482]] [[61.608]
 [58.638]
 [61.608]
 [61.608]
 [64.522]
 [61.608]
 [61.608]] [[1.562]
 [1.587]
 [1.562]
 [1.562]
 [1.716]
 [1.562]
 [1.562]]
printing an ep nov before normalisation:  30.444319773896215
printing an ep nov before normalisation:  71.58476803055045
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04445032946953473, 0.04386550385327064, 0.10393809368278739, 0.2888629472638333, 0.21258105944149497, 0.306302066289079]
printing an ep nov before normalisation:  51.525959968566895
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04445032946953473, 0.04386550385327064, 0.10393809368278739, 0.2888629472638333, 0.21258105944149497, 0.306302066289079]
printing an ep nov before normalisation:  67.09772717029149
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04445032946953473, 0.04386550385327064, 0.10393809368278739, 0.2888629472638333, 0.21258105944149497, 0.306302066289079]
siam score:  -0.7389264
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04445032946953473, 0.04386550385327064, 0.10393809368278739, 0.2888629472638333, 0.21258105944149497, 0.306302066289079]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.044473421344940554, 0.04388829099936348, 0.10347153864306473, 0.2890133928102064, 0.2126917575373061, 0.3064615986651188]
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.398]
 [0.418]
 [0.419]
 [0.417]
 [0.419]
 [0.418]] [[57.463]
 [44.714]
 [55.459]
 [56.066]
 [56.114]
 [57.098]
 [57.017]] [[1.356]
 [0.927]
 [1.291]
 [1.31 ]
 [1.31 ]
 [1.344]
 [1.34 ]]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.044414581992552986, 0.04382856643664929, 0.10350195417734916, 0.289324504033184, 0.21288740611132356, 0.30604298724894097]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.044414581992552986, 0.04382856643664929, 0.10350195417734916, 0.289324504033184, 0.21288740611132356, 0.30604298724894097]
printing an ep nov before normalisation:  42.68563513870775
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.298790454864502
printing an ep nov before normalisation:  21.56484070953745
printing an ep nov before normalisation:  0.0005998585402267054
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.044414581992552986, 0.04382856643664929, 0.10350195417734916, 0.289324504033184, 0.21288740611132356, 0.30604298724894097]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.044414581992552986, 0.04382856643664929, 0.10350195417734916, 0.289324504033184, 0.21288740611132356, 0.30604298724894097]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.044414581992552986, 0.04382856643664929, 0.10350195417734916, 0.289324504033184, 0.21288740611132356, 0.30604298724894097]
actor:  1 policy actor:  1  step number:  69 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  1.0
from probs:  [0.044414581992552986, 0.04382856643664929, 0.10350195417734916, 0.289324504033184, 0.21288740611132356, 0.30604298724894097]
printing an ep nov before normalisation:  29.94904375651068
using another actor
from probs:  [0.04175143277865092, 0.041384701773576615, 0.1537920203497641, 0.2731501397070111, 0.20098814123298475, 0.2889335641580125]
printing an ep nov before normalisation:  30.96848726272583
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04175143277865092, 0.041384701773576615, 0.1537920203497641, 0.2731501397070111, 0.20098814123298475, 0.2889335641580125]
Printing some Q and Qe and total Qs values:  [[0.689]
 [0.542]
 [0.485]
 [0.473]
 [0.475]
 [0.523]
 [0.689]] [[ 0.   ]
 [29.184]
 [19.872]
 [14.213]
 [14.362]
 [26.999]
 [ 0.   ]] [[0.507]
 [1.115]
 [0.817]
 [0.659]
 [0.664]
 [1.039]
 [0.507]]
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.466]
 [0.457]
 [0.457]
 [0.457]
 [0.457]
 [0.457]] [[35.746]
 [46.253]
 [35.746]
 [35.746]
 [35.746]
 [35.746]
 [35.746]] [[0.879]
 [1.12 ]
 [0.879]
 [0.879]
 [0.879]
 [0.879]
 [0.879]]
printing an ep nov before normalisation:  34.249471108190804
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.041784388495806624, 0.04141736709178366, 0.15312247111694544, 0.2733663303846269, 0.20114718984134428, 0.2891622530694932]
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.413]] [[31.357]
 [31.357]
 [31.357]
 [31.357]
 [31.357]
 [31.357]
 [34.231]] [[1.473]
 [1.473]
 [1.473]
 [1.473]
 [1.473]
 [1.473]
 [1.645]]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.041784388495806624, 0.04141736709178366, 0.15312247111694544, 0.2733663303846269, 0.20114718984134428, 0.2891622530694932]
actor:  1 policy actor:  1  step number:  88 total reward:  0.033333333333332216  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  26.5376079710915
Printing some Q and Qe and total Qs values:  [[0.052]
 [0.051]
 [0.052]
 [0.052]
 [0.051]
 [0.052]
 [0.05 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.052]
 [0.051]
 [0.052]
 [0.052]
 [0.051]
 [0.052]
 [0.05 ]]
printing an ep nov before normalisation:  37.540600567893705
printing an ep nov before normalisation:  50.669087439010454
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04299036092303639, 0.04263398390539131, 0.15109940688036813, 0.2678559433827186, 0.19773130857505591, 0.29768899633342966]
printing an ep nov before normalisation:  38.17712283783502
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04299036092303639, 0.04263398390539131, 0.15109940688036813, 0.2678559433827186, 0.19773130857505591, 0.29768899633342966]
printing an ep nov before normalisation:  39.06355870851719
printing an ep nov before normalisation:  42.023327787026695
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04299036092303639, 0.04263398390539131, 0.15109940688036813, 0.2678559433827186, 0.19773130857505591, 0.29768899633342966]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04299036092303639, 0.04263398390539131, 0.15109940688036813, 0.2678559433827186, 0.19773130857505591, 0.29768899633342966]
maxi score, test score, baseline:  -0.9844066666666667 -0.945 -0.945
probs:  [0.04299036092303639, 0.04263398390539131, 0.15109940688036813, 0.2678559433827186, 0.19773130857505591, 0.29768899633342966]
printing an ep nov before normalisation:  23.952334783157163
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  77 total reward:  0.09333333333333282  reward:  1.0 rdn_beta:  1.333
using another actor
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04281882087329985, 0.04264160729798551, 0.15112648579699567, 0.26790396500661323, 0.19776675189076343, 0.2977423691343424]
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04281882087329985, 0.04264160729798551, 0.15112648579699567, 0.26790396500661323, 0.19776675189076343, 0.2977423691343424]
using another actor
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04281882087329985, 0.04264160729798551, 0.15112648579699567, 0.26790396500661323, 0.19776675189076343, 0.2977423691343424]
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04281882087329985, 0.04264160729798551, 0.15112648579699567, 0.26790396500661323, 0.19776675189076343, 0.2977423691343424]
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]] [[17.291]
 [17.5  ]
 [17.5  ]
 [17.5  ]
 [17.5  ]
 [17.5  ]
 [17.5  ]] [[0.49 ]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]]
printing an ep nov before normalisation:  34.764252612953804
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04281882087329985, 0.04264160729798551, 0.15112648579699567, 0.26790396500661323, 0.19776675189076343, 0.2977423691343424]
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04281882087329985, 0.04264160729798551, 0.15112648579699567, 0.26790396500661323, 0.19776675189076343, 0.2977423691343424]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04281882087329985, 0.04264160729798551, 0.15112648579699567, 0.26790396500661323, 0.19776675189076343, 0.2977423691343424]
siam score:  -0.73820376
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04281882087329985, 0.04264160729798551, 0.15112648579699567, 0.26790396500661323, 0.19776675189076343, 0.2977423691343424]
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
printing an ep nov before normalisation:  21.979242245051747
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04281882087329985, 0.04264160729798551, 0.15112648579699567, 0.26790396500661323, 0.19776675189076343, 0.2977423691343424]
printing an ep nov before normalisation:  28.962955235269504
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.37 ]
 [0.265]
 [0.335]
 [0.345]
 [0.308]
 [0.332]] [[21.407]
 [25.036]
 [21.432]
 [19.868]
 [21.471]
 [25.162]
 [24.436]] [[0.7  ]
 [0.892]
 [0.661]
 [0.676]
 [0.742]
 [0.835]
 [0.833]]
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04281882087329985, 0.04264160729798551, 0.15112648579699567, 0.26790396500661323, 0.19776675189076343, 0.2977423691343424]
printing an ep nov before normalisation:  45.446319580078125
printing an ep nov before normalisation:  36.45829777402682
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04281882087329985, 0.04264160729798551, 0.15112648579699567, 0.26790396500661323, 0.19776675189076343, 0.2977423691343424]
printing an ep nov before normalisation:  36.69787075612816
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04301063685263616, 0.04265829212962818, 0.1505269951672126, 0.2680303926100875, 0.19791355471033992, 0.2978601285300957]
printing an ep nov before normalisation:  38.71398167488672
line 256 mcts: sample exp_bonus 39.04429256916046
printing an ep nov before normalisation:  22.94843388278124
siam score:  -0.7387294
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04304318334316993, 0.04269057116967542, 0.1506411529334716, 0.26823374240551934, 0.1973052292885964, 0.29808612085956737]
printing an ep nov before normalisation:  53.89532981186063
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04304318334316993, 0.04269057116967542, 0.1506411529334716, 0.26823374240551934, 0.1973052292885964, 0.29808612085956737]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
printing an ep nov before normalisation:  38.84888014018817
printing an ep nov before normalisation:  43.40489055137079
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.75490223383957
printing an ep nov before normalisation:  37.47186163812184
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04287347053358789, 0.04269812381723192, 0.15066786355972095, 0.26828132217658235, 0.1973402213836987, 0.2981389985291781]
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04287347053358789, 0.04269812381723192, 0.15066786355972095, 0.26828132217658235, 0.1973402213836987, 0.2981389985291781]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  79 total reward:  0.026666666666665506  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  32.70938210221946
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.0440264213227496, 0.04385580314594484, 0.14818190048275398, 0.263355744871249, 0.19432769844218103, 0.3062524317351216]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.0440264213227496, 0.04385580314594484, 0.14818190048275398, 0.263355744871249, 0.19432769844218103, 0.3062524317351216]
printing an ep nov before normalisation:  36.7240528393922
siam score:  -0.73517364
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.0440264213227496, 0.04385580314594484, 0.14818190048275398, 0.263355744871249, 0.19432769844218103, 0.3062524317351216]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.392]] [[21.213]
 [21.213]
 [21.213]
 [21.213]
 [21.213]
 [21.213]
 [22.356]] [[2.028]
 [2.028]
 [2.028]
 [2.028]
 [2.028]
 [2.028]
 [2.186]]
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.47 ]
 [0.445]
 [0.445]
 [0.445]
 [0.456]
 [0.451]] [[56.255]
 [52.868]
 [56.255]
 [56.255]
 [56.255]
 [54.119]
 [53.933]] [[1.936]
 [1.794]
 [1.936]
 [1.936]
 [1.936]
 [1.842]
 [1.828]]
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.479]
 [0.428]] [[69.571]
 [69.571]
 [69.571]
 [69.571]
 [69.571]
 [56.154]
 [69.571]] [[1.695]
 [1.695]
 [1.695]
 [1.695]
 [1.695]
 [1.339]
 [1.695]]
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04409058013018893, 0.04391971256030951, 0.14839830365054388, 0.2637404979752061, 0.19315101863538522, 0.3066998870483664]
siam score:  -0.7347649
using explorer policy with actor:  1
printing an ep nov before normalisation:  5.488689112098655e-06
printing an ep nov before normalisation:  69.99684583817546
actions average: 
K:  0  action  0 :  tensor([0.4804, 0.0050, 0.0914, 0.1042, 0.0853, 0.1095, 0.1242],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0027, 0.9588, 0.0032, 0.0059, 0.0040, 0.0057, 0.0197],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1769, 0.0363, 0.2782, 0.0897, 0.0892, 0.2030, 0.1267],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0979, 0.0611, 0.0907, 0.3271, 0.1172, 0.1554, 0.1508],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0991, 0.0042, 0.0844, 0.1021, 0.4612, 0.1234, 0.1255],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0899, 0.0536, 0.1027, 0.0812, 0.0686, 0.4999, 0.1041],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1397, 0.0252, 0.1118, 0.1520, 0.1313, 0.1508, 0.2891],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04400593616886247, 0.043834437943515596, 0.14869864928191867, 0.2644665603504197, 0.19289241047514055, 0.30610200578014307]
printing an ep nov before normalisation:  40.72198509833031
printing an ep nov before normalisation:  27.79305403276908
printing an ep nov before normalisation:  40.47554696758666
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
Printing some Q and Qe and total Qs values:  [[1.025]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]] [[8.031]
 [7.734]
 [7.734]
 [7.734]
 [7.734]
 [7.734]
 [7.734]] [[1.033]
 [0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.816]]
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04400593616886247, 0.043834437943515596, 0.14869864928191867, 0.2644665603504197, 0.19289241047514055, 0.30610200578014307]
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.552]
 [0.479]
 [0.446]
 [0.62 ]
 [0.471]
 [0.552]] [[18.644]
 [12.506]
 [18.14 ]
 [20.072]
 [15.697]
 [19.572]
 [12.506]] [[0.975]
 [0.902]
 [1.04 ]
 [1.08 ]
 [1.09 ]
 [1.086]
 [0.902]]
printing an ep nov before normalisation:  42.19138874469952
Printing some Q and Qe and total Qs values:  [[0.137]
 [0.203]
 [0.137]
 [0.137]
 [0.137]
 [0.137]
 [0.137]] [[17.734]
 [19.913]
 [17.734]
 [17.734]
 [17.734]
 [17.734]
 [17.734]] [[0.314]
 [0.422]
 [0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]]
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
using explorer policy with actor:  1
printing an ep nov before normalisation:  27.301417343952927
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04419090655696744, 0.043849914165075554, 0.14812568174535254, 0.2645878514279924, 0.19303437550541078, 0.30621127059920134]
using explorer policy with actor:  1
siam score:  -0.74610007
siam score:  -0.7461061
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
Printing some Q and Qe and total Qs values:  [[0.563]
 [0.563]
 [0.563]
 [0.493]
 [0.563]
 [0.563]
 [0.497]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.563]
 [0.563]
 [0.563]
 [0.493]
 [0.563]
 [0.563]
 [0.497]]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.041 0.265 0.041 0.061 0.082 0.122 0.388]
from probs:  [0.04402698941677843, 0.04385741661725473, 0.14815107951408038, 0.264633235886253, 0.19306748029283788, 0.30626379827279554]
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04386483824159614, 0.04386483824159614, 0.14817620365937403, 0.2646781313940289, 0.19310022842509103, 0.3063157600383136]
printing an ep nov before normalisation:  23.915701463342113
printing an ep nov before normalisation:  34.46849726466993
actor:  1 policy actor:  1  step number:  83 total reward:  0.013333333333332753  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.042617665121604635, 0.042617665121604635, 0.1724501995512627, 0.25713362793619693, 0.18759704092903917, 0.29758380134029194]
printing an ep nov before normalisation:  30.855863170093283
printing an ep nov before normalisation:  27.621571910336428
printing an ep nov before normalisation:  25.468626022338867
printing an ep nov before normalisation:  34.44949055909233
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.385]
 [0.375]
 [0.378]
 [0.375]
 [0.375]
 [0.375]] [[16.815]
 [41.586]
 [16.815]
 [18.811]
 [16.815]
 [16.815]
 [16.815]] [[0.375]
 [0.385]
 [0.375]
 [0.378]
 [0.375]
 [0.375]
 [0.375]]
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04260315469918732, 0.04276593971485229, 0.17247168230543755, 0.2570724105592291, 0.1876037315828913, 0.2974830811384024]
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04260315469918732, 0.04276593971485229, 0.17247168230543755, 0.2570724105592291, 0.1876037315828913, 0.2974830811384024]
printing an ep nov before normalisation:  30.155321640953915
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
printing an ep nov before normalisation:  0.0
printing an ep nov before normalisation:  32.03537677215797
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.042639171063439, 0.042802094127947055, 0.17177046685624903, 0.2572903062723093, 0.18776271480501555, 0.29773524687504005]
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04267486647333507, 0.042837926356484916, 0.17107550018195267, 0.25750626024086215, 0.1879202812723209, 0.29798516547504433]
Printing some Q and Qe and total Qs values:  [[-0.105]
 [-0.104]
 [-0.088]
 [-0.088]
 [-0.099]
 [-0.09 ]
 [-0.089]] [[18.012]
 [21.513]
 [15.69 ]
 [16.398]
 [19.336]
 [16.061]
 [16.223]] [[0.487]
 [0.603]
 [0.427]
 [0.451]
 [0.536]
 [0.437]
 [0.443]]
printing an ep nov before normalisation:  79.51388573160156
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04265193276528138, 0.042815362320310885, 0.17051150972678114, 0.2579703692290635, 0.18822663239178616, 0.297824193566777]
using another actor
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04268173919184151, 0.04284528331411649, 0.17063094816353258, 0.2581511179999528, 0.18765803069938045, 0.29803288063117617]
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04268173919184151, 0.04284528331411649, 0.17063094816353258, 0.2581511179999528, 0.18765803069938045, 0.29803288063117617]
printing an ep nov before normalisation:  30.003933906555176
actor:  1 policy actor:  1  step number:  73 total reward:  0.10666666666666635  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  44.25353223112852
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04393040431919742, 0.044088929091019025, 0.16795271019508343, 0.2527867889752309, 0.18445721268796014, 0.3067839547315092]
printing an ep nov before normalisation:  43.18294746789286
siam score:  -0.72848344
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04393040431919742, 0.044088929091019025, 0.16795271019508343, 0.2527867889752309, 0.18445721268796014, 0.3067839547315092]
printing an ep nov before normalisation:  44.6738998045803
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.533]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]] [[29.485]
 [34.279]
 [29.485]
 [29.485]
 [29.485]
 [29.485]
 [29.485]] [[1.563]
 [1.866]
 [1.563]
 [1.563]
 [1.563]
 [1.563]
 [1.563]]
actions average: 
K:  3  action  0 :  tensor([0.5087, 0.0054, 0.0580, 0.1393, 0.0980, 0.0938, 0.0968],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0188, 0.8795, 0.0120, 0.0142, 0.0140, 0.0208, 0.0408],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.2029, 0.0025, 0.1178, 0.1649, 0.1447, 0.1738, 0.1934],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2534, 0.0059, 0.0825, 0.2474, 0.1167, 0.1473, 0.1468],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1767, 0.0177, 0.0726, 0.1487, 0.3650, 0.1083, 0.1110],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0931, 0.0272, 0.1258, 0.0840, 0.0999, 0.4579, 0.1121],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1471, 0.0886, 0.0752, 0.1441, 0.1095, 0.1221, 0.3133],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04393040431919742, 0.044088929091019025, 0.16795271019508343, 0.2527867889752309, 0.18445721268796014, 0.3067839547315092]
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]] [[36.009]
 [36.009]
 [36.009]
 [36.009]
 [36.009]
 [36.009]
 [36.009]] [[1.247]
 [1.247]
 [1.247]
 [1.247]
 [1.247]
 [1.247]
 [1.247]]
printing an ep nov before normalisation:  42.90420644250975
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04393040431919742, 0.044088929091019025, 0.16795271019508343, 0.2527867889752309, 0.18445721268796014, 0.3067839547315092]
siam score:  -0.7224094
siam score:  -0.7235348
printing an ep nov before normalisation:  40.66992319585587
siam score:  -0.7231478
printing an ep nov before normalisation:  58.79542664069884
siam score:  -0.722841
siam score:  -0.7235208
Printing some Q and Qe and total Qs values:  [[0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]] [[21.24]
 [21.24]
 [21.24]
 [21.24]
 [21.24]
 [21.24]
 [21.24]] [[1.44]
 [1.44]
 [1.44]
 [1.44]
 [1.44]
 [1.44]
 [1.44]]
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04390238859410665, 0.04406125200115715, 0.1681896267039593, 0.25320492515946275, 0.1840543958718424, 0.3065874116694718]
Printing some Q and Qe and total Qs values:  [[0.537]
 [0.115]
 [0.354]
 [0.36 ]
 [0.357]
 [0.359]
 [0.363]] [[25.64 ]
 [25.695]
 [28.192]
 [27.877]
 [28.151]
 [28.662]
 [28.47 ]] [[1.436]
 [1.016]
 [1.342]
 [1.338]
 [1.344]
 [1.364]
 [1.361]]
actor:  1 policy actor:  1  step number:  78 total reward:  0.11333333333333273  reward:  1.0 rdn_beta:  2.0
siam score:  -0.7224208
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04512161416319614, 0.045275830129924116, 0.1649958975077045, 0.24830114816868942, 0.18117357032156323, 0.3151319397089226]
line 256 mcts: sample exp_bonus 16.59526302476459
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04515090099691217, 0.045305217356155496, 0.16510322088385188, 0.24846270213997856, 0.18064095899854377, 0.31533699962455813]
printing an ep nov before normalisation:  35.87007485762176
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04515090099691217, 0.045305217356155496, 0.16510322088385188, 0.24846270213997856, 0.18064095899854377, 0.31533699962455813]
printing an ep nov before normalisation:  27.15322162838442
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04515090099691217, 0.045305217356155496, 0.16510322088385188, 0.24846270213997856, 0.18064095899854377, 0.31533699962455813]
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04515090099691217, 0.045305217356155496, 0.16510322088385188, 0.24846270213997856, 0.18064095899854377, 0.31533699962455813]
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.045179994018223917, 0.04533441010561188, 0.16520983402270398, 0.24862318699045052, 0.180111872354295, 0.31554070250871474]
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.045179994018223917, 0.04533441010561188, 0.16520983402270398, 0.24862318699045052, 0.180111872354295, 0.31554070250871474]
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.045164047869278634, 0.045470902026321654, 0.16523659744343153, 0.2485735976292603, 0.180124995128263, 0.31542985990344485]
siam score:  -0.72914064
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.045164047869278634, 0.045470902026321654, 0.16523659744343153, 0.2485735976292603, 0.180124995128263, 0.31542985990344485]
printing an ep nov before normalisation:  32.060619525221476
printing an ep nov before normalisation:  24.923531359439757
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.045164047869278634, 0.045470902026321654, 0.16523659744343153, 0.2485735976292603, 0.180124995128263, 0.31542985990344485]
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.045164047869278634, 0.045470902026321654, 0.16523659744343153, 0.2485735976292603, 0.180124995128263, 0.31542985990344485]
printing an ep nov before normalisation:  31.754753589630127
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.617]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.47 ]] [[36.126]
 [48.345]
 [36.126]
 [36.126]
 [36.126]
 [36.126]
 [36.126]] [[0.944]
 [1.368]
 [0.944]
 [0.944]
 [0.944]
 [0.944]
 [0.944]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.378938923628493
printing an ep nov before normalisation:  22.43988032847577
printing an ep nov before normalisation:  42.29001522064209
printing an ep nov before normalisation:  17.228629790875193
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.517]
 [0.25 ]
 [0.271]
 [0.517]
 [0.517]
 [0.517]] [[23.032]
 [23.032]
 [21.069]
 [21.081]
 [23.032]
 [23.032]
 [23.032]] [[1.517]
 [1.517]
 [1.106]
 [1.127]
 [1.517]
 [1.517]
 [1.517]]
actor:  1 policy actor:  1  step number:  73 total reward:  0.09333333333333238  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  37.056483774479396
from probs:  [0.04384314346632078, 0.04414099656593144, 0.19032475129742107, 0.24128601166876518, 0.17422394055573995, 0.30618115644582167]
printing an ep nov before normalisation:  46.28955467796883
printing an ep nov before normalisation:  36.80034567636359
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
printing an ep nov before normalisation:  38.69614901104098
printing an ep nov before normalisation:  49.8954421499774
printing an ep nov before normalisation:  24.2387445352378
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04378618294556642, 0.044084465305324186, 0.19047889724696612, 0.24151360200466224, 0.1743548823271294, 0.3057819701703516]
printing an ep nov before normalisation:  30.25181531906128
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04378618294556642, 0.044084465305324186, 0.19047889724696612, 0.24151360200466224, 0.1743548823271294, 0.3057819701703516]
printing an ep nov before normalisation:  26.163142972516265
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.784]
 [0.86 ]
 [0.802]
 [0.805]
 [0.801]
 [0.822]
 [0.814]] [[24.079]
 [28.019]
 [23.004]
 [24.254]
 [24.924]
 [25.808]
 [25.595]] [[0.784]
 [0.86 ]
 [0.802]
 [0.805]
 [0.801]
 [0.822]
 [0.814]]
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04378618294556642, 0.044084465305324186, 0.19047889724696612, 0.24151360200466224, 0.1743548823271294, 0.3057819701703516]
actions average: 
K:  3  action  0 :  tensor([0.6304, 0.0465, 0.0509, 0.0668, 0.0569, 0.0658, 0.0826],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0198, 0.8378, 0.0131, 0.0270, 0.0161, 0.0162, 0.0700],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1307, 0.0395, 0.0957, 0.1250, 0.1401, 0.3187, 0.1504],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1387, 0.1905, 0.0692, 0.1253, 0.1134, 0.1225, 0.2404],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1759, 0.0335, 0.0819, 0.1383, 0.2632, 0.1425, 0.1648],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0784, 0.0258, 0.0953, 0.0980, 0.1358, 0.4105, 0.1562],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1296, 0.0764, 0.1112, 0.1356, 0.1432, 0.1857, 0.2183],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.934]] [[34.797]
 [34.797]
 [34.797]
 [34.797]
 [34.797]
 [34.797]
 [34.797]] [[0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.934]]
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.04378618294556642, 0.044084465305324186, 0.19047889724696612, 0.24151360200466224, 0.1743548823271294, 0.3057819701703516]
Printing some Q and Qe and total Qs values:  [[0.938]
 [0.974]
 [0.938]
 [0.881]
 [0.938]
 [0.874]
 [0.938]] [[33.866]
 [43.259]
 [33.866]
 [39.947]
 [33.866]
 [36.915]
 [33.866]] [[0.938]
 [0.974]
 [0.938]
 [0.881]
 [0.938]
 [0.874]
 [0.938]]
printing an ep nov before normalisation:  36.50074243545532
printing an ep nov before normalisation:  17.535260086941825
maxi score, test score, baseline:  -0.98222 -0.945 -0.945
probs:  [0.043729341912629616, 0.04402805263206242, 0.19063271983986255, 0.2417407149173866, 0.17448554941798075, 0.30538362128007795]
maxi score, test score, baseline:  -0.9822200000000001 -0.945 -0.945
probs:  [0.043729341912629616, 0.04402805263206242, 0.19063271983986255, 0.2417407149173866, 0.17448554941798075, 0.30538362128007795]
maxi score, test score, baseline:  -0.9822200000000001 -0.945 -0.945
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]]
printing an ep nov before normalisation:  44.92169877763946
maxi score, test score, baseline:  -0.9822200000000001 -0.945 -0.945
probs:  [0.04376866498769174, 0.044067645139939805, 0.18990328292179057, 0.24195864143119827, 0.17464281273023188, 0.30565895278914773]
maxi score, test score, baseline:  -0.9822200000000001 -0.945 -0.945
probs:  [0.04376866498769174, 0.044067645139939805, 0.18990328292179057, 0.24195864143119827, 0.17464281273023188, 0.30565895278914773]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.9822200000000001 -0.945 -0.945
probs:  [0.04376866498769174, 0.044067645139939805, 0.18990328292179057, 0.24195864143119827, 0.17464281273023188, 0.30565895278914773]
siam score:  -0.73466784
line 256 mcts: sample exp_bonus 50.56736469268799
maxi score, test score, baseline:  -0.9822200000000001 -0.945 -0.945
probs:  [0.0436978682662494, 0.0441441324567034, 0.1900597536742982, 0.24214366190231923, 0.17479091389039872, 0.30516366981003107]
printing an ep nov before normalisation:  37.342921323817414
actor:  1 policy actor:  1  step number:  82 total reward:  0.12666666666666626  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.383]
 [0.454]
 [0.128]
 [0.421]
 [0.433]
 [0.13 ]
 [0.475]] [[23.107]
 [27.494]
 [24.351]
 [21.469]
 [25.886]
 [20.605]
 [38.483]] [[0.749]
 [0.955]
 [0.532]
 [0.738]
 [0.885]
 [0.42 ]
 [1.311]]
printing an ep nov before normalisation:  50.426750992906264
printing an ep nov before normalisation:  28.589841328385287
maxi score, test score, baseline:  -0.9822200000000001 -0.945 -0.945
probs:  [0.043181560523789145, 0.043622535799914805, 0.18780883336507506, 0.2517292823351357, 0.17210919621932408, 0.3015485917567612]
maxi score, test score, baseline:  -0.9822200000000001 -0.945 -0.945
probs:  [0.043181560523789145, 0.043622535799914805, 0.18780883336507506, 0.2517292823351357, 0.17210919621932408, 0.3015485917567612]
printing an ep nov before normalisation:  20.52879991837962
actor:  1 policy actor:  1  step number:  78 total reward:  0.15333333333333288  reward:  1.0 rdn_beta:  1.667
from probs:  [0.04216193628534107, 0.04259246681156126, 0.18336363001568765, 0.2457700868426896, 0.19170248242328525, 0.29440939762143525]
maxi score, test score, baseline:  -0.9822200000000001 -0.945 -0.945
probs:  [0.04216193628534107, 0.04259246681156126, 0.18336363001568765, 0.24577008684268956, 0.19170248242328525, 0.29440939762143525]
maxi score, test score, baseline:  -0.9822200000000001 -0.945 -0.945
probs:  [0.04216193628534107, 0.04259246681156126, 0.18336363001568765, 0.24577008684268956, 0.19170248242328525, 0.29440939762143525]
printing an ep nov before normalisation:  42.96242249820981
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.74182155825278
maxi score, test score, baseline:  -0.9822200000000001 -0.945 -0.945
probs:  [0.04219818308407434, 0.042629084912518794, 0.18265991321702477, 0.24598193131586113, 0.19186769739968806, 0.29466319007083286]
siam score:  -0.7488878
maxi score, test score, baseline:  -0.9822200000000001 -0.945 -0.945
printing an ep nov before normalisation:  50.322687270250384
Printing some Q and Qe and total Qs values:  [[0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.333]
 [0.32 ]] [[24.075]
 [24.075]
 [24.075]
 [24.075]
 [24.075]
 [25.165]
 [25.184]] [[1.673]
 [1.673]
 [1.673]
 [1.673]
 [1.673]
 [1.73 ]
 [1.718]]
maxi score, test score, baseline:  -0.9822200000000001 -0.945 -0.945
probs:  [0.04222684329672335, 0.042658038712493476, 0.18278427454066207, 0.24614943597452882, 0.19131754463302103, 0.2948638628425712]
printing an ep nov before normalisation:  51.04627009046082
maxi score, test score, baseline:  -0.9822200000000001 -0.945 -0.945
probs:  [0.04222684329672335, 0.042658038712493476, 0.18278427454066207, 0.24614943597452882, 0.19131754463302103, 0.2948638628425712]
maxi score, test score, baseline:  -0.9822200000000001 -0.945 -0.945
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.502]
 [0.471]
 [0.476]
 [0.491]
 [0.471]
 [0.458]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.471]
 [0.502]
 [0.471]
 [0.476]
 [0.491]
 [0.471]
 [0.458]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9822200000000001 -0.945 -0.945
probs:  [0.04219863935210359, 0.042630721388543374, 0.18304508369603445, 0.24654053608339566, 0.1909189802079804, 0.2946660392719426]
maxi score, test score, baseline:  -0.9822200000000001 -0.945 -0.945
maxi score, test score, baseline:  -0.9822200000000001 -0.945 -0.945
probs:  [0.04219863935210359, 0.042630721388543374, 0.18304508369603445, 0.24654053608339566, 0.1909189802079804, 0.2946660392719426]
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]] [[37.602]
 [37.602]
 [37.602]
 [37.602]
 [37.602]
 [37.602]
 [37.602]] [[1.7]
 [1.7]
 [1.7]
 [1.7]
 [1.7]
 [1.7]
 [1.7]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.26383347043
maxi score, test score, baseline:  -0.9822200000000001 -0.945 -0.945
probs:  [0.042234674906434184, 0.042667127088196144, 0.1823458056679685, 0.24675162218558763, 0.19108241781524968, 0.2949183523365638]
actor:  1 policy actor:  1  step number:  65 total reward:  0.21333333333333304  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9822200000000001 -0.945 -0.945
probs:  [0.040788169160700403, 0.04120576331279605, 0.21041557465731509, 0.23827839996555147, 0.18452185721403985, 0.28479023568959705]
printing an ep nov before normalisation:  32.86880922295729
maxi score, test score, baseline:  -0.9822200000000001 -0.945 -0.945
probs:  [0.04073163778925912, 0.041149786380306266, 0.21058425726546665, 0.23848407599320384, 0.18465616078341057, 0.28439408178835346]
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]] [[76.254]
 [76.254]
 [76.254]
 [76.254]
 [76.254]
 [76.254]
 [76.254]] [[1.812]
 [1.812]
 [1.812]
 [1.812]
 [1.812]
 [1.812]
 [1.812]]
siam score:  -0.7454347
actor:  1 policy actor:  1  step number:  75 total reward:  0.13333333333333208  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]] [[14.893]
 [15.376]
 [15.376]
 [15.376]
 [15.376]
 [15.376]
 [15.376]] [[1.033]
 [0.982]
 [0.982]
 [0.982]
 [0.982]
 [0.982]
 [0.982]]
printing an ep nov before normalisation:  38.33018042950431
maxi score, test score, baseline:  -0.9822200000000001 -0.945 -0.945
probs:  [0.039882170232683006, 0.04029156962264452, 0.2052120044744525, 0.23349690264480194, 0.20267105017195977, 0.2784463028534584]
actor:  0 policy actor:  0  step number:  59 total reward:  0.13333333333333275  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9799533333333335 -0.945 -0.945
probs:  [0.039882170232683006, 0.04029156962264452, 0.2052120044744525, 0.23349690264480194, 0.20267105017195977, 0.2784463028534584]
printing an ep nov before normalisation:  28.90108824838243
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]] [[24.59 ]
 [19.846]
 [19.846]
 [19.846]
 [19.846]
 [19.846]
 [19.846]] [[0.428]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]]
printing an ep nov before normalisation:  17.902958774841544
printing an ep nov before normalisation:  37.39907076053177
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.328]
 [0.328]
 [0.33 ]
 [0.328]
 [0.329]
 [0.328]] [[15.085]
 [14.622]
 [14.622]
 [15.806]
 [14.622]
 [15.878]
 [14.622]] [[0.332]
 [0.328]
 [0.328]
 [0.33 ]
 [0.328]
 [0.329]
 [0.328]]
printing an ep nov before normalisation:  44.62414898765269
Printing some Q and Qe and total Qs values:  [[0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]] [[38.223]
 [38.223]
 [38.223]
 [38.223]
 [38.223]
 [38.223]
 [38.223]] [[0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]]
siam score:  -0.74498594
maxi score, test score, baseline:  -0.9799533333333335 -0.945 -0.945
probs:  [0.039825848897571035, 0.040235778543812266, 0.2053698192249792, 0.23369135214673128, 0.20282557386488265, 0.27805162732202365]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.039825848897571035, 0.040235778543812266, 0.2053698192249792, 0.23369135214673128, 0.20282557386488265, 0.27805162732202365]
siam score:  -0.7422321
printing an ep nov before normalisation:  14.024809773030178
printing an ep nov before normalisation:  0.00014970866772046065
printing an ep nov before normalisation:  51.6638447814497
printing an ep nov before normalisation:  21.104956821267276
printing an ep nov before normalisation:  45.31552854958985
printing an ep nov before normalisation:  45.75643901099233
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.03980783792250689, 0.04021869196005578, 0.2047626426414764, 0.23411050786039722, 0.2031751272518819, 0.27792519236368196]
siam score:  -0.7437044
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.03980783792250689, 0.04021869196005578, 0.2047626426414764, 0.23411050786039722, 0.2031751272518819, 0.27792519236368196]
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.358]
 [0.314]
 [0.314]
 [0.269]
 [0.314]
 [0.314]] [[26.241]
 [27.754]
 [23.92 ]
 [23.92 ]
 [25.299]
 [23.92 ]
 [23.92 ]] [[1.705]
 [1.881]
 [1.458]
 [1.458]
 [1.549]
 [1.458]
 [1.458]]
printing an ep nov before normalisation:  13.820799975829345
Printing some Q and Qe and total Qs values:  [[0.365]
 [0.436]
 [0.365]
 [0.365]
 [0.365]
 [0.397]
 [0.39 ]] [[37.425]
 [37.423]
 [37.425]
 [37.425]
 [37.425]
 [41.994]
 [40.814]] [[1.688]
 [1.759]
 [1.688]
 [1.688]
 [1.688]
 [1.984]
 [1.909]]
printing an ep nov before normalisation:  37.517571449279785
printing an ep nov before normalisation:  12.132982389362407
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.03980783792250689, 0.04021869196005578, 0.2047626426414764, 0.23411050786039722, 0.2031751272518819, 0.27792519236368196]
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.03975177335009661, 0.04016315619807614, 0.20491889147807946, 0.23430453034853177, 0.20332933279665846, 0.27753231582855753]
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.03975177335009661, 0.04016315619807614, 0.20491889147807946, 0.23430453034853177, 0.20332933279665846, 0.27753231582855753]
printing an ep nov before normalisation:  18.19772481918335
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.03975177335009661, 0.04016315619807614, 0.20491889147807946, 0.23430453034853177, 0.20332933279665846, 0.27753231582855753]
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.03975177335009661, 0.04016315619807614, 0.20491889147807946, 0.23430453034853177, 0.20332933279665846, 0.27753231582855753]
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.03975177335009661, 0.04016315619807614, 0.20491889147807946, 0.23430453034853177, 0.20332933279665846, 0.27753231582855753]
printing an ep nov before normalisation:  11.73383931089667
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.03975177335009661, 0.04016315619807614, 0.20491889147807946, 0.23430453034853177, 0.20332933279665846, 0.27753231582855753]
siam score:  -0.7399873
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.03975177335009661, 0.04016315619807614, 0.20491889147807946, 0.23430453034853177, 0.20332933279665846, 0.27753231582855753]
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.44 ]
 [0.435]
 [0.443]
 [0.48 ]
 [0.462]
 [0.45 ]] [[23.227]
 [23.89 ]
 [24.591]
 [24.852]
 [24.942]
 [28.346]
 [23.035]] [[1.159]
 [1.202]
 [1.24 ]
 [1.264]
 [1.308]
 [1.502]
 [1.158]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.03975177335009661, 0.04016315619807614, 0.20491889147807946, 0.23430453034853177, 0.20332933279665846, 0.27753231582855753]
line 256 mcts: sample exp_bonus 0.0
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 44.43634244513495
actor:  1 policy actor:  1  step number:  86 total reward:  0.16666666666666663  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  54.48976109737074
siam score:  -0.73484796
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
actor:  1 policy actor:  1  step number:  72 total reward:  0.09999999999999942  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  43.52511405944824
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.03829214399944567, 0.03868837065087524, 0.22214581261146465, 0.2377181469472326, 0.19584316720534922, 0.26731235858563246]
printing an ep nov before normalisation:  16.84926714366739
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.541]
 [0.504]
 [0.488]
 [0.019]
 [0.572]
 [0.518]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.481]
 [0.541]
 [0.504]
 [0.488]
 [0.019]
 [0.572]
 [0.518]]
printing an ep nov before normalisation:  43.201688954970415
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.038275950449880435, 0.038673077559895866, 0.22151212308154503, 0.23815516586066218, 0.1961850216847223, 0.2671986613632942]
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.038275950449880435, 0.038673077559895866, 0.22151212308154503, 0.23815516586066218, 0.1961850216847223, 0.2671986613632942]
printing an ep nov before normalisation:  44.900527235222775
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.038275950449880435, 0.038673077559895866, 0.22151212308154503, 0.23815516586066218, 0.1961850216847223, 0.2671986613632942]
Printing some Q and Qe and total Qs values:  [[0.642]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]] [[16.398]
 [15.803]
 [15.803]
 [15.803]
 [15.803]
 [15.803]
 [15.803]] [[0.642]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]]
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.0383025493619996, 0.03869995340947716, 0.22166650214097294, 0.23832115098251455, 0.19562494407453077, 0.2673849000305049]
siam score:  -0.73334914
siam score:  -0.73253417
printing an ep nov before normalisation:  13.588683855582318
using another actor
from probs:  [0.0383025493619996, 0.03869995340947716, 0.22166650214097294, 0.23832115098251455, 0.19562494407453077, 0.2673849000305049]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.605]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]] [[20.822]
 [17.497]
 [17.497]
 [17.497]
 [17.497]
 [17.497]
 [17.497]] [[0.605]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.03834177440118438, 0.03873958684441719, 0.22086732545289933, 0.23856592689213005, 0.19582584285260488, 0.267659543556764]
printing an ep nov before normalisation:  27.177345777544506
printing an ep nov before normalisation:  37.640446930565254
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.03838069250008529, 0.038778910143331236, 0.22007440241108375, 0.23880878740309341, 0.1960251695754496, 0.26793203796695675]
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.03838069250008529, 0.038778910143331236, 0.22007440241108375, 0.23880878740309341, 0.1960251695754496, 0.26793203796695675]
printing an ep nov before normalisation:  23.31215082479082
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.03832512028250259, 0.038723829853387806, 0.22024328071522736, 0.23900080873452034, 0.19616433929551666, 0.26754262111884514]
printing an ep nov before normalisation:  33.63069161092925
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]] [[18.335]
 [18.666]
 [18.666]
 [18.666]
 [18.666]
 [18.666]
 [18.666]] [[0.5  ]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]]
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]] [[18.225]
 [17.946]
 [17.946]
 [17.946]
 [17.946]
 [17.946]
 [17.946]] [[0.43 ]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]]
printing an ep nov before normalisation:  30.016143031256288
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.038269681363996166, 0.03866888168255156, 0.22041175393706694, 0.2391923694712974, 0.19630317519418994, 0.26715413835089796]
printing an ep nov before normalisation:  25.796288941843383
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.038269681363996166, 0.03866888168255156, 0.22041175393706694, 0.2391923694712974, 0.19630317519418994, 0.26715413835089796]
printing an ep nov before normalisation:  15.587335927530269
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.038269681363996166, 0.03866888168255156, 0.22041175393706694, 0.2391923694712974, 0.19630317519418994, 0.26715413835089796]
printing an ep nov before normalisation:  32.463462352752686
printing an ep nov before normalisation:  83.24399796474756
printing an ep nov before normalisation:  37.81176937168549
printing an ep nov before normalisation:  75.16184469020816
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.03821437526553099, 0.03861406515602788, 0.22057982353234096, 0.2393834712686571, 0.19644167847111876, 0.2667665863063243]
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.03825810920302153, 0.038523401298446405, 0.21982032757839828, 0.23965823596580257, 0.19666712706947928, 0.26707279888485197]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]] [[15.328]
 [14.668]
 [14.668]
 [14.668]
 [14.668]
 [14.668]
 [14.668]] [[0.593]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]]
actions average: 
K:  3  action  0 :  tensor([0.7268, 0.0009, 0.0413, 0.0428, 0.0598, 0.0492, 0.0791],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0302, 0.7387, 0.0246, 0.0720, 0.0380, 0.0343, 0.0623],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1184, 0.0406, 0.1387, 0.1392, 0.1206, 0.2738, 0.1687],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2162, 0.1148, 0.1040, 0.1371, 0.1664, 0.1323, 0.1292],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1220, 0.0185, 0.0835, 0.1703, 0.3243, 0.1152, 0.1663],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1262, 0.0175, 0.1050, 0.1064, 0.1600, 0.3276, 0.1574],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1147, 0.1580, 0.0780, 0.1321, 0.1802, 0.1383, 0.1987],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.03820292550862231, 0.03846854215005507, 0.21998725865549645, 0.23984943581714177, 0.1968057335981858, 0.2666861042704985]
printing an ep nov before normalisation:  75.09995670600293
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
printing an ep nov before normalisation:  47.946180243246
printing an ep nov before normalisation:  37.99653199271106
printing an ep nov before normalisation:  36.225915747441206
printing an ep nov before normalisation:  35.2282134239968
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.03820292550862231, 0.03846854215005507, 0.21998725865549645, 0.23984943581714177, 0.1968057335981858, 0.2666861042704985]
printing an ep nov before normalisation:  27.205615043640137
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.03820292550862231, 0.03846854215005507, 0.21998725865549645, 0.23984943581714177, 0.1968057335981858, 0.2666861042704985]
Printing some Q and Qe and total Qs values:  [[0.34 ]
 [0.333]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]] [[29.041]
 [36.236]
 [29.041]
 [29.041]
 [29.041]
 [29.041]
 [29.041]] [[0.741]
 [0.974]
 [0.741]
 [0.741]
 [0.741]
 [0.741]
 [0.741]]
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.03824121703019918, 0.038507100835956556, 0.21920337098591147, 0.240090548793906, 0.19700355206760972, 0.2669542102864171]
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.03827921277623679, 0.03854536168265902, 0.2184255383067949, 0.24032979933956844, 0.1971998425260783, 0.2672202453686624]
printing an ep nov before normalisation:  38.41870241027625
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.598]
 [0.552]
 [0.552]
 [0.56 ]
 [0.548]
 [0.552]] [[47.39 ]
 [48.883]
 [47.39 ]
 [47.39 ]
 [49.614]
 [53.544]
 [47.39 ]] [[1.908]
 [2.019]
 [1.908]
 [1.908]
 [2.012]
 [2.169]
 [1.908]]
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.03827921277623679, 0.03854536168265902, 0.2184255383067949, 0.24032979933956844, 0.1971998425260783, 0.2672202453686624]
Starting evaluation
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.6  ]
 [0.6  ]
 [0.6  ]
 [0.701]
 [0.686]
 [0.684]
 [0.696]] [[36.626]
 [36.626]
 [36.626]
 [38.202]
 [41.439]
 [42.529]
 [44.947]] [[1.016]
 [1.016]
 [1.016]
 [1.152]
 [1.211]
 [1.234]
 [1.301]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  34.217122961794885
Printing some Q and Qe and total Qs values:  [[0.127]
 [0.1  ]
 [0.088]
 [0.113]
 [0.134]
 [0.057]
 [0.127]] [[42.124]
 [37.185]
 [35.837]
 [35.942]
 [44.804]
 [36.85 ]
 [42.124]] [[0.127]
 [0.1  ]
 [0.088]
 [0.113]
 [0.134]
 [0.057]
 [0.127]]
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.03830589773323012, 0.03857223282364867, 0.21857824430689946, 0.24049782842670128, 0.19663871108981001, 0.26740708561971066]
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.03830589773323012, 0.03857223282364867, 0.21857824430689946, 0.24049782842670128, 0.19663871108981001, 0.26740708561971066]
printing an ep nov before normalisation:  46.44140167164831
printing an ep nov before normalisation:  37.058201810834866
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.03830589773323012, 0.03857223282364867, 0.21857824430689946, 0.24049782842670128, 0.19663871108981001, 0.26740708561971066]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
printing an ep nov before normalisation:  24.759577694420614
printing an ep nov before normalisation:  45.944880778925395
printing an ep nov before normalisation:  32.77123689651489
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]] [[28.899]
 [28.899]
 [28.899]
 [28.899]
 [28.899]
 [28.899]
 [28.899]] [[2.063]
 [2.063]
 [2.063]
 [2.063]
 [2.063]
 [2.063]
 [2.063]]
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.038332416698471414, 0.03859893681474286, 0.21873000041090676, 0.2406648123016356, 0.19608107012910192, 0.2675927636451413]
printing an ep nov before normalisation:  17.35220432281494
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]] [[32.1  ]
 [32.1  ]
 [32.1  ]
 [32.1  ]
 [32.1  ]
 [32.1  ]
 [31.952]] [[0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]]
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.038277335638025885, 0.03854418173319093, 0.21889556237572402, 0.24085720258174706, 0.19621893034540622, 0.2672067873259058]
Printing some Q and Qe and total Qs values:  [[0.697]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.628]] [[21.72 ]
 [28.054]
 [28.054]
 [28.054]
 [28.054]
 [28.054]
 [24.785]] [[0.697]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.628]]
printing an ep nov before normalisation:  19.73003657703023
printing an ep nov before normalisation:  20.54732737214877
printing an ep nov before normalisation:  39.176066392404046
Printing some Q and Qe and total Qs values:  [[0.554]
 [0.422]
 [0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.491]] [[10.802]
 [14.49 ]
 [10.445]
 [10.445]
 [10.445]
 [10.445]
 [14.724]] [[0.554]
 [0.422]
 [0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.491]]
printing an ep nov before normalisation:  35.302262699246135
using explorer policy with actor:  0
printing an ep nov before normalisation:  19.21509531418183
printing an ep nov before normalisation:  19.48832661724077
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]] [[22.538]
 [22.538]
 [22.538]
 [22.538]
 [22.538]
 [22.538]
 [22.538]] [[0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]]
printing an ep nov before normalisation:  11.456109285354614
using explorer policy with actor:  0
printing an ep nov before normalisation:  17.561814785003662
siam score:  -0.73607606
actor:  1 policy actor:  1  step number:  67 total reward:  0.19999999999999973  reward:  1.0 rdn_beta:  1.0
from probs:  [0.03830368436358643, 0.03857071478746524, 0.21904667637242392, 0.24102348697428383, 0.19566416433677664, 0.26739127316546374]
printing an ep nov before normalisation:  19.655797496610532
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.03717757932280658, 0.03743673181371872, 0.24206671314290046, 0.23391674125299772, 0.1898956095899907, 0.25950662487758575]
printing an ep nov before normalisation:  22.942346401411825
printing an ep nov before normalisation:  22.604185218733477
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.03717757932280658, 0.03743673181371872, 0.24206671314290046, 0.23391674125299772, 0.1898956095899907, 0.25950662487758575]
printing an ep nov before normalisation:  19.667528931022062
printing an ep nov before normalisation:  46.93715743478056
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.208]
 [0.44 ]
 [0.523]
 [0.415]
 [0.411]
 [0.423]] [[14.514]
 [22.398]
 [16.456]
 [15.207]
 [16.815]
 [17.524]
 [17.917]] [[0.457]
 [0.208]
 [0.44 ]
 [0.523]
 [0.415]
 [0.411]
 [0.423]]
printing an ep nov before normalisation:  22.91275259000283
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.03717757932280658, 0.03743673181371872, 0.24206671314290046, 0.23391674125299772, 0.1898956095899907, 0.25950662487758575]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9799533333333333 -0.945 -0.945
probs:  [0.03717757932280658, 0.03743673181371872, 0.24206671314290046, 0.23391674125299772, 0.1898956095899907, 0.25950662487758575]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.409]
 [0.408]
 [0.406]] [[18.33 ]
 [20.579]
 [20.579]
 [20.413]
 [20.328]
 [20.045]
 [19.84 ]] [[0.42 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.409]
 [0.408]
 [0.406]]
actor:  0 policy actor:  1  step number:  53 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  2
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  55 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  23.096129894256592
printing an ep nov before normalisation:  13.66269574986908
printing an ep nov before normalisation:  11.981931431367627
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.423]
 [0.423]
 [0.436]
 [0.423]
 [0.423]
 [0.428]] [[13.927]
 [13.927]
 [13.927]
 [15.372]
 [13.927]
 [13.927]
 [15.99 ]] [[0.423]
 [0.423]
 [0.423]
 [0.436]
 [0.423]
 [0.423]
 [0.428]]
siam score:  -0.73880464
maxi score, test score, baseline:  -0.9740333333333333 -0.945 -0.945
probs:  [0.03717757932280658, 0.03743673181371872, 0.24206671314290046, 0.23391674125299772, 0.1898956095899907, 0.25950662487758575]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]] [[20.495]
 [20.495]
 [20.495]
 [20.495]
 [20.495]
 [20.495]
 [20.495]] [[0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]]
printing an ep nov before normalisation:  21.52180185401992
maxi score, test score, baseline:  -0.9740333333333333 -0.945 -0.945
probs:  [0.03717757932280658, 0.03743673181371872, 0.24206671314290046, 0.23391674125299772, 0.1898956095899907, 0.25950662487758575]
printing an ep nov before normalisation:  14.668639278506035
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.203]
 [0.443]
 [0.447]
 [0.442]
 [0.405]
 [0.432]] [[14.29 ]
 [34.811]
 [14.53 ]
 [14.594]
 [14.935]
 [17.707]
 [17.029]] [[0.453]
 [0.203]
 [0.443]
 [0.447]
 [0.442]
 [0.405]
 [0.432]]
Printing some Q and Qe and total Qs values:  [[0.64 ]
 [0.487]
 [0.578]
 [0.587]
 [0.585]
 [0.582]
 [0.663]] [[13.772]
 [22.462]
 [15.119]
 [14.964]
 [15.344]
 [15.648]
 [16.594]] [[0.64 ]
 [0.487]
 [0.578]
 [0.587]
 [0.585]
 [0.582]
 [0.663]]
Printing some Q and Qe and total Qs values:  [[0.431]
 [0.244]
 [0.443]
 [0.447]
 [0.442]
 [0.421]
 [0.431]] [[18.332]
 [31.053]
 [17.177]
 [17.213]
 [17.46 ]
 [18.992]
 [19.537]] [[0.431]
 [0.244]
 [0.443]
 [0.447]
 [0.442]
 [0.421]
 [0.431]]
maxi score, test score, baseline:  -0.9740333333333333 -0.945 -0.945
probs:  [0.03717757932280658, 0.03743673181371872, 0.24206671314290046, 0.23391674125299772, 0.1898956095899907, 0.25950662487758575]
printing an ep nov before normalisation:  13.629523515585955
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]] [[15.597]
 [15.597]
 [15.597]
 [15.597]
 [15.597]
 [15.597]
 [15.597]] [[0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]]
printing an ep nov before normalisation:  16.43700708803369
printing an ep nov before normalisation:  30.799596309661865
Printing some Q and Qe and total Qs values:  [[0.865]
 [0.836]
 [0.879]
 [0.896]
 [0.732]
 [0.775]
 [0.787]] [[33.161]
 [31.521]
 [35.503]
 [34.716]
 [29.925]
 [31.564]
 [32.823]] [[2.385]
 [2.189]
 [2.637]
 [2.574]
 [1.921]
 [2.132]
 [2.272]]
printing an ep nov before normalisation:  15.173499075253005
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]] [[14.431]
 [14.431]
 [14.431]
 [14.431]
 [14.431]
 [14.431]
 [14.431]] [[0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]]
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.439]
 [0.439]
 [0.439]
 [0.449]
 [0.45 ]
 [0.455]] [[11.475]
 [13.643]
 [13.643]
 [13.643]
 [13.028]
 [13.23 ]
 [13.209]] [[0.469]
 [0.439]
 [0.439]
 [0.439]
 [0.449]
 [0.45 ]
 [0.455]]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  0
printing an ep nov before normalisation:  13.67657021041957
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9740333333333333 -0.945 -0.945
printing an ep nov before normalisation:  26.91806235965636
Printing some Q and Qe and total Qs values:  [[0.416]
 [0.405]
 [0.391]
 [0.402]
 [0.401]
 [0.395]
 [0.398]] [[14.239]
 [16.369]
 [15.967]
 [15.357]
 [15.661]
 [15.975]
 [16.573]] [[0.416]
 [0.405]
 [0.391]
 [0.402]
 [0.401]
 [0.395]
 [0.398]]
printing an ep nov before normalisation:  16.388277872606476
printing an ep nov before normalisation:  13.324773985449925
printing an ep nov before normalisation:  18.993663953425898
printing an ep nov before normalisation:  16.214053630828857
printing an ep nov before normalisation:  19.477534729154765
printing an ep nov before normalisation:  24.658846564967057
printing an ep nov before normalisation:  18.21836353654599
actions average: 
K:  4  action  0 :  tensor([0.3616, 0.0442, 0.0658, 0.1057, 0.1580, 0.0858, 0.1790],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0275, 0.8086, 0.0225, 0.0292, 0.0246, 0.0182, 0.0695],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1088, 0.1288, 0.2378, 0.1108, 0.2018, 0.1024, 0.1095],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1005, 0.1339, 0.1280, 0.2099, 0.1319, 0.1192, 0.1766],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1009, 0.0164, 0.0808, 0.1059, 0.4798, 0.0850, 0.1312],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1353, 0.0449, 0.1020, 0.1745, 0.1164, 0.2540, 0.1729],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1516, 0.0577, 0.1185, 0.1274, 0.1750, 0.1456, 0.2242],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  14.808392664756973
printing an ep nov before normalisation:  21.917188704426927
printing an ep nov before normalisation:  27.23250016518948
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]] [[10.494]
 [10.494]
 [10.494]
 [10.494]
 [10.494]
 [10.494]
 [10.494]] [[0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]]
printing an ep nov before normalisation:  19.286632271435856
actor:  1 policy actor:  1  step number:  74 total reward:  0.2599999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]] [[10.608]
 [15.682]
 [15.682]
 [15.682]
 [15.682]
 [15.682]
 [15.682]] [[0.485]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]]
maxi score, test score, baseline:  -0.9740333333333333 -0.945 -0.945
probs:  [0.0386263616439901, 0.038877385469079775, 0.23708887269223014, 0.2291945356130227, 0.18655418584856506, 0.26965865873311223]
printing an ep nov before normalisation:  18.139168858654386
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9740333333333333 -0.945 -0.945
probs:  [0.0386263616439901, 0.038877385469079775, 0.23708887269223014, 0.2291945356130227, 0.18655418584856506, 0.26965865873311223]
printing an ep nov before normalisation:  17.968015050782384
printing an ep nov before normalisation:  51.46272495115517
printing an ep nov before normalisation:  15.599180557298666
Printing some Q and Qe and total Qs values:  [[0.723]
 [0.624]
 [0.668]
 [0.668]
 [0.668]
 [0.636]
 [0.638]] [[10.588]
 [16.792]
 [ 9.879]
 [ 9.879]
 [ 9.879]
 [15.664]
 [15.865]] [[0.723]
 [0.624]
 [0.668]
 [0.668]
 [0.668]
 [0.636]
 [0.638]]
maxi score, test score, baseline:  -0.9740333333333333 -0.945 -0.945
probs:  [0.0386263616439901, 0.038877385469079775, 0.23708887269223014, 0.2291945356130227, 0.18655418584856506, 0.26965865873311223]
printing an ep nov before normalisation:  15.152699869434992
printing an ep nov before normalisation:  19.68841686638102
printing an ep nov before normalisation:  17.649662401512103
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.479]
 [0.528]
 [0.528]
 [0.493]
 [0.528]
 [0.49 ]] [[10.004]
 [17.565]
 [10.191]
 [10.191]
 [14.56 ]
 [10.191]
 [16.005]] [[0.569]
 [0.479]
 [0.528]
 [0.528]
 [0.493]
 [0.528]
 [0.49 ]]
printing an ep nov before normalisation:  26.794572153941388
printing an ep nov before normalisation:  17.362605995303234
printing an ep nov before normalisation:  16.005215102268178
printing an ep nov before normalisation:  27.48702714335244
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]] [[11.447]
 [11.447]
 [11.447]
 [11.447]
 [11.447]
 [11.447]
 [11.447]] [[0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.545]
 [0.507]
 [0.458]
 [0.461]
 [0.456]
 [0.457]
 [0.454]] [[11.523]
 [16.243]
 [14.845]
 [17.776]
 [16.915]
 [18.198]
 [19.44 ]] [[0.545]
 [0.507]
 [0.458]
 [0.461]
 [0.456]
 [0.457]
 [0.454]]
maxi score, test score, baseline:  -0.9740333333333333 -0.945 -0.945
probs:  [0.03857182335771505, 0.038823153180122034, 0.23727625963336046, 0.2293722993800683, 0.18667997115247967, 0.26927649329625464]
Printing some Q and Qe and total Qs values:  [[ 0.204]
 [-0.047]
 [ 0.166]
 [ 0.169]
 [ 0.166]
 [ 0.163]
 [ 0.162]] [[14.837]
 [20.325]
 [12.587]
 [12.671]
 [12.765]
 [12.995]
 [13.587]] [[0.77 ]
 [0.729]
 [0.647]
 [0.652]
 [0.653]
 [0.659]
 [0.681]]
printing an ep nov before normalisation:  16.914881467819214
printing an ep nov before normalisation:  18.40479743727831
printing an ep nov before normalisation:  25.011343619109226
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.256]
 [0.418]
 [0.424]
 [0.415]
 [0.414]
 [0.413]] [[12.521]
 [22.554]
 [14.845]
 [17.638]
 [18.509]
 [19.614]
 [20.66 ]] [[0.507]
 [0.256]
 [0.418]
 [0.424]
 [0.415]
 [0.414]
 [0.413]]
printing an ep nov before normalisation:  23.90844974935376
printing an ep nov before normalisation:  25.050518217724758
printing an ep nov before normalisation:  42.401278330349754
actor:  0 policy actor:  1  step number:  75 total reward:  0.18666666666666631  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.97166 -0.852 -0.852
probs:  [0.03851613691285055, 0.03875898631770494, 0.23772024363444008, 0.22930227083892582, 0.18681267972285284, 0.2688896825732259]
maxi score, test score, baseline:  -0.97166 -0.852 -0.852
probs:  [0.03851613691285055, 0.03875898631770494, 0.23772024363444008, 0.22930227083892582, 0.18681267972285284, 0.2688896825732259]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.97166 -0.852 -0.852
maxi score, test score, baseline:  -0.97166 -0.852 -0.852
probs:  [0.038540835270956954, 0.03878384094279427, 0.23787312441392744, 0.22944973488201248, 0.18628984445190758, 0.26906262003840115]
printing an ep nov before normalisation:  36.90268039703369
printing an ep nov before normalisation:  48.506669053166036
maxi score, test score, baseline:  -0.97166 -0.852 -0.852
probs:  [0.03855211700043386, 0.038795832427236325, 0.23739943578786504, 0.23001861075471552, 0.18609267382293376, 0.2691413302068155]
actions average: 
K:  4  action  0 :  tensor([0.7139, 0.0228, 0.0372, 0.0354, 0.1045, 0.0441, 0.0422],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0224, 0.7853, 0.0205, 0.0444, 0.0337, 0.0311, 0.0625],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0844, 0.0304, 0.1898, 0.1226, 0.2286, 0.1131, 0.2310],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1841, 0.0305, 0.0607, 0.2846, 0.1094, 0.0998, 0.2309],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1630, 0.0296, 0.0912, 0.0884, 0.3456, 0.1305, 0.1516],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0815, 0.0330, 0.1109, 0.1234, 0.1485, 0.3373, 0.1654],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1197, 0.0362, 0.0861, 0.1096, 0.2069, 0.1436, 0.2980],
       grad_fn=<DivBackward0>)
siam score:  -0.76182663
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.268]
 [0.268]
 [0.268]
 [0.327]
 [0.268]
 [0.268]] [[34.652]
 [34.652]
 [34.652]
 [34.652]
 [42.647]
 [34.652]
 [34.652]] [[0.97 ]
 [0.97 ]
 [0.97 ]
 [0.97 ]
 [1.312]
 [0.97 ]
 [0.97 ]]
maxi score, test score, baseline:  -0.97166 -0.852 -0.852
probs:  [0.03859279955269277, 0.03883677305476817, 0.23659260776435265, 0.2302620411111823, 0.186289590102258, 0.26942618841474614]
printing an ep nov before normalisation:  40.53893375981612
maxi score, test score, baseline:  -0.97166 -0.852 -0.852
maxi score, test score, baseline:  -0.97166 -0.852 -0.852
probs:  [0.03859279955269277, 0.03883677305476817, 0.23659260776435265, 0.2302620411111823, 0.186289590102258, 0.26942618841474614]
printing an ep nov before normalisation:  31.349334716796875
printing an ep nov before normalisation:  29.597718715667725
printing an ep nov before normalisation:  16.630375385284424
maxi score, test score, baseline:  -0.97166 -0.852 -0.852
probs:  [0.03859279955269277, 0.03883677305476817, 0.23659260776435265, 0.2302620411111823, 0.186289590102258, 0.26942618841474614]
actor:  0 policy actor:  0  step number:  80 total reward:  0.05999999999999894  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03859279955269277, 0.03883677305476817, 0.23659260776435265, 0.2302620411111823, 0.186289590102258, 0.26942618841474614]
line 256 mcts: sample exp_bonus 50.05279246557937
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03859279955269277, 0.03883677305476817, 0.23659260776435265, 0.2302620411111823, 0.186289590102258, 0.26942618841474614]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03859279955269277, 0.03883677305476817, 0.23659260776435265, 0.2302620411111823, 0.186289590102258, 0.26942618841474614]
printing an ep nov before normalisation:  25.055098538477832
printing an ep nov before normalisation:  35.53057797248006
siam score:  -0.75171995
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03861728986112444, 0.03886141872078392, 0.23674318050218715, 0.23040858266721753, 0.18577185931012505, 0.2695976689385618]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03861728986112444, 0.03886141872078392, 0.23674318050218715, 0.23040858266721753, 0.18577185931012505, 0.2695976689385618]
line 256 mcts: sample exp_bonus 11.6098305584727
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
printing an ep nov before normalisation:  43.85646343231201
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03860356960341778, 0.03884825094800197, 0.236126798678875, 0.23082890285075447, 0.18609116269139678, 0.2695013152275539]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.650343146236665
printing an ep nov before normalisation:  41.058082580566406
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03860356960341778, 0.03884825094800197, 0.236126798678875, 0.23082890285075447, 0.18609116269139678, 0.2695013152275539]
using another actor
printing an ep nov before normalisation:  35.22557953659204
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03860356960341778, 0.03884825094800197, 0.236126798678875, 0.23082890285075447, 0.18609116269139678, 0.2695013152275539]
siam score:  -0.7468149
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.038549545220830636, 0.038794521870328356, 0.2363111642731835, 0.23100687443626727, 0.18621514048138046, 0.26912275371800976]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.038549545220830636, 0.038794521870328356, 0.2363111642731835, 0.23100687443626727, 0.18621514048138046, 0.26912275371800976]
printing an ep nov before normalisation:  38.132353384240666
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.038549545220830636, 0.038794521870328356, 0.2363111642731835, 0.23100687443626727, 0.18621514048138046, 0.26912275371800976]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.038549545220830636, 0.038794521870328356, 0.2363111642731835, 0.23100687443626727, 0.18621514048138046, 0.26912275371800976]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03857392520838167, 0.03881905732638608, 0.2364610488900881, 0.2311533928138927, 0.18569911434453024, 0.2692934614167212]
printing an ep nov before normalisation:  13.149543415736087
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03856610850725507, 0.03893154400155099, 0.2364306825143483, 0.23112686274266306, 0.18570543803228495, 0.26923936420189765]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  102 total reward:  0.07333333333333136  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03791721574256848, 0.03827648130014932, 0.23244108552460807, 0.22722681434043077, 0.19944257082312486, 0.26469583226911847]
printing an ep nov before normalisation:  46.027905877771154
printing an ep nov before normalisation:  39.91562765347289
printing an ep nov before normalisation:  30.983221900991403
printing an ep nov before normalisation:  45.1750827185559
printing an ep nov before normalisation:  41.83362167289359
printing an ep nov before normalisation:  79.92631688912732
printing an ep nov before normalisation:  43.35847128307916
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03788915566906386, 0.038249090649130166, 0.23277548338686366, 0.22755149640805394, 0.19903569559943332, 0.26449907828745506]
printing an ep nov before normalisation:  59.81498983633495
printing an ep nov before normalisation:  28.333251926383145
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03788915566906386, 0.038249090649130166, 0.23277548338686366, 0.22755149640805394, 0.19903569559943332, 0.26449907828745506]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03792798498030248, 0.0382882901281217, 0.23198713212961547, 0.22778538006625262, 0.19924025276675825, 0.2647709599289495]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03792798498030248, 0.0382882901281217, 0.23198713212961547, 0.22778538006625262, 0.19924025276675825, 0.2647709599289495]
printing an ep nov before normalisation:  33.086078899369674
actions average: 
K:  3  action  0 :  tensor([0.6070, 0.0064, 0.0573, 0.0520, 0.0811, 0.0684, 0.1278],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0104, 0.8810, 0.0104, 0.0341, 0.0140, 0.0154, 0.0346],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0958, 0.0209, 0.3051, 0.1185, 0.1355, 0.1951, 0.1292],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1269, 0.1194, 0.1160, 0.1659, 0.1300, 0.1331, 0.2087],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0826, 0.0756, 0.0359, 0.0619, 0.5644, 0.0696, 0.1101],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0947, 0.0593, 0.1222, 0.1228, 0.1194, 0.3100, 0.1717],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0875, 0.0898, 0.1167, 0.0884, 0.1244, 0.1288, 0.3643],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03792798498030248, 0.0382882901281217, 0.23198713212961547, 0.22778538006625262, 0.19924025276675825, 0.2647709599289495]
actions average: 
K:  0  action  0 :  tensor([0.5027, 0.0058, 0.0702, 0.0694, 0.1440, 0.0833, 0.1246],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0082, 0.9205, 0.0123, 0.0082, 0.0054, 0.0093, 0.0361],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1072, 0.0072, 0.1951, 0.1224, 0.1648, 0.2230, 0.1803],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0887, 0.0139, 0.0889, 0.3445, 0.1208, 0.1714, 0.1718],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0863, 0.0057, 0.0709, 0.0787, 0.5038, 0.1298, 0.1249],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0830, 0.0038, 0.1186, 0.0898, 0.1209, 0.4543, 0.1298],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0960, 0.0704, 0.1393, 0.1434, 0.1370, 0.1094, 0.3045],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  32.91884302019042
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03792798498030248, 0.0382882901281217, 0.23198713212961547, 0.22778538006625262, 0.19924025276675825, 0.2647709599289495]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
Printing some Q and Qe and total Qs values:  [[0.589]
 [0.588]
 [0.589]
 [0.556]
 [0.589]
 [0.578]
 [0.589]] [[41.106]
 [43.395]
 [41.106]
 [45.663]
 [41.106]
 [42.107]
 [41.106]] [[1.949]
 [2.096]
 [1.949]
 [2.211]
 [1.949]
 [2.003]
 [1.949]]
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.264]
 [0.306]
 [0.306]
 [0.306]
 [0.344]
 [0.306]] [[37.005]
 [38.578]
 [37.005]
 [37.005]
 [37.005]
 [33.857]
 [37.005]] [[1.726]
 [1.807]
 [1.726]
 [1.726]
 [1.726]
 [1.518]
 [1.726]]
actor:  1 policy actor:  1  step number:  69 total reward:  0.11999999999999933  reward:  1.0 rdn_beta:  2.0
using another actor
printing an ep nov before normalisation:  31.555624965405613
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03901014430991306, 0.03936252606610449, 0.22780625093158352, 0.22469242698405903, 0.19677502932991614, 0.2723536223784238]
Printing some Q and Qe and total Qs values:  [[0.136]
 [0.178]
 [0.214]
 [0.194]
 [0.043]
 [0.169]
 [0.204]] [[33.104]
 [36.077]
 [32.649]
 [32.39 ]
 [31.544]
 [36.128]
 [33.049]] [[1.32 ]
 [1.559]
 [1.368]
 [1.331]
 [1.123]
 [1.553]
 [1.385]]
printing an ep nov before normalisation:  45.191070755157625
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03901014430991306, 0.03936252606610449, 0.22780625093158352, 0.22469242698405903, 0.19677502932991614, 0.2723536223784238]
printing an ep nov before normalisation:  41.35709308473233
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03895666214796471, 0.03930946878290709, 0.22798040670081257, 0.22486282830912002, 0.19691176969257929, 0.2719788643666164]
Printing some Q and Qe and total Qs values:  [[0.339]
 [0.331]
 [0.339]
 [0.339]
 [0.336]
 [0.339]
 [0.339]] [[59.579]
 [51.877]
 [59.579]
 [59.579]
 [63.617]
 [59.579]
 [59.579]] [[1.237]
 [1.034]
 [1.237]
 [1.237]
 [1.336]
 [1.237]
 [1.237]]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
printing an ep nov before normalisation:  30.89566732930181
printing an ep nov before normalisation:  28.787622451782227
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03895666214796471, 0.03930946878290709, 0.22798040670081257, 0.22486282830912002, 0.19691176969257929, 0.2719788643666164]
from probs:  [0.03895666214796471, 0.03930946878290709, 0.22798040670081257, 0.22486282830912002, 0.19691176969257929, 0.2719788643666164]
printing an ep nov before normalisation:  27.286023732752028
printing an ep nov before normalisation:  42.31039725754347
printing an ep nov before normalisation:  38.939499255836594
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03899969312703162, 0.03923386448096912, 0.2272444830635434, 0.2251119132930138, 0.1971298743831024, 0.2722801716523396]
printing an ep nov before normalisation:  34.994672123462216
printing an ep nov before normalisation:  35.69382577319354
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.038972095848593846, 0.03920670452389976, 0.2275684380469376, 0.22543188564439678, 0.1967342135637755, 0.27208666237239637]
siam score:  -0.7502766
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.038972095848593846, 0.03920670452389976, 0.2275684380469376, 0.22543188564439678, 0.1967342135637755, 0.27208666237239637]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.038972095848593846, 0.03920670452389976, 0.2275684380469376, 0.22543188564439678, 0.1967342135637755, 0.27208666237239637]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.038972095848593846, 0.03920670452389976, 0.2275684380469376, 0.22543188564439678, 0.1967342135637755, 0.27208666237239637]
siam score:  -0.74841017
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.07256259559645
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.039048156867142435, 0.03928322499390043, 0.22605703295419718, 0.22587310460549323, 0.19711923175952545, 0.27261924881974114]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.039048156867142435, 0.03928322499390043, 0.22605703295419718, 0.22587310460549323, 0.19711923175952545, 0.27261924881974114]
actions average: 
K:  4  action  0 :  tensor([0.6550, 0.0049, 0.0368, 0.0436, 0.1224, 0.0516, 0.0857],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0403, 0.7208, 0.0466, 0.0464, 0.0489, 0.0345, 0.0625],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1341, 0.0213, 0.1872, 0.1215, 0.1314, 0.2206, 0.1839],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1266, 0.0600, 0.0928, 0.3025, 0.1015, 0.1136, 0.2030],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0854, 0.1251, 0.0426, 0.1164, 0.4164, 0.1295, 0.0846],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1016, 0.0156, 0.1140, 0.1073, 0.0892, 0.4137, 0.1587],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1139, 0.0535, 0.1087, 0.1740, 0.1419, 0.1728, 0.2352],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  52.65247058510191
printing an ep nov before normalisation:  42.75537911215621
printing an ep nov before normalisation:  48.178762370760225
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.357]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]] [[30.644]
 [38.54 ]
 [30.644]
 [30.644]
 [30.644]
 [30.644]
 [30.644]] [[1.064]
 [1.519]
 [1.064]
 [1.064]
 [1.064]
 [1.064]
 [1.064]]
printing an ep nov before normalisation:  34.22656950524838
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.713]
 [0.634]
 [0.651]
 [0.673]
 [0.682]
 [0.668]] [[48.076]
 [42.265]
 [49.116]
 [48.852]
 [48.313]
 [48.071]
 [49.291]] [[1.633]
 [1.745]
 [1.892]
 [1.901]
 [1.905]
 [1.906]
 [1.932]]
printing an ep nov before normalisation:  33.57815691862573
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.0389948973384979, 0.03923024795718653, 0.2262285103951189, 0.2260443610114536, 0.1972559333444425, 0.27224604995330065]
printing an ep nov before normalisation:  28.075804710388184
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.0389948973384979, 0.03923024795718653, 0.2262285103951189, 0.2260443610114536, 0.1972559333444425, 0.27224604995330065]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03902060614023921, 0.03925611245430336, 0.2263780829362396, 0.22619381172919603, 0.19672532150386385, 0.2724260652361581]
actor:  1 policy actor:  1  step number:  68 total reward:  0.15333333333333243  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  36.47756690108495
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.038597896602016527, 0.038830842939892196, 0.2239187796818866, 0.2345979181659009, 0.19458834660614666, 0.2694662160041571]
printing an ep nov before normalisation:  30.22141729102553
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.038597896602016527, 0.038830842939892196, 0.2239187796818866, 0.2345979181659009, 0.19458834660614666, 0.2694662160041571]
Printing some Q and Qe and total Qs values:  [[0.26 ]
 [0.184]
 [0.232]
 [0.273]
 [0.269]
 [0.308]
 [0.29 ]] [[50.13 ]
 [42.689]
 [39.15 ]
 [42.869]
 [43.925]
 [41.6  ]
 [42.649]] [[2.134]
 [1.603]
 [1.434]
 [1.703]
 [1.763]
 [1.66 ]
 [1.706]]
printing an ep nov before normalisation:  32.40899351714341
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.38695985061495
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
Printing some Q and Qe and total Qs values:  [[0.312]
 [0.167]
 [0.349]
 [0.399]
 [0.375]
 [0.399]
 [0.399]] [[35.188]
 [34.352]
 [32.117]
 [36.549]
 [44.453]
 [36.549]
 [36.549]] [[1.048]
 [0.863]
 [0.937]
 [1.201]
 [1.556]
 [1.201]
 [1.201]]
printing an ep nov before normalisation:  43.1712941639535
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03862289801457875, 0.038855995763802054, 0.22406423666763667, 0.23475031641874328, 0.19406527504925056, 0.26964127808598853]
printing an ep nov before normalisation:  16.1212289678135
line 256 mcts: sample exp_bonus 33.840986740251225
Printing some Q and Qe and total Qs values:  [[0.274]
 [0.45 ]
 [0.431]
 [0.418]
 [0.954]
 [0.434]
 [0.387]] [[29.584]
 [31.858]
 [29.855]
 [30.223]
 [17.882]
 [32.213]
 [32.464]] [[1.02 ]
 [1.269]
 [1.186]
 [1.185]
 [1.326]
 [1.265]
 [1.225]]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03856982930611777, 0.038803203522892946, 0.22423111222327055, 0.23492986628941384, 0.19419657008338534, 0.26926941857491954]
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.569]
 [0.569]
 [0.569]
 [0.569]
 [0.431]
 [0.569]] [[29.872]
 [29.872]
 [29.872]
 [29.872]
 [29.872]
 [30.573]
 [29.872]] [[2.458]
 [2.458]
 [2.458]
 [2.458]
 [2.458]
 [2.409]
 [2.458]]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03859467558251382, 0.03882820065758313, 0.22437597409014187, 0.23508164406932006, 0.19367611139381, 0.26944339420663116]
actor:  1 policy actor:  1  step number:  84 total reward:  0.03333333333333266  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03941232400069281, 0.039641584323168985, 0.22180079082062235, 0.23231094846010225, 0.1916615844863722, 0.2751727679090414]
printing an ep nov before normalisation:  33.21336337498256
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03941232400069281, 0.039641584323168985, 0.22180079082062235, 0.23231094846010225, 0.1916615844863722, 0.2751727679090414]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.0394493491867052, 0.039678825614099245, 0.2210678741100911, 0.232529803371144, 0.19184212257325475, 0.27543202514470577]
printing an ep nov before normalisation:  43.78985713976057
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.0394493491867052, 0.039678825614099245, 0.2210678741100911, 0.232529803371144, 0.19184212257325475, 0.27543202514470577]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.0394493491867052, 0.039678825614099245, 0.2210678741100911, 0.232529803371144, 0.19184212257325475, 0.27543202514470577]
printing an ep nov before normalisation:  44.189520788261326
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03939655059066969, 0.03962630290096282, 0.2212334223209729, 0.2327091314299588, 0.1919725347784316, 0.27506205797900424]
printing an ep nov before normalisation:  12.39363431930542
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03939655059066969, 0.03962630290096282, 0.2212334223209729, 0.2327091314299588, 0.1919725347784316, 0.27506205797900424]
actor:  1 policy actor:  1  step number:  80 total reward:  0.11333333333333206  reward:  1.0 rdn_beta:  2.0
siam score:  -0.7647421
line 256 mcts: sample exp_bonus 35.681382234081
actor:  1 policy actor:  1  step number:  64 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  67.54974532901501
printing an ep nov before normalisation:  39.4066173926982
printing an ep nov before normalisation:  33.6095211103574
printing an ep nov before normalisation:  30.838122874561446
printing an ep nov before normalisation:  56.24608983203754
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03975178962166162, 0.039973364099391395, 0.21511633692154228, 0.24070744531085234, 0.18689696508203354, 0.27755409896451877]
printing an ep nov before normalisation:  14.148847822835108
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03975178962166162, 0.039973364099391395, 0.21511633692154228, 0.24070744531085234, 0.18689696508203354, 0.27755409896451877]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03975178962166162, 0.039973364099391395, 0.21511633692154228, 0.24070744531085234, 0.18689696508203354, 0.27755409896451877]
actor:  1 policy actor:  1  step number:  85 total reward:  0.013333333333331976  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  61.739105556147784
printing an ep nov before normalisation:  46.465102365217284
printing an ep nov before normalisation:  38.44213491710917
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.04050008219974816, 0.040718230577756656, 0.21226593435459556, 0.2383484571091354, 0.185370022489551, 0.2827972732692132]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.04044785199956557, 0.040666266177547115, 0.21242298995927725, 0.23853729261476422, 0.18549430713020454, 0.2824312921186413]
actor:  1 policy actor:  1  step number:  60 total reward:  0.15333333333333277  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03973771713976711, 0.03995228399135764, 0.2086835440310097, 0.23433784795052734, 0.19982994432531984, 0.27745866256201834]
printing an ep nov before normalisation:  39.12787835207291
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.525]
 [0.51 ]
 [0.525]
 [0.51 ]
 [0.51 ]
 [0.507]] [[39.379]
 [39.379]
 [48.334]
 [39.379]
 [48.148]
 [49.069]
 [49.977]] [[1.154]
 [1.154]
 [1.498]
 [1.154]
 [1.491]
 [1.528]
 [1.562]]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03971148265505514, 0.03992644562801647, 0.2089692079222277, 0.23467087338878886, 0.19944727257402603, 0.2772747178318859]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03971148265505514, 0.03992644562801647, 0.2089692079222277, 0.23467087338878886, 0.19944727257402603, 0.2772747178318859]
printing an ep nov before normalisation:  80.36597180634952
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.039659553428874846, 0.03987477210718948, 0.20911861619079367, 0.2348508546130511, 0.1995853541949893, 0.2769108494651015]
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]] [[19.399]
 [19.399]
 [19.399]
 [19.399]
 [19.399]
 [19.399]
 [19.399]] [[0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]]
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([0.5833, 0.0027, 0.0667, 0.0596, 0.1154, 0.0730, 0.0991],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0061, 0.9226, 0.0076, 0.0296, 0.0055, 0.0049, 0.0237],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0763, 0.0958, 0.2183, 0.0951, 0.1115, 0.2477, 0.1553],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1055, 0.0255, 0.0918, 0.3618, 0.1476, 0.1251, 0.1427],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1887, 0.0923, 0.0536, 0.0698, 0.3908, 0.0947, 0.1100],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0605, 0.0288, 0.0724, 0.0753, 0.0843, 0.5844, 0.0942],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1510, 0.0541, 0.1024, 0.1147, 0.1555, 0.1424, 0.2800],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.039659553428874846, 0.03987477210718948, 0.20911861619079367, 0.2348508546130511, 0.1995853541949893, 0.2769108494651015]
printing an ep nov before normalisation:  27.185334465989094
printing an ep nov before normalisation:  37.36116831257909
Printing some Q and Qe and total Qs values:  [[0.44]
 [0.44]
 [0.44]
 [0.44]
 [0.44]
 [0.44]
 [0.44]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.44]
 [0.44]
 [0.44]
 [0.44]
 [0.44]
 [0.44]
 [0.44]]
printing an ep nov before normalisation:  60.506864781872224
actor:  1 policy actor:  1  step number:  75 total reward:  0.306666666666666  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  45.87556159682494
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03878740317277742, 0.038997873040838495, 0.20365930384758074, 0.22967179912118144, 0.21807988260786867, 0.27080373820975334]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03878740317277742, 0.038997873040838495, 0.20365930384758074, 0.22967179912118144, 0.21807988260786867, 0.27080373820975334]
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.066722869873047
printing an ep nov before normalisation:  0.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03868416317578644, 0.03889512046587603, 0.2039378866597716, 0.23001062363604297, 0.2183918616863108, 0.2700803443762121]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03868416317578644, 0.03889512046587603, 0.2039378866597716, 0.23001062363604297, 0.2183918616863108, 0.2700803443762121]
printing an ep nov before normalisation:  0.0
using another actor
from probs:  [0.03868416317578644, 0.03889512046587603, 0.2039378866597716, 0.23001062363604297, 0.2183918616863108, 0.2700803443762121]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03863272048025991, 0.03884392064427177, 0.20407669963152827, 0.23017945400085282, 0.21854731543689893, 0.2697198898061883]
using explorer policy with actor:  1
printing an ep nov before normalisation:  22.62604421976741
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03861390599116986, 0.03882552726261483, 0.20354286811761363, 0.23054256039544493, 0.21888722876232297, 0.2695879094708337]
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.366]
 [0.509]
 [0.433]
 [0.509]
 [0.509]
 [0.509]] [[10.607]
 [15.536]
 [12.718]
 [13.779]
 [12.718]
 [12.718]
 [12.718]] [[0.508]
 [0.366]
 [0.509]
 [0.433]
 [0.509]
 [0.509]
 [0.509]]
printing an ep nov before normalisation:  14.951501652755148
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.314]
 [0.414]
 [0.415]
 [0.415]
 [0.413]
 [0.413]] [[27.893]
 [33.572]
 [28.506]
 [28.168]
 [28.887]
 [29.729]
 [30.379]] [[0.421]
 [0.314]
 [0.414]
 [0.415]
 [0.415]
 [0.413]
 [0.413]]
printing an ep nov before normalisation:  30.060911748099773
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03856269207000391, 0.0387745555155221, 0.20368039476244565, 0.230710984810666, 0.2190423151084689, 0.26922905773289363]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03856269207000391, 0.0387745555155221, 0.20368039476244565, 0.230710984810666, 0.2190423151084689, 0.26922905773289363]
Printing some Q and Qe and total Qs values:  [[0.735]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]] [[15.811]
 [12.173]
 [12.173]
 [12.173]
 [12.173]
 [12.173]
 [12.173]] [[0.827]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]]
line 256 mcts: sample exp_bonus 32.589271657871834
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.038588179377993685, 0.0389043242757079, 0.20300705297211935, 0.23088176760840043, 0.21921060448033802, 0.2694080712854407]
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.482]
 [0.482]
 [0.476]
 [0.441]
 [0.482]
 [0.482]] [[55.233]
 [55.233]
 [55.233]
 [41.034]
 [49.894]
 [55.233]
 [55.233]] [[1.079]
 [1.079]
 [1.079]
 [0.865]
 [0.96 ]
 [1.079]
 [1.079]]
printing an ep nov before normalisation:  20.835493894875075
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.038588179377993685, 0.0389043242757079, 0.20300705297211935, 0.23088176760840043, 0.21921060448033802, 0.2694080712854407]
printing an ep nov before normalisation:  32.13268790177424
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.038588179377993685, 0.0389043242757079, 0.20300705297211935, 0.23088176760840043, 0.21921060448033802, 0.2694080712854407]
printing an ep nov before normalisation:  37.239038185675476
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.038592279827277594, 0.03880191373145633, 0.20302868539599828, 0.23090637231142322, 0.21923396468793893, 0.2694367840459056]
printing an ep nov before normalisation:  19.166910648345947
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.038592279827277594, 0.03880191373145633, 0.20302868539599828, 0.23090637231142322, 0.21923396468793893, 0.2694367840459056]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.038592279827277594, 0.03880191373145633, 0.20302868539599828, 0.23090637231142322, 0.21923396468793893, 0.2694367840459056]
printing an ep nov before normalisation:  38.34764503873899
using explorer policy with actor:  1
printing an ep nov before normalisation:  8.836043519977466e-05
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.504]
 [0.493]
 [0.464]
 [0.468]
 [0.466]
 [0.493]] [[40.431]
 [40.085]
 [40.431]
 [42.839]
 [42.287]
 [43.084]
 [40.431]] [[2.077]
 [2.066]
 [2.077]
 [2.2  ]
 [2.17 ]
 [2.219]
 [2.077]]
actor:  1 policy actor:  1  step number:  89 total reward:  0.02666666666666584  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.468]
 [0.392]
 [0.411]
 [0.397]
 [0.391]
 [0.393]] [[32.016]
 [36.583]
 [37.739]
 [34.863]
 [37.099]
 [35.305]
 [35.824]] [[0.94 ]
 [1.187]
 [1.155]
 [1.064]
 [1.136]
 [1.061]
 [1.082]]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03830791874023791, 0.038411395244445055, 0.20152850355162447, 0.2366926296694455, 0.21761396317909731, 0.26744558961514986]
printing an ep nov before normalisation:  39.85741376876831
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.0383394528920565, 0.038443014872887656, 0.2008694775289279, 0.23688803923890583, 0.21779361284234056, 0.2676664026248815]
printing an ep nov before normalisation:  41.75285478577701
printing an ep nov before normalisation:  38.50127477198255
printing an ep nov before normalisation:  44.29868537020978
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03837075439010411, 0.03847440121692682, 0.2002153136915297, 0.23708200710879282, 0.21797193707999912, 0.26788558651264743]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03837075439010411, 0.03847440121692682, 0.2002153136915297, 0.23708200710879282, 0.21797193707999912, 0.26788558651264743]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03837075439010411, 0.03847440121692682, 0.2002153136915297, 0.23708200710879282, 0.21797193707999912, 0.26788558651264743]
printing an ep nov before normalisation:  38.57685847305909
using explorer policy with actor:  1
printing an ep nov before normalisation:  61.02191464050146
printing an ep nov before normalisation:  48.82593146080085
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03831971789866887, 0.03842348228665856, 0.20034784900680666, 0.23725635838345283, 0.21812461280315015, 0.2675279796212629]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03831971789866887, 0.03842348228665856, 0.20034784900680666, 0.23725635838345283, 0.21812461280315015, 0.2675279796212629]
line 256 mcts: sample exp_bonus 1.2937447119535742e-05
siam score:  -0.7864476
printing an ep nov before normalisation:  18.588153222199637
actor:  1 policy actor:  1  step number:  103 total reward:  0.013333333333332198  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03799238149637892, 0.038095510214259616, 0.19902791348880072, 0.24295279027001632, 0.21669577520805505, 0.2652356293224894]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03799238149637892, 0.038095510214259616, 0.19902791348880072, 0.24295279027001632, 0.21669577520805505, 0.2652356293224894]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03799238149637892, 0.038095510214259616, 0.19902791348880072, 0.24295279027001632, 0.21669577520805505, 0.2652356293224894]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03799238149637892, 0.038095510214259616, 0.19902791348880072, 0.24295279027001632, 0.21669577520805505, 0.2652356293224894]
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
actor:  1 policy actor:  1  step number:  65 total reward:  0.026666666666666172  reward:  1.0 rdn_beta:  1.333
siam score:  -0.78933877
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03774170282376895, 0.03784414869042699, 0.1969090821089824, 0.24876293353977236, 0.2152618390161232, 0.26348029382092625]
printing an ep nov before normalisation:  26.539218425750732
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.682254056092646
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.96954 -0.852 -0.852
probs:  [0.03774170282376895, 0.03784414869042699, 0.1969090821089824, 0.24876293353977236, 0.2152618390161232, 0.26348029382092625]
actor:  0 policy actor:  0  step number:  64 total reward:  0.23333333333333273  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  26.629377595396775
siam score:  -0.7929262
printing an ep nov before normalisation:  13.114802421570683
maxi score, test score, baseline:  -0.9670733333333333 -0.852 -0.852
probs:  [0.037771638775504025, 0.03787416618799534, 0.19627036041329562, 0.24896084017358788, 0.2154330791307525, 0.2636899153188645]
printing an ep nov before normalisation:  26.356611251831055
maxi score, test score, baseline:  -0.9670733333333333 -0.852 -0.852
probs:  [0.037805224245187175, 0.037805224245187175, 0.19565639049905603, 0.24918287377749412, 0.21562519527689403, 0.26392509195618147]
Printing some Q and Qe and total Qs values:  [[0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.46 ]
 [0.405]
 [0.405]] [[35.553]
 [35.553]
 [35.553]
 [35.553]
 [25.023]
 [35.553]
 [35.553]] [[2.072]
 [2.072]
 [2.072]
 [2.072]
 [1.492]
 [2.072]
 [2.072]]
maxi score, test score, baseline:  -0.9670733333333333 -0.852 -0.852
probs:  [0.03775452346766626, 0.03775452346766626, 0.19578134656595136, 0.24936739413756845, 0.2157723726381724, 0.26356983972297526]
maxi score, test score, baseline:  -0.9670733333333333 -0.852 -0.852
probs:  [0.03775452346766626, 0.03775452346766626, 0.19578134656595136, 0.24936739413756845, 0.2157723726381724, 0.26356983972297526]
printing an ep nov before normalisation:  43.49683019593411
printing an ep nov before normalisation:  38.264500311578296
printing an ep nov before normalisation:  26.32485629584762
maxi score, test score, baseline:  -0.9670733333333333 -0.852 -0.852
probs:  [0.03778121267675066, 0.03778121267675066, 0.19592014278066303, 0.2495442052362021, 0.2152165003435183, 0.26375672628611513]
maxi score, test score, baseline:  -0.9670733333333333 -0.852 -0.852
probs:  [0.03778121267675066, 0.03778121267675066, 0.19592014278066303, 0.2495442052362021, 0.2152165003435183, 0.26375672628611513]
printing an ep nov before normalisation:  13.115769624710083
printing an ep nov before normalisation:  35.60438045107027
maxi score, test score, baseline:  -0.9670733333333333 -0.852 -0.852
probs:  [0.03778121267675066, 0.03778121267675066, 0.19592014278066303, 0.2495442052362021, 0.2152165003435183, 0.26375672628611513]
maxi score, test score, baseline:  -0.9670733333333333 -0.852 -0.852
probs:  [0.03787674031312398, 0.03777510179092896, 0.19591924523249848, 0.24951061042548153, 0.21520383684475777, 0.2637144653932092]
maxi score, test score, baseline:  -0.9670733333333333 -0.852 -0.852
Printing some Q and Qe and total Qs values:  [[0.51 ]
 [0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]] [[22.338]
 [23.427]
 [23.427]
 [23.427]
 [23.427]
 [23.427]
 [23.427]] [[0.51 ]
 [0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]]
maxi score, test score, baseline:  -0.9670733333333333 -0.852 -0.852
probs:  [0.03787674031312398, 0.03777510179092896, 0.19591924523249848, 0.24951061042548153, 0.21520383684475777, 0.2637144653932092]
printing an ep nov before normalisation:  26.386870274965283
maxi score, test score, baseline:  -0.9670733333333333 -0.852 -0.852
probs:  [0.03787674031312398, 0.03777510179092896, 0.19591924523249848, 0.24951061042548153, 0.21520383684475777, 0.2637144653932092]
using another actor
siam score:  -0.7849849
maxi score, test score, baseline:  -0.9670733333333333 -0.852 -0.852
probs:  [0.03782628727755912, 0.03772453602769255, 0.1960440777402799, 0.24969488132024195, 0.21535005796644843, 0.26336015966777815]
printing an ep nov before normalisation:  39.99388561583889
maxi score, test score, baseline:  -0.9670733333333333 -0.852 -0.852
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9670733333333333 -0.852 -0.852
probs:  [0.03782628727755912, 0.03772453602769255, 0.1960440777402799, 0.24969488132024195, 0.21535005796644843, 0.26336015966777815]
printing an ep nov before normalisation:  15.942762997586026
maxi score, test score, baseline:  -0.9670733333333333 -0.852 -0.852
probs:  [0.03782628727755912, 0.03772453602769255, 0.1960440777402799, 0.24969488132024195, 0.21535005796644843, 0.26336015966777815]
maxi score, test score, baseline:  -0.9670733333333333 -0.852 -0.852
probs:  [0.03782628727755912, 0.03772453602769255, 0.1960440777402799, 0.24969488132024195, 0.21535005796644843, 0.26336015966777815]
maxi score, test score, baseline:  -0.9670733333333333 -0.852 -0.852
probs:  [0.03772836444022303, 0.03772836444022303, 0.19606402989335017, 0.24972029743327137, 0.21537197629888397, 0.2633869674940484]
printing an ep nov before normalisation:  22.498947405622083
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.56 ]
 [0.437]
 [0.445]
 [0.47 ]
 [0.444]
 [0.455]] [[23.764]
 [22.401]
 [22.907]
 [23.902]
 [24.259]
 [23.764]
 [24.065]] [[0.444]
 [0.56 ]
 [0.437]
 [0.445]
 [0.47 ]
 [0.444]
 [0.455]]
maxi score, test score, baseline:  -0.9670733333333333 -0.852 -0.852
probs:  [0.03772836444022303, 0.03772836444022303, 0.19606402989335017, 0.24972029743327137, 0.21537197629888397, 0.2633869674940484]
printing an ep nov before normalisation:  24.76564347743988
maxi score, test score, baseline:  -0.9670733333333333 -0.852 -0.852
probs:  [0.0377578820299556, 0.0377578820299556, 0.195433347551649, 0.24991625917747368, 0.21554096967128106, 0.26359365953968517]
maxi score, test score, baseline:  -0.9670733333333333 -0.852 -0.852
probs:  [0.0377578820299556, 0.0377578820299556, 0.195433347551649, 0.24991625917747368, 0.21554096967128106, 0.26359365953968517]
actions average: 
K:  1  action  0 :  tensor([0.3981, 0.0017, 0.0980, 0.1168, 0.1054, 0.1433, 0.1367],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0200, 0.8980, 0.0093, 0.0152, 0.0086, 0.0114, 0.0374],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2415, 0.0209, 0.1077, 0.1202, 0.1342, 0.1865, 0.1889],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1630, 0.0062, 0.1093, 0.2141, 0.1196, 0.2240, 0.1637],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1203, 0.0021, 0.0355, 0.0440, 0.7163, 0.0444, 0.0373],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0769, 0.0273, 0.0865, 0.0966, 0.0778, 0.5477, 0.0872],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1462, 0.0046, 0.1081, 0.1427, 0.1190, 0.1536, 0.3258],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9670733333333333 -0.852 -0.852
probs:  [0.0377578820299556, 0.0377578820299556, 0.195433347551649, 0.24991625917747368, 0.21554096967128106, 0.26359365953968517]
maxi score, test score, baseline:  -0.9670733333333333 -0.852 -0.852
probs:  [0.0377578820299556, 0.0377578820299556, 0.195433347551649, 0.24991625917747368, 0.21554096967128106, 0.26359365953968517]
printing an ep nov before normalisation:  28.337908790272355
printing an ep nov before normalisation:  31.326420103637776
printing an ep nov before normalisation:  32.5431506255189
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]] [[44.941]
 [41.1  ]
 [41.1  ]
 [41.1  ]
 [41.1  ]
 [41.1  ]
 [41.1  ]] [[0.459]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]]
maxi score, test score, baseline:  -0.9670733333333333 -0.852 -0.852
probs:  [0.0377578820299556, 0.0377578820299556, 0.195433347551649, 0.24991625917747368, 0.21554096967128106, 0.26359365953968517]
printing an ep nov before normalisation:  47.29579788271183
printing an ep nov before normalisation:  27.335631847381592
actor:  0 policy actor:  1  step number:  80 total reward:  0.0733333333333327  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  38.73235408151326
printing an ep nov before normalisation:  18.425290314435294
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.0377578820299556, 0.0377578820299556, 0.195433347551649, 0.24991625917747368, 0.21554096967128106, 0.26359365953968517]
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.03770742511401694, 0.03770742511401694, 0.19555732676896131, 0.25010051275897327, 0.2156871939210707, 0.2632401163229609]
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.324]
 [0.354]
 [0.354]
 [0.329]
 [0.33 ]
 [0.326]] [[44.126]
 [41.645]
 [44.126]
 [44.126]
 [38.335]
 [38.189]
 [39.909]] [[1.434]
 [1.272]
 [1.434]
 [1.434]
 [1.101]
 [1.094]
 [1.182]]
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.03770742511401694, 0.03770742511401694, 0.19555732676896131, 0.25010051275897327, 0.2156871939210707, 0.2632401163229609]
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.0376570844716015, 0.0376570844716015, 0.19568102028707585, 0.25028434174430625, 0.21583308120994257, 0.26288738781547244]
printing an ep nov before normalisation:  30.211940764429087
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.0376570844716015, 0.0376570844716015, 0.19568102028707585, 0.25028434174430625, 0.21583308120994257, 0.26288738781547244]
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.0376570844716015, 0.0376570844716015, 0.19568102028707585, 0.25028434174430625, 0.21583308120994257, 0.26288738781547244]
Printing some Q and Qe and total Qs values:  [[0.446]
 [0.446]
 [0.446]
 [0.446]
 [0.446]
 [0.446]
 [0.435]] [[32.42 ]
 [32.42 ]
 [32.42 ]
 [32.42 ]
 [32.42 ]
 [32.42 ]
 [36.817]] [[1.02 ]
 [1.02 ]
 [1.02 ]
 [1.02 ]
 [1.02 ]
 [1.02 ]
 [1.158]]
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.0376570844716015, 0.0376570844716015, 0.19568102028707585, 0.25028434174430625, 0.21583308120994257, 0.26288738781547244]
printing an ep nov before normalisation:  33.182013280104094
actions average: 
K:  4  action  0 :  tensor([0.3445, 0.1070, 0.0844, 0.0985, 0.1082, 0.1166, 0.1408],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0249, 0.8264, 0.0281, 0.0141, 0.0316, 0.0186, 0.0562],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1153, 0.0017, 0.3935, 0.1010, 0.1005, 0.1345, 0.1534],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1143, 0.0361, 0.1026, 0.3231, 0.1341, 0.1274, 0.1623],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1082, 0.0840, 0.0632, 0.0968, 0.4715, 0.0850, 0.0912],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0955, 0.0844, 0.1578, 0.1189, 0.1081, 0.2968, 0.1385],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1350, 0.2238, 0.0958, 0.1345, 0.1256, 0.1122, 0.1731],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  34.6358175984831
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.427842766250045
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.0376570844716015, 0.0376570844716015, 0.19568102028707585, 0.25028434174430625, 0.21583308120994257, 0.26288738781547244]
line 256 mcts: sample exp_bonus 23.652812204362043
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.0376570844716015, 0.0376570844716015, 0.19568102028707585, 0.25028434174430625, 0.21583308120994257, 0.26288738781547244]
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
actor:  1 policy actor:  1  step number:  76 total reward:  0.09999999999999953  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.03733060745411865, 0.03733060745411865, 0.19397965309595042, 0.2568014609736604, 0.21395638064037345, 0.2606012903817785]
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.03733060745411865, 0.03733060745411865, 0.19397965309595042, 0.2568014609736604, 0.21395638064037345, 0.2606012903817785]
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.03733060745411865, 0.03733060745411865, 0.19397965309595042, 0.2568014609736604, 0.21395638064037345, 0.2606012903817785]
printing an ep nov before normalisation:  43.99937336977904
actor:  1 policy actor:  1  step number:  58 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
printing an ep nov before normalisation:  22.186543614348544
printing an ep nov before normalisation:  32.711797166977455
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.036297368127365136, 0.036297368127365136, 0.21703085202025302, 0.24967193441775748, 0.20733626212769454, 0.25336621517956476]
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.421]
 [0.341]
 [0.341]
 [0.304]
 [0.341]
 [0.341]] [[32.485]
 [30.033]
 [32.485]
 [32.485]
 [31.122]
 [32.485]
 [32.485]] [[0.674]
 [0.711]
 [0.674]
 [0.674]
 [0.613]
 [0.674]
 [0.674]]
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.036297368127365136, 0.036297368127365136, 0.21703085202025302, 0.24967193441775748, 0.20733626212769454, 0.25336621517956476]
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.036297368127365136, 0.036297368127365136, 0.21703085202025302, 0.24967193441775748, 0.20733626212769454, 0.25336621517956476]
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
printing an ep nov before normalisation:  26.54170516799809
printing an ep nov before normalisation:  46.98995676112638
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.036297368127365136, 0.036297368127365136, 0.21703085202025302, 0.24967193441775748, 0.20733626212769454, 0.25336621517956476]
printing an ep nov before normalisation:  1.0034119562533306
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.036321845641462534, 0.036321845641462534, 0.21717765806840664, 0.24984083341556101, 0.206800202600703, 0.2535376146324044]
Printing some Q and Qe and total Qs values:  [[0.116]
 [0.605]
 [0.605]
 [0.171]
 [0.334]
 [0.605]
 [0.524]] [[11.931]
 [11.756]
 [11.756]
 [13.436]
 [13.088]
 [11.756]
 [11.835]] [[0.369]
 [0.851]
 [0.851]
 [0.478]
 [0.628]
 [0.851]
 [0.774]]
printing an ep nov before normalisation:  12.691528555992129
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.036321845641462534, 0.036321845641462534, 0.21717765806840664, 0.24984083341556101, 0.206800202600703, 0.2535376146324044]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.56]
 [0.56]
 [0.56]
 [0.56]
 [0.56]
 [0.56]
 [0.56]] [[48.448]
 [48.448]
 [48.448]
 [48.448]
 [48.448]
 [48.448]
 [48.448]] [[2.227]
 [2.227]
 [2.227]
 [2.227]
 [2.227]
 [2.227]
 [2.227]]
actor:  1 policy actor:  1  step number:  83 total reward:  0.026666666666665506  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  17.85672430381623
Printing some Q and Qe and total Qs values:  [[0.22 ]
 [0.058]
 [0.189]
 [0.187]
 [0.189]
 [0.149]
 [0.183]] [[16.391]
 [11.802]
 [14.633]
 [16.118]
 [14.633]
 [17.055]
 [16.033]] [[1.587]
 [1.042]
 [1.41 ]
 [1.531]
 [1.41 ]
 [1.572]
 [1.521]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.651]
 [0.678]
 [0.678]] [[33.681]
 [33.681]
 [33.681]
 [33.681]
 [43.585]
 [33.681]
 [33.681]] [[1.447]
 [1.447]
 [1.447]
 [1.447]
 [1.798]
 [1.447]
 [1.447]]
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.05445298323831836, 0.03561452129781958, 0.2133681736088533, 0.24547108827949843, 0.20250874677342265, 0.2485844868020877]
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.448]
 [0.414]
 [0.396]
 [0.393]
 [0.404]
 [0.404]] [[27.001]
 [31.978]
 [28.105]
 [28.518]
 [28.709]
 [28.134]
 [28.46 ]] [[1.342]
 [1.705]
 [1.404]
 [1.415]
 [1.426]
 [1.397]
 [1.419]]
printing an ep nov before normalisation:  54.276738456328836
Printing some Q and Qe and total Qs values:  [[0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.708]
 [0.67 ]
 [0.751]] [[39.462]
 [39.462]
 [39.462]
 [39.462]
 [49.37 ]
 [48.286]
 [39.462]] [[1.842]
 [1.842]
 [1.842]
 [1.842]
 [2.22 ]
 [2.135]
 [1.842]]
printing an ep nov before normalisation:  36.34942639920136
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.05452407192570125, 0.03566095594368448, 0.2136472358406587, 0.24579216384517574, 0.20146593554817693, 0.24890963689660286]
printing an ep nov before normalisation:  55.12910498959395
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  52.01002017450499
printing an ep nov before normalisation:  44.40587520599365
printing an ep nov before normalisation:  34.90243286978245
Printing some Q and Qe and total Qs values:  [[0.442]
 [0.308]
 [0.431]
 [0.426]
 [0.423]
 [0.424]
 [0.426]] [[22.157]
 [29.38 ]
 [21.488]
 [21.529]
 [21.681]
 [22.233]
 [22.413]] [[0.442]
 [0.308]
 [0.431]
 [0.426]
 [0.423]
 [0.424]
 [0.426]]
printing an ep nov before normalisation:  33.88956435719826
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.054570884607554604, 0.035691533668268674, 0.21297101729417683, 0.24600359563355556, 0.20163921698944598, 0.24912375180699844]
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.054570884607554604, 0.035691533668268674, 0.21297101729417683, 0.24600359563355556, 0.20163921698944598, 0.24912375180699844]
printing an ep nov before normalisation:  52.10133360158111
printing an ep nov before normalisation:  53.34518340999122
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.054570884607554604, 0.035691533668268674, 0.21297101729417683, 0.24600359563355556, 0.20163921698944598, 0.24912375180699844]
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
Printing some Q and Qe and total Qs values:  [[ 0.097]
 [-0.   ]
 [ 0.113]
 [ 0.112]
 [ 0.115]
 [ 0.115]
 [ 0.113]] [[11.003]
 [14.802]
 [10.459]
 [10.502]
 [10.554]
 [10.766]
 [10.766]] [[0.61 ]
 [0.69 ]
 [0.601]
 [0.602]
 [0.607]
 [0.617]
 [0.615]]
printing an ep nov before normalisation:  16.954896494594834
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.442]
 [0.364]
 [0.366]
 [0.364]
 [0.35 ]
 [0.344]] [[42.13 ]
 [43.038]
 [42.13 ]
 [46.937]
 [42.13 ]
 [46.495]
 [46.919]] [[1.795]
 [1.933]
 [1.795]
 [2.116]
 [1.795]
 [2.071]
 [2.093]]
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.054606183024993815, 0.03571459035385007, 0.2131090253840422, 0.24616302267833645, 0.20112197653812153, 0.2492852020206559]
line 256 mcts: sample exp_bonus 34.980836331548005
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.054606183024993815, 0.03571459035385007, 0.2131090253840422, 0.24616302267833645, 0.20112197653812153, 0.2492852020206559]
printing an ep nov before normalisation:  44.933873468469045
printing an ep nov before normalisation:  31.93333275421395
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.05465272678534564, 0.03574499242074318, 0.212437245053413, 0.24637323986948004, 0.2012937089507442, 0.249498086920274]
Printing some Q and Qe and total Qs values:  [[0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]] [[39.704]
 [39.704]
 [39.704]
 [39.704]
 [39.704]
 [39.704]
 [39.704]] [[1.292]
 [1.292]
 [1.292]
 [1.292]
 [1.292]
 [1.292]
 [1.292]]
printing an ep nov before normalisation:  27.149312234218044
siam score:  -0.7839112
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.05465272678534564, 0.03574499242074318, 0.212437245053413, 0.24637323986948004, 0.2012937089507442, 0.249498086920274]
printing an ep nov before normalisation:  27.49353849594481
printing an ep nov before normalisation:  25.971384587939905
Printing some Q and Qe and total Qs values:  [[0.424]
 [0.425]
 [0.424]
 [0.418]
 [0.424]
 [0.407]
 [0.424]] [[34.781]
 [36.939]
 [34.781]
 [37.78 ]
 [34.781]
 [37.762]
 [34.781]] [[1.523]
 [1.657]
 [1.523]
 [1.702]
 [1.523]
 [1.69 ]
 [1.523]]
printing an ep nov before normalisation:  37.52191198718649
printing an ep nov before normalisation:  44.64676599627983
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.05465802082350652, 0.03571839356385092, 0.21270868441523336, 0.24670192126254126, 0.20090136185803867, 0.24931161807682933]
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.252814123997226
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.05465802082350652, 0.03571839356385092, 0.21270868441523336, 0.24670192126254126, 0.20090136185803867, 0.24931161807682933]
printing an ep nov before normalisation:  21.659408717253825
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.05465802082350652, 0.03571839356385092, 0.21270868441523336, 0.24670192126254126, 0.20090136185803867, 0.24931161807682933]
printing an ep nov before normalisation:  44.210778123028724
actions average: 
K:  4  action  0 :  tensor([0.4424, 0.0069, 0.0760, 0.0998, 0.1220, 0.1027, 0.1502],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0239, 0.7732, 0.0262, 0.0299, 0.0311, 0.0325, 0.0832],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1734, 0.0667, 0.1798, 0.1152, 0.1341, 0.1078, 0.2230],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1005, 0.1175, 0.0759, 0.1558, 0.1919, 0.1567, 0.2019],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0864, 0.0521, 0.0578, 0.0893, 0.4353, 0.1096, 0.1693],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1293, 0.0290, 0.0873, 0.1250, 0.1232, 0.3453, 0.1610],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0835, 0.1676, 0.0344, 0.0861, 0.0525, 0.0265, 0.5494],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  66 total reward:  0.27333333333333265  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.05539547802661703, 0.03690800821251003, 0.20967290029014057, 0.24285459555175168, 0.19752227230288186, 0.2576467456160989]
Printing some Q and Qe and total Qs values:  [[0.196]
 [0.265]
 [0.196]
 [0.196]
 [0.218]
 [0.227]
 [0.196]] [[28.364]
 [25.849]
 [28.364]
 [28.364]
 [28.468]
 [27.884]
 [28.364]] [[1.135]
 [1.041]
 [1.135]
 [1.135]
 [1.163]
 [1.134]
 [1.135]]
printing an ep nov before normalisation:  22.265308559745247
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.468]
 [0.47 ]
 [0.47 ]] [[38.243]
 [38.243]
 [38.243]
 [38.243]
 [37.226]
 [38.243]
 [38.243]] [[1.76 ]
 [1.76 ]
 [1.76 ]
 [1.76 ]
 [1.697]
 [1.76 ]
 [1.76 ]]
printing an ep nov before normalisation:  40.341756739686055
printing an ep nov before normalisation:  31.972603797912598
printing an ep nov before normalisation:  42.73350236847293
printing an ep nov before normalisation:  41.65500046273213
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.05541150638003254, 0.03688898341068859, 0.20915235775964083, 0.24322605564424582, 0.19780778007306946, 0.2575133167323228]
printing an ep nov before normalisation:  46.165841696254724
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.05541150638003254, 0.03688898341068859, 0.20915235775964083, 0.24322605564424582, 0.19780778007306946, 0.2575133167323228]
line 256 mcts: sample exp_bonus 39.78074751846733
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.05541150638003254, 0.03688898341068859, 0.20915235775964083, 0.24322605564424582, 0.19780778007306946, 0.2575133167323228]
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.05541150638003254, 0.03688898341068859, 0.20915235775964083, 0.24322605564424582, 0.19780778007306946, 0.2575133167323228]
printing an ep nov before normalisation:  31.809770396124687
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.05541150638003254, 0.03688898341068859, 0.20915235775964083, 0.24322605564424582, 0.19780778007306946, 0.2575133167323228]
printing an ep nov before normalisation:  30.63402222812856
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.05541150638003254, 0.03688898341068859, 0.20915235775964083, 0.24322605564424582, 0.19780778007306946, 0.2575133167323228]
actor:  1 policy actor:  1  step number:  75 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 42.86725544211783
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.054499288694659205, 0.03628242959807043, 0.20570306539436845, 0.23921446955301956, 0.21103478851126573, 0.25326595824861664]
printing an ep nov before normalisation:  49.62062504486542
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.054543306096260756, 0.03631169774666156, 0.2050605033908707, 0.23940804132746332, 0.21120554464865401, 0.25347090679008955]
printing an ep nov before normalisation:  43.88759935697775
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.054543306096260756, 0.03631169774666156, 0.2050605033908707, 0.23940804132746332, 0.21120554464865401, 0.25347090679008955]
actor:  1 policy actor:  1  step number:  52 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.002]
 [ 0.026]
 [-0.002]
 [-0.002]
 [ 0.023]
 [-0.002]
 [-0.002]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.002]
 [ 0.026]
 [-0.002]
 [-0.002]
 [ 0.023]
 [-0.002]
 [-0.002]]
printing an ep nov before normalisation:  62.04424571779285
printing an ep nov before normalisation:  19.5470165630269
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.05327586036830458, 0.03544043694385885, 0.1997376206823201, 0.23412336245766546, 0.23005296737773517, 0.24736975217011592]
printing an ep nov before normalisation:  33.16082939606963
actions average: 
K:  0  action  0 :  tensor([0.3769, 0.0029, 0.1040, 0.1289, 0.1174, 0.1282, 0.1416],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0038, 0.9401, 0.0058, 0.0105, 0.0058, 0.0068, 0.0272],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0911, 0.0616, 0.3373, 0.1007, 0.0816, 0.1844, 0.1433],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1255, 0.0066, 0.0900, 0.3782, 0.0886, 0.1825, 0.1286],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0551, 0.0057, 0.0238, 0.0475, 0.7793, 0.0448, 0.0439],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0797, 0.0579, 0.1022, 0.1177, 0.1196, 0.3851, 0.1378],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2106, 0.0417, 0.0898, 0.1353, 0.1225, 0.1438, 0.2564],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  32.21510114222567
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
printing an ep nov before normalisation:  39.397251176097
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.05327586036830458, 0.03544043694385885, 0.1997376206823201, 0.23412336245766546, 0.23005296737773517, 0.24736975217011592]
printing an ep nov before normalisation:  38.82838249206543
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.05327586036830458, 0.03544043694385885, 0.1997376206823201, 0.23412336245766546, 0.23005296737773517, 0.24736975217011592]
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]] [[33.789]
 [33.789]
 [33.789]
 [33.789]
 [33.789]
 [33.789]
 [33.789]] [[1.369]
 [1.369]
 [1.369]
 [1.369]
 [1.369]
 [1.369]
 [1.369]]
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.053286629326265884, 0.03541911383220506, 0.1992326613100789, 0.23445953835936179, 0.23038181923675763, 0.24722023793533066]
printing an ep nov before normalisation:  18.60914424582075
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.053286629326265884, 0.03541911383220506, 0.1992326613100789, 0.23445953835936179, 0.23038181923675763, 0.24722023793533066]
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.053286629326265884, 0.03541911383220506, 0.1992326613100789, 0.23445953835936179, 0.23038181923675763, 0.24722023793533066]
actions average: 
K:  1  action  0 :  tensor([0.3978, 0.0716, 0.0734, 0.0991, 0.1241, 0.0970, 0.1370],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0076, 0.8935, 0.0098, 0.0379, 0.0179, 0.0080, 0.0253],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1044, 0.0335, 0.5012, 0.0728, 0.0694, 0.1166, 0.1020],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1011, 0.0980, 0.0881, 0.2796, 0.1378, 0.1557, 0.1397],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0892, 0.0493, 0.0840, 0.1315, 0.3279, 0.1475, 0.1705],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1095, 0.0282, 0.0944, 0.1333, 0.1383, 0.3054, 0.1908],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1057, 0.0603, 0.1054, 0.1361, 0.1242, 0.1804, 0.2879],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.053286629326265884, 0.03541911383220506, 0.1992326613100789, 0.23445953835936179, 0.23038181923675763, 0.24722023793533066]
printing an ep nov before normalisation:  42.12891455078221
Printing some Q and Qe and total Qs values:  [[0.44]
 [0.44]
 [0.44]
 [0.44]
 [0.44]
 [0.44]
 [0.44]] [[35.849]
 [35.849]
 [35.849]
 [35.849]
 [35.849]
 [35.849]
 [35.849]] [[0.44]
 [0.44]
 [0.44]
 [0.44]
 [0.44]
 [0.44]
 [0.44]]
actor:  1 policy actor:  1  step number:  83 total reward:  0.05333333333333279  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.05291001601897794, 0.03516909908262612, 0.19782196146968317, 0.2398789816373355, 0.22875041504221266, 0.2454695267491646]
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.05291001601897794, 0.03516909908262612, 0.19782196146968317, 0.2398789816373355, 0.22875041504221266, 0.2454695267491646]
actions average: 
K:  2  action  0 :  tensor([0.5523, 0.0014, 0.0602, 0.0860, 0.0956, 0.1023, 0.1021],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0116, 0.8996, 0.0099, 0.0113, 0.0123, 0.0115, 0.0439],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1204, 0.0114, 0.2022, 0.1367, 0.1804, 0.1674, 0.1815],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0813, 0.0607, 0.0776, 0.3307, 0.1396, 0.1414, 0.1686],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1103, 0.0140, 0.0621, 0.1020, 0.4515, 0.1294, 0.1307],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0835, 0.0137, 0.0686, 0.0780, 0.0897, 0.5690, 0.0975],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1233, 0.0452, 0.0992, 0.1704, 0.1594, 0.1507, 0.2518],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
siam score:  -0.7952752
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
Printing some Q and Qe and total Qs values:  [[1.018]
 [1.018]
 [1.018]
 [1.018]
 [1.018]
 [1.018]
 [1.018]] [[50.364]
 [50.364]
 [50.364]
 [50.364]
 [50.364]
 [50.364]
 [50.364]] [[2.191]
 [2.191]
 [2.191]
 [2.191]
 [2.191]
 [2.191]
 [2.191]]
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.05291001601897794, 0.03516909908262612, 0.19782196146968317, 0.2398789816373355, 0.22875041504221266, 0.2454695267491646]
actor:  1 policy actor:  1  step number:  82 total reward:  0.04666666666666586  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  60.58827600735931
printing an ep nov before normalisation:  35.51096104548002
printing an ep nov before normalisation:  35.4409563791725
printing an ep nov before normalisation:  60.021711467218424
printing an ep nov before normalisation:  22.413493991279974
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.052359395829672625, 0.03480356986186223, 0.19500034964953294, 0.23737771441791092, 0.23754903686280637, 0.2429099333782149]
actor:  1 policy actor:  1  step number:  78 total reward:  0.01999999999999913  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.0521319456433087, 0.03470632280843718, 0.19371499998827166, 0.24222934620952677, 0.23594812401541174, 0.241269261335044]
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.0521319456433087, 0.03470632280843718, 0.19371499998827166, 0.24222934620952677, 0.23594812401541174, 0.241269261335044]
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
maxi score, test score, baseline:  -0.9649266666666666 -0.852 -0.852
probs:  [0.052196720941283435, 0.0347493908425487, 0.1939561466417124, 0.2425309276448146, 0.23549250334601235, 0.24107431058362846]
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.452]
 [0.438]
 [0.437]
 [0.439]
 [0.437]
 [0.436]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.438]
 [0.452]
 [0.438]
 [0.437]
 [0.439]
 [0.437]
 [0.436]]
printing an ep nov before normalisation:  49.25354437314378
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.671859053373424
actor:  0 policy actor:  0  step number:  83 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  1.333
from probs:  [0.05223572548209599, 0.034775324322544036, 0.19335277976012424, 0.24271252534901333, 0.23566882805591635, 0.24125481703030602]
printing an ep nov before normalisation:  51.35710931477287
maxi score, test score, baseline:  -0.9625 -0.852 -0.852
probs:  [0.05223572548209599, 0.034775324322544036, 0.19335277976012424, 0.24271252534901333, 0.23566882805591635, 0.24125481703030602]
printing an ep nov before normalisation:  44.52873674304834
printing an ep nov before normalisation:  16.742208967990784
printing an ep nov before normalisation:  32.015671730041504
maxi score, test score, baseline:  -0.9625 -0.852 -0.852
maxi score, test score, baseline:  -0.9625 -0.852 -0.852
probs:  [0.05223572548209599, 0.034775324322544036, 0.19335277976012424, 0.24271252534901333, 0.23566882805591635, 0.24125481703030602]
printing an ep nov before normalisation:  21.97838331201594
printing an ep nov before normalisation:  41.47937808672337
maxi score, test score, baseline:  -0.9625 -0.852 -0.852
probs:  [0.05231136920268409, 0.03477159243020803, 0.19335336239401657, 0.24268685321049782, 0.2356469025061876, 0.24122992025640574]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9625 -0.852 -0.852
maxi score, test score, baseline:  -0.9625 -0.852 -0.852
probs:  [0.05235013652569482, 0.034797328047609155, 0.19275453489527117, 0.24286706573518133, 0.23582188448738842, 0.24140905030885498]
maxi score, test score, baseline:  -0.9625 -0.852 -0.852
maxi score, test score, baseline:  -0.9625 -0.852 -0.852
probs:  [0.05235013652569482, 0.034797328047609155, 0.19275453489527117, 0.24286706573518133, 0.23582188448738842, 0.24140905030885498]
Printing some Q and Qe and total Qs values:  [[-0.068]
 [-0.078]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.019]] [[28.764]
 [17.039]
 [27.064]
 [27.064]
 [27.064]
 [27.064]
 [27.064]] [[1.058]
 [0.588]
 [1.039]
 [1.039]
 [1.039]
 [1.039]
 [1.039]]
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.548]
 [0.552]
 [0.543]
 [0.548]
 [0.555]
 [0.552]] [[36.618]
 [34.324]
 [36.618]
 [36.182]
 [36.685]
 [38.889]
 [39.816]] [[1.863]
 [1.734]
 [1.863]
 [1.83 ]
 [1.863]
 [1.99 ]
 [2.038]]
printing an ep nov before normalisation:  22.721169817499444
maxi score, test score, baseline:  -0.9625 -0.852 -0.852
probs:  [0.05235013652569482, 0.034797328047609155, 0.19275453489527117, 0.24286706573518133, 0.23582188448738842, 0.24140905030885498]
maxi score, test score, baseline:  -0.9625 -0.852 -0.852
probs:  [0.05235013652569482, 0.034797328047609155, 0.19275453489527117, 0.24286706573518133, 0.23582188448738842, 0.24140905030885498]
actions average: 
K:  3  action  0 :  tensor([0.4247, 0.0602, 0.0722, 0.0909, 0.1315, 0.0900, 0.1305],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0164, 0.8825, 0.0122, 0.0230, 0.0215, 0.0097, 0.0347],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0576, 0.2147, 0.2903, 0.1084, 0.0498, 0.1677, 0.1114],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0885, 0.1339, 0.0967, 0.2921, 0.1294, 0.1321, 0.1272],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0909, 0.0567, 0.0739, 0.1014, 0.4048, 0.1192, 0.1532],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1315, 0.0355, 0.1401, 0.1243, 0.0971, 0.3176, 0.1539],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1400, 0.0540, 0.1243, 0.1611, 0.1173, 0.1399, 0.2634],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  42.980298481358474
actor:  1 policy actor:  1  step number:  62 total reward:  0.24666666666666603  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.712]
 [0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]] [[12.504]
 [12.202]
 [12.202]
 [12.202]
 [12.202]
 [12.202]
 [12.202]] [[0.854]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]]
maxi score, test score, baseline:  -0.9625 -0.852 -0.852
probs:  [0.05257260342030501, 0.03543605516364949, 0.188928684097113, 0.23857147392494416, 0.24734314844425917, 0.23714803494972928]
maxi score, test score, baseline:  -0.9625 -0.852 -0.852
probs:  [0.05257260342030501, 0.03543605516364949, 0.188928684097113, 0.23857147392494416, 0.24734314844425917, 0.23714803494972928]
maxi score, test score, baseline:  -0.9625 -0.852 -0.852
probs:  [0.052610029631416956, 0.035461250945768366, 0.1883502943953261, 0.2387416483385779, 0.24751958324013282, 0.23731719344877794]
maxi score, test score, baseline:  -0.9625 -0.852 -0.852
probs:  [0.052610029631416956, 0.035461250945768366, 0.1883502943953261, 0.2387416483385779, 0.24751958324013282, 0.23731719344877794]
actions average: 
K:  0  action  0 :  tensor([0.4221, 0.0083, 0.0918, 0.0978, 0.1048, 0.1091, 0.1661],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0092, 0.9318, 0.0131, 0.0065, 0.0061, 0.0151, 0.0182],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1247, 0.0155, 0.1515, 0.1270, 0.1323, 0.2793, 0.1698],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1112, 0.0074, 0.1022, 0.2894, 0.1447, 0.1229, 0.2221],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1051, 0.0037, 0.0772, 0.0886, 0.4497, 0.1166, 0.1590],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0960, 0.0084, 0.1190, 0.0893, 0.1213, 0.3816, 0.1845],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2075, 0.0227, 0.0740, 0.1102, 0.0880, 0.0924, 0.4053],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  40.37236920035047
printing an ep nov before normalisation:  38.405585289001465
Printing some Q and Qe and total Qs values:  [[0.44 ]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]] [[29.052]
 [27.634]
 [27.634]
 [27.634]
 [27.634]
 [27.634]
 [27.634]] [[0.44 ]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]]
printing an ep nov before normalisation:  39.119350460245705
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.398]
 [0.567]
 [0.598]] [[40.76 ]
 [40.76 ]
 [40.76 ]
 [40.76 ]
 [39.996]
 [38.211]
 [44.396]] [[1.535]
 [1.535]
 [1.535]
 [1.535]
 [1.368]
 [1.462]
 [1.754]]
printing an ep nov before normalisation:  41.418602707141375
Printing some Q and Qe and total Qs values:  [[0.837]
 [0.837]
 [0.837]
 [0.837]
 [0.912]
 [0.837]
 [0.837]] [[33.063]
 [33.063]
 [33.063]
 [33.063]
 [33.528]
 [33.063]
 [33.063]] [[2.004]
 [2.004]
 [2.004]
 [2.004]
 [2.111]
 [2.004]
 [2.004]]
maxi score, test score, baseline:  -0.9625 -0.852 -0.852
maxi score, test score, baseline:  -0.9625 -0.852 -0.852
probs:  [0.05244231361380714, 0.03546750945041216, 0.18838363782221648, 0.23878391880087027, 0.2475634087496603, 0.23735921156303372]
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.52 ]
 [0.373]
 [0.373]
 [0.373]
 [0.373]
 [0.373]] [[43.099]
 [45.475]
 [43.099]
 [43.099]
 [43.099]
 [43.099]
 [43.099]] [[1.91 ]
 [2.218]
 [1.91 ]
 [1.91 ]
 [1.91 ]
 [1.91 ]
 [1.91 ]]
printing an ep nov before normalisation:  31.99747085571289
printing an ep nov before normalisation:  36.620337681893886
printing an ep nov before normalisation:  42.94848291916906
maxi score, test score, baseline:  -0.9625 -0.852 -0.852
probs:  [0.05246757682291956, 0.035484574504042445, 0.18847455528580986, 0.23889917759622978, 0.2476829076895871, 0.2369912081014112]
actor:  1 policy actor:  1  step number:  65 total reward:  0.2799999999999998  reward:  1.0 rdn_beta:  1.333
using another actor
from probs:  [0.05246757682291956, 0.035484574504042445, 0.18847455528580986, 0.23889917759622978, 0.2476829076895871, 0.2369912081014112]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9625 -0.852 -0.852
probs:  [0.052151993413116125, 0.035371169812730606, 0.18653984022335532, 0.24688947622030577, 0.24504333087252458, 0.2340041894579675]
maxi score, test score, baseline:  -0.9625 -0.852 -0.852
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9625 -0.852 -0.852
probs:  [0.052151993413116125, 0.035371169812730606, 0.18653984022335532, 0.24688947622030577, 0.24504333087252458, 0.2340041894579675]
maxi score, test score, baseline:  -0.9625 -0.852 -0.852
siam score:  -0.7907762
maxi score, test score, baseline:  -0.9625 -0.852 -0.852
probs:  [0.05222434653727315, 0.03536729939411575, 0.1865438613497165, 0.2468628114528767, 0.2450176048120012, 0.23398407645401662]
maxi score, test score, baseline:  -0.9625 -0.852 -0.852
probs:  [0.05222434653727315, 0.03536729939411575, 0.1865438613497165, 0.2468628114528767, 0.2450176048120012, 0.23398407645401662]
printing an ep nov before normalisation:  24.015049934387207
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]] [[26.695]
 [27.451]
 [27.451]
 [27.451]
 [27.451]
 [27.451]
 [27.451]] [[1.925]
 [1.926]
 [1.926]
 [1.926]
 [1.926]
 [1.926]
 [1.926]]
maxi score, test score, baseline:  -0.9625 -0.852 -0.852
probs:  [0.05222434653727315, 0.03536729939411575, 0.1865438613497165, 0.2468628114528767, 0.2450176048120012, 0.23398407645401662]
actor:  0 policy actor:  1  step number:  70 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  33.27789231886521
line 256 mcts: sample exp_bonus 40.411276473530634
printing an ep nov before normalisation:  36.68924247114471
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.05222434653727315, 0.03536729939411575, 0.1865438613497165, 0.2468628114528767, 0.2450176048120012, 0.23398407645401662]
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.05222434653727315, 0.03536729939411575, 0.1865438613497165, 0.2468628114528767, 0.2450176048120012, 0.23398407645401662]
printing an ep nov before normalisation:  22.953422256128082
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.05227353897008903, 0.03540057281496796, 0.18671989889040302, 0.24709581140697842, 0.2452488622386992, 0.23326131567886244]
using another actor
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.05231006656194687, 0.03542527982511224, 0.18615061132624044, 0.24726882434289438, 0.24542058126971647, 0.2334246366740896]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.05231006656194687, 0.03542527982511224, 0.18615061132624044, 0.24726882434289438, 0.24542058126971647, 0.2334246366740896]
printing an ep nov before normalisation:  43.82614388557094
siam score:  -0.7910794
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.05231006656194687, 0.03542527982511224, 0.18615061132624044, 0.24726882434289438, 0.24542058126971647, 0.2334246366740896]
actions average: 
K:  1  action  0 :  tensor([0.5748, 0.0121, 0.0693, 0.0643, 0.0765, 0.0898, 0.1133],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0069, 0.9399, 0.0069, 0.0100, 0.0094, 0.0074, 0.0194],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1418, 0.0279, 0.2164, 0.0993, 0.1508, 0.1563, 0.2075],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0804, 0.0511, 0.0821, 0.3004, 0.1432, 0.1233, 0.2196],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1438, 0.0340, 0.0652, 0.0978, 0.4505, 0.0988, 0.1098],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0744, 0.0038, 0.0769, 0.0757, 0.1122, 0.5499, 0.1072],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1577, 0.0174, 0.0736, 0.1336, 0.1351, 0.1284, 0.3543],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.05231006656194687, 0.03542527982511224, 0.18615061132624044, 0.24726882434289438, 0.24542058126971647, 0.2334246366740896]
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.05231006656194687, 0.03542527982511224, 0.18615061132624044, 0.24726882434289438, 0.24542058126971647, 0.2334246366740896]
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.05235021529214684, 0.035452436148151294, 0.18629374689775016, 0.24745898880027842, 0.24484046447493416, 0.2336041483867391]
printing an ep nov before normalisation:  47.70394348631035
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.05235021529214684, 0.035452436148151294, 0.18629374689775016, 0.24745898880027842, 0.24484046447493416, 0.2336041483867391]
printing an ep nov before normalisation:  28.563596242492306
printing an ep nov before normalisation:  33.01607500009544
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.611]
 [0.611]
 [0.611]
 [0.507]
 [0.611]
 [0.611]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.501]
 [0.611]
 [0.611]
 [0.611]
 [0.507]
 [0.611]
 [0.611]]
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.052426538284299634, 0.03550406049143403, 0.18587012898419938, 0.2478204926465116, 0.24443337829626002, 0.23394540129729546]
using explorer policy with actor:  1
printing an ep nov before normalisation:  29.398637131005284
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.052426538284299634, 0.03550406049143403, 0.18587012898419938, 0.2478204926465116, 0.24443337829626002, 0.23394540129729546]
printing an ep nov before normalisation:  39.28659541064869
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.05245115108268985, 0.0355207084177981, 0.18595754943615217, 0.24793707116412014, 0.24454836260688145, 0.23358515729235838]
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.05245115108268985, 0.0355207084177981, 0.18595754943615217, 0.24793707116412014, 0.24454836260688145, 0.23358515729235838]
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.05245115108268985, 0.0355207084177981, 0.18595754943615217, 0.24793707116412014, 0.24454836260688145, 0.23358515729235838]
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.73288378106807
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.05251553780588425, 0.035564259151427406, 0.18618624006905676, 0.24824203887338567, 0.24408827177418443, 0.23340365232606153]
printing an ep nov before normalisation:  32.11558469754037
printing an ep nov before normalisation:  53.558133593845376
printing an ep nov before normalisation:  35.29372302332504
Printing some Q and Qe and total Qs values:  [[0.44 ]
 [0.526]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]] [[19.958]
 [30.104]
 [19.958]
 [19.958]
 [19.958]
 [19.958]
 [19.958]] [[1.03 ]
 [1.701]
 [1.03 ]
 [1.03 ]
 [1.03 ]
 [1.03 ]
 [1.03 ]]
actions average: 
K:  2  action  0 :  tensor([0.4880, 0.0336, 0.0690, 0.0782, 0.1456, 0.0732, 0.1124],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0094, 0.9332, 0.0058, 0.0088, 0.0184, 0.0048, 0.0196],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1115, 0.0075, 0.3033, 0.1055, 0.1465, 0.1775, 0.1482],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1211, 0.0390, 0.0808, 0.2468, 0.1815, 0.1587, 0.1720],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1031, 0.0244, 0.0553, 0.1124, 0.4971, 0.1066, 0.1011],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0756, 0.0289, 0.0866, 0.1182, 0.2110, 0.3447, 0.1351],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0844, 0.1232, 0.0632, 0.1352, 0.0873, 0.0899, 0.4169],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  42.10199405857057
printing an ep nov before normalisation:  35.1258068970154
printing an ep nov before normalisation:  36.63321611802615
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.05259149958727132, 0.03561563917424993, 0.18576392092657687, 0.2486018318448558, 0.24368518093481276, 0.23374192753223333]
printing an ep nov before normalisation:  31.460467709459383
printing an ep nov before normalisation:  39.58116639584107
actor:  1 policy actor:  1  step number:  74 total reward:  0.0066666666666659324  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.446]
 [0.437]
 [0.435]
 [0.435]
 [0.435]
 [0.437]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.434]
 [0.446]
 [0.437]
 [0.435]
 [0.435]
 [0.435]
 [0.437]]
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.052307682250238796, 0.03542366709318653, 0.18475959488905142, 0.24725753105067957, 0.24161976764890608, 0.23863175706793754]
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.052307682250238796, 0.03542366709318653, 0.18475959488905142, 0.24725753105067957, 0.24161976764890608, 0.23863175706793754]
printing an ep nov before normalisation:  31.66111417271992
printing an ep nov before normalisation:  39.555316388168606
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.052307682250238796, 0.03542366709318653, 0.18475959488905142, 0.24725753105067957, 0.24161976764890608, 0.23863175706793754]
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.05234335402928057, 0.03544779523746933, 0.1842026688388753, 0.2474264904292606, 0.24178487247743122, 0.23879481898768287]
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.05234335402928057, 0.03544779523746933, 0.1842026688388753, 0.2474264904292606, 0.24178487247743122, 0.23879481898768287]
line 256 mcts: sample exp_bonus 35.51259423481294
printing an ep nov before normalisation:  48.86347330913088
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.05234335402928057, 0.03544779523746933, 0.1842026688388753, 0.2474264904292606, 0.24178487247743122, 0.23879481898768287]
actor:  1 policy actor:  1  step number:  73 total reward:  0.013333333333332753  reward:  1.0 rdn_beta:  1.333
siam score:  -0.8030821
printing an ep nov before normalisation:  20.877674237957244
Printing some Q and Qe and total Qs values:  [[0.946]
 [0.926]
 [0.946]
 [0.946]
 [0.946]
 [0.946]
 [0.946]] [[29.24 ]
 [29.498]
 [29.24 ]
 [29.24 ]
 [29.24 ]
 [29.24 ]
 [29.24 ]] [[0.946]
 [0.926]
 [0.946]
 [0.946]
 [0.946]
 [0.946]
 [0.946]]
printing an ep nov before normalisation:  34.30270398299649
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.05272114319862588, 0.036031331676243596, 0.1829747286976283, 0.2515149990758598, 0.23985571959073315, 0.23690207776090932]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  33.90185274424554
printing an ep nov before normalisation:  30.313629483658104
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.05272114319862588, 0.036031331676243596, 0.1829747286976283, 0.2515149990758598, 0.23985571959073315, 0.23690207776090932]
printing an ep nov before normalisation:  36.14876985549927
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.05272114319862588, 0.036031331676243596, 0.1829747286976283, 0.2515149990758598, 0.23985571959073315, 0.23690207776090932]
actions average: 
K:  4  action  0 :  tensor([0.3661, 0.0424, 0.0921, 0.1147, 0.1235, 0.1133, 0.1479],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0132, 0.8829, 0.0075, 0.0135, 0.0528, 0.0064, 0.0237],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1242, 0.1995, 0.1004, 0.1173, 0.1159, 0.1491, 0.1936],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0642, 0.1700, 0.0880, 0.2792, 0.1098, 0.1169, 0.1720],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1108, 0.0542, 0.0675, 0.0934, 0.4366, 0.1279, 0.1096],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0565, 0.0626, 0.1076, 0.0671, 0.1008, 0.5216, 0.0838],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0950, 0.1510, 0.0913, 0.1263, 0.1304, 0.1244, 0.2815],
       grad_fn=<DivBackward0>)
using another actor
using explorer policy with actor:  0
printing an ep nov before normalisation:  34.88760384776418
line 256 mcts: sample exp_bonus 30.654104251076348
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.05275639111463238, 0.03605539297154986, 0.18242755055756243, 0.2516834918436883, 0.2400163975347056, 0.23706077597786146]
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.05259394464881387, 0.03606155773787936, 0.1824588349155988, 0.25172666152880513, 0.2400575649767027, 0.2371014361922002]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  54.3110381519766
printing an ep nov before normalisation:  13.81977283869169
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.05259394464881387, 0.03606155773787936, 0.1824588349155988, 0.25172666152880513, 0.2400575649767027, 0.2371014361922002]
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
printing an ep nov before normalisation:  38.15793961528177
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.05259394464881387, 0.03606155773787936, 0.1824588349155988, 0.25172666152880513, 0.2400575649767027, 0.2371014361922002]
actor:  1 policy actor:  1  step number:  48 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  0.333
siam score:  -0.80141467
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.25751649989734
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.08514539925315064, 0.034826245474789595, 0.17618999209826974, 0.24307620527982693, 0.23180832352603026, 0.22895383436793276]
printing an ep nov before normalisation:  40.40023475076299
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
siam score:  -0.8072667
printing an ep nov before normalisation:  29.093996048259076
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.08520001433959248, 0.03484853360319203, 0.17566117284194852, 0.24323228118498688, 0.23195716051095217, 0.2291008375193279]
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.08520001433959248, 0.03484853360319203, 0.17566117284194852, 0.24323228118498688, 0.23195716051095217, 0.2291008375193279]
printing an ep nov before normalisation:  0.00012475705204906262
actor:  1 policy actor:  1  step number:  71 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  28.51970763675226
printing an ep nov before normalisation:  33.54197544124773
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.08431407272952586, 0.034486985510006216, 0.1738331090866265, 0.24070048735806995, 0.22954279322436807, 0.2371225520914035]
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.08431407272952586, 0.034486985510006216, 0.1738331090866265, 0.24070048735806995, 0.22954279322436807, 0.2371225520914035]
printing an ep nov before normalisation:  29.721572054235352
printing an ep nov before normalisation:  28.102603991280787
Printing some Q and Qe and total Qs values:  [[0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]] [[12.521]
 [12.521]
 [12.521]
 [12.521]
 [12.521]
 [12.521]
 [12.521]] [[1.696]
 [1.696]
 [1.696]
 [1.696]
 [1.696]
 [1.696]
 [1.696]]
actor:  1 policy actor:  1  step number:  58 total reward:  0.21999999999999964  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  15.702218334886231
actor:  1 policy actor:  1  step number:  68 total reward:  0.27333333333333254  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  42.155130998721994
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.08135039890624707, 0.03327752587129411, 0.20335221764862121, 0.23223106792284512, 0.221466191961118, 0.22832259768987465]
printing an ep nov before normalisation:  28.46164945988676
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.08105094638866571, 0.03328833776951226, 0.20341850908044953, 0.23230677983415618, 0.22153839229588884, 0.22839703463132757]
Starting evaluation
printing an ep nov before normalisation:  41.01958955758106
printing an ep nov before normalisation:  38.531884037693146
printing an ep nov before normalisation:  43.88750575326477
printing an ep nov before normalisation:  43.39510180831385
printing an ep nov before normalisation:  68.30455261071342
printing an ep nov before normalisation:  38.8106939565025
Printing some Q and Qe and total Qs values:  [[0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.272]] [[37.483]
 [37.483]
 [37.483]
 [37.483]
 [37.483]
 [37.483]
 [38.684]] [[0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.272]]
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.0811030893165632, 0.033285795356955895, 0.2034143842797643, 0.2322893714692466, 0.22152593551041708, 0.2283814240670529]
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.0811030893165632, 0.033285795356955895, 0.2034143842797643, 0.2322893714692466, 0.22152593551041708, 0.2283814240670529]
printing an ep nov before normalisation:  41.542303217330826
Printing some Q and Qe and total Qs values:  [[0.331]
 [0.161]
 [0.322]
 [0.317]
 [0.318]
 [0.32 ]
 [0.318]] [[24.183]
 [28.906]
 [24.057]
 [24.072]
 [24.354]
 [25.042]
 [24.503]] [[0.331]
 [0.161]
 [0.322]
 [0.317]
 [0.318]
 [0.32 ]
 [0.318]]
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.349]
 [0.34 ]
 [0.349]
 [0.349]
 [0.334]
 [0.331]] [[24.865]
 [24.175]
 [24.213]
 [24.175]
 [24.175]
 [24.669]
 [24.617]] [[0.354]
 [0.349]
 [0.34 ]
 [0.349]
 [0.349]
 [0.334]
 [0.331]]
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.319]
 [0.5  ]
 [0.517]
 [0.494]
 [0.473]
 [0.488]] [[20.477]
 [25.285]
 [22.801]
 [22.904]
 [23.716]
 [25.064]
 [25.348]] [[0.574]
 [0.319]
 [0.5  ]
 [0.517]
 [0.494]
 [0.473]
 [0.488]]
printing an ep nov before normalisation:  14.267247915267944
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
printing an ep nov before normalisation:  16.767484662028806
printing an ep nov before normalisation:  17.965263846191814
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.0811030893165632, 0.033285795356955895, 0.2034143842797643, 0.2322893714692466, 0.22152593551041708, 0.2283814240670529]
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.0811030893165632, 0.033285795356955895, 0.2034143842797643, 0.2322893714692466, 0.22152593551041708, 0.2283814240670529]
printing an ep nov before normalisation:  42.53912999167956
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]] [[17.054]
 [17.054]
 [17.054]
 [17.054]
 [17.054]
 [17.054]
 [17.054]] [[0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  39.836934722273156
printing an ep nov before normalisation:  31.925076910354917
printing an ep nov before normalisation:  11.455387027955526
Printing some Q and Qe and total Qs values:  [[0.913]
 [0.892]
 [0.913]
 [0.913]
 [0.913]
 [0.913]
 [0.913]] [[27.68 ]
 [29.367]
 [27.68 ]
 [27.68 ]
 [27.68 ]
 [27.68 ]
 [27.68 ]] [[0.913]
 [0.892]
 [0.913]
 [0.913]
 [0.913]
 [0.913]
 [0.913]]
line 256 mcts: sample exp_bonus 29.40938949584961
Printing some Q and Qe and total Qs values:  [[0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.537]] [[13.909]
 [13.909]
 [13.909]
 [13.909]
 [13.909]
 [13.909]
 [13.909]] [[0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.537]]
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
using explorer policy with actor:  0
printing an ep nov before normalisation:  14.934527118752756
line 256 mcts: sample exp_bonus 12.719605922698975
printing an ep nov before normalisation:  17.17523088214309
maxi score, test score, baseline:  -0.9598200000000001 -0.852 -0.852
probs:  [0.0811030893165632, 0.033285795356955895, 0.2034143842797643, 0.2322893714692466, 0.22152593551041708, 0.2283814240670529]
printing an ep nov before normalisation:  11.115756352067292
printing an ep nov before normalisation:  14.651559218994867
printing an ep nov before normalisation:  15.498288627097613
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.349]
 [0.371]
 [0.368]
 [0.383]
 [0.364]
 [0.361]] [[10.077]
 [13.572]
 [10.908]
 [11.218]
 [ 9.962]
 [11.988]
 [12.147]] [[0.422]
 [0.349]
 [0.371]
 [0.368]
 [0.383]
 [0.364]
 [0.361]]
printing an ep nov before normalisation:  23.840153581793235
printing an ep nov before normalisation:  35.4713689820792
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.119]
 [0.381]
 [0.386]
 [0.324]
 [0.327]
 [0.322]] [[13.245]
 [20.866]
 [15.389]
 [15.347]
 [17.898]
 [17.685]
 [18.66 ]] [[0.455]
 [0.119]
 [0.381]
 [0.386]
 [0.324]
 [0.327]
 [0.322]]
printing an ep nov before normalisation:  17.93777844572411
printing an ep nov before normalisation:  0.040805696354055954
actor:  0 policy actor:  1  step number:  46 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  2
actions average: 
K:  3  action  0 :  tensor([0.4090, 0.0201, 0.0820, 0.1603, 0.1152, 0.1030, 0.1104],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0109, 0.8665, 0.0135, 0.0198, 0.0181, 0.0163, 0.0548],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1643, 0.0195, 0.1391, 0.1577, 0.1850, 0.1810, 0.1533],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0878, 0.0087, 0.0707, 0.3840, 0.1984, 0.1057, 0.1448],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1156, 0.0257, 0.0854, 0.2024, 0.3366, 0.1042, 0.1301],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1224, 0.0432, 0.0859, 0.1100, 0.1093, 0.4284, 0.1007],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  printing an ep nov before normalisation:  35.33217510494081
tensor([0.1060, 0.1526, 0.1090, 0.1224, 0.1522, 0.1536, 0.2043],
       grad_fn=<DivBackward0>)
siam score:  -0.810152
maxi score, test score, baseline:  -0.9567933333333333 -0.852 -0.852
probs:  [0.0811030893165632, 0.033285795356955895, 0.2034143842797643, 0.2322893714692466, 0.22152593551041708, 0.2283814240670529]
printing an ep nov before normalisation:  12.160897715618688
printing an ep nov before normalisation:  15.048349937593786
printing an ep nov before normalisation:  14.59766286199276
printing an ep nov before normalisation:  12.450983818626753
actor:  0 policy actor:  1  step number:  52 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  52 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  53 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  14.405712386315628
maxi score, test score, baseline:  -0.9479933333333332 -0.852 -0.852
probs:  [0.0811030893165632, 0.033285795356955895, 0.2034143842797643, 0.2322893714692466, 0.22152593551041708, 0.2283814240670529]
actor:  0 policy actor:  1  step number:  55 total reward:  0.3999999999999999  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.9451933333333333 -0.852 -0.852
probs:  [0.0811030893165632, 0.033285795356955895, 0.2034143842797643, 0.2322893714692466, 0.22152593551041708, 0.2283814240670529]
printing an ep nov before normalisation:  33.50499285591898
actor:  0 policy actor:  1  step number:  59 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  2
actor:  1 policy actor:  1  step number:  70 total reward:  0.09999999999999953  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  9.169340598536664
printing an ep nov before normalisation:  13.744002395884003
printing an ep nov before normalisation:  12.76470348679716
printing an ep nov before normalisation:  37.9145670665412
printing an ep nov before normalisation:  12.761846640268999
printing an ep nov before normalisation:  17.025059942770486
maxi score, test score, baseline:  -0.9424733333333333 -0.852 -0.852
probs:  [0.09672669914938083, 0.03272170753736018, 0.19995557460860683, 0.22833925662175053, 0.21775895950786867, 0.22449780257503285]
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.536]
 [0.544]
 [0.548]
 [0.54 ]
 [0.559]
 [0.556]] [[10.197]
 [12.765]
 [10.765]
 [11.207]
 [11.6  ]
 [13.177]
 [13.61 ]] [[0.606]
 [0.536]
 [0.544]
 [0.548]
 [0.54 ]
 [0.559]
 [0.556]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  25.09703028343744
printing an ep nov before normalisation:  16.488947290698945
actions average: 
K:  1  action  0 :  tensor([0.4121, 0.0105, 0.0860, 0.1067, 0.0975, 0.1088, 0.1784],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0048, 0.9557, 0.0083, 0.0068, 0.0049, 0.0058, 0.0138],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1301, 0.0288, 0.1243, 0.1910, 0.1683, 0.1848, 0.1728],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1145, 0.0109, 0.0847, 0.3691, 0.1284, 0.1295, 0.1630],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1215, 0.0575, 0.0417, 0.0520, 0.5947, 0.0533, 0.0793],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0900, 0.0121, 0.1078, 0.1140, 0.1141, 0.4227, 0.1393],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1191, 0.0488, 0.0873, 0.1189, 0.1108, 0.0961, 0.4190],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.218]
 [0.304]
 [0.303]
 [0.304]
 [0.304]
 [0.303]] [[13.056]
 [18.02 ]
 [17.109]
 [15.634]
 [16.986]
 [17.494]
 [17.904]] [[0.425]
 [0.218]
 [0.304]
 [0.303]
 [0.304]
 [0.304]
 [0.303]]
printing an ep nov before normalisation:  42.92951648237124
maxi score, test score, baseline:  -0.9424733333333333 -0.852 -0.852
probs:  [0.09672669914938083, 0.03272170753736018, 0.19995557460860683, 0.22833925662175053, 0.21775895950786867, 0.22449780257503285]
printing an ep nov before normalisation:  14.259092807769775
printing an ep nov before normalisation:  14.079530239105225
printing an ep nov before normalisation:  35.58551098329359
printing an ep nov before normalisation:  20.367521803759047
printing an ep nov before normalisation:  29.02799367904663
printing an ep nov before normalisation:  13.35584524129743
printing an ep nov before normalisation:  18.730339115904773
using explorer policy with actor:  0
printing an ep nov before normalisation:  20.367521803759047
printing an ep nov before normalisation:  10.149051433884893
printing an ep nov before normalisation:  14.079530239105225
printing an ep nov before normalisation:  15.54198659766465
printing an ep nov before normalisation:  15.631548218144768
printing an ep nov before normalisation:  16.913536057772294
printing an ep nov before normalisation:  10.07480913465022
printing an ep nov before normalisation:  17.666899541671796
Printing some Q and Qe and total Qs values:  [[0.585]
 [0.601]
 [0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.585]] [[11.675]
 [15.922]
 [11.675]
 [11.675]
 [11.675]
 [11.675]
 [11.675]] [[0.585]
 [0.601]
 [0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.585]]
printing an ep nov before normalisation:  11.727792024612427
maxi score, test score, baseline:  -0.9424733333333333 -0.852 -0.852
probs:  [0.09672669914938083, 0.03272170753736018, 0.19995557460860683, 0.22833925662175053, 0.21775895950786867, 0.22449780257503285]
printing an ep nov before normalisation:  12.611122480828936
Printing some Q and Qe and total Qs values:  [[0.904]
 [0.871]
 [0.89 ]
 [0.904]
 [0.879]
 [0.873]
 [0.882]] [[29.104]
 [30.765]
 [31.876]
 [29.104]
 [31.862]
 [33.405]
 [31.936]] [[0.904]
 [0.871]
 [0.89 ]
 [0.904]
 [0.879]
 [0.873]
 [0.882]]
printing an ep nov before normalisation:  8.926401980621819
Printing some Q and Qe and total Qs values:  [[ 0.697]
 [ 0.447]
 [ 0.507]
 [ 0.352]
 [ 0.13 ]
 [-0.01 ]
 [ 0.176]] [[12.868]
 [15.745]
 [16.34 ]
 [14.8  ]
 [18.816]
 [17.656]
 [17.651]] [[ 0.697]
 [ 0.447]
 [ 0.507]
 [ 0.352]
 [ 0.13 ]
 [-0.01 ]
 [ 0.176]]
maxi score, test score, baseline:  -0.9424733333333333 -0.852 -0.852
printing an ep nov before normalisation:  14.809867511715744
printing an ep nov before normalisation:  11.942669438006046
using explorer policy with actor:  0
printing an ep nov before normalisation:  12.813831604847046
printing an ep nov before normalisation:  11.780598057487808
printing an ep nov before normalisation:  18.13885216802671
maxi score, test score, baseline:  -0.9424733333333333 -0.852 -0.852
maxi score, test score, baseline:  -0.9424733333333333 -0.852 -0.852
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.582]] [[14.861]
 [14.861]
 [14.861]
 [14.861]
 [14.861]
 [14.861]
 [17.026]] [[0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.582]]
maxi score, test score, baseline:  -0.9424733333333333 -0.852 -0.852
probs:  [0.09672669914938083, 0.03272170753736018, 0.19995557460860683, 0.22833925662175053, 0.21775895950786867, 0.22449780257503285]
line 256 mcts: sample exp_bonus 15.276414536764271
maxi score, test score, baseline:  -0.9424733333333333 -0.852 -0.852
probs:  [0.09672669914938083, 0.03272170753736018, 0.19995557460860683, 0.22833925662175053, 0.21775895950786867, 0.22449780257503285]
printing an ep nov before normalisation:  17.873798608779907
printing an ep nov before normalisation:  12.225947943870459
maxi score, test score, baseline:  -0.9424733333333333 -0.852 -0.852
probs:  [0.09672669914938083, 0.03272170753736018, 0.19995557460860683, 0.22833925662175053, 0.21775895950786867, 0.22449780257503285]
printing an ep nov before normalisation:  17.403371334075928
maxi score, test score, baseline:  -0.9424733333333333 -0.852 -0.852
probs:  [0.09636256836888732, 0.03273485441828041, 0.2000361871609308, 0.22843131975042882, 0.2178467543213968, 0.22458831598007584]
printing an ep nov before normalisation:  22.53208437444301
printing an ep nov before normalisation:  13.732092380523682
printing an ep nov before normalisation:  9.48781759724639
printing an ep nov before normalisation:  12.437524621706242
maxi score, test score, baseline:  -0.9424733333333333 -0.852 -0.852
probs:  [0.09636256836888732, 0.03273485441828041, 0.2000361871609308, 0.22843131975042882, 0.2178467543213968, 0.22458831598007584]
printing an ep nov before normalisation:  16.45158290863037
printing an ep nov before normalisation:  17.761201858520508
maxi score, test score, baseline:  -0.9424733333333333 -0.852 -0.852
probs:  [0.09636256836888732, 0.03273485441828041, 0.2000361871609308, 0.22843131975042882, 0.2178467543213968, 0.22458831598007584]
printing an ep nov before normalisation:  15.386133245972573
maxi score, test score, baseline:  -0.9424733333333333 -0.852 -0.852
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
Printing some Q and Qe and total Qs values:  [[0.659]
 [0.599]
 [0.599]
 [0.599]
 [0.599]
 [0.591]
 [0.588]] [[ 9.916]
 [ 9.152]
 [ 9.152]
 [ 9.152]
 [ 9.152]
 [13.392]
 [13.676]] [[0.659]
 [0.599]
 [0.599]
 [0.599]
 [0.599]
 [0.591]
 [0.588]]
maxi score, test score, baseline:  -0.9424733333333333 -0.852 -0.852
probs:  [0.09636256836888732, 0.03273485441828041, 0.2000361871609308, 0.22843131975042882, 0.2178467543213968, 0.22458831598007584]
printing an ep nov before normalisation:  9.947621882705135
printing an ep nov before normalisation:  14.972634851585864
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.603]] [[9.287]
 [9.175]
 [9.175]
 [9.175]
 [9.175]
 [9.175]
 [9.175]] [[0.628]
 [0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.603]]
printing an ep nov before normalisation:  16.614255952891234
maxi score, test score, baseline:  -0.9424733333333333 -0.852 -0.852
probs:  [0.09636256836888732, 0.03273485441828041, 0.2000361871609308, 0.22843131975042882, 0.2178467543213968, 0.22458831598007584]
Printing some Q and Qe and total Qs values:  [[0.232]
 [0.272]
 [0.246]
 [0.248]
 [0.246]
 [0.242]
 [0.246]] [[39.573]
 [40.786]
 [44.943]
 [44.032]
 [43.306]
 [42.52 ]
 [43.906]] [[0.232]
 [0.272]
 [0.246]
 [0.248]
 [0.246]
 [0.242]
 [0.246]]
printing an ep nov before normalisation:  43.0643480718513
maxi score, test score, baseline:  -0.9424733333333333 -0.852 -0.852
probs:  [0.09636256836888732, 0.03273485441828041, 0.2000361871609308, 0.22843131975042882, 0.2178467543213968, 0.22458831598007584]
printing an ep nov before normalisation:  38.53841554067292
maxi score, test score, baseline:  -0.9424733333333333 -0.5663333333333334 -0.5663333333333334
probs:  [0.09788861257268738, 0.032085072213166074, 0.204638780779434, 0.2238887894315753, 0.22064708185493312, 0.2208516631482041]
maxi score, test score, baseline:  -0.9424733333333333 -0.5663333333333334 -0.5663333333333334
probs:  [0.09788861257268738, 0.032085072213166074, 0.204638780779434, 0.2238887894315753, 0.22064708185493312, 0.2208516631482041]
printing an ep nov before normalisation:  21.34704398886373
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
printing an ep nov before normalisation:  19.38654679852106
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.468]
 [0.445]
 [0.445]
 [0.439]
 [0.445]
 [0.441]] [[39.974]
 [42.501]
 [39.974]
 [39.974]
 [40.855]
 [39.974]
 [40.878]] [[1.042]
 [1.138]
 [1.042]
 [1.042]
 [1.061]
 [1.042]
 [1.064]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  63 total reward:  0.3066666666666664  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9424733333333333 -0.5663333333333334 -0.5663333333333334
probs:  [0.09736436836017613, 0.032811746376430036, 0.2020852271058006, 0.22096929511971433, 0.21778921206288304, 0.2289801509749959]
printing an ep nov before normalisation:  45.22029473330457
actor:  1 policy actor:  1  step number:  72 total reward:  0.11333333333333306  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.024]
 [0.022]
 [0.025]
 [0.027]
 [0.029]
 [0.019]] [[36.938]
 [32.008]
 [37.214]
 [36.782]
 [36.311]
 [37.639]
 [37.95 ]] [[0.821]
 [0.622]
 [0.83 ]
 [0.815]
 [0.799]
 [0.854]
 [0.856]]
using another actor
maxi score, test score, baseline:  -0.9424733333333333 -0.5663333333333334 -0.5663333333333334
probs:  [0.09632975636330358, 0.03246402251055419, 0.21121939871411663, 0.2186194343457244, 0.2148223393572287, 0.22654504870907247]
actions average: 
K:  1  action  0 :  tensor([0.5790, 0.0214, 0.0856, 0.0649, 0.0701, 0.0861, 0.0929],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0044, 0.9615, 0.0045, 0.0068, 0.0042, 0.0032, 0.0154],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1132, 0.0351, 0.2496, 0.1178, 0.1554, 0.1795, 0.1494],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0902, 0.0391, 0.0910, 0.1618, 0.1094, 0.1225, 0.3859],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0830, 0.0109, 0.0874, 0.0818, 0.4886, 0.1305, 0.1179],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0927, 0.0143, 0.1055, 0.0850, 0.1255, 0.4233, 0.1538],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0928, 0.2157, 0.1011, 0.1338, 0.0951, 0.1068, 0.2546],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9424733333333333 -0.5663333333333334 -0.5663333333333334
probs:  [0.09600236809505146, 0.032385889654369594, 0.21152909271918122, 0.2189451834348547, 0.21513985028612836, 0.22599761581041466]
Printing some Q and Qe and total Qs values:  [[ 0.036]
 [-0.001]
 [ 0.067]
 [-0.007]
 [ 0.067]
 [ 0.067]
 [ 0.067]] [[52.25 ]
 [53.827]
 [ 0.   ]
 [50.493]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[1.225]
 [1.224]
 [0.066]
 [1.143]
 [0.066]
 [0.066]
 [0.066]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.9424733333333333 -0.5663333333333334 -0.5663333333333334
probs:  [0.09601356028473657, 0.03234073301792703, 0.21164261366830903, 0.2190652732481593, 0.21525656949387265, 0.22568125028699557]
printing an ep nov before normalisation:  45.70342537315783
printing an ep nov before normalisation:  41.13934908971845
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.025]
 [0.025]
 [0.025]
 [0.025]
 [0.025]
 [0.025]] [[13.801]
 [13.801]
 [13.801]
 [13.801]
 [13.801]
 [13.801]
 [13.801]] [[0.025]
 [0.025]
 [0.025]
 [0.025]
 [0.025]
 [0.025]
 [0.025]]
maxi score, test score, baseline:  -0.9424733333333333 -0.5663333333333334 -0.5663333333333334
probs:  [0.09601356028473657, 0.03234073301792703, 0.21164261366830903, 0.2190652732481593, 0.21525656949387265, 0.22568125028699557]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
printing an ep nov before normalisation:  38.45311204565784
actions average: 
K:  4  action  0 :  tensor([0.7706, 0.0295, 0.0366, 0.0265, 0.0669, 0.0278, 0.0420],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0083, 0.8482, 0.0310, 0.0220, 0.0142, 0.0214, 0.0549],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1338, 0.0025, 0.1223, 0.1712, 0.1281, 0.2457, 0.1964],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0811, 0.1182, 0.0878, 0.2621, 0.1361, 0.1324, 0.1822],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1187, 0.0313, 0.0635, 0.0918, 0.4418, 0.1197, 0.1333],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0619, 0.0968, 0.0841, 0.0935, 0.1356, 0.4167, 0.1114],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1765, 0.1244, 0.0860, 0.1033, 0.1034, 0.1040, 0.3023],
       grad_fn=<DivBackward0>)
siam score:  -0.81302935
actor:  1 policy actor:  1  step number:  68 total reward:  0.24666666666666626  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  77 total reward:  0.09333333333333282  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9424733333333333 -0.5663333333333334 -0.5663333333333334
probs:  [0.09474160551374036, 0.03325045934776848, 0.2064087534612998, 0.2135770835756867, 0.23205572684080472, 0.2199663712606998]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.474506119928066
printing an ep nov before normalisation:  35.81092884714203
maxi score, test score, baseline:  -0.9424733333333333 -0.5663333333333334 -0.5663333333333334
probs:  [0.09474160551374036, 0.03325045934776848, 0.2064087534612998, 0.2135770835756867, 0.23205572684080472, 0.2199663712606998]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[ 0.55 ]
 [-0.138]
 [-0.138]
 [-0.138]
 [ 0.55 ]
 [ 0.55 ]
 [ 0.55 ]] [[ 0.   ]
 [25.787]
 [25.787]
 [25.787]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.55 ]
 [0.713]
 [0.713]
 [0.713]
 [0.55 ]
 [0.55 ]
 [0.55 ]]
siam score:  -0.8112448
maxi score, test score, baseline:  -0.9424733333333333 -0.5663333333333334 -0.5663333333333334
probs:  [0.09481177328549104, 0.03327502125706457, 0.2058206682509092, 0.21373538752321442, 0.23222773576076575, 0.22012941392255497]
printing an ep nov before normalisation:  48.75259371595859
maxi score, test score, baseline:  -0.9424733333333333 -0.5663333333333334 -0.5663333333333334
probs:  [0.09488148262716249, 0.03329942269499693, 0.20523642520256608, 0.21389265721684764, 0.2323986208875546, 0.22029139137087228]
printing an ep nov before normalisation:  36.228385562817024
actor:  0 policy actor:  1  step number:  71 total reward:  0.07999999999999952  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9403133333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.09488148262716249, 0.03329942269499693, 0.20523642520256608, 0.21389265721684764, 0.2323986208875546, 0.22029139137087228]
maxi score, test score, baseline:  -0.9403133333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.09488148262716249, 0.03329942269499693, 0.20523642520256608, 0.21389265721684764, 0.2323986208875546, 0.22029139137087228]
line 256 mcts: sample exp_bonus 36.4326181000237
maxi score, test score, baseline:  -0.9403133333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.09492192564401782, 0.03331357958906492, 0.2053239729050055, 0.2139838998066918, 0.23249776269405184, 0.21995885936116805]
actor:  1 policy actor:  1  step number:  63 total reward:  0.23999999999999944  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9403133333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.09461725035433523, 0.034633317829859005, 0.2021083514392462, 0.21053994397825557, 0.24174377329257343, 0.2163573631057305]
actor:  1 policy actor:  1  step number:  65 total reward:  0.27999999999999936  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9403133333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.0929713877168746, 0.03403234415484815, 0.21599560600794418, 0.2068747682019697, 0.23753504300986386, 0.21259085090849955]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9403133333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.0929713877168746, 0.03403234415484815, 0.21599560600794418, 0.2068747682019697, 0.23753504300986386, 0.21259085090849955]
printing an ep nov before normalisation:  32.041239029532704
from probs:  [0.09300915111501866, 0.03404613315986412, 0.21608341146302254, 0.20695886361124483, 0.23763160997343025, 0.21227083067741953]
printing an ep nov before normalisation:  25.58123692258775
maxi score, test score, baseline:  -0.9403133333333334 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.9403133333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.09302277396169403, 0.033976453508395385, 0.21627091267786464, 0.2071334737350802, 0.23714344060950665, 0.21245294550745916]
actor:  1 policy actor:  1  step number:  67 total reward:  0.03999999999999948  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  38.54951507894171
maxi score, test score, baseline:  -0.9403133333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.09289016485897235, 0.03465473729734427, 0.2144457164870853, 0.20543376350565956, 0.2418954357591875, 0.21068018209175107]
Printing some Q and Qe and total Qs values:  [[0.472]
 [0.479]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]] [[30.349]
 [36.483]
 [30.349]
 [30.349]
 [30.349]
 [30.349]
 [30.349]] [[1.308]
 [1.646]
 [1.308]
 [1.308]
 [1.308]
 [1.308]
 [1.308]]
maxi score, test score, baseline:  -0.9403133333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.09289016485897235, 0.03465473729734427, 0.2144457164870853, 0.20543376350565956, 0.2418954357591875, 0.21068018209175107]
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.14050214986099
maxi score, test score, baseline:  -0.9403133333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.09296036747261743, 0.03468086472583342, 0.21385167790995857, 0.20558914416566168, 0.24207841224913298, 0.21083953347679585]
printing an ep nov before normalisation:  20.516941899224335
actor:  1 policy actor:  1  step number:  62 total reward:  0.3133333333333328  reward:  1.0 rdn_beta:  1.333
siam score:  -0.819342
actions average: 
K:  1  action  0 :  tensor([0.5025, 0.0033, 0.0743, 0.0919, 0.1300, 0.0857, 0.1123],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0054, 0.8905, 0.0051, 0.0426, 0.0077, 0.0033, 0.0454],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1557, 0.0268, 0.1395, 0.1677, 0.1293, 0.1690, 0.2120],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1207, 0.0336, 0.0797, 0.3827, 0.1092, 0.1096, 0.1645],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1376, 0.0107, 0.0844, 0.0915, 0.4234, 0.1206, 0.1318],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0650, 0.0020, 0.1056, 0.0664, 0.0722, 0.6200, 0.0688],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1728, 0.0010, 0.1163, 0.1541, 0.1513, 0.1710, 0.2335],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  35.606468874585474
printing an ep nov before normalisation:  35.93815291565584
maxi score, test score, baseline:  -0.9403133333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.09213554260255313, 0.03437388822325435, 0.21120906599376985, 0.21338563769264207, 0.2399285840346576, 0.20896728145312307]
maxi score, test score, baseline:  -0.9403133333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.09213554260255313, 0.03437388822325435, 0.21120906599376985, 0.21338563769264207, 0.2399285840346576, 0.20896728145312307]
printing an ep nov before normalisation:  39.012041091918945
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.549]
 [0.46 ]
 [0.456]
 [0.46 ]
 [0.464]
 [0.462]] [[37.092]
 [38.154]
 [37.871]
 [40.201]
 [40.154]
 [37.352]
 [38.489]] [[1.254]
 [1.38 ]
 [1.278]
 [1.377]
 [1.379]
 [1.259]
 [1.308]]
printing an ep nov before normalisation:  25.84474072783757
maxi score, test score, baseline:  -0.9403133333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.09213554260255313, 0.03437388822325435, 0.21120906599376985, 0.21338563769264207, 0.2399285840346576, 0.20896728145312307]
printing an ep nov before normalisation:  31.736755246999827
maxi score, test score, baseline:  -0.9403133333333334 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.9403133333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.09213554260255313, 0.03437388822325435, 0.21120906599376985, 0.21338563769264207, 0.2399285840346576, 0.20896728145312307]
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.711]
 [0.578]
 [0.578]
 [0.578]
 [0.443]
 [0.578]] [[53.711]
 [50.801]
 [53.711]
 [53.711]
 [53.711]
 [47.459]
 [53.711]] [[2.162]
 [2.179]
 [2.162]
 [2.162]
 [2.162]
 [1.776]
 [2.162]]
printing an ep nov before normalisation:  38.36857073566352
actor:  0 policy actor:  1  step number:  54 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  84 total reward:  0.086666666666666  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9378200000000001 -0.5663333333333334 -0.5663333333333334
using explorer policy with actor:  1
printing an ep nov before normalisation:  53.60401738472085
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.9378200000000001 -0.5663333333333334 -0.5663333333333334
probs:  [0.09175308878075293, 0.03517920480182855, 0.20870253471764177, 0.21157424650739298, 0.2455704495153981, 0.20722047567698568]
maxi score, test score, baseline:  -0.9378200000000001 -0.5663333333333334 -0.5663333333333334
probs:  [0.09175308878075293, 0.03517920480182855, 0.20870253471764177, 0.21157424650739298, 0.2455704495153981, 0.20722047567698568]
maxi score, test score, baseline:  -0.9378200000000001 -0.5663333333333334 -0.5663333333333334
probs:  [0.09175308878075293, 0.03517920480182855, 0.20870253471764177, 0.21157424650739298, 0.2455704495153981, 0.20722047567698568]
Printing some Q and Qe and total Qs values:  [[0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]] [[43.873]
 [43.873]
 [43.873]
 [43.873]
 [43.873]
 [43.873]
 [43.873]] [[1.909]
 [1.909]
 [1.909]
 [1.909]
 [1.909]
 [1.909]
 [1.909]]
maxi score, test score, baseline:  -0.9378200000000001 -0.5663333333333334 -0.5663333333333334
probs:  [0.09144420332646233, 0.03519113232731152, 0.2087735190609004, 0.21164620820718313, 0.245653981460913, 0.20729095561722974]
maxi score, test score, baseline:  -0.9378200000000001 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.9378200000000001 -0.5663333333333334 -0.5663333333333334
probs:  [0.09144420332646233, 0.03519113232731152, 0.2087735190609004, 0.21164620820718313, 0.245653981460913, 0.20729095561722974]
printing an ep nov before normalisation:  37.341204401394506
maxi score, test score, baseline:  -0.9378200000000001 -0.5663333333333334 -0.5663333333333334
probs:  [0.09145584709185704, 0.035121687450427735, 0.20895429270211138, 0.21183112282164987, 0.24516745778651183, 0.20746959214744212]
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.436]
 [0.389]
 [0.41 ]
 [0.404]
 [0.394]
 [0.381]] [[28.065]
 [25.362]
 [29.679]
 [30.73 ]
 [30.649]
 [27.81 ]
 [27.896]] [[1.982]
 [1.727]
 [2.211]
 [2.361]
 [2.345]
 [1.986]
 [1.984]]
maxi score, test score, baseline:  -0.9378200000000001 -0.5663333333333334 -0.5663333333333334
probs:  [0.09145584709185704, 0.035121687450427735, 0.20895429270211138, 0.21183112282164987, 0.24516745778651183, 0.20746959214744212]
actor:  1 policy actor:  1  step number:  84 total reward:  0.08666666666666634  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  15.21522045135498
printing an ep nov before normalisation:  28.706636214249322
printing an ep nov before normalisation:  37.23844835542947
maxi score, test score, baseline:  -0.9378200000000001 -0.5663333333333334 -0.5663333333333334
probs:  [0.08221905434681616, 0.03578198191019248, 0.21025526611658826, 0.21339010273930137, 0.24971618365910397, 0.2086374112279978]
maxi score, test score, baseline:  -0.9378200000000001 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.9378200000000001 -0.5663333333333334 -0.5663333333333334
probs:  [0.08221905434681616, 0.03578198191019248, 0.21025526611658826, 0.21339010273930137, 0.24971618365910397, 0.2086374112279978]
actor:  1 policy actor:  1  step number:  76 total reward:  0.16666666666666563  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  30.483884835570365
maxi score, test score, baseline:  -0.9378200000000001 -0.5663333333333334 -0.5663333333333334
probs:  [0.08162629549192059, 0.03552455482557624, 0.2087379311138054, 0.21185013042845685, 0.24791389315327048, 0.21434719498697039]
actor:  1 policy actor:  1  step number:  64 total reward:  0.3399999999999995  reward:  1.0 rdn_beta:  1.667
siam score:  -0.821268
printing an ep nov before normalisation:  40.293714770065215
maxi score, test score, baseline:  -0.9378200000000001 -0.5663333333333334 -0.5663333333333334
probs:  [0.08171790856281813, 0.03710800482005142, 0.20471625412892186, 0.20772774371365801, 0.25900980216633224, 0.20972028660821834]
maxi score, test score, baseline:  -0.9378200000000001 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.9378200000000001 -0.5663333333333334 -0.5663333333333334
probs:  [0.08171790856281813, 0.03710800482005142, 0.20471625412892186, 0.20772774371365801, 0.25900980216633224, 0.20972028660821834]
actor:  1 policy actor:  1  step number:  62 total reward:  0.24666666666666648  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  76 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9378200000000001 -0.5663333333333334 -0.5663333333333334
probs:  [0.08113173858033082, 0.037982323663602206, 0.20010322700824587, 0.21070097695085196, 0.26513830191065374, 0.2049434318863153]
maxi score, test score, baseline:  -0.9378200000000001 -0.5663333333333334 -0.5663333333333334
printing an ep nov before normalisation:  36.42650602200641
printing an ep nov before normalisation:  39.26281001389857
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.544]
 [0.511]
 [0.49 ]
 [0.552]
 [0.557]
 [0.558]] [[31.629]
 [34.482]
 [30.801]
 [29.77 ]
 [31.611]
 [30.259]
 [30.766]] [[1.155]
 [1.224]
 [1.063]
 [1.006]
 [1.133]
 [1.09 ]
 [1.109]]
printing an ep nov before normalisation:  12.283772862612707
printing an ep nov before normalisation:  15.828889325095119
maxi score, test score, baseline:  -0.9378200000000001 -0.5663333333333334 -0.5663333333333334
Printing some Q and Qe and total Qs values:  [[0.724]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]] [[14.484]
 [14.089]
 [14.089]
 [14.089]
 [14.089]
 [14.089]
 [14.089]] [[0.724]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]]
maxi score, test score, baseline:  -0.9378200000000001 -0.5663333333333334 -0.5663333333333334
probs:  [0.08113173858033082, 0.037982323663602206, 0.20010322700824587, 0.21070097695085196, 0.26513830191065374, 0.2049434318863153]
actor:  1 policy actor:  1  step number:  63 total reward:  0.42666666666666664  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9378200000000001 -0.5663333333333334 -0.5663333333333334
probs:  [0.08015032774692747, 0.03752372864864265, 0.1976803093499191, 0.22025637301499962, 0.26192739289767547, 0.20246186834183585]
maxi score, test score, baseline:  -0.9378200000000001 -0.5663333333333334 -0.5663333333333334
printing an ep nov before normalisation:  53.56973928625672
maxi score, test score, baseline:  -0.9378200000000001 -0.5663333333333334 -0.5663333333333334
probs:  [0.08014515997964768, 0.03745007143967385, 0.19786398054685386, 0.22047631785394817, 0.26141124796614024, 0.2026532222137362]
Printing some Q and Qe and total Qs values:  [[0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.357]
 [0.359]
 [0.359]] [[39.735]
 [39.735]
 [39.735]
 [39.735]
 [40.429]
 [39.735]
 [39.735]] [[0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.357]
 [0.359]
 [0.359]]
printing an ep nov before normalisation:  31.003107412203462
actor:  1 policy actor:  1  step number:  67 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9378200000000001 -0.5663333333333334 -0.5663333333333334
probs:  [0.07936052246218733, 0.03708412357172382, 0.1959249326883093, 0.21831552196991694, 0.258849023437094, 0.2104658758707686]
maxi score, test score, baseline:  -0.9378200000000001 -0.5663333333333334 -0.5663333333333334
probs:  [0.07936052246218733, 0.03708412357172382, 0.1959249326883093, 0.21831552196991694, 0.258849023437094, 0.2104658758707686]
printing an ep nov before normalisation:  45.136651769297956
printing an ep nov before normalisation:  23.392348289489746
maxi score, test score, baseline:  -0.9378200000000001 -0.5663333333333334 -0.5663333333333334
probs:  [0.07936052246218733, 0.03708412357172382, 0.1959249326883093, 0.21831552196991694, 0.258849023437094, 0.2104658758707686]
printing an ep nov before normalisation:  30.738898532140844
printing an ep nov before normalisation:  24.74031817093897
actor:  1 policy actor:  1  step number:  74 total reward:  0.13999999999999968  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  29.176642854875503
from probs:  [0.07847501492300964, 0.036601592786214845, 0.20501709771204207, 0.21610550155648178, 0.25547011508747225, 0.2083306779347795]
printing an ep nov before normalisation:  0.5062601786195842
printing an ep nov before normalisation:  49.51611912408248
maxi score, test score, baseline:  -0.9378200000000001 -0.5663333333333334 -0.5663333333333334
probs:  [0.0784597971096264, 0.036456107387689604, 0.20539555041094498, 0.2165184501126179, 0.25445065584730886, 0.20871943913181232]
printing an ep nov before normalisation:  35.008276146088384
printing an ep nov before normalisation:  39.33725415241332
actor:  1 policy actor:  1  step number:  68 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9378200000000001 -0.5663333333333334 -0.5663333333333334
probs:  [0.07727226458848185, 0.035905405348526764, 0.2174326862835427, 0.21323776912961986, 0.250594873366786, 0.2055570012830429]
printing an ep nov before normalisation:  34.719402016347544
printing an ep nov before normalisation:  34.04545879363512
maxi score, test score, baseline:  -0.9378200000000001 -0.5663333333333334 -0.5663333333333334
printing an ep nov before normalisation:  34.198005419633496
using another actor
maxi score, test score, baseline:  -0.9378200000000001 -0.5663333333333334 -0.5663333333333334
probs:  [0.07726300368503082, 0.035833284246096705, 0.2176364101741664, 0.21343511851340785, 0.25008950424583365, 0.2057426791354645]
printing an ep nov before normalisation:  30.398306846618652
printing an ep nov before normalisation:  46.978302584120954
printing an ep nov before normalisation:  22.670865058898926
actor:  0 policy actor:  1  step number:  48 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.07726300368503082, 0.035833284246096705, 0.2176364101741664, 0.21343511851340785, 0.25008950424583365, 0.2057426791354645]
actor:  1 policy actor:  1  step number:  83 total reward:  0.053333333333332344  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  26.401197805928195
printing an ep nov before normalisation:  40.858915369127914
printing an ep nov before normalisation:  31.188134601092262
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.07663146858011877, 0.03547391090685217, 0.22414552964291765, 0.21190903611953824, 0.24757292452934715, 0.20426713022122606]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.07663146858011877, 0.03547391090685217, 0.22414552964291765, 0.21190903611953824, 0.24757292452934715, 0.20426713022122606]
printing an ep nov before normalisation:  31.314109769282844
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.07663146858011877, 0.03547391090685217, 0.22414552964291765, 0.21190903611953824, 0.24757292452934715, 0.20426713022122606]
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.07663146858011877, 0.03547391090685217, 0.22414552964291765, 0.21190903611953824, 0.24757292452934715, 0.20426713022122606]
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.07662141468210394, 0.03540241936425572, 0.22435567628562272, 0.21210091682562038, 0.2470719693144841, 0.20444760352791308]
printing an ep nov before normalisation:  33.970348834991455
actions average: 
K:  2  action  0 :  tensor([0.4710, 0.0068, 0.0711, 0.0800, 0.1325, 0.1146, 0.1240],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0085, 0.9053, 0.0088, 0.0350, 0.0070, 0.0087, 0.0267],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1902, 0.0228, 0.2450, 0.0957, 0.1629, 0.1348, 0.1487],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2028, 0.0145, 0.0828, 0.2241, 0.2276, 0.1195, 0.1287],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1226, 0.0079, 0.0705, 0.0824, 0.4442, 0.1502, 0.1222],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0828, 0.0152, 0.1050, 0.1020, 0.1781, 0.3953, 0.1214],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1017, 0.0326, 0.1078, 0.1392, 0.1778, 0.1592, 0.2817],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.062]
 [0.062]
 [0.062]
 [0.062]
 [0.062]
 [0.062]
 [0.062]] [[51.799]
 [51.799]
 [51.799]
 [51.799]
 [51.799]
 [51.799]
 [51.799]] [[1.493]
 [1.493]
 [1.493]
 [1.493]
 [1.493]
 [1.493]
 [1.493]]
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.456]
 [0.442]
 [0.397]
 [0.471]
 [0.392]
 [0.463]] [[38.288]
 [37.293]
 [38.989]
 [41.567]
 [47.355]
 [39.139]
 [39.522]] [[1.2  ]
 [1.219]
 [1.277]
 [1.341]
 [1.659]
 [1.233]
 [1.321]]
printing an ep nov before normalisation:  4.8544624824808125
printing an ep nov before normalisation:  56.22307952689912
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.07602182727189107, 0.035425337624243436, 0.22450137818614485, 0.21223865546578446, 0.24723243248032115, 0.2045803689716149]
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.07602182727189107, 0.035425337624243436, 0.22450137818614485, 0.21223865546578446, 0.24723243248032115, 0.2045803689716149]
actor:  1 policy actor:  1  step number:  70 total reward:  0.23333333333333273  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.124]
 [0.034]
 [0.034]
 [0.034]
 [0.049]
 [0.034]] [[69.264]
 [66.549]
 [69.264]
 [69.264]
 [69.264]
 [83.493]
 [69.264]] [[0.95 ]
 [0.974]
 [0.95 ]
 [0.95 ]
 [0.95 ]
 [1.314]
 [0.95 ]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
Printing some Q and Qe and total Qs values:  [[0.1  ]
 [0.015]
 [0.144]
 [0.36 ]
 [0.435]
 [0.48 ]
 [0.395]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.1  ]
 [0.015]
 [0.144]
 [0.36 ]
 [0.435]
 [0.48 ]
 [0.395]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 42.87397525054177
UNIT TEST: sample policy line 217 mcts : [0.735 0.041 0.02  0.041 0.041 0.061 0.061]
actions average: 
K:  1  action  0 :  tensor([0.5241, 0.0162, 0.0761, 0.0804, 0.1128, 0.0889, 0.1015],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0061, 0.9316, 0.0057, 0.0166, 0.0051, 0.0041, 0.0307],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1282, 0.0513, 0.1535, 0.1441, 0.1731, 0.2110, 0.1388],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1266, 0.0610, 0.0936, 0.2613, 0.1601, 0.1512, 0.1462],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1297, 0.0155, 0.0615, 0.0917, 0.4741, 0.1142, 0.1133],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1015, 0.0016, 0.0952, 0.0965, 0.1008, 0.4832, 0.1212],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1210, 0.1139, 0.1037, 0.1193, 0.1364, 0.1437, 0.2619],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  47.71935907387027
printing an ep nov before normalisation:  50.67992860368124
printing an ep nov before normalisation:  45.5755096614788
printing an ep nov before normalisation:  29.07210350036621
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.0762614915805272, 0.03660134005945027, 0.22278297277968295, 0.2061187566727225, 0.25547185435698533, 0.2027635845506318]
actor:  1 policy actor:  1  step number:  74 total reward:  0.153333333333333  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  0.15875588392191278
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.07580480600596681, 0.03638257162545962, 0.2214473200176065, 0.20488307081682125, 0.25394010438057607, 0.20754212715356976]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  10.985671408874111
actor:  1 policy actor:  1  step number:  65 total reward:  0.27999999999999936  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.0746911655515642, 0.03584909872460696, 0.2333003047244183, 0.20186981654578487, 0.25020488953083714, 0.20408472492278845]
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
printing an ep nov before normalisation:  30.313869748696117
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.07484785076814965, 0.03592415644784439, 0.23209461792246946, 0.20229377061122655, 0.25073042085956243, 0.20410918339074763]
printing an ep nov before normalisation:  30.518256039096695
using explorer policy with actor:  1
actions average: 
K:  4  action  0 :  tensor([0.3996, 0.0314, 0.0870, 0.1198, 0.0969, 0.1066, 0.1586],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0148, 0.9168, 0.0108, 0.0112, 0.0136, 0.0113, 0.0214],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1324, 0.0708, 0.0850, 0.1887, 0.1517, 0.1627, 0.2086],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0724, 0.0721, 0.0783, 0.2442, 0.1592, 0.2278, 0.1460],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1579, 0.0497, 0.0887, 0.0923, 0.3776, 0.1254, 0.1083],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0663, 0.0198, 0.1033, 0.1276, 0.1108, 0.4174, 0.1549],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1296, 0.0680, 0.0949, 0.1236, 0.1143, 0.1935, 0.2762],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.07484785076814965, 0.03592415644784439, 0.23209461792246946, 0.20229377061122655, 0.25073042085956243, 0.20410918339074763]
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]] [[44.083]
 [44.083]
 [44.083]
 [44.083]
 [44.083]
 [44.083]
 [44.083]] [[1.457]
 [1.457]
 [1.457]
 [1.457]
 [1.457]
 [1.457]
 [1.457]]
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.07484785076814965, 0.03592415644784439, 0.23209461792246946, 0.20229377061122655, 0.25073042085956243, 0.20410918339074763]
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.07455155512391017, 0.035864372604095154, 0.23238760806490202, 0.202533169791908, 0.2503114471809823, 0.20435184723420236]
printing an ep nov before normalisation:  34.28117434183756
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.262337387241494
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
printing an ep nov before normalisation:  38.688161064955175
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.07455155512391017, 0.035864372604095154, 0.23238760806490202, 0.202533169791908, 0.2503114471809823, 0.20435184723420236]
printing an ep nov before normalisation:  17.674970401218257
printing an ep nov before normalisation:  45.18885507810614
actor:  1 policy actor:  1  step number:  75 total reward:  0.09333333333333294  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.07424260318609016, 0.03571603271265906, 0.23058726824740597, 0.2066772558593656, 0.24927281801941925, 0.20350402197505987]
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.07433403489205119, 0.035759932645547575, 0.23004082506271417, 0.2069320775006479, 0.24958019151179048, 0.20335293838724877]
printing an ep nov before normalisation:  26.047503874362626
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.07433403489205119, 0.035759932645547575, 0.23004082506271417, 0.2069320775006479, 0.24958019151179048, 0.20335293838724877]
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
Printing some Q and Qe and total Qs values:  [[-0.079]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.077]
 [-0.079]
 [-0.079]] [[35.024]
 [35.024]
 [35.024]
 [35.024]
 [37.658]
 [35.024]
 [35.024]] [[0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.136]
 [0.114]
 [0.114]]
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
Printing some Q and Qe and total Qs values:  [[0.263]
 [0.263]
 [0.212]
 [0.263]
 [0.263]
 [0.441]
 [0.263]] [[41.533]
 [41.533]
 [42.067]
 [41.533]
 [41.533]
 [45.828]
 [41.533]] [[1.173]
 [1.173]
 [1.134]
 [1.173]
 [1.173]
 [1.448]
 [1.173]]
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.07405383353742628, 0.03577072470292306, 0.23011046877323396, 0.20699472109922043, 0.2496557540957651, 0.20341449779143128]
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.07405383353742628, 0.03577072470292306, 0.23011046877323396, 0.20699472109922043, 0.2496557540957651, 0.20341449779143128]
actions average: 
K:  3  action  0 :  tensor([0.4568, 0.0040, 0.0731, 0.0918, 0.1066, 0.1058, 0.1619],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0081, 0.9080, 0.0085, 0.0308, 0.0104, 0.0093, 0.0249],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1136, 0.0046, 0.3188, 0.1138, 0.1795, 0.1286, 0.1411],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1625, 0.1129, 0.0754, 0.2674, 0.1398, 0.1079, 0.1341],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1765, 0.0877, 0.0509, 0.0749, 0.4461, 0.0706, 0.0933],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0404, 0.0029, 0.0682, 0.0676, 0.0653, 0.6749, 0.0807],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1737, 0.0333, 0.0920, 0.1224, 0.1454, 0.1326, 0.3007],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.07405383353742628, 0.03577072470292306, 0.23011046877323396, 0.20699472109922043, 0.2496557540957651, 0.20341449779143128]
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.07405383353742628, 0.03577072470292306, 0.23011046877323396, 0.20699472109922043, 0.2496557540957651, 0.20341449779143128]
line 256 mcts: sample exp_bonus 29.704436847336993
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.07405383353742628, 0.03577072470292306, 0.23011046877323396, 0.20699472109922043, 0.2496557540957651, 0.20341449779143128]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]] [[37.73]
 [37.73]
 [37.73]
 [37.73]
 [37.73]
 [37.73]
 [37.73]] [[1.562]
 [1.562]
 [1.562]
 [1.562]
 [1.562]
 [1.562]
 [1.562]]
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.07405383353742628, 0.03577072470292306, 0.23011046877323396, 0.20699472109922043, 0.2496557540957651, 0.20341449779143128]
siam score:  -0.81051064
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.07405383353742628, 0.03577072470292306, 0.23011046877323396, 0.20699472109922043, 0.2496557540957651, 0.20341449779143128]
printing an ep nov before normalisation:  19.92624549599725
actor:  1 policy actor:  1  step number:  68 total reward:  0.13999999999999957  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  74 total reward:  0.21999999999999942  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.0888432986523044, 0.03483893420315289, 0.23389058333826537, 0.2015860480361975, 0.2431316507264051, 0.19770948504367475]
using explorer policy with actor:  1
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.4424],
        [-0.1992],
        [ 0.2590],
        [-0.2796],
        [-0.4673],
        [ 0.5249],
        [-0.0000],
        [-0.0000],
        [-0.1686],
        [-0.1288]], dtype=torch.float64)
-0.032346567066 -0.4747848729618578
-0.057834381198 -0.25702175361517554
-0.083839701198 0.1751918449978026
-0.032346567066 -0.3119808525215583
-0.032346567066 -0.4996104997972628
-0.08410238119800001 0.440824172183459
-0.8649271488 -0.8649271488
-0.6211880400000005 -0.6211880400000005
-0.032346567066 -0.20096024414370697
-0.057834381198 -0.186680692834984
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.08891768723533904, 0.03486803696843314, 0.23324871438259148, 0.20175497800776251, 0.24333541911540385, 0.19787516429046992]
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.08891768723533904, 0.03486803696843314, 0.23324871438259148, 0.20175497800776251, 0.24333541911540385, 0.19787516429046992]
printing an ep nov before normalisation:  48.1170117337828
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
actor:  1 policy actor:  1  step number:  68 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.08817522712626776, 0.034577567095197444, 0.23129928596426133, 0.2000689159805396, 0.24130164062212456, 0.20457736321160921]
printing an ep nov before normalisation:  35.252189526563214
Printing some Q and Qe and total Qs values:  [[-0.003]
 [ 0.08 ]
 [-0.003]
 [-0.002]
 [-0.001]
 [-0.005]
 [-0.006]] [[40.376]
 [41.653]
 [40.376]
 [42.743]
 [43.035]
 [43.043]
 [42.551]] [[0.798]
 [0.93 ]
 [0.798]
 [0.89 ]
 [0.902]
 [0.899]
 [0.879]]
Printing some Q and Qe and total Qs values:  [[0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.073]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.073]]
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.08825493595714196, 0.034536187559196595, 0.23087586859833203, 0.20040141614669105, 0.24101154283203122, 0.2049200489066072]
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]] [[34.579]
 [34.579]
 [34.579]
 [34.579]
 [34.579]
 [34.579]
 [34.579]] [[2.372]
 [2.372]
 [2.372]
 [2.372]
 [2.372]
 [2.372]
 [2.372]]
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.08825493595714196, 0.034536187559196595, 0.23087586859833203, 0.20040141614669105, 0.24101154283203122, 0.2049200489066072]
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.08825493595714196, 0.034536187559196595, 0.23087586859833203, 0.20040141614669105, 0.24101154283203122, 0.2049200489066072]
siam score:  -0.83014333
printing an ep nov before normalisation:  31.88500539117179
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.08829049216076407, 0.03455006880016093, 0.23096897088188723, 0.20048222230201165, 0.24110873475592765, 0.2045995110992486]
Printing some Q and Qe and total Qs values:  [[0.201]
 [0.201]
 [0.201]
 [0.201]
 [0.201]
 [0.201]
 [0.201]] [[34.909]
 [34.909]
 [34.909]
 [34.909]
 [34.909]
 [34.909]
 [34.909]] [[58.395]
 [58.395]
 [58.395]
 [58.395]
 [58.395]
 [58.395]
 [58.395]]
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]] [[34.906]
 [34.906]
 [34.906]
 [34.906]
 [34.906]
 [34.906]
 [34.906]] [[0.896]
 [0.896]
 [0.896]
 [0.896]
 [0.896]
 [0.896]
 [0.896]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11277067447340544, 0.033625271373455265, 0.22476630121614857, 0.19509874652384068, 0.23463360466151356, 0.19910540175163646]
printing an ep nov before normalisation:  38.88427409622327
printing an ep nov before normalisation:  38.949090331805415
printing an ep nov before normalisation:  33.61726760864258
printing an ep nov before normalisation:  30.187762106399187
printing an ep nov before normalisation:  25.890483856201172
printing an ep nov before normalisation:  24.22920498750012
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11277067447340544, 0.033625271373455265, 0.22476630121614857, 0.19509874652384068, 0.23463360466151356, 0.19910540175163646]
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.557]
 [0.53 ]
 [0.53 ]
 [0.55 ]
 [0.53 ]
 [0.53 ]] [[34.445]
 [49.162]
 [34.445]
 [34.445]
 [41.39 ]
 [34.445]
 [34.445]] [[1.288]
 [1.927]
 [1.288]
 [1.288]
 [1.597]
 [1.288]
 [1.288]]
printing an ep nov before normalisation:  46.64479800341681
actor:  1 policy actor:  1  step number:  82 total reward:  0.04666666666666619  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  32.258706549424026
printing an ep nov before normalisation:  39.03177880515549
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11248602611413729, 0.034193452476607974, 0.22327484656861274, 0.1939269739264124, 0.2386147278049572, 0.1975039731092723]
printing an ep nov before normalisation:  34.38019037246704
printing an ep nov before normalisation:  29.915784397235324
printing an ep nov before normalisation:  22.44159547995873
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
printing an ep nov before normalisation:  32.841325698683036
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.735]
 [0.422]
 [0.422]
 [0.463]
 [0.449]
 [0.422]] [[45.516]
 [50.004]
 [45.516]
 [45.516]
 [47.112]
 [42.541]
 [45.516]] [[2.061]
 [2.594]
 [2.061]
 [2.061]
 [2.18 ]
 [1.943]
 [2.061]]
actor:  1 policy actor:  1  step number:  74 total reward:  0.1266666666666657  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  29.63188648223877
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11201685054052185, 0.03405121991958616, 0.22155638911496298, 0.19311770799792807, 0.23761884968097294, 0.20163898274602804]
actions average: 
K:  1  action  0 :  tensor([0.5482, 0.0190, 0.0541, 0.0549, 0.1638, 0.0649, 0.0951],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0069, 0.9670, 0.0041, 0.0046, 0.0045, 0.0044, 0.0084],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0624, 0.0077, 0.5253, 0.0704, 0.1104, 0.1192, 0.1046],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1412, 0.0264, 0.1013, 0.2152, 0.1489, 0.1797, 0.1872],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1046, 0.0117, 0.0615, 0.0870, 0.4899, 0.1187, 0.1265],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1174, 0.0010, 0.0791, 0.1020, 0.1331, 0.4570, 0.1105],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1201, 0.0300, 0.0886, 0.1286, 0.2014, 0.1740, 0.2573],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  47.89943408422196
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.675]
 [0.496]
 [0.496]
 [0.496]
 [0.532]
 [0.496]] [[39.828]
 [43.052]
 [39.828]
 [39.828]
 [39.828]
 [42.09 ]
 [39.828]] [[1.918]
 [2.292]
 [1.918]
 [1.918]
 [1.918]
 [2.091]
 [1.918]]
Printing some Q and Qe and total Qs values:  [[-0.064]
 [-0.053]
 [-0.065]
 [-0.068]
 [-0.079]
 [-0.059]
 [-0.064]] [[24.316]
 [31.162]
 [27.568]
 [28.407]
 [32.47 ]
 [29.87 ]
 [26.773]] [[0.055]
 [0.137]
 [0.088]
 [0.093]
 [0.125]
 [0.118]
 [0.081]]
printing an ep nov before normalisation:  30.480058647968846
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11206085029757236, 0.03406455863179875, 0.22164346678566604, 0.1931936017704196, 0.23771224413279735, 0.20132527838174583]
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11206085029757236, 0.03406455863179875, 0.22164346678566604, 0.1931936017704196, 0.23771224413279735, 0.20132527838174583]
printing an ep nov before normalisation:  53.66964199973495
printing an ep nov before normalisation:  39.53859760253803
actor:  1 policy actor:  1  step number:  51 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11083278384358489, 0.033692265098703336, 0.21921306322323167, 0.20203854803697185, 0.2351055344379581, 0.1991178053595501]
actor:  1 policy actor:  1  step number:  68 total reward:  0.15333333333333266  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.12210751993991217, 0.03326641384370463, 0.21643302458379937, 0.19947644737211703, 0.23212382646754287, 0.19659276779292395]
printing an ep nov before normalisation:  48.294161770272325
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.482]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]] [[30.691]
 [29.565]
 [30.691]
 [30.691]
 [30.691]
 [30.691]
 [30.691]] [[1.051]
 [1.078]
 [1.051]
 [1.051]
 [1.051]
 [1.051]
 [1.051]]
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.12220068743971535, 0.03329172193216466, 0.21659824063748762, 0.19962871150075004, 0.2323010276104102, 0.19597961087947216]
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.12220068743971535, 0.03329172193216466, 0.21659824063748762, 0.19962871150075004, 0.2323010276104102, 0.19597961087947216]
Printing some Q and Qe and total Qs values:  [[0.031]
 [0.092]
 [0.031]
 [0.031]
 [0.031]
 [0.031]
 [0.031]] [[39.881]
 [48.682]
 [39.881]
 [39.881]
 [39.881]
 [39.881]
 [39.881]] [[0.963]
 [1.425]
 [0.963]
 [0.963]
 [0.963]
 [0.963]
 [0.963]]
actor:  1 policy actor:  1  step number:  74 total reward:  0.2466666666666657  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  0.0
from probs:  [0.12136517748059004, 0.03306476338976037, 0.21511661175879593, 0.20510241937821433, 0.23071191853324488, 0.19463910945939455]
printing an ep nov before normalisation:  49.25752378156948
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.12136517748059004, 0.03306476338976037, 0.21511661175879593, 0.20510241937821433, 0.23071191853324488, 0.19463910945939455]
actor:  1 policy actor:  1  step number:  52 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.12040820798681469, 0.03433158559529002, 0.2117985699700327, 0.2020365788743541, 0.23958827658369533, 0.1918367809898131]
printing an ep nov before normalisation:  17.966640708368686
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.12045992696269157, 0.03426312077220605, 0.21197789201148523, 0.20220227081343148, 0.23910855789791494, 0.1919882315422708]
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.574]
 [0.412]
 [0.412]
 [0.292]
 [0.412]
 [0.412]] [[27.366]
 [37.879]
 [27.366]
 [27.366]
 [33.689]
 [27.366]
 [27.366]] [[1.079]
 [1.741]
 [1.079]
 [1.079]
 [1.26 ]
 [1.079]
 [1.079]]
printing an ep nov before normalisation:  29.56159087164132
actor:  1 policy actor:  1  step number:  59 total reward:  0.34666666666666657  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8081636
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11946292876448578, 0.03398032865056084, 0.21022259788978703, 0.20052797514597273, 0.23712846558132306, 0.19867770396787052]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.12413199150747
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11946292876448578, 0.03398032865056084, 0.21022259788978703, 0.20052797514597273, 0.23712846558132306, 0.19867770396787052]
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11946292876448578, 0.03398032865056084, 0.21022259788978703, 0.20052797514597273, 0.23712846558132306, 0.19867770396787052]
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.558]
 [0.39 ]
 [0.356]
 [0.356]
 [0.399]
 [0.48 ]] [[30.759]
 [39.559]
 [31.206]
 [30.796]
 [30.912]
 [30.308]
 [29.039]] [[0.715]
 [1.119]
 [0.745]
 [0.701]
 [0.705]
 [0.732]
 [0.782]]
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11946292876448578, 0.03398032865056084, 0.21022259788978703, 0.20052797514597273, 0.23712846558132306, 0.19867770396787052]
actor:  1 policy actor:  1  step number:  53 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8049377
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11855049117778986, 0.035228710606388866, 0.20701594760416067, 0.19756638444269783, 0.24587558195782025, 0.19576288421114243]
printing an ep nov before normalisation:  54.912944698006385
Printing some Q and Qe and total Qs values:  [[ 0.034]
 [-0.011]
 [ 0.024]
 [ 0.012]
 [ 0.026]
 [ 0.004]
 [ 0.008]] [[39.721]
 [35.756]
 [40.266]
 [38.318]
 [40.538]
 [38.151]
 [38.392]] [[1.519]
 [1.168]
 [1.552]
 [1.389]
 [1.575]
 [1.368]
 [1.391]]
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11855049117778986, 0.035228710606388866, 0.20701594760416067, 0.19756638444269783, 0.24587558195782025, 0.19576288421114243]
printing an ep nov before normalisation:  31.720437792434613
actor:  1 policy actor:  1  step number:  56 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  55 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11583007242716184, 0.03434141005851154, 0.21811831172890067, 0.2010673273594233, 0.23966229790646273, 0.1909805805195401]
Printing some Q and Qe and total Qs values:  [[0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11583007242716184, 0.03434141005851154, 0.21811831172890067, 0.2010673273594233, 0.23966229790646273, 0.1909805805195401]
printing an ep nov before normalisation:  47.089471359093295
printing an ep nov before normalisation:  40.63258495893306
line 256 mcts: sample exp_bonus 44.57680088200403
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11587196612809664, 0.03435379680417089, 0.21819724388910763, 0.20114008537688977, 0.23974903112043736, 0.19068787668129783]
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11591371759015053, 0.03436614149394114, 0.21827590805622993, 0.20121259636392558, 0.23983546985493132, 0.19039616664082137]
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11591371759015053, 0.03436614149394114, 0.21827590805622993, 0.20121259636392558, 0.23983546985493132, 0.19039616664082137]
UNIT TEST: sample policy line 217 mcts : [0.02  0.408 0.224 0.02  0.02  0.02  0.286]
printing an ep nov before normalisation:  30.583045482635498
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11595857597344139, 0.034298605948344954, 0.21846184836134758, 0.2013750189541983, 0.2393622694061091, 0.19054368135655866]
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11595857597344139, 0.034298605948344954, 0.21846184836134758, 0.2013750189541983, 0.2393622694061091, 0.19054368135655866]
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11595857597344139, 0.034298605948344954, 0.21846184836134758, 0.2013750189541983, 0.2393622694061091, 0.19054368135655866]
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
printing an ep nov before normalisation:  10.341052257919472
printing an ep nov before normalisation:  31.27379955604427
actor:  1 policy actor:  1  step number:  75 total reward:  0.07999999999999963  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11515400803812216, 0.03420475483087979, 0.21710680852636965, 0.20480868271230895, 0.23870511548271367, 0.19002063040960562]
siam score:  -0.81066924
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11515400803812216, 0.03420475483087979, 0.21710680852636965, 0.20480868271230895, 0.23870511548271367, 0.19002063040960562]
printing an ep nov before normalisation:  26.874753100235502
actor:  1 policy actor:  1  step number:  68 total reward:  0.0999999999999992  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  33.013853618722116
Printing some Q and Qe and total Qs values:  [[0.043]
 [0.04 ]
 [0.043]
 [0.043]
 [0.043]
 [0.043]
 [0.043]] [[39.731]
 [44.187]
 [39.731]
 [39.731]
 [39.731]
 [39.731]
 [39.731]] [[0.644]
 [0.749]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.31644548505925
printing an ep nov before normalisation:  39.87139701843262
from probs:  [0.11424344718320456, 0.03407718319131804, 0.2162944095196561, 0.2082634643747748, 0.23781184744588815, 0.18930964828515842]
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11432880535871891, 0.0341025743804723, 0.21570867040593947, 0.20841915259696656, 0.23798963882464916, 0.18945115843325366]
actor:  1 policy actor:  1  step number:  73 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11353084343538192, 0.03386520752004849, 0.21420229863795096, 0.21394580987909778, 0.2363275748255768, 0.18812826570194413]
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11353084343538192, 0.03386520752004849, 0.21420229863795096, 0.21394580987909778, 0.2363275748255768, 0.18812826570194413]
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11353084343538192, 0.03386520752004849, 0.21420229863795096, 0.21394580987909778, 0.2363275748255768, 0.18812826570194413]
Printing some Q and Qe and total Qs values:  [[0.65 ]
 [0.653]
 [0.629]
 [0.611]
 [0.636]
 [0.615]
 [0.662]] [[26.197]
 [29.587]
 [23.004]
 [23.542]
 [24.892]
 [22.892]
 [26.516]] [[0.65 ]
 [0.653]
 [0.629]
 [0.611]
 [0.636]
 [0.615]
 [0.662]]
printing an ep nov before normalisation:  21.90109627985215
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
printing an ep nov before normalisation:  25.500045714494604
actor:  1 policy actor:  1  step number:  68 total reward:  0.2866666666666665  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  10.051238558101772
siam score:  -0.81233954
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11277522081693712, 0.033640435180216297, 0.21277585390871026, 0.2125210742572725, 0.234753698786793, 0.19353371705007083]
printing an ep nov before normalisation:  20.414299964904785
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  64 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.039]
 [0.053]
 [0.044]
 [0.041]
 [0.043]
 [0.041]
 [0.042]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.039]
 [0.053]
 [0.044]
 [0.041]
 [0.043]
 [0.041]
 [0.042]]
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.035]
 [0.016]
 [0.016]] [[41.055]
 [41.055]
 [41.055]
 [41.055]
 [41.663]
 [41.055]
 [41.055]] [[1.114]
 [1.114]
 [1.114]
 [1.114]
 [1.165]
 [1.114]
 [1.114]]
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.1122025650842508, 0.03347008934115295, 0.21096672473211386, 0.21761280568398503, 0.23356092197711764, 0.1921868931813798]
printing an ep nov before normalisation:  27.720103566498455
printing an ep nov before normalisation:  40.17566726038652
Printing some Q and Qe and total Qs values:  [[-0.012]
 [-0.015]
 [-0.012]
 [-0.012]
 [-0.014]
 [-0.012]
 [-0.012]] [[29.447]
 [38.163]
 [27.844]
 [28.791]
 [34.994]
 [29.602]
 [27.688]] [[0.729]
 [1.119]
 [0.657]
 [0.699]
 [0.977]
 [0.736]
 [0.65 ]]
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11224115914214962, 0.03340330765251197, 0.2111375049934318, 0.21779248107701651, 0.233093008666685, 0.192332538468205]
Printing some Q and Qe and total Qs values:  [[0.054]
 [0.081]
 [0.081]
 [0.048]
 [0.048]
 [0.081]
 [0.081]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.054]
 [0.081]
 [0.081]
 [0.048]
 [0.048]
 [0.081]
 [0.081]]
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11224115914214962, 0.03340330765251197, 0.2111375049934318, 0.21779248107701646, 0.233093008666685, 0.192332538468205]
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11232241299943975, 0.03342742122189006, 0.21056623413607237, 0.21795023667567698, 0.23326185381858874, 0.19247184114833202]
printing an ep nov before normalisation:  54.912240452401036
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11232241299943975, 0.03342742122189006, 0.21056623413607237, 0.21795023667567698, 0.23326185381858874, 0.19247184114833202]
printing an ep nov before normalisation:  0.002029671023251467
maxi score, test score, baseline:  -0.9347933333333335 -0.5663333333333334 -0.5663333333333334
probs:  [0.11236983850920995, 0.033424148002045524, 0.21055692635435097, 0.2179366648215324, 0.2332394398975485, 0.19247298241531272]
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.82988005755315
printing an ep nov before normalisation:  0.0
siam score:  -0.8062361
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.112530250404738, 0.033393649252296306, 0.2102345791447945, 0.21835236400128943, 0.2330255845350941, 0.19246357266178774]
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.11211416171237426, 0.03340925505887909, 0.2103331588025901, 0.21845475252772917, 0.23313485774381934, 0.19255381415460812]
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.11211416171237426, 0.03340925505887909, 0.2103331588025901, 0.21845475252772917, 0.23313485774381934, 0.19255381415460812]
Printing some Q and Qe and total Qs values:  [[0.662]
 [0.67 ]
 [0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]] [[32.6  ]
 [32.286]
 [32.6  ]
 [32.6  ]
 [32.6  ]
 [32.6  ]
 [32.6  ]] [[1.341]
 [1.337]
 [1.341]
 [1.341]
 [1.341]
 [1.341]
 [1.341]]
Printing some Q and Qe and total Qs values:  [[0.04 ]
 [0.068]
 [0.04 ]
 [0.04 ]
 [0.04 ]
 [0.04 ]
 [0.04 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.04 ]
 [0.068]
 [0.04 ]
 [0.04 ]
 [0.04 ]
 [0.04 ]
 [0.04 ]]
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.11178162831064709, 0.033448585249075895, 0.20986549705563795, 0.2187127949555578, 0.23341025114048752, 0.1927812432885937]
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
printing an ep nov before normalisation:  43.2367272178507
Printing some Q and Qe and total Qs values:  [[-0.003]
 [-0.011]
 [-0.001]
 [-0.004]
 [-0.005]
 [-0.004]
 [ 0.002]] [[30.263]
 [27.439]
 [28.654]
 [29.242]
 [28.948]
 [29.161]
 [27.697]] [[0.92 ]
 [0.722]
 [0.813]
 [0.849]
 [0.829]
 [0.844]
 [0.752]]
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.11178162831064709, 0.033448585249075895, 0.20986549705563795, 0.2187127949555578, 0.23341025114048752, 0.1927812432885937]
Printing some Q and Qe and total Qs values:  [[0.312]
 [0.345]
 [0.312]
 [0.229]
 [0.312]
 [0.368]
 [0.312]] [[30.181]
 [34.326]
 [30.181]
 [31.719]
 [30.181]
 [27.246]
 [30.181]] [[0.808]
 [0.931]
 [0.808]
 [0.758]
 [0.808]
 [0.8  ]
 [0.808]]
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.1117816283106471, 0.0334485852490759, 0.209865497055638, 0.2187127949555578, 0.23341025114048755, 0.19278124328859372]
printing an ep nov before normalisation:  57.24065733218119
printing an ep nov before normalisation:  34.560766273190495
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.1117816283106471, 0.0334485852490759, 0.209865497055638, 0.2187127949555578, 0.23341025114048755, 0.19278124328859372]
printing an ep nov before normalisation:  54.535668708820914
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.11182903409523752, 0.03344532265766254, 0.20985665585251342, 0.21869888019790631, 0.23338790800773165, 0.1927821991889485]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9347933333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.11182903409523752, 0.03344532265766254, 0.20985665585251342, 0.21869888019790631, 0.23338790800773165, 0.1927821991889485]
printing an ep nov before normalisation:  42.2991943359375
actor:  0 policy actor:  1  step number:  83 total reward:  0.013333333333332531  reward:  1.0 rdn_beta:  1.0
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[ 0.4914],
        [-0.0000],
        [-0.0000],
        [ 0.2190],
        [-0.1883],
        [-0.0000],
        [-0.1297],
        [-0.2246],
        [-0.0000],
        [-0.4887]], dtype=torch.float64)
-0.08410238119800001 0.40731072376383876
-0.27960398003999903 -0.27960398003999903
-0.8281851600000001 -0.8281851600000001
-0.09703970119800001 0.1219705975708236
-0.032346567066 -0.22062776860849448
-0.6336000000000004 -0.6336000000000004
-0.032346567066 -0.1620169413964579
-0.032346567066 -0.25698419073494205
-0.5478 -0.5478
-0.032346567066 -0.5210315570561055
printing an ep nov before normalisation:  46.08358126639713
printing an ep nov before normalisation:  39.366570499447555
Printing some Q and Qe and total Qs values:  [[0.369]
 [0.369]
 [0.369]
 [0.369]
 [0.369]
 [0.369]
 [0.369]] [[39.172]
 [39.172]
 [39.172]
 [39.172]
 [39.172]
 [39.172]
 [39.172]] [[2.369]
 [2.369]
 [2.369]
 [2.369]
 [2.369]
 [2.369]
 [2.369]]
printing an ep nov before normalisation:  41.893270255956466
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.93587210596016
printing an ep nov before normalisation:  44.567745667821725
printing an ep nov before normalisation:  40.00333303928432
siam score:  -0.8039888
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
actor:  1 policy actor:  1  step number:  64 total reward:  0.25999999999999945  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]] [[41.897]
 [41.897]
 [41.897]
 [41.897]
 [41.897]
 [41.897]
 [41.897]] [[1.315]
 [1.315]
 [1.315]
 [1.315]
 [1.315]
 [1.315]
 [1.315]]
printing an ep nov before normalisation:  37.74777889251709
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
probs:  [0.11127272058933789, 0.033202209205752085, 0.2082010191453683, 0.2240973155372716, 0.2316853060684385, 0.1915414294538316]
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
probs:  [0.11127272058933789, 0.033202209205752085, 0.2082010191453683, 0.2240973155372716, 0.2316853060684385, 0.1915414294538316]
printing an ep nov before normalisation:  48.29412739436903
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
probs:  [0.1113509118477025, 0.03322547461088956, 0.20764442426885715, 0.2242548835927482, 0.23184821259098565, 0.1916760930888169]
printing an ep nov before normalisation:  51.61037750577539
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
probs:  [0.1113509118477025, 0.03322547461088956, 0.20764442426885715, 0.2242548835927482, 0.23184821259098565, 0.1916760930888169]
actor:  1 policy actor:  1  step number:  89 total reward:  0.1066666666666648  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  68 total reward:  0.2599999999999998  reward:  1.0 rdn_beta:  1.333
from probs:  [0.1113509118477025, 0.03322547461088956, 0.20764442426885715, 0.2242548835927482, 0.23184821259098565, 0.1916760930888169]
actions average: 
K:  2  action  0 :  tensor([0.5639, 0.0142, 0.0728, 0.0788, 0.0879, 0.0823, 0.1001],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0080, 0.9185, 0.0022, 0.0187, 0.0162, 0.0015, 0.0350],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0950, 0.0034, 0.5053, 0.0912, 0.0939, 0.1191, 0.0921],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1643, 0.0853, 0.1104, 0.1560, 0.1478, 0.1533, 0.1830],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1038, 0.0146, 0.0623, 0.0807, 0.5201, 0.1177, 0.1009],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1019, 0.0211, 0.1359, 0.0983, 0.1121, 0.4079, 0.1228],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0995, 0.0337, 0.1113, 0.1317, 0.1175, 0.1301, 0.3762],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
printing an ep nov before normalisation:  37.04399700271933
printing an ep nov before normalisation:  60.282760093800704
printing an ep nov before normalisation:  23.127158306744057
actor:  1 policy actor:  1  step number:  62 total reward:  0.35333333333333317  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
probs:  [0.1097307486948298, 0.033508469007923064, 0.20367852416201634, 0.23383271422866664, 0.2272927014162162, 0.19195684249034806]
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
Printing some Q and Qe and total Qs values:  [[-0.016]
 [-0.001]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]] [[36.385]
 [42.883]
 [36.385]
 [36.385]
 [36.385]
 [36.385]
 [36.385]] [[0.979]
 [1.315]
 [0.979]
 [0.979]
 [0.979]
 [0.979]
 [0.979]]
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
probs:  [0.1097307486948298, 0.033508469007923064, 0.20367852416201634, 0.23383271422866664, 0.2272927014162162, 0.19195684249034806]
actor:  1 policy actor:  1  step number:  69 total reward:  0.18666666666666554  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  44.879963938785835
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
probs:  [0.10237800150609838, 0.034007451554608964, 0.20447964196895707, 0.23725095834471494, 0.23014332824406897, 0.19174061838155165]
printing an ep nov before normalisation:  42.09315165397644
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
probs:  [0.10237800150609838, 0.034007451554608964, 0.20447964196895707, 0.23725095834471494, 0.23014332824406897, 0.19174061838155165]
actor:  1 policy actor:  1  step number:  60 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
Printing some Q and Qe and total Qs values:  [[0.044]
 [0.057]
 [0.041]
 [0.041]
 [0.04 ]
 [0.045]
 [0.046]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.044]
 [0.057]
 [0.041]
 [0.041]
 [0.04 ]
 [0.045]
 [0.046]]
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
probs:  [0.10115997223369427, 0.03360391213179874, 0.21463700280973186, 0.23442620432886926, 0.22671488511615262, 0.1894580233797532]
printing an ep nov before normalisation:  36.0753300650345
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.584]
 [0.595]
 [0.584]
 [0.584]
 [0.584]
 [0.584]] [[12.362]
 [11.034]
 [13.765]
 [11.034]
 [11.034]
 [11.034]
 [11.034]] [[1.833]
 [1.627]
 [2.359]
 [1.627]
 [1.627]
 [1.627]
 [1.627]]
actor:  1 policy actor:  1  step number:  77 total reward:  0.1599999999999996  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  62 total reward:  0.21999999999999909  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
probs:  [0.10009750986405747, 0.033251912831687845, 0.21238114284790074, 0.2319622283692498, 0.22433200635950357, 0.19797519972760064]
printing an ep nov before normalisation:  46.67866093817532
printing an ep nov before normalisation:  31.261981633709688
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
printing an ep nov before normalisation:  25.042440329688624
actor:  1 policy actor:  1  step number:  70 total reward:  0.08666666666666623  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  30.392136573791504
printing an ep nov before normalisation:  31.292933748740282
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
probs:  [0.1086483571168008, 0.033004361141202004, 0.2100345389677237, 0.23022937999731238, 0.22198424526059016, 0.19609911751637082]
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
probs:  [0.1086483571168008, 0.033004361141202004, 0.2100345389677237, 0.23022937999731238, 0.22198424526059016, 0.19609911751637082]
from probs:  [0.1086483571168008, 0.033004361141202004, 0.2100345389677237, 0.23022937999731238, 0.22198424526059016, 0.19609911751637082]
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
probs:  [0.108855045974277, 0.03306697081773302, 0.20892758448833393, 0.23066764432841577, 0.22240680507682323, 0.19607594931441708]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
probs:  [0.10893620224310416, 0.03309155447358063, 0.208337579417196, 0.2308397285825293, 0.22257272296161154, 0.19622221232197842]
printing an ep nov before normalisation:  5.266566631689784e-06
line 256 mcts: sample exp_bonus 19.21984243564998
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
probs:  [0.108979243977532, 0.03310459256911595, 0.20841994367620348, 0.23093099454228586, 0.2226607185488544, 0.19590450668600815]
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
probs:  [0.108979243977532, 0.03310459256911595, 0.20841994367620348, 0.23093099454228586, 0.2226607185488544, 0.19590450668600815]
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
probs:  [0.10902214636789344, 0.033117588454891975, 0.2085020412877134, 0.23102196503602263, 0.2227484292576389, 0.19558782959583976]
printing an ep nov before normalisation:  43.6421914874127
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
probs:  [0.10902214636789344, 0.033117588454891975, 0.2085020412877134, 0.23102196503602263, 0.2227484292576389, 0.19558782959583976]
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
probs:  [0.10902214636789344, 0.033117588454891975, 0.2085020412877134, 0.23102196503602263, 0.2227484292576389, 0.19558782959583976]
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
probs:  [0.10902214636789344, 0.033117588454891975, 0.2085020412877134, 0.23102196503602263, 0.2227484292576389, 0.19558782959583976]
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
probs:  [0.10902214636789344, 0.033117588454891975, 0.2085020412877134, 0.23102196503602263, 0.2227484292576389, 0.19558782959583976]
printing an ep nov before normalisation:  42.23652937828472
printing an ep nov before normalisation:  31.914415784757985
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 33.740788703473356
printing an ep nov before normalisation:  13.552548885345459
printing an ep nov before normalisation:  13.24704759890866
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
probs:  [0.10918334431491733, 0.033166418137555394, 0.20733130263888533, 0.23136377015938683, 0.22307798633054213, 0.1958771784187129]
actor:  1 policy actor:  1  step number:  76 total reward:  0.09999999999999964  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
probs:  [0.10844037457327312, 0.03294135958449803, 0.2127272981060216, 0.22978837252630804, 0.2215590406005718, 0.19454355460932735]
Printing some Q and Qe and total Qs values:  [[0.045]
 [0.051]
 [0.045]
 [0.045]
 [0.044]
 [0.045]
 [0.045]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.045]
 [0.051]
 [0.045]
 [0.045]
 [0.044]
 [0.045]
 [0.045]]
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
probs:  [0.10844037457327312, 0.03294135958449803, 0.2127272981060216, 0.22978837252630804, 0.2215590406005718, 0.19454355460932735]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.074]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.08 ]
 [-0.08 ]] [[19.753]
 [17.194]
 [17.194]
 [17.194]
 [17.194]
 [21.544]
 [19.464]] [[0.427]
 [0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.467]
 [0.414]]
Printing some Q and Qe and total Qs values:  [[-0.078]
 [-0.17 ]
 [-0.093]
 [-0.092]
 [-0.093]
 [-0.094]
 [-0.094]] [[22.461]
 [20.556]
 [18.347]
 [19.663]
 [18.003]
 [19.502]
 [20.477]] [[0.47 ]
 [0.332]
 [0.354]
 [0.387]
 [0.346]
 [0.382]
 [0.406]]
printing an ep nov before normalisation:  16.672139741265934
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
probs:  [0.10844037457327312, 0.03294135958449803, 0.2127272981060216, 0.22978837252630804, 0.2215590406005718, 0.19454355460932735]
line 256 mcts: sample exp_bonus 23.623807965476132
actor:  1 policy actor:  1  step number:  52 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  37.79560136977145
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
printing an ep nov before normalisation:  47.95851626285841
line 256 mcts: sample exp_bonus 33.106346621287166
printing an ep nov before normalisation:  27.197004892651936
printing an ep nov before normalisation:  23.938264846801758
printing an ep nov before normalisation:  43.31745560037726
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]] [[38.4]
 [38.4]
 [38.4]
 [38.4]
 [38.4]
 [38.4]
 [38.4]] [[1.955]
 [1.955]
 [1.955]
 [1.955]
 [1.955]
 [1.955]
 [1.955]]
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
probs:  [0.09655514009067948, 0.03367872972004896, 0.21461803851462774, 0.23482546169543267, 0.22548798410741225, 0.19483464587179888]
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
probs:  [0.09655514009067948, 0.03367872972004896, 0.21461803851462774, 0.23482546169543267, 0.22548798410741225, 0.19483464587179888]
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
probs:  [0.09668147903329985, 0.03320274488198415, 0.2158753595046994, 0.23148853087433777, 0.22684943360429113, 0.1959024521013878]
actor:  1 policy actor:  1  step number:  53 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  51.66110397252674
maxi score, test score, baseline:  -0.9327666666666666 -0.5663333333333334 -0.5663333333333334
actor:  0 policy actor:  0  step number:  79 total reward:  0.013333333333332198  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.93074 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.93074 -0.5663333333333334 -0.5663333333333334
probs:  [0.09605273760304035, 0.03376372533588788, 0.21301267989102982, 0.2283332283546195, 0.23542352301205502, 0.19341410580336754]
printing an ep nov before normalisation:  39.52893084381191
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.93074 -0.5663333333333334 -0.5663333333333334
probs:  [0.09605273760304035, 0.03376372533588788, 0.21301267989102982, 0.2283332283546195, 0.23542352301205502, 0.19341410580336754]
Printing some Q and Qe and total Qs values:  [[-0.003]
 [ 0.076]
 [-0.004]
 [-0.001]
 [-0.   ]
 [-0.009]
 [-0.002]] [[23.899]
 [25.841]
 [23.612]
 [23.962]
 [24.223]
 [24.625]
 [24.467]] [[0.246]
 [0.365]
 [0.239]
 [0.249]
 [0.255]
 [0.254]
 [0.258]]
maxi score, test score, baseline:  -0.93074 -0.5663333333333334 -0.5663333333333334
probs:  [0.09613684414924598, 0.03379321407137007, 0.21319934218113956, 0.2285333243914802, 0.2356298361385352, 0.192707439068229]
actor:  1 policy actor:  1  step number:  67 total reward:  0.14666666666666628  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.93074 -0.5663333333333334 -0.5663333333333334
probs:  [0.09565333178354547, 0.03362368899992495, 0.21212625655879852, 0.22738301074153838, 0.23444378163552906, 0.1967699302806636]
actor:  1 policy actor:  1  step number:  58 total reward:  0.2866666666666655  reward:  1.0 rdn_beta:  0.333
from probs:  [0.09565333178354547, 0.03362368899992495, 0.21212625655879852, 0.22738301074153838, 0.23444378163552906, 0.1967699302806636]
printing an ep nov before normalisation:  44.49639059657559
maxi score, test score, baseline:  -0.93074 -0.5663333333333334 -0.5663333333333334
probs:  [0.11158407972356661, 0.032957730443880257, 0.20859091001103433, 0.2236024153559381, 0.22978343129616058, 0.19348143316942013]
line 256 mcts: sample exp_bonus 36.208307169100635
maxi score, test score, baseline:  -0.93074 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.93074 -0.5663333333333334 -0.5663333333333334
probs:  [0.11158407972356661, 0.032957730443880257, 0.20859091001103433, 0.2236024153559381, 0.22978343129616058, 0.19348143316942013]
printing an ep nov before normalisation:  33.14893485361421
maxi score, test score, baseline:  -0.93074 -0.5663333333333334 -0.5663333333333334
probs:  [0.11158407972356661, 0.032957730443880257, 0.20859091001103433, 0.2236024153559381, 0.22978343129616058, 0.19348143316942013]
printing an ep nov before normalisation:  46.740292735268156
maxi score, test score, baseline:  -0.93074 -0.5663333333333334 -0.5663333333333334
probs:  [0.11158407972356661, 0.032957730443880257, 0.20859091001103433, 0.2236024153559381, 0.22978343129616058, 0.19348143316942013]
printing an ep nov before normalisation:  49.837873288051156
using explorer policy with actor:  0
printing an ep nov before normalisation:  53.19446149158233
printing an ep nov before normalisation:  29.15619574384131
printing an ep nov before normalisation:  42.075115075001
printing an ep nov before normalisation:  30.861668517813424
actor:  0 policy actor:  0  step number:  61 total reward:  0.0666666666666661  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.11158407972356661, 0.032957730443880257, 0.20859091001103433, 0.2236024153559381, 0.22978343129616058, 0.19348143316942013]
actor:  1 policy actor:  1  step number:  63 total reward:  0.14666666666666628  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.1113631233167301, 0.032892654365962656, 0.20817763313210252, 0.22315937748196882, 0.2293281392757551, 0.19507907242748102]
printing an ep nov before normalisation:  44.973461823617704
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.1113631233167301, 0.032892654365962656, 0.20817763313210252, 0.22315937748196882, 0.2293281392757551, 0.19507907242748102]
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.1113631233167301, 0.032892654365962656, 0.20817763313210252, 0.22315937748196882, 0.2293281392757551, 0.19507907242748102]
Printing some Q and Qe and total Qs values:  [[0.67 ]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]] [[43.906]
 [48.264]
 [48.264]
 [48.264]
 [48.264]
 [48.264]
 [48.264]] [[0.67 ]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]]
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.1113631233167301, 0.032892654365962656, 0.20817763313210252, 0.22315937748196882, 0.2293281392757551, 0.19507907242748102]
printing an ep nov before normalisation:  28.45838591211495
printing an ep nov before normalisation:  29.257561945026538
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.1113631233167301, 0.032892654365962656, 0.20817763313210252, 0.22315937748196882, 0.2293281392757551, 0.19507907242748102]
printing an ep nov before normalisation:  32.95121908187866
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.11140576858739173, 0.032815770482056826, 0.20836774988678644, 0.22337231499024351, 0.22878915908915276, 0.19524923696436874]
Printing some Q and Qe and total Qs values:  [[0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]] [[44.655]
 [44.655]
 [44.655]
 [44.655]
 [44.655]
 [44.655]
 [44.655]] [[0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]]
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
actor:  1 policy actor:  1  step number:  68 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.11060053547659417, 0.03301901019274846, 0.20631829327152815, 0.2302163895555545, 0.2264776534008115, 0.19336811810276308]
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.11060053547659417, 0.03301901019274846, 0.20631829327152815, 0.2302163895555545, 0.2264776534008115, 0.19336811810276308]
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.11060053547659417, 0.03301901019274846, 0.20631829327152815, 0.2302163895555545, 0.2264776534008115, 0.19336811810276308]
siam score:  -0.80768603
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.11068986880823842, 0.03304560445518153, 0.20567699292668448, 0.23040245451654423, 0.2266606948993787, 0.19352438439397243]
actor:  1 policy actor:  1  step number:  59 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.10990534896300155, 0.0336202314806467, 0.20322974301917882, 0.22752239049164655, 0.23443242264732703, 0.19128986339819937]
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.10990534896300155, 0.0336202314806467, 0.20322974301917882, 0.22752239049164655, 0.23443242264732703, 0.19128986339819937]
printing an ep nov before normalisation:  26.32945958124528
printing an ep nov before normalisation:  39.32357311248779
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.10990534896300155, 0.0336202314806467, 0.20322974301917882, 0.22752239049164655, 0.23443242264732703, 0.19128986339819937]
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.1093527776171148, 0.03364103580489898, 0.20335591989923013, 0.22766365955802684, 0.23457798467901497, 0.19140862244171433]
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.1093527776171148, 0.03364103580489898, 0.20335591989923013, 0.22766365955802684, 0.23457798467901497, 0.19140862244171433]
Printing some Q and Qe and total Qs values:  [[0.22 ]
 [0.175]
 [0.042]
 [0.156]
 [0.252]
 [0.02 ]
 [0.192]] [[35.577]
 [31.109]
 [35.782]
 [37.017]
 [35.509]
 [36.638]
 [31.957]] [[1.412]
 [1.062]
 [1.248]
 [1.446]
 [1.44 ]
 [1.285]
 [1.137]]
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.1093527776171148, 0.03364103580489898, 0.20335591989923013, 0.22766365955802684, 0.23457798467901497, 0.19140862244171433]
printing an ep nov before normalisation:  35.38166197422226
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.1093527776171148, 0.03364103580489898, 0.20335591989923013, 0.22766365955802684, 0.23457798467901497, 0.19140862244171433]
siam score:  -0.80596215
printing an ep nov before normalisation:  33.61428260803223
siam score:  -0.80576384
printing an ep nov before normalisation:  33.928746442416134
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.1093527776171148, 0.03364103580489898, 0.20335591989923013, 0.22766365955802684, 0.23457798467901497, 0.19140862244171433]
actor:  1 policy actor:  1  step number:  64 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  50.59928585123343
siam score:  -0.8055014
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
actor:  1 policy actor:  1  step number:  56 total reward:  0.20666666666666644  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
actor:  1 policy actor:  1  step number:  73 total reward:  0.119999999999999  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.756]
 [0.58 ]
 [0.567]
 [0.529]
 [0.531]
 [0.487]] [[39.531]
 [33.946]
 [38.733]
 [39.379]
 [41.224]
 [39.465]
 [40.843]] [[1.958]
 [1.765]
 [1.971]
 [2.009]
 [2.119]
 [1.981]
 [2.046]]
siam score:  -0.8047504
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.09958527864068037, 0.03463983311969279, 0.20089136435912544, 0.22708753076072688, 0.24147341513643492, 0.19632257798333957]
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.09958527864068037, 0.03463983311969279, 0.20089136435912544, 0.22708753076072688, 0.24147341513643492, 0.19632257798333957]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.097]
 [0.087]
 [0.097]
 [0.097]
 [0.098]
 [0.097]
 [0.097]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.097]
 [0.087]
 [0.097]
 [0.097]
 [0.098]
 [0.097]
 [0.097]]
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.09871451172229288, 0.03433770550422159, 0.19913359725924726, 0.22510039931232206, 0.23936032598310392, 0.2033534602188123]
printing an ep nov before normalisation:  22.175750255924527
printing an ep nov before normalisation:  21.897497177124023
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]] [[45.799]
 [45.799]
 [45.799]
 [45.799]
 [45.799]
 [45.799]
 [45.799]] [[2.12]
 [2.12]
 [2.12]
 [2.12]
 [2.12]
 [2.12]
 [2.12]]
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.09871451172229288, 0.03433770550422159, 0.19913359725924726, 0.22510039931232206, 0.23936032598310392, 0.2033534602188123]
printing an ep nov before normalisation:  16.712197254588126
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.51725898226243
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
actor:  1 policy actor:  1  step number:  67 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  2.0
siam score:  -0.7983075
actor:  1 policy actor:  1  step number:  57 total reward:  0.31999999999999984  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  36.370666910791414
printing an ep nov before normalisation:  42.303053282887504
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
printing an ep nov before normalisation:  38.92709600863018
actions average: 
K:  1  action  0 :  tensor([0.5220, 0.0289, 0.0695, 0.0676, 0.1151, 0.0879, 0.1090],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0085, 0.8933, 0.0061, 0.0119, 0.0043, 0.0039, 0.0721],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0979, 0.0125, 0.3568, 0.1019, 0.1136, 0.1637, 0.1535],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1233, 0.0204, 0.1044, 0.2849, 0.1164, 0.1283, 0.2222],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0993, 0.0118, 0.0847, 0.0738, 0.5172, 0.0933, 0.1199],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0616, 0.0291, 0.0765, 0.0739, 0.0917, 0.5353, 0.1320],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1036, 0.0470, 0.1009, 0.1130, 0.1177, 0.1300, 0.3878],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
printing an ep nov before normalisation:  37.30286093580285
actions average: 
K:  2  action  0 :  tensor([0.4831, 0.0120, 0.0779, 0.0740, 0.1444, 0.0911, 0.1176],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0079, 0.8992, 0.0103, 0.0198, 0.0112, 0.0099, 0.0417],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1560, 0.1184, 0.1354, 0.1329, 0.1327, 0.1510, 0.1737],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0964, 0.1453, 0.1229, 0.2455, 0.0893, 0.1618, 0.1387],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0970, 0.0059, 0.0960, 0.0965, 0.4388, 0.1355, 0.1302],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2073, 0.0186, 0.0965, 0.1512, 0.0963, 0.2558, 0.1744],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1525, 0.0283, 0.1179, 0.1136, 0.1327, 0.1723, 0.2826],
       grad_fn=<DivBackward0>)
siam score:  -0.8026488
siam score:  -0.80306154
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.0961444343948447, 0.033893837806117336, 0.19571945444711245, 0.2221810254561508, 0.23625590265220336, 0.21580534524357137]
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.575]
 [0.378]
 [0.475]
 [0.436]
 [0.403]
 [0.382]] [[34.722]
 [31.502]
 [34.722]
 [35.088]
 [34.037]
 [34.112]
 [37.388]] [[1.519]
 [1.477]
 [1.519]
 [1.644]
 [1.526]
 [1.499]
 [1.723]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  25.906507137887758
actor:  1 policy actor:  1  step number:  57 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.667
Starting evaluation
siam score:  -0.8011406
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.09595455439814844, 0.034957446155016274, 0.19271520544124088, 0.2194532517416213, 0.24371358897990145, 0.21320595328407166]
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[ 0.249]
 [ 0.249]
 [ 0.079]
 [ 0.249]
 [ 0.24 ]
 [-0.007]
 [ 0.241]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.249]
 [ 0.249]
 [ 0.079]
 [ 0.249]
 [ 0.24 ]
 [-0.007]
 [ 0.241]]
siam score:  -0.80145
maxi score, test score, baseline:  -0.9286066666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.09595455439814844, 0.034957446155016274, 0.19271520544124088, 0.2194532517416213, 0.24371358897990145, 0.21320595328407166]
printing an ep nov before normalisation:  52.16136726171011
Printing some Q and Qe and total Qs values:  [[0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]] [[44.311]
 [44.311]
 [44.311]
 [44.311]
 [44.311]
 [44.311]
 [44.311]] [[0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]]
siam score:  -0.80216706
Printing some Q and Qe and total Qs values:  [[0.554]
 [0.687]
 [0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.554]] [[42.976]
 [48.644]
 [42.976]
 [42.976]
 [42.976]
 [42.976]
 [42.976]] [[0.554]
 [0.687]
 [0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.554]]
printing an ep nov before normalisation:  46.339893948285884
actor:  0 policy actor:  0  step number:  84 total reward:  0.12666666666666626  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.43 ]] [[38.545]
 [38.545]
 [38.545]
 [38.545]
 [38.545]
 [38.545]
 [40.245]] [[0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.43 ]]
printing an ep nov before normalisation:  22.7253154754964
printing an ep nov before normalisation:  51.2302618976804
printing an ep nov before normalisation:  53.478525871357434
printing an ep nov before normalisation:  17.140672206878662
maxi score, test score, baseline:  -0.9263533333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.09595455439814844, 0.034957446155016274, 0.19271520544124088, 0.2194532517416213, 0.24371358897990145, 0.21320595328407166]
maxi score, test score, baseline:  -0.9263533333333334 -0.5663333333333334 -0.5663333333333334
printing an ep nov before normalisation:  23.324961110538744
printing an ep nov before normalisation:  32.64138702186667
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.75 ]
 [0.79 ]
 [0.75 ]
 [0.788]
 [0.779]
 [0.759]
 [0.796]] [[43.119]
 [48.303]
 [43.119]
 [55.509]
 [47.401]
 [53.359]
 [50.426]] [[0.75 ]
 [0.79 ]
 [0.75 ]
 [0.788]
 [0.779]
 [0.759]
 [0.796]]
printing an ep nov before normalisation:  28.90486836442637
printing an ep nov before normalisation:  39.513914754409555
printing an ep nov before normalisation:  34.52053464579377
printing an ep nov before normalisation:  13.37717237161911
printing an ep nov before normalisation:  27.18329379139046
printing an ep nov before normalisation:  36.26970237632149
printing an ep nov before normalisation:  19.439414606028308
maxi score, test score, baseline:  -0.9263533333333334 -0.5663333333333334 -0.5663333333333334
printing an ep nov before normalisation:  40.72018138128213
printing an ep nov before normalisation:  16.665503978729248
printing an ep nov before normalisation:  27.04553695614363
printing an ep nov before normalisation:  37.93040566819562
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9263533333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.09595455439814844, 0.034957446155016274, 0.19271520544124088, 0.2194532517416213, 0.24371358897990145, 0.21320595328407166]
printing an ep nov before normalisation:  39.00077729283149
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]] [[14.22]
 [14.22]
 [14.22]
 [14.22]
 [14.22]
 [14.22]
 [14.22]] [[0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]]
maxi score, test score, baseline:  -0.9263533333333334 -0.5663333333333334 -0.5663333333333334
probs:  [0.09595455439814844, 0.034957446155016274, 0.19271520544124088, 0.2194532517416213, 0.24371358897990145, 0.21320595328407166]
actor:  0 policy actor:  1  step number:  43 total reward:  0.56  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  45 total reward:  0.52  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  46 total reward:  0.5266666666666667  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  26.114977248020786
printing an ep nov before normalisation:  0.087673533087127
from probs:  [0.09595455439814844, 0.034957446155016274, 0.19271520544124088, 0.2194532517416213, 0.24371358897990145, 0.21320595328407166]
actor:  0 policy actor:  1  step number:  47 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  47 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[-0.005]
 [ 0.004]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.005]
 [ 0.004]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]]
maxi score, test score, baseline:  -0.9111133333333333 -0.5663333333333334 -0.5663333333333334
probs:  [0.09595455439814844, 0.034957446155016274, 0.19271520544124088, 0.2194532517416213, 0.24371358897990145, 0.21320595328407166]
printing an ep nov before normalisation:  18.860862289456573
actor:  0 policy actor:  1  step number:  48 total reward:  0.5  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  10.363318920135498
actor:  0 policy actor:  1  step number:  49 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.566]
 [0.504]
 [0.525]
 [0.504]
 [0.498]
 [0.497]] [[11.824]
 [15.861]
 [14.187]
 [15.34 ]
 [15.634]
 [18.744]
 [15.626]] [[0.602]
 [0.566]
 [0.504]
 [0.525]
 [0.504]
 [0.498]
 [0.497]]
actor:  0 policy actor:  1  step number:  50 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  50 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  51 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  14.187452793121338
maxi score, test score, baseline:  -0.8963266666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.09595455439814844, 0.034957446155016274, 0.19271520544124088, 0.2194532517416213, 0.24371358897990145, 0.21320595328407166]
actor:  0 policy actor:  1  step number:  52 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  18.751336434361292
actor:  0 policy actor:  1  step number:  54 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.8908866666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.09595455439814844, 0.034957446155016274, 0.19271520544124088, 0.2194532517416213, 0.24371358897990145, 0.21320595328407166]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.3179],
        [-0.0000],
        [-0.4154],
        [-0.0000],
        [-0.0000],
        [-0.4154],
        [-0.0000],
        [-0.3004],
        [-0.4527]], dtype=torch.float64)
-0.2718803999999996 -0.2718803999999996
-0.09703970119800001 -0.4149546971750959
0.99 0.99
-0.032346567066 -0.4477104239815174
0.9734999999999999 0.9734999999999999
-0.7043795356620002 -0.7043795356620002
-0.032346567066 -0.4477104239815174
-0.8184000000000001 -0.8184000000000001
-0.084359833866 -0.38477786211557746
-0.032346567066 -0.4850571412942415
maxi score, test score, baseline:  -0.8908866666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.09595455439814844, 0.034957446155016274, 0.19271520544124088, 0.2194532517416213, 0.24371358897990145, 0.21320595328407166]
printing an ep nov before normalisation:  44.44063186645508
printing an ep nov before normalisation:  21.263437999572222
printing an ep nov before normalisation:  32.816064855553506
printing an ep nov before normalisation:  12.857142238971306
actor:  1 policy actor:  1  step number:  68 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.023]
 [-0.021]
 [-0.024]
 [-0.024]
 [-0.025]
 [-0.025]
 [-0.024]] [[27.892]
 [42.16 ]
 [27.087]
 [27.482]
 [27.622]
 [28.955]
 [27.858]] [[0.672]
 [1.366]
 [0.632]
 [0.651]
 [0.657]
 [0.722]
 [0.669]]
printing an ep nov before normalisation:  12.870508696722691
printing an ep nov before normalisation:  21.41329983102596
printing an ep nov before normalisation:  28.595571390527514
printing an ep nov before normalisation:  37.53381668127504
using explorer policy with actor:  0
printing an ep nov before normalisation:  21.35896079128136
printing an ep nov before normalisation:  23.59328923501419
printing an ep nov before normalisation:  19.706947214460047
printing an ep nov before normalisation:  15.253037443808592
printing an ep nov before normalisation:  16.255556344985962
maxi score, test score, baseline:  -0.8908866666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.09495164052942061, 0.03441764010404999, 0.20187012603916094, 0.21751269978036042, 0.23993506091410882, 0.2113128326328992]
maxi score, test score, baseline:  -0.8908866666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.09495164052942061, 0.03441764010404999, 0.20187012603916094, 0.21751269978036042, 0.23993506091410882, 0.2113128326328992]
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.8908866666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.09495164052942061, 0.03441764010404999, 0.20187012603916094, 0.21751269978036042, 0.23993506091410882, 0.2113128326328992]
maxi score, test score, baseline:  -0.8908866666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.09495164052942061, 0.03441764010404999, 0.20187012603916094, 0.21751269978036042, 0.23993506091410882, 0.2113128326328992]
printing an ep nov before normalisation:  12.337642283840111
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  23.670544525527582
Printing some Q and Qe and total Qs values:  [[0.079]
 [0.12 ]
 [0.114]
 [0.13 ]
 [0.065]
 [0.128]
 [0.058]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.079]
 [0.12 ]
 [0.114]
 [0.13 ]
 [0.065]
 [0.128]
 [0.058]]
maxi score, test score, baseline:  -0.8908866666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.09495164052942061, 0.03441764010404999, 0.20187012603916094, 0.21751269978036042, 0.23993506091410882, 0.2113128326328992]
printing an ep nov before normalisation:  21.44749099853138
maxi score, test score, baseline:  -0.8908866666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.09495164052942061, 0.03441764010404999, 0.20187012603916094, 0.21751269978036042, 0.23993506091410882, 0.2113128326328992]
printing an ep nov before normalisation:  37.75963075255652
UNIT TEST: sample policy line 217 mcts : [0. 0. 0. 0. 0. 0. 1.]
printing an ep nov before normalisation:  23.670544525527582
printing an ep nov before normalisation:  12.64818397050922
maxi score, test score, baseline:  -0.8908866666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.09495164052942061, 0.03441764010404999, 0.20187012603916094, 0.21751269978036042, 0.23993506091410882, 0.2113128326328992]
printing an ep nov before normalisation:  18.59110267156714
siam score:  -0.8104573
actions average: 
K:  0  action  0 :  tensor([0.5746, 0.0019, 0.0646, 0.0740, 0.0804, 0.0793, 0.1252],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0058, 0.9659, 0.0035, 0.0040, 0.0049, 0.0025, 0.0134],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1959, 0.0136, 0.3327, 0.0961, 0.0959, 0.1214, 0.1444],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1018, 0.0208, 0.0872, 0.3188, 0.1075, 0.1045, 0.2594],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.1178,     0.0004,     0.0366,     0.0471,     0.7021,     0.0441,
            0.0519], grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0923, 0.0017, 0.1108, 0.1055, 0.1156, 0.4377, 0.1363],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1254, 0.0206, 0.1223, 0.1243, 0.1314, 0.1131, 0.3630],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]] [[10.892]
 [10.758]
 [10.758]
 [10.758]
 [10.758]
 [10.758]
 [10.758]] [[0.607]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]]
using explorer policy with actor:  0
using explorer policy with actor:  1
printing an ep nov before normalisation:  18.704347757558715
maxi score, test score, baseline:  -0.8908866666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.09495164052942061, 0.03441764010404999, 0.20187012603916094, 0.21751269978036042, 0.23993506091410882, 0.2113128326328992]
actor:  1 policy actor:  1  step number:  59 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8908866666666667 -0.5663333333333334 -0.5663333333333334
probs:  [0.09372907723100724, 0.03397558603667975, 0.21215241047890443, 0.21470986692368907, 0.23684312012266215, 0.20858993920705735]
actor:  1 policy actor:  1  step number:  45 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  10.637967586517334
printing an ep nov before normalisation:  57.09948972339548
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.1090789560291715, 0.036832417369112205, 0.20953473784648788, 0.20244509351032455, 0.2569133417935347, 0.1851954534513693]
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.1090789560291715, 0.036832417369112205, 0.20953473784648788, 0.20244509351032455, 0.2569133417935347, 0.1851954534513693]
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.1090789560291715, 0.036832417369112205, 0.20953473784648788, 0.20244509351032455, 0.2569133417935347, 0.1851954534513693]
printing an ep nov before normalisation:  38.15234235519258
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.527]
 [0.505]
 [0.51 ]
 [0.473]
 [0.449]
 [0.454]] [[31.432]
 [29.003]
 [31.772]
 [35.048]
 [35.354]
 [35.934]
 [30.687]] [[0.913]
 [0.875]
 [0.919]
 [1.002]
 [0.972]
 [0.961]
 [0.842]]
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.1090789560291715, 0.036832417369112205, 0.20953473784648788, 0.20244509351032455, 0.2569133417935347, 0.1851954534513693]
printing an ep nov before normalisation:  31.940690371476826
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.1090789560291715, 0.036832417369112205, 0.20953473784648788, 0.20244509351032455, 0.2569133417935347, 0.1851954534513693]
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
actor:  1 policy actor:  1  step number:  65 total reward:  0.0933333333333326  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  26.542380201249024
printing an ep nov before normalisation:  30.549532729821692
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
Printing some Q and Qe and total Qs values:  [[-0.194]
 [-0.105]
 [-0.096]
 [-0.102]
 [-0.162]
 [-0.098]
 [-0.112]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.194]
 [-0.105]
 [-0.096]
 [-0.102]
 [-0.162]
 [-0.098]
 [-0.112]]
printing an ep nov before normalisation:  34.309289313940866
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.10588791274467785, 0.03713553405081745, 0.21013106064611445, 0.20281799444092133, 0.2590027460150142, 0.18502475210245475]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.10588791274467785, 0.03713553405081745, 0.21013106064611445, 0.20281799444092133, 0.2590027460150142, 0.18502475210245475]
printing an ep nov before normalisation:  37.00867692181007
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.1060129408282736, 0.037088005925565944, 0.20969534675309232, 0.20318629974253502, 0.25866900721139024, 0.18534839953914287]
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.1060129408282736, 0.037088005925565944, 0.20969534675309232, 0.20318629974253502, 0.25866900721139024, 0.18534839953914287]
printing an ep nov before normalisation:  32.64805707347677
printing an ep nov before normalisation:  56.49809248501412
Printing some Q and Qe and total Qs values:  [[-0.126]
 [-0.13 ]
 [-0.126]
 [-0.126]
 [-0.13 ]
 [-0.126]
 [-0.126]] [[ 0.   ]
 [31.677]
 [ 0.   ]
 [ 0.   ]
 [33.284]
 [ 0.   ]
 [ 0.   ]] [[-0.301]
 [ 0.604]
 [-0.301]
 [-0.301]
 [ 0.649]
 [-0.301]
 [-0.301]]
using another actor
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.10609955505704156, 0.037118236548356505, 0.2090493879689427, 0.20335240602143345, 0.2588805007673445, 0.1854999136368814]
siam score:  -0.81455195
printing an ep nov before normalisation:  18.91662717459425
from probs:  [0.10609955505704156, 0.037118236548356505, 0.2090493879689427, 0.20335240602143345, 0.2588805007673445, 0.1854999136368814]
Printing some Q and Qe and total Qs values:  [[-0.005]
 [ 0.032]
 [ 0.018]
 [ 0.016]
 [ 0.011]
 [ 0.002]
 [ 0.012]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.005]
 [ 0.032]
 [ 0.018]
 [ 0.016]
 [ 0.011]
 [ 0.002]
 [ 0.012]]
actions average: 
K:  3  action  0 :  tensor([0.6482, 0.0031, 0.0600, 0.0664, 0.0637, 0.0676, 0.0911],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0067, 0.9172, 0.0094, 0.0187, 0.0069, 0.0106, 0.0305],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.2409, 0.0624, 0.1174, 0.1262, 0.1090, 0.1752, 0.1689],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1503, 0.1146, 0.0724, 0.2299, 0.2101, 0.0916, 0.1312],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0869, 0.0475, 0.0599, 0.1572, 0.3557, 0.0870, 0.2057],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1173, 0.0084, 0.1351, 0.1180, 0.1235, 0.3311, 0.1666],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1961, 0.0986, 0.1089, 0.1243, 0.1363, 0.1466, 0.1892],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.279]
 [0.271]
 [0.28 ]
 [0.28 ]
 [0.283]
 [0.283]
 [0.281]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.279]
 [0.271]
 [0.28 ]
 [0.28 ]
 [0.283]
 [0.283]
 [0.281]]
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.10613777177041263, 0.03704048745542342, 0.20926067560413153, 0.20355411635193568, 0.25833533701514644, 0.18567161180295022]
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.465]
 [0.465]
 [0.494]
 [0.465]
 [0.426]
 [0.465]] [[50.588]
 [50.588]
 [50.588]
 [48.061]
 [50.588]
 [52.565]
 [50.588]] [[0.465]
 [0.465]
 [0.465]
 [0.494]
 [0.465]
 [0.426]
 [0.465]]
printing an ep nov before normalisation:  47.11741709875172
actions average: 
K:  2  action  0 :  tensor([0.6652, 0.0124, 0.0490, 0.0744, 0.0653, 0.0664, 0.0673],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0033, 0.9608, 0.0048, 0.0085, 0.0070, 0.0050, 0.0106],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1091, 0.0435, 0.2977, 0.1056, 0.1530, 0.1830, 0.1081],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1386, 0.0035, 0.1061, 0.3004, 0.1338, 0.1434, 0.1742],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1055, 0.0193, 0.0700, 0.0926, 0.4776, 0.0935, 0.1416],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0562, 0.0025, 0.1062, 0.0547, 0.0823, 0.5885, 0.1095],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1140, 0.1540, 0.1024, 0.1230, 0.1326, 0.1423, 0.2317],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  43.61138227601982
actions average: 
K:  3  action  0 :  tensor([0.6716, 0.0164, 0.0536, 0.0429, 0.0917, 0.0497, 0.0742],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0145, 0.8827, 0.0106, 0.0182, 0.0170, 0.0104, 0.0465],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1251, 0.0131, 0.2704, 0.1251, 0.1332, 0.1427, 0.1904],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1174, 0.0119, 0.1077, 0.3341, 0.1309, 0.1236, 0.1744],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0861, 0.0295, 0.0711, 0.1140, 0.4518, 0.1061, 0.1414],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0608, 0.0160, 0.0984, 0.0920, 0.1426, 0.5035, 0.0865],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0871, 0.1556, 0.0621, 0.1734, 0.0830, 0.1083, 0.3304],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.10613777177041263, 0.03704048745542342, 0.20926067560413153, 0.20355411635193568, 0.25833533701514644, 0.18567161180295022]
printing an ep nov before normalisation:  37.38370017468378
printing an ep nov before normalisation:  31.425999123519595
printing an ep nov before normalisation:  28.311465806037468
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.09891413468423
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.328]
 [0.409]
 [0.408]
 [0.406]
 [0.404]
 [0.401]] [[16.615]
 [26.503]
 [17.2  ]
 [19.368]
 [19.448]
 [18.63 ]
 [20.286]] [[0.496]
 [0.328]
 [0.409]
 [0.408]
 [0.406]
 [0.404]
 [0.401]]
printing an ep nov before normalisation:  34.86721222303431
printing an ep nov before normalisation:  28.8741049074458
printing an ep nov before normalisation:  51.04811268194006
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.10622410921273374, 0.037070547284383235, 0.2086171852847859, 0.20371979641140572, 0.25854563469434655, 0.18582272711234482]
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.787667963162754
printing an ep nov before normalisation:  39.057673015865326
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.10622410921273374, 0.037070547284383235, 0.2086171852847859, 0.20371979641140572, 0.25854563469434655, 0.18582272711234482]
printing an ep nov before normalisation:  30.788857565818677
actions average: 
K:  2  action  0 :  tensor([0.6784, 0.0007, 0.0422, 0.0453, 0.0661, 0.0606, 0.1067],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0038, 0.9612, 0.0043, 0.0080, 0.0040, 0.0061, 0.0126],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1480, 0.0066, 0.2384, 0.1349, 0.1443, 0.1791, 0.1486],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1166, 0.0834, 0.0933, 0.2116, 0.1294, 0.1798, 0.1859],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0955, 0.0190, 0.0352, 0.0467, 0.6726, 0.0637, 0.0673],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0562, 0.0499, 0.0745, 0.0874, 0.0873, 0.5295, 0.1152],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0941, 0.0804, 0.0778, 0.1149, 0.1602, 0.0994, 0.3732],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[-0.053]
 [-0.053]
 [-0.053]
 [-0.053]
 [-0.018]
 [-0.053]
 [-0.053]] [[27.528]
 [27.528]
 [27.528]
 [27.528]
 [32.204]
 [27.528]
 [27.528]] [[0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.554]
 [0.354]
 [0.354]]
printing an ep nov before normalisation:  35.7413387298584
printing an ep nov before normalisation:  35.237998962402344
siam score:  -0.8165782
actor:  1 policy actor:  1  step number:  65 total reward:  0.13333333333333286  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.10588021148092894, 0.03695081351975223, 0.2079413764047567, 0.2062988104384336, 0.25770798079892865, 0.1852208073571999]
actor:  1 policy actor:  1  step number:  79 total reward:  0.01333333333333242  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.10587508367117612, 0.036949028186761546, 0.2071256954757513, 0.2062888131993309, 0.2576954906618409, 0.18606588880513916]
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.10587508367117612, 0.036949028186761546, 0.2071256954757513, 0.2062888131993309, 0.2576954906618409, 0.18606588880513916]
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.597]
 [0.585]
 [0.559]
 [0.509]
 [0.572]
 [0.592]] [[2.765]
 [2.305]
 [2.879]
 [2.14 ]
 [2.403]
 [2.889]
 [2.358]] [[0.699]
 [0.68 ]
 [0.689]
 [0.637]
 [0.596]
 [0.677]
 [0.677]]
actor:  1 policy actor:  1  step number:  80 total reward:  0.01999999999999924  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.10476615930120083, 0.0370640830452555, 0.20728441687305227, 0.20643705692882822, 0.25848733995513323, 0.18596094389652998]
Printing some Q and Qe and total Qs values:  [[-0.068]
 [-0.06 ]
 [-0.059]
 [-0.056]
 [-0.135]
 [-0.058]
 [-0.056]] [[32.82 ]
 [39.519]
 [38.424]
 [41.314]
 [41.332]
 [45.735]
 [40.294]] [[0.521]
 [0.794]
 [0.753]
 [0.87 ]
 [0.791]
 [1.043]
 [0.83 ]]
printing an ep nov before normalisation:  50.523063871595596
printing an ep nov before normalisation:  39.607775186840094
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.10485106340355062, 0.037094050485966555, 0.20664171054841093, 0.20660446159852186, 0.2586969806823447, 0.18611173328120523]
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
printing an ep nov before normalisation:  40.560764256930206
printing an ep nov before normalisation:  32.82589861945135
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.10485106340355062, 0.037094050485966555, 0.20664171054841093, 0.20660446159852186, 0.2586969806823447, 0.18611173328120523]
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.10485106340355062, 0.037094050485966555, 0.20664171054841093, 0.20660446159852186, 0.2586969806823447, 0.18611173328120523]
printing an ep nov before normalisation:  39.446992574526256
printing an ep nov before normalisation:  38.41959347591708
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.10485106340355062, 0.037094050485966555, 0.20664171054841093, 0.20660446159852186, 0.2586969806823447, 0.18611173328120523]
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.10488742096513935, 0.03701574444903063, 0.20685032602685885, 0.20681301404144845, 0.2581478881552942, 0.18628560636222855]
Printing some Q and Qe and total Qs values:  [[0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.609]
 [0.708]] [[24.239]
 [24.239]
 [24.239]
 [24.239]
 [24.239]
 [37.008]
 [24.239]] [[1.429]
 [1.429]
 [1.429]
 [1.429]
 [1.429]
 [1.958]
 [1.429]]
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.10492368933415618, 0.03693763051289914, 0.2070584297286571, 0.2070210548623644, 0.2576001426650787, 0.1864590528968444]
printing an ep nov before normalisation:  32.980832689104474
actor:  1 policy actor:  1  step number:  90 total reward:  0.0466666666666653  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  31.933566599051094
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
siam score:  -0.8086088
line 256 mcts: sample exp_bonus 44.334989844026715
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.10494533256299904, 0.03689101585000309, 0.20718261606962474, 0.2071452036790275, 0.25727327434114416, 0.18656255749720146]
line 256 mcts: sample exp_bonus 41.264300136080806
Printing some Q and Qe and total Qs values:  [[-0.128]
 [-0.128]
 [-0.128]
 [-0.128]
 [-0.128]
 [-0.128]
 [-0.128]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.128]
 [-0.128]
 [-0.128]
 [-0.128]
 [-0.128]
 [-0.128]
 [-0.128]]
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.10516538992156398, 0.036795105407378785, 0.2073618331657403, 0.20732443572010048, 0.2566032241909254, 0.18675001159429103]
Printing some Q and Qe and total Qs values:  [[-0.108]
 [-0.067]
 [-0.108]
 [-0.108]
 [-0.108]
 [-0.108]
 [-0.093]] [[38.042]
 [44.642]
 [38.042]
 [38.042]
 [38.042]
 [38.042]
 [43.835]] [[0.803]
 [1.133]
 [0.803]
 [0.803]
 [0.803]
 [0.803]
 [1.072]]
Printing some Q and Qe and total Qs values:  [[-0.109]
 [-0.109]
 [-0.109]
 [-0.109]
 [-0.109]
 [-0.088]
 [-0.109]] [[38.952]
 [38.952]
 [38.952]
 [38.952]
 [38.952]
 [40.947]
 [38.952]] [[0.82 ]
 [0.82 ]
 [0.82 ]
 [0.82 ]
 [0.82 ]
 [0.924]
 [0.82 ]]
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.1052504464499693, 0.03682479471424581, 0.20672048303360363, 0.20749222208590612, 0.2568109172152713, 0.18690113650100387]
printing an ep nov before normalisation:  30.008790493011475
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
printing an ep nov before normalisation:  19.279981473686433
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.1052504464499693, 0.03682479471424581, 0.20672048303360363, 0.20749222208590612, 0.2568109172152713, 0.18690113650100387]
printing an ep nov before normalisation:  33.73513761732556
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.1052504464499693, 0.03682479471424581, 0.20672048303360363, 0.20749222208590612, 0.2568109172152713, 0.18690113650100387]
printing an ep nov before normalisation:  29.455866813659668
siam score:  -0.80599433
Printing some Q and Qe and total Qs values:  [[-0.045]
 [ 0.013]
 [-0.032]
 [-0.036]
 [-0.016]
 [-0.036]
 [-0.003]] [[26.323]
 [35.757]
 [25.593]
 [24.784]
 [31.42 ]
 [24.819]
 [30.949]] [[0.319]
 [0.699]
 [0.306]
 [0.275]
 [0.521]
 [0.276]
 [0.518]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.1052504464499693, 0.03682479471424581, 0.20672048303360363, 0.20749222208590612, 0.2568109172152713, 0.18690113650100387]
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.1052504464499693, 0.03682479471424581, 0.20672048303360363, 0.20749222208590612, 0.2568109172152713, 0.18690113650100387]
printing an ep nov before normalisation:  30.58121681213379
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.1052504464499693, 0.03682479471424581, 0.20672048303360363, 0.20749222208590612, 0.2568109172152713, 0.18690113650100387]
printing an ep nov before normalisation:  35.39958161500694
actor:  1 policy actor:  1  step number:  69 total reward:  0.09333333333333282  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  79 total reward:  0.14666666666666628  reward:  1.0 rdn_beta:  1.333
UNIT TEST: sample policy line 217 mcts : [0.061 0.306 0.061 0.082 0.306 0.061 0.122]
actor:  1 policy actor:  1  step number:  60 total reward:  0.23333333333333295  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.127]
 [-0.127]
 [-0.127]
 [-0.127]
 [-0.127]
 [-0.127]
 [-0.127]] [[22.13]
 [22.13]
 [22.13]
 [22.13]
 [22.13]
 [22.13]
 [22.13]] [[-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]]
actions average: 
K:  0  action  0 :  tensor([0.5337, 0.0024, 0.0789, 0.0848, 0.0921, 0.0829, 0.1251],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0031, 0.9735, 0.0017, 0.0034, 0.0031, 0.0022, 0.0129],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1578, 0.0027, 0.1818, 0.1590, 0.1759, 0.1321, 0.1907],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1486, 0.0059, 0.0944, 0.2714, 0.1877, 0.1204, 0.1716],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1052, 0.0021, 0.0594, 0.0836, 0.5450, 0.0820, 0.1227],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1101, 0.0053, 0.1369, 0.0968, 0.0916, 0.4490, 0.1103],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1468, 0.0090, 0.0919, 0.1103, 0.1231, 0.1195, 0.3995],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  33.60508807762334
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.1034995946947933, 0.03621365320620822, 0.21631952123727594, 0.2076412886663506, 0.252535646368561, 0.18379029582681106]
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.49 ]
 [0.458]
 [0.474]
 [0.472]
 [0.467]
 [0.472]] [[41.092]
 [42.976]
 [41.778]
 [41.802]
 [41.867]
 [42.21 ]
 [42.826]] [[1.237]
 [1.319]
 [1.242]
 [1.259]
 [1.26 ]
 [1.267]
 [1.296]]
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.1034995946947933, 0.03621365320620822, 0.21631952123727594, 0.2076412886663506, 0.252535646368561, 0.18379029582681106]
printing an ep nov before normalisation:  43.191434811628056
printing an ep nov before normalisation:  39.38403779418297
printing an ep nov before normalisation:  43.0280211631743
printing an ep nov before normalisation:  35.526514542910185
printing an ep nov before normalisation:  39.71851348876953
maxi score, test score, baseline:  -0.8908866666666667 -0.11333333333333336 -0.11333333333333336
siam score:  -0.8000061
actor:  0 policy actor:  0  step number:  62 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  62 total reward:  0.12666666666666593  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8885266666666668 -0.11333333333333336 -0.11333333333333336
printing an ep nov before normalisation:  58.486664082370304
siam score:  -0.8024977
printing an ep nov before normalisation:  45.12635388099626
printing an ep nov before normalisation:  47.09368885882382
printing an ep nov before normalisation:  34.65154074094082
printing an ep nov before normalisation:  34.214088962200776
Printing some Q and Qe and total Qs values:  [[0.251]
 [0.322]
 [0.251]
 [0.251]
 [0.253]
 [0.251]
 [0.251]] [[34.557]
 [30.753]
 [34.557]
 [34.557]
 [36.323]
 [34.557]
 [34.557]] [[1.069]
 [0.955]
 [1.069]
 [1.069]
 [1.156]
 [1.069]
 [1.069]]
Printing some Q and Qe and total Qs values:  [[-0.118]
 [-0.112]
 [-0.119]
 [-0.119]
 [-0.119]
 [-0.119]
 [-0.119]] [[11.965]
 [30.233]
 [11.583]
 [11.555]
 [11.777]
 [11.683]
 [11.517]] [[-0.097]
 [ 0.781]
 [-0.116]
 [-0.117]
 [-0.106]
 [-0.112]
 [-0.119]]
maxi score, test score, baseline:  -0.8885266666666668 -0.11333333333333336 -0.11333333333333336
probs:  [0.10268528843699197, 0.036063807910416086, 0.21977662063620074, 0.20685014897586818, 0.2514895014749905, 0.18313463256553258]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8885266666666668 -0.11333333333333336 -0.11333333333333336
probs:  [0.10268528843699197, 0.036063807910416086, 0.21977662063620074, 0.20685014897586818, 0.2514895014749905, 0.18313463256553258]
maxi score, test score, baseline:  -0.8885266666666668 -0.11333333333333336 -0.11333333333333336
probs:  [0.10268528843699197, 0.036063807910416086, 0.21977662063620074, 0.20685014897586818, 0.2514895014749905, 0.18313463256553258]
actor:  1 policy actor:  1  step number:  62 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  32.23487389403151
maxi score, test score, baseline:  -0.8885266666666668 -0.11333333333333336 -0.11333333333333336
actor:  1 policy actor:  1  step number:  66 total reward:  0.2733333333333331  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8885266666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.10108104513095093, 0.0354157043681513, 0.22461258388573435, 0.21156046656623617, 0.24695440561805596, 0.18037579443087118]
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.442]] [[33.967]
 [35.896]
 [35.896]
 [35.896]
 [35.896]
 [35.896]
 [35.896]] [[0.5  ]
 [0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.442]]
maxi score, test score, baseline:  -0.8885266666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.10108104513095093, 0.0354157043681513, 0.22461258388573435, 0.21156046656623617, 0.24695440561805596, 0.18037579443087118]
printing an ep nov before normalisation:  49.34594664519999
maxi score, test score, baseline:  -0.8885266666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.10108104513095093, 0.0354157043681513, 0.22461258388573435, 0.21156046656623617, 0.24695440561805596, 0.18037579443087118]
printing an ep nov before normalisation:  35.02838936878746
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  54.70519833961412
printing an ep nov before normalisation:  31.083459750412434
printing an ep nov before normalisation:  38.05846773656801
maxi score, test score, baseline:  -0.8885266666666667 -0.11333333333333336 -0.11333333333333336
Printing some Q and Qe and total Qs values:  [[0.565]
 [0.612]
 [0.576]
 [0.573]
 [0.571]
 [0.573]
 [0.571]] [[44.202]
 [38.979]
 [43.254]
 [43.442]
 [42.28 ]
 [43.593]
 [41.411]] [[0.565]
 [0.612]
 [0.576]
 [0.573]
 [0.571]
 [0.573]
 [0.571]]
actor:  0 policy actor:  0  step number:  61 total reward:  0.4800000000000001  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  35.29675483703613
printing an ep nov before normalisation:  34.72630687235062
printing an ep nov before normalisation:  42.23301509336286
actions average: 
K:  1  action  0 :  tensor([    0.6894,     0.0006,     0.0337,     0.0449,     0.1110,     0.0442,
            0.0761], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0148, 0.8714, 0.0158, 0.0166, 0.0161, 0.0157, 0.0496],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1673, 0.0073, 0.2373, 0.1217, 0.1264, 0.1588, 0.1812],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1332, 0.0029, 0.1335, 0.1492, 0.1941, 0.1785, 0.2085],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1217, 0.0041, 0.0629, 0.0588, 0.5765, 0.0943, 0.0817],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0899, 0.0084, 0.0995, 0.1091, 0.1405, 0.4027, 0.1499],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1603, 0.0176, 0.1049, 0.1379, 0.1506, 0.1360, 0.2927],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.8855666666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.10125211104099263, 0.03547549396280388, 0.22329974321891516, 0.21191875061602752, 0.2473726681447583, 0.18068123301650243]
printing an ep nov before normalisation:  39.383510744768046
maxi score, test score, baseline:  -0.8855666666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.10128049922606308, 0.035399480079445315, 0.2235218480439139, 0.21212279128355976, 0.2468396885461357, 0.1808356928208823]
printing an ep nov before normalisation:  35.40130771741889
actor:  1 policy actor:  1  step number:  55 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  68 total reward:  0.08666666666666611  reward:  1.0 rdn_beta:  1.667
from probs:  [0.10046637991733082, 0.03511562726682583, 0.22172382635241064, 0.2104165188996052, 0.24485398503458478, 0.18742366252924272]
printing an ep nov before normalisation:  39.21928684005848
printing an ep nov before normalisation:  27.172137250879924
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.8833933333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.10046637991733082, 0.03511562726682583, 0.22172382635241064, 0.2104165188996052, 0.24485398503458478, 0.18742366252924272]
Printing some Q and Qe and total Qs values:  [[-0.04 ]
 [ 0.235]
 [ 0.118]
 [ 0.099]
 [-0.03 ]
 [ 0.123]
 [ 0.176]] [[34.294]
 [38.41 ]
 [35.708]
 [34.483]
 [35.089]
 [37.962]
 [33.316]] [[0.931]
 [1.446]
 [1.172]
 [1.081]
 [0.988]
 [1.308]
 [1.091]]
maxi score, test score, baseline:  -0.8833933333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.10046637991733082, 0.03511562726682583, 0.22172382635241064, 0.2104165188996052, 0.24485398503458478, 0.18742366252924272]
actions average: 
K:  3  action  0 :  tensor([0.6187, 0.0064, 0.0750, 0.0632, 0.0697, 0.0863, 0.0808],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0116, 0.8887, 0.0087, 0.0114, 0.0089, 0.0100, 0.0607],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0797, 0.0073, 0.4880, 0.0761, 0.0903, 0.1034, 0.1553],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1204, 0.0165, 0.1357, 0.1460, 0.1440, 0.2023, 0.2350],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0826, 0.0825, 0.0719, 0.0828, 0.4639, 0.1080, 0.1083],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0740, 0.0722, 0.0708, 0.0764, 0.0920, 0.4984, 0.1162],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0787, 0.1107, 0.1016, 0.1152, 0.1275, 0.1767, 0.2895],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  57.98279371807718
printing an ep nov before normalisation:  49.85328532708336
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8833933333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.10046637991733082, 0.03511562726682583, 0.22172382635241064, 0.2104165188996052, 0.24485398503458478, 0.18742366252924272]
maxi score, test score, baseline:  -0.8833933333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.10046637991733082, 0.03511562726682583, 0.22172382635241064, 0.2104165188996052, 0.24485398503458478, 0.18742366252924272]
printing an ep nov before normalisation:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  1 policy actor:  1  step number:  77 total reward:  0.05333333333333279  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  32.64247265984202
printing an ep nov before normalisation:  31.83867362932807
maxi score, test score, baseline:  -0.8833933333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.1004496076242358, 0.03516295435990469, 0.22158811850662102, 0.21029190184370866, 0.2451858196136754, 0.18732159805185442]
Printing some Q and Qe and total Qs values:  [[0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]] [[36.977]
 [36.977]
 [36.977]
 [36.977]
 [36.977]
 [36.977]
 [36.977]] [[0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]]
maxi score, test score, baseline:  -0.8833933333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.1004496076242358, 0.03516295435990469, 0.22158811850662102, 0.21029190184370866, 0.2451858196136754, 0.18732159805185442]
maxi score, test score, baseline:  -0.8833933333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.1004496076242358, 0.03516295435990469, 0.22158811850662102, 0.21029190184370866, 0.2451858196136754, 0.18732159805185442]
maxi score, test score, baseline:  -0.8833933333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.1004496076242358, 0.03516295435990469, 0.22158811850662102, 0.21029190184370866, 0.2451858196136754, 0.18732159805185442]
maxi score, test score, baseline:  -0.8833933333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.1004496076242358, 0.03516295435990469, 0.22158811850662102, 0.21029190184370866, 0.2451858196136754, 0.18732159805185442]
Printing some Q and Qe and total Qs values:  [[-0.114]
 [-0.114]
 [-0.114]
 [-0.114]
 [-0.114]
 [-0.114]
 [-0.114]] [[41.436]
 [41.436]
 [41.436]
 [41.436]
 [41.436]
 [41.436]
 [41.436]] [[1.694]
 [1.694]
 [1.694]
 [1.694]
 [1.694]
 [1.694]
 [1.694]]
maxi score, test score, baseline:  -0.8833933333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.10053320396563274, 0.035192145575164785, 0.2209400020326991, 0.21046703272659864, 0.24539002853783054, 0.18747758716207408]
printing an ep nov before normalisation:  41.270554079501665
maxi score, test score, baseline:  -0.8833933333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.10053320396563274, 0.035192145575164785, 0.2209400020326991, 0.21046703272659864, 0.24539002853783054, 0.18747758716207408]
maxi score, test score, baseline:  -0.8833933333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.10053320396563274, 0.035192145575164785, 0.2209400020326991, 0.21046703272659864, 0.24539002853783054, 0.18747758716207408]
maxi score, test score, baseline:  -0.8833933333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.10053320396563274, 0.035192145575164785, 0.2209400020326991, 0.21046703272659864, 0.24539002853783054, 0.18747758716207408]
actor:  1 policy actor:  1  step number:  59 total reward:  0.2799999999999997  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8833933333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.09973166987213752, 0.034912255882978006, 0.21917721120969363, 0.21676487939784855, 0.24343204315029832, 0.18598194048704383]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 41.05520327835263
maxi score, test score, baseline:  -0.8833933333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.09973166987213752, 0.034912255882978006, 0.21917721120969363, 0.21676487939784855, 0.24343204315029832, 0.18598194048704383]
UNIT TEST: sample policy line 217 mcts : [0.082 0.    0.    0.    0.755 0.02  0.143]
printing an ep nov before normalisation:  32.13352374045583
actions average: 
K:  2  action  0 :  tensor([0.4905, 0.0051, 0.0852, 0.0849, 0.1072, 0.0962, 0.1309],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0139, 0.8910, 0.0096, 0.0158, 0.0121, 0.0093, 0.0484],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2320, 0.0154, 0.3025, 0.0690, 0.0751, 0.1347, 0.1713],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0901, 0.1595, 0.0857, 0.1886, 0.1134, 0.1659, 0.1967],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1079, 0.0372, 0.0577, 0.0711, 0.5039, 0.0903, 0.1319],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0572, 0.0135, 0.0738, 0.0674, 0.0723, 0.6187, 0.0971],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1784, 0.1121, 0.0762, 0.0999, 0.0911, 0.0963, 0.3460],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  39.82729074872961
maxi score, test score, baseline:  -0.8833933333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.09989498309330876, 0.034969283634604054, 0.2178979753867255, 0.21712009391619752, 0.24383098425990196, 0.1862866797092622]
maxi score, test score, baseline:  -0.8833933333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.09989498309330876, 0.034969283634604054, 0.2178979753867255, 0.21712009391619752, 0.24383098425990196, 0.1862866797092622]
printing an ep nov before normalisation:  49.33359127229067
maxi score, test score, baseline:  -0.8833933333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.09989498309330877, 0.034969283634604054, 0.21789797538672553, 0.21712009391619752, 0.24383098425990196, 0.1862866797092622]
maxi score, test score, baseline:  -0.8833933333333334 -0.11333333333333336 -0.11333333333333336
printing an ep nov before normalisation:  56.34116364594283
maxi score, test score, baseline:  -0.8833933333333334 -0.11333333333333336 -0.11333333333333336
maxi score, test score, baseline:  -0.8833933333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.09989498309330877, 0.034969283634604054, 0.21789797538672553, 0.21712009391619752, 0.24383098425990196, 0.1862866797092622]
actor:  1 policy actor:  1  step number:  71 total reward:  0.10666666666666613  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  34.85022160677148
maxi score, test score, baseline:  -0.8833933333333334 -0.11333333333333336 -0.11333333333333336
maxi score, test score, baseline:  -0.8833933333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.09964603845552386, 0.034882354034719745, 0.21735456708619696, 0.21657862673447065, 0.24322286294972553, 0.18831555073936315]
printing an ep nov before normalisation:  42.96752968076599
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.149]
 [-0.149]
 [-0.149]
 [-0.149]
 [-0.149]
 [-0.149]
 [-0.149]] [[44.481]
 [44.481]
 [44.481]
 [44.481]
 [44.481]
 [44.481]
 [44.481]] [[1.851]
 [1.851]
 [1.851]
 [1.851]
 [1.851]
 [1.851]
 [1.851]]
using explorer policy with actor:  1
using another actor
from probs:  [0.09967123520401502, 0.03480731147711928, 0.21756194959722658, 0.21678480826808336, 0.2426967079296795, 0.18847798752387612]
maxi score, test score, baseline:  -0.8833933333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.09967123520401502, 0.03480731147711928, 0.21756194959722658, 0.21678480826808336, 0.2426967079296795, 0.18847798752387612]
printing an ep nov before normalisation:  35.083061502081904
maxi score, test score, baseline:  -0.8833933333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.09967123520401502, 0.03480731147711928, 0.21756194959722658, 0.21678480826808336, 0.2426967079296795, 0.18847798752387612]
actor:  1 policy actor:  1  step number:  74 total reward:  0.059999999999999054  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  42 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8805266666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09898970372983704, 0.03469304419737359, 0.21925244825144996, 0.21674339898191985, 0.24189615975113726, 0.18842524508828218]
printing an ep nov before normalisation:  49.44025988667934
maxi score, test score, baseline:  -0.8805266666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.0990136587937731, 0.03461841220235436, 0.21946080403496457, 0.21694790760152177, 0.2413728843212863, 0.18858633304609987]
Printing some Q and Qe and total Qs values:  [[-0.115]
 [-0.082]
 [-0.115]
 [-0.115]
 [-0.115]
 [-0.086]
 [-0.115]] [[41.192]
 [38.703]
 [41.192]
 [41.192]
 [41.192]
 [49.706]
 [41.192]] [[0.642]
 [0.578]
 [0.642]
 [0.642]
 [0.642]
 [1.003]
 [0.642]]
printing an ep nov before normalisation:  46.83150759528221
maxi score, test score, baseline:  -0.8805266666666667 -0.11333333333333336 -0.11333333333333336
maxi score, test score, baseline:  -0.8805266666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09850598453399635, 0.034564289554019224, 0.21979824550961077, 0.21728002583793055, 0.2409930835208726, 0.18885837104357064]
Printing some Q and Qe and total Qs values:  [[-0.124]
 [-0.124]
 [-0.124]
 [-0.124]
 [-0.124]
 [-0.124]
 [-0.124]] [[42.057]
 [42.057]
 [42.057]
 [42.057]
 [42.057]
 [42.057]
 [41.557]] [[1.082]
 [1.082]
 [1.082]
 [1.082]
 [1.082]
 [1.082]
 [1.053]]
maxi score, test score, baseline:  -0.8805266666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09850598453399635, 0.034564289554019224, 0.21979824550961077, 0.21728002583793055, 0.2409930835208726, 0.18885837104357064]
printing an ep nov before normalisation:  41.25621811034689
maxi score, test score, baseline:  -0.8805266666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09850598453399635, 0.034564289554019224, 0.21979824550961077, 0.21728002583793055, 0.2409930835208726, 0.18885837104357064]
maxi score, test score, baseline:  -0.8805266666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09869081597781197, 0.03455117973892679, 0.21974940884797756, 0.2172360404966953, 0.2409034152546214, 0.188869139683967]
maxi score, test score, baseline:  -0.8805266666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09869081597781197, 0.03455117973892679, 0.21974940884797756, 0.2172360404966953, 0.2409034152546214, 0.188869139683967]
actor:  1 policy actor:  1  step number:  75 total reward:  0.0666666666666661  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8805266666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09852585039591766, 0.034493571007963436, 0.2193818155296198, 0.2185451149569724, 0.24050041438415445, 0.18855323372537228]
using another actor
printing an ep nov before normalisation:  28.76758652586915
actor:  1 policy actor:  1  step number:  71 total reward:  0.18666666666666631  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[-0.113]
 [-0.109]
 [-0.113]
 [-0.113]
 [-0.113]
 [-0.113]
 [-0.113]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.113]
 [-0.109]
 [-0.113]
 [-0.113]
 [-0.113]
 [-0.113]
 [-0.113]]
maxi score, test score, baseline:  -0.8805266666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09807049680602072, 0.03433455394420781, 0.21836714971136706, 0.22215854600170684, 0.23938801329423337, 0.18768124024246438]
Printing some Q and Qe and total Qs values:  [[0.44 ]
 [0.55 ]
 [0.471]
 [0.485]
 [0.515]
 [0.507]
 [0.48 ]] [[36.928]
 [38.54 ]
 [38.916]
 [39.117]
 [37.936]
 [38.038]
 [38.786]] [[0.44 ]
 [0.55 ]
 [0.471]
 [0.485]
 [0.515]
 [0.507]
 [0.48 ]]
actions average: 
K:  4  action  0 :  tensor([0.4468, 0.0177, 0.0831, 0.0749, 0.1179, 0.1194, 0.1401],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0126, 0.8590, 0.0084, 0.0255, 0.0248, 0.0108, 0.0588],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0856, 0.2052, 0.1159, 0.1110, 0.1064, 0.2219, 0.1541],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1681, 0.0460, 0.1192, 0.1262, 0.1217, 0.2340, 0.1848],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1087, 0.0425, 0.0533, 0.0706, 0.5522, 0.0802, 0.0926],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0497, 0.0414, 0.1594, 0.0743, 0.0704, 0.4882, 0.1167],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1895, 0.0128, 0.1068, 0.1053, 0.1396, 0.1337, 0.3122],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  34.93515302832762
actor:  0 policy actor:  0  step number:  55 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  1  action  0 :  using explorer policy with actor:  1
tensor([0.6382, 0.0035, 0.0429, 0.0506, 0.1273, 0.0613, 0.0763],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0071, 0.9266, 0.0075, 0.0160, 0.0100, 0.0063, 0.0265],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1484, 0.0376, 0.3368, 0.0729, 0.1169, 0.1314, 0.1560],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1463, 0.0706, 0.1067, 0.2058, 0.1455, 0.1404, 0.1846],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1092, 0.0133, 0.0663, 0.0861, 0.4994, 0.1080, 0.1177],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0852, 0.0040, 0.0848, 0.0843, 0.1126, 0.5122, 0.1170],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1367, 0.0359, 0.0980, 0.1174, 0.1312, 0.1328, 0.3480],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.8776999999999999 -0.11333333333333336 -0.11333333333333336
probs:  [0.0980926874639469, 0.03426055597575263, 0.2185708889474061, 0.22236800711436466, 0.23886919139606438, 0.18783866910246533]
maxi score, test score, baseline:  -0.8776999999999999 -0.11333333333333336 -0.11333333333333336
probs:  [0.0980926874639469, 0.03426055597575263, 0.2185708889474061, 0.22236800711436466, 0.23886919139606438, 0.18783866910246533]
printing an ep nov before normalisation:  52.21750036186403
maxi score, test score, baseline:  -0.8776999999999999 -0.11333333333333336 -0.11333333333333336
probs:  [0.0980926874639469, 0.03426055597575263, 0.2185708889474061, 0.22236800711436466, 0.23886919139606438, 0.18783866910246533]
printing an ep nov before normalisation:  58.578360137650904
using explorer policy with actor:  1
printing an ep nov before normalisation:  24.676457384490675
actor:  0 policy actor:  1  step number:  66 total reward:  0.01999999999999924  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.87566 -0.11333333333333336 -0.11333333333333336
probs:  [0.09819455132775312, 0.03421445999355705, 0.2181389350502333, 0.22275793526066617, 0.23854555848289774, 0.18814855988489268]
printing an ep nov before normalisation:  41.67014911565165
Printing some Q and Qe and total Qs values:  [[-0.253]
 [-0.   ]
 [-0.253]
 [-0.253]
 [-0.253]
 [-0.   ]
 [-0.253]] [[31.74 ]
 [ 0.001]
 [31.74 ]
 [31.74 ]
 [31.74 ]
 [ 0.001]
 [31.74 ]] [[0.035]
 [0.   ]
 [0.035]
 [0.035]
 [0.035]
 [0.   ]
 [0.035]]
line 256 mcts: sample exp_bonus 29.075163831370368
Printing some Q and Qe and total Qs values:  [[-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.028]
 [-0.04 ]
 [-0.04 ]] [[33.454]
 [33.454]
 [33.454]
 [33.454]
 [36.235]
 [33.454]
 [33.454]] [[1.082]
 [1.082]
 [1.082]
 [1.082]
 [1.239]
 [1.082]
 [1.082]]
printing an ep nov before normalisation:  35.35106658935547
maxi score, test score, baseline:  -0.87566 -0.11333333333333336 -0.11333333333333336
probs:  [0.09766934155321157, 0.03423432351136247, 0.21826599501422475, 0.222887687310131, 0.23868451201086419, 0.18825814060020604]
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  -0.87566 -0.11333333333333336 -0.11333333333333336
probs:  [0.0977698774304653, 0.03418826471891885, 0.2178351006139871, 0.2232775955656534, 0.23836113970131734, 0.18856802196965802]
printing an ep nov before normalisation:  33.53612092755271
printing an ep nov before normalisation:  33.068361276607995
siam score:  -0.8364222
maxi score, test score, baseline:  -0.87566 -0.11333333333333336 -0.11333333333333336
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.250927653608144
printing an ep nov before normalisation:  0.19333337047953592
maxi score, test score, baseline:  -0.87566 -0.11333333333333336 -0.11333333333333336
probs:  [0.09779139680762672, 0.03411478892723814, 0.21803600544021123, 0.22348663184222622, 0.23784597715376046, 0.1887251998289373]
printing an ep nov before normalisation:  38.58280641580797
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[ 0.2584],
        [-0.1738],
        [ 0.4110],
        [ 0.1961],
        [-0.4344],
        [-0.3654],
        [-0.0000],
        [-0.3766],
        [ 0.2361],
        [-0.1338]], dtype=torch.float64)
-0.070771701198 0.1876655989399302
-0.083839701198 -0.2576769567452838
-0.08397170119799999 0.3270683770039592
-0.058226434398 0.13791720806157198
-0.032346567066 -0.4667953263264699
-0.032346567066 -0.397701642297436
0.943965 0.943965
-0.045026434398 -0.42162137938625854
-0.083839701198 0.15226196749242574
-0.032346567066 -0.1661592661571911
maxi score, test score, baseline:  -0.87566 -0.11333333333333336 -0.11333333333333336
printing an ep nov before normalisation:  31.939093490018333
maxi score, test score, baseline:  -0.87566 -0.11333333333333336 -0.11333333333333336
probs:  [0.09779139680762672, 0.03411478892723814, 0.21803600544021123, 0.22348663184222622, 0.23784597715376046, 0.1887251998289373]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8756600000000001 -0.11333333333333336 -0.11333333333333336
maxi score, test score, baseline:  -0.8756600000000001 -0.11333333333333336 -0.11333333333333336
probs:  [0.09779139680762672, 0.03411478892723814, 0.21803600544021123, 0.22348663184222622, 0.23784597715376046, 0.1887251998289373]
maxi score, test score, baseline:  -0.8756600000000001 -0.11333333333333336 -0.11333333333333336
probs:  [0.09787024214010635, 0.03414222429251644, 0.21740521654087724, 0.22366695852124088, 0.23803789699855565, 0.18887746150670334]
maxi score, test score, baseline:  -0.8756600000000001 -0.11333333333333336 -0.11333333333333336
probs:  [0.09787024214010635, 0.03414222429251644, 0.21740521654087724, 0.22366695852124088, 0.23803789699855565, 0.18887746150670334]
maxi score, test score, baseline:  -0.8756600000000001 -0.11333333333333336 -0.11333333333333336
probs:  [0.09787024214010635, 0.03414222429251644, 0.21740521654087724, 0.22366695852124088, 0.23803789699855565, 0.18887746150670334]
printing an ep nov before normalisation:  39.69520168380171
maxi score, test score, baseline:  -0.8756600000000001 -0.11333333333333336 -0.11333333333333336
probs:  [0.09789184093234835, 0.03406891911055388, 0.21760482721094362, 0.22387589419810108, 0.2375239299980979, 0.18903458854995506]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8756600000000001 -0.11333333333333336 -0.11333333333333336
probs:  [0.09801373563663093, 0.03395011906207314, 0.21737248005896032, 0.22447291131432187, 0.23669054493265063, 0.18950020899536305]
printing an ep nov before normalisation:  59.61718559265137
printing an ep nov before normalisation:  44.18058323365864
maxi score, test score, baseline:  -0.8756600000000001 -0.11333333333333336 -0.11333333333333336
Printing some Q and Qe and total Qs values:  [[-0.101]
 [-0.101]
 [-0.101]
 [-0.101]
 [-0.101]
 [-0.101]
 [-0.101]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.101]
 [-0.101]
 [-0.101]
 [-0.101]
 [-0.101]
 [-0.101]
 [-0.101]]
maxi score, test score, baseline:  -0.8756600000000001 -0.11333333333333336 -0.11333333333333336
probs:  [0.09809221611885124, 0.033977233408391254, 0.21674550429673298, 0.2246527866217373, 0.23688021632499534, 0.18965204322929188]
Printing some Q and Qe and total Qs values:  [[-0.11]
 [-0.11]
 [-0.11]
 [-0.11]
 [-0.11]
 [-0.11]
 [-0.11]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.11]
 [-0.11]
 [-0.11]
 [-0.11]
 [-0.11]
 [-0.11]
 [-0.11]]
printing an ep nov before normalisation:  50.37990737755311
printing an ep nov before normalisation:  56.39560073024194
line 256 mcts: sample exp_bonus 19.509453677123577
Printing some Q and Qe and total Qs values:  [[-0.146]
 [-0.146]
 [-0.146]
 [-0.146]
 [-0.146]
 [-0.146]
 [-0.146]] [[23.183]
 [23.183]
 [23.183]
 [23.183]
 [23.183]
 [23.183]
 [23.183]] [[0.854]
 [0.854]
 [0.854]
 [0.854]
 [0.854]
 [0.854]
 [0.854]]
maxi score, test score, baseline:  -0.8756600000000001 -0.11333333333333336 -0.11333333333333336
probs:  [0.09809221611885124, 0.033977233408391254, 0.21674550429673298, 0.2246527866217373, 0.23688021632499534, 0.18965204322929188]
printing an ep nov before normalisation:  25.190908251956408
Printing some Q and Qe and total Qs values:  [[-0.103]
 [-0.103]
 [-0.103]
 [-0.103]
 [-0.103]
 [-0.103]
 [-0.103]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.103]
 [-0.103]
 [-0.103]
 [-0.103]
 [-0.103]
 [-0.103]
 [-0.103]]
maxi score, test score, baseline:  -0.8756600000000001 -0.11333333333333336 -0.11333333333333336
printing an ep nov before normalisation:  40.962868063119366
printing an ep nov before normalisation:  32.02667705579529
using another actor
maxi score, test score, baseline:  -0.8756600000000001 -0.11333333333333336 -0.11333333333333336
probs:  [0.09723870554517199, 0.03392507417181407, 0.21614237779204237, 0.22569085413141232, 0.2365130015795482, 0.19048998678001106]
printing an ep nov before normalisation:  47.500589783272304
maxi score, test score, baseline:  -0.8756600000000001 -0.11333333333333336 -0.11333333333333336
probs:  [0.09723870554517199, 0.03392507417181407, 0.21614237779204237, 0.22569085413141232, 0.2365130015795482, 0.19048998678001106]
maxi score, test score, baseline:  -0.8756600000000001 -0.11333333333333336 -0.11333333333333336
probs:  [0.09723870554517199, 0.03392507417181407, 0.21614237779204237, 0.22569085413141232, 0.2365130015795482, 0.19048998678001106]
printing an ep nov before normalisation:  35.70283915377187
printing an ep nov before normalisation:  40.89665104730084
printing an ep nov before normalisation:  30.459758533611897
printing an ep nov before normalisation:  34.47657965808463
Printing some Q and Qe and total Qs values:  [[0.079]
 [0.279]
 [0.079]
 [0.079]
 [0.079]
 [0.079]
 [0.079]] [[27.652]
 [30.173]
 [27.652]
 [27.652]
 [27.652]
 [27.652]
 [27.652]] [[0.897]
 [1.265]
 [0.897]
 [0.897]
 [0.897]
 [0.897]
 [0.897]]
printing an ep nov before normalisation:  30.333246283906306
maxi score, test score, baseline:  -0.8756600000000001 -0.11333333333333336 -0.11333333333333336
probs:  [0.09723870554517199, 0.03392507417181407, 0.21614237779204237, 0.22569085413141232, 0.2365130015795482, 0.19048998678001106]
maxi score, test score, baseline:  -0.8756600000000001 -0.11333333333333336 -0.11333333333333336
printing an ep nov before normalisation:  32.418689985505324
maxi score, test score, baseline:  -0.8756600000000001 -0.11333333333333336 -0.11333333333333336
probs:  [0.09723870554517199, 0.03392507417181407, 0.21614237779204237, 0.22569085413141232, 0.2365130015795482, 0.19048998678001106]
printing an ep nov before normalisation:  37.08118035707238
maxi score, test score, baseline:  -0.8756600000000001 -0.11333333333333336 -0.11333333333333336
probs:  [0.09723870554517199, 0.03392507417181407, 0.21614237779204237, 0.22569085413141232, 0.2365130015795482, 0.19048998678001106]
printing an ep nov before normalisation:  38.67008149970985
printing an ep nov before normalisation:  31.10336855539309
printing an ep nov before normalisation:  0.045196713060704496
printing an ep nov before normalisation:  36.14470958709717
printing an ep nov before normalisation:  32.83811275614745
maxi score, test score, baseline:  -0.8756600000000001 -0.11333333333333336 -0.11333333333333336
probs:  [0.09723870554517199, 0.03392507417181407, 0.21614237779204237, 0.22569085413141232, 0.2365130015795482, 0.19048998678001106]
maxi score, test score, baseline:  -0.8756600000000001 -0.11333333333333336 -0.11333333333333336
probs:  [0.09723870554517199, 0.03392507417181407, 0.21614237779204237, 0.22569085413141232, 0.2365130015795482, 0.19048998678001106]
actor:  0 policy actor:  1  step number:  73 total reward:  0.2933333333333332  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  95 total reward:  0.06666666666666532  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.655]
 [0.657]
 [0.658]
 [0.658]
 [0.656]
 [0.656]
 [0.656]] [[4.227]
 [4.556]
 [3.269]
 [3.183]
 [3.806]
 [4.379]
 [2.97 ]] [[0.887]
 [0.907]
 [0.837]
 [0.832]
 [0.865]
 [0.897]
 [0.819]]
actor:  1 policy actor:  1  step number:  80 total reward:  0.006666666666665488  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.09703349765892544, 0.03385366364310267, 0.21610138912724852, 0.22691026102091894, 0.2360134726698861, 0.19008771587991827]
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.09703349765892544, 0.03385366364310267, 0.21610138912724852, 0.22691026102091894, 0.2360134726698861, 0.19008771587991827]
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.09703349765892544, 0.03385366364310267, 0.21610138912724852, 0.22691026102091894, 0.2360134726698861, 0.19008771587991827]
printing an ep nov before normalisation:  31.608593463897705
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.09703349765892544, 0.03385366364310267, 0.21610138912724852, 0.22691026102091894, 0.2360134726698861, 0.19008771587991827]
printing an ep nov before normalisation:  47.50174178719257
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
printing an ep nov before normalisation:  49.644322164645786
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.09703349765892544, 0.03385366364310267, 0.21610138912724852, 0.22691026102091894, 0.2360134726698861, 0.19008771587991827]
printing an ep nov before normalisation:  42.88372664025963
printing an ep nov before normalisation:  48.92502069565899
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.0970933180833673, 0.03363694294375424, 0.21668237534813325, 0.22753855818566648, 0.234493966417307, 0.1905548390217717]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.0971131570359567, 0.03356506930327723, 0.216875054657897, 0.22774692778149905, 0.2339900346450822, 0.19070975657628775]
printing an ep nov before normalisation:  32.585508094044116
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.36738528586623
printing an ep nov before normalisation:  40.48420853084988
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.0971131570359567, 0.03356506930327723, 0.216875054657897, 0.22774692778149905, 0.2339900346450822, 0.19070975657628775]
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.0971131570359567, 0.03356506930327723, 0.216875054657897, 0.22774692778149905, 0.2339900346450822, 0.19070975657628775]
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.09715268431024157, 0.033421867736148914, 0.2172589503243506, 0.22816208488870957, 0.23298599729523606, 0.1910184154453133]
printing an ep nov before normalisation:  38.15389949051364
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.09715268431024157, 0.033421867736148914, 0.2172589503243506, 0.22816208488870957, 0.23298599729523606, 0.1910184154453133]
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.09715268431024157, 0.033421867736148914, 0.2172589503243506, 0.22816208488870957, 0.23298599729523606, 0.1910184154453133]
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.14962703477397
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
printing an ep nov before normalisation:  54.16194754835469
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.09717237301268614, 0.03335053843009899, 0.21745017037894102, 0.22836887639911585, 0.23248588204615805, 0.1911721597329999]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.81408028902256
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.344]
 [0.344]
 [0.344]
 [0.344]
 [0.358]
 [0.35 ]
 [0.344]] [[41.501]
 [41.501]
 [41.501]
 [41.501]
 [47.879]
 [47.359]
 [41.501]] [[1.476]
 [1.476]
 [1.476]
 [1.476]
 [1.774]
 [1.743]
 [1.476]]
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.09717237301268614, 0.03335053843009899, 0.21745017037894102, 0.22836887639911585, 0.23248588204615805, 0.1911721597329999]
actor:  1 policy actor:  1  step number:  71 total reward:  0.0666666666666661  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  36.41450699985337
using another actor
printing an ep nov before normalisation:  37.36962788135328
Printing some Q and Qe and total Qs values:  [[0.199]
 [0.199]
 [0.199]
 [0.199]
 [0.199]
 [0.199]
 [0.199]] [[41.741]
 [41.741]
 [41.741]
 [41.741]
 [41.741]
 [41.741]
 [41.741]] [[0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]]
printing an ep nov before normalisation:  37.775690612963686
printing an ep nov before normalisation:  41.80413590681388
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.09957488478636946, 0.03326207867760901, 0.2168714419366343, 0.22776107076834143, 0.23186711257508055, 0.19066341125596517]
printing an ep nov before normalisation:  30.340865226718773
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.09957488478636946, 0.03326207867760901, 0.2168714419366343, 0.22776107076834143, 0.23186711257508055, 0.19066341125596517]
actor:  1 policy actor:  1  step number:  68 total reward:  0.23333333333333295  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.113]
 [-0.113]
 [-0.113]
 [-0.113]
 [-0.113]
 [-0.113]
 [-0.113]] [[32.208]
 [44.191]
 [32.208]
 [32.208]
 [32.208]
 [32.208]
 [32.208]] [[0.663]
 [1.226]
 [0.663]
 [0.663]
 [0.663]
 [0.663]
 [0.663]]
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
Printing some Q and Qe and total Qs values:  [[-0.144]
 [-0.144]
 [-0.144]
 [-0.144]
 [-0.144]
 [-0.144]
 [-0.145]] [[36.844]
 [36.844]
 [36.844]
 [36.844]
 [36.844]
 [36.844]
 [40.755]] [[0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]
 [1.09 ]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.036]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]] [[39.189]
 [39.189]
 [39.189]
 [39.189]
 [39.189]
 [39.189]
 [39.189]] [[1.618]
 [1.618]
 [1.618]
 [1.618]
 [1.618]
 [1.618]
 [1.618]]
actor:  1 policy actor:  1  step number:  74 total reward:  0.13999999999999901  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.1054356797834148, 0.0338007299898051, 0.2133657030413894, 0.22401546443154624, 0.23564746228123365, 0.18773496047261076]
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.10546711896389366, 0.03373011917180343, 0.21355089759883455, 0.2242158304679526, 0.23515239191150114, 0.18788364188601467]
printing an ep nov before normalisation:  43.38542428673511
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.10554909234165165, 0.03375626508453859, 0.21293938232325105, 0.22439021695968553, 0.23533528949936613, 0.18802975379150696]
Printing some Q and Qe and total Qs values:  [[0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]] [[24.966]
 [24.966]
 [24.966]
 [24.966]
 [24.966]
 [24.966]
 [24.966]] [[0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]
 [0.72]]
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.10554909234165165, 0.03375626508453859, 0.21293938232325105, 0.22439021695968553, 0.23533528949936613, 0.18802975379150696]
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
actor:  1 policy actor:  1  step number:  78 total reward:  0.21999999999999942  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.1052449008538125, 0.03443658061761716, 0.21116252986767162, 0.2224563372978287, 0.2401051601341838, 0.18659449122888613]
Printing some Q and Qe and total Qs values:  [[-0.113]
 [-0.108]
 [-0.113]
 [-0.113]
 [-0.12 ]
 [-0.104]
 [-0.12 ]] [[47.682]
 [47.05 ]
 [47.682]
 [47.682]
 [44.038]
 [47.17 ]
 [50.787]] [[0.624]
 [0.612]
 [0.624]
 [0.624]
 [0.518]
 [0.619]
 [0.701]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.1052449008538125, 0.03443658061761716, 0.21116252986767162, 0.2224563372978287, 0.2401051601341838, 0.18659449122888613]
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.10479337585099265, 0.03448318209381521, 0.21068611095230236, 0.22275836564132379, 0.2404311645938497, 0.1868478008677163]
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.10479337585099265, 0.03448318209381521, 0.21068611095230236, 0.22275836564132379, 0.2404311645938497, 0.1868478008677163]
actor:  1 policy actor:  1  step number:  73 total reward:  0.29333333333333256  reward:  1.0 rdn_beta:  1.333
from probs:  [0.10391646024846686, 0.03419537425057535, 0.20892195228349186, 0.22926505590382282, 0.23841778115457227, 0.18528337615907084]
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.1039457098551355, 0.03412538621715811, 0.20910066165654925, 0.22947272067202953, 0.23792708207919222, 0.18542843951993548]
printing an ep nov before normalisation:  36.68415546417236
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.1039457098551355, 0.03412538621715811, 0.20910066165654925, 0.22947272067202953, 0.23792708207919222, 0.18542843951993548]
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.1039457098551355, 0.03412538621715811, 0.20910066165654925, 0.22947272067202953, 0.23792708207919222, 0.18542843951993548]
printing an ep nov before normalisation:  40.19264615957299
printing an ep nov before normalisation:  39.05412630297311
printing an ep nov before normalisation:  52.58393306085565
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.1039457098551355, 0.03412538621715811, 0.20910066165654925, 0.22947272067202953, 0.23792708207919222, 0.18542843951993548]
printing an ep nov before normalisation:  22.55632930165339
printing an ep nov before normalisation:  19.04620260264814
maxi score, test score, baseline:  -0.8730733333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.1039457098551355, 0.03412538621715811, 0.20910066165654925, 0.22947272067202953, 0.23792708207919222, 0.18542843951993548]
printing an ep nov before normalisation:  43.73507437260124
printing an ep nov before normalisation:  16.149775981903076
actor:  0 policy actor:  1  step number:  61 total reward:  0.1599999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8707533333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.1039457098551355, 0.03412538621715811, 0.20910066165654925, 0.22947272067202953, 0.23792708207919222, 0.18542843951993548]
printing an ep nov before normalisation:  29.083059532489617
printing an ep nov before normalisation:  34.95337123896324
Printing some Q and Qe and total Qs values:  [[0.94]
 [0.94]
 [0.94]
 [0.94]
 [0.94]
 [0.94]
 [0.94]] [[40.538]
 [40.538]
 [40.538]
 [40.538]
 [40.538]
 [40.538]
 [40.538]] [[0.94]
 [0.94]
 [0.94]
 [0.94]
 [0.94]
 [0.94]
 [0.94]]
actions average: 
K:  4  action  0 :  tensor([0.4048, 0.0148, 0.0875, 0.0898, 0.1135, 0.1324, 0.1571],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0117, 0.8968, 0.0100, 0.0136, 0.0120, 0.0114, 0.0445],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1625, 0.0789, 0.1752, 0.1104, 0.1343, 0.1663, 0.1724],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1766, 0.0237, 0.0839, 0.2008, 0.1584, 0.1634, 0.1932],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0931, 0.0605, 0.0695, 0.1077, 0.4293, 0.1318, 0.1081],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1298, 0.0118, 0.0867, 0.0991, 0.1513, 0.4271, 0.0941],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1209, 0.0614, 0.0980, 0.1356, 0.1728, 0.1705, 0.2408],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  39.22049121668441
Printing some Q and Qe and total Qs values:  [[0.73 ]
 [0.651]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.576]
 [0.73 ]] [[27.027]
 [33.56 ]
 [27.027]
 [27.027]
 [27.027]
 [39.67 ]
 [27.027]] [[1.29 ]
 [1.452]
 [1.29 ]
 [1.29 ]
 [1.29 ]
 [1.601]
 [1.29 ]]
maxi score, test score, baseline:  -0.8707533333333335 -0.11333333333333336 -0.11333333333333336
actor:  1 policy actor:  1  step number:  76 total reward:  0.17999999999999927  reward:  1.0 rdn_beta:  1.333
from probs:  [0.10342119982796585, 0.03414529850699299, 0.20922307283004726, 0.22960706560760777, 0.23806637950079537, 0.18553698372659078]
maxi score, test score, baseline:  -0.8707533333333335 -0.11333333333333336 -0.11333333333333336
probs:  [0.10299967995450628, 0.033927643768576814, 0.20849019918541872, 0.23316598616611184, 0.23654267741609022, 0.18487381350929621]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8707533333333335 -0.11333333333333336 -0.11333333333333336
probs:  [0.10299967995450628, 0.033927643768576814, 0.20849019918541872, 0.23316598616611184, 0.23654267741609022, 0.18487381350929621]
printing an ep nov before normalisation:  47.69410418828285
printing an ep nov before normalisation:  46.20000607646733
printing an ep nov before normalisation:  48.79412389685247
maxi score, test score, baseline:  -0.8707533333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.10307707483656543, 0.03395307008432731, 0.2078951825358367, 0.23334131585202286, 0.23672054766418557, 0.18501280902706208]
printing an ep nov before normalisation:  1.919350995028266
maxi score, test score, baseline:  -0.8707533333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.10310482760193193, 0.03388349452592936, 0.20807052181270666, 0.23355248391964106, 0.23623274278881656, 0.1851559293509743]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  57 total reward:  0.33333333333333315  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8707533333333334 -0.11333333333333336 -0.11333333333333336
from probs:  [0.10254493742316886, 0.03451764540175316, 0.20570001426150342, 0.24068026542568383, 0.23337644788652462, 0.1831806896013662]
maxi score, test score, baseline:  -0.8707533333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.10254493742316886, 0.03451764540175316, 0.20570001426150342, 0.24068026542568383, 0.23337644788652462, 0.1831806896013662]
using explorer policy with actor:  0
printing an ep nov before normalisation:  41.72288738938094
printing an ep nov before normalisation:  35.40534347384408
Printing some Q and Qe and total Qs values:  [[-0.001]
 [ 0.006]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.007]] [[34.288]
 [34.779]
 [34.288]
 [34.288]
 [34.288]
 [34.288]
 [35.307]] [[0.855]
 [0.885]
 [0.855]
 [0.855]
 [0.855]
 [0.855]
 [0.898]]
maxi score, test score, baseline:  -0.8707533333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.10261549571646106, 0.03454133513293599, 0.20584164300287652, 0.24084599440720425, 0.23284872847026528, 0.1833068032702569]
printing an ep nov before normalisation:  50.32567974458384
printing an ep nov before normalisation:  45.56176705140302
maxi score, test score, baseline:  -0.8707533333333334 -0.11333333333333336 -0.11333333333333336
probs:  [0.10261549571646106, 0.03454133513293599, 0.20584164300287652, 0.24084599440720425, 0.23284872847026528, 0.1833068032702569]
printing an ep nov before normalisation:  0.0
siam score:  -0.85646653
actor:  1 policy actor:  1  step number:  42 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  28.367622242756376
using another actor
from probs:  [0.09998492860777614, 0.0336581302713178, 0.22620940578307205, 0.23466725641055003, 0.22687526837139455, 0.1786050105558893]
actor:  0 policy actor:  1  step number:  56 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.8679666666666668 -0.11333333333333336 -0.11333333333333336
probs:  [0.10006445825241973, 0.03368483210627957, 0.22559362946216172, 0.2348540575134747, 0.22705586333032463, 0.1787471593353395]
maxi score, test score, baseline:  -0.8679666666666668 -0.11333333333333336 -0.11333333333333336
probs:  [0.10006445825241973, 0.03368483210627957, 0.22559362946216172, 0.2348540575134747, 0.22705586333032463, 0.1787471593353395]
actions average: 
K:  3  action  0 :  tensor([0.5836, 0.0218, 0.0589, 0.0598, 0.1004, 0.0885, 0.0870],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0077, 0.9134, 0.0083, 0.0241, 0.0198, 0.0140, 0.0127],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1093, 0.1103, 0.3657, 0.1050, 0.1030, 0.0913, 0.1155],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1920, 0.0022, 0.0895, 0.2524, 0.1758, 0.1149, 0.1732],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1133, 0.0252, 0.0674, 0.0912, 0.5365, 0.0884, 0.0781],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0832, 0.0101, 0.0880, 0.0877, 0.1204, 0.4720, 0.1385],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0954, 0.1047, 0.1327, 0.1230, 0.1426, 0.1312, 0.2705],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  39.544717143300005
maxi score, test score, baseline:  -0.8679666666666668 -0.11333333333333336 -0.11333333333333336
probs:  [0.10013128506150229, 0.033707269003051106, 0.2257444011384571, 0.23501102189697606, 0.22653941943301456, 0.17886660346699895]
Printing some Q and Qe and total Qs values:  [[0.898]
 [0.922]
 [0.864]
 [0.881]
 [0.889]
 [0.88 ]
 [0.899]] [[27.345]
 [34.575]
 [27.547]
 [27.982]
 [27.808]
 [29.259]
 [27.671]] [[0.898]
 [0.922]
 [0.864]
 [0.881]
 [0.889]
 [0.88 ]
 [0.899]]
maxi score, test score, baseline:  -0.8679666666666668 -0.11333333333333336 -0.11333333333333336
probs:  [0.10013128506150229, 0.033707269003051106, 0.2257444011384571, 0.23501102189697606, 0.22653941943301456, 0.17886660346699895]
Printing some Q and Qe and total Qs values:  [[0.755]
 [0.788]
 [0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]] [[45.445]
 [38.685]
 [45.445]
 [45.445]
 [45.445]
 [45.445]
 [45.445]] [[0.755]
 [0.788]
 [0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.8679666666666668 -0.11333333333333336 -0.11333333333333336
probs:  [0.10013128506150229, 0.033707269003051106, 0.2257444011384571, 0.23501102189697606, 0.22653941943301456, 0.17886660346699895]
printing an ep nov before normalisation:  55.02799851310444
maxi score, test score, baseline:  -0.8679666666666668 -0.11333333333333336 -0.11333333333333336
probs:  [0.10013128506150229, 0.033707269003051106, 0.2257444011384571, 0.23501102189697606, 0.22653941943301456, 0.17886660346699895]
printing an ep nov before normalisation:  53.0745253656694
Printing some Q and Qe and total Qs values:  [[0.394]
 [0.386]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]] [[40.74 ]
 [31.883]
 [40.74 ]
 [40.74 ]
 [40.74 ]
 [40.74 ]
 [40.74 ]] [[2.964]
 [2.053]
 [2.964]
 [2.964]
 [2.964]
 [2.964]
 [2.964]]
siam score:  -0.855959
maxi score, test score, baseline:  -0.8679666666666668 -0.11333333333333336 -0.11333333333333336
maxi score, test score, baseline:  -0.8679666666666668 -0.11333333333333336 -0.11333333333333336
probs:  [0.10013128506150229, 0.033707269003051106, 0.2257444011384571, 0.23501102189697606, 0.22653941943301456, 0.17886660346699895]
maxi score, test score, baseline:  -0.8679666666666668 -0.11333333333333336 -0.11333333333333336
probs:  [0.10013128506150229, 0.033707269003051106, 0.2257444011384571, 0.23501102189697606, 0.22653941943301456, 0.17886660346699895]
printing an ep nov before normalisation:  21.27257157597312
maxi score, test score, baseline:  -0.8679666666666668 -0.11333333333333336 -0.11333333333333336
probs:  [0.10013128506150229, 0.033707269003051106, 0.2257444011384571, 0.23501102189697606, 0.22653941943301456, 0.17886660346699895]
actor:  1 policy actor:  1  step number:  54 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8563341
maxi score, test score, baseline:  -0.8679666666666668 -0.11333333333333336 -0.11333333333333336
printing an ep nov before normalisation:  35.55493354797363
Printing some Q and Qe and total Qs values:  [[-0.024]
 [-0.023]
 [-0.027]
 [-0.026]
 [-0.026]
 [-0.031]
 [-0.026]] [[22.206]
 [35.318]
 [23.185]
 [23.244]
 [22.522]
 [23.617]
 [22.854]] [[0.062]
 [0.163]
 [0.068]
 [0.068]
 [0.063]
 [0.066]
 [0.066]]
maxi score, test score, baseline:  -0.8679666666666668 -0.11333333333333336 -0.11333333333333336
probs:  [0.09921252268665971, 0.03366985218282925, 0.22315894397420186, 0.23230261078478898, 0.23475292707070577, 0.17690314330081436]
maxi score, test score, baseline:  -0.8679666666666668 -0.11333333333333336 -0.11333333333333336
probs:  [0.09921252268665971, 0.03366985218282925, 0.22315894397420186, 0.23230261078478898, 0.23475292707070577, 0.17690314330081436]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.537]
 [0.405]
 [0.437]
 [0.473]
 [0.462]
 [0.542]] [[31.152]
 [40.811]
 [34.09 ]
 [29.962]
 [34.085]
 [35.11 ]
 [37.613]] [[0.597]
 [0.782]
 [0.586]
 [0.578]
 [0.654]
 [0.652]
 [0.756]]
printing an ep nov before normalisation:  34.38385198807989
maxi score, test score, baseline:  -0.8679666666666668 -0.11333333333333336 -0.11333333333333336
maxi score, test score, baseline:  -0.8679666666666668 -0.11333333333333336 -0.11333333333333336
actor:  1 policy actor:  1  step number:  69 total reward:  0.09333333333333305  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  23.867428193221745
maxi score, test score, baseline:  -0.8679666666666668 -0.11333333333333336 -0.11333333333333336
probs:  [0.10235474886966729, 0.03351486849714733, 0.22198970902095586, 0.23190198214447205, 0.2336676654510633, 0.17657102601669408]
maxi score, test score, baseline:  -0.8679666666666668 -0.11333333333333336 -0.11333333333333336
probs:  [0.10235474886966729, 0.03351486849714733, 0.22198970902095586, 0.23190198214447205, 0.2336676654510633, 0.17657102601669408]
maxi score, test score, baseline:  -0.8679666666666668 -0.11333333333333336 -0.11333333333333336
probs:  [0.10254969899821569, 0.03357853175723742, 0.22163630130116252, 0.2312149045289318, 0.23411304689228699, 0.1769075165221656]
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  39.08690376575605
printing an ep nov before normalisation:  42.45622390136161
maxi score, test score, baseline:  -0.8679666666666668 -0.11333333333333336 -0.11333333333333336
probs:  [0.10257580840048644, 0.03351076680907723, 0.22182449549746258, 0.2314161358419288, 0.23363796158652164, 0.1770348318645234]
printing an ep nov before normalisation:  31.01952314376831
printing an ep nov before normalisation:  28.81997998161165
Printing some Q and Qe and total Qs values:  [[0.744]
 [0.809]
 [0.769]
 [0.82 ]
 [0.822]
 [0.769]
 [0.769]] [[24.696]
 [25.88 ]
 [28.629]
 [24.501]
 [25.504]
 [28.629]
 [28.629]] [[0.744]
 [0.809]
 [0.769]
 [0.82 ]
 [0.822]
 [0.769]
 [0.769]]
maxi score, test score, baseline:  -0.8679666666666668 -0.11333333333333336 -0.11333333333333336
probs:  [0.10257580840048644, 0.03351076680907723, 0.22182449549746258, 0.2314161358419288, 0.23363796158652164, 0.1770348318645234]
printing an ep nov before normalisation:  31.679588831905996
maxi score, test score, baseline:  -0.8679666666666668 -0.11333333333333336 -0.11333333333333336
probs:  [0.10257580840048644, 0.03351076680907723, 0.22182449549746258, 0.2314161358419288, 0.23363796158652164, 0.1770348318645234]
actor:  0 policy actor:  1  step number:  51 total reward:  0.5733333333333336  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.092]
 [-0.087]
 [-0.092]
 [-0.092]
 [-0.092]
 [-0.092]
 [-0.092]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.092]
 [-0.087]
 [-0.092]
 [-0.092]
 [-0.092]
 [-0.092]
 [-0.092]]
Printing some Q and Qe and total Qs values:  [[-0.083]
 [-0.038]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]] [[55.865]
 [53.496]
 [55.865]
 [55.865]
 [55.865]
 [55.865]
 [55.865]] [[1.581]
 [1.484]
 [1.581]
 [1.581]
 [1.581]
 [1.581]
 [1.581]]
printing an ep nov before normalisation:  54.909974878116785
maxi score, test score, baseline:  -0.86482 -0.11333333333333336 -0.11333333333333336
probs:  [0.10276471920641943, 0.03343309250634437, 0.22196334041300697, 0.23155095376123683, 0.23309541264463357, 0.17719248146835892]
printing an ep nov before normalisation:  44.56882586706808
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.492]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]] [[38.659]
 [41.716]
 [38.659]
 [38.659]
 [38.659]
 [38.659]
 [38.659]] [[1.851]
 [2.063]
 [1.851]
 [1.851]
 [1.851]
 [1.851]
 [1.851]]
printing an ep nov before normalisation:  11.943096192143877
maxi score, test score, baseline:  -0.86482 -0.11333333333333336 -0.11333333333333336
probs:  [0.10279087155679541, 0.03336577239374601, 0.22215019559465393, 0.23175073490288725, 0.23262344869230173, 0.17731897685961556]
printing an ep nov before normalisation:  27.098138003149046
actor:  1 policy actor:  1  step number:  59 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.86482 -0.11333333333333336 -0.11333333333333336
maxi score, test score, baseline:  -0.86482 -0.11333333333333336 -0.11333333333333336
printing an ep nov before normalisation:  34.45395856127778
Printing some Q and Qe and total Qs values:  [[0.376]
 [0.497]
 [0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.376]] [[24.865]
 [30.591]
 [24.865]
 [24.865]
 [24.865]
 [24.865]
 [24.865]] [[0.482]
 [0.659]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]]
printing an ep nov before normalisation:  23.32039298877231
printing an ep nov before normalisation:  18.01126707508791
printing an ep nov before normalisation:  28.903755386978194
actions average: 
K:  1  action  0 :  tensor([0.5236, 0.0038, 0.0680, 0.0734, 0.1069, 0.1000, 0.1243],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0053, 0.9409, 0.0033, 0.0116, 0.0036, 0.0023, 0.0329],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1515, 0.0045, 0.2837, 0.0924, 0.0829, 0.2270, 0.1580],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0894, 0.0878, 0.1253, 0.2115, 0.1093, 0.1363, 0.2404],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1076, 0.0103, 0.0541, 0.0769, 0.5652, 0.0955, 0.0903],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0515, 0.0073, 0.0784, 0.0727, 0.0712, 0.6293, 0.0896],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0998, 0.0380, 0.0743, 0.0985, 0.1274, 0.1421, 0.4199],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  36.581673060284686
maxi score, test score, baseline:  -0.86482 -0.11333333333333336 -0.11333333333333336
maxi score, test score, baseline:  -0.86482 -0.11333333333333336 -0.11333333333333336
probs:  [0.10114868526038831, 0.033013687260499164, 0.21903276475384348, 0.22929684280269785, 0.23016031054896705, 0.18734770937360432]
printing an ep nov before normalisation:  20.868241994107386
actor:  1 policy actor:  1  step number:  64 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  1.667
from probs:  [0.10114868526038831, 0.033013687260499164, 0.21903276475384348, 0.22929684280269785, 0.23016031054896705, 0.18734770937360432]
printing an ep nov before normalisation:  35.63170988055936
siam score:  -0.85088384
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.86482 -0.11333333333333336 -0.11333333333333336
probs:  [0.10070562836493975, 0.03426828116323326, 0.2156525095385914, 0.22566084794110025, 0.23895581437152819, 0.1847569186206072]
maxi score, test score, baseline:  -0.86482 -0.11333333333333336 -0.11333333333333336
actor:  0 policy actor:  1  step number:  59 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.8620466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.10070562836493975, 0.03426828116323326, 0.2156525095385914, 0.22566084794110025, 0.23895581437152819, 0.1847569186206072]
maxi score, test score, baseline:  -0.8620466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.10072915410401694, 0.034201663875235294, 0.21583199664967198, 0.2258539144914208, 0.23848878467374487, 0.18489448620591006]
printing an ep nov before normalisation:  25.389750784425
maxi score, test score, baseline:  -0.8620466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.10072915410401694, 0.034201663875235294, 0.21583199664967198, 0.2258539144914208, 0.23848878467374487, 0.18489448620591006]
printing an ep nov before normalisation:  41.86918461664319
printing an ep nov before normalisation:  30.632327312651878
printing an ep nov before normalisation:  47.29496334259114
maxi score, test score, baseline:  -0.8620466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.10072915410401694, 0.034201663875235294, 0.21583199664967198, 0.2258539144914208, 0.23848878467374487, 0.18489448620591006]
printing an ep nov before normalisation:  47.31508731842041
printing an ep nov before normalisation:  34.73295207300375
maxi score, test score, baseline:  -0.8620466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.10072915410401694, 0.034201663875235294, 0.21583199664967198, 0.2258539144914208, 0.23848878467374487, 0.18489448620591006]
Printing some Q and Qe and total Qs values:  [[0.397]
 [0.41 ]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]] [[44.954]
 [45.74 ]
 [44.954]
 [44.954]
 [44.954]
 [44.954]
 [44.954]] [[0.605]
 [0.626]
 [0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]]
actor:  1 policy actor:  1  step number:  58 total reward:  0.40666666666666607  reward:  1.0 rdn_beta:  1.667
from probs:  [0.10072915410401694, 0.034201663875235294, 0.21583199664967198, 0.2258539144914208, 0.23848878467374487, 0.18489448620591006]
printing an ep nov before normalisation:  47.10821519233465
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.10233563614691
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.76 ]
 [0.604]
 [0.714]
 [0.699]
 [0.647]
 [0.698]] [[40.408]
 [48.818]
 [40.201]
 [45.35 ]
 [44.867]
 [41.635]
 [45.353]] [[0.543]
 [0.76 ]
 [0.604]
 [0.714]
 [0.699]
 [0.647]
 [0.698]]
maxi score, test score, baseline:  -0.8620466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09973724027201682, 0.035701696593927836, 0.21195075279034198, 0.22167574494283582, 0.24900471789374806, 0.18192984750712943]
actor:  1 policy actor:  1  step number:  45 total reward:  0.33333333333333315  reward:  1.0 rdn_beta:  1.0
line 256 mcts: sample exp_bonus 31.500209446529045
maxi score, test score, baseline:  -0.8620466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09843290264532616, 0.035235919243205197, 0.2222617170296149, 0.21877459435486357, 0.2457456880752215, 0.1795491786517685]
maxi score, test score, baseline:  -0.8620466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09843290264532616, 0.035235919243205197, 0.2222617170296149, 0.21877459435486357, 0.2457456880752215, 0.1795491786517685]
maxi score, test score, baseline:  -0.8620466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09843290264532616, 0.035235919243205197, 0.2222617170296149, 0.21877459435486357, 0.2457456880752215, 0.1795491786517685]
maxi score, test score, baseline:  -0.8620466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09843290264532616, 0.035235919243205197, 0.2222617170296149, 0.21877459435486357, 0.2457456880752215, 0.1795491786517685]
maxi score, test score, baseline:  -0.8620466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09843290264532616, 0.035235919243205197, 0.2222617170296149, 0.21877459435486357, 0.2457456880752215, 0.1795491786517685]
printing an ep nov before normalisation:  37.494149832582295
actor:  0 policy actor:  1  step number:  59 total reward:  0.23999999999999944  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.8595666666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09843290264532616, 0.035235919243205197, 0.2222617170296149, 0.21877459435486357, 0.2457456880752215, 0.1795491786517685]
printing an ep nov before normalisation:  55.866321953446445
maxi score, test score, baseline:  -0.8595666666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09854808109731682, 0.0351307411757337, 0.22205662180696173, 0.2193093826506116, 0.24500797829440435, 0.17994719497497172]
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.884536512430756
Printing some Q and Qe and total Qs values:  [[-0.038]
 [-0.018]
 [-0.041]
 [-0.026]
 [-0.032]
 [-0.039]
 [-0.036]] [[29.409]
 [37.343]
 [27.264]
 [36.851]
 [31.441]
 [27.199]
 [29.247]] [[0.44 ]
 [0.698]
 [0.373]
 [0.674]
 [0.507]
 [0.373]
 [0.437]]
actions average: 
K:  2  action  0 :  tensor([0.3832, 0.0944, 0.0690, 0.0971, 0.0970, 0.1475, 0.1119],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0045, 0.9361, 0.0052, 0.0081, 0.0046, 0.0052, 0.0362],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1251, 0.0587, 0.1177, 0.1445, 0.1327, 0.2106, 0.2107],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1087, 0.0282, 0.0863, 0.3529, 0.1274, 0.1450, 0.1516],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1002, 0.0477, 0.0618, 0.1140, 0.4633, 0.1122, 0.1008],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0966, 0.0188, 0.0852, 0.0819, 0.1088, 0.5049, 0.1038],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0786, 0.1228, 0.0589, 0.1168, 0.0891, 0.1095, 0.4243],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.8595666666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09854808109731682, 0.0351307411757337, 0.22205662180696173, 0.2193093826506116, 0.24500797829440435, 0.17994719497497172]
maxi score, test score, baseline:  -0.8595666666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09862175033280662, 0.03515693884831874, 0.22147478463673081, 0.2194734487354526, 0.2451912812709406, 0.18008179617575074]
using another actor
printing an ep nov before normalisation:  37.32251955813248
maxi score, test score, baseline:  -0.8595666666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09816015926798506, 0.03517488757164004, 0.22158821631327746, 0.2195858547804424, 0.2453168670228482, 0.1801740150438069]
actor:  1 policy actor:  1  step number:  79 total reward:  0.03999999999999926  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8595666666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09807442475903198, 0.03514424008011739, 0.22226843387875067, 0.21939392116771408, 0.24510242902644844, 0.18001655108793738]
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.017584800720215
Printing some Q and Qe and total Qs values:  [[-0.092]
 [-0.098]
 [-0.092]
 [-0.099]
 [-0.092]
 [-0.092]
 [-0.092]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.092]
 [-0.098]
 [-0.092]
 [-0.099]
 [-0.092]
 [-0.092]
 [-0.092]]
maxi score, test score, baseline:  -0.8595666666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09807442475903198, 0.03514424008011739, 0.22226843387875067, 0.21939392116771408, 0.24510242902644844, 0.18001655108793738]
printing an ep nov before normalisation:  36.97889743734718
maxi score, test score, baseline:  -0.8595666666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09807442475903198, 0.03514424008011739, 0.22226843387875067, 0.21939392116771408, 0.24510242902644844, 0.18001655108793735]
Printing some Q and Qe and total Qs values:  [[0.548]
 [0.682]
 [0.592]
 [0.558]
 [0.556]
 [0.593]
 [0.561]] [[29.86 ]
 [41.959]
 [35.601]
 [32.399]
 [32.933]
 [35.654]
 [36.328]] [[0.548]
 [0.682]
 [0.592]
 [0.558]
 [0.556]
 [0.593]
 [0.561]]
printing an ep nov before normalisation:  15.680551528930664
actor:  1 policy actor:  1  step number:  75 total reward:  0.05333333333333268  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  32.92679060292647
maxi score, test score, baseline:  -0.8595666666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09795285703089271, 0.035100783297509396, 0.2219927125988436, 0.2203620074188203, 0.24479836546595937, 0.17979327418797453]
maxi score, test score, baseline:  -0.8595666666666667 -0.11333333333333336 -0.11333333333333336
maxi score, test score, baseline:  -0.8595666666666667 -0.11333333333333336 -0.11333333333333336
printing an ep nov before normalisation:  40.43245572863693
printing an ep nov before normalisation:  38.832950592041016
actor:  0 policy actor:  0  step number:  71 total reward:  0.03999999999999937  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  14.536362375766487
maxi score, test score, baseline:  -0.8574866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.0980256067177596, 0.03512678910822383, 0.22141459422572282, 0.22052579432730587, 0.24498032600064887, 0.1799268896203391]
maxi score, test score, baseline:  -0.8574866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.0980256067177596, 0.03512678910822383, 0.22141459422572282, 0.22052579432730587, 0.24498032600064887, 0.1799268896203391]
Printing some Q and Qe and total Qs values:  [[-0.071]
 [-0.071]
 [-0.071]
 [-0.071]
 [-0.071]
 [-0.071]
 [-0.071]] [[46.791]
 [46.791]
 [46.791]
 [46.791]
 [46.791]
 [46.791]
 [46.791]] [[0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8574866666666667 -0.11333333333333336 -0.11333333333333336
printing an ep nov before normalisation:  48.52082545141602
maxi score, test score, baseline:  -0.8574866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09802560671775958, 0.035126789108223824, 0.22141459422572274, 0.22052579432730585, 0.24498032600064884, 0.17992688962033906]
maxi score, test score, baseline:  -0.8574866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09802560671775958, 0.035126789108223824, 0.22141459422572274, 0.22052579432730585, 0.24498032600064884, 0.17992688962033906]
maxi score, test score, baseline:  -0.8574866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09802560671775958, 0.035126789108223824, 0.22141459422572274, 0.22052579432730585, 0.24498032600064884, 0.17992688962033906]
printing an ep nov before normalisation:  10.566324870395913
maxi score, test score, baseline:  -0.8574866666666667 -0.11333333333333336 -0.11333333333333336
actor:  1 policy actor:  1  step number:  59 total reward:  0.4800000000000001  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8574866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09490063334560285, 0.034362730681492516, 0.2393874493877813, 0.21571368641070113, 0.23963427155963643, 0.17600122861478573]
maxi score, test score, baseline:  -0.8574866666666667 -0.11333333333333336 -0.11333333333333336
maxi score, test score, baseline:  -0.8574866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09494585626859833, 0.03435278350363677, 0.23956434770523094, 0.21586901008097106, 0.2391476413723445, 0.17612036106921847]
maxi score, test score, baseline:  -0.8574866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09494585626859833, 0.03435278350363677, 0.23956434770523094, 0.21586901008097106, 0.2391476413723445, 0.17612036106921847]
printing an ep nov before normalisation:  12.311434949358373
using another actor
from probs:  [0.09494585626859833, 0.03435278350363677, 0.23956434770523094, 0.21586901008097106, 0.2391476413723445, 0.17612036106921847]
siam score:  -0.84517014
line 256 mcts: sample exp_bonus 40.019170272525194
maxi score, test score, baseline:  -0.8574866666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09450401519720623, 0.034369501555458586, 0.239681322390698, 0.21597440834874188, 0.23926441247486865, 0.17620634003302668]
Printing some Q and Qe and total Qs values:  [[0.95 ]
 [0.971]
 [0.95 ]
 [0.95 ]
 [0.95 ]
 [0.95 ]
 [0.95 ]] [[30.868]
 [32.237]
 [30.868]
 [30.868]
 [30.868]
 [30.868]
 [30.868]] [[0.95 ]
 [0.971]
 [0.95 ]
 [0.95 ]
 [0.95 ]
 [0.95 ]
 [0.95 ]]
printing an ep nov before normalisation:  54.77023443334564
actor:  0 policy actor:  1  step number:  63 total reward:  0.13333333333333286  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.8552200000000001 -0.11333333333333336 -0.11333333333333336
probs:  [0.09450401519720623, 0.034369501555458586, 0.239681322390698, 0.21597440834874188, 0.23926441247486865, 0.17620634003302668]
actor:  1 policy actor:  1  step number:  88 total reward:  0.059999999999999054  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8552200000000001 -0.11333333333333336 -0.11333333333333336
probs:  [0.09447703619222078, 0.03448819163835453, 0.24051337712064538, 0.21565318099445743, 0.23888676770814748, 0.17598144634617433]
Printing some Q and Qe and total Qs values:  [[-0.1  ]
 [-0.101]
 [-0.1  ]
 [-0.1  ]
 [-0.1  ]
 [-0.1  ]
 [-0.1  ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.1  ]
 [-0.101]
 [-0.1  ]
 [-0.1  ]
 [-0.1  ]
 [-0.1  ]
 [-0.1  ]]
siam score:  -0.84218544
maxi score, test score, baseline:  -0.8552200000000001 -0.11333333333333336 -0.11333333333333336
probs:  [0.09447703619222078, 0.03448819163835453, 0.24051337712064538, 0.21565318099445743, 0.23888676770814748, 0.17598144634617433]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.717]
 [0.692]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.715]] [[49.435]
 [42.144]
 [49.435]
 [49.435]
 [49.435]
 [49.435]
 [42.936]] [[0.717]
 [0.692]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.715]]
maxi score, test score, baseline:  -0.8552200000000001 -0.11333333333333336 -0.11333333333333336
printing an ep nov before normalisation:  22.622146149424417
printing an ep nov before normalisation:  25.438534185563228
maxi score, test score, baseline:  -0.8552200000000001 -0.11333333333333336 -0.11333333333333336
probs:  [0.09460134451744673, 0.034533458075798225, 0.2408301043557144, 0.2159371521921821, 0.23788479518019584, 0.17621314567866278]
printing an ep nov before normalisation:  41.425650451607524
printing an ep nov before normalisation:  33.63502231832862
printing an ep nov before normalisation:  37.4879789352417
maxi score, test score, baseline:  -0.8552200000000001 -0.11333333333333336 -0.11333333333333336
probs:  [0.09461915947441883, 0.034456022812240514, 0.24028725789757088, 0.21614737049682994, 0.23812981607169265, 0.17636037324724713]
printing an ep nov before normalisation:  47.400630868217576
maxi score, test score, baseline:  -0.8552200000000001 -0.11333333333333336 -0.11333333333333336
probs:  [0.09461915947441883, 0.034456022812240514, 0.24028725789757088, 0.21614737049682994, 0.23812981607169265, 0.17636037324724713]
maxi score, test score, baseline:  -0.8552200000000001 -0.11333333333333336 -0.11333333333333336
probs:  [0.09461915947441883, 0.034456022812240514, 0.24028725789757088, 0.21614737049682994, 0.23812981607169265, 0.17636037324724713]
maxi score, test score, baseline:  -0.8552200000000001 -0.11333333333333336 -0.11333333333333336
actor:  0 policy actor:  1  step number:  69 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  49 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  23.34510326385498
printing an ep nov before normalisation:  32.33163325649908
actions average: 
K:  2  action  0 :  tensor([0.4184, 0.0027, 0.0967, 0.0850, 0.1733, 0.1133, 0.1106],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0069, 0.9301, 0.0073, 0.0079, 0.0088, 0.0080, 0.0309],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1104, 0.0029, 0.3056, 0.1137, 0.1547, 0.1782, 0.1346],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0835, 0.1752, 0.0817, 0.1338, 0.1787, 0.1982, 0.1490],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1260, 0.0206, 0.0869, 0.1100, 0.3831, 0.1456, 0.1278],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1018, 0.0054, 0.1160, 0.1097, 0.1328, 0.4037, 0.1306],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1593, 0.0583, 0.1013, 0.0938, 0.1250, 0.1235, 0.3387],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[-0.077]
 [-0.078]
 [-0.083]
 [-0.08 ]
 [-0.076]
 [-0.081]
 [-0.074]] [[26.842]
 [34.642]
 [26.55 ]
 [36.965]
 [30.139]
 [27.748]
 [27.106]] [[0.578]
 [1.013]
 [0.556]
 [1.141]
 [0.763]
 [0.625]
 [0.596]]
printing an ep nov before normalisation:  23.494460582733154
actor:  0 policy actor:  0  step number:  80 total reward:  0.1399999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.108]
 [-0.106]
 [-0.108]
 [-0.108]
 [-0.108]
 [-0.108]
 [-0.108]] [[19.543]
 [38.281]
 [19.543]
 [19.543]
 [19.543]
 [19.543]
 [19.543]] [[0.228]
 [0.645]
 [0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.228]]
printing an ep nov before normalisation:  41.17943992313132
printing an ep nov before normalisation:  30.89428476757116
Printing some Q and Qe and total Qs values:  [[0.124]
 [0.165]
 [0.124]
 [0.124]
 [0.124]
 [0.124]
 [0.124]] [[40.771]
 [37.849]
 [40.771]
 [40.771]
 [40.771]
 [40.771]
 [40.771]] [[1.124]
 [1.025]
 [1.124]
 [1.124]
 [1.124]
 [1.124]
 [1.124]]
printing an ep nov before normalisation:  24.875479065061512
maxi score, test score, baseline:  -0.8474466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09469894431554629, 0.034401288134768136, 0.2399032480824212, 0.21649888180286608, 0.23787471336175148, 0.17662292430264673]
actor:  1 policy actor:  1  step number:  72 total reward:  0.15333333333333288  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.049]
 [-0.025]
 [-0.057]
 [-0.044]
 [-0.055]
 [-0.044]
 [-0.038]] [[29.214]
 [33.01 ]
 [29.446]
 [29.933]
 [29.339]
 [30.013]
 [30.849]] [[0.484]
 [0.648]
 [0.485]
 [0.517]
 [0.484]
 [0.519]
 [0.556]]
maxi score, test score, baseline:  -0.8474466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09447918351338185, 0.034455831727646, 0.239022927961286, 0.21572503216400657, 0.24028654738989771, 0.1760304772437819]
maxi score, test score, baseline:  -0.8474466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09447918351338185, 0.034455831727646, 0.239022927961286, 0.21572503216400657, 0.24028654738989771, 0.1760304772437819]
siam score:  -0.83862704
printing an ep nov before normalisation:  35.1269631728611
printing an ep nov before normalisation:  36.72111826863116
printing an ep nov before normalisation:  46.08988927090243
maxi score, test score, baseline:  -0.8474466666666667 -0.11333333333333336 -0.11333333333333336
actor:  1 policy actor:  1  step number:  77 total reward:  0.013333333333332753  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8474466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.0945054385593676, 0.03436070859475415, 0.23961968356139954, 0.21599646845808748, 0.2392960569276643, 0.17622164389872688]
actor:  1 policy actor:  1  step number:  87 total reward:  0.18666666666666654  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8474466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09439178735808541, 0.034860725821027845, 0.24312496121303565, 0.21464322054859902, 0.23770507876604507, 0.1752742262932071]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8474466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09439178735808541, 0.034860725821027845, 0.24312496121303565, 0.21464322054859902, 0.23770507876604507, 0.1752742262932071]
printing an ep nov before normalisation:  39.2492230042078
printing an ep nov before normalisation:  40.962857775413006
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8474466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09439178735808541, 0.034860725821027845, 0.24312496121303565, 0.21464322054859902, 0.23770507876604507, 0.1752742262932071]
actor:  1 policy actor:  1  step number:  64 total reward:  0.23333333333333295  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  41.626342913215865
siam score:  -0.83947355
actor:  1 policy actor:  1  step number:  63 total reward:  0.2799999999999998  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  43.085147067801266
maxi score, test score, baseline:  -0.8474466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09345509680879589, 0.03436514312610922, 0.23965790783454333, 0.22405894111138447, 0.23507888739990185, 0.17338402371926512]
maxi score, test score, baseline:  -0.8474466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09345509680879589, 0.03436514312610922, 0.23965790783454333, 0.22405894111138447, 0.23507888739990185, 0.17338402371926512]
printing an ep nov before normalisation:  46.52385805460225
maxi score, test score, baseline:  -0.8474466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09345509680879589, 0.03436514312610922, 0.23965790783454333, 0.22405894111138447, 0.23507888739990185, 0.17338402371926512]
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.0
siam score:  -0.83961576
actor:  1 policy actor:  1  step number:  52 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  59 total reward:  0.26666666666666616  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.8474466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09203130053840579, 0.03410663854341151, 0.23535089860175276, 0.22005955348135736, 0.23785232238355758, 0.18059928645151513]
actions average: 
K:  3  action  0 :  tensor([0.5720, 0.0070, 0.0590, 0.0777, 0.0939, 0.0894, 0.1010],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0029, 0.9788, 0.0029, 0.0027, 0.0030, 0.0040, 0.0057],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1083, 0.0139, 0.3015, 0.1110, 0.1303, 0.1574, 0.1775],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0775, 0.0307, 0.0852, 0.3442, 0.1744, 0.1074, 0.1806],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0847, 0.0228, 0.0658, 0.0875, 0.5090, 0.0912, 0.1390],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0989, 0.0265, 0.0902, 0.0757, 0.0997, 0.5075, 0.1014],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0984, 0.0328, 0.0754, 0.1070, 0.0841, 0.0829, 0.5194],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.8474466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09203130053840579, 0.03410663854341151, 0.23535089860175276, 0.22005955348135736, 0.23785232238355758, 0.18059928645151513]
maxi score, test score, baseline:  -0.8474466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09203130053840579, 0.03410663854341151, 0.23535089860175276, 0.22005955348135736, 0.23785232238355758, 0.18059928645151513]
printing an ep nov before normalisation:  53.29397997114803
maxi score, test score, baseline:  -0.8474466666666667 -0.11333333333333336 -0.11333333333333336
printing an ep nov before normalisation:  37.061887811181926
Printing some Q and Qe and total Qs values:  [[-0.158]
 [-0.211]
 [-0.16 ]
 [-0.16 ]
 [-0.163]
 [-0.162]
 [-0.16 ]] [[13.338]
 [41.367]
 [14.935]
 [14.803]
 [15.093]
 [15.075]
 [14.425]] [[0.266]
 [1.102]
 [0.314]
 [0.31 ]
 [0.316]
 [0.317]
 [0.298]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  86 total reward:  0.006666666666665599  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8474466666666667 -0.11333333333333336 -0.11333333333333336
maxi score, test score, baseline:  -0.8474466666666667 -0.11333333333333336 -0.11333333333333336
maxi score, test score, baseline:  -0.8474466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09202880685530783, 0.0340384586785206, 0.23565881954866066, 0.22020224298634894, 0.23737444365525368, 0.1806972282759083]
actor:  1 policy actor:  1  step number:  69 total reward:  0.2799999999999997  reward:  1.0 rdn_beta:  1.333
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.02 ]
 [0.038]
 [0.01 ]
 [0.014]
 [0.018]
 [0.009]
 [0.013]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.02 ]
 [0.038]
 [0.01 ]
 [0.014]
 [0.018]
 [0.009]
 [0.013]]
printing an ep nov before normalisation:  48.210421765693575
printing an ep nov before normalisation:  64.65093121567035
printing an ep nov before normalisation:  64.68238872949611
Printing some Q and Qe and total Qs values:  [[-0.132]
 [-0.108]
 [-0.087]
 [-0.119]
 [-0.189]
 [-0.111]
 [-0.108]] [[52.934]
 [55.648]
 [50.534]
 [53.313]
 [59.279]
 [59.838]
 [55.304]] [[1.189]
 [1.362]
 [1.102]
 [1.223]
 [1.48 ]
 [1.589]
 [1.343]]
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.449]
 [0.475]
 [0.475]] [[46.973]
 [46.973]
 [46.973]
 [46.973]
 [42.188]
 [46.973]
 [46.973]] [[0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.449]
 [0.475]
 [0.475]]
printing an ep nov before normalisation:  61.12278489265627
maxi score, test score, baseline:  -0.8474466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09150347040709068, 0.03384463375139946, 0.23356059843336394, 0.2254080788634065, 0.23601821457308786, 0.17966500397165158]
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.655]
 [0.559]
 [0.577]
 [0.581]
 [0.64 ]
 [0.571]] [[50.309]
 [57.759]
 [48.927]
 [48.917]
 [49.146]
 [53.966]
 [52.056]] [[0.544]
 [0.655]
 [0.559]
 [0.577]
 [0.581]
 [0.64 ]
 [0.571]]
printing an ep nov before normalisation:  45.57589164917538
printing an ep nov before normalisation:  49.50448139137592
Printing some Q and Qe and total Qs values:  [[0.696]
 [0.767]
 [0.727]
 [0.716]
 [0.715]
 [0.697]
 [0.694]] [[48.749]
 [45.74 ]
 [48.756]
 [50.059]
 [49.797]
 [50.367]
 [49.226]] [[0.696]
 [0.767]
 [0.727]
 [0.716]
 [0.715]
 [0.697]
 [0.694]]
printing an ep nov before normalisation:  48.279916267104795
maxi score, test score, baseline:  -0.8474466666666667 -0.11333333333333336 -0.11333333333333336
using another actor
from probs:  [0.09150347040709068, 0.03384463375139946, 0.23356059843336394, 0.2254080788634065, 0.23601821457308786, 0.17966500397165158]
Printing some Q and Qe and total Qs values:  [[-0.112]
 [-0.112]
 [-0.112]
 [-0.112]
 [-0.119]
 [-0.112]
 [-0.112]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.112]
 [-0.112]
 [-0.112]
 [-0.112]
 [-0.119]
 [-0.112]
 [-0.112]]
printing an ep nov before normalisation:  46.72256926876244
maxi score, test score, baseline:  -0.8474466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09150347040709068, 0.03384463375139946, 0.23356059843336394, 0.2254080788634065, 0.23601821457308786, 0.17966500397165158]
printing an ep nov before normalisation:  49.4534423394951
printing an ep nov before normalisation:  42.40582276531044
maxi score, test score, baseline:  -0.8474466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09150347040709068, 0.03384463375139946, 0.23356059843336394, 0.2254080788634065, 0.23601821457308786, 0.17966500397165158]
using explorer policy with actor:  0
printing an ep nov before normalisation:  32.70671043289803
maxi score, test score, baseline:  -0.8474466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09150347040709068, 0.03384463375139946, 0.23356059843336394, 0.2254080788634065, 0.23601821457308786, 0.17966500397165158]
printing an ep nov before normalisation:  41.56921947048883
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.8474466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09150347040709068, 0.03384463375139946, 0.23356059843336394, 0.2254080788634065, 0.23601821457308786, 0.17966500397165158]
printing an ep nov before normalisation:  35.268119155014105
using explorer policy with actor:  0
using explorer policy with actor:  0
siam score:  -0.83365405
maxi score, test score, baseline:  -0.8474466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09150347040709068, 0.03384463375139946, 0.23356059843336394, 0.2254080788634065, 0.23601821457308786, 0.17966500397165158]
printing an ep nov before normalisation:  49.88024536839718
printing an ep nov before normalisation:  43.6584116319127
Printing some Q and Qe and total Qs values:  [[0.875]
 [0.875]
 [0.875]
 [0.875]
 [0.875]
 [0.875]
 [0.875]] [[36.731]
 [36.731]
 [36.731]
 [36.731]
 [36.731]
 [36.731]
 [36.731]] [[0.875]
 [0.875]
 [0.875]
 [0.875]
 [0.875]
 [0.875]
 [0.875]]
printing an ep nov before normalisation:  39.44125804026678
maxi score, test score, baseline:  -0.8474466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09150347040709068, 0.03384463375139946, 0.23356059843336394, 0.2254080788634065, 0.23601821457308786, 0.17966500397165158]
printing an ep nov before normalisation:  39.436696535033114
printing an ep nov before normalisation:  59.741131011417124
maxi score, test score, baseline:  -0.8474466666666667 -0.11333333333333336 -0.11333333333333336
using explorer policy with actor:  0
using explorer policy with actor:  0
printing an ep nov before normalisation:  40.97926314654464
maxi score, test score, baseline:  -0.8474466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09150347040709068, 0.03384463375139946, 0.23356059843336394, 0.2254080788634065, 0.23601821457308786, 0.17966500397165158]
printing an ep nov before normalisation:  43.26189997864836
Printing some Q and Qe and total Qs values:  [[0.932]
 [0.932]
 [0.932]
 [0.932]
 [0.932]
 [0.932]
 [0.932]] [[43.491]
 [43.491]
 [43.491]
 [43.491]
 [43.491]
 [43.491]
 [43.491]] [[0.932]
 [0.932]
 [0.932]
 [0.932]
 [0.932]
 [0.932]
 [0.932]]
actor:  0 policy actor:  1  step number:  38 total reward:  0.6066666666666667  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  25.23077964782715
printing an ep nov before normalisation:  39.01538359457159
actor:  0 policy actor:  1  step number:  40 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  40 total reward:  0.5133333333333333  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  40 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  22.61927552677956
actor:  0 policy actor:  1  step number:  41 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.8318466666666667 -0.11333333333333336 -0.11333333333333336
probs:  [0.09150347040709068, 0.03384463375139946, 0.23356059843336394, 0.2254080788634065, 0.23601821457308786, 0.17966500397165158]
actor:  0 policy actor:  1  step number:  42 total reward:  0.47333333333333305  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  26.658353841208065
actions average: 
K:  4  action  0 :  tensor([0.6019, 0.0014, 0.0603, 0.0574, 0.0827, 0.1002, 0.0961],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0107, 0.8425, 0.0101, 0.0484, 0.0141, 0.0200, 0.0542],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1384, 0.0011, 0.1392, 0.1389, 0.1883, 0.1488, 0.2454],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1056, 0.0495, 0.0935, 0.3000, 0.1294, 0.1083, 0.2137],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1123, 0.0725, 0.0746, 0.0787, 0.4288, 0.1024, 0.1307],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0498, 0.0024, 0.0741, 0.0830, 0.1077, 0.5994, 0.0836],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0675, 0.0029, 0.0720, 0.0821, 0.1058, 0.1221, 0.5476],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.8289 -0.11333333333333336 -0.11333333333333336
probs:  [0.09150347040709068, 0.03384463375139946, 0.23356059843336394, 0.2254080788634065, 0.23601821457308786, 0.17966500397165158]
line 256 mcts: sample exp_bonus 32.451049783903464
actor:  0 policy actor:  1  step number:  44 total reward:  0.5133333333333333  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  44 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  45 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  45 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  46 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  46 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  46 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  47 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  2
actor:  1 policy actor:  1  step number:  66 total reward:  0.09999999999999953  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  1  step number:  49 total reward:  0.46666666666666634  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  50 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  51 total reward:  0.3999999999999996  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  33.791748807472224
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.7191308760678
printing an ep nov before normalisation:  17.227155521576485
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.994]
 [0.994]
 [0.994]
 [0.994]
 [0.994]
 [0.994]
 [0.994]] [[30.12]
 [30.12]
 [30.12]
 [30.12]
 [30.12]
 [30.12]
 [30.12]] [[0.994]
 [0.994]
 [0.994]
 [0.994]
 [0.994]
 [0.994]
 [0.994]]
printing an ep nov before normalisation:  31.105654900456194
actor:  0 policy actor:  1  step number:  63 total reward:  0.0666666666666661  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  73.76325795357266
actor:  0 policy actor:  1  step number:  68 total reward:  0.16666666666666552  reward:  1.0 rdn_beta:  2
siam score:  -0.83628917
maxi score, test score, baseline:  -0.7924333333333333 0.37533333333333313 0.37533333333333313
maxi score, test score, baseline:  -0.7924333333333333 0.37533333333333313 0.37533333333333313
probs:  [0.22407522905038962, 0.04360093320253759, 0.302615451088498, 0.116273482154118, 0.26858189048095715, 0.044853014023499664]
maxi score, test score, baseline:  -0.7924333333333333 0.37533333333333313 0.37533333333333313
probs:  [0.22407522905038962, 0.04360093320253759, 0.302615451088498, 0.116273482154118, 0.26858189048095715, 0.044853014023499664]
printing an ep nov before normalisation:  36.53803838296141
printing an ep nov before normalisation:  40.54630554313737
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.15350370493686
printing an ep nov before normalisation:  44.38044635270881
maxi score, test score, baseline:  -0.7924333333333333 0.37533333333333313 0.37533333333333313
printing an ep nov before normalisation:  35.863301732841755
printing an ep nov before normalisation:  33.55338248861922
actor:  1 policy actor:  1  step number:  49 total reward:  0.4666666666666669  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  33.47439227910589
printing an ep nov before normalisation:  47.73326701642895
printing an ep nov before normalisation:  0.006614883349698175
maxi score, test score, baseline:  -0.7924333333333333 0.37533333333333313 0.37533333333333313
probs:  [0.31466468244486095, 0.04527075494567017, 0.25801380860371814, 0.10513841695369912, 0.23061011764727676, 0.046302219404774754]
printing an ep nov before normalisation:  25.031387035857318
actor:  1 policy actor:  1  step number:  68 total reward:  0.2733333333333331  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.115]
 [-0.112]
 [-0.112]
 [-0.106]
 [-0.092]
 [-0.088]
 [-0.097]] [[53.148]
 [61.874]
 [61.874]
 [56.34 ]
 [55.918]
 [56.455]
 [54.652]] [[1.106]
 [1.399]
 [1.399]
 [1.221]
 [1.221]
 [1.243]
 [1.173]]
printing an ep nov before normalisation:  52.80665512886356
maxi score, test score, baseline:  -0.7924333333333333 0.37533333333333313 0.37533333333333313
probs:  [0.31509958838980046, 0.045333167079642855, 0.2583703827908204, 0.10528360873226904, 0.22954719525198658, 0.04636605775548073]
siam score:  -0.84085953
maxi score, test score, baseline:  -0.7924333333333333 0.37533333333333313 0.37533333333333313
maxi score, test score, baseline:  -0.7924333333333333 0.37533333333333313 0.37533333333333313
maxi score, test score, baseline:  -0.7924333333333333 0.37533333333333313 0.37533333333333313
printing an ep nov before normalisation:  37.155858450439794
maxi score, test score, baseline:  -0.7924333333333333 0.37533333333333313 0.37533333333333313
probs:  [0.31468656383269566, 0.04527489111391139, 0.25830134536742905, 0.1054001899023585, 0.2300262153670895, 0.04631079441651591]
printing an ep nov before normalisation:  36.71575806457035
from probs:  [0.31468656383269566, 0.04527489111391139, 0.25830134536742905, 0.1054001899023585, 0.2300262153670895, 0.04631079441651591]
maxi score, test score, baseline:  -0.7924333333333333 0.37533333333333313 0.37533333333333313
probs:  [0.31483059172259975, 0.04529556060902836, 0.2584195555562313, 0.10544838960902789, 0.22967396427139092, 0.04633193823172177]
maxi score, test score, baseline:  -0.7924333333333333 0.37533333333333313 0.37533333333333313
probs:  [0.31483059172259975, 0.04529556060902836, 0.2584195555562313, 0.10544838960902789, 0.22967396427139092, 0.04633193823172177]
actions average: 
K:  4  action  0 :  tensor([0.5431, 0.0649, 0.0474, 0.0789, 0.0948, 0.0707, 0.1002],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0368, 0.8295, 0.0088, 0.0194, 0.0330, 0.0140, 0.0585],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1331, 0.0968, 0.0983, 0.1654, 0.1700, 0.1313, 0.2052],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1061, 0.0357, 0.1025, 0.1988, 0.1896, 0.2129, 0.1545],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1075, 0.0032, 0.0570, 0.1091, 0.4946, 0.1337, 0.0949],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0818, 0.0211, 0.0938, 0.0591, 0.0888, 0.5697, 0.0857],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1290, 0.1353, 0.0971, 0.1239, 0.2146, 0.1427, 0.1575],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  75 total reward:  0.10666666666666647  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.23481133016995
printing an ep nov before normalisation:  39.746690997912125
printing an ep nov before normalisation:  34.20055446091004
actor:  1 policy actor:  1  step number:  53 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.7924333333333333 0.37533333333333313 0.37533333333333313
probs:  [0.3151723483103265, 0.0453446062218323, 0.258070683517664, 0.1055627602860664, 0.22946749232913186, 0.046382109334978966]
siam score:  -0.8554867
using another actor
from probs:  [0.3151723483103265, 0.0453446062218323, 0.258070683517664, 0.1055627602860664, 0.22946749232913186, 0.046382109334978966]
maxi score, test score, baseline:  -0.7924333333333333 0.37533333333333313 0.37533333333333313
maxi score, test score, baseline:  -0.7924333333333333 0.37533333333333313 0.37533333333333313
maxi score, test score, baseline:  -0.7924333333333333 0.37533333333333313 0.37533333333333313
probs:  [0.3145630565935735, 0.045258162178004324, 0.2584678773561576, 0.1056132236721086, 0.22979965612058134, 0.04629802407957466]
Printing some Q and Qe and total Qs values:  [[0.803]
 [0.666]
 [0.532]
 [0.615]
 [0.707]
 [0.386]
 [0.666]] [[32.663]
 [30.895]
 [25.662]
 [27.012]
 [31.758]
 [24.645]
 [30.895]] [[0.803]
 [0.666]
 [0.532]
 [0.615]
 [0.707]
 [0.386]
 [0.666]]
maxi score, test score, baseline:  -0.7924333333333333 0.37533333333333313 0.37533333333333313
probs:  [0.3145630565935735, 0.045258162178004324, 0.2584678773561576, 0.1056132236721086, 0.22979965612058134, 0.04629802407957466]
actor:  1 policy actor:  1  step number:  67 total reward:  0.30666666666666587  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.775]
 [0.871]
 [0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.817]] [[49.804]
 [50.267]
 [49.804]
 [49.804]
 [49.804]
 [49.804]
 [54.33 ]] [[0.775]
 [0.871]
 [0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.817]]
actor:  1 policy actor:  1  step number:  66 total reward:  0.15333333333333288  reward:  1.0 rdn_beta:  1.667
using another actor
maxi score, test score, baseline:  -0.7924333333333333 0.37533333333333313 0.37533333333333313
probs:  [0.3149037920777604, 0.04530706233079534, 0.2581200577092589, 0.10572752827813386, 0.2295935085134334, 0.04634805109061795]
printing an ep nov before normalisation:  46.75804121532488
maxi score, test score, baseline:  -0.7924333333333333 0.37533333333333313 0.37533333333333313
probs:  [0.3149037920777604, 0.04530706233079534, 0.2581200577092589, 0.10572752827813386, 0.2295935085134334, 0.04634805109061795]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.7924333333333333 0.37533333333333313 0.37533333333333313
probs:  [0.3149037920777604, 0.04530706233079534, 0.2581200577092589, 0.10572752827813386, 0.2295935085134334, 0.04634805109061795]
maxi score, test score, baseline:  -0.7924333333333333 0.37533333333333313 0.37533333333333313
probs:  [0.3149037920777604, 0.04530706233079534, 0.2581200577092589, 0.10572752827813386, 0.2295935085134334, 0.04634805109061795]
maxi score, test score, baseline:  -0.7924333333333333 0.37533333333333313 0.37533333333333313
probs:  [0.3149037920777604, 0.04530706233079534, 0.2581200577092589, 0.10572752827813386, 0.2295935085134334, 0.04634805109061795]
Printing some Q and Qe and total Qs values:  [[0.821]
 [0.905]
 [0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.821]] [[53.473]
 [54.106]
 [53.473]
 [53.473]
 [53.473]
 [53.473]
 [53.473]] [[0.821]
 [0.905]
 [0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.821]]
Printing some Q and Qe and total Qs values:  [[0.758]
 [0.803]
 [0.764]
 [0.771]
 [0.772]
 [0.773]
 [0.763]] [[33.447]
 [47.247]
 [37.221]
 [41.944]
 [41.818]
 [35.599]
 [37.336]] [[0.758]
 [0.803]
 [0.764]
 [0.771]
 [0.772]
 [0.773]
 [0.763]]
actor:  0 policy actor:  1  step number:  72 total reward:  0.11333333333333306  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8541225
maxi score, test score, baseline:  -0.7902066666666666 0.37533333333333313 0.37533333333333313
probs:  [0.3149037920777604, 0.04530706233079534, 0.2581200577092589, 0.10572752827813386, 0.2295935085134334, 0.04634805109061795]
printing an ep nov before normalisation:  38.497068327782245
maxi score, test score, baseline:  -0.7902066666666666 0.37533333333333313 0.37533333333333313
probs:  [0.3149037920777604, 0.04530706233079534, 0.2581200577092589, 0.10572752827813386, 0.2295935085134334, 0.04634805109061795]
printing an ep nov before normalisation:  39.105729012433265
printing an ep nov before normalisation:  33.12550754806463
maxi score, test score, baseline:  -0.7902066666666666 0.37533333333333313 0.37533333333333313
probs:  [0.3149037920777604, 0.04530706233079534, 0.2581200577092589, 0.10572752827813386, 0.2295935085134334, 0.04634805109061795]
maxi score, test score, baseline:  -0.7902066666666666 0.37533333333333313 0.37533333333333313
probs:  [0.3149037920777604, 0.04530706233079534, 0.2581200577092589, 0.10572752827813386, 0.2295935085134334, 0.04634805109061795]
printing an ep nov before normalisation:  50.70986747741699
printing an ep nov before normalisation:  50.76233022823224
maxi score, test score, baseline:  -0.7902066666666666 0.37533333333333313 0.37533333333333313
probs:  [0.3149037920777604, 0.04530706233079534, 0.2581200577092589, 0.10572752827813386, 0.2295935085134334, 0.04634805109061795]
actor:  1 policy actor:  1  step number:  76 total reward:  0.12666666666666604  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.428]
 [0.355]
 [0.355]
 [0.355]
 [0.324]
 [0.355]] [[31.13 ]
 [40.876]
 [31.13 ]
 [31.13 ]
 [31.13 ]
 [33.319]
 [31.13 ]] [[0.767]
 [1.093]
 [0.767]
 [0.767]
 [0.767]
 [0.793]
 [0.767]]
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.506]
 [0.395]
 [0.445]
 [0.384]
 [0.39 ]
 [0.45 ]] [[36.515]
 [47.102]
 [36.233]
 [43.66 ]
 [37.921]
 [36.79 ]
 [40.802]] [[0.884]
 [1.289]
 [0.887]
 [1.136]
 [0.921]
 [0.897]
 [1.064]]
maxi score, test score, baseline:  -0.7902066666666666 0.37533333333333313 0.37533333333333313
probs:  [0.31429696861455925, 0.04522096973553184, 0.25851535817663096, 0.10577810963502504, 0.22992428057708, 0.046264313261172994]
printing an ep nov before normalisation:  26.39362335205078
printing an ep nov before normalisation:  17.9425311088562
maxi score, test score, baseline:  -0.7902066666666666 0.37533333333333313 0.37533333333333313
probs:  [0.31369248481283685, 0.04513520907787617, 0.25890913452812386, 0.10582849597102559, 0.2302537773198095, 0.04618089829032822]
UNIT TEST: sample policy line 217 mcts : [0.122 0.224 0.224 0.204 0.041 0.041 0.143]
maxi score, test score, baseline:  -0.7902066666666666 0.37533333333333313 0.37533333333333313
probs:  [0.31369248481283685, 0.04513520907787617, 0.25890913452812386, 0.10582849597102559, 0.2302537773198095, 0.04618089829032822]
actor:  1 policy actor:  1  step number:  53 total reward:  0.2799999999999997  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  12.42210865020752
printing an ep nov before normalisation:  31.660107563638114
maxi score, test score, baseline:  -0.7902066666666666 0.37533333333333313 0.37533333333333313
maxi score, test score, baseline:  -0.7902066666666666 0.37533333333333313 0.37533333333333313
probs:  [0.3140323135889946, 0.04518398125788792, 0.2585620078569952, 0.10594304622926551, 0.2300478473010514, 0.046230803765805406]
siam score:  -0.8453837
maxi score, test score, baseline:  -0.7902066666666666 0.37533333333333313 0.37533333333333313
maxi score, test score, baseline:  -0.7902066666666666 0.37533333333333313 0.37533333333333313
probs:  [0.3140323135889946, 0.04518398125788792, 0.2585620078569952, 0.10594304622926551, 0.2300478473010514, 0.046230803765805406]
Printing some Q and Qe and total Qs values:  [[0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]] [[54.496]
 [54.496]
 [54.496]
 [54.496]
 [54.496]
 [54.496]
 [54.496]] [[1.382]
 [1.382]
 [1.382]
 [1.382]
 [1.382]
 [1.382]
 [1.382]]
maxi score, test score, baseline:  -0.7902066666666666 0.37533333333333313 0.37533333333333313
probs:  [0.3140323135889946, 0.04518398125788792, 0.2585620078569952, 0.10594304622926551, 0.2300478473010514, 0.046230803765805406]
maxi score, test score, baseline:  -0.7902066666666666 0.37533333333333313 0.37533333333333313
probs:  [0.3140323135889946, 0.04518398125788792, 0.2585620078569952, 0.10594304622926551, 0.2300478473010514, 0.046230803765805406]
from probs:  [0.3140323135889946, 0.04518398125788792, 0.2585620078569952, 0.10594304622926551, 0.2300478473010514, 0.046230803765805406]
maxi score, test score, baseline:  -0.7902066666666666 0.37533333333333313 0.37533333333333313
probs:  [0.3141747189033979, 0.045204419249358904, 0.2586792481847545, 0.10599104853714554, 0.22969884846038266, 0.0462517166649605]
maxi score, test score, baseline:  -0.7902066666666666 0.37533333333333313 0.37533333333333313
probs:  [0.313714594697158, 0.045139375209945605, 0.2591888110820713, 0.10608962293941641, 0.22967810445140296, 0.04618949162000579]
maxi score, test score, baseline:  -0.7902066666666666 0.37533333333333313 0.37533333333333313
Printing some Q and Qe and total Qs values:  [[0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]] [[49.411]
 [49.411]
 [49.411]
 [49.411]
 [49.411]
 [49.411]
 [49.411]] [[1.135]
 [1.135]
 [1.135]
 [1.135]
 [1.135]
 [1.135]
 [1.135]]
maxi score, test score, baseline:  -0.7902066666666666 0.37533333333333313 0.37533333333333313
probs:  [0.3131149398923891, 0.0450543019530718, 0.2595800961623076, 0.10614019198655783, 0.23000371465106664, 0.046106755354607]
maxi score, test score, baseline:  -0.7902066666666666 0.37533333333333313 0.37533333333333313
maxi score, test score, baseline:  -0.7902066666666666 0.37533333333333313 0.37533333333333313
probs:  [0.312658982573153, 0.044989849700115485, 0.2600875025084915, 0.10623857559038301, 0.22997998101496187, 0.04604510861289512]
maxi score, test score, baseline:  -0.7902066666666666 0.37533333333333313 0.37533333333333313
maxi score, test score, baseline:  -0.7902066666666666 0.37533333333333313 0.37533333333333313
probs:  [0.312658982573153, 0.044989849700115485, 0.2600875025084915, 0.10623857559038301, 0.22997998101496187, 0.04604510861289512]
Printing some Q and Qe and total Qs values:  [[0.95 ]
 [0.978]
 [0.95 ]
 [0.95 ]
 [0.95 ]
 [0.95 ]
 [0.95 ]] [[39.823]
 [41.752]
 [39.823]
 [39.823]
 [39.823]
 [39.823]
 [39.823]] [[0.95 ]
 [0.978]
 [0.95 ]
 [0.95 ]
 [0.95 ]
 [0.95 ]
 [0.95 ]]
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.7902066666666666 0.37533333333333313 0.37533333333333313
probs:  [0.312658982573153, 0.044989849700115485, 0.2600875025084915, 0.10623857559038301, 0.22997998101496187, 0.04604510861289512]
actor:  0 policy actor:  1  step number:  67 total reward:  0.10666666666666591  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  73 total reward:  0.11999999999999922  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  54.076076067503315
maxi score, test score, baseline:  -0.7879933333333333 0.37533333333333313 0.37533333333333313
maxi score, test score, baseline:  -0.7879933333333333 0.37533333333333313 0.37533333333333313
printing an ep nov before normalisation:  55.5833897375054
actions average: 
K:  1  action  0 :  tensor([0.4976, 0.0227, 0.0728, 0.0834, 0.1141, 0.0994, 0.1100],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0016, 0.9847, 0.0020, 0.0038, 0.0026, 0.0022, 0.0031],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1047, 0.0118, 0.2798, 0.1300, 0.1805, 0.1395, 0.1538],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0917, 0.0422, 0.1169, 0.2994, 0.1273, 0.1341, 0.1884],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0655, 0.0454, 0.0561, 0.0760, 0.6054, 0.0864, 0.0653],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0615, 0.0282, 0.0599, 0.0670, 0.0818, 0.6205, 0.0810],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0897, 0.1085, 0.0951, 0.1005, 0.0901, 0.1149, 0.4012],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7879933333333333 0.37533333333333313 0.37533333333333313
probs:  [0.31319296067570496, 0.04506649123753315, 0.2592753008978827, 0.10641986603377214, 0.2299218280009092, 0.04612355315419789]
printing an ep nov before normalisation:  64.30533528327942
maxi score, test score, baseline:  -0.7879933333333333 0.37533333333333313 0.37533333333333313
probs:  [0.31319296067570496, 0.04506649123753315, 0.2592753008978827, 0.10641986603377214, 0.2299218280009092, 0.04612355315419789]
maxi score, test score, baseline:  -0.7879933333333333 0.37533333333333313 0.37533333333333313
probs:  [0.3125980784066373, 0.04498209753100995, 0.2596629123846907, 0.10647066336620828, 0.23024475964718386, 0.04604148866427007]
printing an ep nov before normalisation:  25.091835957580443
printing an ep nov before normalisation:  34.958215179876376
printing an ep nov before normalisation:  20.796908305142477
maxi score, test score, baseline:  -0.7879933333333333 0.37533333333333313 0.37533333333333313
probs:  [0.3121461786454518, 0.044918223143864545, 0.2601663287902146, 0.10656927158039137, 0.23021958413797336, 0.045980413702104254]
printing an ep nov before normalisation:  35.0361446907463
maxi score, test score, baseline:  -0.7879933333333333 0.37533333333333313 0.37533333333333313
probs:  [0.3121461786454518, 0.044918223143864545, 0.2601663287902146, 0.10656927158039137, 0.23021958413797336, 0.045980413702104254]
maxi score, test score, baseline:  -0.7879933333333333 0.37533333333333313 0.37533333333333313
probs:  [0.3121461786454518, 0.044918223143864545, 0.2601663287902146, 0.10656927158039137, 0.23021958413797336, 0.045980413702104254]
actor:  0 policy actor:  0  step number:  70 total reward:  0.2466666666666658  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.018]
 [ 0.039]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.021]
 [-0.021]] [[35.982]
 [36.831]
 [35.982]
 [35.982]
 [35.982]
 [35.91 ]
 [36.294]] [[1.217]
 [1.332]
 [1.217]
 [1.217]
 [1.217]
 [1.21 ]
 [1.236]]
using explorer policy with actor:  1
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 24.06826045511498
using explorer policy with actor:  1
siam score:  -0.856745
maxi score, test score, baseline:  -0.7855000000000002 0.37533333333333313 0.37533333333333313
probs:  [0.3121461786454518, 0.044918223143864545, 0.2601663287902146, 0.10656927158039137, 0.23021958413797336, 0.045980413702104254]
printing an ep nov before normalisation:  42.81381911853106
maxi score, test score, baseline:  -0.7855000000000002 0.37533333333333313 0.37533333333333313
probs:  [0.3121461786454518, 0.044918223143864545, 0.2601663287902146, 0.10656927158039137, 0.23021958413797336, 0.045980413702104254]
maxi score, test score, baseline:  -0.7855000000000002 0.37533333333333313 0.37533333333333313
probs:  [0.31169605080923707, 0.04485460022781313, 0.2606686749511425, 0.1066677527017885, 0.23019333762129207, 0.04591958368872673]
maxi score, test score, baseline:  -0.7855000000000002 0.37533333333333313 0.37533333333333313
probs:  [0.31110798936391726, 0.04477117518387745, 0.261052747669892, 0.10671822768510149, 0.23051139447876348, 0.045838465618448426]
maxi score, test score, baseline:  -0.7855000000000002 0.37533333333333313 0.37533333333333313
probs:  [0.31110798936391726, 0.04477117518387745, 0.261052747669892, 0.10671822768510149, 0.23051139447876348, 0.045838465618448426]
maxi score, test score, baseline:  -0.7855000000000002 0.37533333333333313 0.37533333333333313
probs:  [0.31110798936391726, 0.04477117518387745, 0.261052747669892, 0.10671822768510149, 0.23051139447876348, 0.045838465618448426]
maxi score, test score, baseline:  -0.7855000000000002 0.37533333333333313 0.37533333333333313
Printing some Q and Qe and total Qs values:  [[0.888]
 [0.973]
 [0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]] [[36.699]
 [35.393]
 [36.699]
 [36.699]
 [36.699]
 [36.699]
 [36.699]] [[0.888]
 [0.973]
 [0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]]
maxi score, test score, baseline:  -0.7855000000000002 0.37533333333333313 0.37533333333333313
probs:  [0.3113037867429656, 0.04479928033696337, 0.26058790767193574, 0.10678533623003064, 0.23065644625419374, 0.04586724276391092]
printing an ep nov before normalisation:  42.708718019756816
maxi score, test score, baseline:  -0.7855000000000002 0.37533333333333313 0.37533333333333313
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.7855000000000002 0.37533333333333313 0.37533333333333313
probs:  [0.31071799755450674, 0.04471617835334546, 0.26096977028346763, 0.1068357969913043, 0.2309738148744888, 0.04578644194288719]
printing an ep nov before normalisation:  47.96486854553223
printing an ep nov before normalisation:  51.25053917722619
printing an ep nov before normalisation:  37.90383109421208
from probs:  [0.31071799755450674, 0.04471617835334546, 0.26096977028346763, 0.1068357969913043, 0.2309738148744888, 0.04578644194288719]
Printing some Q and Qe and total Qs values:  [[0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]] [[25.301]
 [25.301]
 [25.301]
 [25.301]
 [25.301]
 [25.301]
 [25.301]] [[0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]]
printing an ep nov before normalisation:  30.85869036752527
maxi score, test score, baseline:  -0.7855000000000002 0.37533333333333313 0.37533333333333313
probs:  [0.31071799755450674, 0.04471617835334546, 0.26096977028346763, 0.1068357969913043, 0.2309738148744888, 0.04578644194288719]
actor:  0 policy actor:  0  step number:  56 total reward:  0.35333333333333317  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  32.7110438483845
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
probs:  [0.31071799755450674, 0.04471617835334546, 0.26096977028346763, 0.1068357969913043, 0.2309738148744888, 0.04578644194288719]
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
probs:  [0.31071799755450674, 0.04471617835334546, 0.26096977028346763, 0.1068357969913043, 0.2309738148744888, 0.04578644194288719]
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
probs:  [0.31071799755450674, 0.04471617835334546, 0.26096977028346763, 0.1068357969913043, 0.2309738148744888, 0.04578644194288719]
printing an ep nov before normalisation:  27.391237832915383
printing an ep nov before normalisation:  30.036844458146465
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
probs:  [0.31013441406489645, 0.044633389277430845, 0.2613501950502266, 0.10688606775036347, 0.2312899884921887, 0.04570594536489396]
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
probs:  [0.31013441406489645, 0.044633389277430845, 0.2613501950502266, 0.10688606775036347, 0.2312899884921887, 0.04570594536489396]
Printing some Q and Qe and total Qs values:  [[0.665]
 [0.772]
 [0.665]
 [0.665]
 [0.665]
 [0.665]
 [0.665]] [[25.9  ]
 [28.435]
 [25.9  ]
 [25.9  ]
 [25.9  ]
 [25.9  ]
 [25.9  ]] [[1.433]
 [1.682]
 [1.433]
 [1.433]
 [1.433]
 [1.433]
 [1.433]]
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
probs:  [0.31013441406489645, 0.044633389277430845, 0.2613501950502266, 0.10688606775036347, 0.2312899884921887, 0.04570594536489396]
Printing some Q and Qe and total Qs values:  [[0.1  ]
 [0.282]
 [0.1  ]
 [0.1  ]
 [0.118]
 [0.1  ]
 [0.1  ]] [[38.264]
 [34.796]
 [38.264]
 [38.264]
 [37.349]
 [38.264]
 [38.264]] [[1.582]
 [1.508]
 [1.582]
 [1.582]
 [1.532]
 [1.582]
 [1.582]]
actor:  1 policy actor:  1  step number:  86 total reward:  0.09999999999999953  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
probs:  [0.31032942423551896, 0.04466138266791443, 0.2608859570810664, 0.10695322197618433, 0.23143540057895606, 0.0457346134603599]
Printing some Q and Qe and total Qs values:  [[-0.103]
 [-0.106]
 [-0.213]
 [-0.108]
 [-0.104]
 [-0.2  ]
 [-0.095]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.103]
 [-0.106]
 [-0.213]
 [-0.108]
 [-0.104]
 [-0.2  ]
 [-0.095]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
probs:  [0.30974808301319534, 0.044578912387589854, 0.2612642018650183, 0.1070034784630587, 0.23175089433163107, 0.045654429939506885]
actor:  1 policy actor:  1  step number:  76 total reward:  0.07333333333333314  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8445532
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
probs:  [0.30988823721897685, 0.04459903173795275, 0.26138240873940377, 0.1070518557128816, 0.2314034304434364, 0.04567503614734851]
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
probs:  [0.30988823721897685, 0.04459903173795275, 0.26138240873940377, 0.1070518557128816, 0.2314034304434364, 0.04567503614734851]
from probs:  [0.30988823721897685, 0.04459903173795275, 0.26138240873940377, 0.1070518557128816, 0.2314034304434364, 0.04567503614734851]
printing an ep nov before normalisation:  35.533174993915225
actions average: 
K:  3  action  0 :  tensor([0.5312, 0.0027, 0.0647, 0.0714, 0.1774, 0.0588, 0.0937],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0129, 0.8756, 0.0124, 0.0209, 0.0293, 0.0148, 0.0341],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1561, 0.0060, 0.1475, 0.1502, 0.1722, 0.1600, 0.2080],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1780, 0.0283, 0.1265, 0.0974, 0.1648, 0.0982, 0.3067],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0708, 0.0026, 0.0574, 0.0742, 0.6096, 0.1107, 0.0747],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1253, 0.0364, 0.0921, 0.1074, 0.1607, 0.3074, 0.1706],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0880, 0.1326, 0.1002, 0.1520, 0.1617, 0.1115, 0.2539],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
printing an ep nov before normalisation:  44.40715246728111
printing an ep nov before normalisation:  45.920871531475484
Printing some Q and Qe and total Qs values:  [[-0.   ]
 [ 0.064]
 [-0.   ]
 [ 0.008]
 [ 0.014]
 [ 0.046]
 [-0.003]] [[31.207]
 [37.918]
 [31.207]
 [35.775]
 [35.405]
 [38.806]
 [37.466]] [[0.711]
 [1.071]
 [0.711]
 [0.921]
 [0.911]
 [1.092]
 [0.984]]
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
probs:  [0.30944874868563227, 0.044536921832221264, 0.2618778206075466, 0.1071503614857486, 0.2313704538876071, 0.045615693501244226]
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
probs:  [0.30944874868563227, 0.044536921832221264, 0.2618778206075466, 0.1071503614857486, 0.2313704538876071, 0.045615693501244226]
printing an ep nov before normalisation:  47.36501332873321
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
printing an ep nov before normalisation:  48.12166410149427
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
probs:  [0.30964332388231575, 0.04456485401502835, 0.26141392324401497, 0.10721768070694114, 0.2315159138653968, 0.045644304286303]
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
probs:  [0.30964332388231575, 0.04456485401502835, 0.26141392324401497, 0.10721768070694114, 0.2315159138653968, 0.045644304286303]
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
probs:  [0.30964332388231575, 0.04456485401502835, 0.26141392324401497, 0.10721768070694114, 0.2315159138653968, 0.045644304286303]
actions average: 
K:  4  action  0 :  tensor([0.2683, 0.0362, 0.0971, 0.1051, 0.1959, 0.1586, 0.1387],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0327, 0.8535, 0.0127, 0.0194, 0.0406, 0.0147, 0.0263],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0649, 0.0927, 0.3974, 0.0964, 0.0837, 0.1623, 0.1026],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1111, 0.0644, 0.0949, 0.3221, 0.1484, 0.1373, 0.1218],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1121, 0.0429, 0.0631, 0.0854, 0.4728, 0.1220, 0.1016],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1054, 0.0460, 0.0834, 0.1159, 0.1847, 0.3684, 0.0963],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0851, 0.0925, 0.0804, 0.1157, 0.0954, 0.0845, 0.4463],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
probs:  [0.30983702044307115, 0.0445926600655237, 0.2609521206844332, 0.10728469593724788, 0.23166071699501278, 0.0456727858747113]
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
probs:  [0.30983702044307115, 0.0445926600655237, 0.2609521206844332, 0.10728469593724788, 0.23166071699501278, 0.0456727858747113]
printing an ep nov before normalisation:  36.755292908182106
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
probs:  [0.30983702044307115, 0.0445926600655237, 0.2609521206844332, 0.10728469593724788, 0.23166071699501278, 0.0456727858747113]
printing an ep nov before normalisation:  39.74602407407271
printing an ep nov before normalisation:  36.31237173266193
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
probs:  [0.3092601853002454, 0.044510831413551284, 0.2613268971306049, 0.10733517258456851, 0.23197367685032164, 0.045593236720708155]
printing an ep nov before normalisation:  28.170557022094727
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
probs:  [0.30868549915885085, 0.04442930761452062, 0.26170027734575785, 0.10738546118095522, 0.2322854707725466, 0.04551398392736877]
actor:  1 policy actor:  1  step number:  69 total reward:  0.19999999999999962  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
probs:  [0.30887841591673304, 0.04445700294225306, 0.2612390852133372, 0.10745251871120859, 0.2324306197870141, 0.04554235742945392]
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
probs:  [0.30887841591673304, 0.04445700294225306, 0.2612390852133372, 0.10745251871120859, 0.2324306197870141, 0.04554235742945392]
printing an ep nov before normalisation:  36.69205245246351
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
probs:  [0.30887841591673304, 0.04445700294225306, 0.2612390852133372, 0.10745251871120859, 0.2324306197870141, 0.04554235742945392]
printing an ep nov before normalisation:  54.52555807384501
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
probs:  [0.3090704664489941, 0.04448457391374641, 0.2607799639038237, 0.10751927514293928, 0.2325751170603025, 0.045570603530193904]
printing an ep nov before normalisation:  31.932053097009195
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
probs:  [0.3084980055050167, 0.04440336714596891, 0.26115049865208795, 0.10756971885323172, 0.2328867448652731, 0.045491664978421505]
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
printing an ep nov before normalisation:  42.465964619886854
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
from probs:  [0.3084980055050167, 0.04440336714596891, 0.26115049865208795, 0.10756971885323172, 0.2328867448652731, 0.045491664978421505]
Printing some Q and Qe and total Qs values:  [[-0.001]
 [ 0.021]
 [-0.001]
 [-0.001]
 [-0.001]
 [ 0.013]
 [ 0.013]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.001]
 [ 0.021]
 [-0.001]
 [-0.001]
 [-0.001]
 [ 0.013]
 [ 0.013]]
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
printing an ep nov before normalisation:  42.86284678517987
actions average: 
K:  1  action  0 :  tensor([0.5025, 0.0602, 0.0580, 0.0574, 0.1253, 0.0800, 0.1165],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0033, 0.9061, 0.0039, 0.0282, 0.0147, 0.0067, 0.0371],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1218, 0.0111, 0.3016, 0.1193, 0.1488, 0.1557, 0.1417],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1672, 0.0103, 0.1120, 0.2537, 0.1693, 0.1348, 0.1528],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1028, 0.0103, 0.0782, 0.0880, 0.5041, 0.1121, 0.1044],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0994, 0.0879, 0.1014, 0.0986, 0.1271, 0.3892, 0.0964],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1220, 0.2156, 0.1051, 0.1066, 0.1183, 0.1492, 0.1831],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
probs:  [0.3086892401727956, 0.044430821596587455, 0.2606927088499212, 0.10763634637780632, 0.23303108865800695, 0.04551979434488247]
printing an ep nov before normalisation:  23.388848304748535
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
probs:  [0.3086892401727956, 0.044430821596587455, 0.2606927088499212, 0.10763634637780632, 0.23303108865800695, 0.04551979434488247]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.895]
 [0.895]
 [0.895]
 [0.895]
 [0.895]
 [0.895]
 [0.895]] [[38.845]
 [38.845]
 [38.845]
 [38.845]
 [38.845]
 [38.845]
 [38.845]] [[0.895]
 [0.895]
 [0.895]
 [0.895]
 [0.895]
 [0.895]
 [0.895]]
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
probs:  [0.3088796204971724, 0.04445815339408555, 0.260236964229737, 0.10770267624303555, 0.23317478759294188, 0.04554779804302758]
actor:  1 policy actor:  1  step number:  61 total reward:  0.26666666666666605  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
printing an ep nov before normalisation:  32.644589946781366
printing an ep nov before normalisation:  39.453072031527455
Printing some Q and Qe and total Qs values:  [[-0.103]
 [-0.103]
 [-0.103]
 [-0.103]
 [-0.094]
 [-0.103]
 [-0.103]] [[50.569]
 [50.569]
 [50.569]
 [50.569]
 [44.226]
 [50.569]
 [50.569]] [[0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.631]
 [0.813]
 [0.813]]
Printing some Q and Qe and total Qs values:  [[-0.024]
 [-0.012]
 [-0.023]
 [-0.023]
 [-0.023]
 [-0.021]
 [-0.024]] [[45.327]
 [43.044]
 [46.703]
 [47.06 ]
 [47.214]
 [47.703]
 [47.469]] [[0.757]
 [0.701]
 [0.798]
 [0.809]
 [0.813]
 [0.83 ]
 [0.821]]
maxi score, test score, baseline:  -0.7827933333333335 0.37533333333333313 0.37533333333333313
probs:  [0.30849894267077294, 0.04440447799796081, 0.26015025334675296, 0.10781947442331233, 0.2336297918122072, 0.04549705974899392]
using explorer policy with actor:  1
printing an ep nov before normalisation:  25.383505216699167
actor:  0 policy actor:  1  step number:  62 total reward:  0.20666666666666633  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.7803800000000001 0.37533333333333313 0.37533333333333313
probs:  [0.3086876751754964, 0.04443157382510232, 0.25969784256465167, 0.10788538283724457, 0.2337727013163875, 0.045524824281117364]
siam score:  -0.84194213
maxi score, test score, baseline:  -0.7803800000000001 0.37533333333333313 0.37533333333333313
probs:  [0.3086876751754964, 0.04443157382510232, 0.25969784256465167, 0.10788538283724457, 0.2337727013163875, 0.045524824281117364]
maxi score, test score, baseline:  -0.7803800000000001 0.37533333333333313 0.37533333333333313
probs:  [0.3086876751754964, 0.04443157382510232, 0.25969784256465167, 0.10788538283724457, 0.2337727013163875, 0.045524824281117364]
printing an ep nov before normalisation:  26.475875701246903
actor:  1 policy actor:  1  step number:  54 total reward:  0.5000000000000002  reward:  1.0 rdn_beta:  1.0
from probs:  [0.3086876751754964, 0.04443157382510232, 0.25969784256465167, 0.10788538283724457, 0.2337727013163875, 0.045524824281117364]
printing an ep nov before normalisation:  29.019257963925774
siam score:  -0.84200907
siam score:  -0.8430953
printing an ep nov before normalisation:  39.36936284060499
maxi score, test score, baseline:  -0.7803800000000001 0.37533333333333313 0.37533333333333313
probs:  [0.2576497097215078, 0.049480635357769955, 0.3444260512474189, 0.09946669289593156, 0.19863506186837857, 0.05034184890899323]
printing an ep nov before normalisation:  36.45977654074432
maxi score, test score, baseline:  -0.7803800000000002 0.37533333333333313 0.37533333333333313
printing an ep nov before normalisation:  26.60892963409424
line 256 mcts: sample exp_bonus 43.321049691570906
printing an ep nov before normalisation:  29.08400214182594
printing an ep nov before normalisation:  34.75398983790403
printing an ep nov before normalisation:  28.98114321340639
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  77 total reward:  0.23999999999999944  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  9.800453980763754
maxi score, test score, baseline:  -0.7803800000000002 0.37533333333333313 0.37533333333333313
maxi score, test score, baseline:  -0.7803800000000002 0.37533333333333313 0.37533333333333313
probs:  [0.25832239011503483, 0.049385650796206784, 0.34375562947288535, 0.09955604188502888, 0.19873024748678222, 0.05025004024406192]
printing an ep nov before normalisation:  41.129839234329744
maxi score, test score, baseline:  -0.7803800000000002 0.37533333333333313 0.37533333333333313
probs:  [0.25771662820719377, 0.049425913657601135, 0.34403653384340543, 0.09963731802732677, 0.19889259653810995, 0.05029100972636284]
maxi score, test score, baseline:  -0.7803800000000002 0.37533333333333313 0.37533333333333313
probs:  [0.25771662820719377, 0.049425913657601135, 0.34403653384340543, 0.09963731802732677, 0.19889259653810995, 0.05029100972636284]
printing an ep nov before normalisation:  66.4619947541523
maxi score, test score, baseline:  -0.7803800000000002 0.37533333333333313 0.37533333333333313
probs:  [0.25771662820719377, 0.049425913657601135, 0.34403653384340543, 0.09963731802732677, 0.19889259653810995, 0.05029100972636284]
from probs:  [0.25771662820719377, 0.049425913657601135, 0.34403653384340543, 0.09963731802732677, 0.19889259653810995, 0.05029100972636284]
maxi score, test score, baseline:  -0.7803800000000002 0.37533333333333313 0.37533333333333313
probs:  [0.25771662820719377, 0.049425913657601135, 0.34403653384340543, 0.09963731802732677, 0.19889259653810995, 0.05029100972636284]
line 256 mcts: sample exp_bonus 24.97484371218945
maxi score, test score, baseline:  -0.7803800000000002 0.37533333333333313 0.37533333333333313
probs:  [0.25780906169273077, 0.04944360227742846, 0.34415994312048837, 0.09967302494760896, 0.19860535917689726, 0.05030900878484618]
maxi score, test score, baseline:  -0.7803800000000002 0.37533333333333313 0.37533333333333313
probs:  [0.2579011188002416, 0.04946121887132519, 0.3442828498898508, 0.09970858647361006, 0.19831929141164237, 0.05032693455333007]
printing an ep nov before normalisation:  34.529926014013824
maxi score, test score, baseline:  -0.7803800000000002 0.37533333333333313 0.37533333333333313
probs:  [0.2579011188002416, 0.04946121887132519, 0.3442828498898508, 0.09970858647361006, 0.19831929141164237, 0.05032693455333007]
printing an ep nov before normalisation:  46.087443552984325
actor:  1 policy actor:  1  step number:  57 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  25.365697057099855
printing an ep nov before normalisation:  31.752665699486766
maxi score, test score, baseline:  -0.7803800000000002 0.37533333333333313 0.37533333333333313
printing an ep nov before normalisation:  34.14891421068774
Printing some Q and Qe and total Qs values:  [[0.867]
 [0.899]
 [0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.867]] [[24.842]
 [26.995]
 [24.842]
 [24.842]
 [24.842]
 [24.842]
 [24.842]] [[0.867]
 [0.899]
 [0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.867]]
printing an ep nov before normalisation:  25.349861973060744
printing an ep nov before normalisation:  43.6225531433044
maxi score, test score, baseline:  -0.7803800000000002 0.37533333333333313 0.37533333333333313
probs:  [0.25818998331544357, 0.04940510343954374, 0.34388748341028325, 0.09973563330691028, 0.1985095445973402, 0.05027225193047909]
from probs:  [0.25818998331544357, 0.04940510343954374, 0.34388748341028325, 0.09973563330691028, 0.1985095445973402, 0.05027225193047909]
maxi score, test score, baseline:  -0.7803800000000001 0.37533333333333313 0.37533333333333313
probs:  [0.25818998331544357, 0.04940510343954374, 0.34388748341028325, 0.09973563330691028, 0.1985095445973402, 0.05027225193047909]
printing an ep nov before normalisation:  25.649020671844482
maxi score, test score, baseline:  -0.7803800000000001 0.37533333333333313 0.37533333333333313
probs:  [0.25818998331544357, 0.04940510343954374, 0.34388748341028325, 0.09973563330691028, 0.1985095445973402, 0.05027225193047909]
Printing some Q and Qe and total Qs values:  [[0.143]
 [0.158]
 [0.143]
 [0.128]
 [0.143]
 [0.143]
 [0.143]] [[32.73 ]
 [32.493]
 [32.73 ]
 [32.82 ]
 [32.73 ]
 [32.73 ]
 [32.73 ]] [[0.42 ]
 [0.431]
 [0.42 ]
 [0.407]
 [0.42 ]
 [0.42 ]
 [0.42 ]]
printing an ep nov before normalisation:  55.56860305351876
printing an ep nov before normalisation:  28.81299160582232
maxi score, test score, baseline:  -0.7803800000000001 0.37533333333333313 0.37533333333333313
probs:  [0.25818998331544357, 0.04940510343954374, 0.34388748341028325, 0.09973563330691028, 0.1985095445973402, 0.05027225193047909]
maxi score, test score, baseline:  -0.7803800000000001 0.37533333333333313 0.37533333333333313
probs:  [0.25818998331544357, 0.04940510343954374, 0.34388748341028325, 0.09973563330691028, 0.1985095445973402, 0.05027225193047909]
printing an ep nov before normalisation:  34.4546698650994
maxi score, test score, baseline:  -0.7803800000000001 0.37533333333333313 0.37533333333333313
probs:  [0.25818998331544357, 0.04940510343954374, 0.34388748341028325, 0.09973563330691028, 0.1985095445973402, 0.05027225193047909]
maxi score, test score, baseline:  -0.7803800000000001 0.37533333333333313 0.37533333333333313
actions average: 
K:  0  action  0 :  tensor([0.4183, 0.0164, 0.0680, 0.1020, 0.1355, 0.1233, 0.1366],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0034, 0.9512, 0.0030, 0.0062, 0.0027, 0.0054, 0.0282],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1573, 0.0044, 0.1162, 0.1722, 0.1365, 0.2105, 0.2031],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1490, 0.0432, 0.0878, 0.2823, 0.1012, 0.1455, 0.1909],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0902, 0.0115, 0.0545, 0.0891, 0.5651, 0.1130, 0.0766],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0899, 0.0073, 0.0652, 0.1227, 0.1185, 0.4574, 0.1391],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1630, 0.0039, 0.0970, 0.1421, 0.1047, 0.1438, 0.3455],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  21.397413884989312
printing an ep nov before normalisation:  26.01626870138499
printing an ep nov before normalisation:  43.918035292191206
Printing some Q and Qe and total Qs values:  [[0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]] [[36.827]
 [36.827]
 [36.827]
 [36.827]
 [36.827]
 [36.827]
 [36.827]] [[1.416]
 [1.416]
 [1.416]
 [1.416]
 [1.416]
 [1.416]
 [1.416]]
maxi score, test score, baseline:  -0.7803800000000001 0.37533333333333313 0.37533333333333313
probs:  [0.2571704881562639, 0.04952019341399514, 0.3446904296602748, 0.09996828607348525, 0.19826123529284773, 0.05038936740313304]
maxi score, test score, baseline:  -0.7803800000000001 0.37533333333333313 0.37533333333333313
maxi score, test score, baseline:  -0.7803800000000001 0.37533333333333313 0.37533333333333313
probs:  [0.2565745129207493, 0.04955985234888325, 0.3449671174708236, 0.10004845605323469, 0.19842033690044392, 0.05042972430586512]
actions average: 
K:  1  action  0 :  tensor([0.3875, 0.0080, 0.0995, 0.0856, 0.1724, 0.1205, 0.1265],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0089, 0.9551, 0.0047, 0.0020, 0.0021, 0.0013, 0.0259],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1685, 0.0250, 0.3638, 0.0818, 0.1092, 0.1180, 0.1337],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0613, 0.0212, 0.0744, 0.4189, 0.1481, 0.1445, 0.1317],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1448, 0.0406, 0.1035, 0.0950, 0.3333, 0.1449, 0.1378],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1197, 0.0061, 0.1059, 0.0892, 0.1266, 0.4430, 0.1094],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0957, 0.0660, 0.1114, 0.1033, 0.1118, 0.1382, 0.3736],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.7803800000000001 0.37533333333333313 0.37533333333333313
probs:  [0.2553934418118734, 0.04963844625583426, 0.3455154422329006, 0.10020733253205258, 0.198735635760044, 0.05050970140729524]
printing an ep nov before normalisation:  41.16977256734109
maxi score, test score, baseline:  -0.7803800000000001 0.37533333333333313 0.37533333333333313
probs:  [0.2557691333718369, 0.049600142502741626, 0.3452443313976707, 0.10027077737489677, 0.19864246466442628, 0.050473150688427657]
printing an ep nov before normalisation:  54.2691825489485
maxi score, test score, baseline:  -0.7803800000000001 0.37533333333333313 0.37533333333333313
probs:  [0.25518308928255584, 0.04963913008823253, 0.3455163323601728, 0.10034970126284304, 0.1987989206667134, 0.05051282633948233]
maxi score, test score, baseline:  -0.7803800000000001 0.37533333333333313 0.37533333333333313
maxi score, test score, baseline:  -0.7803800000000001 0.37533333333333313 0.37533333333333313
probs:  [0.2555573861014639, 0.04960094924261371, 0.34524608204287227, 0.10041328442939422, 0.19870589938995986, 0.05047639879369597]
printing an ep nov before normalisation:  38.03475540437536
printing an ep nov before normalisation:  36.16888752386464
maxi score, test score, baseline:  -0.7803800000000001 0.37533333333333313 0.37533333333333313
probs:  [0.2549740083499995, 0.04963974902922607, 0.3455167697654573, 0.10049193864356573, 0.19886164897650802, 0.050515885235243314]
maxi score, test score, baseline:  -0.7803800000000001 0.37533333333333313 0.37533333333333313
probs:  [0.2549740083499995, 0.04963974902922607, 0.3455167697654573, 0.10049193864356573, 0.19886164897650802, 0.050515885235243314]
actor:  1 policy actor:  1  step number:  55 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  37.81967495248522
maxi score, test score, baseline:  -0.7803800000000001 0.37533333333333313 0.37533333333333313
probs:  [0.2553469197960683, 0.04960169026439855, 0.345247374184517, 0.10055565897375857, 0.1987687767511458, 0.050479580030111854]
maxi score, test score, baseline:  -0.7803800000000001 0.37533333333333313 0.37533333333333313
probs:  [0.2553469197960683, 0.04960169026439855, 0.345247374184517, 0.10055565897375857, 0.1987687767511458, 0.050479580030111854]
actor:  0 policy actor:  1  step number:  79 total reward:  0.039999999999998925  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.2556291859774839, 0.049546272818031374, 0.34485688409883664, 0.10058387078597918, 0.1989581828811274, 0.05042560343854143]
siam score:  -0.8447434
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.2556291859774839, 0.049546272818031374, 0.34485688409883664, 0.10058387078597918, 0.1989581828811274, 0.05042560343854143]
actor:  1 policy actor:  1  step number:  64 total reward:  0.25999999999999934  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.25504771868491394, 0.04958490652691638, 0.34512640711925313, 0.10066240836356007, 0.19911363465116988, 0.05046492465418652]
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.25504771868491394, 0.04958490652691638, 0.34512640711925313, 0.10066240836356007, 0.19911363465116988, 0.05046492465418652]
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.25504771868491394, 0.04958490652691638, 0.34512640711925313, 0.10066240836356007, 0.19911363465116988, 0.05046492465418652]
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
actor:  1 policy actor:  1  step number:  73 total reward:  0.1466666666666665  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  35.06873230119423
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.2554188639663749, 0.04954710334114958, 0.3448588050006934, 0.10072626866553845, 0.19902008598993434, 0.050428873036309316]
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.2554188639663749, 0.04954710334114958, 0.3448588050006934, 0.10072626866553845, 0.19902008598993434, 0.050428873036309316]
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.2556993720940589, 0.04949196508623089, 0.3444702757830944, 0.10075457119751344, 0.19920864345010814, 0.05037517238899434]
actions average: 
K:  2  action  0 :  tensor([0.4600, 0.0077, 0.0611, 0.0902, 0.1668, 0.0866, 0.1277],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0275, 0.8918, 0.0067, 0.0070, 0.0403, 0.0024, 0.0243],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1097, 0.0462, 0.2279, 0.1314, 0.1581, 0.1587, 0.1680],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0678, 0.0125, 0.0932, 0.3236, 0.2077, 0.1572, 0.1380],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0923, 0.1102, 0.0756, 0.0876, 0.4218, 0.1019, 0.1106],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0570, 0.0119, 0.0762, 0.0923, 0.1852, 0.4651, 0.1122],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0808, 0.0210, 0.0873, 0.1553, 0.1099, 0.1065, 0.4393],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  26.02870225906372
printing an ep nov before normalisation:  27.947719503980526
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.2556993720940589, 0.04949196508623089, 0.3444702757830944, 0.10075457119751344, 0.19920864345010814, 0.05037517238899434]
printing an ep nov before normalisation:  49.19856938770703
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
Printing some Q and Qe and total Qs values:  [[0.351]
 [0.55 ]
 [0.345]
 [0.351]
 [0.342]
 [0.354]
 [0.348]] [[33.01 ]
 [47.781]
 [34.52 ]
 [38.01 ]
 [33.623]
 [38.041]
 [37.063]] [[0.351]
 [0.55 ]
 [0.345]
 [0.351]
 [0.342]
 [0.354]
 [0.348]]
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.2556993720940589, 0.04949196508623089, 0.3444702757830944, 0.10075457119751344, 0.19920864345010814, 0.05037517238899434]
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.2556993720940589, 0.04949196508623089, 0.3444702757830944, 0.10075457119751344, 0.19920864345010814, 0.05037517238899434]
printing an ep nov before normalisation:  26.586236125872347
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.517]
 [0.345]
 [0.274]
 [0.546]
 [0.513]
 [0.486]] [[23.463]
 [23.463]
 [22.146]
 [20.427]
 [24.17 ]
 [25.098]
 [25.194]] [[1.575]
 [1.575]
 [1.254]
 [0.99 ]
 [1.684]
 [1.755]
 [1.739]]
siam score:  -0.8382679
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.626]] [[24.751]
 [24.751]
 [24.751]
 [24.751]
 [24.751]
 [24.751]
 [24.751]] [[1.834]
 [1.834]
 [1.834]
 [1.834]
 [1.834]
 [1.834]
 [1.834]]
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.2556993720940589, 0.04949196508623089, 0.3444702757830944, 0.10075457119751344, 0.19920864345010814, 0.05037517238899434]
printing an ep nov before normalisation:  36.82709500932536
printing an ep nov before normalisation:  12.057594250314182
actions average: 
K:  4  action  0 :  tensor([0.4055, 0.0627, 0.0754, 0.0948, 0.1246, 0.1258, 0.1111],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0079, 0.8739, 0.0097, 0.0205, 0.0114, 0.0137, 0.0629],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1100, 0.0222, 0.3675, 0.0956, 0.0909, 0.1651, 0.1487],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1299, 0.0838, 0.0571, 0.3993, 0.0953, 0.1132, 0.1212],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1496, 0.0350, 0.0688, 0.0886, 0.4492, 0.1038, 0.1050],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0775, 0.0063, 0.0985, 0.0708, 0.0765, 0.5733, 0.0970],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1167, 0.0385, 0.1119, 0.1013, 0.1224, 0.1213, 0.3879],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  67 total reward:  0.29333333333333245  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  29.541218178603987
actor:  1 policy actor:  1  step number:  74 total reward:  0.19333333333333236  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.2558792785773617, 0.04952671130436591, 0.3447126727855578, 0.10082540386668469, 0.19864539312271703, 0.05041054034331298]
printing an ep nov before normalisation:  12.257177631060284
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.2559686939579477, 0.04954398053369791, 0.3448331465996765, 0.10086060843494483, 0.19836545189178237, 0.05042811858195063]
printing an ep nov before normalisation:  41.443855034546104
printing an ep nov before normalisation:  37.95025655332638
printing an ep nov before normalisation:  36.45265125473848
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.25624930271349994, 0.04948899312977614, 0.34444567440680907, 0.10088904933258218, 0.19855241184681083, 0.050374568570521784]
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.25624930271349994, 0.04948899312977614, 0.34444567440680907, 0.10088904933258218, 0.19855241184681083, 0.050374568570521784]
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
siam score:  -0.83482003
Printing some Q and Qe and total Qs values:  [[-0.116]
 [-0.122]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.092]
 [-0.112]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.116]
 [-0.122]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.092]
 [-0.112]]
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
printing an ep nov before normalisation:  33.413034469208554
printing an ep nov before normalisation:  26.071664974814855
printing an ep nov before normalisation:  24.573164037258362
printing an ep nov before normalisation:  36.55754100500488
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.25652928026939154, 0.049434129414192736, 0.3440590737915807, 0.10091742625543408, 0.19873895125523175, 0.05032113901416923]
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.2568086287529548, 0.0493793889700808, 0.3436733418165129, 0.10094573941911478, 0.19892507153441355, 0.050267829506923156]
printing an ep nov before normalisation:  33.346658685894774
printing an ep nov before normalisation:  41.926323280682716
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.2568086287529548, 0.0493793889700808, 0.3436733418165129, 0.10094573941911478, 0.19892507153441355, 0.050267829506923156]
actor:  1 policy actor:  1  step number:  72 total reward:  0.15333333333333243  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  35.5305544849314
Printing some Q and Qe and total Qs values:  [[-0.129]
 [-0.109]
 [-0.108]
 [-0.106]
 [-0.244]
 [-0.111]
 [-0.094]] [[44.042]
 [46.53 ]
 [42.353]
 [48.645]
 [52.622]
 [44.632]
 [43.867]] [[0.507]
 [0.598]
 [0.478]
 [0.663]
 [0.64 ]
 [0.541]
 [0.536]]
printing an ep nov before normalisation:  26.561061017392394
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.25736544696473557, 0.04927027623814938, 0.34290447210279834, 0.10100217532658572, 0.19929606034275868, 0.05016156902497223]
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.25736544696473557, 0.04927027623814938, 0.34290447210279834, 0.10100217532658572, 0.19929606034275868, 0.05016156902497223]
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.25736544696473557, 0.04927027623814938, 0.34290447210279834, 0.10100217532658572, 0.19929606034275868, 0.05016156902497223]
actor:  1 policy actor:  1  step number:  84 total reward:  0.019999999999999463  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.28 ]
 [0.319]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]] [[27.727]
 [30.459]
 [27.727]
 [27.727]
 [27.727]
 [27.727]
 [27.727]] [[0.794]
 [0.929]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]]
printing an ep nov before normalisation:  45.08386370133743
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.25764292090008717, 0.04921590312591021, 0.3425213285548266, 0.10103029849678705, 0.19948093167499056, 0.05010861724739843]
printing an ep nov before normalisation:  36.138469849776584
actor:  1 policy actor:  1  step number:  72 total reward:  0.19333333333333302  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.25791977417748463, 0.04916165163628239, 0.342139042028219, 0.10105835876064989, 0.19966538948419124, 0.050055783913172794]
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.25819600887704725, 0.04910752136165085, 0.34175760965069035, 0.10108635632900338, 0.19984943515627263, 0.05000306862533548]
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.25819600887704725, 0.04910752136165085, 0.34175760965069035, 0.10108635632900338, 0.19984943515627263, 0.05000306862533548]
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.25819600887704725, 0.04910752136165085, 0.34175760965069035, 0.10108635632900338, 0.19984943515627263, 0.05000306862533548]
Printing some Q and Qe and total Qs values:  [[-0.059]
 [-0.059]
 [-0.059]
 [-0.059]
 [-0.059]
 [-0.037]
 [-0.059]] [[34.377]
 [34.377]
 [34.377]
 [34.377]
 [34.377]
 [37.228]
 [34.377]] [[0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.804]
 [0.655]]
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.25819600887704725, 0.04910752136165085, 0.34175760965069035, 0.10108635632900338, 0.19984943515627263, 0.05000306862533548]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.25819600887704725, 0.04910752136165085, 0.34175760965069035, 0.10108635632900338, 0.19984943515627263, 0.05000306862533548]
printing an ep nov before normalisation:  36.75058263041611
UNIT TEST: sample policy line 217 mcts : [0.02  0.837 0.041 0.    0.041 0.041 0.02 ]
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.505]
 [0.589]
 [0.6  ]
 [0.535]
 [0.568]
 [0.629]] [[25.522]
 [32.218]
 [26.487]
 [27.557]
 [28.048]
 [28.564]
 [27.461]] [[1.082]
 [1.343]
 [1.156]
 [1.218]
 [1.176]
 [1.234]
 [1.243]]
printing an ep nov before normalisation:  25.445809025391952
actor:  1 policy actor:  1  step number:  45 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.2383129013679023, 0.0530037663725146, 0.3692127446568173, 0.09907111833592112, 0.18660200501668542, 0.053797464250159174]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
printing an ep nov before normalisation:  23.295539376145005
printing an ep nov before normalisation:  30.046184062957764
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.2383129013679023, 0.0530037663725146, 0.3692127446568173, 0.09907111833592112, 0.18660200501668542, 0.053797464250159174]
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.578]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]] [[21.686]
 [37.562]
 [21.686]
 [21.686]
 [21.686]
 [21.686]
 [21.686]] [[0.898]
 [1.459]
 [0.898]
 [0.898]
 [0.898]
 [0.898]
 [0.898]]
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.23857475644514506, 0.05295245389342616, 0.36885116805243434, 0.09909765846804579, 0.1867764700413741, 0.05374749309957467]
printing an ep nov before normalisation:  40.30940499327597
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.2386493936571183, 0.052968987422033764, 0.36896658452027276, 0.09912863641587775, 0.18652212249252811, 0.0537642754921693]
actor:  1 policy actor:  1  step number:  74 total reward:  0.13999999999999935  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.2389109430317677, 0.052917787330542636, 0.36860579802944954, 0.09915518499950417, 0.18669587167027657, 0.05371441493845932]
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.2391719804222867, 0.0528666874634713, 0.3682457177798783, 0.09918168161415328, 0.18686928073319276, 0.05366465198701755]
Printing some Q and Qe and total Qs values:  [[ 0.024]
 [ 0.099]
 [ 0.024]
 [ 0.024]
 [-0.002]
 [ 0.109]
 [ 0.199]] [[42.694]
 [43.796]
 [42.694]
 [42.694]
 [44.134]
 [40.888]
 [45.183]] [[1.637]
 [1.785]
 [1.637]
 [1.637]
 [1.706]
 [1.603]
 [1.976]]
printing an ep nov before normalisation:  45.75997478362958
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.2391719804222867, 0.0528666874634713, 0.3682457177798783, 0.09918168161415328, 0.18686928073319276, 0.05366465198701755]
Printing some Q and Qe and total Qs values:  [[0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.222]
 [0.074]] [[38.148]
 [38.148]
 [38.148]
 [38.148]
 [38.148]
 [38.856]
 [38.148]] [[1.755]
 [1.755]
 [1.755]
 [1.755]
 [1.755]
 [1.961]
 [1.755]]
printing an ep nov before normalisation:  44.93086055947418
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
printing an ep nov before normalisation:  35.996098285375865
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.2391719804222867, 0.0528666874634713, 0.3682457177798783, 0.09918168161415328, 0.18686928073319276, 0.05366465198701755]
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
printing an ep nov before normalisation:  44.72140909202125
printing an ep nov before normalisation:  43.81002294064894
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.2391719804222867, 0.0528666874634713, 0.3682457177798783, 0.09918168161415328, 0.18686928073319276, 0.05366465198701755]
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.2391719804222867, 0.0528666874634713, 0.3682457177798783, 0.09918168161415328, 0.18686928073319276, 0.05366465198701755]
actor:  1 policy actor:  1  step number:  63 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  1.0
UNIT TEST: sample policy line 217 mcts : [0.122 0.122 0.102 0.163 0.163 0.204 0.122]
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.23943250733052238, 0.05281568752682286, 0.3678863416998807, 0.09920812641227028, 0.1870423506789649, 0.053614986351539]
printing an ep nov before normalisation:  21.512230655601456
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
actor:  1 policy actor:  1  step number:  64 total reward:  0.086666666666666  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.042]
 [0.026]
 [0.001]
 [0.042]
 [0.042]
 [0.042]
 [0.008]] [[19.634]
 [24.382]
 [12.724]
 [19.634]
 [19.634]
 [19.634]
 [12.321]] [[0.042]
 [0.026]
 [0.001]
 [0.042]
 [0.042]
 [0.042]
 [0.008]]
actor:  1 policy actor:  1  step number:  71 total reward:  0.19999999999999973  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  45.55868287640167
printing an ep nov before normalisation:  28.56011972784027
printing an ep nov before normalisation:  43.119790787789974
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.23969252525245344, 0.05276478722774914, 0.3675276677258735, 0.09923451954570445, 0.18721508250138255, 0.05356541774683718]
Printing some Q and Qe and total Qs values:  [[0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]] [[31.679]
 [31.679]
 [31.679]
 [31.679]
 [31.679]
 [31.679]
 [31.679]] [[0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]]
printing an ep nov before normalisation:  48.456889865270966
printing an ep nov before normalisation:  26.636874517807296
maxi score, test score, baseline:  -0.7783000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.23969252525245344, 0.05276478722774914, 0.3675276677258735, 0.09923451954570445, 0.18721508250138255, 0.05356541774683718]
printing an ep nov before normalisation:  46.26966786786428
actor:  0 policy actor:  1  step number:  58 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.7757266666666668 0.37533333333333313 0.37533333333333313
probs:  [0.23969252525245344, 0.05276478722774914, 0.3675276677258735, 0.09923451954570445, 0.18721508250138255, 0.05356541774683718]
maxi score, test score, baseline:  -0.7757266666666668 0.37533333333333313 0.37533333333333313
printing an ep nov before normalisation:  18.40718496626007
maxi score, test score, baseline:  -0.7757266666666668 0.37533333333333313 0.37533333333333313
probs:  [0.23915587607932612, 0.05280196883160051, 0.3677872137936319, 0.0993045297254617, 0.18734724661328359, 0.0536031649566961]
maxi score, test score, baseline:  -0.7757266666666668 0.37533333333333313 0.37533333333333313
probs:  [0.23915587607932612, 0.05280196883160051, 0.3677872137936319, 0.0993045297254617, 0.18734724661328359, 0.0536031649566961]
siam score:  -0.8306999
maxi score, test score, baseline:  -0.7757266666666668 0.37533333333333313 0.37533333333333313
probs:  [0.23915587607932612, 0.05280196883160051, 0.3677872137936319, 0.0993045297254617, 0.18734724661328359, 0.0536031649566961]
printing an ep nov before normalisation:  25.04283064059909
maxi score, test score, baseline:  -0.7757266666666668 0.37533333333333313 0.37533333333333313
probs:  [0.23915587607932612, 0.05280196883160051, 0.3677872137936319, 0.0993045297254617, 0.18734724661328359, 0.0536031649566961]
printing an ep nov before normalisation:  29.339245819017716
actor:  1 policy actor:  1  step number:  67 total reward:  0.1466666666666664  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  40.71032836402616
Printing some Q and Qe and total Qs values:  [[-0.127]
 [-0.135]
 [-0.13 ]
 [-0.135]
 [-0.13 ]
 [-0.13 ]
 [-0.133]] [[33.022]
 [32.68 ]
 [32.876]
 [33.322]
 [33.169]
 [33.141]
 [33.253]] [[0.851]
 [0.823]
 [0.839]
 [0.861]
 [0.856]
 [0.855]
 [0.858]]
maxi score, test score, baseline:  -0.7757266666666668 0.37533333333333313 0.37533333333333313
probs:  [0.23941467864040875, 0.052751193697801436, 0.36742941772613685, 0.09933100628882441, 0.18751998284862106, 0.05355372079820747]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  75 total reward:  0.30666666666666687  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  21.146756194529964
maxi score, test score, baseline:  -0.7731133333333334 0.37533333333333313 0.37533333333333313
probs:  [0.23941467864040875, 0.052751193697801436, 0.36742941772613685, 0.09933100628882441, 0.18751998284862106, 0.05355372079820747]
maxi score, test score, baseline:  -0.7731133333333334 0.37533333333333313 0.37533333333333313
maxi score, test score, baseline:  -0.7731133333333334 0.37533333333333313 0.37533333333333313
probs:  [0.23941467864040875, 0.052751193697801436, 0.36742941772613685, 0.09933100628882441, 0.18751998284862106, 0.05355372079820747]
printing an ep nov before normalisation:  4.052539281929057e-06
from probs:  [0.23941467864040875, 0.052751193697801436, 0.36742941772613685, 0.09933100628882441, 0.18751998284862106, 0.05355372079820747]
printing an ep nov before normalisation:  42.504081886305094
maxi score, test score, baseline:  -0.7731133333333334 0.37533333333333313 0.37533333333333313
maxi score, test score, baseline:  -0.7731133333333334 0.37533333333333313 0.37533333333333313
probs:  [0.2394899318328786, 0.052767741867175595, 0.3675449311131322, 0.09936220364882517, 0.1872646701783978, 0.0535705213595906]
maxi score, test score, baseline:  -0.7731133333333334 0.37533333333333313 0.37533333333333313
Printing some Q and Qe and total Qs values:  [[0.737]
 [0.737]
 [0.737]
 [0.737]
 [0.827]
 [0.614]
 [0.724]] [[39.623]
 [39.623]
 [39.623]
 [39.623]
 [35.545]
 [38.365]
 [33.253]] [[0.737]
 [0.737]
 [0.737]
 [0.737]
 [0.827]
 [0.614]
 [0.724]]
maxi score, test score, baseline:  -0.7731133333333334 0.37533333333333313 0.37533333333333313
probs:  [0.2394899318328786, 0.052767741867175595, 0.3675449311131322, 0.09936220364882517, 0.1872646701783978, 0.0535705213595906]
Printing some Q and Qe and total Qs values:  [[0.689]
 [0.901]
 [0.811]
 [0.738]
 [0.553]
 [0.835]
 [0.858]] [[32.413]
 [32.811]
 [31.713]
 [33.06 ]
 [34.512]
 [29.869]
 [31.282]] [[0.689]
 [0.901]
 [0.811]
 [0.738]
 [0.553]
 [0.835]
 [0.858]]
maxi score, test score, baseline:  -0.7731133333333334 0.37533333333333313 0.37533333333333313
probs:  [0.2394899318328786, 0.052767741867175595, 0.3675449311131322, 0.09936220364882517, 0.1872646701783978, 0.0535705213595906]
Printing some Q and Qe and total Qs values:  [[0.817]
 [0.92 ]
 [0.82 ]
 [0.849]
 [0.871]
 [0.867]
 [0.897]] [[31.428]
 [29.977]
 [30.894]
 [30.931]
 [31.118]
 [31.62 ]
 [28.96 ]] [[0.817]
 [0.92 ]
 [0.82 ]
 [0.849]
 [0.871]
 [0.867]
 [0.897]]
actor:  1 policy actor:  1  step number:  68 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7731133333333334 0.37533333333333313 0.37533333333333313
probs:  [0.2394899318328786, 0.052767741867175595, 0.3675449311131322, 0.09936220364882517, 0.1872646701783978, 0.0535705213595906]
actor:  0 policy actor:  1  step number:  74 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.7707533333333335 0.37533333333333313 0.37533333333333313
probs:  [0.2394899318328786, 0.052767741867175595, 0.3675449311131322, 0.09936220364882517, 0.1872646701783978, 0.0535705213595906]
line 256 mcts: sample exp_bonus 42.61921723085478
printing an ep nov before normalisation:  29.97852087020874
printing an ep nov before normalisation:  42.61685156363466
maxi score, test score, baseline:  -0.7707533333333335 0.37533333333333313 0.37533333333333313
probs:  [0.2394899318328786, 0.052767741867175595, 0.3675449311131322, 0.09936220364882517, 0.1872646701783978, 0.0535705213595906]
maxi score, test score, baseline:  -0.7707533333333335 0.37533333333333313 0.37533333333333313
probs:  [0.23895574743017084, 0.05280474490917604, 0.36780322836728496, 0.0994319634631169, 0.18739622705976267, 0.053608088770488685]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7707533333333335 0.37533333333333313 0.37533333333333313
probs:  [0.23895574743017084, 0.05280474490917604, 0.36780322836728496, 0.0994319634631169, 0.18739622705976267, 0.053608088770488685]
actor:  1 policy actor:  1  step number:  60 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  66 total reward:  0.24666666666666648  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.576]
 [0.569]
 [0.569]
 [0.569]
 [0.569]
 [0.569]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.569]
 [0.576]
 [0.569]
 [0.569]
 [0.569]
 [0.569]
 [0.569]]
printing an ep nov before normalisation:  45.930271487585436
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.7707533333333335 0.37533333333333313 0.37533333333333313
probs:  [0.23936332837694496, 0.052787071765324954, 0.3676764995448091, 0.09952080822760909, 0.18706004125360642, 0.05359225083170543]
printing an ep nov before normalisation:  28.7581205368042
maxi score, test score, baseline:  -0.7707533333333335 0.37533333333333313 0.37533333333333313
probs:  [0.23936332837694496, 0.052787071765324954, 0.3676764995448091, 0.09952080822760909, 0.18706004125360642, 0.05359225083170543]
Printing some Q and Qe and total Qs values:  [[-0.129]
 [-0.049]
 [-0.089]
 [-0.089]
 [-0.148]
 [-0.063]
 [-0.134]] [[30.216]
 [34.662]
 [33.395]
 [33.395]
 [35.375]
 [43.01 ]
 [33.64 ]] [[0.358]
 [0.598]
 [0.512]
 [0.512]
 [0.525]
 [0.884]
 [0.477]]
printing an ep nov before normalisation:  44.49258910285102
printing an ep nov before normalisation:  37.145804041911205
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.708]
 [0.556]
 [0.578]
 [0.582]
 [0.591]
 [0.593]] [[29.618]
 [26.602]
 [27.799]
 [27.291]
 [28.542]
 [27.902]
 [29.679]] [[1.394]
 [1.468]
 [1.351]
 [1.358]
 [1.397]
 [1.389]
 [1.441]]
maxi score, test score, baseline:  -0.7707533333333335 0.37533333333333313 0.37533333333333313
probs:  [0.2394377678145504, 0.05280345566761794, 0.3677908652301327, 0.09955173391639063, 0.18680729209577487, 0.0536088852755334]
printing an ep nov before normalisation:  27.788967715224082
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  68 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.0
UNIT TEST: sample policy line 217 mcts : [0.224 0.102 0.184 0.163 0.    0.245 0.082]
printing an ep nov before normalisation:  34.04338359832764
Printing some Q and Qe and total Qs values:  [[-0.09]
 [-0.09]
 [-0.09]
 [-0.09]
 [-0.09]
 [-0.09]
 [-0.09]] [[29.949]
 [29.949]
 [29.949]
 [29.949]
 [29.949]
 [29.949]
 [29.949]] [[0.804]
 [0.804]
 [0.804]
 [0.804]
 [0.804]
 [0.804]
 [0.804]]
printing an ep nov before normalisation:  29.6388797173611
printing an ep nov before normalisation:  52.373738748522726
printing an ep nov before normalisation:  48.27639874228015
using another actor
from probs:  [0.2396956807572529, 0.05275294892089414, 0.3674349491202923, 0.09957848032474229, 0.18697823134757982, 0.05355970952923853]
maxi score, test score, baseline:  -0.7707533333333335 0.37533333333333313 0.37533333333333313
probs:  [0.2399530940436179, 0.05270254002120283, 0.3670797225289306, 0.09960517491711189, 0.18714883943771599, 0.053510629051420865]
maxi score, test score, baseline:  -0.7707533333333335 0.37533333333333313 0.37533333333333313
probs:  [0.2399530940436179, 0.05270254002120283, 0.3670797225289306, 0.09960517491711189, 0.18714883943771599, 0.053510629051420865]
maxi score, test score, baseline:  -0.7707533333333335 0.37533333333333313 0.37533333333333313
probs:  [0.2399530940436179, 0.05270254002120283, 0.3670797225289306, 0.09960517491711189, 0.18714883943771599, 0.053510629051420865]
maxi score, test score, baseline:  -0.7707533333333335 0.37533333333333313 0.37533333333333313
probs:  [0.2399530940436179, 0.05270254002120283, 0.3670797225289306, 0.09960517491711189, 0.18714883943771599, 0.053510629051420865]
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.535]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]] [[38.763]
 [38.969]
 [38.763]
 [38.763]
 [38.763]
 [38.763]
 [38.763]] [[0.413]
 [0.535]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]]
maxi score, test score, baseline:  -0.7707533333333335 0.37533333333333313 0.37533333333333313
printing an ep nov before normalisation:  43.40768780521823
actor:  1 policy actor:  1  step number:  59 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.859]
 [0.899]
 [0.859]
 [0.835]
 [0.868]
 [0.866]
 [0.842]] [[34.729]
 [36.007]
 [34.729]
 [34.013]
 [39.005]
 [39.607]
 [41.594]] [[0.859]
 [0.899]
 [0.859]
 [0.835]
 [0.868]
 [0.866]
 [0.842]]
maxi score, test score, baseline:  -0.7707533333333335 0.37533333333333313 0.37533333333333313
probs:  [0.24054137868904535, 0.052618377747646036, 0.3664855463389891, 0.09968944756484359, 0.18723588090366336, 0.05342936875581263]
maxi score, test score, baseline:  -0.7707533333333335 0.37533333333333313 0.37533333333333313
probs:  [0.24054137868904535, 0.052618377747646036, 0.3664855463389891, 0.09968944756484359, 0.18723588090366336, 0.05342936875581263]
maxi score, test score, baseline:  -0.7707533333333335 0.37533333333333313 0.37533333333333313
printing an ep nov before normalisation:  54.17821611481513
actor:  0 policy actor:  1  step number:  62 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.7680733333333334 0.37533333333333313 0.37533333333333313
probs:  [0.24054137868904535, 0.052618377747646036, 0.3664855463389891, 0.09968944756484359, 0.18723588090366336, 0.05342936875581263]
Printing some Q and Qe and total Qs values:  [[0.049]
 [0.036]
 [0.049]
 [0.049]
 [0.049]
 [0.049]
 [0.049]] [[45.503]
 [41.143]
 [45.503]
 [45.503]
 [45.503]
 [45.503]
 [45.503]] [[0.972]
 [0.792]
 [0.972]
 [0.972]
 [0.972]
 [0.972]
 [0.972]]
printing an ep nov before normalisation:  27.590229537581283
printing an ep nov before normalisation:  44.79921802589133
maxi score, test score, baseline:  -0.7680733333333334 0.37533333333333313 0.37533333333333313
probs:  [0.24054137868904535, 0.052618377747646036, 0.3664855463389891, 0.09968944756484359, 0.18723588090366336, 0.05342936875581263]
siam score:  -0.8322432
printing an ep nov before normalisation:  29.035976271091975
printing an ep nov before normalisation:  34.78262532119429
printing an ep nov before normalisation:  24.539647588948196
printing an ep nov before normalisation:  21.804442824787706
maxi score, test score, baseline:  -0.7680733333333334 0.37533333333333313 0.37533333333333313
probs:  [0.239729637316702, 0.05264208968290491, 0.36664770533275715, 0.09985623903565854, 0.18766878281280822, 0.053455545819169094]
maxi score, test score, baseline:  -0.7680733333333334 0.37533333333333313 0.37533333333333313
probs:  [0.239729637316702, 0.05264208968290491, 0.36664770533275715, 0.09985623903565854, 0.18766878281280822, 0.053455545819169094]
printing an ep nov before normalisation:  22.036380767822266
maxi score, test score, baseline:  -0.7680733333333334 0.37533333333333313 0.37533333333333313
probs:  [0.239729637316702, 0.05264208968290491, 0.36664770533275715, 0.09985623903565854, 0.18766878281280822, 0.053455545819169094]
printing an ep nov before normalisation:  15.663021642986076
Printing some Q and Qe and total Qs values:  [[-0.212]
 [-0.   ]
 [-0.212]
 [-0.212]
 [-0.212]
 [-0.212]
 [-0.212]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.212]
 [-0.   ]
 [-0.212]
 [-0.212]
 [-0.212]
 [-0.212]
 [-0.212]]
printing an ep nov before normalisation:  31.592839737939418
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.712]
 [0.553]
 [0.532]
 [0.798]
 [0.409]
 [0.467]] [[31.518]
 [30.386]
 [31.956]
 [33.009]
 [32.451]
 [34.336]
 [30.727]] [[0.666]
 [0.712]
 [0.553]
 [0.532]
 [0.798]
 [0.409]
 [0.467]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.7680733333333334 0.37533333333333313 0.37533333333333313
probs:  [0.23945411874247316, 0.052628728657684844, 0.3665510926264995, 0.09995264423700795, 0.18796933977179836, 0.053444075964536285]
maxi score, test score, baseline:  -0.7680733333333334 0.37533333333333313 0.37533333333333313
probs:  [0.23945411874247316, 0.052628728657684844, 0.3665510926264995, 0.09995264423700795, 0.18796933977179836, 0.053444075964536285]
actions average: 
K:  2  action  0 :  tensor([0.5020, 0.0123, 0.0763, 0.0685, 0.1374, 0.0906, 0.1128],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0038, 0.9463, 0.0044, 0.0075, 0.0039, 0.0042, 0.0299],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1344, 0.0025, 0.1474, 0.1435, 0.1777, 0.1951, 0.1993],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0546, 0.0071, 0.0937, 0.4042, 0.0683, 0.2583, 0.1138],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1290, 0.0091, 0.0619, 0.0901, 0.4940, 0.1122, 0.1037],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0848, 0.0854, 0.1124, 0.1162, 0.1300, 0.3040, 0.1671],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0849, 0.0129, 0.1071, 0.0978, 0.1105, 0.1037, 0.4830],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.7680733333333334 0.37533333333333313 0.37533333333333313
probs:  [0.23945411874247316, 0.052628728657684844, 0.3665510926264995, 0.09995264423700795, 0.18796933977179836, 0.053444075964536285]
maxi score, test score, baseline:  -0.7680733333333334 0.37533333333333313 0.37533333333333313
maxi score, test score, baseline:  -0.7680733333333334 0.37533333333333313 0.37533333333333313
probs:  [0.23945411874247316, 0.052628728657684844, 0.3665510926264995, 0.09995264423700795, 0.18796933977179836, 0.053444075964536285]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.7680733333333334 0.37533333333333313 0.37533333333333313
probs:  [0.23945411874247316, 0.052628728657684844, 0.3665510926264995, 0.09995264423700795, 0.18796933977179836, 0.053444075964536285]
printing an ep nov before normalisation:  37.40148067474365
Printing some Q and Qe and total Qs values:  [[-0.058]
 [-0.038]
 [-0.058]
 [-0.058]
 [-0.058]
 [-0.058]
 [-0.058]] [[29.82 ]
 [37.553]
 [29.82 ]
 [29.82 ]
 [29.82 ]
 [29.82 ]
 [29.82 ]] [[0.474]
 [0.762]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]]
from probs:  [0.23945411874247316, 0.052628728657684844, 0.3665510926264995, 0.09995264423700795, 0.18796933977179836, 0.053444075964536285]
printing an ep nov before normalisation:  18.011745992409534
maxi score, test score, baseline:  -0.7680733333333334 0.37533333333333313 0.37533333333333313
probs:  [0.23892753778134568, 0.05266510695561873, 0.3668050126350559, 0.10002181703042894, 0.18809950631624717, 0.05348101928130359]
actor:  1 policy actor:  1  step number:  64 total reward:  0.13999999999999946  reward:  1.0 rdn_beta:  1.0
using another actor
printing an ep nov before normalisation:  36.12674737773057
maxi score, test score, baseline:  -0.7680733333333334 0.37533333333333313 0.37533333333333313
probs:  [0.23917994371542559, 0.052615292038968746, 0.36645395406142467, 0.10004884093276747, 0.18826944102495863, 0.05343252822645494]
printing an ep nov before normalisation:  41.96486949920654
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  47 total reward:  0.3599999999999999  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.763]
 [0.792]
 [0.674]
 [0.673]
 [0.763]
 [0.763]
 [0.675]] [[38.672]
 [35.562]
 [34.089]
 [36.321]
 [38.672]
 [38.672]
 [36.524]] [[0.763]
 [0.792]
 [0.674]
 [0.673]
 [0.763]
 [0.763]
 [0.675]]
printing an ep nov before normalisation:  51.145176579512885
siam score:  -0.83357733
Printing some Q and Qe and total Qs values:  [[0.78 ]
 [0.897]
 [0.78 ]
 [0.767]
 [0.783]
 [0.77 ]
 [0.78 ]] [[28.071]
 [45.5  ]
 [28.071]
 [31.897]
 [31.095]
 [32.681]
 [28.071]] [[0.78 ]
 [0.897]
 [0.78 ]
 [0.767]
 [0.783]
 [0.77 ]
 [0.78 ]]
siam score:  -0.83279437
maxi score, test score, baseline:  -0.7680733333333334 0.37533333333333313 0.37533333333333313
probs:  [0.23943186602522928, 0.052565572570562075, 0.36610356813619693, 0.10007581305575593, 0.18843905012900153, 0.053384130083254266]
printing an ep nov before normalisation:  27.28084266442061
printing an ep nov before normalisation:  32.81125061527358
maxi score, test score, baseline:  -0.7680733333333334 0.37533333333333313 0.37533333333333313
probs:  [0.23943186602522928, 0.052565572570562075, 0.36610356813619693, 0.10007581305575593, 0.18843905012900153, 0.053384130083254266]
actor:  0 policy actor:  1  step number:  56 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  22.82075876060878
maxi score, test score, baseline:  -0.7656333333333334 0.37533333333333313 0.37533333333333313
probs:  [0.23943186602522928, 0.052565572570562075, 0.36610356813619693, 0.10007581305575593, 0.18843905012900153, 0.053384130083254266]
printing an ep nov before normalisation:  52.529302590608474
printing an ep nov before normalisation:  35.54486463767269
Printing some Q and Qe and total Qs values:  [[0.07]
 [0.07]
 [0.07]
 [0.07]
 [0.07]
 [0.07]
 [0.07]] [[36.046]
 [36.046]
 [36.046]
 [36.046]
 [36.046]
 [36.046]
 [36.046]] [[0.926]
 [0.926]
 [0.926]
 [0.926]
 [0.926]
 [0.926]
 [0.926]]
printing an ep nov before normalisation:  7.093205473522062
siam score:  -0.8326354
maxi score, test score, baseline:  -0.7656333333333334 0.37533333333333313 0.37533333333333313
actor:  0 policy actor:  1  step number:  66 total reward:  0.09999999999999931  reward:  1.0 rdn_beta:  1.0
using another actor
siam score:  -0.83254606
printing an ep nov before normalisation:  30.208360945823294
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.43 ]
 [0.43 ]
 [0.506]
 [0.43 ]
 [0.43 ]
 [0.443]] [[25.46 ]
 [25.46 ]
 [25.46 ]
 [24.829]
 [25.46 ]
 [25.46 ]
 [24.728]] [[0.43 ]
 [0.43 ]
 [0.43 ]
 [0.506]
 [0.43 ]
 [0.43 ]
 [0.443]]
maxi score, test score, baseline:  -0.7634333333333334 0.37533333333333313 0.37533333333333313
printing an ep nov before normalisation:  47.67336668523857
maxi score, test score, baseline:  -0.7634333333333333 0.37533333333333313 0.37533333333333313
probs:  [0.2396833060994042, 0.052515948276334795, 0.3657538529279735, 0.10010273354807034, 0.18860833456329607, 0.05333582458492112]
maxi score, test score, baseline:  -0.7634333333333333 0.37533333333333313 0.37533333333333313
probs:  [0.23975850717121977, 0.05253239259066765, 0.3658686308610103, 0.10013411660617831, 0.1883538264909281, 0.05335252627999596]
actor:  0 policy actor:  0  step number:  65 total reward:  0.0933333333333326  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  45.5438232421875
maxi score, test score, baseline:  -0.7612466666666668 0.37533333333333313 0.37533333333333313
probs:  [0.24000966607216193, 0.05248287403294837, 0.36551965902360556, 0.10016104446426463, 0.18852243158368556, 0.05330432482333396]
printing an ep nov before normalisation:  45.512409161023776
maxi score, test score, baseline:  -0.7612466666666668 0.37533333333333313 0.37533333333333313
probs:  [0.24000966607216193, 0.05248287403294837, 0.36551965902360556, 0.10016104446426463, 0.18852243158368556, 0.05330432482333396]
maxi score, test score, baseline:  -0.7612466666666667 0.37533333333333313 0.37533333333333313
probs:  [0.24000966607216193, 0.05248287403294837, 0.36551965902360556, 0.10016104446426463, 0.18852243158368556, 0.05330432482333396]
printing an ep nov before normalisation:  22.867243160430917
maxi score, test score, baseline:  -0.7612466666666667 0.37533333333333313 0.37533333333333313
maxi score, test score, baseline:  -0.7612466666666667 0.37533333333333313 0.37533333333333313
maxi score, test score, baseline:  -0.7612466666666667 0.37533333333333313 0.37533333333333313
probs:  [0.24008476725773692, 0.05249926380672683, 0.3656340552283116, 0.1001923614528024, 0.18826858047520204, 0.05332097177922012]
maxi score, test score, baseline:  -0.7612466666666667 0.37533333333333313 0.37533333333333313
probs:  [0.24033564538817634, 0.05244985054629596, 0.36528582367918794, 0.10021929645150444, 0.1884365100061374, 0.053272873928697874]
printing an ep nov before normalisation:  46.075574667476694
actions average: 
K:  0  action  0 :  tensor([0.5398, 0.0043, 0.0801, 0.0912, 0.0831, 0.0895, 0.1121],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0057, 0.9610, 0.0076, 0.0070, 0.0052, 0.0049, 0.0087],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1272, 0.0016, 0.1234, 0.1052, 0.1434, 0.2921, 0.2071],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1493, 0.0109, 0.1239, 0.2232, 0.1493, 0.1764, 0.1670],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1162, 0.0019, 0.0639, 0.0512, 0.6315, 0.0635, 0.0718],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1184, 0.0058, 0.1234, 0.0701, 0.1129, 0.4008, 0.1686],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1112, 0.0225, 0.1176, 0.0981, 0.1704, 0.1409, 0.3393],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.633]
 [0.578]
 [0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]] [[22.503]
 [28.126]
 [22.503]
 [22.503]
 [22.503]
 [22.503]
 [22.503]] [[1.163]
 [1.377]
 [1.163]
 [1.163]
 [1.163]
 [1.163]
 [1.163]]
line 256 mcts: sample exp_bonus 49.29939186885401
actions average: 
K:  4  action  0 :  tensor([0.5198, 0.0724, 0.0632, 0.0702, 0.1247, 0.0826, 0.0671],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0138, 0.8682, 0.0153, 0.0126, 0.0105, 0.0111, 0.0686],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1242, 0.0141, 0.1251, 0.1641, 0.1759, 0.1957, 0.2009],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1089, 0.0096, 0.0945, 0.3202, 0.1632, 0.1498, 0.1538],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0841, 0.0101, 0.0551, 0.0733, 0.6464, 0.0674, 0.0636],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0724, 0.0892, 0.0878, 0.0953, 0.1078, 0.4607, 0.0868],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1179, 0.1942, 0.1022, 0.0914, 0.1182, 0.1110, 0.2651],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  76 total reward:  0.12666666666666626  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.7612466666666667 0.37533333333333313 0.37533333333333313
probs:  [0.24058604515910803, 0.052400531504133124, 0.3649382561172844, 0.10024618009215183, 0.1886041193390244, 0.05322486778829818]
actor:  1 policy actor:  1  step number:  60 total reward:  0.17999999999999972  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  32.414538706398126
actor:  1 policy actor:  1  step number:  74 total reward:  0.03333333333333277  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.7612466666666667 0.37533333333333313 0.37533333333333313
maxi score, test score, baseline:  -0.7612466666666667 0.37533333333333313 0.37533333333333313
probs:  [0.24070708006057706, 0.05237128657946163, 0.36472746793430993, 0.10043249101726091, 0.1885623377167881, 0.05319933669160233]
printing an ep nov before normalisation:  40.64333438873291
maxi score, test score, baseline:  -0.7612466666666667 0.37533333333333313 0.37533333333333313
using explorer policy with actor:  0
printing an ep nov before normalisation:  32.803904455275266
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
printing an ep nov before normalisation:  31.779080647653334
maxi score, test score, baseline:  -0.7612466666666668 0.37533333333333313 0.37533333333333313
probs:  [0.24070708006057706, 0.05237128657946163, 0.36472746793430993, 0.10043249101726091, 0.1885623377167881, 0.05319933669160233]
maxi score, test score, baseline:  -0.7612466666666668 0.37533333333333313 0.37533333333333313
printing an ep nov before normalisation:  39.993072433441085
printing an ep nov before normalisation:  29.613347095212994
printing an ep nov before normalisation:  0.05812608032400135
actor:  0 policy actor:  0  step number:  79 total reward:  0.026666666666665728  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.7591933333333334 0.37533333333333313 0.37533333333333313
probs:  [0.24018101375406847, 0.052407510825287475, 0.36498029415838645, 0.10050204308747364, 0.18869300302975855, 0.053236135145025426]
maxi score, test score, baseline:  -0.7591933333333334 0.37533333333333313 0.37533333333333313
probs:  [0.24042905956641697, 0.05235854027415937, 0.3646351712219501, 0.10052914750354171, 0.18885960613869332, 0.05318847529523854]
using explorer policy with actor:  1
siam score:  -0.8479316
maxi score, test score, baseline:  -0.7591933333333334 0.37533333333333313 0.37533333333333313
probs:  [0.24042905956641697, 0.05235854027415937, 0.3646351712219501, 0.10052914750354171, 0.18885960613869332, 0.05318847529523854]
actor:  1 policy actor:  1  step number:  68 total reward:  0.13999999999999946  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.7591933333333334 0.37533333333333313 0.37533333333333313
probs:  [0.24042905956641697, 0.05235854027415937, 0.3646351712219501, 0.10052914750354171, 0.18885960613869332, 0.05318847529523854]
maxi score, test score, baseline:  -0.7591933333333334 0.37533333333333313 0.37533333333333313
probs:  [0.24042905956641697, 0.05235854027415937, 0.3646351712219501, 0.10052914750354171, 0.18885960613869332, 0.05318847529523854]
maxi score, test score, baseline:  -0.7591933333333334 0.37533333333333313 0.37533333333333313
probs:  [0.24067663579325782, 0.05230966243115197, 0.36429070165163085, 0.10055620060714918, 0.18902589384457916, 0.053140905672230905]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7591933333333334 0.37533333333333313 0.37533333333333313
probs:  [0.24099910538513472, 0.05227719172924804, 0.36406074848469655, 0.10061464096108717, 0.18893831215607637, 0.05311000128375725]
maxi score, test score, baseline:  -0.7591933333333334 0.37533333333333313 0.37533333333333313
probs:  [0.241245941754793, 0.05222850879791841, 0.3637176508219898, 0.10064164955350174, 0.18910362662440422, 0.05306262244739271]
maxi score, test score, baseline:  -0.7591933333333334 0.37533333333333313 0.37533333333333313
probs:  [0.241245941754793, 0.05222850879791841, 0.3637176508219898, 0.10064164955350174, 0.18910362662440422, 0.05306262244739271]
maxi score, test score, baseline:  -0.7591933333333334 0.37533333333333313 0.37533333333333313
maxi score, test score, baseline:  -0.7591933333333334 0.37533333333333313 0.37533333333333313
probs:  [0.24072027909617585, 0.052264631975161055, 0.3639697624159295, 0.10071134268939905, 0.18923465981935234, 0.053099324003982144]
maxi score, test score, baseline:  -0.7591933333333334 0.37533333333333313 0.37533333333333313
probs:  [0.24072027909617585, 0.052264631975161055, 0.3639697624159295, 0.10071134268939905, 0.18923465981935234, 0.053099324003982144]
maxi score, test score, baseline:  -0.7591933333333334 0.37533333333333313 0.37533333333333313
maxi score, test score, baseline:  -0.7591933333333334 0.37533333333333313 0.37533333333333313
maxi score, test score, baseline:  -0.7591933333333334 0.37533333333333313 0.37533333333333313
probs:  [0.24072027909617585, 0.052264631975161055, 0.3639697624159295, 0.10071134268939905, 0.18923465981935234, 0.053099324003982144]
maxi score, test score, baseline:  -0.7591933333333334 0.37533333333333313 0.37533333333333313
actor:  0 policy actor:  1  step number:  61 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.7568733333333334 0.37533333333333313 0.37533333333333313
probs:  [0.2402727680983491, 0.052316846842331156, 0.3643341814508659, 0.10081208183006497, 0.18911174687887278, 0.05315237489951607]
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.653345695043285
maxi score, test score, baseline:  -0.7568733333333334 0.37533333333333313 0.37533333333333313
probs:  [0.23975319755835686, 0.052352566064058484, 0.3645834737504737, 0.10088099560686996, 0.18924110098750724, 0.05318866603273367]
printing an ep nov before normalisation:  36.47122932819343
siam score:  -0.83498603
maxi score, test score, baseline:  -0.7568733333333334 0.37533333333333313 0.37533333333333313
probs:  [0.23975319755835686, 0.052352566064058484, 0.3645834737504737, 0.10088099560686996, 0.18924110098750724, 0.05318866603273367]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7568733333333334 0.37533333333333313 0.37533333333333313
maxi score, test score, baseline:  -0.7568733333333334 0.37533333333333313 0.37533333333333313
probs:  [0.23975319755835686, 0.052352566064058484, 0.3645834737504737, 0.10088099560686996, 0.18924110098750724, 0.05318866603273367]
printing an ep nov before normalisation:  38.773108137441085
maxi score, test score, baseline:  -0.7568733333333334 0.37533333333333313 0.37533333333333313
probs:  [0.23975319755835686, 0.052352566064058484, 0.3645834737504737, 0.10088099560686996, 0.18924110098750724, 0.05318866603273367]
maxi score, test score, baseline:  -0.7568733333333334 0.37533333333333313 0.37533333333333313
maxi score, test score, baseline:  -0.7568733333333334 0.37533333333333313 0.37533333333333313
probs:  [0.23999782082652363, 0.05230405314738666, 0.36424156101376104, 0.10090839192582325, 0.18940671212450397, 0.053141460962001376]
maxi score, test score, baseline:  -0.7568733333333334 0.37533333333333313 0.37533333333333313
probs:  [0.24024198478691092, 0.05225563131918816, 0.36390029025677934, 0.10093573680509736, 0.18957201230791013, 0.053094344524114094]
rdn beta is 0 so we're just using the maxi policy
actions average: 
K:  2  action  0 :  tensor([0.3902, 0.0357, 0.1029, 0.0962, 0.1211, 0.1028, 0.1511],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0073, 0.9470, 0.0075, 0.0052, 0.0042, 0.0114, 0.0174],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1065, 0.0157, 0.2265, 0.1341, 0.1653, 0.1799, 0.1721],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1425, 0.0335, 0.1473, 0.1974, 0.1442, 0.1407, 0.1944],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1123, 0.0120, 0.0602, 0.0762, 0.5791, 0.0721, 0.0881],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0776, 0.0087, 0.0957, 0.0830, 0.0949, 0.5553, 0.0848],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1610, 0.2050, 0.0629, 0.0715, 0.0738, 0.0542, 0.3716],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.7568733333333334 0.37533333333333313 0.37533333333333313
probs:  [0.24024198478691092, 0.05225563131918816, 0.36390029025677934, 0.10093573680509736, 0.18957201230791013, 0.053094344524114094]
printing an ep nov before normalisation:  34.269980724832
printing an ep nov before normalisation:  33.218428912698265
siam score:  -0.8341961
printing an ep nov before normalisation:  35.48471212387085
maxi score, test score, baseline:  -0.7568733333333334 0.37533333333333313 0.37533333333333313
probs:  [0.24024198478691092, 0.05225563131918816, 0.36390029025677934, 0.10093573680509736, 0.18957201230791013, 0.053094344524114094]
Printing some Q and Qe and total Qs values:  [[0.939]
 [0.971]
 [0.939]
 [0.956]
 [0.939]
 [0.963]
 [0.974]] [[32.863]
 [36.037]
 [32.863]
 [39.943]
 [32.863]
 [38.096]
 [37.926]] [[0.939]
 [0.971]
 [0.939]
 [0.956]
 [0.939]
 [0.963]
 [0.974]]
maxi score, test score, baseline:  -0.7568733333333334 0.37533333333333313 0.37533333333333313
probs:  [0.24024198478691092, 0.05225563131918816, 0.36390029025677934, 0.10093573680509736, 0.18957201230791013, 0.053094344524114094]
maxi score, test score, baseline:  -0.7568733333333334 0.37533333333333313 0.37533333333333313
probs:  [0.24024198478691092, 0.05225563131918816, 0.36390029025677934, 0.10093573680509736, 0.18957201230791013, 0.053094344524114094]
actor:  0 policy actor:  1  step number:  61 total reward:  0.37333333333333285  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  21.825539605112134
printing an ep nov before normalisation:  21.945509528745887
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.117]
 [-0.117]
 [-0.117]
 [-0.117]
 [-0.117]
 [-0.117]
 [-0.117]] [[41.896]
 [41.896]
 [41.896]
 [41.896]
 [41.896]
 [41.896]
 [41.896]] [[1.166]
 [1.166]
 [1.166]
 [1.166]
 [1.166]
 [1.166]
 [1.166]]
maxi score, test score, baseline:  -0.7541266666666667 0.37533333333333313 0.37533333333333313
probs:  [0.24024198478691092, 0.05225563131918816, 0.36390029025677934, 0.10093573680509736, 0.18957201230791013, 0.053094344524114094]
printing an ep nov before normalisation:  33.28451439023793
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]] [[24.664]
 [24.664]
 [24.664]
 [24.664]
 [24.664]
 [24.664]
 [24.664]] [[1.766]
 [1.766]
 [1.766]
 [1.766]
 [1.766]
 [1.766]
 [1.766]]
actor:  1 policy actor:  1  step number:  83 total reward:  0.07999999999999874  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 34.62678923603275
Printing some Q and Qe and total Qs values:  [[-0.116]
 [-0.13 ]
 [-0.116]
 [-0.117]
 [-0.117]
 [-0.116]
 [-0.116]] [[58.88 ]
 [58.047]
 [58.88 ]
 [57.512]
 [57.876]
 [58.804]
 [60.681]] [[0.675]
 [0.638]
 [0.675]
 [0.636]
 [0.646]
 [0.673]
 [0.726]]
maxi score, test score, baseline:  -0.7541266666666667 0.37533333333333313 0.37533333333333313
probs:  [0.2403170019499014, 0.05227191591851256, 0.3640139419757622, 0.1009672305264776, 0.18931901846679797, 0.05311089116254824]
using explorer policy with actor:  1
from probs:  [0.2403170019499014, 0.05227191591851256, 0.3640139419757622, 0.1009672305264776, 0.18931901846679797, 0.05311089116254824]
printing an ep nov before normalisation:  0.0
Printing some Q and Qe and total Qs values:  [[-0.143]
 [-0.197]
 [-0.152]
 [-0.143]
 [-0.143]
 [-0.143]
 [-0.143]] [[27.691]
 [34.59 ]
 [29.262]
 [27.735]
 [28.257]
 [28.118]
 [28.668]] [[0.392]
 [0.597]
 [0.442]
 [0.393]
 [0.413]
 [0.408]
 [0.428]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7541266666666667 0.37533333333333313 0.37533333333333313
probs:  [0.24063581640313808, 0.05223982611931426, 0.3637866581248365, 0.10102600932337093, 0.18923132308410207, 0.05308036694523798]
printing an ep nov before normalisation:  52.455182827425915
maxi score, test score, baseline:  -0.7541266666666667 0.37533333333333313 0.37533333333333313
probs:  [0.2408794503582595, 0.05219160590959192, 0.363446804879929, 0.10105336636310845, 0.18939532362747066, 0.05303344886164046]
actions average: 
K:  2  action  0 :  tensor([0.4383, 0.0107, 0.0855, 0.0983, 0.1113, 0.1518, 0.1042],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0090, 0.9406, 0.0083, 0.0078, 0.0090, 0.0068, 0.0186],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1370, 0.0148, 0.3134, 0.1110, 0.1230, 0.1733, 0.1275],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1314, 0.0164, 0.1279, 0.1410, 0.2557, 0.1741, 0.1535],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1198, 0.0073, 0.1018, 0.1087, 0.3958, 0.1493, 0.1172],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0573, 0.0395, 0.0667, 0.0455, 0.0554, 0.6612, 0.0744],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1142, 0.0077, 0.1040, 0.1228, 0.1542, 0.1588, 0.3382],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.7541266666666667 0.37533333333333313 0.37533333333333313
probs:  [0.24112262861777062, 0.05214347589147641, 0.3631075873002419, 0.1010806722339402, 0.18955901742243203, 0.052986618534138885]
maxi score, test score, baseline:  -0.7541266666666667 0.37533333333333313 0.37533333333333313
probs:  [0.24136535245898397, 0.05209543581216111, 0.362769003604009, 0.1011079270792922, 0.18972240532880028, 0.05293987571675332]
actions average: 
K:  1  action  0 :  tensor([0.5023, 0.0792, 0.0772, 0.0705, 0.0865, 0.0879, 0.0963],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0020,     0.9800,     0.0010,     0.0022,     0.0020,     0.0009,
            0.0118], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1805, 0.0154, 0.2925, 0.0907, 0.1397, 0.1503, 0.1308],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0928, 0.1126, 0.0945, 0.2726, 0.1394, 0.1233, 0.1648],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1280, 0.0517, 0.0791, 0.1077, 0.4076, 0.1026, 0.1235],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0314, 0.0033, 0.0730, 0.0315, 0.0481, 0.7799, 0.0328],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1425, 0.0161, 0.0765, 0.1005, 0.0989, 0.0905, 0.4750],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.536]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]] [[45.413]
 [54.941]
 [45.413]
 [45.413]
 [45.413]
 [45.413]
 [45.413]] [[0.924]
 [1.23 ]
 [0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.924]]
printing an ep nov before normalisation:  43.647113893810236
actor:  1 policy actor:  1  step number:  52 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.156474113464355
maxi score, test score, baseline:  -0.7541266666666667 0.37533333333333313 0.37533333333333313
probs:  [0.24184944197194375, 0.05199962446341997, 0.3620937307680729, 0.10116228426458679, 0.19004826689897794, 0.05284665163299866]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  64 total reward:  0.27333333333333265  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.7541266666666667 0.37533333333333313 0.37533333333333313
probs:  [0.2418086526424997, 0.051939942753981894, 0.3616706163084043, 0.10128628593989165, 0.19050436773678778, 0.05279013461843479]
Printing some Q and Qe and total Qs values:  [[0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]] [[38.075]
 [38.075]
 [38.075]
 [38.075]
 [38.075]
 [38.075]
 [38.075]] [[0.911]
 [0.911]
 [0.911]
 [0.911]
 [0.911]
 [0.911]
 [0.911]]
maxi score, test score, baseline:  -0.7541266666666667 0.37533333333333313 0.37533333333333313
probs:  [0.2418086526424997, 0.051939942753981894, 0.3616706163084043, 0.10128628593989165, 0.19050436773678778, 0.05279013461843479]
maxi score, test score, baseline:  -0.7541266666666667 0.37533333333333313 0.37533333333333313
probs:  [0.2418086526424997, 0.051939942753981894, 0.3616706163084043, 0.10128628593989165, 0.19050436773678778, 0.05279013461843479]
maxi score, test score, baseline:  -0.7541266666666667 0.37533333333333313 0.37533333333333313
probs:  [0.2418086526424997, 0.051939942753981894, 0.3616706163084043, 0.10128628593989165, 0.19050436773678778, 0.05279013461843479]
maxi score, test score, baseline:  -0.7541266666666667 0.37533333333333313 0.37533333333333313
probs:  [0.2418086526424997, 0.051939942753981894, 0.3616706163084043, 0.10128628593989165, 0.19050436773678778, 0.05279013461843479]
printing an ep nov before normalisation:  1.141156076300831e-05
actions average: 
K:  4  action  0 :  tensor([0.4416, 0.0499, 0.0818, 0.0666, 0.1000, 0.1458, 0.1143],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0109, 0.8958, 0.0134, 0.0148, 0.0131, 0.0176, 0.0343],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0610, 0.1182, 0.4652, 0.0637, 0.1027, 0.0921, 0.0970],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1390, 0.0037, 0.1607, 0.1545, 0.1688, 0.1900, 0.1833],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0542, 0.0179, 0.0287, 0.0564, 0.7283, 0.0657, 0.0488],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0651, 0.0207, 0.0896, 0.0747, 0.0927, 0.5722, 0.0851],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1059, 0.0542, 0.0990, 0.1474, 0.2303, 0.1406, 0.2226],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.7541266666666667 0.37533333333333313 0.37533333333333313
probs:  [0.2418086526424997, 0.051939942753981894, 0.3616706163084043, 0.10128628593989165, 0.19050436773678778, 0.05279013461843479]
Printing some Q and Qe and total Qs values:  [[0.34]
 [0.34]
 [0.34]
 [0.34]
 [0.34]
 [0.34]
 [0.34]] [[28.225]
 [28.225]
 [28.225]
 [28.225]
 [28.225]
 [28.225]
 [28.225]] [[9.739]
 [9.739]
 [9.739]
 [9.739]
 [9.739]
 [9.739]
 [9.739]]
printing an ep nov before normalisation:  27.733349800109863
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7541266666666667 0.37533333333333313 0.37533333333333313
probs:  [0.2418086526424997, 0.051939942753981894, 0.3616706163084043, 0.10128628593989165, 0.19050436773678778, 0.05279013461843479]
actor:  1 policy actor:  1  step number:  46 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.7541266666666667 0.37533333333333313 0.37533333333333313
probs:  [0.24160378696067475, 0.05194424577939212, 0.36169733856904507, 0.10141482074057231, 0.1905432299078552, 0.05279657804246066]
siam score:  -0.8285263
Printing some Q and Qe and total Qs values:  [[-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]] [[52.353]
 [52.353]
 [52.353]
 [52.353]
 [52.353]
 [52.353]
 [52.353]] [[1.66]
 [1.66]
 [1.66]
 [1.66]
 [1.66]
 [1.66]
 [1.66]]
maxi score, test score, baseline:  -0.7541266666666667 0.37533333333333313 0.37533333333333313
probs:  [0.24160378696067475, 0.05194424577939212, 0.36169733856904507, 0.10141482074057231, 0.1905432299078552, 0.05279657804246066]
maxi score, test score, baseline:  -0.7541266666666667 0.37533333333333313 0.37533333333333313
probs:  [0.24160378696067475, 0.05194424577939212, 0.36169733856904507, 0.10141482074057231, 0.1905432299078552, 0.05279657804246066]
Printing some Q and Qe and total Qs values:  [[-0.002]
 [ 0.013]
 [ 0.001]
 [ 0.003]
 [ 0.001]
 [ 0.   ]
 [ 0.001]] [[20.599]
 [31.57 ]
 [20.555]
 [21.084]
 [17.706]
 [20.943]
 [17.602]] [[-0.002]
 [ 0.013]
 [ 0.001]
 [ 0.003]
 [ 0.001]
 [ 0.   ]
 [ 0.001]]
actor:  1 policy actor:  1  step number:  63 total reward:  0.33333333333333326  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  48.787440814619984
maxi score, test score, baseline:  -0.7541266666666667 0.37533333333333313 0.37533333333333313
probs:  [0.24175475673273686, 0.05197663853868383, 0.36192339201216317, 0.10147814299047861, 0.19003756603766123, 0.052829503688276394]
maxi score, test score, baseline:  -0.7541266666666667 0.37533333333333313 0.37533333333333313
probs:  [0.24175475673273686, 0.05197663853868383, 0.36192339201216317, 0.10147814299047861, 0.19003756603766123, 0.052829503688276394]
printing an ep nov before normalisation:  38.73280535596986
printing an ep nov before normalisation:  53.61721946804107
maxi score, test score, baseline:  -0.7541266666666667 0.37533333333333313 0.37533333333333313
probs:  [0.24175475673273686, 0.05197663853868383, 0.36192339201216317, 0.10147814299047861, 0.19003756603766123, 0.052829503688276394]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.7541266666666667 0.37533333333333313 0.37533333333333313
probs:  [0.24175475673273686, 0.05197663853868383, 0.36192339201216317, 0.10147814299047861, 0.19003756603766123, 0.052829503688276394]
Printing some Q and Qe and total Qs values:  [[0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]] [[34.607]
 [34.607]
 [34.607]
 [34.607]
 [34.607]
 [34.607]
 [34.607]] [[0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]]
maxi score, test score, baseline:  -0.7541266666666667 0.37533333333333313 0.37533333333333313
probs:  [0.24175475673273686, 0.05197663853868383, 0.36192339201216317, 0.10147814299047861, 0.19003756603766123, 0.052829503688276394]
printing an ep nov before normalisation:  35.09015472443893
maxi score, test score, baseline:  -0.7541266666666667 0.37533333333333313 0.37533333333333313
probs:  [0.24175475673273686, 0.05197663853868383, 0.36192339201216317, 0.10147814299047861, 0.19003756603766123, 0.052829503688276394]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  57 total reward:  0.346666666666666  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  81 total reward:  0.013333333333332753  reward:  1.0 rdn_beta:  1.0
Starting evaluation
maxi score, test score, baseline:  -0.7521000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.24199410334863763, 0.0519292038282829, 0.361589062325181, 0.10150551198893358, 0.19019876073235387, 0.05278335777661093]
siam score:  -0.82896405
printing an ep nov before normalisation:  28.643278918905395
printing an ep nov before normalisation:  41.58522366372554
Printing some Q and Qe and total Qs values:  [[0.809]
 [0.834]
 [0.809]
 [0.719]
 [0.809]
 [0.809]
 [0.809]] [[36.217]
 [53.097]
 [36.217]
 [39.612]
 [36.217]
 [36.217]
 [36.217]] [[0.809]
 [0.834]
 [0.809]
 [0.719]
 [0.809]
 [0.809]
 [0.809]]
printing an ep nov before normalisation:  43.74305783267522
printing an ep nov before normalisation:  37.18701124191284
maxi score, test score, baseline:  -0.7521000000000001 0.37533333333333313 0.37533333333333313
printing an ep nov before normalisation:  30.57618782742589
maxi score, test score, baseline:  -0.7521000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.24199410334863763, 0.0519292038282829, 0.361589062325181, 0.10150551198893358, 0.19019876073235387, 0.05278335777661093]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]] [[29.24]
 [29.24]
 [29.24]
 [29.24]
 [29.24]
 [29.24]
 [29.24]] [[0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]]
printing an ep nov before normalisation:  33.95391242664669
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  30.825011730194092
maxi score, test score, baseline:  -0.7521000000000001 0.37533333333333313 0.37533333333333313
probs:  [0.24223300728308142, 0.05188185685033905, 0.36125535099477096, 0.10153283036729495, 0.19035965729079735, 0.0527372972137161]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  34.00105610243551
printing an ep nov before normalisation:  31.16040690749601
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.381]
 [0.319]
 [0.292]
 [0.294]
 [0.305]
 [0.308]] [[31.251]
 [33.319]
 [31.203]
 [32.984]
 [32.95 ]
 [32.578]
 [31.88 ]] [[1.279]
 [1.501]
 [1.286]
 [1.387]
 [1.387]
 [1.372]
 [1.324]]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  34.988095485666776
actor:  0 policy actor:  0  step number:  32 total reward:  0.66  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  36.39445114525287
printing an ep nov before normalisation:  27.83565044403076
actor:  0 policy actor:  0  step number:  33 total reward:  0.6533333333333333  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  23.895737312486187
actor:  0 policy actor:  0  step number:  35 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  35 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  0.0011158938593780476
printing an ep nov before normalisation:  35.192361043799224
actor:  0 policy actor:  0  step number:  36 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  28.477833395977093
actor:  0 policy actor:  0  step number:  37 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  37 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.72906 0.37533333333333313 0.37533333333333313
probs:  [0.24223300728308142, 0.05188185685033905, 0.36125535099477096, 0.10153283036729495, 0.19035965729079735, 0.0527372972137161]
actor:  0 policy actor:  0  step number:  38 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.7257666666666668 0.37533333333333313 0.37533333333333313
probs:  [0.24223300728308142, 0.05188185685033905, 0.36125535099477096, 0.10153283036729495, 0.19035965729079735, 0.0527372972137161]
actor:  0 policy actor:  0  step number:  39 total reward:  0.6133333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  41 total reward:  0.6266666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  41 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.7160333333333334 0.37533333333333313 0.37533333333333313
probs:  [0.24223300728308142, 0.05188185685033905, 0.36125535099477096, 0.10153283036729495, 0.19035965729079735, 0.0527372972137161]
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.577]
 [0.485]
 [0.485]
 [0.485]
 [0.43 ]
 [0.435]] [[41.619]
 [35.552]
 [41.619]
 [41.619]
 [41.619]
 [41.947]
 [43.146]] [[1.999]
 [1.692]
 [1.999]
 [1.999]
 [1.999]
 [1.966]
 [2.049]]
actor:  0 policy actor:  0  step number:  46 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  46 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  46 total reward:  0.5933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  47 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  47 total reward:  0.5600000000000002  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  43.224020831449835
actor:  0 policy actor:  0  step number:  48 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  2.435712076476193e-05
actor:  0 policy actor:  0  step number:  49 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  2
siam score:  -0.83140975
printing an ep nov before normalisation:  25.789779977719743
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  23.142075610168117
printing an ep nov before normalisation:  29.969557776334142
actor:  1 policy actor:  1  step number:  57 total reward:  0.37333333333333285  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.106]
 [0.24 ]
 [0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.106]] [[18.674]
 [22.377]
 [18.674]
 [18.674]
 [18.674]
 [18.674]
 [18.674]] [[0.871]
 [1.311]
 [0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]]
actor:  0 policy actor:  0  step number:  60 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.69438 0.37533333333333313 0.37533333333333313
probs:  [0.24230842912141137, 0.05189797820660183, 0.36136785210655337, 0.101564419592588, 0.19010763590578567, 0.05275368506705972]
Printing some Q and Qe and total Qs values:  [[-0.157]
 [-0.106]
 [-0.091]
 [-0.106]
 [-0.106]
 [-0.106]
 [-0.074]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.157]
 [-0.106]
 [-0.091]
 [-0.106]
 [-0.106]
 [-0.106]
 [-0.074]]
maxi score, test score, baseline:  -0.69438 0.37533333333333313 0.37533333333333313
maxi score, test score, baseline:  -0.69438 0.37533333333333313 0.37533333333333313
from probs:  [0.24230842912141137, 0.05189797820660183, 0.36136785210655337, 0.101564419592588, 0.19010763590578567, 0.05275368506705972]
printing an ep nov before normalisation:  30.016825997990246
maxi score, test score, baseline:  -0.69438 0.37533333333333313 0.37533333333333313
probs:  [0.24230842912141137, 0.05189797820660183, 0.36136785210655337, 0.101564419592588, 0.19010763590578567, 0.05275368506705972]
printing an ep nov before normalisation:  10.740313529968262
Printing some Q and Qe and total Qs values:  [[0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]] [[14.508]
 [14.508]
 [14.508]
 [14.508]
 [14.508]
 [14.508]
 [14.508]] [[0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]]
maxi score, test score, baseline:  -0.69438 0.37533333333333313 0.37533333333333313
probs:  [0.2412728864176454, 0.05196878773036784, 0.3618619885810915, 0.10170316834068843, 0.1903675038093667, 0.052825665120840255]
printing an ep nov before normalisation:  43.606991223549315
maxi score, test score, baseline:  -0.69438 0.37533333333333313 0.37533333333333313
printing an ep nov before normalisation:  40.39255697406399
maxi score, test score, baseline:  -0.69438 0.37533333333333313 0.37533333333333313
probs:  [0.2412728864176454, 0.05196878773036784, 0.3618619885810915, 0.10170316834068843, 0.1903675038093667, 0.052825665120840255]
printing an ep nov before normalisation:  16.615056647920127
printing an ep nov before normalisation:  34.79962522543224
maxi score, test score, baseline:  -0.69438 0.37533333333333313 0.37533333333333313
probs:  [0.24134779783909727, 0.05198489074535937, 0.36197436170032965, 0.10173472162691169, 0.19011619375858316, 0.052842034329718866]
maxi score, test score, baseline:  -0.69438 0.37533333333333313 0.37533333333333313
printing an ep nov before normalisation:  37.75126652834392
printing an ep nov before normalisation:  52.57184597702105
maxi score, test score, baseline:  -0.69438 0.37533333333333313 0.37533333333333313
probs:  [0.24134779783909727, 0.05198489074535937, 0.36197436170032965, 0.10173472162691169, 0.19011619375858316, 0.052842034329718866]
printing an ep nov before normalisation:  13.86298837488664
maxi score, test score, baseline:  -0.69438 0.37533333333333313 0.37533333333333313
probs:  [0.24134779783909727, 0.05198489074535937, 0.36197436170032965, 0.10173472162691169, 0.19011619375858316, 0.052842034329718866]
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.27676773071289
printing an ep nov before normalisation:  45.41676032303943
actor:  1 policy actor:  1  step number:  81 total reward:  0.11999999999999855  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.182]
 [0.343]
 [0.182]
 [0.182]
 [0.249]
 [0.205]
 [0.201]] [[31.922]
 [35.284]
 [31.922]
 [31.922]
 [37.217]
 [37.661]
 [35.988]] [[0.943]
 [1.185]
 [0.943]
 [0.943]
 [1.137]
 [1.103]
 [1.06 ]]
maxi score, test score, baseline:  -0.6943800000000001 0.5026666666666667 0.5026666666666667
maxi score, test score, baseline:  -0.6943800000000001 0.5026666666666667 0.5026666666666667
probs:  [0.15583064600020885, 0.15583064600020885, 0.15583064600020885, 0.15583064600020885, 0.22084676999895586, 0.15583064600020885]
printing an ep nov before normalisation:  37.03278369996096
maxi score, test score, baseline:  -0.6943800000000001 0.5026666666666667 0.5026666666666667
probs:  [0.15583064600020885, 0.15583064600020885, 0.15583064600020885, 0.15583064600020885, 0.22084676999895586, 0.15583064600020885]
Printing some Q and Qe and total Qs values:  [[-0.151]
 [-0.202]
 [-0.099]
 [-0.129]
 [-0.096]
 [-0.099]
 [-0.099]] [[37.248]
 [39.476]
 [39.503]
 [46.936]
 [47.77 ]
 [39.503]
 [39.503]] [[ 0.028]
 [-0.001]
 [ 0.102]
 [ 0.145]
 [ 0.186]
 [ 0.102]
 [ 0.102]]
maxi score, test score, baseline:  -0.6943800000000001 0.5026666666666667 0.5026666666666667
probs:  [0.15583064600020885, 0.15583064600020885, 0.15583064600020885, 0.15583064600020885, 0.22084676999895586, 0.15583064600020885]
maxi score, test score, baseline:  -0.6943800000000001 0.5026666666666667 0.5026666666666667
Printing some Q and Qe and total Qs values:  [[-0.012]
 [-0.006]
 [-0.012]
 [-0.012]
 [-0.012]
 [-0.019]
 [-0.018]] [[41.834]
 [47.337]
 [41.834]
 [41.834]
 [41.834]
 [45.704]
 [48.006]] [[0.452]
 [0.577]
 [0.452]
 [0.452]
 [0.452]
 [0.529]
 [0.58 ]]
Printing some Q and Qe and total Qs values:  [[-0.011]
 [ 0.009]
 [-0.011]
 [-0.008]
 [-0.008]
 [-0.006]
 [-0.012]] [[40.155]
 [36.165]
 [40.136]
 [41.655]
 [39.499]
 [36.774]
 [38.921]] [[0.272]
 [0.241]
 [0.272]
 [0.293]
 [0.267]
 [0.234]
 [0.255]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6943800000000001 0.5026666666666667 0.5026666666666667
probs:  [0.11807913614158819, 0.11807913614158819, 0.3759438658548486, 0.11807913614158819, 0.15173958957879852, 0.11807913614158819]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6943800000000001 0.5026666666666667 0.5026666666666667
probs:  [0.11807913614158819, 0.11807913614158819, 0.3759438658548486, 0.11807913614158819, 0.15173958957879852, 0.11807913614158819]
Printing some Q and Qe and total Qs values:  [[0.02 ]
 [0.098]
 [0.02 ]
 [0.02 ]
 [0.025]
 [0.03 ]
 [0.02 ]] [[26.022]
 [36.944]
 [26.022]
 [26.022]
 [28.191]
 [28.771]
 [26.022]] [[0.154]
 [0.35 ]
 [0.154]
 [0.154]
 [0.182]
 [0.193]
 [0.154]]
printing an ep nov before normalisation:  28.181039914301657
maxi score, test score, baseline:  -0.6943800000000001 0.5026666666666667 0.5026666666666667
probs:  [0.11807913614158819, 0.11807913614158819, 0.3759438658548486, 0.11807913614158819, 0.15173958957879852, 0.11807913614158819]
actor:  1 policy actor:  1  step number:  57 total reward:  0.48  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.481]
 [0.401]
 [0.409]
 [0.404]
 [0.405]
 [0.364]] [[40.123]
 [38.175]
 [39.666]
 [40.588]
 [40.355]
 [40.573]
 [40.153]] [[0.727]
 [0.765]
 [0.708]
 [0.731]
 [0.723]
 [0.727]
 [0.679]]
Printing some Q and Qe and total Qs values:  [[-0.111]
 [-0.129]
 [-0.111]
 [-0.111]
 [-0.111]
 [-0.111]
 [-0.111]] [[41.366]
 [39.53 ]
 [41.366]
 [41.366]
 [40.047]
 [41.366]
 [39.895]] [[1.183]
 [1.06 ]
 [1.183]
 [1.183]
 [1.108]
 [1.183]
 [1.099]]
maxi score, test score, baseline:  -0.6943800000000001 0.5026666666666667 0.5026666666666667
probs:  [0.11808802560097119, 0.11808802560097119, 0.37597219030822326, 0.11808802560097119, 0.15167570728789193, 0.11808802560097119]
printing an ep nov before normalisation:  35.809268951416016
Printing some Q and Qe and total Qs values:  [[0.34]
 [0.34]
 [0.34]
 [0.34]
 [0.34]
 [0.34]
 [0.34]] [[47.187]
 [47.187]
 [47.187]
 [47.187]
 [47.187]
 [47.187]
 [47.187]] [[1.34]
 [1.34]
 [1.34]
 [1.34]
 [1.34]
 [1.34]
 [1.34]]
printing an ep nov before normalisation:  36.18861197376163
actor:  1 policy actor:  1  step number:  81 total reward:  0.19999999999999907  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6943800000000001 0.5026666666666667 0.5026666666666667
probs:  [0.11808802560097119, 0.11808802560097119, 0.37597219030822326, 0.11808802560097119, 0.15167570728789193, 0.11808802560097119]
printing an ep nov before normalisation:  48.61880634713344
Printing some Q and Qe and total Qs values:  [[0.426]
 [0.445]
 [0.426]
 [0.427]
 [0.45 ]
 [0.421]
 [0.426]] [[53.051]
 [48.613]
 [53.051]
 [51.03 ]
 [52.816]
 [50.32 ]
 [53.051]] [[1.315]
 [1.215]
 [1.315]
 [1.262]
 [1.333]
 [1.236]
 [1.315]]
maxi score, test score, baseline:  -0.6943800000000001 0.5026666666666667 0.5026666666666667
probs:  [0.11808802560097119, 0.11808802560097119, 0.37597219030822326, 0.11808802560097119, 0.15167570728789193, 0.11808802560097119]
printing an ep nov before normalisation:  39.880474252547735
maxi score, test score, baseline:  -0.6943800000000001 0.5026666666666667 0.5026666666666667
probs:  [0.11817437127220624, 0.11817437127220624, 0.37545693556091836, 0.11817437127220624, 0.15184557935025647, 0.11817437127220624]
maxi score, test score, baseline:  -0.6943800000000001 0.5026666666666667 0.5026666666666667
probs:  [0.11821739321137743, 0.11821739321137743, 0.3752002086390191, 0.11821739321137743, 0.15193021851547125, 0.11821739321137743]
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.7113877773958
printing an ep nov before normalisation:  50.06130184768794
maxi score, test score, baseline:  -0.6943800000000001 0.5026666666666667 0.5026666666666667
probs:  [0.11821739321137743, 0.11821739321137743, 0.3752002086390191, 0.11821739321137743, 0.15193021851547125, 0.11821739321137743]
printing an ep nov before normalisation:  35.937166633724594
maxi score, test score, baseline:  -0.6943800000000001 0.5026666666666667 0.5026666666666667
probs:  [0.11821739321137743, 0.11821739321137743, 0.3752002086390191, 0.11821739321137743, 0.15193021851547125, 0.11821739321137743]
actor:  1 policy actor:  1  step number:  63 total reward:  0.2933333333333331  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  35.13226884638089
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
actor:  1 policy actor:  1  step number:  64 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6943800000000001 0.5026666666666667 0.5026666666666667
probs:  [0.11843100792553388, 0.11843100792553388, 0.37392549510685497, 0.11843100792553388, 0.15235047319100958, 0.11843100792553388]
maxi score, test score, baseline:  -0.6943800000000001 0.5026666666666667 0.5026666666666667
probs:  [0.11843100792553388, 0.11843100792553388, 0.37392549510685497, 0.11843100792553388, 0.15235047319100958, 0.11843100792553388]
line 256 mcts: sample exp_bonus 41.70590658844263
maxi score, test score, baseline:  -0.6943800000000001 0.5026666666666667 0.5026666666666667
probs:  [0.11843100792553388, 0.11843100792553388, 0.37392549510685497, 0.11843100792553388, 0.15235047319100958, 0.11843100792553388]
maxi score, test score, baseline:  -0.6943800000000001 0.5026666666666667 0.5026666666666667
probs:  [0.11843100792553388, 0.11843100792553388, 0.37392549510685497, 0.11843100792553388, 0.15235047319100958, 0.11843100792553388]
maxi score, test score, baseline:  -0.6943800000000001 0.5026666666666667 0.5026666666666667
probs:  [0.11847343429353777, 0.11847343429353777, 0.3736723221659158, 0.11847343429353777, 0.15243394065993335, 0.11847343429353777]
printing an ep nov before normalisation:  21.156549517828495
maxi score, test score, baseline:  -0.6943800000000001 0.5026666666666667 0.5026666666666667
probs:  [0.11851576260296674, 0.11851576260296674, 0.37341973437467973, 0.11851576260296674, 0.1525172152134532, 0.11851576260296674]
maxi score, test score, baseline:  -0.6943800000000001 0.5026666666666667 0.5026666666666667
probs:  [0.11851576260296674, 0.11851576260296674, 0.37341973437467973, 0.11851576260296674, 0.1525172152134532, 0.11851576260296674]
printing an ep nov before normalisation:  28.586427813086225
Printing some Q and Qe and total Qs values:  [[0.79 ]
 [0.79 ]
 [0.79 ]
 [0.79 ]
 [0.79 ]
 [0.811]
 [0.79 ]] [[48.323]
 [48.323]
 [48.323]
 [48.323]
 [48.323]
 [53.595]
 [48.323]] [[0.79 ]
 [0.79 ]
 [0.79 ]
 [0.79 ]
 [0.79 ]
 [0.811]
 [0.79 ]]
printing an ep nov before normalisation:  47.801558770661934
maxi score, test score, baseline:  -0.69438 0.5026666666666667 0.5026666666666667
probs:  [0.11851576260296674, 0.11851576260296674, 0.37341973437467973, 0.11851576260296674, 0.1525172152134532, 0.11851576260296674]
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]] [[30.187]
 [30.187]
 [30.187]
 [30.187]
 [30.187]
 [30.187]
 [30.187]] [[2.573]
 [2.573]
 [2.573]
 [2.573]
 [2.573]
 [2.573]
 [2.573]]
maxi score, test score, baseline:  -0.69438 0.5026666666666667 0.5026666666666667
probs:  [0.11851576260296674, 0.11851576260296674, 0.37341973437467973, 0.11851576260296674, 0.1525172152134532, 0.11851576260296674]
maxi score, test score, baseline:  -0.69438 0.5026666666666667 0.5026666666666667
maxi score, test score, baseline:  -0.69438 0.5026666666666667 0.5026666666666667
probs:  [0.11851576260296674, 0.11851576260296674, 0.37341973437467973, 0.11851576260296674, 0.1525172152134532, 0.11851576260296674]
Printing some Q and Qe and total Qs values:  [[0.321]
 [0.42 ]
 [0.286]
 [0.267]
 [0.29 ]
 [0.287]
 [0.279]] [[28.669]
 [40.121]
 [33.529]
 [35.488]
 [35.925]
 [34.719]
 [33.046]] [[0.647]
 [1.148]
 [0.783]
 [0.833]
 [0.871]
 [0.826]
 [0.759]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.69438 0.5026666666666667 0.5026666666666667
probs:  [0.11851576260296674, 0.11851576260296674, 0.37341973437467973, 0.11851576260296674, 0.1525172152134532, 0.11851576260296674]
maxi score, test score, baseline:  -0.69438 0.5026666666666667 0.5026666666666667
actor:  1 policy actor:  1  step number:  65 total reward:  0.30666666666666587  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.69438 0.5026666666666667 0.5026666666666667
probs:  [0.11851576260296674, 0.11851576260296674, 0.37341973437467973, 0.11851576260296674, 0.1525172152134532, 0.11851576260296674]
printing an ep nov before normalisation:  47.4122305933472
maxi score, test score, baseline:  -0.69438 0.5026666666666667 0.5026666666666667
probs:  [0.11851576260296674, 0.11851576260296674, 0.37341973437467973, 0.11851576260296674, 0.1525172152134532, 0.11851576260296674]
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]] [[43.969]
 [43.969]
 [43.969]
 [43.969]
 [43.969]
 [43.969]
 [43.969]] [[2.038]
 [2.038]
 [2.038]
 [2.038]
 [2.038]
 [2.038]
 [2.038]]
maxi score, test score, baseline:  -0.69438 0.5026666666666667 0.5026666666666667
probs:  [0.11851576260296674, 0.11851576260296674, 0.37341973437467973, 0.11851576260296674, 0.1525172152134532, 0.11851576260296674]
printing an ep nov before normalisation:  65.83348782061928
actor:  1 policy actor:  1  step number:  63 total reward:  0.14666666666666617  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  40.8695428161223
Printing some Q and Qe and total Qs values:  [[0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.106]] [[36.579]
 [36.579]
 [36.579]
 [36.579]
 [36.579]
 [36.579]
 [36.579]] [[0.773]
 [0.773]
 [0.773]
 [0.773]
 [0.773]
 [0.773]
 [0.773]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.69438 0.5026666666666667 0.5026666666666667
actor:  1 policy actor:  1  step number:  61 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.69438 0.5026666666666667 0.5026666666666667
probs:  [0.11855799319339712, 0.11855799319339712, 0.37316772970682666, 0.11855799319339712, 0.15260029751958484, 0.11855799319339712]
maxi score, test score, baseline:  -0.69438 0.5026666666666667 0.5026666666666667
probs:  [0.11855799319339712, 0.11855799319339712, 0.37316772970682666, 0.11855799319339712, 0.15260029751958484, 0.11855799319339712]
maxi score, test score, baseline:  -0.69438 0.5026666666666667 0.5026666666666667
probs:  [0.11855799319339712, 0.11855799319339712, 0.37316772970682666, 0.11855799319339712, 0.15260029751958484, 0.11855799319339712]
printing an ep nov before normalisation:  26.934155729578762
Printing some Q and Qe and total Qs values:  [[0.86 ]
 [0.908]
 [0.848]
 [0.86 ]
 [0.869]
 [0.855]
 [0.854]] [[32.987]
 [37.265]
 [33.237]
 [35.061]
 [35.004]
 [35.746]
 [34.614]] [[0.86 ]
 [0.908]
 [0.848]
 [0.86 ]
 [0.869]
 [0.855]
 [0.854]]
maxi score, test score, baseline:  -0.69438 0.5026666666666667 0.5026666666666667
probs:  [0.11855799319339712, 0.11855799319339712, 0.37316772970682666, 0.11855799319339712, 0.15260029751958484, 0.11855799319339712]
printing an ep nov before normalisation:  29.515331884136494
maxi score, test score, baseline:  -0.69438 0.5026666666666667 0.5026666666666667
probs:  [0.11855799319339712, 0.11855799319339712, 0.37316772970682666, 0.11855799319339712, 0.15260029751958484, 0.11855799319339712]
actor:  0 policy actor:  0  step number:  89 total reward:  0.21333333333333315  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.393]
 [0.501]
 [0.501]
 [0.376]
 [0.501]
 [0.501]] [[46.648]
 [51.966]
 [46.648]
 [46.648]
 [55.398]
 [46.648]
 [46.648]] [[1.304]
 [1.316]
 [1.304]
 [1.304]
 [1.376]
 [1.304]
 [1.304]]
printing an ep nov before normalisation:  36.16566713395684
maxi score, test score, baseline:  -0.6919533333333333 0.5026666666666667 0.5026666666666667
maxi score, test score, baseline:  -0.6919533333333333 0.5026666666666667 0.5026666666666667
probs:  [0.11855799319339712, 0.11855799319339712, 0.37316772970682666, 0.11855799319339712, 0.15260029751958484, 0.11855799319339712]
printing an ep nov before normalisation:  55.95368293282836
maxi score, test score, baseline:  -0.6919533333333333 0.5026666666666667 0.5026666666666667
probs:  [0.11855799319339712, 0.11855799319339712, 0.37316772970682666, 0.11855799319339712, 0.15260029751958484, 0.11855799319339712]
maxi score, test score, baseline:  -0.6919533333333333 0.5026666666666667 0.5026666666666667
probs:  [0.11855799319339712, 0.11855799319339712, 0.37316772970682666, 0.11855799319339712, 0.15260029751958484, 0.11855799319339712]
actor:  0 policy actor:  0  step number:  62 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6894600000000001 0.5026666666666667 0.5026666666666667
probs:  [0.11855799319339712, 0.11855799319339712, 0.37316772970682666, 0.11855799319339712, 0.15260029751958484, 0.11855799319339712]
actor:  1 policy actor:  1  step number:  65 total reward:  0.2133333333333326  reward:  1.0 rdn_beta:  1.0
from probs:  [0.11855799319339712, 0.11855799319339712, 0.37316772970682666, 0.11855799319339712, 0.15260029751958484, 0.11855799319339712]
printing an ep nov before normalisation:  27.59283383687337
printing an ep nov before normalisation:  54.87417394277361
printing an ep nov before normalisation:  41.63642989024375
printing an ep nov before normalisation:  44.39489473666937
maxi score, test score, baseline:  -0.6894600000000001 0.5026666666666667 0.5026666666666667
probs:  [0.11860012640282079, 0.11860012640282079, 0.3729163061454092, 0.11860012640282079, 0.15268318824330762, 0.11860012640282079]
maxi score, test score, baseline:  -0.6894600000000001 0.5026666666666667 0.5026666666666667
probs:  [0.11860012640282079, 0.11860012640282079, 0.3729163061454092, 0.11860012640282079, 0.15268318824330762, 0.11860012640282079]
maxi score, test score, baseline:  -0.6894600000000001 0.5026666666666667 0.5026666666666667
probs:  [0.11860012640282079, 0.11860012640282079, 0.3729163061454092, 0.11860012640282079, 0.15268318824330762, 0.11860012640282079]
maxi score, test score, baseline:  -0.6894600000000001 0.5026666666666667 0.5026666666666667
actor:  1 policy actor:  1  step number:  61 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  2.0
from probs:  [0.11860012640282079, 0.11860012640282079, 0.3729163061454092, 0.11860012640282079, 0.15268318824330762, 0.11860012640282079]
printing an ep nov before normalisation:  31.480040550231934
maxi score, test score, baseline:  -0.6894600000000001 0.5026666666666667 0.5026666666666667
probs:  [0.11860012640282079, 0.11860012640282079, 0.3729163061454092, 0.11860012640282079, 0.15268318824330762, 0.11860012640282079]
printing an ep nov before normalisation:  43.18194389343262
maxi score, test score, baseline:  -0.68946 0.5026666666666667 0.5026666666666667
probs:  [0.11860012640282079, 0.11860012640282079, 0.3729163061454092, 0.11860012640282079, 0.15268318824330762, 0.11860012640282079]
maxi score, test score, baseline:  -0.68946 0.5026666666666667 0.5026666666666667
actor:  0 policy actor:  1  step number:  53 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  2.0
using another actor
maxi score, test score, baseline:  -0.68666 0.5026666666666667 0.5026666666666667
probs:  [0.11864216256768496, 0.11864216256768496, 0.3726654616827202, 0.11864216256768496, 0.15276588804653993, 0.11864216256768496]
printing an ep nov before normalisation:  37.59755065744338
printing an ep nov before normalisation:  28.90937089920044
printing an ep nov before normalisation:  47.757370608869884
maxi score, test score, baseline:  -0.68666 0.5026666666666667 0.5026666666666667
probs:  [0.11864216256768496, 0.11864216256768496, 0.3726654616827202, 0.11864216256768496, 0.15276588804653993, 0.11864216256768496]
maxi score, test score, baseline:  -0.68666 0.5026666666666667 0.5026666666666667
probs:  [0.11864216256768496, 0.11864216256768496, 0.3726654616827202, 0.11864216256768496, 0.15276588804653993, 0.11864216256768496]
printing an ep nov before normalisation:  31.639208756024985
maxi score, test score, baseline:  -0.68666 0.5026666666666667 0.5026666666666667
probs:  [0.11864216256768496, 0.11864216256768496, 0.3726654616827202, 0.11864216256768496, 0.15276588804653993, 0.11864216256768496]
printing an ep nov before normalisation:  42.59512527099263
maxi score, test score, baseline:  -0.68666 0.5026666666666667 0.5026666666666667
probs:  [0.11864216256768496, 0.11864216256768496, 0.3726654616827202, 0.11864216256768496, 0.15276588804653993, 0.11864216256768496]
maxi score, test score, baseline:  -0.68666 0.5026666666666667 0.5026666666666667
maxi score, test score, baseline:  -0.68666 0.5026666666666667 0.5026666666666667
probs:  [0.11864216256768496, 0.11864216256768496, 0.3726654616827202, 0.11864216256768496, 0.15276588804653993, 0.11864216256768496]
actor:  0 policy actor:  1  step number:  67 total reward:  0.10666666666666602  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6844466666666666 0.5026666666666667 0.5026666666666667
probs:  [0.11864216256768496, 0.11864216256768496, 0.3726654616827202, 0.11864216256768496, 0.15276588804653993, 0.11864216256768496]
maxi score, test score, baseline:  -0.6844466666666666 0.5026666666666667 0.5026666666666667
actor:  0 policy actor:  0  step number:  52 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6817666666666666 0.5026666666666667 0.5026666666666667
probs:  [0.11876769213619165, 0.11876769213619165, 0.37191638294865276, 0.11876769213619165, 0.15301284850658065, 0.11876769213619165]
line 256 mcts: sample exp_bonus 37.53661143860251
Printing some Q and Qe and total Qs values:  [[0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]] [[35.669]
 [35.669]
 [35.669]
 [35.669]
 [35.669]
 [35.669]
 [35.669]] [[0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]]
printing an ep nov before normalisation:  30.000927534974174
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
printing an ep nov before normalisation:  42.738106790797126
printing an ep nov before normalisation:  7.732449203746228
maxi score, test score, baseline:  -0.6817666666666666 0.5026666666666667 0.5026666666666667
probs:  [0.11880934345642359, 0.11880934345642359, 0.37166783498817263, 0.11880934345642359, 0.153094791186133, 0.11880934345642359]
actor:  0 policy actor:  1  step number:  54 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6790066666666666 0.5026666666666667 0.5026666666666667
probs:  [0.11880934345642359, 0.11880934345642359, 0.37166783498817263, 0.11880934345642359, 0.153094791186133, 0.11880934345642359]
actions average: 
K:  3  action  0 :  tensor([0.5645, 0.0104, 0.0514, 0.0654, 0.1342, 0.0685, 0.1055],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0123, 0.9270, 0.0093, 0.0099, 0.0072, 0.0063, 0.0280],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0712, 0.0183, 0.3729, 0.1025, 0.1779, 0.1393, 0.1179],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0613, 0.0376, 0.0755, 0.3974, 0.0877, 0.1148, 0.2257],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1111, 0.0466, 0.0750, 0.1006, 0.4392, 0.1020, 0.1255],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0673, 0.0711, 0.0950, 0.0854, 0.1236, 0.4654, 0.0922],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1262, 0.0705, 0.0837, 0.1571, 0.1702, 0.1465, 0.2457],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.6790066666666666 0.5026666666666667 0.5026666666666667
printing an ep nov before normalisation:  33.741488456726074
maxi score, test score, baseline:  -0.6790066666666666 0.5026666666666667 0.5026666666666667
probs:  [0.11880934345642359, 0.11880934345642359, 0.37166783498817263, 0.11880934345642359, 0.153094791186133, 0.11880934345642359]
maxi score, test score, baseline:  -0.6790066666666666 0.5026666666666667 0.5026666666666667
probs:  [0.11880934345642359, 0.11880934345642359, 0.37166783498817263, 0.11880934345642359, 0.153094791186133, 0.11880934345642359]
printing an ep nov before normalisation:  34.051483256592206
printing an ep nov before normalisation:  44.50941908641613
maxi score, test score, baseline:  -0.6790066666666666 0.5026666666666667 0.5026666666666667
maxi score, test score, baseline:  -0.6790066666666666 0.5026666666666667 0.5026666666666667
probs:  [0.11881843370374431, 0.11881843370374431, 0.3716962934646572, 0.11881843370374431, 0.15302997172036556, 0.11881843370374431]
actor:  1 policy actor:  1  step number:  73 total reward:  0.07999999999999952  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6790066666666666 0.5026666666666667 0.5026666666666667
probs:  [0.11881843370374431, 0.11881843370374431, 0.3716962934646572, 0.11881843370374431, 0.15302997172036556, 0.11881843370374431]
printing an ep nov before normalisation:  56.1177004191829
actor:  1 policy actor:  1  step number:  56 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6790066666666666 0.5026666666666667 0.5026666666666667
probs:  [0.11886000348436249, 0.11886000348436249, 0.3714483290549446, 0.11886000348436249, 0.15311165700760543, 0.11886000348436249]
printing an ep nov before normalisation:  38.608294461824634
printing an ep nov before normalisation:  47.57791246994154
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]] [[32.92]
 [32.92]
 [32.92]
 [32.92]
 [32.92]
 [32.92]
 [32.92]] [[0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]]
maxi score, test score, baseline:  -0.6790066666666666 0.5026666666666667 0.5026666666666667
probs:  [0.11890147818265169, 0.11890147818265169, 0.37120093181285213, 0.11890147818265169, 0.1531931554565411, 0.11890147818265169]
siam score:  -0.83540744
maxi score, test score, baseline:  -0.6790066666666666 0.5026666666666667 0.5026666666666667
probs:  [0.11890147818265169, 0.11890147818265169, 0.37120093181285213, 0.11890147818265169, 0.1531931554565411, 0.11890147818265169]
maxi score, test score, baseline:  -0.6790066666666666 0.5026666666666667 0.5026666666666667
probs:  [0.11894285812446126, 0.11894285812446126, 0.3709540997946935, 0.11894285812446126, 0.1532744677074614, 0.11894285812446126]
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.447]
 [0.447]
 [0.447]
 [0.472]
 [0.356]
 [0.447]] [[22.568]
 [27.971]
 [27.971]
 [27.971]
 [25.917]
 [24.93 ]
 [27.971]] [[0.887]
 [1.068]
 [1.068]
 [1.068]
 [1.01 ]
 [0.855]
 [1.068]]
maxi score, test score, baseline:  -0.6790066666666666 0.5026666666666667 0.5026666666666667
probs:  [0.11894285812446126, 0.11894285812446126, 0.3709540997946935, 0.11894285812446126, 0.1532744677074614, 0.11894285812446126]
printing an ep nov before normalisation:  33.826724246210475
maxi score, test score, baseline:  -0.6790066666666666 0.5026666666666667 0.5026666666666667
actor:  1 policy actor:  1  step number:  57 total reward:  0.3333333333333328  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6790066666666666 0.5026666666666667 0.5026666666666667
probs:  [0.11895195055525727, 0.11895195055525727, 0.37098247849670607, 0.11895195055525727, 0.153209719282265, 0.11895195055525727]
printing an ep nov before normalisation:  48.064826496840155
siam score:  -0.838085
maxi score, test score, baseline:  -0.6790066666666666 0.5026666666666667 0.5026666666666667
probs:  [0.11895195055525727, 0.11895195055525727, 0.37098247849670607, 0.11895195055525727, 0.153209719282265, 0.11895195055525727]
maxi score, test score, baseline:  -0.6790066666666666 0.5026666666666667 0.5026666666666667
probs:  [0.11895195055525727, 0.11895195055525727, 0.37098247849670607, 0.11895195055525727, 0.153209719282265, 0.11895195055525727]
Printing some Q and Qe and total Qs values:  [[-0.018]
 [ 0.188]
 [-0.018]
 [-0.012]
 [-0.018]
 [-0.012]
 [ 0.001]] [[44.038]
 [52.557]
 [44.038]
 [46.435]
 [44.038]
 [47.693]
 [45.521]] [[0.77 ]
 [1.273]
 [0.77 ]
 [0.859]
 [0.77 ]
 [0.903]
 [0.841]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 34.430597654821604
maxi score, test score, baseline:  -0.6790066666666668 0.5026666666666667 0.5026666666666667
probs:  [0.11899324978057133, 0.11899324978057133, 0.37073622383444116, 0.11899324978057133, 0.1532907770432735, 0.11899324978057133]
printing an ep nov before normalisation:  36.88370767172163
printing an ep nov before normalisation:  44.692246446367186
printing an ep nov before normalisation:  64.35452368845094
maxi score, test score, baseline:  -0.6790066666666668 0.5026666666666667 0.5026666666666667
maxi score, test score, baseline:  -0.6790066666666668 0.5026666666666667 0.5026666666666667
probs:  [0.11903445487269651, 0.11903445487269651, 0.3704905304596179, 0.11903445487269651, 0.153371650049596, 0.11903445487269651]
Printing some Q and Qe and total Qs values:  [[-0.158]
 [-0.152]
 [-0.158]
 [-0.158]
 [-0.158]
 [-0.158]
 [-0.158]] [[25.04 ]
 [31.743]
 [26.423]
 [26.31 ]
 [26.241]
 [26.552]
 [26.117]] [[0.683]
 [0.914]
 [0.729]
 [0.725]
 [0.723]
 [0.734]
 [0.719]]
maxi score, test score, baseline:  -0.6790066666666668 0.5026666666666667 0.5026666666666667
probs:  [0.11903445487269651, 0.11903445487269651, 0.3704905304596179, 0.11903445487269651, 0.153371650049596, 0.11903445487269651]
maxi score, test score, baseline:  -0.6790066666666668 0.5026666666666667 0.5026666666666667
probs:  [0.11903445487269651, 0.11903445487269651, 0.3704905304596179, 0.11903445487269651, 0.153371650049596, 0.11903445487269651]
actor:  1 policy actor:  1  step number:  50 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  28.593115324531706
printing an ep nov before normalisation:  35.25013942834808
maxi score, test score, baseline:  -0.6790066666666668 0.5026666666666667 0.5026666666666667
probs:  [0.11908466045919751, 0.11908466045919751, 0.37027369515356495, 0.11908466045919751, 0.15338766300964507, 0.11908466045919751]
maxi score, test score, baseline:  -0.6790066666666668 0.5026666666666667 0.5026666666666667
probs:  [0.11908466045919751, 0.11908466045919751, 0.37027369515356495, 0.11908466045919751, 0.15338766300964507, 0.11908466045919751]
actions average: 
K:  0  action  0 :  tensor([0.6046, 0.0046, 0.0587, 0.0681, 0.0949, 0.0729, 0.0961],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0008,     0.9792,     0.0016,     0.0041,     0.0004,     0.0005,
            0.0132], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1099, 0.0136, 0.2455, 0.1285, 0.1551, 0.1673, 0.1802],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1067, 0.0036, 0.1117, 0.3774, 0.1395, 0.1244, 0.1367],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0787, 0.0179, 0.0472, 0.0699, 0.6417, 0.0751, 0.0695],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1260, 0.0055, 0.1397, 0.1075, 0.1306, 0.3696, 0.1212],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1709, 0.0478, 0.1139, 0.1220, 0.1342, 0.1236, 0.2876],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  31.31887435913086
printing an ep nov before normalisation:  0.0004787672986594771
siam score:  -0.8335356
maxi score, test score, baseline:  -0.6790066666666668 0.5026666666666667 0.5026666666666667
probs:  [0.11912569183487413, 0.11912569183487413, 0.37002913239849244, 0.11912569183487413, 0.15346810026201108, 0.11912569183487413]
maxi score, test score, baseline:  -0.6790066666666668 0.5026666666666667 0.5026666666666667
probs:  [0.11912569183487413, 0.11912569183487413, 0.37002913239849244, 0.11912569183487413, 0.15346810026201108, 0.11912569183487413]
printing an ep nov before normalisation:  29.642689987696613
printing an ep nov before normalisation:  36.996262073516846
printing an ep nov before normalisation:  30.878041574802097
siam score:  -0.83129656
maxi score, test score, baseline:  -0.6790066666666668 0.5026666666666667 0.5026666666666667
probs:  [0.11912569183487413, 0.11912569183487413, 0.37002913239849244, 0.11912569183487413, 0.15346810026201108, 0.11912569183487413]
maxi score, test score, baseline:  -0.6790066666666668 0.5026666666666667 0.5026666666666667
probs:  [0.11912569183487413, 0.11912569183487413, 0.37002913239849244, 0.11912569183487413, 0.15346810026201108, 0.11912569183487413]
using explorer policy with actor:  1
printing an ep nov before normalisation:  29.69098386355803
actor:  1 policy actor:  1  step number:  71 total reward:  0.013333333333332753  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  40.16104120920378
printing an ep nov before normalisation:  43.05111947401259
actor:  1 policy actor:  1  step number:  74 total reward:  0.32666666666666555  reward:  1.0 rdn_beta:  0.667
using another actor
from probs:  [0.11912569183487413, 0.11912569183487413, 0.37002913239849244, 0.11912569183487413, 0.15346810026201108, 0.11912569183487413]
printing an ep nov before normalisation:  35.32658040810877
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.759]
 [0.273]
 [0.681]
 [0.594]
 [0.496]
 [0.628]
 [0.781]] [[47.749]
 [43.648]
 [41.706]
 [45.533]
 [48.344]
 [43.44 ]
 [50.072]] [[0.759]
 [0.273]
 [0.681]
 [0.594]
 [0.496]
 [0.628]
 [0.781]]
printing an ep nov before normalisation:  23.230607173469547
maxi score, test score, baseline:  -0.6790066666666668 0.5026666666666667 0.5026666666666667
actor:  1 policy actor:  1  step number:  61 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6790066666666668 0.5026666666666667 0.5026666666666667
probs:  [0.11917571240903095, 0.11917571240903095, 0.3698133300276366, 0.11917571240903095, 0.15348382033623958, 0.11917571240903095]
printing an ep nov before normalisation:  42.28946354069814
maxi score, test score, baseline:  -0.6790066666666668 0.5026666666666667 0.5026666666666667
probs:  [0.11917571240903095, 0.11917571240903095, 0.3698133300276366, 0.11917571240903095, 0.15348382033623958, 0.11917571240903095]
maxi score, test score, baseline:  -0.6790066666666668 0.5026666666666667 0.5026666666666667
probs:  [0.11917571240903095, 0.11917571240903095, 0.3698133300276366, 0.11917571240903095, 0.15348382033623958, 0.11917571240903095]
printing an ep nov before normalisation:  40.68495506242689
actor:  0 policy actor:  1  step number:  58 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  1.667
from probs:  [0.11917571240903095, 0.11917571240903095, 0.3698133300276366, 0.11917571240903095, 0.15348382033623958, 0.11917571240903095]
printing an ep nov before normalisation:  29.891911633596624
Printing some Q and Qe and total Qs values:  [[-0.109]
 [-0.04 ]
 [-0.109]
 [-0.109]
 [-0.108]
 [-0.105]
 [-0.111]] [[34.811]
 [34.733]
 [32.862]
 [33.519]
 [33.798]
 [35.596]
 [33.42 ]] [[0.683]
 [0.749]
 [0.589]
 [0.62 ]
 [0.635]
 [0.725]
 [0.614]]
maxi score, test score, baseline:  -0.6764066666666667 0.5026666666666667 0.5026666666666667
maxi score, test score, baseline:  -0.6764066666666667 0.5026666666666667 0.5026666666666667
probs:  [0.11921657119147519, 0.11921657119147519, 0.3695698899453021, 0.11921657119147519, 0.15356382528879703, 0.11921657119147519]
maxi score, test score, baseline:  -0.6764066666666667 0.5026666666666667 0.5026666666666667
probs:  [0.11921657119147519, 0.11921657119147519, 0.3695698899453021, 0.11921657119147519, 0.15356382528879703, 0.11921657119147519]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[-0.113]
 [-0.115]
 [-0.111]
 [-0.111]
 [-0.113]
 [-0.11 ]
 [-0.11 ]] [[32.855]
 [35.34 ]
 [31.051]
 [31.682]
 [32.728]
 [31.495]
 [33.365]] [[0.713]
 [0.828]
 [0.631]
 [0.66 ]
 [0.708]
 [0.652]
 [0.741]]
siam score:  -0.84308124
maxi score, test score, baseline:  -0.6764066666666667 0.5026666666666667 0.5026666666666667
probs:  [0.11921657119147519, 0.11921657119147519, 0.3695698899453021, 0.11921657119147519, 0.15356382528879703, 0.11921657119147519]
printing an ep nov before normalisation:  40.131425857543945
maxi score, test score, baseline:  -0.6764066666666667 0.5026666666666667 0.5026666666666667
probs:  [0.11921657119147519, 0.11921657119147519, 0.3695698899453021, 0.11921657119147519, 0.15356382528879703, 0.11921657119147519]
printing an ep nov before normalisation:  0.004635917470636741
printing an ep nov before normalisation:  43.917952271271055
printing an ep nov before normalisation:  29.91996877361291
Printing some Q and Qe and total Qs values:  [[0.889]
 [0.877]
 [0.877]
 [0.877]
 [0.912]
 [0.877]
 [0.877]] [[30.943]
 [40.365]
 [40.365]
 [40.365]
 [43.484]
 [40.365]
 [40.365]] [[0.889]
 [0.877]
 [0.877]
 [0.877]
 [0.912]
 [0.877]
 [0.877]]
maxi score, test score, baseline:  -0.6764066666666667 0.5026666666666667 0.5026666666666667
probs:  [0.119298011308479, 0.119298011308479, 0.3690846628386076, 0.119298011308479, 0.15372329192747639, 0.119298011308479]
maxi score, test score, baseline:  -0.6764066666666667 0.5026666666666667 0.5026666666666667
probs:  [0.11933859327032574, 0.11933859327032574, 0.3688428720768164, 0.11933859327032574, 0.15380275484188072, 0.11933859327032574]
line 256 mcts: sample exp_bonus 62.72500988719396
maxi score, test score, baseline:  -0.6764066666666667 0.5026666666666667 0.5026666666666667
probs:  [0.11933859327032574, 0.11933859327032574, 0.3688428720768164, 0.11933859327032574, 0.15380275484188072, 0.11933859327032574]
printing an ep nov before normalisation:  29.411210789177222
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6764066666666667 0.5026666666666667 0.5026666666666667
probs:  [0.11941948255830263, 0.11941948255830263, 0.36836092685606225, 0.11941948255830263, 0.15396114291072738, 0.11941948255830263]
maxi score, test score, baseline:  -0.6764066666666667 0.5026666666666667 0.5026666666666667
probs:  [0.11941948255830263, 0.11941948255830263, 0.36836092685606225, 0.11941948255830263, 0.15396114291072738, 0.11941948255830263]
printing an ep nov before normalisation:  54.53880843997
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.92371261958599
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6764066666666667 0.5026666666666667 0.5026666666666667
maxi score, test score, baseline:  -0.6764066666666667 0.5026666666666667 0.5026666666666667
probs:  [0.1194286063570025, 0.1194286063570025, 0.36838909133644254, 0.1194286063570025, 0.15389648323554747, 0.1194286063570025]
siam score:  -0.84904456
Printing some Q and Qe and total Qs values:  [[0.176]
 [0.161]
 [0.176]
 [0.176]
 [0.176]
 [0.173]
 [0.176]] [[28.442]
 [28.589]
 [28.442]
 [28.442]
 [28.442]
 [26.372]
 [28.442]] [[1.496]
 [1.494]
 [1.496]
 [1.496]
 [1.496]
 [1.318]
 [1.496]]
maxi score, test score, baseline:  -0.6764066666666667 0.5026666666666667 0.5026666666666667
probs:  [0.1194286063570025, 0.1194286063570025, 0.36838909133644254, 0.1194286063570025, 0.15389648323554747, 0.1194286063570025]
printing an ep nov before normalisation:  41.401478879480855
printing an ep nov before normalisation:  35.6348010812554
maxi score, test score, baseline:  -0.6764066666666667 0.5026666666666667 0.5026666666666667
probs:  [0.1194286063570025, 0.1194286063570025, 0.36838909133644254, 0.1194286063570025, 0.15389648323554747, 0.1194286063570025]
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.352]
 [0.281]
 [0.316]
 [0.294]
 [0.316]
 [0.325]] [[38.59 ]
 [34.913]
 [25.571]
 [36.326]
 [35.716]
 [35.474]
 [35.211]] [[1.285]
 [1.177]
 [0.762]
 [1.192]
 [1.148]
 [1.161]
 [1.16 ]]
printing an ep nov before normalisation:  45.18015335206426
actions average: 
K:  3  action  0 :  tensor([0.4991, 0.0757, 0.0683, 0.0641, 0.1066, 0.0769, 0.1093],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0100, 0.9384, 0.0086, 0.0085, 0.0105, 0.0088, 0.0151],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1075, 0.0050, 0.5270, 0.0506, 0.0708, 0.1343, 0.1048],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1640, 0.1668, 0.1150, 0.1087, 0.1559, 0.1155, 0.1739],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0720, 0.0447, 0.0512, 0.0580, 0.6055, 0.0789, 0.0897],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0802, 0.0546, 0.1039, 0.0789, 0.0955, 0.4741, 0.1129],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0730, 0.2471, 0.0780, 0.0766, 0.0860, 0.1093, 0.3301],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  58 total reward:  0.23333333333333295  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  37.29729345220669
maxi score, test score, baseline:  -0.6764066666666667 0.5026666666666667 0.5026666666666667
probs:  [0.11943769126060105, 0.11943769126060105, 0.3684171357505808, 0.11943769126060105, 0.15383209920701507, 0.11943769126060105]
actor:  1 policy actor:  1  step number:  63 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  52.51125054654119
actor:  1 policy actor:  1  step number:  80 total reward:  0.11333333333333273  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.6764066666666667 0.5026666666666667 0.5026666666666667
maxi score, test score, baseline:  -0.6764066666666667 0.5026666666666667 0.5026666666666667
probs:  [0.11943769126060105, 0.11943769126060105, 0.3684171357505808, 0.11943769126060105, 0.15383209920701507, 0.11943769126060105]
maxi score, test score, baseline:  -0.6764066666666667 0.5026666666666667 0.5026666666666667
probs:  [0.11943769126060105, 0.11943769126060105, 0.3684171357505808, 0.11943769126060105, 0.15383209920701507, 0.11943769126060105]
printing an ep nov before normalisation:  55.00047910163312
maxi score, test score, baseline:  -0.6764066666666667 0.5026666666666667 0.5026666666666667
probs:  [0.11943769126060105, 0.11943769126060105, 0.3684171357505808, 0.11943769126060105, 0.15383209920701507, 0.11943769126060105]
maxi score, test score, baseline:  -0.6764066666666667 0.5026666666666667 0.5026666666666667
probs:  [0.11943769126060105, 0.11943769126060105, 0.3684171357505808, 0.11943769126060105, 0.15383209920701507, 0.11943769126060105]
printing an ep nov before normalisation:  29.342267513275146
siam score:  -0.84540504
maxi score, test score, baseline:  -0.6764066666666667 0.5026666666666667 0.5026666666666667
probs:  [0.11943769126060105, 0.11943769126060105, 0.3684171357505808, 0.11943769126060105, 0.15383209920701507, 0.11943769126060105]
maxi score, test score, baseline:  -0.6764066666666667 0.5026666666666667 0.5026666666666667
maxi score, test score, baseline:  -0.6764066666666667 0.5026666666666667 0.5026666666666667
probs:  [0.11943769126060105, 0.11943769126060105, 0.3684171357505808, 0.11943769126060105, 0.15383209920701507, 0.11943769126060105]
printing an ep nov before normalisation:  26.396684646606445
printing an ep nov before normalisation:  32.05155372619629
maxi score, test score, baseline:  -0.6764066666666667 0.5026666666666667 0.5026666666666667
probs:  [0.11943769126060105, 0.11943769126060105, 0.3684171357505808, 0.11943769126060105, 0.15383209920701507, 0.11943769126060105]
maxi score, test score, baseline:  -0.6764066666666667 0.5026666666666667 0.5026666666666667
probs:  [0.11943769126060105, 0.11943769126060105, 0.3684171357505808, 0.11943769126060105, 0.15383209920701507, 0.11943769126060105]
maxi score, test score, baseline:  -0.6764066666666667 0.5026666666666667 0.5026666666666667
probs:  [0.11943769126060105, 0.11943769126060105, 0.3684171357505808, 0.11943769126060105, 0.15383209920701507, 0.11943769126060105]
printing an ep nov before normalisation:  50.36182892534542
actor:  1 policy actor:  1  step number:  70 total reward:  0.33999999999999986  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  63 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  70 total reward:  0.08666666666666556  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6739800000000001 0.5026666666666667 0.5026666666666667
Printing some Q and Qe and total Qs values:  [[-0.071]
 [-0.071]
 [-0.071]
 [-0.071]
 [-0.071]
 [-0.071]
 [-0.071]] [[59.38]
 [59.38]
 [59.38]
 [59.38]
 [59.38]
 [59.38]
 [59.38]] [[0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.163]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6739800000000001 0.5026666666666667 0.5026666666666667
maxi score, test score, baseline:  -0.6739800000000001 0.5026666666666667 0.5026666666666667
probs:  [0.11959848573535684, 0.11959848573535684, 0.3674598428438117, 0.11959848573535684, 0.1541462142147609, 0.11959848573535684]
printing an ep nov before normalisation:  36.905851347927175
Printing some Q and Qe and total Qs values:  [[-0.07 ]
 [-0.068]
 [-0.069]
 [-0.069]
 [-0.057]
 [-0.069]
 [-0.064]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.07 ]
 [-0.068]
 [-0.069]
 [-0.069]
 [-0.057]
 [-0.069]
 [-0.064]]
maxi score, test score, baseline:  -0.6739800000000001 0.5026666666666667 0.5026666666666667
probs:  [0.11963845895809962, 0.11963845895809962, 0.36722186151607994, 0.11963845895809962, 0.1542243026515216, 0.11963845895809962]
printing an ep nov before normalisation:  46.54640964480961
Printing some Q and Qe and total Qs values:  [[0.201]
 [0.201]
 [0.201]
 [0.201]
 [0.201]
 [0.201]
 [0.201]] [[63.743]
 [63.743]
 [63.743]
 [63.743]
 [63.743]
 [63.743]
 [63.743]] [[0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]]
printing an ep nov before normalisation:  46.068932995429336
maxi score, test score, baseline:  -0.6739800000000001 0.5026666666666667 0.5026666666666667
probs:  [0.11964757087710495, 0.11964757087710495, 0.36724985093415996, 0.11964757087710495, 0.15415986555742034, 0.11964757087710495]
maxi score, test score, baseline:  -0.6739800000000001 0.5026666666666667 0.5026666666666667
probs:  [0.11964757087710495, 0.11964757087710495, 0.36724985093415996, 0.11964757087710495, 0.15415986555742034, 0.11964757087710495]
maxi score, test score, baseline:  -0.6739800000000001 0.5026666666666667 0.5026666666666667
probs:  [0.11964757087710495, 0.11964757087710495, 0.36724985093415996, 0.11964757087710495, 0.15415986555742034, 0.11964757087710495]
actor:  1 policy actor:  1  step number:  66 total reward:  0.15333333333333254  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6739800000000001 0.5026666666666667 0.5026666666666667
probs:  [0.11968746761179808, 0.11968746761179808, 0.3670124154127701, 0.11968746761179808, 0.15423771414003765, 0.11968746761179808]
printing an ep nov before normalisation:  26.622137366243468
maxi score, test score, baseline:  -0.6739800000000001 0.5026666666666667 0.5026666666666667
probs:  [0.11969655386934558, 0.11969655386934558, 0.36704029864611587, 0.11969655386934558, 0.15417348587650181, 0.11969655386934558]
maxi score, test score, baseline:  -0.6739800000000001 0.5026666666666667 0.5026666666666667
probs:  [0.11969655386934558, 0.11969655386934558, 0.36704029864611587, 0.11969655386934558, 0.15417348587650181, 0.11969655386934558]
printing an ep nov before normalisation:  54.31356130235459
actions average: 
K:  3  action  0 :  tensor([0.4155, 0.0981, 0.0894, 0.0810, 0.0942, 0.1215, 0.1002],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0048, 0.9467, 0.0074, 0.0098, 0.0070, 0.0054, 0.0190],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0846, 0.0419, 0.2960, 0.1180, 0.1655, 0.1721, 0.1219],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0733, 0.0142, 0.1431, 0.3935, 0.1380, 0.1191, 0.1188],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0779, 0.0192, 0.0533, 0.0552, 0.6590, 0.0724, 0.0629],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0567, 0.0058, 0.0795, 0.1119, 0.1140, 0.5025, 0.1296],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0789, 0.1144, 0.1041, 0.1370, 0.1395, 0.1202, 0.3059],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.6739800000000001 0.5026666666666667 0.5026666666666667
probs:  [0.11969655386934558, 0.11969655386934558, 0.36704029864611587, 0.11969655386934558, 0.15417348587650181, 0.11969655386934558]
Printing some Q and Qe and total Qs values:  [[0.893]
 [0.899]
 [0.893]
 [0.893]
 [0.893]
 [0.893]
 [0.893]] [[39.501]
 [39.842]
 [39.501]
 [39.501]
 [39.501]
 [39.501]
 [39.501]] [[0.893]
 [0.899]
 [0.893]
 [0.893]
 [0.893]
 [0.893]
 [0.893]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.054]
 [0.053]
 [0.054]
 [0.054]
 [0.053]
 [0.051]
 [0.054]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.054]
 [0.053]
 [0.054]
 [0.054]
 [0.053]
 [0.051]
 [0.054]]
maxi score, test score, baseline:  -0.6739800000000001 0.5026666666666667 0.5026666666666667
probs:  [0.11970560164704172, 0.11970560164704172, 0.36706806379536333, 0.11970560164704172, 0.15410952961646987, 0.11970560164704172]
printing an ep nov before normalisation:  41.91256512815277
Printing some Q and Qe and total Qs values:  [[-0.044]
 [-0.014]
 [-0.052]
 [-0.05 ]
 [-0.047]
 [-0.042]
 [-0.043]] [[30.766]
 [40.135]
 [29.521]
 [30.91 ]
 [31.902]
 [33.186]
 [32.278]] [[0.078]
 [0.178]
 [0.062]
 [0.073]
 [0.084]
 [0.098]
 [0.091]]
maxi score, test score, baseline:  -0.6739800000000001 0.5026666666666667 0.5026666666666667
actor:  0 policy actor:  1  step number:  53 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.333
siam score:  -0.84963316
maxi score, test score, baseline:  -0.6711 0.5026666666666667 0.5026666666666667
probs:  [0.11974543502981687, 0.11974543502981687, 0.3668311845949344, 0.11974543502981687, 0.1541870752857981, 0.11974543502981687]
maxi score, test score, baseline:  -0.6711 0.5026666666666667 0.5026666666666667
actor:  0 policy actor:  1  step number:  56 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  33.63927432993534
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6711666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.11978517939276358, 0.11978517939276358, 0.36659483477323523, 0.11978517939276358, 0.15426444765571043, 0.11978517939276358]
actions average: 
K:  2  action  0 :  tensor([0.5603, 0.0473, 0.0459, 0.0801, 0.0948, 0.0629, 0.1087],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0098, 0.9279, 0.0086, 0.0122, 0.0056, 0.0117, 0.0241],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1794, 0.0132, 0.2557, 0.1156, 0.1419, 0.1716, 0.1226],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1919, 0.0197, 0.1132, 0.1450, 0.1725, 0.1707, 0.1870],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1524, 0.0276, 0.0521, 0.0897, 0.5176, 0.0689, 0.0917],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1174, 0.0006, 0.0759, 0.0793, 0.0936, 0.5322, 0.1010],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2017, 0.0435, 0.0999, 0.1307, 0.1645, 0.1065, 0.2532],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[-0.077]
 [-0.064]
 [-0.077]
 [-0.077]
 [-0.061]
 [-0.077]
 [-0.061]] [[20.221]
 [22.797]
 [20.221]
 [20.221]
 [32.667]
 [20.221]
 [29.455]] [[0.428]
 [0.521]
 [0.428]
 [0.428]
 [0.826]
 [0.428]
 [0.727]]
printing an ep nov before normalisation:  54.93637237821283
printing an ep nov before normalisation:  34.87921201454436
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6711666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.11982483503396417, 0.11982483503396417, 0.36635901255764586, 0.11982483503396417, 0.1543416473064976, 0.11982483503396417]
printing an ep nov before normalisation:  25.864924970036704
Printing some Q and Qe and total Qs values:  [[-0.077]
 [-0.073]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.077]
 [-0.073]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]]
maxi score, test score, baseline:  -0.6711666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.11982483503396417, 0.11982483503396417, 0.36635901255764586, 0.11982483503396417, 0.1543416473064976, 0.11982483503396417]
actor:  1 policy actor:  1  step number:  53 total reward:  0.26666666666666616  reward:  1.0 rdn_beta:  1.333
from probs:  [0.11982483503396417, 0.11982483503396417, 0.36635901255764586, 0.11982483503396417, 0.1543416473064976, 0.11982483503396417]
maxi score, test score, baseline:  -0.6711666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.11982483503396417, 0.11982483503396417, 0.36635901255764586, 0.11982483503396417, 0.1543416473064976, 0.11982483503396417]
maxi score, test score, baseline:  -0.6711666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.11982483503396417, 0.11982483503396417, 0.36635901255764586, 0.11982483503396417, 0.1543416473064976, 0.11982483503396417]
actor:  1 policy actor:  1  step number:  65 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  30.616784272409845
using another actor
from probs:  [0.11982483503396417, 0.11982483503396417, 0.36635901255764586, 0.11982483503396417, 0.1543416473064976, 0.11982483503396417]
maxi score, test score, baseline:  -0.6711666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.11982483503396417, 0.11982483503396417, 0.36635901255764586, 0.11982483503396417, 0.1543416473064976, 0.11982483503396417]
printing an ep nov before normalisation:  24.659724235534668
printing an ep nov before normalisation:  38.68407098273958
maxi score, test score, baseline:  -0.6711666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.11982483503396417, 0.11982483503396417, 0.36635901255764586, 0.11982483503396417, 0.1543416473064976, 0.11982483503396417]
Printing some Q and Qe and total Qs values:  [[-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.021]
 [-0.031]
 [-0.031]] [[22.638]
 [22.638]
 [22.638]
 [22.638]
 [19.264]
 [22.638]
 [22.638]] [[0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.591]
 [0.74 ]
 [0.74 ]]
maxi score, test score, baseline:  -0.6711666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.11982483503396417, 0.11982483503396417, 0.36635901255764586, 0.11982483503396417, 0.1543416473064976, 0.11982483503396417]
printing an ep nov before normalisation:  39.546682834625244
maxi score, test score, baseline:  -0.6711666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.1198644022501682, 0.1198644022501682, 0.3661237161834802, 0.1198644022501682, 0.1544186748158469, 0.1198644022501682]
actor:  1 policy actor:  1  step number:  69 total reward:  0.07999999999999896  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  33.001215999975365
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.56 ]
 [0.564]
 [0.513]
 [0.529]
 [0.545]
 [0.536]] [[34.06 ]
 [30.424]
 [33.193]
 [35.605]
 [35.487]
 [32.783]
 [35.268]] [[1.63 ]
 [1.447]
 [1.611]
 [1.699]
 [1.708]
 [1.568]
 [1.702]]
printing an ep nov before normalisation:  23.819083753659598
line 256 mcts: sample exp_bonus 25.70219303880419
actor:  1 policy actor:  1  step number:  61 total reward:  0.2799999999999997  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[ 0.036]
 [ 0.225]
 [ 0.036]
 [ 0.036]
 [-0.08 ]
 [ 0.036]
 [ 0.234]] [[31.4  ]
 [38.378]
 [31.4  ]
 [31.4  ]
 [37.344]
 [31.934]
 [39.379]] [[0.543]
 [0.952]
 [0.543]
 [0.543]
 [0.614]
 [0.56 ]
 [0.992]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.72029725708641
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6711666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.1199916755426034, 0.1199916755426034, 0.36544869808918146, 0.1199916755426034, 0.15458459974040506, 0.1199916755426034]
maxi score, test score, baseline:  -0.6711666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.1199916755426034, 0.1199916755426034, 0.36544869808918146, 0.1199916755426034, 0.15458459974040506, 0.1199916755426034]
printing an ep nov before normalisation:  33.96567920370283
maxi score, test score, baseline:  -0.6711666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.1199916755426034, 0.1199916755426034, 0.36544869808918146, 0.1199916755426034, 0.15458459974040506, 0.1199916755426034]
actor:  1 policy actor:  1  step number:  44 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6711666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.12003090472682765, 0.12003090472682765, 0.36521549957084853, 0.12003090472682765, 0.1546608815218409, 0.12003090472682765]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.084]
 [-0.084]
 [-0.084]
 [-0.084]
 [-0.084]
 [-0.084]
 [-0.084]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.084]
 [-0.084]
 [-0.084]
 [-0.084]
 [-0.084]
 [-0.084]
 [-0.084]]
maxi score, test score, baseline:  -0.6711666666666668 0.5026666666666667 0.5026666666666667
probs:  [0.12003090472682765, 0.12003090472682765, 0.36521549957084853, 0.12003090472682765, 0.1546608815218409, 0.12003090472682765]
maxi score, test score, baseline:  -0.6711666666666668 0.5026666666666667 0.5026666666666667
probs:  [0.12003090472682765, 0.12003090472682765, 0.36521549957084853, 0.12003090472682765, 0.1546608815218409, 0.12003090472682765]
maxi score, test score, baseline:  -0.6711666666666668 0.5026666666666667 0.5026666666666667
probs:  [0.12003997828560063, 0.12003997828560063, 0.365243128091396, 0.12003997828560063, 0.15459695876620155, 0.12003997828560063]
maxi score, test score, baseline:  -0.6711666666666668 0.5026666666666667 0.5026666666666667
probs:  [0.12003997828560063, 0.12003997828560063, 0.365243128091396, 0.12003997828560063, 0.15459695876620155, 0.12003997828560063]
Printing some Q and Qe and total Qs values:  [[0.667]
 [0.766]
 [0.603]
 [0.647]
 [0.675]
 [0.705]
 [0.707]] [[18.489]
 [18.988]
 [17.914]
 [18.884]
 [19.505]
 [19.387]
 [19.281]] [[2.015]
 [2.191]
 [1.862]
 [2.055]
 [2.179]
 [2.191]
 [2.177]]
actor:  1 policy actor:  1  step number:  80 total reward:  0.13999999999999924  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6711666666666668 0.5026666666666667 0.5026666666666667
probs:  [0.12005801112830894, 0.12005801112830894, 0.3652980371714774, 0.12005801112830894, 0.15446991831528686, 0.12005801112830894]
maxi score, test score, baseline:  -0.6711666666666668 0.5026666666666667 0.5026666666666667
probs:  [0.12005801112830894, 0.12005801112830894, 0.3652980371714774, 0.12005801112830894, 0.15446991831528686, 0.12005801112830894]
siam score:  -0.8473611
maxi score, test score, baseline:  -0.6711666666666668 0.5026666666666667 0.5026666666666667
probs:  [0.12005801112830894, 0.12005801112830894, 0.3652980371714774, 0.12005801112830894, 0.15446991831528686, 0.12005801112830894]
actor:  1 policy actor:  1  step number:  67 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6711666666666668 0.5026666666666667 0.5026666666666667
probs:  [0.12013628437974698, 0.12013628437974698, 0.36483326038860486, 0.12013628437974698, 0.1546216020924073, 0.12013628437974698]
printing an ep nov before normalisation:  56.034666937413306
from probs:  [0.12013628437974698, 0.12013628437974698, 0.36483326038860486, 0.12013628437974698, 0.1546216020924073, 0.12013628437974698]
maxi score, test score, baseline:  -0.6711666666666668 0.5026666666666667 0.5026666666666667
probs:  [0.12013628437974698, 0.12013628437974698, 0.36483326038860486, 0.12013628437974698, 0.1546216020924073, 0.12013628437974698]
maxi score, test score, baseline:  -0.6711666666666668 0.5026666666666667 0.5026666666666667
probs:  [0.12013628437974698, 0.12013628437974698, 0.36483326038860486, 0.12013628437974698, 0.1546216020924073, 0.12013628437974698]
printing an ep nov before normalisation:  54.496444972951146
maxi score, test score, baseline:  -0.6711666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.12013628437974698, 0.12013628437974698, 0.36483326038860486, 0.12013628437974698, 0.1546216020924073, 0.12013628437974698]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.58448087166339
printing an ep nov before normalisation:  51.69535566622017
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.45 ]
 [0.404]
 [0.404]
 [0.416]
 [0.404]
 [0.404]] [[48.509]
 [48.932]
 [47.485]
 [47.485]
 [48.072]
 [47.485]
 [47.485]] [[1.717]
 [1.758]
 [1.639]
 [1.639]
 [1.681]
 [1.639]
 [1.639]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6711666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.12013628437974698, 0.12013628437974698, 0.36483326038860486, 0.12013628437974698, 0.1546216020924073, 0.12013628437974698]
actor:  1 policy actor:  1  step number:  43 total reward:  0.56  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]] [[29.437]
 [29.437]
 [29.437]
 [29.437]
 [29.437]
 [29.437]
 [29.437]] [[1.035]
 [1.035]
 [1.035]
 [1.035]
 [1.035]
 [1.035]
 [1.035]]
maxi score, test score, baseline:  -0.6711666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.08391055318306237, 0.5711805564071069, 0.09203258690922182, 0.08391055318306237, 0.08505519713448406, 0.08391055318306237]
printing an ep nov before normalisation:  3.230837177078172
actor:  1 policy actor:  1  step number:  58 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6711666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.08391076136563208, 0.5711819754318668, 0.09203281527464918, 0.08391076136563208, 0.08505292519658772, 0.08391076136563208]
printing an ep nov before normalisation:  29.914129359406594
Printing some Q and Qe and total Qs values:  [[0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]] [[56.721]
 [56.721]
 [56.721]
 [56.721]
 [56.721]
 [56.721]
 [56.721]] [[1.735]
 [1.735]
 [1.735]
 [1.735]
 [1.735]
 [1.735]
 [1.735]]
maxi score, test score, baseline:  -0.6711666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.08391076136563208, 0.5711819754318668, 0.09203281527464918, 0.08391076136563208, 0.08505292519658772, 0.08391076136563208]
maxi score, test score, baseline:  -0.6711666666666667 0.5026666666666667 0.5026666666666667
Printing some Q and Qe and total Qs values:  [[0.756]
 [0.875]
 [0.765]
 [0.797]
 [0.749]
 [0.807]
 [0.782]] [[43.769]
 [42.281]
 [43.715]
 [44.757]
 [47.137]
 [47.77 ]
 [39.615]] [[0.756]
 [0.875]
 [0.765]
 [0.797]
 [0.749]
 [0.807]
 [0.782]]
printing an ep nov before normalisation:  43.29022185973383
printing an ep nov before normalisation:  37.97357273985958
actor:  0 policy actor:  1  step number:  45 total reward:  0.5466666666666667  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6680733333333334 0.5026666666666667 0.5026666666666667
probs:  [0.0839122353959151, 0.5711920227930419, 0.09201685169645249, 0.0839122353959151, 0.08505441932276026, 0.0839122353959151]
actor:  1 policy actor:  1  step number:  67 total reward:  0.2799999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6680733333333334 0.5026666666666667 0.5026666666666667
probs:  [0.08391244268558365, 0.5711934357315628, 0.0920170790389154, 0.08391244268558365, 0.08505215717277086, 0.08391244268558365]
line 256 mcts: sample exp_bonus 38.4568233009064
printing an ep nov before normalisation:  31.465744972229004
printing an ep nov before normalisation:  42.65632184924571
maxi score, test score, baseline:  -0.6680733333333334 0.5026666666666667 0.5026666666666667
probs:  [0.08391244268558365, 0.5711934357315628, 0.0920170790389154, 0.08391244268558365, 0.08505215717277086, 0.08391244268558365]
actor:  1 policy actor:  1  step number:  53 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.8347228
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.83623231837037
printing an ep nov before normalisation:  54.485297107776894
printing an ep nov before normalisation:  38.49995673342165
actor:  1 policy actor:  1  step number:  48 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.6680733333333334 0.5026666666666667 0.5026666666666667
printing an ep nov before normalisation:  6.810866792996725
maxi score, test score, baseline:  -0.6680733333333334 0.5026666666666667 0.5026666666666667
probs:  [0.08391483934450336, 0.5711408588342788, 0.0920550646097783, 0.08391483934450336, 0.08505955852243283, 0.08391483934450336]
printing an ep nov before normalisation:  30.46508239317238
actor:  1 policy actor:  1  step number:  59 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.0
using another actor
printing an ep nov before normalisation:  50.82344183247466
line 256 mcts: sample exp_bonus 49.40880672960487
printing an ep nov before normalisation:  56.3197663953598
printing an ep nov before normalisation:  30.220643321156274
maxi score, test score, baseline:  -0.6680733333333334 0.5026666666666667 0.5026666666666667
probs:  [0.08392244526014002, 0.5709860551711198, 0.09216943035533864, 0.08392244526014002, 0.08507717869312163, 0.08392244526014002]
maxi score, test score, baseline:  -0.6680733333333334 0.5026666666666667 0.5026666666666667
probs:  [0.08392484157668176, 0.5709335377462017, 0.09220738384002138, 0.08392484157668176, 0.08508455368373184, 0.08392484157668176]
printing an ep nov before normalisation:  33.69775235953943
Printing some Q and Qe and total Qs values:  [[-0.083]
 [-0.02 ]
 [-0.069]
 [-0.069]
 [-0.068]
 [-0.071]
 [-0.081]] [[40.767]
 [53.342]
 [41.335]
 [41.584]
 [41.948]
 [41.654]
 [41.533]] [[0.478]
 [0.889]
 [0.508]
 [0.514]
 [0.525]
 [0.514]
 [0.501]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.821699142456055
actor:  1 policy actor:  1  step number:  60 total reward:  0.42000000000000004  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  64 total reward:  0.2599999999999998  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  49.930068937560456
using explorer policy with actor:  1
printing an ep nov before normalisation:  21.86534447839113
maxi score, test score, baseline:  -0.6680733333333334 0.5026666666666667 0.5026666666666667
probs:  [0.08392984354065476, 0.5708299750999349, 0.09228349712213105, 0.08392984354065476, 0.0850969971559697, 0.08392984354065476]
printing an ep nov before normalisation:  32.31095396282529
actor:  1 policy actor:  1  step number:  55 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6680733333333334 0.5026666666666667 0.5026666666666667
probs:  [0.08393463419998085, 0.5707250350538411, 0.09235934638349819, 0.08393463419998085, 0.08511171596271802, 0.08393463419998085]
maxi score, test score, baseline:  -0.6680733333333334 0.5026666666666667 0.5026666666666667
probs:  [0.08393463419998085, 0.5707250350538411, 0.09235934638349819, 0.08393463419998085, 0.08511171596271802, 0.08393463419998085]
maxi score, test score, baseline:  -0.6680733333333334 0.5026666666666667 0.5026666666666667
probs:  [0.08393463419998085, 0.5707250350538411, 0.09235934638349819, 0.08393463419998085, 0.08511171596271802, 0.08393463419998085]
printing an ep nov before normalisation:  47.57779682894077
actor:  0 policy actor:  0  step number:  88 total reward:  0.11333333333333317  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6658466666666667 0.5026666666666667 0.5026666666666667
probs:  [0.08393463419998085, 0.5707250350538411, 0.09235934638349819, 0.08393463419998085, 0.08511171596271802, 0.08393463419998085]
Printing some Q and Qe and total Qs values:  [[0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]] [[48.225]
 [48.225]
 [48.225]
 [48.225]
 [48.225]
 [48.225]
 [48.225]] [[1.141]
 [1.141]
 [1.141]
 [1.141]
 [1.141]
 [1.141]
 [1.141]]
actor:  0 policy actor:  0  step number:  39 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  42.841194031091916
printing an ep nov before normalisation:  30.16721197604184
actor:  1 policy actor:  1  step number:  38 total reward:  0.5133333333333332  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  57 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6628866666666667 0.5026666666666667 0.5026666666666667
probs:  [0.08392074711794921, 0.5710292334802635, 0.09213947583062465, 0.08392074711794921, 0.08506904933526432, 0.08392074711794921]
maxi score, test score, baseline:  -0.6628866666666667 0.5026666666666667 0.5026666666666667
probs:  [0.08392074711794921, 0.5710292334802635, 0.09213947583062465, 0.08392074711794921, 0.08506904933526432, 0.08392074711794921]
maxi score, test score, baseline:  -0.6628866666666667 0.5026666666666667 0.5026666666666667
probs:  [0.08392074711794921, 0.5710292334802635, 0.09213947583062465, 0.08392074711794921, 0.08506904933526432, 0.08392074711794921]
maxi score, test score, baseline:  -0.6628866666666667 0.5026666666666667 0.5026666666666667
probs:  [0.08392074711794921, 0.5710292334802635, 0.09213947583062465, 0.08392074711794921, 0.08506904933526432, 0.08392074711794921]
maxi score, test score, baseline:  -0.6628866666666667 0.5026666666666667 0.5026666666666667
probs:  [0.08392074711794921, 0.5710292334802635, 0.09213947583062465, 0.08392074711794921, 0.08506904933526432, 0.08392074711794921]
Printing some Q and Qe and total Qs values:  [[-0.102]
 [-0.096]
 [-0.099]
 [-0.099]
 [-0.099]
 [-0.1  ]
 [-0.103]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.102]
 [-0.096]
 [-0.099]
 [-0.099]
 [-0.099]
 [-0.1  ]
 [-0.103]]
maxi score, test score, baseline:  -0.6628866666666667 0.5026666666666667 0.5026666666666667
printing an ep nov before normalisation:  46.128736982599605
printing an ep nov before normalisation:  32.01182575563492
printing an ep nov before normalisation:  23.429232668189677
maxi score, test score, baseline:  -0.6628866666666667 0.5026666666666667 0.5026666666666667
actor:  1 policy actor:  1  step number:  57 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  74 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  1.333
siam score:  -0.8368938
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.586758414001956
maxi score, test score, baseline:  -0.6603666666666667 0.5026666666666667 0.5026666666666667
printing an ep nov before normalisation:  28.45249346324376
printing an ep nov before normalisation:  22.419431913694318
Printing some Q and Qe and total Qs values:  [[-0.01]
 [-0.01]
 [-0.01]
 [-0.01]
 [-0.01]
 [-0.01]
 [-0.01]] [[44.283]
 [49.5  ]
 [44.283]
 [44.283]
 [44.283]
 [44.283]
 [44.283]] [[0.521]
 [0.622]
 [0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.521]]
maxi score, test score, baseline:  -0.6603666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.08393232929342478, 0.5707755243463957, 0.09232285339831506, 0.08393232929342478, 0.08510463437501493, 0.08393232929342478]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.236]
 [-0.249]
 [-0.23 ]
 [-0.227]
 [-0.227]
 [-0.224]
 [-0.242]] [[5.344]
 [8.036]
 [8.357]
 [4.447]
 [6.062]
 [9.009]
 [8.723]] [[-0.141]
 [-0.105]
 [-0.081]
 [-0.148]
 [-0.119]
 [-0.063]
 [-0.087]]
using another actor
maxi score, test score, baseline:  -0.6603666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.0839346442147794, 0.5707248156783119, 0.09235950494519599, 0.0839346442147794, 0.08511174673215387, 0.0839346442147794]
printing an ep nov before normalisation:  36.28469784891088
printing an ep nov before normalisation:  30.736490166065416
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.6603666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.0839392725450671, 0.5706234314719512, 0.09243278409317604, 0.0839392725450671, 0.08512596679967137, 0.0839392725450671]
maxi score, test score, baseline:  -0.6603666666666667 0.5026666666666667 0.5026666666666667
maxi score, test score, baseline:  -0.6603666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.0839408111846842, 0.5706339054040148, 0.09241613381549492, 0.0839408111846842, 0.08512752722643767, 0.0839408111846842]
Printing some Q and Qe and total Qs values:  [[0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]] [[33.611]
 [33.611]
 [33.611]
 [33.611]
 [33.611]
 [33.611]
 [33.611]] [[0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]]
maxi score, test score, baseline:  -0.6603666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.08394313085288413, 0.5705832712403561, 0.092452694827826, 0.08394313085288413, 0.08513464137316562, 0.08394313085288413]
actor:  0 policy actor:  1  step number:  61 total reward:  0.22666666666666624  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  4  action  0 :  tensor([0.4604, 0.0173, 0.0891, 0.0803, 0.1335, 0.0973, 0.1221],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0087, 0.9166, 0.0103, 0.0133, 0.0121, 0.0072, 0.0319],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0990, 0.0107, 0.1131, 0.1412, 0.3528, 0.1405, 0.1427],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1034, 0.0373, 0.1286, 0.1460, 0.1779, 0.2511, 0.1557],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1107, 0.0313, 0.0761, 0.0717, 0.5271, 0.0925, 0.0907],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0544, 0.0029, 0.0797, 0.0491, 0.0902, 0.6686, 0.0551],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0657, 0.1451, 0.0797, 0.1492, 0.0973, 0.1243, 0.3386],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  39.14410570301647
maxi score, test score, baseline:  -0.6579133333333333 0.5026666666666667 0.5026666666666667
probs:  [0.08394313085288413, 0.5705832712403561, 0.092452694827826, 0.08394313085288413, 0.08513464137316562, 0.08394313085288413]
siam score:  -0.83098036
maxi score, test score, baseline:  -0.6579133333333333 0.5026666666666667 0.5026666666666667
probs:  [0.08394313085288413, 0.5705832712403561, 0.092452694827826, 0.08394313085288413, 0.08513464137316562, 0.08394313085288413]
maxi score, test score, baseline:  -0.6579133333333333 0.5026666666666667 0.5026666666666667
maxi score, test score, baseline:  -0.6579133333333333 0.5026666666666667 0.5026666666666667
probs:  [0.08394313085288413, 0.5705832712403561, 0.092452694827826, 0.08394313085288413, 0.08513464137316562, 0.08394313085288413]
printing an ep nov before normalisation:  29.2710959187182
printing an ep nov before normalisation:  28.56414586730034
maxi score, test score, baseline:  -0.6579133333333333 0.5026666666666667 0.5026666666666667
probs:  [0.08394313085288413, 0.5705832712403561, 0.092452694827826, 0.08394313085288413, 0.08513464137316562, 0.08394313085288413]
Printing some Q and Qe and total Qs values:  [[0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]] [[22.895]
 [22.895]
 [22.895]
 [22.895]
 [22.895]
 [22.895]
 [22.895]] [[0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]]
maxi score, test score, baseline:  -0.6579133333333333 0.5026666666666667 0.5026666666666667
maxi score, test score, baseline:  -0.6579133333333333 0.5026666666666667 0.5026666666666667
probs:  [0.08394545001636373, 0.5705326480938306, 0.09248924788510171, 0.08394545001636373, 0.08514175397197644, 0.08394545001636373]
printing an ep nov before normalisation:  16.607086912192518
printing an ep nov before normalisation:  13.549245332273877
printing an ep nov before normalisation:  46.69995212247666
actor:  1 policy actor:  1  step number:  66 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6579133333333333 0.5026666666666667 0.5026666666666667
probs:  [0.08394776867528768, 0.5704820359608429, 0.09252579298991942, 0.08394776867528768, 0.08514886502337472, 0.08394776867528768]
printing an ep nov before normalisation:  39.31663990020752
Printing some Q and Qe and total Qs values:  [[0.831]
 [0.893]
 [0.836]
 [0.825]
 [0.836]
 [0.833]
 [0.835]] [[28.336]
 [33.301]
 [27.309]
 [26.427]
 [28.925]
 [26.575]
 [26.219]] [[0.831]
 [0.893]
 [0.836]
 [0.825]
 [0.836]
 [0.833]
 [0.835]]
maxi score, test score, baseline:  -0.6579133333333333 0.5026666666666667 0.5026666666666667
probs:  [0.08394776867528768, 0.5704820359608429, 0.09252579298991942, 0.08394776867528768, 0.08514886502337472, 0.08394776867528768]
actor:  0 policy actor:  0  step number:  72 total reward:  0.07333333333333258  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.60106666729804
actor:  1 policy actor:  1  step number:  48 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6557666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.08370298044516579, 0.575825320533165, 0.08866760952207631, 0.08370298044516579, 0.0843981286092614, 0.08370298044516579]
maxi score, test score, baseline:  -0.6557666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.08370298044516579, 0.575825320533165, 0.08866760952207631, 0.08370298044516579, 0.0843981286092614, 0.08370298044516579]
actor:  1 policy actor:  1  step number:  56 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  36.804615541212065
Printing some Q and Qe and total Qs values:  [[0.036]
 [0.042]
 [0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.036]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.036]
 [0.042]
 [0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.036]]
printing an ep nov before normalisation:  34.42964822084133
maxi score, test score, baseline:  -0.6557666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.08370310520894764, 0.57582618000109, 0.088667741697701, 0.08370310520894764, 0.08439676267436604, 0.08370310520894764]
actor:  0 policy actor:  1  step number:  62 total reward:  0.16666666666666607  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  57 total reward:  0.3466666666666659  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.77 ]
 [0.807]
 [0.77 ]
 [0.77 ]
 [0.77 ]
 [0.77 ]
 [0.821]] [[34.061]
 [32.075]
 [34.061]
 [34.061]
 [34.061]
 [34.061]
 [36.694]] [[0.77 ]
 [0.807]
 [0.77 ]
 [0.77 ]
 [0.77 ]
 [0.77 ]
 [0.821]]
actions average: 
K:  2  action  0 :  tensor([0.5730, 0.0041, 0.0675, 0.0841, 0.0990, 0.0920, 0.0804],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0098, 0.9554, 0.0047, 0.0038, 0.0033, 0.0033, 0.0197],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1145, 0.0100, 0.2569, 0.1388, 0.1514, 0.1841, 0.1444],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0901, 0.0274, 0.0985, 0.3531, 0.1318, 0.1329, 0.1662],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1205, 0.0010, 0.0800, 0.0753, 0.5452, 0.0928, 0.0851],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0843, 0.0028, 0.0837, 0.0877, 0.1438, 0.5290, 0.0687],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1885, 0.0325, 0.0818, 0.1265, 0.1007, 0.0980, 0.3721],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6534333333333333 0.5026666666666667 0.5026666666666667
probs:  [0.08370445730343766, 0.5757966806207783, 0.08868904510836496, 0.08370445730343766, 0.08440090236054365, 0.08370445730343766]
maxi score, test score, baseline:  -0.6534333333333333 0.5026666666666667 0.5026666666666667
probs:  [0.08370580922841116, 0.5757671849389003, 0.08871034584815066, 0.08370580922841116, 0.08440504152771547, 0.08370580922841116]
actor:  0 policy actor:  1  step number:  50 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6505666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.08370580922841116, 0.5757671849389003, 0.08871034584815066, 0.08370580922841116, 0.08440504152771547, 0.08370580922841116]
printing an ep nov before normalisation:  42.12833754231853
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.39 ]
 [0.333]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]] [[30.918]
 [36.079]
 [28.655]
 [34.103]
 [34.103]
 [34.103]
 [34.103]] [[0.726]
 [0.871]
 [0.656]
 [0.789]
 [0.789]
 [0.789]
 [0.789]]
maxi score, test score, baseline:  -0.6505666666666667 0.5026666666666667 0.5026666666666667
Printing some Q and Qe and total Qs values:  [[0.736]
 [0.751]
 [0.75 ]
 [0.732]
 [0.736]
 [0.716]
 [0.736]] [[39.637]
 [36.222]
 [32.401]
 [31.848]
 [39.637]
 [31.732]
 [39.637]] [[1.318]
 [1.243]
 [1.14 ]
 [1.106]
 [1.318]
 [1.088]
 [1.318]]
maxi score, test score, baseline:  -0.6505666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.08370580922841116, 0.5757671849389003, 0.08871034584815066, 0.08370580922841116, 0.08440504152771547, 0.08370580922841116]
actor:  1 policy actor:  1  step number:  60 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6505666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.08370716098390002, 0.5757376929547606, 0.08873164391756028, 0.08370716098390002, 0.08440918017597908, 0.08370716098390002]
maxi score, test score, baseline:  -0.6505666666666667 0.5026666666666667 0.5026666666666667
maxi score, test score, baseline:  -0.6505666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.08370716098390002, 0.5757376929547606, 0.08873164391756028, 0.08370716098390002, 0.08440918017597908, 0.08370716098390002]
Printing some Q and Qe and total Qs values:  [[-0.247]
 [-0.252]
 [-0.246]
 [-0.244]
 [-0.246]
 [-0.245]
 [-0.246]] [[7.183]
 [6.742]
 [7.51 ]
 [5.786]
 [5.852]
 [1.703]
 [5.171]] [[-0.176]
 [-0.185]
 [-0.172]
 [-0.187]
 [-0.188]
 [-0.228]
 [-0.195]]
maxi score, test score, baseline:  -0.6505666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.08370716098390002, 0.5757376929547606, 0.08873164391756028, 0.08370716098390002, 0.08440918017597908, 0.08370716098390002]
maxi score, test score, baseline:  -0.6505666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.08370716098390002, 0.5757376929547606, 0.08873164391756028, 0.08370716098390002, 0.08440918017597908, 0.08370716098390002]
Printing some Q and Qe and total Qs values:  [[0.158]
 [0.245]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]] [[35.755]
 [37.225]
 [35.755]
 [35.755]
 [35.755]
 [35.755]
 [35.755]] [[0.779]
 [0.912]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]]
maxi score, test score, baseline:  -0.6505666666666667 0.5026666666666667 0.5026666666666667
maxi score, test score, baseline:  -0.6505666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.08370716098390002, 0.5757376929547606, 0.08873164391756028, 0.08370716098390002, 0.08440918017597908, 0.08370716098390002]
printing an ep nov before normalisation:  31.178233595894977
printing an ep nov before normalisation:  26.16395372722271
actor:  1 policy actor:  1  step number:  72 total reward:  0.12666666666666582  reward:  1.0 rdn_beta:  1.0
UNIT TEST: sample policy line 217 mcts : [0.143 0.143 0.082 0.061 0.061 0.449 0.061]
actor:  1 policy actor:  1  step number:  71 total reward:  0.0799999999999993  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  0.00011864407522605485
Printing some Q and Qe and total Qs values:  [[-0.092]
 [-0.092]
 [-0.092]
 [-0.092]
 [-0.101]
 [-0.092]
 [-0.092]] [[28.222]
 [28.222]
 [28.222]
 [28.222]
 [31.512]
 [28.222]
 [28.222]] [[0.273]
 [0.273]
 [0.273]
 [0.273]
 [0.341]
 [0.273]
 [0.273]]
Printing some Q and Qe and total Qs values:  [[-0.097]
 [-0.108]
 [-0.097]
 [-0.097]
 [-0.097]
 [-0.094]
 [-0.097]] [[30.074]
 [29.791]
 [30.074]
 [30.074]
 [30.074]
 [34.496]
 [30.074]] [[0.404]
 [0.384]
 [0.404]
 [0.404]
 [0.404]
 [0.551]
 [0.404]]
maxi score, test score, baseline:  -0.6505666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.0837094219045685, 0.5757144671680654, 0.08874303180957882, 0.0837094219045685, 0.08441423530865017, 0.0837094219045685]
actor:  1 policy actor:  1  step number:  62 total reward:  0.3133333333333328  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.122]
 [-0.066]
 [-0.097]
 [-0.097]
 [-0.097]
 [-0.097]
 [-0.108]] [[25.953]
 [29.923]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [27.54 ]] [[ 0.17 ]
 [ 0.328]
 [-0.47 ]
 [-0.47 ]
 [-0.47 ]
 [-0.47 ]
 [ 0.225]]
Printing some Q and Qe and total Qs values:  [[0.049]
 [0.096]
 [0.011]
 [0.017]
 [0.007]
 [0.011]
 [0.016]] [[34.413]
 [36.578]
 [32.698]
 [32.931]
 [32.763]
 [33.202]
 [33.829]] [[0.577]
 [0.683]
 [0.492]
 [0.505]
 [0.49 ]
 [0.506]
 [0.529]]
maxi score, test score, baseline:  -0.6505666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.08371213178712676, 0.5756555505460729, 0.08878553674058473, 0.08371213178712676, 0.08442251735196198, 0.08371213178712676]
printing an ep nov before normalisation:  51.51588290620745
maxi score, test score, baseline:  -0.6505666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.08371348647385041, 0.5756260977694685, 0.08880678521334459, 0.08371348647385041, 0.08442665759563572, 0.08371348647385041]
Printing some Q and Qe and total Qs values:  [[-0.147]
 [-0.175]
 [-0.15 ]
 [-0.15 ]
 [-0.15 ]
 [-0.151]
 [-0.151]] [[43.243]
 [42.216]
 [43.588]
 [43.249]
 [42.98 ]
 [44.237]
 [44.362]] [[0.403]
 [0.349]
 [0.41 ]
 [0.401]
 [0.394]
 [0.426]
 [0.429]]
printing an ep nov before normalisation:  36.86473639923159
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  63.154148595110925
Printing some Q and Qe and total Qs values:  [[ 0.203]
 [ 0.182]
 [ 0.144]
 [ 0.144]
 [ 0.195]
 [-0.032]
 [ 0.144]] [[26.889]
 [27.205]
 [24.18 ]
 [24.18 ]
 [28.974]
 [26.118]
 [24.18 ]] [[0.637]
 [0.625]
 [0.498]
 [0.498]
 [0.69 ]
 [0.379]
 [0.498]]
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.61 ]
 [0.38 ]
 [0.478]
 [0.48 ]
 [0.483]
 [0.611]] [[42.674]
 [38.488]
 [42.08 ]
 [39.512]
 [42.283]
 [39.299]
 [37.657]] [[1.083]
 [1.097]
 [0.955]
 [0.99 ]
 [1.059]
 [0.99 ]
 [1.078]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.3599999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6505666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.08371767875938693, 0.5755386512946798, 0.08887065187768865, 0.08371767875938693, 0.08443766054947076, 0.08371767875938693]
printing an ep nov before normalisation:  28.08749157951682
printing an ep nov before normalisation:  37.96361616257019
maxi score, test score, baseline:  -0.6505666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.08371767875938693, 0.5755386512946798, 0.08887065187768865, 0.08371767875938693, 0.08443766054947076, 0.08371767875938693]
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.450261234834784
maxi score, test score, baseline:  -0.6505666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.0837190332684722, 0.5755092166561078, 0.08889189027122452, 0.0837190332684722, 0.08444179326725101, 0.0837190332684722]
maxi score, test score, baseline:  -0.6505666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.08372174177793375, 0.5754503584336448, 0.0889343590818557, 0.08372174177793375, 0.0844500571506982, 0.08372174177793375]
maxi score, test score, baseline:  -0.6505666666666667 0.5026666666666667 0.5026666666666667
siam score:  -0.826477
printing an ep nov before normalisation:  43.90185997429744
maxi score, test score, baseline:  -0.6505666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.08372174177793375, 0.5754503584336448, 0.0889343590818557, 0.08372174177793375, 0.0844500571506982, 0.08372174177793375]
actor:  1 policy actor:  1  step number:  68 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  34.06883239746094
maxi score, test score, baseline:  -0.6505666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.08372309577837371, 0.5754209348483699, 0.08895558949994915, 0.08372309577837371, 0.08445418831655974, 0.08372309577837371]
maxi score, test score, baseline:  -0.6505666666666667 0.5026666666666667 0.5026666666666667
maxi score, test score, baseline:  -0.6505666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.08372309577837371, 0.5754209348483699, 0.08895558949994915, 0.08372309577837371, 0.08445418831655974, 0.08372309577837371]
Printing some Q and Qe and total Qs values:  [[0.115]
 [0.175]
 [0.115]
 [0.115]
 [0.115]
 [0.115]
 [0.115]] [[31.789]
 [39.263]
 [31.789]
 [31.789]
 [31.789]
 [31.789]
 [31.789]] [[0.808]
 [1.175]
 [0.808]
 [0.808]
 [0.808]
 [0.808]
 [0.808]]
maxi score, test score, baseline:  -0.6505666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.08372309577837371, 0.5754209348483699, 0.08895558949994915, 0.08372309577837371, 0.08445418831655974, 0.08372309577837371]
printing an ep nov before normalisation:  33.02042007446289
maxi score, test score, baseline:  -0.6505666666666667 0.5026666666666667 0.5026666666666667
probs:  [0.08372309577837371, 0.5754209348483699, 0.08895558949994915, 0.08372309577837371, 0.08445418831655974, 0.08372309577837371]
actor:  1 policy actor:  1  step number:  55 total reward:  0.45333333333333337  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  60 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  44 total reward:  0.39333333333333287  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6481 0.5026666666666667 0.5026666666666667
probs:  [0.08372539457286357, 0.5753980179985788, 0.08896652605790287, 0.08372539457286357, 0.08445927222492765, 0.08372539457286357]
maxi score, test score, baseline:  -0.6481 0.5026666666666667 0.5026666666666667
probs:  [0.08372539457286357, 0.5753980179985788, 0.08896652605790287, 0.08372539457286357, 0.08445927222492765, 0.08372539457286357]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6481 0.5026666666666667 0.5026666666666667
probs:  [0.08372539457286357, 0.5753980179985788, 0.08896652605790287, 0.08372539457286357, 0.08445927222492765, 0.08372539457286357]
printing an ep nov before normalisation:  11.860683868886213
maxi score, test score, baseline:  -0.6481 0.5026666666666667 0.5026666666666667
probs:  [0.08372539457286357, 0.5753980179985788, 0.08896652605790287, 0.08372539457286357, 0.08445927222492765, 0.08372539457286357]
actor:  0 policy actor:  0  step number:  62 total reward:  0.0066666666666660435  reward:  1.0 rdn_beta:  0.667
using another actor
actor:  1 policy actor:  1  step number:  56 total reward:  0.3399999999999995  reward:  1.0 rdn_beta:  0.667
siam score:  -0.83221626
printing an ep nov before normalisation:  28.996519353752088
actor:  1 policy actor:  1  step number:  52 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6460866666666667 0.5026666666666667 0.5026666666666667
probs:  [0.08372810890713417, 0.5753392377743138, 0.08900889622610154, 0.08372810890713417, 0.08446753927818215, 0.08372810890713417]
Printing some Q and Qe and total Qs values:  [[0.789]
 [0.896]
 [0.801]
 [0.797]
 [0.803]
 [0.815]
 [0.818]] [[40.644]
 [40.855]
 [40.071]
 [38.58 ]
 [37.139]
 [39.854]
 [42.811]] [[0.789]
 [0.896]
 [0.801]
 [0.797]
 [0.803]
 [0.815]
 [0.818]]
actor:  0 policy actor:  1  step number:  41 total reward:  0.6266666666666668  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  46.06432939298561
maxi score, test score, baseline:  -0.6428333333333334 0.5026666666666667 0.5026666666666667
maxi score, test score, baseline:  -0.6428333333333334 0.5026666666666667 0.5026666666666667
maxi score, test score, baseline:  -0.6428333333333334 0.5026666666666667 0.5026666666666667
actor:  1 policy actor:  1  step number:  68 total reward:  0.2866666666666665  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6428333333333334 0.5026666666666667 0.5026666666666667
actor:  1 policy actor:  1  step number:  54 total reward:  0.5533333333333337  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  53 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.6428333333333334 0.5026666666666667 0.5026666666666667
probs:  [0.07481123226451884, 0.5139487256044332, 0.18614331934448172, 0.07481123226451884, 0.07547425825752868, 0.07481123226451884]
Printing some Q and Qe and total Qs values:  [[0.131]
 [0.244]
 [0.092]
 [0.095]
 [0.172]
 [0.101]
 [0.121]] [[40.27 ]
 [41.962]
 [39.171]
 [41.132]
 [40.262]
 [35.612]
 [40.504]] [[0.568]
 [0.719]
 [0.503]
 [0.551]
 [0.608]
 [0.43 ]
 [0.563]]
printing an ep nov before normalisation:  42.80285108160599
printing an ep nov before normalisation:  37.94293403625488
maxi score, test score, baseline:  -0.6428333333333334 0.5026666666666667 0.5026666666666667
probs:  [0.07481123226451884, 0.5139487256044332, 0.18614331934448172, 0.07481123226451884, 0.07547425825752868, 0.07481123226451884]
maxi score, test score, baseline:  -0.6428333333333334 0.5026666666666667 0.5026666666666667
probs:  [0.07481123226451884, 0.5139487256044332, 0.18614331934448172, 0.07481123226451884, 0.07547425825752868, 0.07481123226451884]
printing an ep nov before normalisation:  33.40314476517207
line 256 mcts: sample exp_bonus 34.29633988605044
actions average: 
K:  0  action  0 :  tensor([0.5023, 0.0152, 0.1157, 0.0840, 0.0895, 0.0923, 0.1009],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0054, 0.9220, 0.0189, 0.0078, 0.0053, 0.0054, 0.0351],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0943, 0.0066, 0.4195, 0.1027, 0.1033, 0.1484, 0.1252],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1201, 0.0087, 0.1589, 0.2904, 0.1081, 0.1756, 0.1383],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0793, 0.0173, 0.0749, 0.0790, 0.5778, 0.0798, 0.0918],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1194, 0.0153, 0.1188, 0.1217, 0.1201, 0.3503, 0.1546],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0839, 0.1005, 0.1070, 0.0974, 0.0956, 0.1011, 0.4146],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.6428333333333334 0.5026666666666667 0.5026666666666667
probs:  [0.07481123226451884, 0.5139487256044332, 0.18614331934448172, 0.07481123226451884, 0.07547425825752868, 0.07481123226451884]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.599]
 [0.696]
 [0.599]
 [0.599]
 [0.599]
 [0.628]
 [0.599]] [[29.56 ]
 [27.308]
 [29.56 ]
 [29.56 ]
 [29.56 ]
 [30.985]
 [29.56 ]] [[1.085]
 [1.112]
 [1.085]
 [1.085]
 [1.085]
 [1.158]
 [1.085]]
Printing some Q and Qe and total Qs values:  [[-0.032]
 [ 0.242]
 [ 0.107]
 [ 0.081]
 [-0.071]
 [ 0.075]
 [ 0.084]] [[25.332]
 [25.833]
 [24.791]
 [25.954]
 [28.286]
 [24.934]
 [25.165]] [[0.421]
 [0.711]
 [0.541]
 [0.554]
 [0.481]
 [0.514]
 [0.531]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.3866666666666666  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  59 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.6428333333333334 0.5026666666666667 0.5026666666666667
probs:  [0.07475412134306764, 0.5134866672011551, 0.18682939546492072, 0.07475412134306764, 0.0754215733047212, 0.07475412134306764]
Printing some Q and Qe and total Qs values:  [[-0.033]
 [-0.02 ]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.032]
 [-0.031]] [[41.262]
 [41.228]
 [41.262]
 [41.262]
 [41.262]
 [42.912]
 [43.582]] [[0.216]
 [0.229]
 [0.216]
 [0.216]
 [0.216]
 [0.235]
 [0.245]]
Printing some Q and Qe and total Qs values:  [[0.677]
 [0.723]
 [0.677]
 [0.677]
 [0.724]
 [0.677]
 [0.677]] [[13.743]
 [15.904]
 [13.743]
 [13.743]
 [12.742]
 [13.743]
 [13.743]] [[1.253]
 [1.39 ]
 [1.253]
 [1.253]
 [1.258]
 [1.253]
 [1.253]]
actor:  1 policy actor:  1  step number:  73 total reward:  0.09333333333333271  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.6428333333333334 0.5026666666666667 0.5026666666666667
probs:  [0.07472560536245805, 0.5132559574151182, 0.18717195924869498, 0.07472560536245805, 0.07539526724881278, 0.07472560536245805]
maxi score, test score, baseline:  -0.6428333333333334 0.5026666666666667 0.5026666666666667
maxi score, test score, baseline:  -0.6428333333333334 0.5026666666666667 0.5026666666666667
probs:  [0.07472571218234246, 0.5132566922316208, 0.18717222709718237, 0.07472571218234246, 0.07539394412416933, 0.07472571218234246]
maxi score, test score, baseline:  -0.6428333333333334 0.5026666666666667 0.5026666666666667
probs:  [0.07469722278456786, 0.5130261970884886, 0.18751447665409107, 0.07469722278456786, 0.07536765790371673, 0.07469722278456786]
Printing some Q and Qe and total Qs values:  [[0.078]
 [0.163]
 [0.078]
 [0.078]
 [0.078]
 [0.078]
 [0.078]] [[36.797]
 [49.755]
 [36.797]
 [36.797]
 [36.797]
 [36.797]
 [36.797]] [[0.43 ]
 [0.746]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]]
Printing some Q and Qe and total Qs values:  [[-0.107]
 [-0.099]
 [-0.109]
 [-0.105]
 [-0.108]
 [-0.106]
 [-0.104]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.107]
 [-0.099]
 [-0.109]
 [-0.105]
 [-0.108]
 [-0.106]
 [-0.104]]
siam score:  -0.8224608
maxi score, test score, baseline:  -0.6428333333333334 0.5026666666666667 0.5026666666666667
probs:  [0.07469722278456786, 0.5130261970884886, 0.18751447665409107, 0.07469722278456786, 0.07536765790371673, 0.07469722278456786]
printing an ep nov before normalisation:  36.43609765239247
maxi score, test score, baseline:  -0.6428333333333334 0.5026666666666667 0.5026666666666667
printing an ep nov before normalisation:  36.74389123916626
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]] [[28.804]
 [28.804]
 [28.804]
 [28.804]
 [28.804]
 [28.804]
 [28.804]] [[1.634]
 [1.634]
 [1.634]
 [1.634]
 [1.634]
 [1.634]
 [1.634]]
printing an ep nov before normalisation:  24.545613517284437
actor:  1 policy actor:  1  step number:  58 total reward:  0.23333333333333295  reward:  1.0 rdn_beta:  1.0
line 256 mcts: sample exp_bonus 29.053521648531312
printing an ep nov before normalisation:  40.08724919617875
printing an ep nov before normalisation:  45.824767081869325
maxi score, test score, baseline:  -0.6428333333333334 0.5026666666666667 0.5026666666666667
probs:  [0.07460176770395177, 0.5122317709215984, 0.1886817642817512, 0.07460176770395177, 0.07528116168479515, 0.07460176770395177]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.07 ]
 [0.128]
 [0.091]
 [0.101]
 [0.094]
 [0.086]
 [0.102]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.07 ]
 [0.128]
 [0.091]
 [0.101]
 [0.094]
 [0.086]
 [0.102]]
maxi score, test score, baseline:  -0.6428333333333334 0.5026666666666667 0.5026666666666667
probs:  [0.07451692561192083, 0.5115451366792699, 0.189701190513566, 0.07451692561192083, 0.07520289597140156, 0.07451692561192083]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.448]
 [0.352]
 [0.352]
 [0.357]
 [0.352]
 [0.352]] [[36.811]
 [43.601]
 [36.811]
 [36.811]
 [43.125]
 [36.811]
 [36.811]] [[0.352]
 [0.448]
 [0.352]
 [0.352]
 [0.357]
 [0.352]
 [0.352]]
maxi score, test score, baseline:  -0.6428333333333334 0.5026666666666667 0.5026666666666667
probs:  [0.07451692561192083, 0.5115451366792699, 0.189701190513566, 0.07451692561192083, 0.07520289597140156, 0.07451692561192083]
printing an ep nov before normalisation:  27.693045967153147
printing an ep nov before normalisation:  29.67209254125296
maxi score, test score, baseline:  -0.6428333333333334 0.5026666666666667 0.5026666666666667
probs:  [0.07446049372933024, 0.5110884286700942, 0.19037925183957063, 0.07446049372933024, 0.07515083830234448, 0.07446049372933024]
maxi score, test score, baseline:  -0.6428333333333334 0.5026666666666667 0.5026666666666667
probs:  [0.07443231653495422, 0.5108603882476083, 0.1907178169361903, 0.07443231653495422, 0.07512484521133869, 0.07443231653495422]
printing an ep nov before normalisation:  35.13723459561687
printing an ep nov before normalisation:  34.226730617319895
using explorer policy with actor:  1
using explorer policy with actor:  0
printing an ep nov before normalisation:  48.212063990261285
actor:  0 policy actor:  1  step number:  62 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 26.094281504990107
actor:  1 policy actor:  1  step number:  51 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  54 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.09 ]
 [-0.082]
 [-0.097]
 [-0.093]
 [-0.087]
 [-0.089]
 [-0.089]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.09 ]
 [-0.082]
 [-0.097]
 [-0.093]
 [-0.087]
 [-0.089]
 [-0.089]]
maxi score, test score, baseline:  -0.64042 0.5026666666666667 0.5026666666666667
probs:  [0.074404165124583, 0.510632556497259, 0.19105607222325366, 0.074404165124583, 0.07509887590573831, 0.074404165124583]
maxi score, test score, baseline:  -0.64042 0.5026666666666667 0.5026666666666667
probs:  [0.07440427522730293, 0.5106333132853911, 0.19105635525622755, 0.07440427522730293, 0.07509750577647256, 0.07440427522730293]
Printing some Q and Qe and total Qs values:  [[-0.026]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.009]
 [-0.026]
 [-0.026]] [[26.539]
 [26.539]
 [26.539]
 [26.539]
 [29.935]
 [26.539]
 [26.539]] [[0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.478]
 [0.363]
 [0.363]]
maxi score, test score, baseline:  -0.64042 0.5026666666666667 0.5026666666666667
probs:  [0.07440427522730293, 0.5106333132853911, 0.19105635525622755, 0.07440427522730293, 0.07509750577647256, 0.07440427522730293]
maxi score, test score, baseline:  -0.64042 0.5026666666666667 0.5026666666666667
probs:  [0.07437614986925566, 0.510405691957308, 0.1913943025495464, 0.07437614986925566, 0.07507155588537859, 0.07437614986925566]
maxi score, test score, baseline:  -0.64042 0.5026666666666667 0.5026666666666667
probs:  [0.07437614986925566, 0.510405691957308, 0.1913943025495464, 0.07437614986925566, 0.07507155588537859, 0.07437614986925566]
printing an ep nov before normalisation:  37.64995469376977
maxi score, test score, baseline:  -0.64042 0.5026666666666667 0.5026666666666667
probs:  [0.07437614986925566, 0.510405691957308, 0.1913943025495464, 0.07437614986925566, 0.07507155588537859, 0.07437614986925566]
from probs:  [0.07437614986925566, 0.510405691957308, 0.1913943025495464, 0.07437614986925566, 0.07507155588537859, 0.07437614986925566]
maxi score, test score, baseline:  -0.64042 0.5026666666666667 0.5026666666666667
probs:  [0.07439473298085732, 0.5105334138972588, 0.19119207410205152, 0.07439473298085732, 0.07509031305811766, 0.07439473298085732]
maxi score, test score, baseline:  -0.64042 0.5026666666666667 0.5026666666666667
probs:  [0.07439473298085732, 0.5105334138972588, 0.19119207410205152, 0.07439473298085732, 0.07509031305811766, 0.07439473298085732]
printing an ep nov before normalisation:  36.03729843624415
actor:  1 policy actor:  1  step number:  51 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  25.276467669904427
siam score:  -0.8196269
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.63 ]
 [0.582]
 [0.577]
 [0.607]
 [0.605]
 [0.586]] [[37.746]
 [36.989]
 [37.354]
 [37.146]
 [37.028]
 [37.699]
 [37.832]] [[2.068]
 [2.123]
 [2.104]
 [2.082]
 [2.103]
 [2.154]
 [2.145]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.64042 0.5026666666666667 0.5026666666666667
probs:  [0.074338661358134, 0.5100794789624125, 0.1918659483755535, 0.074338661358134, 0.0750385885876322, 0.074338661358134]
Printing some Q and Qe and total Qs values:  [[0.54 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.489]] [[11.582]
 [10.476]
 [10.476]
 [10.476]
 [10.476]
 [10.476]
 [10.892]] [[0.54 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.489]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  73 total reward:  0.03999999999999915  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.63834 0.5026666666666667 0.5026666666666667
probs:  [0.07431066389238049, 0.5098528219267245, 0.19220242467101567, 0.07431066389238049, 0.07501276172511832, 0.07431066389238049]
printing an ep nov before normalisation:  24.91957411406312
maxi score, test score, baseline:  -0.63834 0.5026666666666667 0.5026666666666667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.63834 0.5026666666666667 0.5026666666666667
probs:  [0.07431066389238049, 0.5098528219267245, 0.19220242467101567, 0.07431066389238049, 0.07501276172511832, 0.07431066389238049]
printing an ep nov before normalisation:  27.19270828755112
Printing some Q and Qe and total Qs values:  [[0.898]
 [0.933]
 [0.902]
 [0.898]
 [0.898]
 [0.896]
 [0.897]] [[27.836]
 [25.267]
 [25.647]
 [25.821]
 [26.15 ]
 [26.981]
 [27.225]] [[0.898]
 [0.933]
 [0.902]
 [0.898]
 [0.898]
 [0.896]
 [0.897]]
actor:  1 policy actor:  1  step number:  63 total reward:  0.1866666666666661  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]] [[30.947]
 [30.947]
 [30.947]
 [30.947]
 [30.947]
 [30.947]
 [30.947]] [[1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]]
maxi score, test score, baseline:  -0.63834 0.5026666666666667 0.5026666666666667
probs:  [0.07432932936544519, 0.5099810840740979, 0.19199932396056935, 0.07432932936544519, 0.07503160386899711, 0.07432932936544519]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.399]
 [0.374]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.399]
 [0.374]]
Printing some Q and Qe and total Qs values:  [[0.712]
 [0.74 ]
 [0.712]
 [0.712]
 [0.691]
 [0.712]
 [0.712]] [[25.883]
 [24.996]
 [25.883]
 [25.883]
 [30.778]
 [25.883]
 [25.883]] [[0.712]
 [0.74 ]
 [0.712]
 [0.712]
 [0.691]
 [0.712]
 [0.712]]
printing an ep nov before normalisation:  29.794955253601074
printing an ep nov before normalisation:  43.00060480985722
actions average: 
K:  4  action  0 :  tensor([0.5259, 0.0166, 0.0639, 0.1472, 0.1022, 0.0789, 0.0653],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0295, 0.8108, 0.0229, 0.0405, 0.0293, 0.0151, 0.0518],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1053, 0.0782, 0.2891, 0.1391, 0.0916, 0.1498, 0.1469],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0846, 0.0880, 0.0613, 0.4009, 0.0927, 0.0795, 0.1930],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0910, 0.0668, 0.0424, 0.0816, 0.5667, 0.0751, 0.0764],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1061, 0.0059, 0.0744, 0.1169, 0.1384, 0.4427, 0.1156],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1366, 0.0679, 0.0797, 0.1041, 0.1083, 0.0986, 0.4049],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  34.32031155928796
Printing some Q and Qe and total Qs values:  [[0.957]
 [0.974]
 [0.947]
 [0.953]
 [0.949]
 [0.952]
 [0.955]] [[46.602]
 [41.244]
 [44.468]
 [44.737]
 [46.708]
 [46.533]
 [43.967]] [[0.957]
 [0.974]
 [0.947]
 [0.953]
 [0.949]
 [0.952]
 [0.955]]
printing an ep nov before normalisation:  29.880712032318115
maxi score, test score, baseline:  -0.63834 0.5026666666666667 0.5026666666666667
probs:  [0.07434792474711857, 0.5101088645796542, 0.19179698592092911, 0.07434792474711857, 0.07505037525806091, 0.07434792474711857]
printing an ep nov before normalisation:  34.26777362823486
printing an ep nov before normalisation:  35.24989604949951
printing an ep nov before normalisation:  34.37212203780056
printing an ep nov before normalisation:  41.978244782392366
printing an ep nov before normalisation:  30.91313437232813
actor:  0 policy actor:  0  step number:  27 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  28 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  28 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  28 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  28 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  28 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  29 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  29 total reward:  0.68  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  29 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  29.428368485107512
actor:  0 policy actor:  0  step number:  30 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  30 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  30 total reward:  0.6066666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  30 total reward:  0.6066666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  30 total reward:  0.7133333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  30 total reward:  0.7133333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  30 total reward:  0.7000000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  30 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  31 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  34 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  49.56705461900635
printing an ep nov before normalisation:  40.81708148503218
maxi score, test score, baseline:  -0.5776866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.34641439319133
maxi score, test score, baseline:  -0.5776866666666668 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5776866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.066325265243194
printing an ep nov before normalisation:  38.763826576915164
maxi score, test score, baseline:  -0.5776866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5776866666666668 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5776866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  1.563596612186302e-05
actor:  1 policy actor:  1  step number:  52 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.096]
 [-0.096]
 [-0.102]
 [-0.096]
 [-0.096]
 [-0.096]
 [-0.096]] [[ 0.   ]
 [ 0.   ]
 [29.37 ]
 [33.975]
 [39.849]
 [ 0.   ]
 [ 0.   ]] [[-0.297]
 [-0.297]
 [ 0.041]
 [ 0.101]
 [ 0.169]
 [-0.297]
 [-0.297]]
Printing some Q and Qe and total Qs values:  [[0.647]
 [0.75 ]
 [0.668]
 [0.706]
 [0.696]
 [0.653]
 [0.709]] [[29.086]
 [27.539]
 [29.939]
 [27.206]
 [28.84 ]
 [31.314]
 [28.126]] [[0.647]
 [0.75 ]
 [0.668]
 [0.706]
 [0.696]
 [0.653]
 [0.709]]
maxi score, test score, baseline:  -0.5776866666666668 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5776866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5776866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5776866666666668 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5776866666666668 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5776866666666668 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5776866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5776866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5776866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0010554798157613732
actor:  0 policy actor:  0  step number:  52 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.026]
 [-0.02 ]
 [-0.035]
 [-0.041]
 [-0.044]
 [-0.044]
 [-0.026]] [[15.305]
 [15.951]
 [ 3.864]
 [10.802]
 [16.137]
 [10.45 ]
 [15.594]] [[0.131]
 [0.144]
 [0.005]
 [0.07 ]
 [0.122]
 [0.064]
 [0.135]]
maxi score, test score, baseline:  -0.5750333333333333 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.13007806206554
maxi score, test score, baseline:  -0.5750333333333333 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5750333333333333 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5750333333333333 0.6593333333333334 0.6593333333333334
printing an ep nov before normalisation:  19.25240993499756
printing an ep nov before normalisation:  33.6590142370749
printing an ep nov before normalisation:  28.704474412903185
printing an ep nov before normalisation:  24.16308163805075
Printing some Q and Qe and total Qs values:  [[0.762]
 [0.842]
 [0.762]
 [0.762]
 [0.762]
 [0.762]
 [0.762]] [[26.606]
 [25.445]
 [26.606]
 [26.606]
 [26.606]
 [26.606]
 [26.606]] [[0.762]
 [0.842]
 [0.762]
 [0.762]
 [0.762]
 [0.762]
 [0.762]]
maxi score, test score, baseline:  -0.5750333333333333 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 44.998380612804105
printing an ep nov before normalisation:  37.2184626476466
actor:  0 policy actor:  0  step number:  61 total reward:  0.2666666666666664  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.5725 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  48 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  22.00147673379056
printing an ep nov before normalisation:  49.73185744711759
maxi score, test score, baseline:  -0.5696866666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  36 total reward:  0.5933333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.5696866666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.012774740609984292
printing an ep nov before normalisation:  34.286424028999676
maxi score, test score, baseline:  -0.5696866666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5696866666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5696866666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
Printing some Q and Qe and total Qs values:  [[-0.095]
 [-0.034]
 [-0.057]
 [-0.057]
 [-0.076]
 [-0.057]
 [-0.057]] [[42.997]
 [45.903]
 [40.911]
 [40.911]
 [41.46 ]
 [40.911]
 [40.911]] [[0.288]
 [0.389]
 [0.296]
 [0.296]
 [0.285]
 [0.296]
 [0.296]]
maxi score, test score, baseline:  -0.5696866666666667 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5696866666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5696866666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8294265
actor:  0 policy actor:  0  step number:  65 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5669933333333333 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.79053276937059
printing an ep nov before normalisation:  25.261694804126545
maxi score, test score, baseline:  -0.5669933333333333 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.95100672631719
printing an ep nov before normalisation:  18.670682742383992
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  19.35034840533344
actor:  1 policy actor:  1  step number:  62 total reward:  0.25999999999999945  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5669933333333333 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 17.493702128882305
maxi score, test score, baseline:  -0.5669933333333333 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]] [[36.589]
 [36.589]
 [36.589]
 [36.589]
 [36.589]
 [36.589]
 [36.589]] [[0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.40275415486409
printing an ep nov before normalisation:  33.713730639901186
maxi score, test score, baseline:  -0.5669933333333333 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5669933333333333 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5669933333333333 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.39657377475861
actor:  0 policy actor:  0  step number:  60 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  68 total reward:  0.3399999999999995  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  31.012909492922685
printing an ep nov before normalisation:  23.040428811570912
printing an ep nov before normalisation:  0.006398341895135218
actor:  1 policy actor:  1  step number:  65 total reward:  0.14666666666666595  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
maxi score, test score, baseline:  -0.5614466666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5614466666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.057]
 [-0.057]
 [-0.057]
 [-0.057]
 [-0.057]
 [-0.057]
 [-0.057]] [[44.919]
 [44.919]
 [44.919]
 [44.919]
 [44.919]
 [44.919]
 [44.919]] [[0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]]
printing an ep nov before normalisation:  47.027485385612465
Printing some Q and Qe and total Qs values:  [[-0.039]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.039]] [[36.548]
 [36.548]
 [36.548]
 [36.548]
 [36.548]
 [36.548]
 [36.548]] [[1.458]
 [1.458]
 [1.458]
 [1.458]
 [1.458]
 [1.458]
 [1.458]]
printing an ep nov before normalisation:  24.5890212059021
maxi score, test score, baseline:  -0.5614466666666666 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.446]
 [0.7  ]
 [0.55 ]
 [0.576]
 [0.446]
 [0.539]
 [0.534]] [[24.84 ]
 [23.611]
 [22.488]
 [23.055]
 [24.84 ]
 [23.904]
 [23.01 ]] [[1.049]
 [1.249]
 [1.05 ]
 [1.101]
 [1.049]
 [1.101]
 [1.057]]
actor:  1 policy actor:  1  step number:  63 total reward:  0.15999999999999936  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.5614466666666666 0.6593333333333334 0.6593333333333334
Printing some Q and Qe and total Qs values:  [[-0.07 ]
 [-0.083]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]
 [-0.079]] [[27.687]
 [28.359]
 [27.687]
 [27.687]
 [27.687]
 [27.687]
 [33.815]] [[1.034]
 [1.075]
 [1.034]
 [1.034]
 [1.034]
 [1.034]
 [1.522]]
maxi score, test score, baseline:  -0.5614466666666666 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5614466666666666 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.92455947643004
printing an ep nov before normalisation:  31.467797005360715
maxi score, test score, baseline:  -0.5614466666666666 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 24.520564617036392
maxi score, test score, baseline:  -0.5614466666666666 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5614466666666666 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  38.15848928398435
maxi score, test score, baseline:  -0.5614466666666666 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.5614466666666666 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  15.6760835647583
maxi score, test score, baseline:  -0.5614466666666666 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5614466666666666 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  69 total reward:  0.26666666666666594  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5614466666666666 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5614466666666666 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5614466666666666 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5614466666666666 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.08 ]
 [-0.041]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]] [[33.419]
 [57.438]
 [33.419]
 [33.419]
 [33.419]
 [33.419]
 [33.419]] [[-0.019]
 [ 0.292]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.019]]
maxi score, test score, baseline:  -0.5614466666666666 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.716233253479004
maxi score, test score, baseline:  -0.5614466666666666 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5614466666666666 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  13.080186584503949
printing an ep nov before normalisation:  51.1969384922828
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5614466666666666 0.6593333333333334 0.6593333333333334
actor:  0 policy actor:  0  step number:  60 total reward:  0.23333333333333262  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  46.33538741692878
maxi score, test score, baseline:  -0.55898 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.054]
 [-0.004]
 [-0.054]
 [-0.054]
 [-0.054]
 [-0.054]
 [-0.054]] [[27.854]
 [36.361]
 [27.854]
 [27.854]
 [27.854]
 [27.854]
 [27.854]] [[0.077]
 [0.239]
 [0.077]
 [0.077]
 [0.077]
 [0.077]
 [0.077]]
maxi score, test score, baseline:  -0.55898 0.6593333333333334 0.6593333333333334
printing an ep nov before normalisation:  26.172476590527022
Printing some Q and Qe and total Qs values:  [[0.084]
 [0.172]
 [0.084]
 [0.084]
 [0.084]
 [0.084]
 [0.084]] [[28.887]
 [30.588]
 [28.887]
 [28.887]
 [28.887]
 [28.887]
 [28.887]] [[0.889]
 [1.073]
 [0.889]
 [0.889]
 [0.889]
 [0.889]
 [0.889]]
maxi score, test score, baseline:  -0.55898 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  0.54  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.45 ]
 [0.458]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.392]
 [0.45 ]] [[29.837]
 [39.279]
 [29.837]
 [29.837]
 [29.837]
 [30.77 ]
 [29.837]] [[0.89 ]
 [1.172]
 [0.89 ]
 [0.89 ]
 [0.89 ]
 [0.86 ]
 [0.89 ]]
maxi score, test score, baseline:  -0.55898 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.660897578182865
maxi score, test score, baseline:  -0.55898 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.15539469540718
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.55898 0.6593333333333334 0.6593333333333334
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.693519592285156
actions average: 
K:  2  action  0 :  tensor([0.5487, 0.0295, 0.0613, 0.0770, 0.1446, 0.0688, 0.0702],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0024, 0.9755, 0.0019, 0.0067, 0.0024, 0.0013, 0.0099],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1093, 0.0724, 0.2264, 0.1222, 0.2313, 0.1148, 0.1235],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0911, 0.0123, 0.1177, 0.4087, 0.1290, 0.1227, 0.1185],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1107, 0.0374, 0.0794, 0.0958, 0.5044, 0.0826, 0.0896],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0363, 0.0033, 0.0963, 0.0372, 0.0501, 0.7370, 0.0398],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0898, 0.2652, 0.0667, 0.0928, 0.0917, 0.0521, 0.3418],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  67 total reward:  0.22666666666666613  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  15.425095516459592
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5565266666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5565266666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5565266666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5565266666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  59 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  0  action  0 :  tensor([    0.5857,     0.0004,     0.0743,     0.0604,     0.1247,     0.0785,
            0.0761], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0042, 0.9157, 0.0052, 0.0134, 0.0030, 0.0059, 0.0527],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1097, 0.0027, 0.3275, 0.0929, 0.1188, 0.2352, 0.1133],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1146, 0.0085, 0.1197, 0.2857, 0.1650, 0.1559, 0.1506],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0947, 0.0010, 0.0668, 0.0617, 0.6140, 0.0817, 0.0800],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0767, 0.0007, 0.1145, 0.0594, 0.1052, 0.5620, 0.0816],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0913, 0.1465, 0.0914, 0.0825, 0.0680, 0.0907, 0.4295],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  67 total reward:  0.13333333333333253  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  40.51134154938734
Printing some Q and Qe and total Qs values:  [[0.712]
 [0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]] [[35.546]
 [33.237]
 [33.237]
 [33.237]
 [33.237]
 [33.237]
 [33.237]] [[0.712]
 [0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]]
maxi score, test score, baseline:  -0.5540466666666668 0.6593333333333334 0.6593333333333334
printing an ep nov before normalisation:  23.980444302534977
actor:  1 policy actor:  1  step number:  61 total reward:  0.2799999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5540466666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  10.79997909990255
UNIT TEST: sample policy line 217 mcts : [0.122 0.408 0.224 0.082 0.061 0.041 0.061]
maxi score, test score, baseline:  -0.5540466666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5540466666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.743024535827956
maxi score, test score, baseline:  -0.5540466666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5540466666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  66 total reward:  0.23333333333333262  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.5515800000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  63 total reward:  0.0399999999999987  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.5495 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.99524887369758
maxi score, test score, baseline:  -0.5495 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.239984061439635
printing an ep nov before normalisation:  34.0249093951402
maxi score, test score, baseline:  -0.5495 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5495 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.28402853012085
printing an ep nov before normalisation:  26.747957942384517
maxi score, test score, baseline:  -0.5495 0.6593333333333334 0.6593333333333334
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5495 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.5495 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5495 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
printing an ep nov before normalisation:  30.583651963893175
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5495 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  0.21999999999999942  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  42.66691382773743
siam score:  -0.83386075
maxi score, test score, baseline:  -0.5495 0.6593333333333334 0.6593333333333334
printing an ep nov before normalisation:  30.98762971410677
printing an ep nov before normalisation:  34.00738477706909
printing an ep nov before normalisation:  29.693351711121846
UNIT TEST: sample policy line 217 mcts : [0.02  0.633 0.041 0.061 0.041 0.163 0.041]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5495 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5495 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.40219190267597094
maxi score, test score, baseline:  -0.5495 0.6593333333333334 0.6593333333333334
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  66 total reward:  0.19333333333333247  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  72 total reward:  0.15333333333333288  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.5495 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.19328139380012
printing an ep nov before normalisation:  27.804449627830277
maxi score, test score, baseline:  -0.5495 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.724048381086313
siam score:  -0.8345524
maxi score, test score, baseline:  -0.5495 0.6593333333333334 0.6593333333333334
actor:  1 policy actor:  1  step number:  40 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5495 0.6593333333333334 0.6593333333333334
printing an ep nov before normalisation:  39.12539142339201
maxi score, test score, baseline:  -0.5495 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.629]
 [0.64 ]
 [0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.629]
 [0.64 ]
 [0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]]
maxi score, test score, baseline:  -0.5495 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5495 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.224 0.204 0.02  0.224 0.224 0.041 0.061]
actor:  1 policy actor:  1  step number:  71 total reward:  0.10666666666666602  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.843]
 [0.914]
 [0.851]
 [0.845]
 [0.843]
 [0.854]
 [0.843]] [[32.872]
 [29.463]
 [32.22 ]
 [33.574]
 [32.872]
 [34.904]
 [32.872]] [[0.843]
 [0.914]
 [0.851]
 [0.845]
 [0.843]
 [0.854]
 [0.843]]
maxi score, test score, baseline:  -0.5495 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.5495000000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.911]
 [0.942]
 [0.873]
 [0.915]
 [0.911]
 [0.911]
 [0.909]] [[34.95 ]
 [32.277]
 [34.796]
 [31.657]
 [34.95 ]
 [34.95 ]
 [29.312]] [[0.911]
 [0.942]
 [0.873]
 [0.915]
 [0.911]
 [0.911]
 [0.909]]
maxi score, test score, baseline:  -0.5495 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.590742387117416
maxi score, test score, baseline:  -0.5495 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  59 total reward:  0.23999999999999966  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.072]
 [-0.072]
 [-0.072]
 [-0.072]
 [-0.072]
 [-0.072]
 [-0.072]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.072]
 [-0.072]
 [-0.072]
 [-0.072]
 [-0.072]
 [-0.072]
 [-0.072]]
Printing some Q and Qe and total Qs values:  [[0.302]
 [0.171]
 [0.215]
 [0.157]
 [0.157]
 [0.195]
 [0.157]] [[32.442]
 [29.048]
 [30.858]
 [25.706]
 [25.706]
 [28.194]
 [25.706]] [[1.361]
 [1.019]
 [1.175]
 [0.797]
 [0.797]
 [0.99 ]
 [0.797]]
printing an ep nov before normalisation:  46.607591204591486
maxi score, test score, baseline:  -0.5470200000000002 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
siam score:  -0.834446
printing an ep nov before normalisation:  39.29428602800147
printing an ep nov before normalisation:  25.880231857299805
maxi score, test score, baseline:  -0.5470200000000002 0.6593333333333334 0.6593333333333334
Printing some Q and Qe and total Qs values:  [[-0.116]
 [-0.123]
 [-0.12 ]
 [-0.113]
 [-0.112]
 [-0.111]
 [-0.101]] [[14.19 ]
 [21.426]
 [12.992]
 [12.633]
 [12.925]
 [13.027]
 [18.638]] [[0.244]
 [0.422]
 [0.21 ]
 [0.208]
 [0.216]
 [0.221]
 [0.372]]
printing an ep nov before normalisation:  12.401879819222858
actor:  0 policy actor:  0  step number:  59 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.5444333333333334 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5444333333333334 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5444333333333334 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.156689947980546
printing an ep nov before normalisation:  31.742191619289567
printing an ep nov before normalisation:  34.288320541381836
printing an ep nov before normalisation:  40.85641953725602
siam score:  -0.8315383
printing an ep nov before normalisation:  32.89082050323486
maxi score, test score, baseline:  -0.5444333333333334 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.2799999999999997  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  39.619098432043266
maxi score, test score, baseline:  -0.5444333333333334 0.6593333333333334 0.6593333333333334
printing an ep nov before normalisation:  55.424789185139645
maxi score, test score, baseline:  -0.5444333333333334 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
actor:  1 policy actor:  1  step number:  73 total reward:  0.03999999999999926  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5444333333333334 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.344]
 [0.37 ]
 [0.344]
 [0.344]
 [0.309]
 [0.306]
 [0.3  ]] [[44.126]
 [39.235]
 [44.126]
 [44.126]
 [44.927]
 [46.349]
 [44.055]] [[1.823]
 [1.574]
 [1.823]
 [1.823]
 [1.832]
 [1.91 ]
 [1.775]]
printing an ep nov before normalisation:  38.07436620110877
printing an ep nov before normalisation:  39.15440251000449
maxi score, test score, baseline:  -0.5444333333333334 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5444333333333334 0.6593333333333334 0.6593333333333334
actor:  1 policy actor:  1  step number:  61 total reward:  0.30666666666666653  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  0  action  0 :  tensor([0.5899, 0.0071, 0.0534, 0.0709, 0.1121, 0.0775, 0.0889],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0034, 0.9547, 0.0039, 0.0058, 0.0014, 0.0029, 0.0278],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1164, 0.0058, 0.2192, 0.1260, 0.1459, 0.2106, 0.1761],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1057, 0.0525, 0.0938, 0.3356, 0.1439, 0.1431, 0.1253],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0858, 0.0308, 0.0619, 0.0637, 0.5797, 0.1001, 0.0779],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1013, 0.0122, 0.1819, 0.1004, 0.1405, 0.3299, 0.1338],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0946, 0.1567, 0.0751, 0.1021, 0.1062, 0.1109, 0.3543],
       grad_fn=<DivBackward0>)
siam score:  -0.8279598
Printing some Q and Qe and total Qs values:  [[-0.027]
 [-0.001]
 [-0.088]
 [-0.044]
 [-0.059]
 [-0.006]
 [-0.059]] [[55.512]
 [53.426]
 [45.883]
 [42.405]
 [43.801]
 [51.473]
 [53.912]] [[1.143]
 [1.083]
 [0.687]
 [0.588]
 [0.629]
 [0.999]
 [1.046]]
maxi score, test score, baseline:  -0.5444333333333334 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  52 total reward:  0.24666666666666637  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.74235280347498
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.07739315521409
printing an ep nov before normalisation:  38.222812129380145
Printing some Q and Qe and total Qs values:  [[0.366]
 [0.51 ]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]] [[29.058]
 [33.715]
 [29.058]
 [29.058]
 [29.058]
 [29.058]
 [29.058]] [[0.77 ]
 [1.038]
 [0.77 ]
 [0.77 ]
 [0.77 ]
 [0.77 ]
 [0.77 ]]
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  42 total reward:  0.5666666666666668  reward:  1.0 rdn_beta:  1.0
line 256 mcts: sample exp_bonus 0.0
UNIT TEST: sample policy line 217 mcts : [0.041 0.265 0.122 0.245 0.122 0.061 0.143]
printing an ep nov before normalisation:  34.00377639087234
using explorer policy with actor:  1
printing an ep nov before normalisation:  54.060932084604545
actor:  1 policy actor:  1  step number:  52 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
printing an ep nov before normalisation:  9.439028933387034
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
printing an ep nov before normalisation:  42.255785692728935
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.39070167352183
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.98239040374756
actor:  1 policy actor:  1  step number:  57 total reward:  0.3999999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.58762233425305
Printing some Q and Qe and total Qs values:  [[-0.059]
 [-0.04 ]
 [-0.059]
 [-0.059]
 [-0.059]
 [-0.059]
 [-0.059]] [[42.503]
 [47.483]
 [42.503]
 [42.503]
 [42.503]
 [42.503]
 [42.503]] [[0.321]
 [0.459]
 [0.321]
 [0.321]
 [0.321]
 [0.321]
 [0.321]]
siam score:  -0.8212785
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([0.5270, 0.0101, 0.0750, 0.0769, 0.1379, 0.0860, 0.0872],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0019, 0.9317, 0.0058, 0.0168, 0.0013, 0.0017, 0.0408],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0986, 0.0026, 0.1535, 0.1947, 0.1680, 0.2452, 0.1374],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1252, 0.0007, 0.1517, 0.1642, 0.1406, 0.2230, 0.1945],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0757, 0.0072, 0.0623, 0.0894, 0.5899, 0.1063, 0.0692],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1382, 0.0023, 0.1245, 0.1162, 0.1073, 0.3551, 0.1564],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0892, 0.0502, 0.1161, 0.1013, 0.0787, 0.1798, 0.3847],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.027201437778494
Printing some Q and Qe and total Qs values:  [[0.25 ]
 [0.247]
 [0.25 ]
 [0.25 ]
 [0.222]
 [0.25 ]
 [0.25 ]] [[52.66 ]
 [50.414]
 [52.66 ]
 [52.66 ]
 [44.604]
 [52.66 ]
 [52.66 ]] [[1.161]
 [1.096]
 [1.161]
 [1.161]
 [0.912]
 [1.161]
 [1.161]]
printing an ep nov before normalisation:  35.25954255956377
actor:  1 policy actor:  1  step number:  42 total reward:  0.5666666666666667  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  30.478129506190744
printing an ep nov before normalisation:  47.37631987753792
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.20839341315696
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.573383654519155
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.078]
 [-0.074]
 [-0.071]
 [-0.071]
 [-0.067]
 [-0.071]
 [-0.068]] [[34.151]
 [33.268]
 [36.019]
 [36.238]
 [33.455]
 [35.403]
 [34.81 ]] [[0.825]
 [0.785]
 [0.926]
 [0.937]
 [0.801]
 [0.895]
 [0.868]]
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
actions average: 
K:  2  action  0 :  tensor([0.5063, 0.0279, 0.0610, 0.0876, 0.0924, 0.1062, 0.1185],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0053, 0.9147, 0.0069, 0.0221, 0.0041, 0.0045, 0.0424],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0761, 0.0205, 0.3881, 0.1182, 0.0943, 0.2080, 0.0948],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1090, 0.0661, 0.1135, 0.2738, 0.1194, 0.1341, 0.1842],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0818, 0.0083, 0.0683, 0.1022, 0.5682, 0.0820, 0.0893],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0830, 0.0087, 0.1048, 0.1045, 0.1043, 0.4618, 0.1328],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1259, 0.1398, 0.0945, 0.1108, 0.1125, 0.1461, 0.2703],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  31.647798946215012
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.65881736339236
printing an ep nov before normalisation:  37.485221586914896
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
actor:  1 policy actor:  1  step number:  67 total reward:  0.21333333333333304  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.69678522702858
actions average: 
K:  1  action  0 :  tensor([0.4314, 0.0651, 0.0866, 0.0948, 0.0979, 0.1147, 0.1094],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0082, 0.9370, 0.0035, 0.0052, 0.0064, 0.0042, 0.0355],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0978, 0.0080, 0.3394, 0.1301, 0.1007, 0.1745, 0.1494],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0636, 0.1900, 0.0753, 0.3212, 0.0892, 0.1189, 0.1419],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0787, 0.0015, 0.0693, 0.0703, 0.6179, 0.0903, 0.0720],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0398, 0.0157, 0.0814, 0.0482, 0.0482, 0.7053, 0.0614],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1247, 0.0045, 0.1202, 0.1361, 0.1713, 0.1653, 0.2779],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  0.0004916405714538996
printing an ep nov before normalisation:  35.23830888334848
siam score:  -0.829725
actor:  1 policy actor:  1  step number:  82 total reward:  0.0066666666666660435  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
Printing some Q and Qe and total Qs values:  [[-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]] [[44.075]
 [44.075]
 [44.075]
 [44.075]
 [44.075]
 [44.075]
 [44.075]] [[1.315]
 [1.315]
 [1.315]
 [1.315]
 [1.315]
 [1.315]
 [1.315]]
line 256 mcts: sample exp_bonus 40.147112814824816
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8268097
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5419400000000001 0.6593333333333334 0.6593333333333334
actor:  0 policy actor:  0  step number:  65 total reward:  0.026666666666666172  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.133]
 [-0.093]
 [-0.094]
 [-0.088]
 [-0.113]
 [-0.088]
 [-0.099]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.133]
 [-0.093]
 [-0.094]
 [-0.088]
 [-0.113]
 [-0.088]
 [-0.099]]
actor:  1 policy actor:  1  step number:  64 total reward:  0.153333333333333  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5398866666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.069]
 [-0.066]
 [-0.075]
 [-0.075]
 [-0.069]
 [-0.075]
 [-0.075]] [[29.083]
 [28.935]
 [26.587]
 [26.587]
 [29.546]
 [26.587]
 [26.587]] [[0.202]
 [0.203]
 [0.153]
 [0.153]
 [0.211]
 [0.153]
 [0.153]]
maxi score, test score, baseline:  -0.5398866666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5398866666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5398866666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.69446485036572
actions average: 
K:  3  action  0 :  tensor([0.4473, 0.0071, 0.0941, 0.0785, 0.1731, 0.0880, 0.1119],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0074, 0.9301, 0.0079, 0.0157, 0.0089, 0.0109, 0.0191],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1082, 0.0391, 0.4527, 0.0729, 0.0906, 0.1342, 0.1023],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0488, 0.1726, 0.0532, 0.4791, 0.0623, 0.0707, 0.1135],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0740, 0.0690, 0.0689, 0.0741, 0.5631, 0.0859, 0.0649],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0967, 0.0208, 0.1645, 0.1224, 0.1564, 0.3238, 0.1156],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1142, 0.0780, 0.0751, 0.0940, 0.1495, 0.0847, 0.4046],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.399]
 [0.372]
 [0.372]
 [0.361]
 [0.387]
 [0.364]] [[41.163]
 [41.602]
 [40.611]
 [43.016]
 [43.906]
 [39.116]
 [39.969]] [[1.696]
 [1.763]
 [1.684]
 [1.809]
 [1.845]
 [1.621]
 [1.642]]
siam score:  -0.82234216
maxi score, test score, baseline:  -0.5398866666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5398866666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  47 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.5398866666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5398866666666667 0.6593333333333334 0.6593333333333334
printing an ep nov before normalisation:  34.8170287659826
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.079506186673807
maxi score, test score, baseline:  -0.5398866666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5398866666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.3271661009079
Printing some Q and Qe and total Qs values:  [[-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [ 0.079]
 [-0.002]
 [-0.002]] [[29.041]
 [29.041]
 [29.041]
 [29.041]
 [31.743]
 [29.041]
 [29.041]] [[1.135]
 [1.135]
 [1.135]
 [1.135]
 [1.423]
 [1.135]
 [1.135]]
maxi score, test score, baseline:  -0.5398866666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5398866666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.752]
 [0.768]
 [0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]] [[34.056]
 [33.865]
 [34.056]
 [34.056]
 [34.056]
 [34.056]
 [34.056]] [[1.037]
 [1.05 ]
 [1.037]
 [1.037]
 [1.037]
 [1.037]
 [1.037]]
maxi score, test score, baseline:  -0.5398866666666667 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5398866666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  65 total reward:  0.1466666666666656  reward:  1.0 rdn_beta:  0.333
siam score:  -0.82332015
maxi score, test score, baseline:  -0.5398866666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  45 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  67 total reward:  0.1733333333333329  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.5369266666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.08 ]
 [0.138]
 [0.08 ]
 [0.08 ]
 [0.08 ]
 [0.08 ]
 [0.044]] [[42.658]
 [41.553]
 [42.658]
 [42.658]
 [42.658]
 [42.658]
 [41.935]] [[1.054]
 [1.067]
 [1.054]
 [1.054]
 [1.054]
 [1.054]
 [0.989]]
siam score:  -0.8217522
maxi score, test score, baseline:  -0.5369266666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5369266666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.5975, 0.0208, 0.0787, 0.0436, 0.0622, 0.1156, 0.0815],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0193, 0.8618, 0.0163, 0.0232, 0.0223, 0.0144, 0.0425],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1219, 0.0116, 0.2457, 0.0976, 0.1379, 0.2327, 0.1525],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0693, 0.0383, 0.0833, 0.4325, 0.0971, 0.1015, 0.1779],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0992, 0.0034, 0.0703, 0.0501, 0.5784, 0.0824, 0.1162],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0887, 0.0086, 0.1666, 0.0833, 0.1189, 0.4433, 0.0906],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1001, 0.0222, 0.1523, 0.0867, 0.1063, 0.1040, 0.4284],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.5369266666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5369266666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.81542295
maxi score, test score, baseline:  -0.5369266666666668 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5369266666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.474]
 [0.463]
 [0.379]
 [0.481]
 [0.495]
 [0.492]] [[24.431]
 [23.82 ]
 [24.033]
 [24.246]
 [23.553]
 [23.991]
 [23.781]] [[2.023]
 [1.94 ]
 [1.955]
 [1.897]
 [1.913]
 [1.981]
 [1.953]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
actor:  0 policy actor:  0  step number:  72 total reward:  0.019999999999998797  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.5348866666666668 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5348866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.629]
 [0.728]
 [0.595]
 [0.629]
 [0.629]
 [0.627]
 [0.619]] [[11.156]
 [13.376]
 [10.961]
 [11.156]
 [11.156]
 [10.987]
 [10.715]] [[1.865]
 [2.21 ]
 [1.81 ]
 [1.865]
 [1.865]
 [1.845]
 [1.806]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  62 total reward:  0.259999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.5348866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.992027820891803
maxi score, test score, baseline:  -0.5348866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.589414217941798
printing an ep nov before normalisation:  21.46499488903089
actor:  1 policy actor:  1  step number:  70 total reward:  0.17999999999999927  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  17.319714725856
maxi score, test score, baseline:  -0.5348866666666668 0.6593333333333334 0.6593333333333334
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.392]
 [0.376]
 [0.361]
 [0.364]
 [0.361]
 [0.381]] [[31.611]
 [37.697]
 [27.829]
 [30.423]
 [33.18 ]
 [30.338]
 [32.27 ]] [[0.941]
 [1.216]
 [0.847]
 [0.925]
 [1.026]
 [0.921]
 [1.01 ]]
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.484]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]] [[44.138]
 [44.737]
 [44.138]
 [44.138]
 [44.138]
 [44.138]
 [44.138]] [[1.384]
 [1.47 ]
 [1.384]
 [1.384]
 [1.384]
 [1.384]
 [1.384]]
maxi score, test score, baseline:  -0.5348866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5348866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.654714584350586
maxi score, test score, baseline:  -0.5348866666666668 0.6593333333333334 0.6593333333333334
actor:  1 policy actor:  1  step number:  55 total reward:  0.26666666666666616  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.5348866666666668 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5348866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5348866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]] [[37.998]
 [37.998]
 [37.998]
 [37.998]
 [37.998]
 [37.998]
 [37.998]] [[1.622]
 [1.622]
 [1.622]
 [1.622]
 [1.622]
 [1.622]
 [1.622]]
Printing some Q and Qe and total Qs values:  [[0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.238]
 [0.258]
 [0.258]] [[37.77 ]
 [37.77 ]
 [37.77 ]
 [37.77 ]
 [44.818]
 [37.77 ]
 [37.77 ]] [[1.332]
 [1.332]
 [1.332]
 [1.332]
 [1.702]
 [1.332]
 [1.332]]
maxi score, test score, baseline:  -0.5348866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5348866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.277]
 [0.293]
 [0.277]
 [0.277]
 [0.15 ]
 [0.277]
 [0.277]] [[43.407]
 [40.943]
 [43.407]
 [43.407]
 [39.534]
 [43.407]
 [43.407]] [[1.805]
 [1.644]
 [1.805]
 [1.805]
 [1.4  ]
 [1.805]
 [1.805]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 42.69370234007467
printing an ep nov before normalisation:  35.72852373123169
Printing some Q and Qe and total Qs values:  [[-0.092]
 [-0.09 ]
 [-0.092]
 [-0.093]
 [-0.092]
 [-0.092]
 [-0.092]] [[28.636]
 [33.308]
 [28.636]
 [ 9.81 ]
 [28.636]
 [28.636]
 [28.636]] [[0.744]
 [0.91 ]
 [0.744]
 [0.083]
 [0.744]
 [0.744]
 [0.744]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.629]
 [0.699]
 [0.629]
 [0.629]
 [0.555]
 [0.629]
 [0.584]] [[22.957]
 [32.05 ]
 [22.957]
 [22.957]
 [23.986]
 [22.957]
 [25.316]] [[1.37 ]
 [2.114]
 [1.37 ]
 [1.37 ]
 [1.372]
 [1.37 ]
 [1.5  ]]
maxi score, test score, baseline:  -0.5348866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5348866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.29333333333333256  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5348866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5348866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.57400621355686
printing an ep nov before normalisation:  34.41963092265605
siam score:  -0.8160492
actor:  1 policy actor:  1  step number:  64 total reward:  0.11333333333333273  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  41.96219444274902
Printing some Q and Qe and total Qs values:  [[-0.098]
 [-0.081]
 [-0.107]
 [-0.102]
 [-0.086]
 [-0.102]
 [-0.081]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.098]
 [-0.081]
 [-0.107]
 [-0.102]
 [-0.086]
 [-0.102]
 [-0.081]]
Printing some Q and Qe and total Qs values:  [[-0.062]
 [-0.041]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.062]
 [-0.061]] [[17.081]
 [21.908]
 [17.553]
 [17.552]
 [17.238]
 [17.822]
 [17.483]] [[0.699]
 [1.174]
 [0.744]
 [0.744]
 [0.715]
 [0.769]
 [0.738]]
actor:  1 policy actor:  1  step number:  69 total reward:  0.17333333333333267  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  53.602371135258025
maxi score, test score, baseline:  -0.5348866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5348866666666668 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5348866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.107]
 [-0.1  ]
 [-0.104]
 [-0.106]
 [-0.106]
 [-0.103]
 [-0.105]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.107]
 [-0.1  ]
 [-0.104]
 [-0.106]
 [-0.106]
 [-0.103]
 [-0.105]]
printing an ep nov before normalisation:  24.675694525432114
maxi score, test score, baseline:  -0.5348866666666668 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5348866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5348866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.11947287189729
printing an ep nov before normalisation:  36.05401039123535
Printing some Q and Qe and total Qs values:  [[-0.039]
 [ 0.053]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.039]] [[32.348]
 [40.048]
 [32.348]
 [32.348]
 [32.348]
 [32.348]
 [32.348]] [[0.327]
 [0.58 ]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]]
maxi score, test score, baseline:  -0.5348866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5348866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5348866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5348866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5348866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  74 total reward:  0.27333333333333276  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.5348866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.9957094595562
Printing some Q and Qe and total Qs values:  [[0.319]
 [0.358]
 [0.308]
 [0.324]
 [0.302]
 [0.32 ]
 [0.331]] [[26.118]
 [33.413]
 [25.892]
 [26.09 ]
 [26.834]
 [25.393]
 [24.878]] [[0.784]
 [1.101]
 [0.764]
 [0.788]
 [0.795]
 [0.758]
 [0.749]]
maxi score, test score, baseline:  -0.5348866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.567]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]] [[34.434]
 [38.89 ]
 [34.434]
 [34.434]
 [34.434]
 [34.434]
 [34.434]] [[1.136]
 [1.379]
 [1.136]
 [1.136]
 [1.136]
 [1.136]
 [1.136]]
maxi score, test score, baseline:  -0.5348866666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.705998343121415
actor:  0 policy actor:  0  step number:  53 total reward:  0.2799999999999997  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5323266666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5323266666666667 0.6593333333333334 0.6593333333333334
printing an ep nov before normalisation:  38.323450562207505
maxi score, test score, baseline:  -0.5323266666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.46106362755001
printing an ep nov before normalisation:  38.46535682678223
actor:  1 policy actor:  1  step number:  50 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.5323266666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5323266666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.161374422398396
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
actor:  1 policy actor:  1  step number:  54 total reward:  0.42000000000000004  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.5323266666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.400915069286846
maxi score, test score, baseline:  -0.5323266666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  62 total reward:  0.09999999999999942  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.5301266666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.063]
 [-0.059]
 [-0.071]
 [-0.058]
 [-0.063]
 [-0.063]
 [-0.062]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.063]
 [-0.059]
 [-0.071]
 [-0.058]
 [-0.063]
 [-0.063]
 [-0.062]]
maxi score, test score, baseline:  -0.5301266666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5301266666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.32106157640561
actor:  1 policy actor:  1  step number:  55 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5301266666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.507753919458537
actor:  1 policy actor:  1  step number:  67 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5301266666666667 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5301266666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.92903207846988
printing an ep nov before normalisation:  29.079597993773504
maxi score, test score, baseline:  -0.5301266666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.81630874
printing an ep nov before normalisation:  32.475428912129765
actor:  0 policy actor:  0  step number:  65 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5276466666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.173333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.5276466666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5276466666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.965500831604004
maxi score, test score, baseline:  -0.5276466666666668 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.86912573027094
Printing some Q and Qe and total Qs values:  [[-0.086]
 [-0.079]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.084]
 [-0.083]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.086]
 [-0.079]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.084]
 [-0.083]]
maxi score, test score, baseline:  -0.5276466666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5276466666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5276466666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5276466666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  0.3333333333333326  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  34.134238519010104
maxi score, test score, baseline:  -0.5276466666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.65061534444441
printing an ep nov before normalisation:  39.714095255391456
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.63683827333315
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]] [[28.016]
 [28.016]
 [28.016]
 [28.016]
 [28.016]
 [28.016]
 [28.016]] [[1.593]
 [1.593]
 [1.593]
 [1.593]
 [1.593]
 [1.593]
 [1.593]]
actor:  1 policy actor:  1  step number:  56 total reward:  0.23333333333333295  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.353298677712786
printing an ep nov before normalisation:  0.007377822799838896
actor:  0 policy actor:  0  step number:  69 total reward:  0.17333333333333256  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.075]
 [-0.074]
 [-0.073]
 [-0.074]
 [-0.078]
 [-0.076]
 [-0.077]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.075]
 [-0.074]
 [-0.073]
 [-0.074]
 [-0.078]
 [-0.076]
 [-0.077]]
maxi score, test score, baseline:  -0.5253000000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.36431488929354
maxi score, test score, baseline:  -0.5253000000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.457494795475743
maxi score, test score, baseline:  -0.5253000000000001 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5253000000000001 0.6593333333333334 0.6593333333333334
printing an ep nov before normalisation:  37.92700498539016
maxi score, test score, baseline:  -0.5253000000000001 0.6593333333333334 0.6593333333333334
actions average: 
K:  3  action  0 :  tensor([0.3925, 0.0864, 0.0719, 0.0896, 0.1302, 0.0930, 0.1363],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0117, 0.8752, 0.0059, 0.0282, 0.0254, 0.0162, 0.0375],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0926, 0.0117, 0.2359, 0.1508, 0.1053, 0.2713, 0.1323],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0901, 0.0267, 0.1098, 0.2551, 0.1623, 0.2206, 0.1354],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0820, 0.0380, 0.0577, 0.1118, 0.4581, 0.1203, 0.1321],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0548, 0.0418, 0.0575, 0.0787, 0.0831, 0.5969, 0.0873],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0920, 0.0044, 0.0839, 0.1562, 0.1074, 0.1246, 0.4316],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.5253000000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  0.10666666666666613  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8115628
maxi score, test score, baseline:  -0.5253000000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.122 0.143 0.204 0.163 0.143 0.102 0.122]
printing an ep nov before normalisation:  26.6735833919779
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.25999999999999934  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  31.071606378923793
maxi score, test score, baseline:  -0.5253000000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.158354121116712
maxi score, test score, baseline:  -0.5253000000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5253000000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.65381186736296
printing an ep nov before normalisation:  41.124711269977794
Printing some Q and Qe and total Qs values:  [[0.866]
 [0.93 ]
 [0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]] [[32.73 ]
 [29.189]
 [32.73 ]
 [32.73 ]
 [32.73 ]
 [32.73 ]
 [32.73 ]] [[0.866]
 [0.93 ]
 [0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]]
printing an ep nov before normalisation:  24.238942604508722
siam score:  -0.8084175
maxi score, test score, baseline:  -0.5253000000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5253000000000001 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5253000000000001 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5253000000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  14.991638660430908
actor:  0 policy actor:  0  step number:  67 total reward:  0.27999999999999936  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  58 total reward:  0.3666666666666666  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.03 ]
 [-0.036]
 [-0.03 ]
 [-0.049]
 [-0.03 ]
 [-0.046]
 [-0.048]] [[44.697]
 [47.091]
 [44.697]
 [51.228]
 [44.697]
 [47.359]
 [46.782]] [[1.248]
 [1.387]
 [1.248]
 [1.624]
 [1.248]
 [1.393]
 [1.356]]
maxi score, test score, baseline:  -0.5227400000000002 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5227400000000002 0.6593333333333334 0.6593333333333334
printing an ep nov before normalisation:  37.5347638130188
actor:  1 policy actor:  1  step number:  60 total reward:  0.12666666666666604  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.5227400000000002 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5227400000000002 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.531766233732707
maxi score, test score, baseline:  -0.5227400000000002 0.6593333333333334 0.6593333333333334
line 256 mcts: sample exp_bonus 31.54518604278564
actor:  1 policy actor:  1  step number:  56 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.958]
 [0.968]
 [0.958]
 [0.958]
 [0.958]
 [0.958]
 [0.958]] [[41.947]
 [36.545]
 [41.947]
 [41.947]
 [41.947]
 [41.947]
 [41.947]] [[0.958]
 [0.968]
 [0.958]
 [0.958]
 [0.958]
 [0.958]
 [0.958]]
printing an ep nov before normalisation:  27.91698695082288
printing an ep nov before normalisation:  35.80899238586426
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  54 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5202200000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.86374028822819
printing an ep nov before normalisation:  27.04771451793275
printing an ep nov before normalisation:  33.9751672744751
maxi score, test score, baseline:  -0.5226066666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.317997932434082
printing an ep nov before normalisation:  29.49990749359131
printing an ep nov before normalisation:  25.136300038662185
actor:  0 policy actor:  0  step number:  64 total reward:  0.23333333333333273  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  52 total reward:  0.35333333333333317  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.106]] [[49.369]
 [49.369]
 [49.369]
 [49.369]
 [49.369]
 [49.369]
 [49.369]] [[1.106]
 [1.106]
 [1.106]
 [1.106]
 [1.106]
 [1.106]
 [1.106]]
maxi score, test score, baseline:  -0.5201400000000002 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  35.15934106742168
line 256 mcts: sample exp_bonus 41.756689542495266
maxi score, test score, baseline:  -0.5201400000000002 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.75249145309542
maxi score, test score, baseline:  -0.5201400000000002 0.6593333333333334 0.6593333333333334
Printing some Q and Qe and total Qs values:  [[0.829]
 [0.857]
 [0.817]
 [0.83 ]
 [0.839]
 [0.825]
 [0.814]] [[23.501]
 [24.095]
 [22.614]
 [24.139]
 [23.467]
 [24.429]
 [23.685]] [[0.829]
 [0.857]
 [0.817]
 [0.83 ]
 [0.839]
 [0.825]
 [0.814]]
Printing some Q and Qe and total Qs values:  [[0.818]
 [0.831]
 [0.82 ]
 [0.837]
 [0.829]
 [0.82 ]
 [0.821]] [[24.789]
 [24.039]
 [22.584]
 [23.553]
 [24.035]
 [24.643]
 [23.873]] [[0.818]
 [0.831]
 [0.82 ]
 [0.837]
 [0.829]
 [0.82 ]
 [0.821]]
maxi score, test score, baseline:  -0.5201400000000002 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  22.068300966999665
printing an ep nov before normalisation:  26.05033294390174
printing an ep nov before normalisation:  27.728689807123967
maxi score, test score, baseline:  -0.5201400000000002 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5201400000000002 0.6593333333333334 0.6593333333333334
Printing some Q and Qe and total Qs values:  [[0.648]
 [0.678]
 [0.648]
 [0.678]
 [0.678]
 [0.648]
 [0.648]] [[32.916]
 [30.897]
 [32.916]
 [29.704]
 [29.934]
 [32.916]
 [32.916]] [[0.648]
 [0.678]
 [0.648]
 [0.678]
 [0.678]
 [0.648]
 [0.648]]
actor:  0 policy actor:  0  step number:  52 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5172733333333335 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5172733333333335 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.50339596082163
maxi score, test score, baseline:  -0.5172733333333335 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5172733333333335 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5172733333333335 0.6593333333333334 0.6593333333333334
Printing some Q and Qe and total Qs values:  [[-0.082]
 [-0.036]
 [-0.082]
 [-0.082]
 [-0.088]
 [-0.082]
 [-0.082]] [[40.143]
 [42.006]
 [40.143]
 [40.143]
 [35.528]
 [40.143]
 [40.143]] [[0.49 ]
 [0.589]
 [0.49 ]
 [0.49 ]
 [0.356]
 [0.49 ]
 [0.49 ]]
maxi score, test score, baseline:  -0.5172733333333335 0.6593333333333334 0.6593333333333334
actor:  1 policy actor:  1  step number:  50 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5172733333333335 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5172733333333335 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5172733333333335 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5172733333333335 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  61 total reward:  0.42666666666666686  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5144200000000001 0.6593333333333334 0.6593333333333334
printing an ep nov before normalisation:  5.806634817417944e-05
maxi score, test score, baseline:  -0.5144200000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5144200000000001 0.6593333333333334 0.6593333333333334
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.433]
 [0.313]
 [0.313]
 [0.324]
 [0.313]
 [0.313]] [[27.065]
 [30.879]
 [27.065]
 [27.065]
 [28.58 ]
 [27.065]
 [27.065]] [[1.186]
 [1.545]
 [1.186]
 [1.186]
 [1.291]
 [1.186]
 [1.186]]
printing an ep nov before normalisation:  29.078719110749063
printing an ep nov before normalisation:  24.11261796951294
maxi score, test score, baseline:  -0.5144200000000001 0.6593333333333334 0.6593333333333334
Printing some Q and Qe and total Qs values:  [[0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]] [[34.462]
 [34.462]
 [34.462]
 [34.462]
 [34.462]
 [34.462]
 [34.462]] [[2.298]
 [2.298]
 [2.298]
 [2.298]
 [2.298]
 [2.298]
 [2.298]]
maxi score, test score, baseline:  -0.5144200000000001 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5144200000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5144200000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  77 total reward:  0.053333333333332456  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11563
actor:  0 policy actor:  0  step number:  54 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.5115533333333334 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.838005564757395
maxi score, test score, baseline:  -0.5115533333333334 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5115533333333334 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5115533333333334 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5115533333333334 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5115533333333334 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5115533333333334 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5115533333333334 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5115533333333334 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.082]
 [-0.082]
 [-0.082]
 [-0.082]
 [-0.073]
 [-0.082]
 [-0.082]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.082]
 [-0.082]
 [-0.082]
 [-0.082]
 [-0.073]
 [-0.082]
 [-0.082]]
printing an ep nov before normalisation:  37.635166386603075
actor:  0 policy actor:  0  step number:  54 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.99989337774084
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.905]
 [0.953]
 [0.905]
 [0.905]
 [0.905]
 [0.905]
 [0.905]] [[37.701]
 [37.687]
 [37.701]
 [37.701]
 [37.701]
 [37.701]
 [37.701]] [[0.905]
 [0.953]
 [0.905]
 [0.905]
 [0.905]
 [0.905]
 [0.905]]
maxi score, test score, baseline:  -0.5091933333333334 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5091933333333334 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5091933333333334 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.83281433062155
maxi score, test score, baseline:  -0.5091933333333334 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.5091933333333334 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.734689885162652
maxi score, test score, baseline:  -0.5091933333333334 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  54 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]] [[30.125]
 [30.125]
 [30.125]
 [30.125]
 [30.125]
 [30.125]
 [30.125]] [[0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]]
printing an ep nov before normalisation:  22.22363478789901
Printing some Q and Qe and total Qs values:  [[0.845]
 [0.891]
 [0.849]
 [0.835]
 [0.849]
 [0.82 ]
 [0.829]] [[17.682]
 [22.655]
 [15.466]
 [15.821]
 [17.177]
 [16.069]
 [15.796]] [[0.845]
 [0.891]
 [0.849]
 [0.835]
 [0.849]
 [0.82 ]
 [0.829]]
maxi score, test score, baseline:  -0.5065666666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5065666666666667 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  46 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  69 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.05 ]
 [-0.04 ]
 [-0.048]
 [-0.047]
 [-0.048]
 [-0.046]
 [-0.046]] [[25.76 ]
 [33.214]
 [25.934]
 [26.174]
 [25.934]
 [26.53 ]
 [26.607]] [[0.128]
 [0.239]
 [0.133]
 [0.137]
 [0.133]
 [0.142]
 [0.143]]
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.292]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]] [[43.414]
 [43.303]
 [43.414]
 [43.414]
 [43.414]
 [43.414]
 [43.414]] [[1.282]
 [1.288]
 [1.282]
 [1.282]
 [1.282]
 [1.282]
 [1.282]]
maxi score, test score, baseline:  -0.5013800000000002 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.113]
 [0.163]
 [0.124]
 [0.074]
 [0.07 ]
 [0.109]
 [0.078]] [[36.616]
 [36.783]
 [34.459]
 [33.715]
 [33.427]
 [34.396]
 [35.468]] [[0.331]
 [0.382]
 [0.318]
 [0.261]
 [0.253]
 [0.303]
 [0.284]]
siam score:  -0.8163889
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.089]
 [0.011]] [[56.705]
 [56.705]
 [56.705]
 [56.705]
 [56.705]
 [51.074]
 [56.705]] [[1.839]
 [1.839]
 [1.839]
 [1.839]
 [1.839]
 [1.556]
 [1.839]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.067]
 [-0.071]
 [-0.067]
 [-0.067]
 [-0.067]
 [-0.067]
 [-0.067]] [[34.931]
 [37.731]
 [34.931]
 [34.931]
 [34.931]
 [34.931]
 [34.931]] [[1.556]
 [1.779]
 [1.556]
 [1.556]
 [1.556]
 [1.556]
 [1.556]]
printing an ep nov before normalisation:  30.9392806409482
line 256 mcts: sample exp_bonus 27.180441977600932
maxi score, test score, baseline:  -0.5013800000000002 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  0.2733333333333331  reward:  1.0 rdn_beta:  1.0
siam score:  -0.81672376
maxi score, test score, baseline:  -0.5013800000000002 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5013800000000002 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5013800000000002 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.95328296467793
printing an ep nov before normalisation:  34.64682711564753
maxi score, test score, baseline:  -0.5013800000000002 0.6593333333333334 0.6593333333333334
printing an ep nov before normalisation:  20.563504835870333
maxi score, test score, baseline:  -0.5013800000000002 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.074730396270752
actor:  1 policy actor:  1  step number:  41 total reward:  0.56  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.5013800000000002 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.54432332883598
maxi score, test score, baseline:  -0.5013800000000002 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.85921222751044
printing an ep nov before normalisation:  32.86378945974894
actor:  0 policy actor:  0  step number:  75 total reward:  0.15999999999999936  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.49906000000000017 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.49906000000000017 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.78131593448354
maxi score, test score, baseline:  -0.49906000000000017 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.51793680364114
actor:  1 policy actor:  1  step number:  62 total reward:  0.05999999999999939  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.4990600000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4990600000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.526666666666667  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4990600000000001 0.6593333333333334 0.6593333333333334
maxi score, test score, baseline:  -0.4990600000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.81644225
maxi score, test score, baseline:  -0.4990600000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4990600000000001 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  50 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  38.02302450675432
maxi score, test score, baseline:  -0.4990600000000001 0.6593333333333334 0.6593333333333334
UNIT TEST: sample policy line 217 mcts : [0.245 0.224 0.122 0.102 0.082 0.122 0.102]
printing an ep nov before normalisation:  41.01560086100627
printing an ep nov before normalisation:  27.505005796956628
actor:  1 policy actor:  1  step number:  60 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[-0.002]
 [ 0.225]
 [ 0.153]
 [-0.002]
 [-0.006]
 [-0.002]
 [-0.002]] [[42.675]
 [45.566]
 [43.789]
 [42.675]
 [47.992]
 [42.675]
 [42.675]] [[0.244]
 [0.506]
 [0.412]
 [0.244]
 [0.302]
 [0.244]
 [0.244]]
printing an ep nov before normalisation:  29.975367344406035
maxi score, test score, baseline:  -0.4990600000000001 0.6593333333333334 0.6593333333333334
printing an ep nov before normalisation:  43.03441177586607
Printing some Q and Qe and total Qs values:  [[0.868]
 [0.913]
 [0.9  ]
 [0.92 ]
 [0.872]
 [0.872]
 [0.869]] [[26.643]
 [29.886]
 [28.847]
 [29.442]
 [29.239]
 [30.357]
 [27.477]] [[0.868]
 [0.913]
 [0.9  ]
 [0.92 ]
 [0.872]
 [0.872]
 [0.869]]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  59 total reward:  0.3066666666666664  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.49644666666666676 0.6593333333333334 0.6593333333333334
printing an ep nov before normalisation:  36.3151580739256
actor:  1 policy actor:  1  step number:  69 total reward:  0.013333333333332753  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.49644666666666676 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.631]
 [0.607]
 [0.607]
 [0.607]
 [0.593]
 [0.607]] [[29.31 ]
 [31.404]
 [29.31 ]
 [29.31 ]
 [29.31 ]
 [27.277]
 [29.31 ]] [[0.607]
 [0.631]
 [0.607]
 [0.607]
 [0.607]
 [0.593]
 [0.607]]
siam score:  -0.81862503
maxi score, test score, baseline:  -0.49644666666666676 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
maxi score, test score, baseline:  -0.49644666666666676 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.027]
 [0.015]
 [0.017]
 [0.012]
 [0.012]
 [0.015]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.013]
 [0.027]
 [0.015]
 [0.017]
 [0.012]
 [0.012]
 [0.015]]
Printing some Q and Qe and total Qs values:  [[0.123]
 [0.189]
 [0.123]
 [0.123]
 [0.123]
 [0.123]
 [0.123]] [[42.087]
 [39.344]
 [42.087]
 [42.087]
 [42.087]
 [42.087]
 [42.087]] [[1.456]
 [1.367]
 [1.456]
 [1.456]
 [1.456]
 [1.456]
 [1.456]]
printing an ep nov before normalisation:  37.41855060690539
line 256 mcts: sample exp_bonus 31.921998886196732
printing an ep nov before normalisation:  41.25691808511984
printing an ep nov before normalisation:  38.37287336531264
Printing some Q and Qe and total Qs values:  [[0.941]
 [0.972]
 [0.899]
 [0.829]
 [0.941]
 [0.869]
 [0.825]] [[26.769]
 [30.246]
 [25.91 ]
 [26.665]
 [26.769]
 [25.113]
 [28.136]] [[0.941]
 [0.972]
 [0.899]
 [0.829]
 [0.941]
 [0.869]
 [0.825]]
printing an ep nov before normalisation:  47.38845549347285
printing an ep nov before normalisation:  34.411293654080794
printing an ep nov before normalisation:  39.68704604990981
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.902]
 [0.822]
 [0.85 ]
 [0.677]
 [0.815]
 [0.906]] [[38.197]
 [41.212]
 [32.122]
 [33.903]
 [34.057]
 [33.309]
 [40.38 ]] [[0.595]
 [0.902]
 [0.822]
 [0.85 ]
 [0.677]
 [0.815]
 [0.906]]
printing an ep nov before normalisation:  40.54882505835728
printing an ep nov before normalisation:  38.29705476760864
printing an ep nov before normalisation:  46.41969437576476
maxi score, test score, baseline:  -0.49644666666666676 0.6593333333333334 0.6593333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]] [[46.444]
 [46.444]
 [46.444]
 [46.444]
 [46.444]
 [46.444]
 [46.444]] [[1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]]
printing an ep nov before normalisation:  35.2300249969349
printing an ep nov before normalisation:  1.4896141919962247e-06
printing an ep nov before normalisation:  42.53553204704465
actor:  1 policy actor:  1  step number:  57 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  49.0405512908361
printing an ep nov before normalisation:  27.841142141470442
printing an ep nov before normalisation:  31.31622676296168
printing an ep nov before normalisation:  27.692605485426185
actor:  0 policy actor:  0  step number:  28 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  28 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  28 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  28.70644838470749
actor:  0 policy actor:  0  step number:  29 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  29 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  29 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  29 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  0.026268206151485174
actor:  0 policy actor:  0  step number:  29 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  30 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  30 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
line 256 mcts: sample exp_bonus 28.643580128953445
actor:  0 policy actor:  0  step number:  31 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  31 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  32 total reward:  0.6066666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  32 total reward:  0.6733333333333333  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  51.12116015208058
actor:  0 policy actor:  0  step number:  33 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  35 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  35 total reward:  0.5333333333333333  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  35 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  36 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  38 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  2
actor:  1 policy actor:  1  step number:  45 total reward:  0.5466666666666666  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.07803105578508
maxi score, test score, baseline:  -0.43228666666666676 0.6590000000000001 0.6590000000000001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.43228666666666676 0.6590000000000001 0.6590000000000001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.43228666666666676 0.6590000000000001 0.6590000000000001
actor:  1 policy actor:  1  step number:  56 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  36.16172884951167
maxi score, test score, baseline:  -0.43228666666666676 0.6590000000000001 0.6590000000000001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.43228666666666676 0.6590000000000001 0.6590000000000001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  57 total reward:  0.1599999999999996  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.4299666666666668 0.6590000000000001 0.6590000000000001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.16712156581916
actor:  1 policy actor:  1  step number:  62 total reward:  0.11333333333333284  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4299666666666668 0.6590000000000001 0.6590000000000001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.11267463123224
printing an ep nov before normalisation:  28.188421606810913
printing an ep nov before normalisation:  34.24584839232045
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]] [[42.731]
 [42.731]
 [42.731]
 [42.731]
 [42.731]
 [42.731]
 [42.731]] [[1.46]
 [1.46]
 [1.46]
 [1.46]
 [1.46]
 [1.46]
 [1.46]]
maxi score, test score, baseline:  -0.4299666666666668 0.6590000000000001 0.6590000000000001
maxi score, test score, baseline:  -0.4299666666666668 0.6590000000000001 0.6590000000000001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.06457456691739
maxi score, test score, baseline:  -0.4299666666666668 0.6590000000000001 0.6590000000000001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.4299666666666668 0.6590000000000001 0.6590000000000001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4299666666666668 0.6590000000000001 0.6590000000000001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.086]
 [-0.069]
 [-0.085]
 [-0.084]
 [-0.082]
 [-0.087]
 [-0.081]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.086]
 [-0.069]
 [-0.085]
 [-0.084]
 [-0.082]
 [-0.087]
 [-0.081]]
actor:  1 policy actor:  1  step number:  50 total reward:  0.39333333333333276  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.4299666666666668 0.6590000000000001 0.6590000000000001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4299666666666668 0.6590000000000001 0.6590000000000001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.960310494302185
printing an ep nov before normalisation:  47.76168421403696
printing an ep nov before normalisation:  50.50511494578264
maxi score, test score, baseline:  -0.4299666666666668 0.6590000000000001 0.6590000000000001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4299666666666668 0.6590000000000001 0.6590000000000001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.727]
 [0.727]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]] [[33.993]
 [32.346]
 [20.045]
 [20.045]
 [20.045]
 [20.045]
 [20.045]] [[0.727]
 [0.727]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]]
actor:  0 policy actor:  0  step number:  52 total reward:  0.24666666666666603  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.4274733333333334 0.6590000000000001 0.6590000000000001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.079]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.079]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]]
actor:  0 policy actor:  0  step number:  59 total reward:  0.03999999999999937  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  39.5463708563513
printing an ep nov before normalisation:  34.28035147061336
printing an ep nov before normalisation:  37.43962016155498
printing an ep nov before normalisation:  35.575421272584606
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.513]
 [0.409]
 [0.358]
 [0.409]
 [0.409]
 [0.401]] [[31.166]
 [40.052]
 [31.166]
 [38.395]
 [31.166]
 [31.166]
 [35.548]] [[0.725]
 [1.028]
 [0.725]
 [0.837]
 [0.725]
 [0.725]
 [0.815]]
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.548]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]] [[49.922]
 [46.025]
 [49.922]
 [49.922]
 [49.922]
 [49.922]
 [49.922]] [[2.44 ]
 [2.253]
 [2.44 ]
 [2.44 ]
 [2.44 ]
 [2.44 ]
 [2.44 ]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.54040425738875
actor:  1 policy actor:  1  step number:  40 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.42539333333333346 0.6590000000000001 0.6590000000000001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.733971380422258
maxi score, test score, baseline:  -0.42539333333333346 0.6590000000000001 0.6590000000000001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.80955523
maxi score, test score, baseline:  -0.42539333333333346 0.6590000000000001 0.6590000000000001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.4066666666666663  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  61 total reward:  0.2266666666666659  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.42539333333333346 0.6590000000000001 0.6590000000000001
printing an ep nov before normalisation:  25.265848020554916
maxi score, test score, baseline:  -0.42539333333333346 0.6590000000000001 0.6590000000000001
printing an ep nov before normalisation:  0.0015753265034845754
actor:  1 policy actor:  1  step number:  40 total reward:  0.54  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.42539333333333346 0.6590000000000001 0.6590000000000001
maxi score, test score, baseline:  -0.42539333333333346 0.6590000000000001 0.6590000000000001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.42539333333333346 0.6590000000000001 0.6590000000000001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.679861485548116
actions average: 
K:  1  action  0 :  tensor([0.3814, 0.0043, 0.0924, 0.0972, 0.1975, 0.1204, 0.1068],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0082, 0.9238, 0.0112, 0.0179, 0.0033, 0.0038, 0.0318],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1162, 0.0074, 0.1657, 0.1442, 0.1426, 0.2516, 0.1723],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0997, 0.0038, 0.1450, 0.2656, 0.1400, 0.1702, 0.1756],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0541, 0.0016, 0.0524, 0.0650, 0.7046, 0.0654, 0.0567],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0924, 0.0026, 0.1196, 0.1011, 0.0794, 0.4840, 0.1209],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1060, 0.1443, 0.0833, 0.1522, 0.0916, 0.0912, 0.3314],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  44.092424948860454
Printing some Q and Qe and total Qs values:  [[0.339]
 [0.339]
 [0.339]
 [0.34 ]
 [0.34 ]
 [0.337]
 [0.339]] [[15.863]
 [15.863]
 [15.863]
 [17.706]
 [17.779]
 [18.214]
 [15.863]] [[0.339]
 [0.339]
 [0.339]
 [0.34 ]
 [0.34 ]
 [0.337]
 [0.339]]
printing an ep nov before normalisation:  27.98191190732293
printing an ep nov before normalisation:  28.19073646079266
maxi score, test score, baseline:  -0.42539333333333346 0.6590000000000001 0.6590000000000001
maxi score, test score, baseline:  -0.42539333333333346 0.6590000000000001 0.6590000000000001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.42539333333333346 0.6590000000000001 0.6590000000000001
Printing some Q and Qe and total Qs values:  [[0.344]
 [0.309]
 [0.341]
 [0.34 ]
 [0.339]
 [0.339]
 [0.342]] [[10.402]
 [23.829]
 [10.668]
 [10.45 ]
 [10.407]
 [10.526]
 [10.586]] [[0.344]
 [0.309]
 [0.341]
 [0.34 ]
 [0.339]
 [0.339]
 [0.342]]
printing an ep nov before normalisation:  19.180707931518555
actor:  1 policy actor:  1  step number:  62 total reward:  0.09999999999999942  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  10.65818236908939
printing an ep nov before normalisation:  43.86589527130127
maxi score, test score, baseline:  -0.42539333333333346 0.6590000000000001 0.6590000000000001
maxi score, test score, baseline:  -0.42539333333333346 0.6590000000000001 0.6590000000000001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  12.140550377877647
printing an ep nov before normalisation:  11.584783758519082
maxi score, test score, baseline:  -0.42539333333333346 0.6590000000000001 0.6590000000000001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.42539333333333346 0.6590000000000001 0.6590000000000001
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.365]
 [0.327]
 [0.313]
 [0.327]
 [0.327]
 [0.262]] [[33.158]
 [35.398]
 [33.158]
 [39.019]
 [33.158]
 [33.158]
 [32.941]] [[1.167]
 [1.316]
 [1.167]
 [1.443]
 [1.167]
 [1.167]
 [1.092]]
printing an ep nov before normalisation:  41.208048223061425
actor:  1 policy actor:  1  step number:  49 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.42539333333333346 0.6590000000000001 0.6590000000000001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]] [[33.971]
 [33.971]
 [33.971]
 [33.971]
 [33.971]
 [33.971]
 [33.971]] [[0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]]
siam score:  -0.81327164
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.42539333333333346 0.6590000000000001 0.6590000000000001
actor:  1 policy actor:  1  step number:  61 total reward:  0.25333333333333297  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.42539333333333346 0.6590000000000001 0.6590000000000001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8145297
printing an ep nov before normalisation:  15.345271801143449
Printing some Q and Qe and total Qs values:  [[0.936]
 [0.936]
 [0.893]
 [0.936]
 [0.936]
 [0.976]
 [0.936]] [[41.735]
 [41.735]
 [46.927]
 [41.735]
 [41.735]
 [40.548]
 [41.735]] [[0.936]
 [0.936]
 [0.893]
 [0.936]
 [0.936]
 [0.976]
 [0.936]]
maxi score, test score, baseline:  -0.42539333333333346 0.6590000000000001 0.6590000000000001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.04 ]
 [-0.059]
 [-0.071]
 [-0.11 ]
 [-0.079]
 [-0.064]
 [-0.187]] [[35.202]
 [35.575]
 [35.214]
 [33.312]
 [35.241]
 [32.342]
 [37.881]] [[1.292]
 [1.3  ]
 [1.262]
 [1.08 ]
 [1.255]
 [1.054]
 [1.344]]
maxi score, test score, baseline:  -0.42539333333333346 0.6590000000000001 0.6590000000000001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.13177812283897
printing an ep nov before normalisation:  32.66884121352742
printing an ep nov before normalisation:  35.45102760956877
maxi score, test score, baseline:  -0.42539333333333346 0.6590000000000001 0.6590000000000001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.42539333333333346 0.6590000000000001 0.6590000000000001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.81309736
maxi score, test score, baseline:  -0.42539333333333346 0.6590000000000001 0.6590000000000001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.227]
 [0.307]
 [0.241]
 [0.239]
 [0.237]
 [0.205]
 [0.203]] [[25.463]
 [28.526]
 [25.357]
 [23.366]
 [24.704]
 [22.262]
 [22.295]] [[0.227]
 [0.307]
 [0.241]
 [0.239]
 [0.237]
 [0.205]
 [0.203]]
maxi score, test score, baseline:  -0.42539333333333346 0.6590000000000001 0.6590000000000001
probs:  