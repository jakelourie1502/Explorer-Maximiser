append_mcts_svs:False
dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 50}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:1
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.Car_Driving_Env.RaceWorld'>
same_env_each_time:True
env_size:[7, 42]
observable_size:[7, 9]
game_modes:1
env_map:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
max_steps:150
actions_size:7
optimal_score:0.86
total_frames:255000
exp_gamma:0.975
atari_env:False
memory_size:30
reward_clipping:False
image_size:[48, 48]
deque_length:3
PRESET_CONFIG:7
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:episodic
rdn_beta:[0.3333333333333333, 2, 6]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
contrast_vector:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 294)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:False
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
Training Flag: False
Self play flag: True
add more workers flag:  True
expV_train_flag:  False
expV_train_start_flag:  255000
main train batch thing paused
add a thread
Adding thread: now have 3 threads
main train batch thing paused
add a thread
Starting evaluation
Adding thread: now have 4 threads
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  0.0023231528078516326
siam score:  0.002420054674571888
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.3407290810323496, 0.21099312001444925, 0.08125715899654896, 0.10720435120012903, 0.04882316874207389, 0.21099312001444925]
deleting a thread, now have 3 threads
Frames:  927 train batches done:  12 episodes:  28
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.3407290810323496, 0.21099312001444925, 0.08125715899654896, 0.10720435120012903, 0.04882316874207389, 0.21099312001444925]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.3407290810323496, 0.21099312001444925, 0.08125715899654896, 0.10720435120012903, 0.04882316874207389, 0.21099312001444925]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.3407290810323496, 0.21099312001444925, 0.08125715899654896, 0.10720435120012903, 0.04882316874207389, 0.21099312001444925]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.3407290810323496, 0.21099312001444925, 0.08125715899654896, 0.10720435120012903, 0.04882316874207389, 0.21099312001444925]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.3407290810323496, 0.21099312001444925, 0.08125715899654896, 0.10720435120012903, 0.04882316874207389, 0.21099312001444925]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.3407290810323496, 0.21099312001444925, 0.08125715899654896, 0.10720435120012903, 0.04882316874207389, 0.21099312001444925]
deleting a thread, now have 2 threads
Frames:  927 train batches done:  44 episodes:  28
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.3407290810323496, 0.21099312001444925, 0.08125715899654896, 0.10720435120012903, 0.04882316874207389, 0.21099312001444925]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.3407290810323496, 0.21099312001444925, 0.08125715899654896, 0.10720435120012903, 0.04882316874207389, 0.21099312001444925]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.3407290810323496, 0.21099312001444925, 0.08125715899654896, 0.10720435120012903, 0.04882316874207389, 0.21099312001444925]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.3407290810323496, 0.21099312001444925, 0.08125715899654896, 0.10720435120012903, 0.04882316874207389, 0.21099312001444925]
siam score:  -0.3952205
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.3407290810323496, 0.21099312001444925, 0.08125715899654896, 0.10720435120012903, 0.04882316874207389, 0.21099312001444925]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.3407290810323496, 0.21099312001444925, 0.08125715899654896, 0.10720435120012903, 0.04882316874207389, 0.21099312001444925]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.3407290810323496, 0.21099312001444925, 0.08125715899654896, 0.10720435120012903, 0.04882316874207389, 0.21099312001444925]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.3407290810323496, 0.21099312001444925, 0.08125715899654896, 0.10720435120012903, 0.04882316874207389, 0.21099312001444925]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.3407290810323496, 0.21099312001444925, 0.08125715899654896, 0.10720435120012903, 0.04882316874207389, 0.21099312001444925]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.3407290810323496, 0.21099312001444925, 0.08125715899654896, 0.10720435120012903, 0.04882316874207389, 0.21099312001444925]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.3407290810323496, 0.21099312001444925, 0.08125715899654896, 0.10720435120012903, 0.04882316874207389, 0.21099312001444925]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.3407290810323496, 0.21099312001444925, 0.08125715899654896, 0.10720435120012903, 0.04882316874207389, 0.21099312001444925]
deleting a thread, now have 1 threads
Frames:  927 train batches done:  78 episodes:  28
actions average: 
K:  4  action  0 :  tensor([0.2371, 0.2142, 0.1156, 0.0809, 0.1150, 0.0987, 0.1384],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.1361, 0.3696, 0.0473, 0.1752, 0.1095, 0.0515, 0.1108],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1965, 0.0873, 0.3546, 0.0822, 0.0770, 0.0952, 0.1072],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1729, 0.1602, 0.1447, 0.1359, 0.0941, 0.1199, 0.1723],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1992, 0.2000, 0.0925, 0.1210, 0.1359, 0.0953, 0.1560],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1350, 0.0463, 0.1155, 0.1027, 0.0670, 0.3078, 0.2258],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2545, 0.1695, 0.1402, 0.0814, 0.0643, 0.1033, 0.1868],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.3407290810323496, 0.21099312001444925, 0.08125715899654896, 0.10720435120012903, 0.04882316874207389, 0.21099312001444925]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.3407290810323496, 0.21099312001444925, 0.08125715899654896, 0.10720435120012903, 0.04882316874207389, 0.21099312001444925]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.3407290810323496, 0.21099312001444925, 0.08125715899654896, 0.10720435120012903, 0.04882316874207389, 0.21099312001444925]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.3407290810323496, 0.21099312001444925, 0.08125715899654896, 0.10720435120012903, 0.04882316874207389, 0.21099312001444925]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.3407290810323496, 0.21099312001444925, 0.08125715899654896, 0.10720435120012903, 0.04882316874207389, 0.21099312001444925]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.3407290810323496, 0.21099312001444925, 0.08125715899654896, 0.10720435120012903, 0.04882316874207389, 0.21099312001444925]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.3407290810323496, 0.21099312001444925, 0.08125715899654896, 0.10720435120012903, 0.04882316874207389, 0.21099312001444925]
maxi score, test score, baseline:  -0.9641857142857143 -1.0 -0.9641857142857143
probs:  [0.3407290810323496, 0.21099312001444925, 0.08125715899654896, 0.10720435120012903, 0.04882316874207389, 0.21099312001444925]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
UNIT TEST: sample policy line 217 mcts : [0.  0.4 0.  0.  0.2 0.2 0.2]
deleting a thread, now have 1 threads
Frames:  1076 train batches done:  116 episodes:  41
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.1456192382980265, 0.19154090019324138, 0.1456192382980265, 0.08821716092900789, 0.05377591450759673, 0.375227547774101]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.1456192382980265, 0.19154090019324138, 0.1456192382980265, 0.08821716092900789, 0.05377591450759673, 0.375227547774101]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.1456192382980265, 0.19154090019324138, 0.1456192382980265, 0.08821716092900789, 0.05377591450759673, 0.375227547774101]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.1456192382980265, 0.19154090019324138, 0.1456192382980265, 0.08821716092900789, 0.05377591450759673, 0.375227547774101]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.1456192382980265, 0.19154090019324138, 0.1456192382980265, 0.08821716092900789, 0.05377591450759673, 0.375227547774101]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.1456192382980265, 0.19154090019324138, 0.1456192382980265, 0.08821716092900789, 0.05377591450759673, 0.375227547774101]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.1456192382980265, 0.19154090019324138, 0.1456192382980265, 0.08821716092900789, 0.05377591450759673, 0.375227547774101]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.1456192382980265, 0.19154090019324138, 0.1456192382980265, 0.08821716092900789, 0.05377591450759673, 0.375227547774101]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.1456192382980265, 0.19154090019324138, 0.1456192382980265, 0.08821716092900789, 0.05377591450759673, 0.375227547774101]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.1456192382980265, 0.19154090019324138, 0.1456192382980265, 0.08821716092900789, 0.05377591450759673, 0.375227547774101]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.15806176915656106, 0.15806176915656106, 0.12245529670095164, 0.09575044235924458, 0.05836364628085469, 0.40730707634582697]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.18287323938217268, 0.18287323938217268, 0.13543936801971604, 0.09986396449787363, 0.05005839956729419, 0.34889178915077074]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.18287323938217268, 0.18287323938217268, 0.13543936801971604, 0.09986396449787363, 0.05005839956729419, 0.34889178915077074]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.18287323938217268, 0.18287323938217268, 0.13543936801971604, 0.09986396449787363, 0.05005839956729419, 0.34889178915077074]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.18287323938217268, 0.18287323938217268, 0.13543936801971604, 0.09986396449787363, 0.05005839956729419, 0.34889178915077074]
from probs:  [0.18287323938217268, 0.18287323938217268, 0.13543936801971604, 0.09986396449787363, 0.05005839956729419, 0.34889178915077074]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
UNIT TEST: sample policy line 217 mcts : [1. 0. 0. 0. 0. 0. 0.]
printing an ep nov before normalisation:  20.522158764976837
deleting a thread, now have 1 threads
Frames:  1618 train batches done:  169 episodes:  54
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.1522114985180327, 0.09383485791778019, 0.227267179289786, 0.1522114985180327, 0.04713354543757813, 0.3273414203187904]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.1522114985180327, 0.09383485791778019, 0.227267179289786, 0.1522114985180327, 0.04713354543757813, 0.3273414203187904]
actions average: 
K:  2  action  0 :  tensor([0.2468, 0.0736, 0.1169, 0.1524, 0.1992, 0.1135, 0.0976],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0407, 0.7072, 0.0316, 0.0529, 0.0416, 0.0537, 0.0723],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0565, 0.1241, 0.5689, 0.0334, 0.0561, 0.1241, 0.0368],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1084, 0.2168, 0.1161, 0.1192, 0.1098, 0.1766, 0.1530],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2384, 0.0473, 0.1113, 0.1089, 0.2843, 0.0901, 0.1198],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0842, 0.0525, 0.2002, 0.0561, 0.0867, 0.4214, 0.0990],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1852, 0.1142, 0.1201, 0.1243, 0.1981, 0.1464, 0.1117],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.1522114985180327, 0.09383485791778019, 0.227267179289786, 0.1522114985180327, 0.04713354543757813, 0.3273414203187904]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.1522114985180327, 0.09383485791778019, 0.227267179289786, 0.1522114985180327, 0.04713354543757813, 0.3273414203187904]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.1522114985180327, 0.09383485791778019, 0.227267179289786, 0.1522114985180327, 0.04713354543757813, 0.3273414203187904]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.1522114985180327, 0.09383485791778019, 0.227267179289786, 0.1522114985180327, 0.04713354543757813, 0.3273414203187904]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.1522114985180327, 0.09383485791778019, 0.227267179289786, 0.1522114985180327, 0.04713354543757813, 0.3273414203187904]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.1522114985180327, 0.09383485791778019, 0.227267179289786, 0.1522114985180327, 0.04713354543757813, 0.3273414203187904]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.1522114985180327, 0.09383485791778019, 0.227267179289786, 0.1522114985180327, 0.04713354543757813, 0.3273414203187904]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.1522114985180327, 0.09383485791778019, 0.227267179289786, 0.1522114985180327, 0.04713354543757813, 0.3273414203187904]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.1522114985180327, 0.09383485791778019, 0.227267179289786, 0.1522114985180327, 0.04713354543757813, 0.3273414203187904]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.1522114985180327, 0.09383485791778019, 0.227267179289786, 0.1522114985180327, 0.04713354543757813, 0.3273414203187904]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.1522114985180327, 0.09383485791778019, 0.227267179289786, 0.1522114985180327, 0.04713354543757813, 0.3273414203187904]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.1522114985180327, 0.09383485791778019, 0.227267179289786, 0.1522114985180327, 0.04713354543757813, 0.3273414203187904]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.10623216727533515, 0.10623216727533515, 0.257318415753664, 0.10623216727533515, 0.05335198030792005, 0.37063310211241046]
Printing some Q and Qe and total Qs values:  [[1.113]
 [1.115]
 [1.154]
 [1.154]
 [1.154]
 [1.125]
 [1.154]] [[39.575]
 [40.476]
 [45.527]
 [45.527]
 [45.527]
 [42.882]
 [45.527]] [[1.68 ]
 [1.709]
 [1.901]
 [1.901]
 [1.901]
 [1.792]
 [1.901]]
using another actor
from probs:  [0.10623216727533515, 0.10623216727533515, 0.257318415753664, 0.10623216727533515, 0.05335198030792005, 0.37063310211241046]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.11216095365734328, 0.05632583301364609, 0.27168986978219234, 0.11216095365734328, 0.05632583301364609, 0.391336556875829]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.11216095365734328, 0.05632583301364609, 0.27168986978219234, 0.11216095365734328, 0.05632583301364609, 0.391336556875829]
maxi score, test score, baseline:  -0.9713285714285714 -1.0 -0.9713285714285714
probs:  [0.11879155580504229, 0.05965171297597668, 0.28776253531665785, 0.05965171297597668, 0.05965171297597668, 0.4144907699503698]
maxi score, test score, baseline:  -0.9713285714285714 -1.0 -0.9713285714285714
maxi score, test score, baseline:  -0.9713285714285714 -1.0 -0.9713285714285714
maxi score, test score, baseline:  -0.9713285714285714 -1.0 -0.9713285714285714
probs:  [0.16910561299221796, 0.0666698673190686, 0.16910561299221796, 0.0666698673190686, 0.0666698673190686, 0.4617791720583584]
maxi score, test score, baseline:  -0.9713285714285714 -1.0 -0.9713285714285714
probs:  [0.21121874357947093, 0.058468765592713294, 0.21121874357947093, 0.058468765592713294, 0.058468765592713294, 0.40215621606291824]
maxi score, test score, baseline:  -0.9713285714285714 -1.0 -0.9713285714285714
probs:  [0.21121874357947093, 0.058468765592713294, 0.21121874357947093, 0.058468765592713294, 0.058468765592713294, 0.40215621606291824]
maxi score, test score, baseline:  -0.9713285714285714 -1.0 -0.9713285714285714
maxi score, test score, baseline:  -0.9713285714285714 -1.0 -0.9713285714285714
probs:  [0.04349131361131241, 0.04349131361131241, 0.29833687165687284, 0.1581718147318147, 0.1581718147318147, 0.29833687165687284]
using another actor
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
Printing some Q and Qe and total Qs values:  [[1.073]
 [1.135]
 [1.073]
 [1.073]
 [1.073]
 [1.073]
 [1.073]] [[37.906]
 [51.949]
 [37.906]
 [37.906]
 [37.906]
 [37.906]
 [37.906]] [[1.654]
 [2.135]
 [1.654]
 [1.654]
 [1.654]
 [1.654]
 [1.654]]
printing an ep nov before normalisation:  52.16048177813235
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.04449186451710413, 0.16272554401668074, 0.16272554401668074, 0.16272554401668074, 0.16272554401668074, 0.30460595941617297]
siam score:  -0.4533287
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.04449186451710413, 0.16272554401668074, 0.16272554401668074, 0.16272554401668074, 0.16272554401668074, 0.30460595941617297]
using another actor
from probs:  [0.044491863104767465, 0.16272554397112146, 0.16272554397112146, 0.16272554397112146, 0.16272554397112146, 0.3046059610107467]
siam score:  -0.44774428
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.05044518919945854, 0.18454689396931417, 0.05044518919945854, 0.18454689396931417, 0.18454689396931417, 0.3454689396931405]
printing an ep nov before normalisation:  22.62866973876953
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.05824420462273531, 0.2131334361140659, 0.05824420462273531, 0.05824420462273531, 0.2131334361140659, 0.3990005139036623]
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.05824420462273531, 0.2131334361140659, 0.05824420462273531, 0.05824420462273531, 0.2131334361140659, 0.3990005139036623]
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.05824420462273531, 0.2131334361140659, 0.05824420462273531, 0.05824420462273531, 0.2131334361140659, 0.3990005139036623]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.043370212608682485, 0.2899631207246508, 0.043370212608682485, 0.043370212608682485, 0.2899631207246508, 0.2899631207246508]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
actions average: 
K:  2  action  0 :  tensor([0.3061, 0.0308, 0.0742, 0.0902, 0.1563, 0.1101, 0.2323],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0544, 0.6407, 0.0451, 0.0636, 0.0669, 0.0621, 0.0672],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0923, 0.0291, 0.4952, 0.0660, 0.0835, 0.1295, 0.1044],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1620, 0.0118, 0.0846, 0.1098, 0.1352, 0.1355, 0.3610],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1576, 0.0471, 0.0987, 0.0920, 0.3268, 0.1210, 0.1568],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0806, 0.0087, 0.1390, 0.0590, 0.0986, 0.5028, 0.1113],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1434, 0.0512, 0.0993, 0.0984, 0.1498, 0.0961, 0.3618],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  86.02247847939594
using another actor
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.1273550688938728, 0.23686594840379857, 0.1273550688938728, 0.03469201700085862, 0.23686594840379857, 0.23686594840379857]
printing an ep nov before normalisation:  15.996899604797363
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
printing an ep nov before normalisation:  79.7509241104126
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.1273550685352055, 0.23686594904427594, 0.1273550685352055, 0.034692015796761295, 0.23686594904427594, 0.23686594904427594]
from probs:  [0.1273550685352055, 0.23686594904427594, 0.1273550685352055, 0.034692015796761295, 0.23686594904427594, 0.23686594904427594]
maxi score, test score, baseline:  -0.9760904761904762 -1.0 -0.9760904761904762
maxi score, test score, baseline:  -0.9760904761904762 -1.0 -0.9760904761904762
probs:  [0.18888412042197583, 0.18888412042197583, 0.10931463487970579, 0.04111221870061739, 0.18888412042197583, 0.28292078515374947]
actions average: 
K:  0  action  0 :  tensor([0.3130, 0.0755, 0.0468, 0.1415, 0.2458, 0.0709, 0.1066],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0425, 0.7754, 0.0204, 0.0485, 0.0415, 0.0283, 0.0434],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0821, 0.0308, 0.4757, 0.0616, 0.1037, 0.1539, 0.0922],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2085, 0.0131, 0.0618, 0.1709, 0.2713, 0.1187, 0.1558],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2159, 0.0482, 0.0779, 0.1250, 0.2936, 0.1141, 0.1252],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1099, 0.1183, 0.1102, 0.0822, 0.1293, 0.3511, 0.0990],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1243, 0.0777, 0.0922, 0.0903, 0.1758, 0.0773, 0.3625],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9760904761904762 -1.0 -0.9760904761904762
probs:  [0.18888412042197583, 0.18888412042197583, 0.10931463487970579, 0.04111221870061739, 0.18888412042197583, 0.28292078515374947]
maxi score, test score, baseline:  -0.9760904761904762 -1.0 -0.9760904761904762
probs:  [0.2133380517101929, 0.2133380517101929, 0.11532814311878772, 0.031319650040440634, 0.2133380517101929, 0.2133380517101929]
printing an ep nov before normalisation:  51.18391513824463
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
probs:  [0.23246880395128858, 0.03506239209742286, 0.23246880395128858, 0.03506239209742286, 0.23246880395128858, 0.23246880395128858]
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
probs:  [0.23246880395128858, 0.03506239209742286, 0.23246880395128858, 0.03506239209742286, 0.23246880395128858, 0.23246880395128858]
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
probs:  [0.23246880395128858, 0.03506239209742286, 0.23246880395128858, 0.03506239209742286, 0.23246880395128858, 0.23246880395128858]
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
probs:  [0.18938201428763288, 0.11019555107264464, 0.040907395759529785, 0.18938201428763288, 0.2807510103049271, 0.18938201428763288]
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
probs:  [0.18938201428763288, 0.11019555107264464, 0.040907395759529785, 0.18938201428763288, 0.2807510103049271, 0.18938201428763288]
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
probs:  [0.18938201428763288, 0.11019555107264464, 0.040907395759529785, 0.18938201428763288, 0.2807510103049271, 0.18938201428763288]
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
probs:  [0.18607146505907945, 0.050237876312190524, 0.050237876312190524, 0.3413098521983807, 0.18607146505907945, 0.18607146505907945]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.0389493797113561, 0.1444040203166585, 0.1444040203166585, 0.26391927966933415, 0.1444040203166585, 0.26391927966933415]
from probs:  [0.0389493797113561, 0.1444040203166585, 0.1444040203166585, 0.26391927966933415, 0.1444040203166585, 0.26391927966933415]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.04422299536165028, 0.16400484772525317, 0.16400484772525317, 0.2997576137373369, 0.16400484772525317, 0.16400484772525317]
printing an ep nov before normalisation:  21.61779006322225
from probs:  [0.03543659586695622, 0.23228170206652188, 0.23228170206652188, 0.23228170206652188, 0.03543659586695622, 0.23228170206652188]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.04409770935774009, 0.28923562397559327, 0.28923562397559327, 0.04409770935774009, 0.04409770935774009, 0.28923562397559327]
maxi score, test score, baseline:  -0.9790666666666666 -1.0 -0.9790666666666666
probs:  [0.05839009904191186, 0.3832198019161762, 0.05839009904191186, 0.05839009904191186, 0.05839009904191186, 0.3832198019161762]
maxi score, test score, baseline:  -0.9790666666666666 -1.0 -0.9790666666666666
probs:  [0.05839009904191186, 0.3832198019161762, 0.05839009904191186, 0.05839009904191186, 0.05839009904191186, 0.3832198019161762]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  -0.9790666666666666 -1.0 -0.9790666666666666
probs:  [0.05839009904191186, 0.3832198019161762, 0.05839009904191186, 0.05839009904191186, 0.05839009904191186, 0.3832198019161762]
printing an ep nov before normalisation:  36.90083026885986
using another actor
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.21599537813972586, 0.21599537813972586, 0.05814350142593644, 0.05814350142593644, 0.05814350142593644, 0.3935787394427389]
printing an ep nov before normalisation:  54.91772462494489
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.21599537813972586, 0.21599537813972586, 0.05814350142593644, 0.05814350142593644, 0.05814350142593644, 0.3935787394427389]
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
siam score:  -0.48153123
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.39296493110471903, 0.0581674987854085, 0.0581674987854085, 0.21626628626952774, 0.0581674987854085, 0.21626628626952774]
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.04422651670974876, 0.16444048212199538, 0.16444048212199538, 0.2980115548022697, 0.16444048212199538, 0.16444048212199538]
printing an ep nov before normalisation:  47.583262914272176
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.037724297133786076, 0.17628923155718013, 0.17628923155718013, 0.2571187766374935, 0.17628923155718013, 0.17628923155718013]
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.03496996600919447, 0.2350155872610509, 0.12999163610382639, 0.2350155872610509, 0.12999163610382639, 0.2350155872610509]
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.044242421973381796, 0.16455590382712734, 0.16455590382712734, 0.16455590382712734, 0.16455590382712734, 0.29753396271810884]
Printing some Q and Qe and total Qs values:  [[-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[52.804]
 [52.804]
 [52.804]
 [52.804]
 [52.804]
 [52.804]
 [52.804]] [[1.666]
 [1.666]
 [1.666]
 [1.666]
 [1.666]
 [1.666]
 [1.666]]
line 256 mcts: sample exp_bonus 30.009698688983917
siam score:  -0.4785257
siam score:  -0.47884724
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
probs:  [0.0446740823682285, 0.0446740823682285, 0.28865925096510486, 0.28865925096510486, 0.0446740823682285, 0.28865925096510486]
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
probs:  [0.059063761321921565, 0.059063761321921565, 0.38187247735615687, 0.38187247735615687, 0.059063761321921565, 0.059063761321921565]
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
siam score:  -0.47351602
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
probs:  [0.059063761321921565, 0.059063761321921565, 0.38187247735615687, 0.38187247735615687, 0.059063761321921565, 0.059063761321921565]
siam score:  -0.4838611
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
probs:  [0.2319719689939236, 0.2319719689939236, 0.03605606201215277, 0.2319719689939236, 0.03605606201215277, 0.2319719689939236]
UNIT TEST: sample policy line 217 mcts : [0.2 0.4 0.  0.  0.2 0.  0.2]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.04481740196341106, 0.2885159313699223, 0.04481740196341106, 0.2885159313699223, 0.04481740196341106, 0.2885159313699223]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.04481740196341106, 0.2885159313699223, 0.04481740196341106, 0.2885159313699223, 0.04481740196341106, 0.2885159313699223]
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
probs:  [0.04481739689932347, 0.28851593643400986, 0.04481739689932347, 0.28851593643400986, 0.04481739689932347, 0.28851593643400986]
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
probs:  [0.04481739689932347, 0.28851593643400986, 0.04481739689932347, 0.28851593643400986, 0.04481739689932347, 0.28851593643400986]
from probs:  [0.0496963813391055, 0.3330005586462261, 0.18490973869023133, 0.3330005586462261, 0.0496963813391055, 0.0496963813391055]
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.05832023387137024, 0.21706035633889761, 0.21706035633889761, 0.39091858570809407, 0.05832023387137024, 0.05832023387137024]
siam score:  -0.48541868
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.04778101543278326, 0.2239214128130461, 0.2239214128130461, 0.32457306845891076, 0.04778101543278326, 0.13202207504943048]
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.04778101543278326, 0.2239214128130461, 0.2239214128130461, 0.32457306845891076, 0.04778101543278326, 0.13202207504943048]
Printing some Q and Qe and total Qs values:  [[0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.793]
 [0.772]] [[21.718]
 [21.718]
 [21.718]
 [21.718]
 [21.718]
 [18.191]
 [21.718]] [[2.439]
 [2.439]
 [2.439]
 [2.439]
 [2.439]
 [1.932]
 [2.439]]
actions average: 
K:  3  action  0 :  tensor([0.3895, 0.0083, 0.0607, 0.0692, 0.1836, 0.1787, 0.1100],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0935, 0.4197, 0.0459, 0.0744, 0.1711, 0.0598, 0.1356],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0801, 0.0063, 0.6209, 0.0382, 0.0603, 0.1529, 0.0413],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2967, 0.1415, 0.0698, 0.1156, 0.2254, 0.0989, 0.0521],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2610, 0.0263, 0.0766, 0.0632, 0.3171, 0.1600, 0.0958],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1526, 0.1381, 0.0455, 0.0552, 0.1266, 0.4057, 0.0763],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1725, 0.0119, 0.0738, 0.1287, 0.2437, 0.2223, 0.1470],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.10785682773563367, 0.17361862845723608, 0.17361862845723608, 0.32393131582089885, 0.047355971071759394, 0.17361862845723608]
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.050399560233751056, 0.1848000685873966, 0.1848000685873966, 0.34480067377030815, 0.050399560233751056, 0.1848000685873966]
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.050399560233751056, 0.1848000685873966, 0.1848000685873966, 0.34480067377030815, 0.050399560233751056, 0.1848000685873966]
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.050399560233751056, 0.1848000685873966, 0.1848000685873966, 0.34480067377030815, 0.050399560233751056, 0.1848000685873966]
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.050399560233751056, 0.1848000685873966, 0.1848000685873966, 0.34480067377030815, 0.050399560233751056, 0.1848000685873966]
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.050399560233751056, 0.1848000685873966, 0.1848000685873966, 0.34480067377030815, 0.050399560233751056, 0.1848000685873966]
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.04377413806533306, 0.20518522040738324, 0.20518522040738324, 0.2968960626471842, 0.04377413806533306, 0.20518522040738324]
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.04377413806533306, 0.20518522040738324, 0.20518522040738324, 0.2968960626471842, 0.04377413806533306, 0.20518522040738324]
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.04778319307802445, 0.22402224049441125, 0.13223106996504327, 0.3241580628900853, 0.04778319307802445, 0.22402224049441125]
printing an ep nov before normalisation:  61.01920264107841
maxi score, test score, baseline:  -0.9817181818181818 -1.0 -0.9817181818181818
probs:  [0.047783191019105636, 0.22402224148774078, 0.13223106936865997, 0.3241580656176472, 0.047783191019105636, 0.22402224148774078]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
using another actor
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.03521673957538093, 0.2343832957742988, 0.2343832957742988, 0.2343832957742988, 0.13081668655086132, 0.13081668655086132]
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.03521673698275049, 0.23438329710989622, 0.23438329710989622, 0.23438329710989622, 0.1308166858437804, 0.1308166858437804]
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.03521673698275049, 0.23438329710989622, 0.23438329710989622, 0.23438329710989622, 0.1308166858437804, 0.1308166858437804]
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.03521673698275049, 0.23438329710989622, 0.23438329710989622, 0.23438329710989622, 0.1308166858437804, 0.1308166858437804]
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.03521673698275049, 0.23438329710989622, 0.23438329710989622, 0.23438329710989622, 0.1308166858437804, 0.1308166858437804]
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.03927361752930357, 0.14592826331872386, 0.2614707962572625, 0.2614707962572625, 0.14592826331872386, 0.14592826331872386]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.03927361752930357, 0.14592826331872386, 0.2614707962572625, 0.2614707962572625, 0.14592826331872386, 0.14592826331872386]
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
actions average: 
K:  1  action  0 :  tensor([0.3158, 0.0111, 0.0888, 0.0742, 0.2687, 0.1068, 0.1345],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0202, 0.8272, 0.0153, 0.0361, 0.0342, 0.0258, 0.0412],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1148, 0.0211, 0.3022, 0.0587, 0.1939, 0.1695, 0.1399],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1582, 0.1762, 0.0893, 0.0577, 0.2309, 0.1501, 0.1377],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1516, 0.0550, 0.1010, 0.0722, 0.3397, 0.1339, 0.1466],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0914, 0.0515, 0.0927, 0.0737, 0.2010, 0.3586, 0.1311],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1438, 0.0155, 0.1637, 0.0496, 0.2204, 0.1273, 0.2797],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.04395069248554156, 0.04395069248554156, 0.2926992887986331, 0.2926992887986331, 0.16335001871582533, 0.16335001871582533]
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.0584746493384195, 0.0584746493384195, 0.38967470238407415, 0.0584746493384195, 0.21745067480033373, 0.21745067480033373]
printing an ep nov before normalisation:  17.567687034606934
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.05989487194938016, 0.05989487194938016, 0.38021025610123965, 0.05989487194938016, 0.38021025610123965, 0.05989487194938016]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.08809194087743945, 0.08809194087743945, 0.5595402956128028, 0.08809194087743945, 0.08809194087743945, 0.08809194087743945]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.16505830569050475, 0.16505830569050475, 0.2953355447596111, 0.16505830569050475, 0.04443123247836996, 0.16505830569050475]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.19385913136477634, 0.19385913136477634, 0.19385913136477634, 0.19385913136477634, 0.030704343176118226, 0.19385913136477634]
printing an ep nov before normalisation:  65.54899803191506
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.0882703034573181, 0.5586484827134095, 0.0882703034573181, 0.0882703034573181, 0.0882703034573181, 0.0882703034573181]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.23160372440032767, 0.03679255119934463, 0.23160372440032767, 0.23160372440032767, 0.03679255119934463, 0.23160372440032767]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.23160372440032767, 0.03679255119934463, 0.23160372440032767, 0.23160372440032767, 0.03679255119934463, 0.23160372440032767]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.287662438615497, 0.04567089471783633, 0.287662438615497, 0.287662438615497, 0.04567089471783633, 0.04567089471783633]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.287662438615497, 0.04567089471783633, 0.287662438615497, 0.287662438615497, 0.04567089471783633, 0.04567089471783633]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.287662438615497, 0.04567089471783633, 0.287662438615497, 0.287662438615497, 0.04567089471783633, 0.04567089471783633]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.287662438615497, 0.04567089471783633, 0.287662438615497, 0.287662438615497, 0.04567089471783633, 0.04567089471783633]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.2609462938449111, 0.1462284258098444, 0.03942213488064462, 0.2609462938449111, 0.1462284258098444, 0.1462284258098444]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.2609462938449111, 0.1462284258098444, 0.03942213488064462, 0.2609462938449111, 0.1462284258098444, 0.1462284258098444]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.2609462938449111, 0.1462284258098444, 0.03942213488064462, 0.2609462938449111, 0.1462284258098444, 0.1462284258098444]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.2947738821012997, 0.16517704788254306, 0.044517926368528075, 0.16517704788254306, 0.16517704788254306, 0.16517704788254306]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.23154270740650532, 0.23154270740650532, 0.036914585186989354, 0.23154270740650532, 0.036914585186989354, 0.23154270740650532]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.23154270740650532, 0.23154270740650532, 0.036914585186989354, 0.23154270740650532, 0.036914585186989354, 0.23154270740650532]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.23154270740650532, 0.23154270740650532, 0.036914585186989354, 0.23154270740650532, 0.036914585186989354, 0.23154270740650532]
Printing some Q and Qe and total Qs values:  [[1.191]
 [1.191]
 [1.191]
 [1.191]
 [1.191]
 [1.191]
 [1.101]] [[13.723]
 [13.723]
 [13.723]
 [13.723]
 [13.723]
 [13.723]
 [34.939]] [[1.079]
 [1.079]
 [1.079]
 [1.079]
 [1.079]
 [1.079]
 [2.6  ]]
actions average: 
K:  3  action  0 :  tensor([0.2702, 0.1057, 0.0814, 0.0921, 0.2194, 0.1248, 0.1064],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0339, 0.7450, 0.0253, 0.0504, 0.0327, 0.0410, 0.0716],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0702, 0.0536, 0.4623, 0.1014, 0.1060, 0.0960, 0.1106],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1327, 0.0013, 0.0674, 0.0694, 0.2712, 0.2101, 0.2480],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0936, 0.0935, 0.0895, 0.1120, 0.2990, 0.1581, 0.1542],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0643, 0.0335, 0.1217, 0.0749, 0.1296, 0.4684, 0.1076],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0861, 0.1698, 0.1067, 0.0660, 0.1109, 0.1609, 0.2997],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.06038782892199797, 0.379224342156004, 0.06038782892199797, 0.379224342156004, 0.06038782892199797, 0.06038782892199797]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.03947404511902445, 0.2607892066119218, 0.146315847219044, 0.2607892066119218, 0.146315847219044, 0.146315847219044]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
printing an ep nov before normalisation:  42.59347758624843
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.03947404511902445, 0.2607892066119218, 0.146315847219044, 0.2607892066119218, 0.146315847219044, 0.146315847219044]
actions average: 
K:  0  action  0 :  tensor([0.3288, 0.0332, 0.0845, 0.0958, 0.2302, 0.1313, 0.0961],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0099, 0.9055, 0.0049, 0.0266, 0.0155, 0.0144, 0.0232],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1063, 0.0970, 0.4348, 0.0805, 0.1008, 0.0768, 0.1038],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1971, 0.0458, 0.0907, 0.1777, 0.1945, 0.1526, 0.1416],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1647, 0.0192, 0.0760, 0.1181, 0.3459, 0.1623, 0.1138],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1110, 0.0411, 0.1000, 0.0871, 0.1508, 0.4143, 0.0958],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1701, 0.0356, 0.1074, 0.1140, 0.2144, 0.1276, 0.2309],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.03947404511902445, 0.2607892066119218, 0.146315847219044, 0.2607892066119218, 0.146315847219044, 0.146315847219044]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
printing an ep nov before normalisation:  47.68244307412761
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.03947404511902445, 0.2607892066119218, 0.146315847219044, 0.2607892066119218, 0.146315847219044, 0.146315847219044]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
main train batch thing paused
add a thread
Adding thread: now have 3 threads
Printing some Q and Qe and total Qs values:  [[0.526]
 [0.527]
 [0.533]
 [0.546]
 [0.556]
 [0.551]
 [0.553]] [[59.176]
 [21.302]
 [53.429]
 [49.246]
 [47.65 ]
 [50.   ]
 [51.538]] [[0.992]
 [0.471]
 [0.919]
 [0.875]
 [0.863]
 [0.891]
 [0.913]]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.04418430712834251, 0.2919974531709985, 0.04418430712834251, 0.2919974531709985, 0.16381823970065887, 0.16381823970065887]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
main train batch thing paused
add a thread
Adding thread: now have 4 threads
printing an ep nov before normalisation:  55.222664794065025
Printing some Q and Qe and total Qs values:  [[0.87 ]
 [0.986]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [0.87 ]] [[55.488]
 [52.268]
 [55.488]
 [55.488]
 [55.488]
 [55.488]
 [55.488]] [[2.87 ]
 [2.853]
 [2.87 ]
 [2.87 ]
 [2.87 ]
 [2.87 ]
 [2.87 ]]
printing an ep nov before normalisation:  61.8666574065375
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.088800782520703, 0.088800782520703, 0.088800782520703, 0.555996087396485, 0.088800782520703, 0.088800782520703]
printing an ep nov before normalisation:  64.47170050063463
siam score:  -0.531371
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.088800782520703, 0.088800782520703, 0.088800782520703, 0.555996087396485, 0.088800782520703, 0.088800782520703]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.088800782520703, 0.088800782520703, 0.088800782520703, 0.555996087396485, 0.088800782520703, 0.088800782520703]
printing an ep nov before normalisation:  69.88215994309502
printing an ep nov before normalisation:  58.33770671178352
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.1652796860697358, 0.1652796860697358, 0.1652796860697358, 0.294268881584319, 0.04461237413673781, 0.1652796860697358]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
printing an ep nov before normalisation:  43.97735203214315
using explorer policy with actor:  0
main train batch thing paused
add a thread
Adding thread: now have 5 threads
printing an ep nov before normalisation:  35.74450260402251
printing an ep nov before normalisation:  58.455175728515435
printing an ep nov before normalisation:  33.27457406450993
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.2244828732523436, 0.13333886468239736, 0.2244828732523436, 0.3219126755167682, 0.0478913566480736, 0.0478913566480736]
printing an ep nov before normalisation:  50.277515173438175
printing an ep nov before normalisation:  37.92487621307373
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.21472066366983883, 0.15320126213039031, 0.21472066366983883, 0.28048278255683495, 0.04134780478593918, 0.09552682318715773]
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.21472066366983883, 0.15320126213039031, 0.21472066366983883, 0.28048278255683495, 0.04134780478593918, 0.09552682318715773]
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.21472066366983883, 0.15320126213039031, 0.21472066366983883, 0.28048278255683495, 0.04134780478593918, 0.09552682318715773]
main train batch thing paused
add a thread
Adding thread: now have 6 threads
printing an ep nov before normalisation:  55.979576110839844
Printing some Q and Qe and total Qs values:  [[-0.03 ]
 [-0.03 ]
 [-0.028]
 [-0.028]
 [-0.029]
 [-0.035]
 [-0.038]] [[20.459]
 [19.818]
 [20.675]
 [20.281]
 [19.769]
 [21.399]
 [21.374]] [[0.852]
 [0.801]
 [0.87 ]
 [0.839]
 [0.798]
 [0.921]
 [0.917]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.66122060936076
printing an ep nov before normalisation:  33.514437682888996
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.23462242197871525, 0.16378206895845543, 0.23462242197871525, 0.23462242197871525, 0.0349814271034372, 0.09736923800196154]
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.23462242197871525, 0.16378206895845543, 0.23462242197871525, 0.23462242197871525, 0.0349814271034372, 0.09736923800196154]
printing an ep nov before normalisation:  27.406452599646553
Printing some Q and Qe and total Qs values:  [[1.052]
 [1.09 ]
 [1.181]
 [1.181]
 [1.206]
 [1.22 ]
 [1.223]] [[42.464]
 [33.784]
 [23.143]
 [23.143]
 [42.306]
 [41.963]
 [46.066]] [[1.3  ]
 [1.271]
 [1.278]
 [1.278]
 [1.452]
 [1.464]
 [1.499]]
Printing some Q and Qe and total Qs values:  [[1.037]
 [1.067]
 [1.168]
 [1.185]
 [1.181]
 [1.184]
 [1.185]] [[61.098]
 [54.003]
 [40.802]
 [50.619]
 [51.811]
 [54.792]
 [60.539]] [[1.356]
 [1.336]
 [1.344]
 [1.429]
 [1.434]
 [1.458]
 [1.5  ]]
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.23462242197871525, 0.16378206895845543, 0.23462242197871525, 0.23462242197871525, 0.0349814271034372, 0.09736923800196154]
Printing some Q and Qe and total Qs values:  [[0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]] [[26.623]
 [26.623]
 [26.623]
 [26.623]
 [26.623]
 [26.623]
 [26.623]] [[0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.38]]
from probs:  [0.23462242197871525, 0.16378206895845543, 0.23462242197871525, 0.23462242197871525, 0.0349814271034372, 0.09736923800196154]
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.2525155800676915, 0.17626961492767296, 0.2525155800676915, 0.17626961492767296, 0.0376405874003658, 0.10478902260890528]
Printing some Q and Qe and total Qs values:  [[0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.894]
 [0.861]] [[84.614]
 [84.614]
 [84.614]
 [84.614]
 [84.614]
 [87.562]
 [84.614]] [[1.464]
 [1.464]
 [1.464]
 [1.464]
 [1.464]
 [1.533]
 [1.464]]
printing an ep nov before normalisation:  78.78046035766602
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.20578210428722743, 0.20578210428722743, 0.2948034450788493, 0.20578210428722743, 0.04392512102973423, 0.04392512102973423]
printing an ep nov before normalisation:  25.46758592621159
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
printing an ep nov before normalisation:  67.47652690103683
printing an ep nov before normalisation:  46.16636899618001
Printing some Q and Qe and total Qs values:  [[1.198]
 [1.192]
 [1.196]
 [1.203]
 [1.194]
 [1.194]
 [1.202]] [[27.755]
 [27.762]
 [28.434]
 [29.699]
 [30.096]
 [30.687]
 [31.693]] [[1.965]
 [1.959]
 [2.003]
 [2.084]
 [2.099]
 [2.134]
 [2.202]]
siam score:  -0.5535832
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
probs:  [0.1468300954802201, 0.1468300954802201, 0.3538996089877191, 0.24702502137094534, 0.05270758934044766, 0.05270758934044766]
printing an ep nov before normalisation:  42.92784150652412
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
probs:  [0.1468300954802201, 0.1468300954802201, 0.3538996089877191, 0.24702502137094534, 0.05270758934044766, 0.05270758934044766]
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
probs:  [0.1468300954802201, 0.1468300954802201, 0.3538996089877191, 0.24702502137094534, 0.05270758934044766, 0.05270758934044766]
Printing some Q and Qe and total Qs values:  [[0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.318]
 [0.312]] [[25.023]
 [25.023]
 [25.023]
 [25.023]
 [25.023]
 [22.948]
 [22.905]] [[0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.318]
 [0.312]]
printing an ep nov before normalisation:  21.384724339810532
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
probs:  [0.1640919665973949, 0.1640919665973949, 0.2915396200263288, 0.2915396200263288, 0.044368413376276235, 0.044368413376276235]
printing an ep nov before normalisation:  66.24051810307692
siam score:  -0.56172544
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
probs:  [0.1640919665973949, 0.1640919665973949, 0.2915396200263288, 0.2915396200263288, 0.044368413376276235, 0.044368413376276235]
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
probs:  [0.1640919665973949, 0.1640919665973949, 0.2915396200263288, 0.2915396200263288, 0.044368413376276235, 0.044368413376276235]
printing an ep nov before normalisation:  35.01700883780618
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
probs:  [0.18990271435053196, 0.1129682821239539, 0.27180065833366335, 0.27180065833366335, 0.04055940473423355, 0.1129682821239539]
actions average: 
K:  1  action  0 :  tensor([0.2990, 0.0370, 0.1021, 0.1084, 0.2229, 0.1215, 0.1091],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0508, 0.6668, 0.0259, 0.0524, 0.0396, 0.0492, 0.1153],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0802, 0.0096, 0.5194, 0.0630, 0.0902, 0.1735, 0.0642],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1373, 0.0675, 0.1241, 0.1196, 0.1905, 0.2239, 0.1370],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1815, 0.0177, 0.1008, 0.1252, 0.1905, 0.2245, 0.1599],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0763, 0.0501, 0.0612, 0.0754, 0.0898, 0.5604, 0.0869],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1767, 0.0255, 0.0925, 0.0928, 0.1421, 0.1568, 0.3135],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  24.584475634603166
printing an ep nov before normalisation:  51.267730137203195
using explorer policy with actor:  1
printing an ep nov before normalisation:  68.70861530303955
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
printing an ep nov before normalisation:  37.42138200718636
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
probs:  [0.2047291253327801, 0.043717842567374085, 0.2930256352363897, 0.2930256352363897, 0.043717842567374085, 0.12178391905969226]
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
probs:  [0.2047291253327801, 0.043717842567374085, 0.2930256352363897, 0.2930256352363897, 0.043717842567374085, 0.12178391905969226]
printing an ep nov before normalisation:  68.56815180517965
printing an ep nov before normalisation:  28.782196044921875
printing an ep nov before normalisation:  25.355873107910156
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
main train batch thing paused
printing an ep nov before normalisation:  20.131325535299045
line 256 mcts: sample exp_bonus 17.242155100750022
printing an ep nov before normalisation:  14.901080131530762
printing an ep nov before normalisation:  12.572214972933873
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
probs:  [0.22875978988893916, 0.04405055317370999, 0.16345854458557516, 0.2982740187602619, 0.10199854900593863, 0.16345854458557516]
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
probs:  [0.22875978988893916, 0.04405055317370999, 0.16345854458557516, 0.2982740187602619, 0.10199854900593863, 0.16345854458557516]
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
probs:  [0.22875978988893916, 0.04405055317370999, 0.16345854458557516, 0.2982740187602619, 0.10199854900593863, 0.16345854458557516]
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
probs:  [0.22875978988893916, 0.04405055317370999, 0.16345854458557516, 0.2982740187602619, 0.10199854900593863, 0.16345854458557516]
printing an ep nov before normalisation:  60.22230824083522
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
probs:  [0.24283513004026971, 0.046754312961907116, 0.1735136290529699, 0.31662898592997624, 0.046754312961907116, 0.1735136290529699]
Printing some Q and Qe and total Qs values:  [[0.674]
 [0.674]
 [0.674]
 [0.991]
 [0.674]
 [0.674]
 [0.674]] [[44.392]
 [44.392]
 [44.392]
 [47.21 ]
 [44.392]
 [44.392]
 [44.392]] [[1.767]
 [1.767]
 [1.767]
 [2.223]
 [1.767]
 [1.767]
 [1.767]]
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
probs:  [0.24283513004026971, 0.046754312961907116, 0.1735136290529699, 0.31662898592997624, 0.046754312961907116, 0.1735136290529699]
printing an ep nov before normalisation:  10.93016805275937
using another actor
printing an ep nov before normalisation:  36.79804180904189
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
probs:  [0.2704586555277899, 0.040412232455001924, 0.1891291120172082, 0.2704586555277899, 0.040412232455001924, 0.1891291120172082]
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
probs:  [0.2704586555277899, 0.040412232455001924, 0.1891291120172082, 0.2704586555277899, 0.040412232455001924, 0.1891291120172082]
Printing some Q and Qe and total Qs values:  [[0.795]
 [0.61 ]
 [0.578]
 [0.609]
 [0.618]
 [0.606]
 [0.636]] [[16.306]
 [16.984]
 [17.783]
 [17.777]
 [17.242]
 [16.753]
 [16.42 ]] [[1.923]
 [1.832]
 [1.911]
 [1.941]
 [1.876]
 [1.796]
 [1.78 ]]
printing an ep nov before normalisation:  36.986665391623575
printing an ep nov before normalisation:  39.012853524336826
Printing some Q and Qe and total Qs values:  [[1.159]
 [1.045]
 [1.155]
 [1.045]
 [1.045]
 [1.045]
 [1.079]] [[79.648]
 [69.207]
 [71.909]
 [69.207]
 [69.207]
 [69.207]
 [73.809]] [[2.753]
 [2.255]
 [2.463]
 [2.255]
 [2.255]
 [2.255]
 [2.458]]
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
siam score:  -0.56349504
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.31751868975842495, 0.04693179226546611, 0.2437222631694365, 0.10878022597814242, 0.10878022597814242, 0.17426680285038768]
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.31751868975842495, 0.04693179226546611, 0.2437222631694365, 0.10878022597814242, 0.10878022597814242, 0.17426680285038768]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
printing an ep nov before normalisation:  59.926485516424414
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.3175186925141926, 0.046931790078147115, 0.24372226457708923, 0.10878022492067185, 0.10878022492067185, 0.17426680298922728]
printing an ep nov before normalisation:  70.75006580860209
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.3175186925141926, 0.046931790078147115, 0.24372226457708923, 0.10878022492067185, 0.10878022492067185, 0.17426680298922728]
using explorer policy with actor:  1
printing an ep nov before normalisation:  16.561490184190895
printing an ep nov before normalisation:  19.62013095431479
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.3175186925141926, 0.046931790078147115, 0.24372226457708923, 0.10878022492067185, 0.10878022492067185, 0.17426680298922728]
using explorer policy with actor:  1
printing an ep nov before normalisation:  52.10185527801514
printing an ep nov before normalisation:  101.20675199161877
from probs:  [0.3397787065166157, 0.05021403450113261, 0.26080652323966574, 0.11640024524752865, 0.11640024524752865, 0.11640024524752865]
printing an ep nov before normalisation:  18.514698484004555
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.3638724896460746, 0.05376666862331602, 0.2792981748216858, 0.12464799914280367, 0.12464799914280367, 0.05376666862331602]
Printing some Q and Qe and total Qs values:  [[0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.775]
 [0.755]
 [0.755]] [[31.677]
 [31.677]
 [31.677]
 [31.677]
 [38.045]
 [31.677]
 [31.677]] [[1.675]
 [1.675]
 [1.675]
 [1.675]
 [1.921]
 [1.675]
 [1.675]]
Printing some Q and Qe and total Qs values:  [[1.057]
 [1.057]
 [1.057]
 [1.057]
 [1.057]
 [1.057]
 [1.057]] [[67.924]
 [67.924]
 [67.924]
 [67.924]
 [67.924]
 [67.924]
 [67.924]] [[1.684]
 [1.684]
 [1.684]
 [1.684]
 [1.684]
 [1.684]
 [1.684]]
printing an ep nov before normalisation:  32.336952335566096
Printing some Q and Qe and total Qs values:  [[0.212]
 [0.213]
 [0.217]
 [0.229]
 [0.237]
 [0.241]
 [0.245]] [[21.8  ]
 [22.713]
 [23.051]
 [22.569]
 [22.537]
 [22.272]
 [21.78 ]] [[1.916]
 [2.063]
 [2.121]
 [2.057]
 [2.059]
 [2.021]
 [1.946]]
printing an ep nov before normalisation:  19.50692275796726
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.3152717165026235, 0.0463062274902488, 0.2549582432089396, 0.14467074918620307, 0.14467074918620307, 0.09412231442578196]
printing an ep nov before normalisation:  36.84158239606035
printing an ep nov before normalisation:  13.011864423751831
printing an ep nov before normalisation:  13.282850371062276
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.376]
 [0.332]
 [0.413]
 [0.417]
 [0.42 ]
 [0.421]
 [0.418]] [[23.596]
 [20.334]
 [22.369]
 [20.498]
 [18.737]
 [18.098]
 [18.059]] [[0.376]
 [0.332]
 [0.413]
 [0.417]
 [0.42 ]
 [0.421]
 [0.418]]
Printing some Q and Qe and total Qs values:  [[1.062]
 [0.969]
 [0.927]
 [0.966]
 [0.96 ]
 [0.919]
 [0.95 ]] [[56.856]
 [59.796]
 [58.623]
 [57.889]
 [56.998]
 [55.261]
 [56.023]] [[2.577]
 [2.636]
 [2.533]
 [2.534]
 [2.482]
 [2.352]
 [2.422]]
printing an ep nov before normalisation:  57.28272872731651
actions average: 
K:  1  action  0 :  tensor([0.4514, 0.0380, 0.0701, 0.0655, 0.1477, 0.1020, 0.1252],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0642, 0.6744, 0.0321, 0.0670, 0.0677, 0.0518, 0.0427],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1349, 0.0361, 0.4242, 0.0616, 0.1392, 0.0955, 0.1085],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2411, 0.0022, 0.0981, 0.0974, 0.2208, 0.1887, 0.1517],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1946, 0.0276, 0.1015, 0.0904, 0.2631, 0.1680, 0.1548],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1243, 0.0078, 0.0754, 0.0653, 0.1584, 0.4816, 0.0872],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2363, 0.0357, 0.1035, 0.0800, 0.2158, 0.1649, 0.1637],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
from probs:  [0.28283893877859867, 0.041587443154739126, 0.28283893877859867, 0.13004632488348766, 0.17806686067909402, 0.08462149372548165]
Printing some Q and Qe and total Qs values:  [[0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]] [[19.012]
 [19.012]
 [19.012]
 [19.012]
 [20.388]
 [19.012]
 [19.012]] [[0.84 ]
 [0.84 ]
 [0.84 ]
 [0.84 ]
 [0.874]
 [0.84 ]
 [0.84 ]]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.25490635559625235, 0.04629753686937277, 0.31496040947217224, 0.14480725682373272, 0.14480725682373272, 0.09422118441473715]
printing an ep nov before normalisation:  16.923152585555044
printing an ep nov before normalisation:  12.72552490234375
line 256 mcts: sample exp_bonus 67.66508618299082
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.2677413739286374, 0.04862321293005857, 0.3308208445191372, 0.15209567784605418, 0.15209567784605418, 0.04862321293005857]
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.297]
 [0.406]
 [0.404]
 [0.403]
 [0.399]
 [0.396]] [[25.989]
 [31.452]
 [24.105]
 [32.142]
 [31.195]
 [30.333]
 [29.495]] [[0.384]
 [0.297]
 [0.406]
 [0.404]
 [0.403]
 [0.399]
 [0.396]]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.2943722668046668, 0.04360490653368477, 0.2943722668046668, 0.1620228266616484, 0.1620228266616484, 0.04360490653368477]
siam score:  -0.56941164
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.24283999613556814, 0.04678266028757282, 0.3158809643926645, 0.17385685944831084, 0.17385685944831084, 0.04678266028757282]
line 256 mcts: sample exp_bonus 16.481892160796388
siam score:  -0.5672709
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.20598883674990234, 0.04407401876010823, 0.29388545223007645, 0.20598883674990234, 0.20598883674990234, 0.04407401876010823]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.20598883674990234, 0.04407401876010823, 0.29388545223007645, 0.20598883674990234, 0.20598883674990234, 0.04407401876010823]
printing an ep nov before normalisation:  17.387844124385673
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.13398113709519668, 0.04806229023834125, 0.3205477759843674, 0.22467325322187676, 0.22467325322187676, 0.04806229023834125]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.13398113709519668, 0.04806229023834125, 0.3205477759843674, 0.22467325322187676, 0.22467325322187676, 0.04806229023834125]
Printing some Q and Qe and total Qs values:  [[-0.017]
 [-0.06 ]
 [-0.06 ]
 [ 0.322]
 [ 0.354]
 [ 0.419]
 [-0.06 ]] [[19.675]
 [17.709]
 [17.709]
 [11.387]
 [12.717]
 [12.511]
 [17.709]] [[0.739]
 [0.607]
 [0.607]
 [0.701]
 [0.794]
 [0.85 ]
 [0.607]]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.1459948339592501, 0.039682551463963545, 0.25821335437094095, 0.25821335437094095, 0.25821335437094095, 0.039682551463963545]
using explorer policy with actor:  1
from probs:  [0.1459948339592501, 0.039682551463963545, 0.25821335437094095, 0.25821335437094095, 0.25821335437094095, 0.039682551463963545]
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.14599483329832108, 0.039682547403972604, 0.2582133572979112, 0.2582133572979112, 0.2582133572979112, 0.039682547403972604]
Printing some Q and Qe and total Qs values:  [[1.285]
 [1.285]
 [1.285]
 [1.285]
 [1.285]
 [1.285]
 [1.285]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.285]
 [1.285]
 [1.285]
 [1.285]
 [1.285]
 [1.285]
 [1.285]]
printing an ep nov before normalisation:  54.42536380501295
printing an ep nov before normalisation:  18.57971609380072
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.07043204092765576, 0.07043204092765576, 0.07043204092765576, 0.4588699848196633, 0.2594018514697137, 0.07043204092765576]
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.07043204092765576, 0.07043204092765576, 0.07043204092765576, 0.4588699848196633, 0.2594018514697137, 0.07043204092765576]
line 256 mcts: sample exp_bonus 17.3048640795628
printing an ep nov before normalisation:  30.276509731613565
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.07043204092765576, 0.07043204092765576, 0.07043204092765576, 0.4588699848196633, 0.2594018514697137, 0.07043204092765576]
siam score:  -0.58731705
printing an ep nov before normalisation:  39.92274256301718
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.13469663615013433, 0.13469663615013433, 0.048319190616623285, 0.3218477681394086, 0.22574313279356523, 0.13469663615013433]
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
Printing some Q and Qe and total Qs values:  [[0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.379]
 [0.366]
 [0.366]] [[30.894]
 [30.894]
 [30.894]
 [30.894]
 [37.187]
 [30.894]
 [30.894]] [[0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.379]
 [0.366]
 [0.366]]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.2309381662029006, 0.038123667594198746, 0.038123667594198746, 0.2309381662029006, 0.2309381662029006, 0.2309381662029006]
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
printing an ep nov before normalisation:  13.48140332017919
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9862013698630137 -1.0 -0.9862013698630137
probs:  [0.2118334887240746, 0.11996876589544836, 0.032697279208253466, 0.2118334887240746, 0.2118334887240746, 0.2118334887240746]
from probs:  [0.2118334887240746, 0.11996876589544836, 0.032697279208253466, 0.2118334887240746, 0.2118334887240746, 0.2118334887240746]
maxi score, test score, baseline:  -0.9862013698630137 -1.0 -0.9862013698630137
probs:  [0.25953054052905866, 0.1469682691807047, 0.040034111399768556, 0.1469682691807047, 0.25953054052905866, 0.1469682691807047]
maxi score, test score, baseline:  -0.9862013698630137 -1.0 -0.9862013698630137
maxi score, test score, baseline:  -0.9862013698630137 -1.0 -0.9862013698630137
probs:  [0.25953054052905866, 0.1469682691807047, 0.040034111399768556, 0.1469682691807047, 0.25953054052905866, 0.1469682691807047]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9862013698630137 -1.0 -0.9862013698630137
maxi score, test score, baseline:  -0.9862013698630137 -1.0 -0.9862013698630137
probs:  [0.25953054052905866, 0.1469682691807047, 0.040034111399768556, 0.1469682691807047, 0.25953054052905866, 0.1469682691807047]
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.541]
 [0.542]
 [0.541]
 [0.545]
 [0.538]
 [0.537]] [[23.102]
 [21.394]
 [21.258]
 [22.364]
 [22.754]
 [23.219]
 [24.698]] [[0.534]
 [0.541]
 [0.542]
 [0.541]
 [0.545]
 [0.538]
 [0.537]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.25953054339842113, 0.1469682685720521, 0.04003410748700124, 0.1469682685720521, 0.25953054339842113, 0.1469682685720521]
actions average: 
K:  2  action  0 :  tensor([0.4201, 0.0098, 0.0609, 0.0596, 0.2109, 0.1161, 0.1226],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0641, 0.6863, 0.0285, 0.0557, 0.0537, 0.0512, 0.0605],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1322, 0.0210, 0.3264, 0.0605, 0.1466, 0.1816, 0.1318],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2162, 0.0224, 0.1234, 0.0922, 0.2209, 0.1633, 0.1615],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1822, 0.0160, 0.0819, 0.0969, 0.3527, 0.1443, 0.1260],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1073, 0.0057, 0.1729, 0.0671, 0.1433, 0.3299, 0.1736],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1528, 0.0579, 0.0924, 0.0760, 0.1761, 0.1723, 0.2725],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  22.829162146067574
printing an ep nov before normalisation:  44.73328323761831
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.25953054339842113, 0.1469682685720521, 0.04003410748700124, 0.1469682685720521, 0.25953054339842113, 0.1469682685720521]
printing an ep nov before normalisation:  22.361568709435623
siam score:  -0.59774035
printing an ep nov before normalisation:  49.529437731590335
printing an ep nov before normalisation:  25.82663788616092
printing an ep nov before normalisation:  29.33080300482099
Printing some Q and Qe and total Qs values:  [[1.214]
 [1.214]
 [1.214]
 [1.214]
 [1.214]
 [1.197]
 [1.193]] [[49.638]
 [49.638]
 [49.638]
 [49.638]
 [49.638]
 [55.238]
 [58.481]] [[2.615]
 [2.615]
 [2.615]
 [2.615]
 [2.615]
 [2.822]
 [2.947]]
printing an ep nov before normalisation:  31.27966304949723
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.25157798742325316, 0.17669652154721707, 0.10555912896498267, 0.17669652154721707, 0.25157798742325316, 0.03789185309407675]
printing an ep nov before normalisation:  24.80140252801224
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.2708515926567877, 0.19023035392268972, 0.11364017712529643, 0.11364017712529643, 0.2708515926567877, 0.04078610651314211]
printing an ep nov before normalisation:  23.47399419105728
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.29214268997208853, 0.2051808594800931, 0.12256712051269744, 0.04398332003151621, 0.29214268997208853, 0.04398332003151621]
using explorer policy with actor:  1
printing an ep nov before normalisation:  57.22157803567707
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.22472762432307117, 0.22472762432307117, 0.13423848924936854, 0.048163458325602294, 0.3199793454532845, 0.048163458325602294]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.22472762432307117, 0.22472762432307117, 0.13423848924936854, 0.048163458325602294, 0.3199793454532845, 0.048163458325602294]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.22472762432307117, 0.22472762432307117, 0.13423848924936854, 0.048163458325602294, 0.3199793454532845, 0.048163458325602294]
Printing some Q and Qe and total Qs values:  [[1.093]
 [0.741]
 [1.006]
 [1.112]
 [1.133]
 [1.157]
 [1.17 ]] [[17.136]
 [19.176]
 [18.476]
 [18.194]
 [19.757]
 [19.627]
 [19.411]] [[1.293]
 [0.985]
 [1.235]
 [1.335]
 [1.39 ]
 [1.411]
 [1.419]]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.05847228616602488, 0.2729327839199593, 0.16302177882106791, 0.05847228616602488, 0.38862857876089807, 0.05847228616602488]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.1093220324770991, 0.2435825468960666, 0.17477403325634602, 0.1093220324770991, 0.3160125612536676, 0.04698679363972179]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.1093220324770991, 0.2435825468960666, 0.17477403325634602, 0.1093220324770991, 0.3160125612536676, 0.04698679363972179]
Printing some Q and Qe and total Qs values:  [[1.124]
 [0.963]
 [1.11 ]
 [1.154]
 [1.152]
 [1.153]
 [1.166]] [[30.925]
 [32.188]
 [32.854]
 [34.115]
 [34.486]
 [33.43 ]
 [33.454]] [[1.351]
 [1.208]
 [1.364]
 [1.427]
 [1.43 ]
 [1.415]
 [1.429]]
printing an ep nov before normalisation:  30.802651765167933
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.11372090978491592, 0.2707356874748384, 0.1902656139087534, 0.11372090978491592, 0.2707356874748384, 0.040821191571737955]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.12314428772746014, 0.29318521009459236, 0.12314428772746014, 0.12314428772746014, 0.29318521009459236, 0.04419671662843479]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.12314428772746014, 0.29318521009459236, 0.12314428772746014, 0.12314428772746014, 0.29318521009459236, 0.04419671662843479]
printing an ep nov before normalisation:  26.675159163407205
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.12314428772746014, 0.29318521009459236, 0.12314428772746014, 0.12314428772746014, 0.29318521009459236, 0.04419671662843479]
Printing some Q and Qe and total Qs values:  [[1.104]
 [0.954]
 [1.067]
 [1.115]
 [1.109]
 [1.002]
 [1.123]] [[20.345]
 [19.816]
 [20.674]
 [21.135]
 [23.251]
 [38.795]
 [23.442]] [[1.425]
 [1.26 ]
 [1.397]
 [1.459]
 [1.513]
 [1.848]
 [1.532]]
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.12314428666134915, 0.29318521319375246, 0.12314428666134915, 0.12314428666134915, 0.29318521319375246, 0.044196713628447605]
printing an ep nov before normalisation:  31.177997589111328
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.12314428666134915, 0.29318521319375246, 0.12314428666134915, 0.12314428666134915, 0.29318521319375246, 0.044196713628447605]
printing an ep nov before normalisation:  50.54935308482209
Printing some Q and Qe and total Qs values:  [[0.796]
 [0.803]
 [0.8  ]
 [0.803]
 [0.8  ]
 [0.802]
 [0.799]] [[18.992]
 [18.076]
 [18.855]
 [18.076]
 [20.129]
 [19.416]
 [19.262]] [[1.02 ]
 [1.007]
 [1.021]
 [1.007]
 [1.049]
 [1.035]
 [1.029]]
actions average: 
K:  2  action  0 :  tensor([0.3724, 0.0480, 0.0846, 0.0897, 0.1811, 0.1113, 0.1130],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0156, 0.8730, 0.0106, 0.0277, 0.0170, 0.0170, 0.0391],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0751, 0.0134, 0.4497, 0.0901, 0.1051, 0.1647, 0.1019],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1290, 0.1508, 0.1341, 0.1195, 0.1567, 0.1628, 0.1470],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2510, 0.0560, 0.0727, 0.1046, 0.3048, 0.1039, 0.1070],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1098, 0.0246, 0.0791, 0.0989, 0.1357, 0.4442, 0.1077],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1511, 0.2137, 0.0917, 0.1483, 0.1470, 0.1163, 0.1321],
       grad_fn=<DivBackward0>)
siam score:  -0.61283255
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.1348978790154864, 0.2257133662277607, 0.1348978790154864, 0.1348978790154864, 0.32118605791246035, 0.04840693881331968]
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.1348978790154864, 0.2257133662277607, 0.1348978790154864, 0.1348978790154864, 0.32118605791246035, 0.04840693881331968]
printing an ep nov before normalisation:  35.80422801636587
printing an ep nov before normalisation:  68.20548105656324
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.1348978790154864, 0.2257133662277607, 0.1348978790154864, 0.1348978790154864, 0.32118605791246035, 0.04840693881331968]
printing an ep nov before normalisation:  46.988567193291885
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.082 0.061 0.306 0.041 0.265 0.122 0.122]
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.14837092789089001, 0.14837092789089001, 0.14837092789089001, 0.14837092789089001, 0.3532832021795881, 0.053233086256851844]
printing an ep nov before normalisation:  43.41053598550924
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
printing an ep nov before normalisation:  36.01662806213656
Printing some Q and Qe and total Qs values:  [[1.345]
 [1.345]
 [1.345]
 [1.345]
 [1.343]
 [1.343]
 [1.344]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.345]
 [1.345]
 [1.345]
 [1.345]
 [1.343]
 [1.343]
 [1.344]]
printing an ep nov before normalisation:  20.863908799571693
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  12.735889864558022
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.04762044279920201, 0.28571289053413135, 0.28571289053413135, 0.04762044279920201, 0.28571289053413135, 0.04762044279920201]
Printing some Q and Qe and total Qs values:  [[1.158]
 [1.158]
 [1.158]
 [1.158]
 [1.158]
 [1.171]
 [1.158]] [[12.695]
 [12.695]
 [12.695]
 [12.695]
 [12.695]
 [16.74 ]
 [12.695]] [[1.409]
 [1.409]
 [1.409]
 [1.409]
 [1.409]
 [1.574]
 [1.409]]
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.06247555489405917, 0.3750488902118817, 0.3750488902118817, 0.06247555489405917, 0.06247555489405917, 0.06247555489405917]
printing an ep nov before normalisation:  38.893976939983276
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.06247555489405917, 0.3750488902118817, 0.3750488902118817, 0.06247555489405917, 0.06247555489405917, 0.06247555489405917]
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.06247555489405917, 0.3750488902118817, 0.3750488902118817, 0.06247555489405917, 0.06247555489405917, 0.06247555489405917]
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.06247555489405917, 0.3750488902118817, 0.3750488902118817, 0.06247555489405917, 0.06247555489405917, 0.06247555489405917]
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.06247555489405917, 0.3750488902118817, 0.3750488902118817, 0.06247555489405917, 0.06247555489405917, 0.06247555489405917]
printing an ep nov before normalisation:  19.14253698339693
printing an ep nov before normalisation:  18.0114572678791
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.06247555489405917, 0.3750488902118817, 0.3750488902118817, 0.06247555489405917, 0.06247555489405917, 0.06247555489405917]
printing an ep nov before normalisation:  35.53987249008524
Starting evaluation
Printing some Q and Qe and total Qs values:  [[ 0.193]
 [-0.003]
 [ 0.191]
 [ 0.181]
 [ 0.235]
 [ 0.238]
 [ 0.199]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.193]
 [-0.003]
 [ 0.191]
 [ 0.181]
 [ 0.235]
 [ 0.238]
 [ 0.199]]
printing an ep nov before normalisation:  64.85632252782942
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.09085565181199594, 0.09085565181199594, 0.5457217409400205, 0.09085565181199594, 0.09085565181199594, 0.09085565181199594]
printing an ep nov before normalisation:  65.75112048202209
printing an ep nov before normalisation:  46.01309827952476
Printing some Q and Qe and total Qs values:  [[0.463]
 [0.008]
 [0.462]
 [0.46 ]
 [0.495]
 [0.485]
 [0.507]] [[26.746]
 [15.326]
 [17.402]
 [13.998]
 [15.793]
 [14.935]
 [15.046]] [[1.012]
 [0.243]
 [0.754]
 [0.658]
 [0.742]
 [0.708]
 [0.734]]
printing an ep nov before normalisation:  18.155587120608818
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.09085565181199594, 0.09085565181199594, 0.5457217409400205, 0.09085565181199594, 0.09085565181199594, 0.09085565181199594]
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.16568771563102, 0.16568771563102, 0.29197239922943174, 0.16568771563102, 0.04527673824648832, 0.16568771563102]
printing an ep nov before normalisation:  30.485473918190422
printing an ep nov before normalisation:  37.26909120425622
printing an ep nov before normalisation:  36.815712143148815
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 29.844791728284477
printing an ep nov before normalisation:  16.96470998592274
printing an ep nov before normalisation:  19.875463485400456
printing an ep nov before normalisation:  76.32670402526855
printing an ep nov before normalisation:  41.18310387080632
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.1935202746262275, 0.1935202746262275, 0.1935202746262275, 0.1935202746262275, 0.03239862686886233, 0.1935202746262275]
printing an ep nov before normalisation:  63.54234774925508
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.1935202746262275, 0.1935202746262275, 0.1935202746262275, 0.1935202746262275, 0.03239862686886233, 0.1935202746262275]
printing an ep nov before normalisation:  17.36450527595146
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.056]
 [0.093]
 [0.102]
 [0.103]
 [0.093]
 [0.06 ]] [[26.054]
 [26.189]
 [26.612]
 [26.392]
 [25.645]
 [25.138]
 [24.822]] [[1.61 ]
 [1.673]
 [1.76 ]
 [1.742]
 [1.654]
 [1.584]
 [1.513]]
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.73 ]
 [0.634]
 [0.724]
 [0.751]
 [0.718]
 [0.76 ]
 [0.761]] [[17.533]
 [18.868]
 [19.937]
 [19.472]
 [19.892]
 [19.739]
 [19.102]] [[0.73 ]
 [0.634]
 [0.724]
 [0.751]
 [0.718]
 [0.76 ]
 [0.761]]
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.19411727577274002, 0.19411727577274002, 0.19411727577274002, 0.19411727577274002, 0.029413621136299976, 0.19411727577274002]
printing an ep nov before normalisation:  28.83855855614062
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.19411727577274002, 0.19411727577274002, 0.19411727577274002, 0.19411727577274002, 0.029413621136299976, 0.19411727577274002]
printing an ep nov before normalisation:  21.78228145880987
printing an ep nov before normalisation:  21.524066989519607
printing an ep nov before normalisation:  21.634085415930997
printing an ep nov before normalisation:  20.540386640585666
printing an ep nov before normalisation:  13.678966302828295
printing an ep nov before normalisation:  22.415452718783687
line 256 mcts: sample exp_bonus 11.92274888039471
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.20595200517374446, 0.20595200517374446, 0.20595200517374446, 0.20595200517374446, 0.031199982159502027, 0.1449919971455201]
Printing some Q and Qe and total Qs values:  [[1.096]
 [1.096]
 [1.096]
 [1.096]
 [1.096]
 [1.096]
 [1.096]] [[47.425]
 [47.425]
 [47.425]
 [47.425]
 [47.425]
 [47.425]
 [47.425]] [[1.763]
 [1.763]
 [1.763]
 [1.763]
 [1.763]
 [1.763]
 [1.763]]
printing an ep nov before normalisation:  12.20069686743646
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
using explorer policy with actor:  0
printing an ep nov before normalisation:  62.89086865368532
printing an ep nov before normalisation:  15.744466052694928
Printing some Q and Qe and total Qs values:  [[0.702]
 [0.611]
 [0.721]
 [0.742]
 [0.708]
 [0.742]
 [0.732]] [[19.949]
 [17.38 ]
 [17.979]
 [18.789]
 [18.72 ]
 [17.858]
 [17.421]] [[0.702]
 [0.611]
 [0.721]
 [0.742]
 [0.708]
 [0.742]
 [0.732]]
printing an ep nov before normalisation:  14.592875242233276
actions average: 
K:  4  action  0 :  tensor([0.2457, 0.0439, 0.0897, 0.0975, 0.2874, 0.1382, 0.0977],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0434, 0.6735, 0.0257, 0.0729, 0.0678, 0.0371, 0.0796],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1966, 0.0538, 0.1033, 0.1158, 0.2544, 0.1515, 0.1246],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1442, 0.0480, 0.1458, 0.1271, 0.2299, 0.1660, 0.1390],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1782, 0.1574, 0.0928, 0.1020, 0.2180, 0.1529, 0.0988],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1151, 0.1111, 0.1097, 0.1352, 0.1796, 0.2253, 0.1240],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1672, 0.1391, 0.0994, 0.0988, 0.1802, 0.1576, 0.1577],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
Printing some Q and Qe and total Qs values:  [[0.734]
 [0.652]
 [0.766]
 [0.766]
 [0.766]
 [0.765]
 [0.766]] [[15.224]
 [16.247]
 [14.073]
 [14.073]
 [14.073]
 [17.255]
 [17.582]] [[0.734]
 [0.652]
 [0.766]
 [0.766]
 [0.766]
 [0.765]
 [0.766]]
siam score:  -0.5790436
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.23381755923978878, 0.23381755923978878, 0.16460424987238528, 0.23381755923978878, 0.035406072386565964, 0.09853700002168259]
printing an ep nov before normalisation:  12.03716605074082
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.25036276205014385, 0.25036276205014385, 0.10550413542489495, 0.25036276205014385, 0.03790344299977853, 0.10550413542489495]
printing an ep nov before normalisation:  19.191842187230954
printing an ep nov before normalisation:  35.67604143840736
printing an ep nov before normalisation:  69.7324323159685
printing an ep nov before normalisation:  32.41292393298632
printing an ep nov before normalisation:  67.36255680907387
printing an ep nov before normalisation:  20.139631317591366
printing an ep nov before normalisation:  27.892526309354373
printing an ep nov before normalisation:  39.76214986458171
using explorer policy with actor:  0
printing an ep nov before normalisation:  26.176518611194673
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9870794871794872 -1.0 -0.9870794871794872
line 256 mcts: sample exp_bonus 34.61668808309529
printing an ep nov before normalisation:  27.53585707473343
siam score:  -0.59107023
maxi score, test score, baseline:  -0.9870794871794872 -1.0 -0.9870794871794872
probs:  [0.27041001094852135, 0.27041001094852135, 0.11394596481805477, 0.19035863850967777, 0.040929409957170065, 0.11394596481805477]
printing an ep nov before normalisation:  18.1648301919264
using explorer policy with actor:  1
printing an ep nov before normalisation:  52.292019803842244
printing an ep nov before normalisation:  41.879068872032065
printing an ep nov before normalisation:  57.22536087036133
printing an ep nov before normalisation:  27.22045234639404
maxi score, test score, baseline:  -0.9870794871794872 -1.0 -0.9870794871794872
probs:  [0.20692592070186466, 0.29394919907489087, 0.12385824589124864, 0.20692592070186466, 0.04448246773888264, 0.12385824589124864]
printing an ep nov before normalisation:  24.69532065310373
printing an ep nov before normalisation:  22.216115488510038
siam score:  -0.60515386
maxi score, test score, baseline:  -0.9870794871794872 -1.0 -0.9870794871794872
probs:  [0.22477098196130751, 0.31930388605612053, 0.04830956098432154, 0.22477098196130751, 0.04830956098432154, 0.13453502805262144]
printing an ep nov before normalisation:  22.30893443037658
printing an ep nov before normalisation:  27.414903177463756
printing an ep nov before normalisation:  21.51841735895112
printing an ep nov before normalisation:  21.467870966939984
printing an ep nov before normalisation:  17.69246216737389
maxi score, test score, baseline:  -0.9875543209876543 -1.0 -0.9875543209876543
probs:  [0.2247709866597331, 0.3193038983986577, 0.04830955141374034, 0.2247709866597331, 0.04830955141374034, 0.13453502545439538]
using explorer policy with actor:  0
using explorer policy with actor:  1
printing an ep nov before normalisation:  18.99569576457285
Printing some Q and Qe and total Qs values:  [[0.874]
 [0.897]
 [0.874]
 [0.891]
 [0.874]
 [0.888]
 [0.885]] [[2.498]
 [2.635]
 [2.498]
 [2.569]
 [2.498]
 [2.522]
 [2.483]] [[0.874]
 [0.897]
 [0.874]
 [0.891]
 [0.874]
 [0.888]
 [0.885]]
from probs:  [0.2247709866597331, 0.3193038983986577, 0.04830955141374034, 0.2247709866597331, 0.04830955141374034, 0.13453502545439538]
printing an ep nov before normalisation:  22.392473220825195
printing an ep nov before normalisation:  25.139080235961487
printing an ep nov before normalisation:  25.95389220073011
maxi score, test score, baseline:  -0.9881352941176471 -1.0 -0.9881352941176471
probs:  [0.29004338578367084, 0.1648113776573885, 0.045145236558940656, 0.29004338578367084, 0.045145236558940656, 0.1648113776573885]
line 256 mcts: sample exp_bonus 19.985991985184235
printing an ep nov before normalisation:  14.48705792427063
printing an ep nov before normalisation:  86.28072460741194
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.252]
 [0.159]
 [0.186]
 [0.265]
 [0.284]
 [0.287]
 [0.271]] [[82.004]
 [88.098]
 [87.208]
 [87.106]
 [83.16 ]
 [78.182]
 [71.783]] [[0.252]
 [0.159]
 [0.186]
 [0.265]
 [0.284]
 [0.287]
 [0.271]]
printing an ep nov before normalisation:  7.2993930137919705
printing an ep nov before normalisation:  34.73775779276175
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9887888888888889 -1.0 -0.9887888888888889
maxi score, test score, baseline:  -0.9887888888888889 -1.0 -0.9887888888888889
probs:  [0.21827717265787047, 0.059759189970603, 0.059759189970603, 0.38416808477245, 0.059759189970603, 0.21827717265787047]
printing an ep nov before normalisation:  27.721703052520752
printing an ep nov before normalisation:  32.87560708051268
printing an ep nov before normalisation:  43.9933443069458
maxi score, test score, baseline:  -0.9894833333333334 -1.0 -0.9894833333333334
probs:  [0.1351320150377095, 0.0485375870522418, 0.1351320150377095, 0.3204038144484771, 0.1351320150377095, 0.22566255338615254]
Printing some Q and Qe and total Qs values:  [[0.798]
 [0.798]
 [0.798]
 [0.798]
 [0.796]
 [0.798]
 [0.798]] [[39.808]
 [39.808]
 [39.808]
 [39.808]
 [46.365]
 [39.808]
 [39.808]] [[1.957]
 [1.957]
 [1.957]
 [1.957]
 [2.4  ]
 [1.957]
 [1.957]]
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
probs:  [0.044228720930886825, 0.044228720930886825, 0.12307535988042398, 0.29152045218170836, 0.20542629389438571, 0.29152045218170836]
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
using explorer policy with actor:  1
printing an ep nov before normalisation:  87.50370944386718
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
line 256 mcts: sample exp_bonus 86.73851643598951
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.44925785064697
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
probs:  [0.04838635701579015, 0.04838635701579015, 0.13466721656708602, 0.22478278098732948, 0.22478278098732948, 0.3189945074266746]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
probs:  [0.04838635701579015, 0.04838635701579015, 0.13466721656708602, 0.22478278098732948, 0.22478278098732948, 0.3189945074266746]
printing an ep nov before normalisation:  12.954297915550674
printing an ep nov before normalisation:  51.43525345530148
Printing some Q and Qe and total Qs values:  [[0.924]
 [0.922]
 [0.922]
 [0.922]
 [0.922]
 [0.922]
 [0.922]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.924]
 [0.922]
 [0.922]
 [0.922]
 [0.922]
 [0.922]
 [0.922]]
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
probs:  [0.06593820209145874, 0.06593820209145874, 0.18360331115276315, 0.06593820209145874, 0.18360331115276315, 0.4349787714200974]
printing an ep nov before normalisation:  56.68361254710532
printing an ep nov before normalisation:  44.85829391028702
printing an ep nov before normalisation:  87.1712855586448
printing an ep nov before normalisation:  38.86691541067241
Printing some Q and Qe and total Qs values:  [[0.1  ]
 [0.101]
 [0.098]
 [0.102]
 [0.098]
 [0.102]
 [0.101]] [[63.327]
 [62.297]
 [60.813]
 [62.143]
 [61.937]
 [63.145]
 [63.864]] [[0.1  ]
 [0.101]
 [0.098]
 [0.102]
 [0.098]
 [0.102]
 [0.101]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.90916428961351
printing an ep nov before normalisation:  17.292757946721558
printing an ep nov before normalisation:  16.07115462321288
printing an ep nov before normalisation:  15.405246494252083
actor:  1 policy actor:  1  step number:  65 total reward:  0.3466666666666661  reward:  1.0 rdn_beta:  2.0
siam score:  -0.76766723
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
probs:  [0.08654714201349524, 0.08654714201349524, 0.08123537424292152, 0.08654714201349524, 0.09208985620887652, 0.5670333435077162]
printing an ep nov before normalisation:  32.19998458474305
deleting a thread, now have 5 threads
Frames:  11390 train batches done:  1327 episodes:  334
printing an ep nov before normalisation:  27.328615521797722
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
probs:  [0.08700897498403866, 0.08700897498403866, 0.0816688188516959, 0.0816688188516959, 0.09258131181778756, 0.5700631005107433]
printing an ep nov before normalisation:  64.1099076414448
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
probs:  [0.08700897498403866, 0.08700897498403866, 0.0816688188516959, 0.0816688188516959, 0.09258131181778756, 0.5700631005107433]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.051]
 [-0.051]
 [-0.051]
 [-0.051]
 [-0.051]
 [-0.051]
 [-0.051]] [[38.58]
 [38.58]
 [38.58]
 [38.58]
 [38.58]
 [38.58]
 [38.58]] [[1.941]
 [1.941]
 [1.941]
 [1.941]
 [1.941]
 [1.941]
 [1.941]]
using explorer policy with actor:  1
deleting a thread, now have 4 threads
Frames:  11619 train batches done:  1346 episodes:  338
STARTED EXPV TRAINING ON FRAME NO.  11619
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
probs:  [0.08249543246143606, 0.08249543246143606, 0.08249543246143606, 0.08249543246143606, 0.09425936477866609, 0.5757589053755897]
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
probs:  [0.08249543246143606, 0.08249543246143606, 0.08249543246143606, 0.08249543246143606, 0.09425936477866609, 0.5757589053755897]
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
probs:  [0.08249543246143606, 0.08249543246143606, 0.08249543246143606, 0.08249543246143606, 0.09425936477866609, 0.5757589053755897]
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
probs:  [0.08249543246143606, 0.08249543246143606, 0.08249543246143606, 0.08249543246143606, 0.09425936477866609, 0.5757589053755897]
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
probs:  [0.08249543246143606, 0.08249543246143606, 0.08249543246143606, 0.08249543246143606, 0.09425936477866609, 0.5757589053755897]
deleting a thread, now have 3 threads
Frames:  11723 train batches done:  1369 episodes:  339
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
from probs:  [0.08249543246143606, 0.08249543246143606, 0.08249543246143606, 0.08249543246143606, 0.09425936477866609, 0.5757589053755897]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
probs:  [0.08608513680896783, 0.08074034607783484, 0.08608513680896783, 0.08608513680896783, 0.09747186488833809, 0.5635323786069236]
Printing some Q and Qe and total Qs values:  [[0.087]
 [0.091]
 [0.091]
 [0.091]
 [0.095]
 [0.095]
 [0.094]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.087]
 [0.091]
 [0.091]
 [0.091]
 [0.095]
 [0.095]
 [0.094]]
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
probs:  [0.08616990086575924, 0.08065845592632784, 0.08616990086575924, 0.08616990086575924, 0.09791167486715653, 0.5629201666092378]
Printing some Q and Qe and total Qs values:  [[0.115]
 [0.115]
 [0.115]
 [0.119]
 [0.115]
 [0.115]
 [0.121]] [[38.578]
 [38.578]
 [38.578]
 [40.184]
 [38.578]
 [38.578]
 [40.561]] [[0.115]
 [0.115]
 [0.115]
 [0.119]
 [0.115]
 [0.115]
 [0.121]]
siam score:  -0.8369309
printing an ep nov before normalisation:  26.157527018997442
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.532]
 [0.569]
 [0.567]
 [0.565]
 [0.565]
 [0.556]] [[24.973]
 [24.306]
 [27.249]
 [28.909]
 [29.104]
 [29.927]
 [44.896]] [[1.198]
 [1.136]
 [1.316]
 [1.394]
 [1.401]
 [1.441]
 [2.159]]
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.0867490643949757, 0.0810347885005685, 0.0867490643949757, 0.0810347885005685, 0.0989229565178432, 0.5655093376910684]
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.0867490643949757, 0.0810347885005685, 0.0867490643949757, 0.0810347885005685, 0.0989229565178432, 0.5655093376910684]
line 256 mcts: sample exp_bonus 53.702817358767504
printing an ep nov before normalisation:  85.59525398400639
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.0867490643949757, 0.0810347885005685, 0.0867490643949757, 0.0810347885005685, 0.0989229565178432, 0.5655093376910684]
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.08729131221072807, 0.08154126279375658, 0.08729131221072807, 0.08154126279375658, 0.09328604458161326, 0.5690488054094174]
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.08729131221072807, 0.08154126279375658, 0.08729131221072807, 0.08154126279375658, 0.09328604458161326, 0.5690488054094174]
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.08729131221072807, 0.08154126279375658, 0.08729131221072807, 0.08154126279375658, 0.09328604458161326, 0.5690488054094174]
printing an ep nov before normalisation:  58.50851275903275
siam score:  -0.8495538
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.08741249963738085, 0.08148489754266475, 0.08741249963738085, 0.08148489754266475, 0.09359234011910607, 0.5686128655208028]
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.09149228776173934, 0.08019012439007052, 0.09149228776173934, 0.0857258778782348, 0.09149228776173934, 0.5596071344464766]
siam score:  -0.85063434
printing an ep nov before normalisation:  41.27670587168325
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.09202258335318896, 0.08065481633647839, 0.08622270222221416, 0.08622270222221416, 0.09202258335318896, 0.5628546125127154]
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.09202258335318896, 0.08065481633647839, 0.08622270222221416, 0.08622270222221416, 0.09202258335318896, 0.5628546125127154]
printing an ep nov before normalisation:  82.89153566851301
printing an ep nov before normalisation:  74.12454297088107
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.0928357651617579, 0.08105745811247612, 0.08682642483049172, 0.08682642483049172, 0.08682642483049172, 0.5656275022342908]
printing an ep nov before normalisation:  43.57241153717041
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.0933745867163188, 0.08152730423131573, 0.08733005483621527, 0.08733005483621527, 0.08152730423131573, 0.5689106951486192]
printing an ep nov before normalisation:  23.814943198228136
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
probs:  [0.09337506511441707, 0.08152721618006181, 0.08733024422954191, 0.08733024422954191, 0.08152721618006181, 0.5689100140663753]
Printing some Q and Qe and total Qs values:  [[0.655]
 [0.704]
 [0.501]
 [0.261]
 [0.279]
 [0.502]
 [0.679]] [[40.264]
 [33.024]
 [40.632]
 [40.753]
 [34.39 ]
 [33.415]
 [25.717]] [[0.883]
 [0.848]
 [0.733]
 [0.495]
 [0.439]
 [0.651]
 [0.739]]
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
actions average: 
K:  4  action  0 :  tensor([0.1905, 0.0199, 0.1132, 0.1381, 0.1827, 0.1854, 0.1703],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0458, 0.6798, 0.0489, 0.0540, 0.0495, 0.0492, 0.0728],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2457, 0.0255, 0.1193, 0.1142, 0.1769, 0.1577, 0.1607],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2341, 0.0135, 0.1095, 0.1297, 0.1881, 0.1580, 0.1671],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2040, 0.0758, 0.1184, 0.1245, 0.1796, 0.1606, 0.1371],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1723, 0.0528, 0.1171, 0.1238, 0.1594, 0.1920, 0.1827],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1673, 0.1052, 0.1192, 0.1285, 0.1629, 0.1712, 0.1458],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
probs:  [0.0879966771114214, 0.0819820344670533, 0.0879966771114214, 0.0879966771114214, 0.0819820344670533, 0.5720458997316292]
line 256 mcts: sample exp_bonus 0.0
UNIT TEST: sample policy line 217 mcts : [0.082 0.082 0.102 0.429 0.122 0.082 0.102]
printing an ep nov before normalisation:  59.213552426646594
printing an ep nov before normalisation:  52.729110751635496
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
probs:  [0.0885287700497962, 0.08247770328521864, 0.08247770328521864, 0.0885287700497962, 0.08247770328521864, 0.5755093500447517]
using another actor
siam score:  -0.848102
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.08868458966478963, 0.08245087000223074, 0.08245087000223074, 0.08868458966478963, 0.08245087000223074, 0.5752782106637285]
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.809]
 [0.623]
 [0.641]
 [0.606]
 [0.566]
 [0.623]] [[72.981]
 [72.13 ]
 [71.62 ]
 [70.005]
 [73.278]
 [75.66 ]
 [71.62 ]] [[2.053]
 [2.326]
 [2.122]
 [2.084]
 [2.163]
 [2.205]
 [2.122]]
printing an ep nov before normalisation:  44.459512707076684
Printing some Q and Qe and total Qs values:  [[ 0.023]
 [ 0.041]
 [ 0.013]
 [ 0.02 ]
 [ 0.001]
 [ 0.008]
 [-0.001]] [[22.973]
 [26.757]
 [22.765]
 [24.7  ]
 [23.408]
 [28.606]
 [22.081]] [[0.506]
 [0.682]
 [0.488]
 [0.576]
 [0.502]
 [0.727]
 [0.445]]
from probs:  [0.08884364117567736, 0.0824234802941987, 0.0824234802941987, 0.08884364117567736, 0.0824234802941987, 0.5750422767660491]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  11619
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.0895985053826936, 0.08294333092035856, 0.08294333092035856, 0.08294333092035856, 0.08294333092035856, 0.5786281709358723]
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.0895985053826936, 0.08294333092035856, 0.08294333092035856, 0.08294333092035856, 0.08294333092035856, 0.5786281709358723]
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.08762384326059888, 0.08142612079379635, 0.08762384326059888, 0.08762384326059888, 0.08762384326059888, 0.5680785061638081]
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.08762384326059888, 0.08142612079379635, 0.08762384326059888, 0.08762384326059888, 0.08762384326059888, 0.5680785061638081]
printing an ep nov before normalisation:  45.23394320866878
printing an ep nov before normalisation:  50.65215587615967
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.08762384326059888, 0.08142612079379635, 0.08762384326059888, 0.08762384326059888, 0.08762384326059888, 0.5680785061638081]
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.08762384326059888, 0.08142612079379635, 0.08762384326059888, 0.08762384326059888, 0.08762384326059888, 0.5680785061638081]
actions average: 
K:  1  action  0 :  tensor([0.2341, 0.0304, 0.1164, 0.1241, 0.1782, 0.1604, 0.1563],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0411, 0.7237, 0.0291, 0.0704, 0.0561, 0.0388, 0.0409],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2225, 0.0685, 0.1204, 0.1381, 0.1702, 0.1321, 0.1482],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1860, 0.0984, 0.1191, 0.1389, 0.1784, 0.1505, 0.1287],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1901, 0.0438, 0.1116, 0.1468, 0.2082, 0.1472, 0.1522],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1777, 0.0218, 0.1214, 0.1056, 0.1580, 0.2831, 0.1326],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1851, 0.0828, 0.1093, 0.1400, 0.1726, 0.1622, 0.1481],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.08762384326059888, 0.08142612079379635, 0.08762384326059888, 0.08762384326059888, 0.08762384326059888, 0.5680785061638081]
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.08762384326059888, 0.08142612079379635, 0.08762384326059888, 0.08762384326059888, 0.08762384326059888, 0.5680785061638081]
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.352]
 [0.573]
 [0.573]
 [0.573]
 [0.508]
 [0.573]] [[26.007]
 [34.689]
 [26.007]
 [26.007]
 [26.007]
 [29.583]
 [26.007]] [[0.573]
 [0.352]
 [0.573]
 [0.573]
 [0.573]
 [0.508]
 [0.573]]
printing an ep nov before normalisation:  32.79569546381633
printing an ep nov before normalisation:  65.26862217704391
printing an ep nov before normalisation:  23.398868924739848
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.09209795201659533, 0.07995960992242816, 0.09209795201659533, 0.08590977761564736, 0.09209795201659533, 0.5578367564121384]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.09665266428739665, 0.07917260315313868, 0.0905974797114772, 0.0905974797114772, 0.0905974797114772, 0.552382293425033]
Printing some Q and Qe and total Qs values:  [[0.374]
 [0.369]
 [0.366]
 [0.363]
 [0.366]
 [0.364]
 [0.366]] [[38.013]
 [35.976]
 [39.931]
 [37.861]
 [39.824]
 [38.806]
 [39.028]] [[0.374]
 [0.369]
 [0.366]
 [0.363]
 [0.366]
 [0.364]
 [0.366]]
printing an ep nov before normalisation:  52.17107079321684
printing an ep nov before normalisation:  73.9297121076165
printing an ep nov before normalisation:  39.9334454536438
printing an ep nov before normalisation:  32.77436971664429
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
probs:  [0.09665382017670421, 0.07917223798749584, 0.09059810869939673, 0.09059810869939673, 0.09059810869939673, 0.5523796157376099]
printing an ep nov before normalisation:  68.7116050171706
printing an ep nov before normalisation:  42.7176508138661
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.10014951579904675, 0.07740775310672626, 0.09412963743931482, 0.09412963743931482, 0.09412963743931482, 0.5400538187762824]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.10118627362708052, 0.07771399831948382, 0.09497302428095196, 0.09497302428095196, 0.08899874606352073, 0.5421549334280109]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  11619
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.0958847814312786, 0.07806460150097251, 0.0958847814312786, 0.0958847814312786, 0.08971625760924958, 0.544564796595942]
printing an ep nov before normalisation:  49.679955664081355
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.382]
 [0.388]
 [1.289]
 [0.479]
 [0.427]
 [0.509]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.433]
 [0.382]
 [0.388]
 [1.289]
 [0.479]
 [0.427]
 [0.509]]
Printing some Q and Qe and total Qs values:  [[0.272]
 [0.318]
 [0.271]
 [0.268]
 [0.277]
 [0.256]
 [0.262]] [[48.377]
 [43.543]
 [47.427]
 [46.039]
 [44.53 ]
 [45.651]
 [45.534]] [[0.588]
 [0.574]
 [0.576]
 [0.556]
 [0.545]
 [0.539]
 [0.543]]
Printing some Q and Qe and total Qs values:  [[1.219]
 [1.22 ]
 [1.22 ]
 [1.22 ]
 [1.22 ]
 [1.22 ]
 [1.22 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.219]
 [1.22 ]
 [1.22 ]
 [1.22 ]
 [1.22 ]
 [1.22 ]
 [1.22 ]]
printing an ep nov before normalisation:  53.03124987246963
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.09155329530894073, 0.07937718776825725, 0.09799946988930257, 0.09799946988930257, 0.07937718776825725, 0.5536933893759397]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.09155329530894073, 0.07937718776825725, 0.09799946988930257, 0.09799946988930257, 0.07937718776825725, 0.5536933893759397]
printing an ep nov before normalisation:  61.749726410087504
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.0958940138856862, 0.07800766337200943, 0.10232374773047194, 0.0958940138856862, 0.08374896106775762, 0.5441316000583886]
Printing some Q and Qe and total Qs values:  [[0.323]
 [0.323]
 [0.323]
 [0.413]
 [0.323]
 [0.323]
 [0.323]] [[24.478]
 [24.478]
 [24.478]
 [76.995]
 [24.478]
 [24.478]
 [24.478]] [[0.609]
 [0.609]
 [0.609]
 [1.894]
 [0.609]
 [0.609]
 [0.609]]
using another actor
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.09621061320439882, 0.07787222761392919, 0.1028028433186199, 0.09621061320439882, 0.08375862298864788, 0.5431450796700055]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.09621061320439882, 0.07787222761392919, 0.1028028433186199, 0.09621061320439882, 0.08375862298864788, 0.5431450796700055]
printing an ep nov before normalisation:  45.86539971644441
printing an ep nov before normalisation:  23.96637362252372
printing an ep nov before normalisation:  20.835657882450807
printing an ep nov before normalisation:  23.991082582792995
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
probs:  [0.09653352752564423, 0.07773409023692382, 0.10329149517845214, 0.09653352752564423, 0.08376847751478467, 0.542138882018551]
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
probs:  [0.09653352752564423, 0.07773409023692382, 0.10329149517845214, 0.09653352752564423, 0.08376847751478467, 0.542138882018551]
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
probs:  [0.09653352752564423, 0.07773409023692382, 0.10329149517845214, 0.09653352752564423, 0.08376847751478467, 0.542138882018551]
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
using explorer policy with actor:  1
printing an ep nov before normalisation:  29.748312769071777
printing an ep nov before normalisation:  91.70777038587582
printing an ep nov before normalisation:  16.497620344161987
printing an ep nov before normalisation:  53.74498978363689
printing an ep nov before normalisation:  31.33819580078125
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
probs:  [0.0992550072556516, 0.0782369801421709, 0.10681050719840607, 0.09198462051828406, 0.0782369801421709, 0.5454759047433166]
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
probs:  [0.0992550072556516, 0.0782369801421709, 0.10681050719840607, 0.09198462051828406, 0.0782369801421709, 0.5454759047433166]
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
probs:  [0.0992550072556516, 0.0782369801421709, 0.10681050719840607, 0.09198462051828406, 0.0782369801421709, 0.5454759047433166]
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
probs:  [0.09964758246408152, 0.07811023431722185, 0.10738976643844282, 0.09219755637554512, 0.07811023431722185, 0.5445446260874869]
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.53 ]
 [0.547]
 [0.519]
 [0.489]
 [0.758]
 [0.467]] [[33.454]
 [27.998]
 [27.634]
 [28.485]
 [31.083]
 [48.052]
 [30.534]] [[1.088]
 [0.979]
 [0.982]
 [0.986]
 [1.051]
 [1.945]
 [1.009]]
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
probs:  [0.09964758246408152, 0.07811023431722185, 0.10738976643844282, 0.09219755637554512, 0.07811023431722185, 0.5445446260874869]
printing an ep nov before normalisation:  13.96942506269868
printing an ep nov before normalisation:  50.78820327693939
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
probs:  [0.10004797435896427, 0.07798096481693222, 0.10798055948845292, 0.09241473206455068, 0.07798096481693222, 0.5435948044541676]
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
probs:  [0.0931251296546064, 0.07858024898904578, 0.10881078527432865, 0.0931251296546064, 0.07858024898904578, 0.5477784574383671]
printing an ep nov before normalisation:  47.04536437988281
Printing some Q and Qe and total Qs values:  [[0.367]
 [0.373]
 [0.373]
 [0.373]
 [0.403]
 [0.388]
 [0.373]] [[30.274]
 [17.304]
 [17.304]
 [17.304]
 [59.328]
 [42.537]
 [17.304]] [[0.577]
 [0.458]
 [0.458]
 [0.458]
 [0.893]
 [0.716]
 [0.458]]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.09336641280414344, 0.07846190256586406, 0.10943990423758208, 0.09336641280414344, 0.07846190256586406, 0.5469034650224028]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.09336641280414344, 0.07846190256586406, 0.10943990423758208, 0.09336641280414344, 0.07846190256586406, 0.5469034650224028]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.09336641280414344, 0.07846190256586406, 0.10943990423758208, 0.09336641280414344, 0.07846190256586406, 0.5469034650224028]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.09746981072280081, 0.07640363625866892, 0.11289080118020456, 0.09746981072280081, 0.08317034684411745, 0.5325955942714075]
from probs:  [0.09746981072280081, 0.07640363625866892, 0.11289080118020456, 0.09746981072280081, 0.08317034684411745, 0.5325955942714075]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.09084880787491877, 0.0769635727540462, 0.11371860689753241, 0.09818440378783257, 0.08377996090429278, 0.5365046477813772]
printing an ep nov before normalisation:  56.558901029957426
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.09084880787491877, 0.0769635727540462, 0.11371860689753241, 0.09818440378783257, 0.08377996090429278, 0.5365046477813772]
Printing some Q and Qe and total Qs values:  [[0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]] [[85.055]
 [85.055]
 [85.055]
 [85.055]
 [85.055]
 [85.055]
 [85.055]] [[2.589]
 [2.589]
 [2.589]
 [2.589]
 [2.589]
 [2.589]
 [2.589]]
printing an ep nov before normalisation:  71.60721131241314
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
printing an ep nov before normalisation:  63.23198126413768
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.09084880787491877, 0.0769635727540462, 0.11371860689753241, 0.09818440378783257, 0.08377996090429278, 0.5365046477813772]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.09084880787491877, 0.0769635727540462, 0.11371860689753241, 0.09818440378783257, 0.08377996090429278, 0.5365046477813772]
printing an ep nov before normalisation:  58.71582560437205
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.09084880787491877, 0.0769635727540462, 0.11371860689753241, 0.09818440378783257, 0.08377996090429278, 0.5365046477813772]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.09084880787491877, 0.0769635727540462, 0.11371860689753241, 0.09818440378783257, 0.08377996090429278, 0.5365046477813772]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.09084880787491877, 0.0769635727540462, 0.11371860689753241, 0.09818440378783257, 0.08377996090429278, 0.5365046477813772]
siam score:  -0.85872597
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.09084880787491877, 0.0769635727540462, 0.11371860689753241, 0.09818440378783257, 0.08377996090429278, 0.5365046477813772]
Printing some Q and Qe and total Qs values:  [[0.085]
 [0.085]
 [0.085]
 [0.085]
 [0.085]
 [0.072]
 [0.069]] [[27.222]
 [27.222]
 [27.222]
 [27.222]
 [27.222]
 [29.775]
 [29.454]] [[0.776]
 [0.776]
 [0.776]
 [0.776]
 [0.776]
 [0.889]
 [0.871]]
printing an ep nov before normalisation:  41.73900359174198
using explorer policy with actor:  1
from probs:  [0.09084880787491877, 0.0769635727540462, 0.11371860689753241, 0.09818440378783257, 0.08377996090429278, 0.5365046477813772]
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.09084913516416522, 0.07696329241789733, 0.11371993498154781, 0.09818505208672185, 0.08377997885697427, 0.5365026064926935]
Printing some Q and Qe and total Qs values:  [[-0.035]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.019]] [[39.967]
 [35.325]
 [35.325]
 [35.325]
 [35.325]
 [35.325]
 [35.325]] [[1.235]
 [1.043]
 [1.043]
 [1.043]
 [1.043]
 [1.043]
 [1.043]]
siam score:  -0.8539563
from probs:  [0.08440016085476365, 0.07737043258682916, 0.11527583011000572, 0.09925543568511595, 0.08440016085476365, 0.5392979799085218]
Printing some Q and Qe and total Qs values:  [[0.244]
 [0.231]
 [0.253]
 [0.25 ]
 [0.251]
 [0.247]
 [0.246]] [[27.039]
 [32.313]
 [22.229]
 [22.265]
 [21.813]
 [22.821]
 [23.611]] [[0.244]
 [0.231]
 [0.253]
 [0.25 ]
 [0.251]
 [0.247]
 [0.246]]
printing an ep nov before normalisation:  45.142689194654224
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.08517824772042402, 0.07774646628578995, 0.10918861850924216, 0.10088314433700951, 0.08517824772042402, 0.5418252754271103]
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.08517840345171217, 0.07774598202767578, 0.10919084189859893, 0.1008846524987325, 0.08517840345171217, 0.5418217166715685]
line 256 mcts: sample exp_bonus 32.70317978434162
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.08589129607630035, 0.07839657225462497, 0.10172920302021814, 0.10172920302021814, 0.08589129607630035, 0.5463624295523379]
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.08589129607630035, 0.07839657225462497, 0.10172920302021814, 0.10172920302021814, 0.08589129607630035, 0.5463624295523379]
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.08595200197458572, 0.07827706495473663, 0.1021707368089838, 0.1021707368089838, 0.08595200197458572, 0.5454774574781245]
printing an ep nov before normalisation:  70.73386662139959
printing an ep nov before normalisation:  43.451733059353295
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.08595200197458572, 0.07827706495473663, 0.1021707368089838, 0.1021707368089838, 0.08595200197458572, 0.5454774574781245]
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.417]
 [0.417]
 [0.423]
 [0.417]
 [0.417]
 [0.417]] [[49.966]
 [49.966]
 [49.966]
 [66.696]
 [49.966]
 [49.966]
 [49.966]] [[1.039]
 [1.039]
 [1.039]
 [1.428]
 [1.039]
 [1.039]
 [1.039]]
siam score:  -0.8550992
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.08666730552179049, 0.07892839753916983, 0.10302122427751721, 0.09469283972598977, 0.08666730552179049, 0.5500229274137421]
using explorer policy with actor:  1
printing an ep nov before normalisation:  61.27514740130689
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]] [[45.019]
 [45.019]
 [45.019]
 [45.019]
 [45.019]
 [45.019]
 [45.019]] [[1.71]
 [1.71]
 [1.71]
 [1.71]
 [1.71]
 [1.71]
 [1.71]]
printing an ep nov before normalisation:  15.73448512496725
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
printing an ep nov before normalisation:  61.09862385371936
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.08746551939506648, 0.07947375461661643, 0.10435377704009298, 0.08746551939506648, 0.08746551939506648, 0.5537759101580911]
Printing some Q and Qe and total Qs values:  [[0.353]
 [0.35 ]
 [0.348]
 [0.364]
 [0.406]
 [0.423]
 [0.465]] [[52.465]
 [49.338]
 [53.116]
 [52.979]
 [52.838]
 [53.136]
 [57.856]] [[1.222]
 [1.116]
 [1.239]
 [1.251]
 [1.288]
 [1.315]
 [1.513]]
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.08746551939506648, 0.07947375461661643, 0.10435377704009298, 0.08746551939506648, 0.08746551939506648, 0.5537759101580911]
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.08746551939506648, 0.07947375461661643, 0.10435377704009298, 0.08746551939506648, 0.08746551939506648, 0.5537759101580911]
printing an ep nov before normalisation:  75.23234896476836
from probs:  [0.08816964862370245, 0.08011344908694507, 0.1051940702862842, 0.08011344908694507, 0.08816964862370245, 0.5582397342924207]
printing an ep nov before normalisation:  40.330245821793156
printing an ep nov before normalisation:  75.79015865543576
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
probs:  [0.0888854706791117, 0.0807634262885556, 0.10604903618368317, 0.0807634262885556, 0.0807634262885556, 0.5627752142715383]
actions average: 
K:  2  action  0 :  tensor([0.2397, 0.0177, 0.1184, 0.1096, 0.1889, 0.1767, 0.1490],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0516, 0.5464, 0.0528, 0.0883, 0.0786, 0.0963, 0.0860],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2091, 0.0168, 0.1156, 0.1268, 0.2031, 0.1729, 0.1558],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1986, 0.0792, 0.0949, 0.1349, 0.1812, 0.1598, 0.1513],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1970, 0.0202, 0.1122, 0.1237, 0.2219, 0.1708, 0.1542],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1889, 0.0321, 0.1213, 0.1244, 0.1917, 0.1757, 0.1658],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1912, 0.0587, 0.1208, 0.1208, 0.1887, 0.1717, 0.1480],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
probs:  [0.0888854706791117, 0.0807634262885556, 0.10604903618368317, 0.0807634262885556, 0.0807634262885556, 0.5627752142715383]
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.615]
 [0.466]
 [0.463]
 [0.463]
 [0.494]
 [0.464]] [[41.756]
 [74.078]
 [38.92 ]
 [40.863]
 [41.017]
 [48.927]
 [41.541]] [[0.927]
 [1.703]
 [0.86 ]
 [0.895]
 [0.898]
 [1.086]
 [0.91 ]]
printing an ep nov before normalisation:  42.94755081656892
Printing some Q and Qe and total Qs values:  [[0.399]
 [0.368]
 [0.368]
 [0.368]
 [0.377]
 [0.376]
 [0.395]] [[50.126]
 [47.976]
 [47.976]
 [47.976]
 [51.361]
 [53.179]
 [51.604]] [[1.805]
 [1.666]
 [1.666]
 [1.666]
 [1.844]
 [1.934]
 [1.874]]
printing an ep nov before normalisation:  75.9833680604209
printing an ep nov before normalisation:  38.630104064941406
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.09386822814207288, 0.08592755204170999, 0.11064852480699082, 0.08592755204170999, 0.07826549615539491, 0.5453626468121214]
printing an ep nov before normalisation:  46.3333920853677
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.09386822814207288, 0.08592755204170999, 0.11064852480699082, 0.08592755204170999, 0.07826549615539491, 0.5453626468121214]
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.09386909554539807, 0.08592776335821536, 0.1106507786579351, 0.08592776335821536, 0.07826507440567071, 0.5453595246745655]
printing an ep nov before normalisation:  47.9739055373523
printing an ep nov before normalisation:  41.37856873098228
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.09467774087212966, 0.08666789999938354, 0.1029842425179405, 0.08666789999938354, 0.07893910617480392, 0.5500631104363589]
printing an ep nov before normalisation:  32.10024061275186
printing an ep nov before normalisation:  36.39712333679199
printing an ep nov before normalisation:  57.08145242773818
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.0954147411813929, 0.08734246141142153, 0.10378599427617809, 0.0795534195281158, 0.0795534195281158, 0.554349964074776]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.09621974927208611, 0.08807926896432859, 0.09621974927208611, 0.08022441954456255, 0.08022441954456255, 0.5590323934023741]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.0965299463260262, 0.08819300338994908, 0.0965299463260262, 0.08014858476741857, 0.08014858476741857, 0.5584499344231614]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.09684761530874686, 0.08830947732354012, 0.09684761530874686, 0.08007092312728796, 0.08007092312728796, 0.5578534458043902]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.09684761530874686, 0.08830947732354012, 0.09684761530874686, 0.08007092312728796, 0.08007092312728796, 0.5578534458043902]
printing an ep nov before normalisation:  52.61613396912941
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.09717187130695268, 0.08842836648951576, 0.09717187130695268, 0.07999165131479576, 0.07999165131479576, 0.5572445882669874]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.09717187130695268, 0.08842836648951576, 0.09717187130695268, 0.07999165131479576, 0.07999165131479576, 0.5572445882669874]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.09750350755010792, 0.08854996163833069, 0.09750350755010792, 0.07991057523222984, 0.07991057523222984, 0.5566218727969937]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.09750350755010792, 0.08854996163833069, 0.09750350755010792, 0.07991057523222984, 0.07991057523222984, 0.5566218727969937]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.09750350755010792, 0.08854996163833069, 0.09750350755010792, 0.07991057523222984, 0.07991057523222984, 0.5566218727969937]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
printing an ep nov before normalisation:  33.08763751388909
actor:  1 policy actor:  1  step number:  78 total reward:  0.07333333333333247  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  7.563948353413252
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.09189189449749355, 0.08649245152504208, 0.09189189449749355, 0.08128246269197494, 0.08128246269197494, 0.567158834096021]
printing an ep nov before normalisation:  14.62758376705685
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.09189213759309105, 0.08649254059996414, 0.09189213759309105, 0.08128240315045579, 0.08128240315045579, 0.5671583779129421]
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.599]
 [0.588]
 [0.599]
 [0.604]
 [0.612]
 [0.607]] [[38.332]
 [30.435]
 [35.084]
 [35.156]
 [35.324]
 [34.921]
 [34.405]] [[1.352]
 [1.074]
 [1.225]
 [1.238]
 [1.249]
 [1.243]
 [1.22 ]]
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.09255788012158844, 0.08702836219952582, 0.08702836219952582, 0.08169286245016716, 0.08169286245016716, 0.5699996705790256]
actions average: 
K:  3  action  0 :  tensor([0.2774, 0.0007, 0.0965, 0.0953, 0.2305, 0.1489, 0.1507],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0282, 0.8186, 0.0217, 0.0368, 0.0263, 0.0305, 0.0379],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1524, 0.0743, 0.1577, 0.1269, 0.1628, 0.1646, 0.1612],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1911, 0.0193, 0.1105, 0.1730, 0.1711, 0.1628, 0.1723],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1914, 0.0311, 0.1194, 0.1355, 0.1845, 0.1704, 0.1677],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1827, 0.0084, 0.1332, 0.1115, 0.1659, 0.2407, 0.1575],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1864, 0.0616, 0.1208, 0.1270, 0.1621, 0.1634, 0.1786],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
printing an ep nov before normalisation:  62.906162221455304
siam score:  -0.86837447
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.0925583979606374, 0.08702856839484738, 0.08702856839484738, 0.08169276793662891, 0.08169276793662891, 0.5699989293764101]
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.349]
 [0.353]
 [0.353]] [[56.749]
 [56.749]
 [56.749]
 [56.749]
 [57.659]
 [56.749]
 [56.749]] [[0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.669]
 [0.664]
 [0.664]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.08916354509481256, 0.09433494823040164, 0.09433494823040164, 0.07934664422725354, 0.08916354509481256, 0.5536563691223182]
printing an ep nov before normalisation:  52.96010283283081
actions average: 
K:  0  action  0 :  tensor([0.2167, 0.0462, 0.1230, 0.1146, 0.1914, 0.1621, 0.1460],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0340, 0.8394, 0.0177, 0.0255, 0.0280, 0.0249, 0.0305],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1753, 0.0577, 0.1934, 0.1171, 0.1501, 0.1638, 0.1427],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1983, 0.0228, 0.1347, 0.1360, 0.1898, 0.1745, 0.1440],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1954, 0.0093, 0.1291, 0.1347, 0.1874, 0.1928, 0.1513],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1727, 0.0122, 0.1390, 0.1168, 0.1671, 0.2701, 0.1222],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1700, 0.0771, 0.1178, 0.1198, 0.1620, 0.1632, 0.1900],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.0894705263826886, 0.09491578731336345, 0.09491578731336345, 0.07913375987022106, 0.0894705263826886, 0.5520936127376749]
printing an ep nov before normalisation:  65.14899714252523
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.269]
 [0.317]] [[28.902]
 [28.902]
 [28.902]
 [28.902]
 [28.902]
 [35.864]
 [28.902]] [[1.426]
 [1.426]
 [1.426]
 [1.426]
 [1.426]
 [1.84 ]
 [1.426]]
printing an ep nov before normalisation:  65.4376484817236
actions average: 
K:  1  action  0 :  tensor([0.2031, 0.0094, 0.1208, 0.1235, 0.2048, 0.1677, 0.1706],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0203, 0.7950, 0.0310, 0.0433, 0.0217, 0.0401, 0.0486],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1911, 0.0021, 0.1512, 0.1142, 0.2091, 0.1687, 0.1637],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1530, 0.0205, 0.0989, 0.2274, 0.2045, 0.1490, 0.1467],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2215, 0.0102, 0.1020, 0.1005, 0.3014, 0.1333, 0.1311],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1885, 0.0093, 0.1107, 0.1042, 0.1941, 0.2409, 0.1522],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1536, 0.1383, 0.0980, 0.1122, 0.1806, 0.1483, 0.1691],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  14.877786636352539
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.0894705263826886, 0.09491578731336345, 0.09491578731336345, 0.07913375987022106, 0.0894705263826886, 0.5520936127376749]
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.0894705263826886, 0.09491578731336345, 0.09491578731336345, 0.07913375987022106, 0.0894705263826886, 0.5520936127376749]
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.08957530449681535, 0.09511403791080186, 0.09511403791080186, 0.07906109869399346, 0.08957530449681535, 0.5515602164907722]
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.08957547195610129, 0.09511435484114573, 0.09511435484114573, 0.0790609824116103, 0.08957547195610129, 0.5515593639938957]
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.424]
 [0.417]
 [0.417]] [[41.377]
 [41.377]
 [41.377]
 [41.377]
 [44.166]
 [41.377]
 [41.377]] [[0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.424]
 [0.417]
 [0.417]]
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.08957547195610129, 0.09511435484114573, 0.09511435484114573, 0.0790609824116103, 0.08957547195610129, 0.5515593639938957]
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.08968151638011916, 0.09531500142403435, 0.09531500142403435, 0.07898744307641577, 0.08968151638011916, 0.5510195213152773]
maxi score, test score, baseline:  -0.9920875 -1.0 -0.9920875
probs:  [0.08968168503451322, 0.0953153206157264, 0.0953153206157264, 0.07898732596509153, 0.08968168503451322, 0.5510186627344292]
maxi score, test score, baseline:  -0.9920875 -1.0 -0.9920875
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.08968185106012247, 0.09531563483223286, 0.09531563483223286, 0.07898721067916697, 0.08968185106012247, 0.5510178175361222]
Printing some Q and Qe and total Qs values:  [[0.714]
 [0.673]
 [0.673]
 [0.673]
 [0.673]
 [0.654]
 [0.65 ]] [[50.836]
 [49.794]
 [49.794]
 [49.794]
 [49.794]
 [51.354]
 [51.545]] [[2.594]
 [2.479]
 [2.479]
 [2.479]
 [2.479]
 [2.57 ]
 [2.58 ]]
printing an ep nov before normalisation:  48.6696866061023
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.0851848916637468, 0.0963822820489794, 0.09068536413368566, 0.07987087588770429, 0.09068536413368566, 0.5571912221321982]
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.0851848916637468, 0.0963822820489794, 0.09068536413368566, 0.07987087588770429, 0.09068536413368566, 0.5571912221321982]
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.0851848916637468, 0.0963822820489794, 0.09068536413368566, 0.07987087588770429, 0.09068536413368566, 0.5571912221321982]
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.08521614802347305, 0.09660602674043162, 0.09081117616513694, 0.07981078185271301, 0.09081117616513694, 0.5567446910531084]
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.08521614802347305, 0.09660602674043162, 0.09081117616513694, 0.07981078185271301, 0.09081117616513694, 0.5567446910531084]
siam score:  -0.87079066
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.08521614802347305, 0.09660602674043162, 0.09081117616513694, 0.07981078185271301, 0.09081117616513694, 0.5567446910531084]
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.08521614802347305, 0.09660602674043162, 0.09081117616513694, 0.07981078185271301, 0.09081117616513694, 0.5567446910531084]
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.08521614802347305, 0.09660602674043162, 0.09081117616513694, 0.07981078185271301, 0.09081117616513694, 0.5567446910531084]
Printing some Q and Qe and total Qs values:  [[0.692]
 [0.636]
 [0.586]
 [0.558]
 [0.572]
 [0.593]
 [0.594]] [[50.023]
 [50.006]
 [48.515]
 [49.585]
 [48.708]
 [47.661]
 [46.738]] [[1.884]
 [1.827]
 [1.718]
 [1.733]
 [1.711]
 [1.691]
 [1.656]]
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.575]
 [0.533]
 [0.501]
 [0.506]
 [0.509]
 [0.51 ]] [[37.438]
 [45.42 ]
 [42.745]
 [37.266]
 [36.399]
 [35.652]
 [35.249]] [[1.325]
 [1.665]
 [1.516]
 [1.262]
 [1.232]
 [1.205]
 [1.19 ]]
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.08524779773053986, 0.09683258715899168, 0.09093857148486711, 0.07974993156110514, 0.09093857148486711, 0.556292540579629]
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.08524779773053986, 0.09683258715899168, 0.09093857148486711, 0.07974993156110514, 0.09093857148486711, 0.556292540579629]
printing an ep nov before normalisation:  44.55988857559827
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.423]
 [0.417]
 [0.417]
 [0.417]
 [0.433]
 [0.435]] [[64.247]
 [57.072]
 [61.021]
 [61.021]
 [61.021]
 [59.568]
 [59.371]] [[2.139]
 [1.79 ]
 [1.982]
 [1.982]
 [1.982]
 [1.925]
 [1.917]]
printing an ep nov before normalisation:  10.234449287443681
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.08524779773053986, 0.09683258715899168, 0.09093857148486711, 0.07974993156110514, 0.09093857148486711, 0.556292540579629]
Printing some Q and Qe and total Qs values:  [[0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.397]
 [0.389]] [[39.648]
 [39.648]
 [39.648]
 [39.648]
 [39.648]
 [39.444]
 [39.648]] [[2.118]
 [2.118]
 [2.118]
 [2.118]
 [2.118]
 [2.108]
 [2.118]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
actions average: 
K:  3  action  0 :  tensor([0.2875, 0.0380, 0.1078, 0.1064, 0.1653, 0.1326, 0.1624],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0454, 0.7200, 0.0335, 0.0460, 0.0530, 0.0469, 0.0552],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1629, 0.0376, 0.1731, 0.1431, 0.1557, 0.1452, 0.1825],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1872, 0.0730, 0.1142, 0.1266, 0.1609, 0.1624, 0.1757],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1668, 0.0239, 0.1189, 0.1185, 0.2503, 0.1423, 0.1794],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1809, 0.0288, 0.1161, 0.1081, 0.1404, 0.2562, 0.1696],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1659, 0.0740, 0.1240, 0.1236, 0.1547, 0.1642, 0.1935],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.08920837128309975, 0.10068487203651993, 0.08920837128309975, 0.07849697057990769, 0.09484595060056941, 0.5475554642168035]
printing an ep nov before normalisation:  9.057760660257088
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
siam score:  -0.8686046
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.489]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.434]] [[84.955]
 [77.609]
 [84.955]
 [84.955]
 [84.955]
 [84.955]
 [86.119]] [[2.345]
 [2.076]
 [2.345]
 [2.345]
 [2.345]
 [2.345]
 [2.403]]
actions average: 
K:  2  action  0 :  tensor([0.2268, 0.0201, 0.1089, 0.1360, 0.1797, 0.1670, 0.1615],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0221, 0.8302, 0.0161, 0.0637, 0.0211, 0.0164, 0.0304],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1665, 0.0330, 0.1651, 0.1263, 0.1914, 0.1820, 0.1357],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1588, 0.0657, 0.0868, 0.2139, 0.1690, 0.1560, 0.1498],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1999, 0.0114, 0.1044, 0.1329, 0.2235, 0.1675, 0.1603],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1837, 0.0367, 0.1092, 0.1430, 0.1779, 0.1762, 0.1732],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1409, 0.1152, 0.1104, 0.1297, 0.1660, 0.1854, 0.1523],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  64.25853252410889
printing an ep nov before normalisation:  55.406632167459975
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.09034053669992231, 0.09613779837450694, 0.084739792370239, 0.0793257395182118, 0.09613779837450694, 0.553318334662613]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.09034053669992231, 0.09613779837450694, 0.084739792370239, 0.0793257395182118, 0.09613779837450694, 0.553318334662613]
Printing some Q and Qe and total Qs values:  [[0.231]
 [0.281]
 [0.315]
 [0.3  ]
 [0.27 ]
 [0.249]
 [0.266]] [[61.72 ]
 [66.729]
 [59.887]
 [55.785]
 [60.831]
 [60.033]
 [72.812]] [[0.231]
 [0.281]
 [0.315]
 [0.3  ]
 [0.27 ]
 [0.249]
 [0.266]]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
printing an ep nov before normalisation:  78.17189930458828
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
printing an ep nov before normalisation:  36.24913648150141
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
printing an ep nov before normalisation:  45.56197450048458
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.09057423094748593, 0.09656575845581372, 0.0847858060665591, 0.07919032868166312, 0.09656575845581372, 0.5523181173926645]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.09057423094748593, 0.09656575845581372, 0.0847858060665591, 0.07919032868166312, 0.09656575845581372, 0.5523181173926645]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.09069316950116285, 0.0967835684549099, 0.08480922474923769, 0.07912141148904336, 0.0967835684549099, 0.5518090573507363]
printing an ep nov before normalisation:  32.401522646886335
printing an ep nov before normalisation:  46.14452887018405
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.09069316950116285, 0.0967835684549099, 0.08480922474923769, 0.07912141148904336, 0.0967835684549099, 0.5518090573507363]
printing an ep nov before normalisation:  34.23811693214808
printing an ep nov before normalisation:  19.056765698272436
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.09069316950116285, 0.0967835684549099, 0.08480922474923769, 0.07912141148904336, 0.0967835684549099, 0.5518090573507363]
Printing some Q and Qe and total Qs values:  [[0.28 ]
 [0.291]
 [0.281]
 [0.293]
 [0.286]
 [0.283]
 [0.279]] [[17.52 ]
 [15.661]
 [17.291]
 [17.812]
 [17.713]
 [17.956]
 [17.945]] [[2.036]
 [1.759]
 [2.002]
 [2.094]
 [2.072]
 [2.107]
 [2.101]]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.09093555168593376, 0.09722743851902134, 0.08485694915227286, 0.0789809667030673, 0.09722743851902134, 0.5507716554206833]
printing an ep nov before normalisation:  36.99827058349532
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.396]
 [0.43 ]
 [0.43 ]
 [0.428]
 [0.424]
 [0.428]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.427]
 [0.396]
 [0.43 ]
 [0.43 ]
 [0.428]
 [0.424]
 [0.428]]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.09105905255852088, 0.09745360340933654, 0.08488126614332614, 0.07890940594197116, 0.09745360340933654, 0.5502430685375086]
actions average: 
K:  0  action  0 :  tensor([0.2157, 0.0169, 0.1166, 0.1305, 0.1985, 0.1491, 0.1727],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0090, 0.8734, 0.0146, 0.0118, 0.0049, 0.0061, 0.0803],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1579, 0.0056, 0.2335, 0.1164, 0.1271, 0.2004, 0.1590],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1896, 0.0276, 0.1245, 0.1457, 0.1713, 0.1743, 0.1670],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1984, 0.0109, 0.1180, 0.1335, 0.2409, 0.1426, 0.1557],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1931, 0.0102, 0.1282, 0.1392, 0.1771, 0.1764, 0.1757],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1785, 0.0429, 0.1299, 0.1408, 0.1592, 0.1576, 0.1911],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  43.30954496480761
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.09164469543826044, 0.09164469543826044, 0.08542711860494721, 0.07941679433274434, 0.09808043286221622, 0.5537862633235713]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.09164469543826044, 0.09164469543826044, 0.08542711860494721, 0.07941679433274434, 0.09808043286221622, 0.5537862633235713]
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
probs:  [0.0923444076085193, 0.0923444076085193, 0.07983924551234897, 0.07983924551234897, 0.0989260718696616, 0.5567066218886019]
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
probs:  [0.0923444076085193, 0.0923444076085193, 0.07983924551234897, 0.07983924551234897, 0.0989260718696616, 0.5567066218886019]
printing an ep nov before normalisation:  36.03647552165987
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
probs:  [0.09249178781050055, 0.09249178781050055, 0.07978133904016381, 0.07978133904016381, 0.09918149768962509, 0.5562722486090461]
printing an ep nov before normalisation:  0.16733787730828453
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
probs:  [0.09249178781050055, 0.09249178781050055, 0.07978133904016381, 0.07978133904016381, 0.09918149768962509, 0.5562722486090461]
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.32810131512289
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.09389583801466786, 0.09389583801466786, 0.08080182253563706, 0.08080182253563706, 0.08723786404227932, 0.5633668148571108]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.09407004815658708, 0.09407004815658708, 0.08075938624527428, 0.08075938624527428, 0.08730191498134332, 0.5630392162149339]
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.435]
 [0.452]
 [0.45 ]
 [0.453]
 [0.458]
 [0.463]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.447]
 [0.435]
 [0.452]
 [0.45 ]
 [0.453]
 [0.458]
 [0.463]]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.09487729893076566, 0.09487729893076566, 0.08125629670336137, 0.08125629670336137, 0.08125629670336137, 0.5664765120283847]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.09487729893076566, 0.09487729893076566, 0.08125629670336137, 0.08125629670336137, 0.08125629670336137, 0.5664765120283847]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.09487729893076566, 0.09487729893076566, 0.08125629670336137, 0.08125629670336137, 0.08125629670336137, 0.5664765120283847]
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.492]
 [0.485]
 [0.488]
 [0.5  ]
 [0.476]
 [0.47 ]] [[47.97 ]
 [53.645]
 [47.46 ]
 [47.13 ]
 [46.035]
 [45.858]
 [44.753]] [[1.024]
 [1.1  ]
 [0.986]
 [0.984]
 [0.977]
 [0.95 ]
 [0.924]]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.09487729893076566, 0.09487729893076566, 0.08125629670336137, 0.08125629670336137, 0.08125629670336137, 0.5664765120283847]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.0955385987944226, 0.08856432344965685, 0.08182252394971666, 0.08182252394971666, 0.08182252394971666, 0.5704295059067708]
printing an ep nov before normalisation:  48.10128739083037
printing an ep nov before normalisation:  48.74856472015381
actions average: 
K:  2  action  0 :  tensor([0.2336, 0.0510, 0.1145, 0.1214, 0.2017, 0.1315, 0.1463],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0232, 0.7295, 0.0256, 0.0860, 0.0573, 0.0391, 0.0393],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1682, 0.0084, 0.1173, 0.1339, 0.1998, 0.2043, 0.1681],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1510, 0.0258, 0.1099, 0.2285, 0.1850, 0.1451, 0.1547],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1711, 0.0015, 0.1287, 0.1311, 0.2582, 0.1524, 0.1569],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1592, 0.0147, 0.1319, 0.1259, 0.1798, 0.2195, 0.1690],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1696, 0.0066, 0.1288, 0.1325, 0.1873, 0.1648, 0.2104],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  17.180572136586292
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.09926220849896969, 0.09255049101030545, 0.0797872249662883, 0.08606249743793006, 0.08606249743793006, 0.5562750806485764]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.10260814267066942, 0.09613235251394477, 0.07795836078378215, 0.08987242202911097, 0.08987242202911097, 0.5435562999733817]
printing an ep nov before normalisation:  64.46526077811879
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.365]
 [0.365]
 [0.365]
 [0.678]
 [0.907]
 [0.365]] [[53.552]
 [56.622]
 [56.622]
 [56.622]
 [49.837]
 [47.355]
 [56.622]] [[1.202]
 [1.209]
 [1.209]
 [1.209]
 [1.325]
 [1.481]
 [1.209]]
printing an ep nov before normalisation:  47.21303939819336
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.1065975240574372, 0.1002031275489376, 0.07665550866049463, 0.09402187759072131, 0.08804329156556134, 0.5344786705768478]
printing an ep nov before normalisation:  68.31318343938223
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.10694601077967059, 0.10045569181062769, 0.07655483465478728, 0.09418171680721953, 0.08811344590228387, 0.5337483000454111]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.10729848532846363, 0.10071114623256362, 0.07645300860956669, 0.09434338510652691, 0.08818440303380295, 0.5330095716890763]
printing an ep nov before normalisation:  36.55932664871216
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.10729848532846363, 0.10071114623256362, 0.07645300860956669, 0.09434338510652691, 0.08818440303380295, 0.5330095716890763]
printing an ep nov before normalisation:  93.29370498657227
printing an ep nov before normalisation:  34.529095606725484
printing an ep nov before normalisation:  54.29708003997803
printing an ep nov before normalisation:  52.550776823323986
siam score:  -0.8611899
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.10729848532846363, 0.10071114623256362, 0.07645300860956669, 0.09434338510652691, 0.08818440303380295, 0.5330095716890763]
using explorer policy with actor:  1
main train batch thing paused
add a thread
using another actor
Adding thread: now have 4 threads
printing an ep nov before normalisation:  57.412665979896566
printing an ep nov before normalisation:  75.2741448925935
Printing some Q and Qe and total Qs values:  [[0.284]
 [0.39 ]
 [0.273]
 [0.251]
 [0.261]
 [0.281]
 [0.292]] [[42.896]
 [47.076]
 [42.986]
 [44.024]
 [43.54 ]
 [43.727]
 [43.258]] [[0.284]
 [0.39 ]
 [0.273]
 [0.251]
 [0.261]
 [0.281]
 [0.292]]
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
probs:  [0.11116289838727283, 0.10465926437669786, 0.0751896727662801, 0.09837241816647536, 0.08640713021798745, 0.5242086160852865]
printing an ep nov before normalisation:  63.45055585790601
printing an ep nov before normalisation:  52.33109249805083
printing an ep nov before normalisation:  56.3309263181191
printing an ep nov before normalisation:  29.161589810717498
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
probs:  [0.11116425619435301, 0.10466030444375003, 0.07518927307382996, 0.0983731510848337, 0.08640727856302537, 0.524205736640208]
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
probs:  [0.11116425619435301, 0.10466030444375003, 0.07518927307382996, 0.0983731510848337, 0.08640727856302537, 0.524205736640208]
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
probs:  [0.11116425619435301, 0.10466030444375003, 0.07518927307382996, 0.0983731510848337, 0.08640727856302537, 0.524205736640208]
printing an ep nov before normalisation:  33.34834277043419
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
probs:  [0.11225856364377139, 0.10561955204366422, 0.07553653073067888, 0.09299454637788676, 0.08698748723046038, 0.5266033199735383]
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
probs:  [0.11267687963931551, 0.10594174408210062, 0.07542316108847102, 0.09313394531756099, 0.08703991203443323, 0.5257843578381186]
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.11396064944961572, 0.10693051909063214, 0.0750752409014879, 0.09356174660469618, 0.08720079840574281, 0.5232710455478252]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.2792],
        [-0.5739],
        [-0.6005],
        [-0.5658],
        [-0.6157],
        [-0.5792],
        [-0.3630],
        [-0.2907],
        [-0.5114]], dtype=torch.float64)
-0.7893732000000001 -0.7893732000000001
-0.045546567066 -0.32479123891429657
-0.032346567066 -0.6062850233284353
-0.032346567066 -0.6328030343637273
-0.032346567066 -0.5981254721798184
-0.032346567066 -0.6480824255735129
-0.032346567066 -0.6114981390427343
-0.09703970119800001 -0.4600389268906455
-0.084359833866 -0.37507361470249473
-0.09703970119800001 -0.6084402362824953
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.10768724273865339, 0.10768724273865339, 0.07560625056564176, 0.09422374438407796, 0.08781772500569138, 0.526977794567282]
printing an ep nov before normalisation:  27.008667204088606
siam score:  -0.8673204
using explorer policy with actor:  1
from probs:  [0.10872177781473256, 0.10872177781473256, 0.07597190088628965, 0.09497756716935328, 0.08210600481891864, 0.5295009714959734]
printing an ep nov before normalisation:  78.33861176068768
maxi score, test score, baseline:  -0.9928078014184397 -1.0 -0.9928078014184397
probs:  [0.10981762790278392, 0.10981762790278392, 0.07637089228817581, 0.08910223036083306, 0.0826355189588484, 0.5322561025865749]
maxi score, test score, baseline:  -0.9928078014184397 -1.0 -0.9928078014184397
probs:  [0.10981762790278392, 0.10981762790278392, 0.07637089228817581, 0.08910223036083306, 0.0826355189588484, 0.5322561025865749]
maxi score, test score, baseline:  -0.9928577464788733 -1.0 -0.9928577464788733
maxi score, test score, baseline:  -0.9928577464788733 -1.0 -0.9928577464788733
probs:  [0.10981825519646068, 0.10981825519646068, 0.07637072625101882, 0.08910236630121926, 0.08263550151381596, 0.5322548955410246]
printing an ep nov before normalisation:  82.79395996795263
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.94050084540154
printing an ep nov before normalisation:  61.161118417868124
printing an ep nov before normalisation:  59.78904813270394
maxi score, test score, baseline:  -0.9928577464788733 -1.0 -0.9928577464788733
probs:  [0.11059041662125865, 0.11059041662125865, 0.0761665047805453, 0.08926980025539749, 0.08261415810944087, 0.5307687036120989]
maxi score, test score, baseline:  -0.9928577464788733 -1.0 -0.9928577464788733
probs:  [0.11059041662125865, 0.11059041662125865, 0.0761665047805453, 0.08926980025539749, 0.08261415810944087, 0.5307687036120989]
printing an ep nov before normalisation:  57.104504859337126
siam score:  -0.8589421
maxi score, test score, baseline:  -0.9928577464788733 -1.0 -0.9928577464788733
probs:  [0.11212505448089045, 0.11212505448089045, 0.07646416648536969, 0.09003831094818081, 0.07646416648536969, 0.5327832471192988]
maxi score, test score, baseline:  -0.9928577464788733 -1.0 -0.9928577464788733
probs:  [0.11212505448089045, 0.11212505448089045, 0.07646416648536969, 0.09003831094818081, 0.07646416648536969, 0.5327832471192988]
maxi score, test score, baseline:  -0.992906993006993 -1.0 -0.992906993006993
maxi score, test score, baseline:  -0.992906993006993 -1.0 -0.992906993006993
probs:  [0.10531878335565069, 0.11298499287476688, 0.07704963575390991, 0.09072825556120387, 0.07704963575390991, 0.5368686967005587]
maxi score, test score, baseline:  -0.992906993006993 -1.0 -0.992906993006993
probs:  [0.10531878335565069, 0.11298499287476688, 0.07704963575390991, 0.09072825556120387, 0.07704963575390991, 0.5368686967005587]
printing an ep nov before normalisation:  24.204438897934867
printing an ep nov before normalisation:  42.27896314370833
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.477]
 [0.403]
 [0.402]
 [0.409]
 [0.406]
 [0.403]] [[24.224]
 [36.242]
 [22.517]
 [22.209]
 [22.271]
 [22.207]
 [21.727]] [[0.423]
 [0.477]
 [0.403]
 [0.402]
 [0.409]
 [0.406]
 [0.403]]
printing an ep nov before normalisation:  27.85944686912429
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]] [[63.182]
 [63.182]
 [63.182]
 [63.182]
 [63.182]
 [63.182]
 [63.182]] [[2.326]
 [2.326]
 [2.326]
 [2.326]
 [2.326]
 [2.326]
 [2.326]]
maxi score, test score, baseline:  -0.992906993006993 -1.0 -0.992906993006993
probs:  [0.10531878335565069, 0.11298499287476688, 0.07704963575390991, 0.09072825556120387, 0.07704963575390991, 0.5368686967005587]
using explorer policy with actor:  1
from probs:  [0.10680576290272102, 0.10680576290272102, 0.07747498060133001, 0.0916672946181322, 0.07747498060133001, 0.5397712183737657]
maxi score, test score, baseline:  -0.992906993006993 -1.0 -0.992906993006993
probs:  [0.09988044412176039, 0.10763345820470122, 0.07807509201348917, 0.09237752726730149, 0.07807509201348917, 0.5439583863792586]
printing an ep nov before normalisation:  48.418597449237765
maxi score, test score, baseline:  -0.992906993006993 -1.0 -0.992906993006993
printing an ep nov before normalisation:  60.56063530605027
maxi score, test score, baseline:  -0.9929555555555556 -1.0 -0.9929555555555556
probs:  [0.09988085106876902, 0.10763405626098857, 0.07807496146565138, 0.09237774926984685, 0.07807496146565138, 0.5439574204690928]
maxi score, test score, baseline:  -0.9929555555555556 -1.0 -0.9929555555555556
probs:  [0.09988085106876902, 0.10763405626098857, 0.07807496146565138, 0.09237774926984685, 0.07807496146565138, 0.5439574204690928]
actions average: 
K:  2  action  0 :  tensor([0.3103, 0.0152, 0.0901, 0.1123, 0.1913, 0.1307, 0.1500],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0461, 0.7144, 0.0269, 0.0479, 0.0657, 0.0463, 0.0526],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1145, 0.0456, 0.2240, 0.1004, 0.1394, 0.2298, 0.1463],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1937, 0.0254, 0.0922, 0.1735, 0.1919, 0.1618, 0.1615],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1892, 0.0171, 0.1062, 0.1220, 0.2374, 0.1767, 0.1513],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1757, 0.0598, 0.0952, 0.1089, 0.1649, 0.2607, 0.1349],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1673, 0.0809, 0.0938, 0.1303, 0.1569, 0.1452, 0.2255],
       grad_fn=<DivBackward0>)
siam score:  -0.8552704
maxi score, test score, baseline:  -0.9929555555555556 -1.0 -0.9929555555555556
probs:  [0.09988085106876902, 0.10763405626098857, 0.07807496146565138, 0.09237774926984685, 0.07807496146565138, 0.5439574204690928]
maxi score, test score, baseline:  -0.9929555555555556 -1.0 -0.9929555555555556
probs:  [0.09988085106876902, 0.10763405626098857, 0.07807496146565138, 0.09237774926984685, 0.07807496146565138, 0.5439574204690928]
maxi score, test score, baseline:  -0.9929555555555556 -1.0 -0.9929555555555556
probs:  [0.10012475044880738, 0.10799246328896703, 0.07799680808585814, 0.09251083479703985, 0.07799680808585814, 0.5433783352934696]
printing an ep nov before normalisation:  44.56734395617603
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  25.892681753284705
maxi score, test score, baseline:  -0.9930972789115646 -1.0 -0.9930972789115646
probs:  [0.10012596348630493, 0.10799424598857771, 0.07799641894866277, 0.09251149654862166, 0.07799641894866277, 0.5433754560791701]
maxi score, test score, baseline:  -0.9930972789115646 -1.0 -0.9930972789115646
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.677]
 [0.434]
 [0.434]
 [0.434]
 [0.321]
 [0.306]] [[71.145]
 [67.373]
 [71.145]
 [71.145]
 [71.145]
 [72.303]
 [72.285]] [[2.176]
 [2.263]
 [2.176]
 [2.176]
 [2.176]
 [2.111]
 [2.095]]
maxi score, test score, baseline:  -0.9930972789115646 -1.0 -0.9930972789115646
probs:  [0.10577123668904652, 0.0982433380174651, 0.07704940791132049, 0.0982433380174651, 0.08389328117476302, 0.5367993981899398]
printing an ep nov before normalisation:  67.42909654889286
actions average: 
K:  2  action  0 :  tensor([0.2351, 0.0158, 0.1444, 0.1192, 0.1547, 0.1809, 0.1499],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0769, 0.7610, 0.0242, 0.0341, 0.0294, 0.0253, 0.0491],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1754, 0.0309, 0.1398, 0.1312, 0.1939, 0.1757, 0.1530],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1614, 0.0227, 0.1273, 0.1457, 0.2082, 0.1815, 0.1531],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1701, 0.0214, 0.1286, 0.1377, 0.2130, 0.1763, 0.1530],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1571, 0.0112, 0.1261, 0.1445, 0.1808, 0.2332, 0.1472],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1501, 0.0305, 0.1329, 0.1227, 0.1681, 0.1821, 0.2135],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9931432432432432 -1.0 -0.9931432432432432
probs:  [0.10609319024620553, 0.09845711573697652, 0.07695862904176241, 0.09845711573697652, 0.08390084870375861, 0.5361331005343205]
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.642]
 [0.52 ]
 [0.671]] [[67.964]
 [58.14 ]
 [58.14 ]
 [58.14 ]
 [72.863]
 [58.14 ]
 [73.159]] [[2.188]
 [1.592]
 [1.592]
 [1.592]
 [2.387]
 [1.592]
 [2.429]]
printing an ep nov before normalisation:  77.12803322129993
maxi score, test score, baseline:  -0.9931432432432432 -1.0 -0.9931432432432432
probs:  [0.10609319024620553, 0.09845711573697652, 0.07695862904176241, 0.09845711573697652, 0.08390084870375861, 0.5361331005343205]
printing an ep nov before normalisation:  61.215200863691436
maxi score, test score, baseline:  -0.9931432432432432 -1.0 -0.9931432432432432
probs:  [0.10641874262817379, 0.09867328311869235, 0.07686683557661399, 0.09867328311869235, 0.08390850092874344, 0.535459354629084]
printing an ep nov before normalisation:  86.45616402458907
siam score:  -0.86267626
maxi score, test score, baseline:  -0.9931432432432432 -1.0 -0.9931432432432432
probs:  [0.10641874262817379, 0.09867328311869235, 0.07686683557661399, 0.09867328311869235, 0.08390850092874344, 0.535459354629084]
from probs:  [0.10708251140803482, 0.09911402681204153, 0.07667967787255289, 0.09911402681204153, 0.08392410305092945, 0.5340856540443996]
printing an ep nov before normalisation:  81.3818728262462
Printing some Q and Qe and total Qs values:  [[0.703]
 [0.782]
 [0.782]
 [0.687]
 [0.782]
 [0.68 ]
 [0.793]] [[68.29 ]
 [64.242]
 [64.242]
 [67.636]
 [64.242]
 [67.838]
 [67.747]] [[2.312]
 [2.206]
 [2.206]
 [2.266]
 [2.206]
 [2.268]
 [2.377]]
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.589]
 [0.537]
 [0.515]
 [0.519]
 [0.547]
 [0.49 ]] [[59.433]
 [68.515]
 [58.396]
 [56.719]
 [57.427]
 [65.525]
 [62.364]] [[0.993]
 [1.188]
 [1.001]
 [0.957]
 [0.971]
 [1.106]
 [1.007]]
Printing some Q and Qe and total Qs values:  [[0.193]
 [0.193]
 [0.193]
 [0.193]
 [0.193]
 [0.193]
 [0.193]] [[30.086]
 [30.086]
 [30.086]
 [30.086]
 [30.086]
 [30.086]
 [30.086]] [[0.193]
 [0.193]
 [0.193]
 [0.193]
 [0.193]
 [0.193]
 [0.193]]
maxi score, test score, baseline:  -0.9932333333333333 -1.0 -0.9932333333333333
probs:  [0.1091022839769117, 0.10089314465743379, 0.07778126011182655, 0.08524447282967888, 0.08524447282967888, 0.5417343655944702]
maxi score, test score, baseline:  -0.9932333333333333 -1.0 -0.9932333333333333
probs:  [0.1091022839769117, 0.10089314465743379, 0.07778126011182655, 0.08524447282967888, 0.08524447282967888, 0.5417343655944702]
printing an ep nov before normalisation:  46.651899938762604
printing an ep nov before normalisation:  58.39138192359376
line 256 mcts: sample exp_bonus 71.65903022001287
printing an ep nov before normalisation:  72.67313412182004
printing an ep nov before normalisation:  54.57377417221235
printing an ep nov before normalisation:  61.93647093087039
siam score:  -0.86295104
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.1028325686764014, 0.09463407574176964, 0.07899387383570299, 0.08669178571134514, 0.08669178571134514, 0.5501559103234358]
Printing some Q and Qe and total Qs values:  [[0.331]
 [0.23 ]
 [0.331]
 [0.331]
 [0.331]
 [0.331]
 [0.331]] [[29.507]
 [41.919]
 [29.507]
 [29.507]
 [29.507]
 [29.507]
 [29.507]] [[1.198]
 [1.686]
 [1.198]
 [1.198]
 [1.198]
 [1.198]
 [1.198]]
Printing some Q and Qe and total Qs values:  [[0.209]
 [0.244]
 [0.171]
 [0.292]
 [0.199]
 [0.181]
 [0.156]] [[22.339]
 [25.197]
 [21.834]
 [24.859]
 [21.62 ]
 [21.676]
 [21.744]] [[1.125]
 [1.334]
 [1.055]
 [1.361]
 [1.071]
 [1.056]
 [1.035]]
printing an ep nov before normalisation:  35.97445104178746
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.1028325686764014, 0.09463407574176964, 0.07899387383570299, 0.08669178571134514, 0.08669178571134514, 0.5501559103234358]
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.10311896650288158, 0.09479985090467852, 0.07892953807118348, 0.08674070766891932, 0.08674070766891932, 0.5496702291834179]
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
Printing some Q and Qe and total Qs values:  [[0.295]
 [0.291]
 [0.255]
 [0.266]
 [0.263]
 [0.269]
 [0.285]] [[35.689]
 [31.851]
 [34.649]
 [34.567]
 [34.369]
 [34.287]
 [34.323]] [[1.998]
 [1.631]
 [1.86 ]
 [1.863]
 [1.842]
 [1.84 ]
 [1.859]]
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.444]
 [0.445]] [[50.686]
 [50.686]
 [50.686]
 [50.686]
 [50.686]
 [50.031]
 [50.298]] [[1.017]
 [1.017]
 [1.017]
 [1.017]
 [1.017]
 [0.999]
 [1.005]]
printing an ep nov before normalisation:  15.020003014371603
Printing some Q and Qe and total Qs values:  [[0.321]
 [0.348]
 [0.271]
 [0.284]
 [0.304]
 [0.263]
 [0.265]] [[20.126]
 [24.746]
 [17.596]
 [18.907]
 [20.007]
 [16.962]
 [16.763]] [[0.718]
 [0.901]
 [0.583]
 [0.64 ]
 [0.697]
 [0.553]
 [0.549]]
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.10742213414879472, 0.09935274636635197, 0.07661174443401354, 0.09153552695211069, 0.09153552695211069, 0.5335423211466185]
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.10742213414879472, 0.09935274636635197, 0.07661174443401354, 0.09153552695211069, 0.09153552695211069, 0.5335423211466185]
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.10742213414879472, 0.09935274636635197, 0.07661174443401354, 0.09153552695211069, 0.09153552695211069, 0.5335423211466185]
printing an ep nov before normalisation:  78.66486446701026
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.10775997823436452, 0.09957726098002366, 0.0765168759905178, 0.09165025363988102, 0.09165025363988102, 0.5328453775153321]
printing an ep nov before normalisation:  23.285189153949464
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.10810226557141212, 0.09980472836076047, 0.07642075985801466, 0.09176648918794161, 0.09176648918794161, 0.5321392678339296]
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.10810226557141212, 0.09980472836076047, 0.07642075985801466, 0.09176648918794161, 0.09176648918794161, 0.5321392678339296]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
probs:  [0.11212044268525517, 0.10406552174164743, 0.07425029198023328, 0.09626231707752733, 0.09626231707752733, 0.5170391094378095]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
probs:  [0.10520134721119112, 0.10520134721119112, 0.0747375575785674, 0.09722840226827797, 0.09722840226827797, 0.5204029434624944]
printing an ep nov before normalisation:  29.773043348179442
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
probs:  [0.10520134721119112, 0.10520134721119112, 0.0747375575785674, 0.09722840226827797, 0.09722840226827797, 0.5204029434624944]
printing an ep nov before normalisation:  49.289164543151855
printing an ep nov before normalisation:  40.47438823402245
printing an ep nov before normalisation:  66.35931393143805
maxi score, test score, baseline:  -0.9934064935064936 -1.0 -0.9934064935064936
probs:  [0.11115404822328422, 0.11115404822328422, 0.07365765394614943, 0.1031860644393931, 0.08797445903378265, 0.5128737261341063]
maxi score, test score, baseline:  -0.9934064935064936 -1.0 -0.9934064935064936
probs:  [0.11115404822328422, 0.11115404822328422, 0.07365765394614943, 0.1031860644393931, 0.08797445903378265, 0.5128737261341063]
Printing some Q and Qe and total Qs values:  [[0.298]
 [0.482]
 [0.635]
 [0.551]
 [0.547]
 [0.547]
 [0.534]] [[62.788]
 [48.171]
 [44.811]
 [59.779]
 [60.911]
 [61.349]
 [60.306]] [[1.866]
 [1.681]
 [1.75 ]
 [2.043]
 [2.067]
 [2.078]
 [2.039]]
printing an ep nov before normalisation:  46.407343623966405
maxi score, test score, baseline:  -0.9934064935064936 -1.0 -0.9934064935064936
probs:  [0.11188676220305628, 0.11188676220305628, 0.07340164147668142, 0.10370867404870161, 0.08809596029947904, 0.5110201997690252]
Printing some Q and Qe and total Qs values:  [[-0.028]
 [-0.026]
 [-0.021]
 [-0.026]
 [-0.021]
 [-0.024]
 [-0.026]] [[23.427]
 [18.107]
 [21.912]
 [18.107]
 [23.006]
 [22.909]
 [18.107]] [[1.365]
 [1.051]
 [1.282]
 [1.051]
 [1.347]
 [1.338]
 [1.051]]
maxi score, test score, baseline:  -0.9934064935064936 -1.0 -0.9934064935064936
printing an ep nov before normalisation:  15.737330874595266
maxi score, test score, baseline:  -0.9934483870967742 -1.0 -0.9934483870967742
probs:  [0.11740997152503153, 0.10934148705905308, 0.07250710145349923, 0.10934148705905308, 0.08658113535651679, 0.5048188175468463]
siam score:  -0.86000806
Printing some Q and Qe and total Qs values:  [[0.082]
 [0.095]
 [0.085]
 [0.086]
 [0.079]
 [0.079]
 [0.083]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.082]
 [0.095]
 [0.085]
 [0.086]
 [0.079]
 [0.079]
 [0.083]]
from probs:  [0.12111254845260366, 0.08931040647037207, 0.07479203730457072, 0.1047222137386843, 0.08931040647037207, 0.520752387563397]
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.12201184774675576, 0.08255202894670972, 0.07534632290496225, 0.10549943126427497, 0.08997283069119602, 0.5246175384461014]
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.12201184774675576, 0.08255202894670972, 0.07534632290496225, 0.10549943126427497, 0.08997283069119602, 0.5246175384461014]
printing an ep nov before normalisation:  31.734324229655726
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.12201184774675576, 0.08255202894670972, 0.07534632290496225, 0.10549943126427497, 0.08997283069119602, 0.5246175384461014]
Printing some Q and Qe and total Qs values:  [[0.127]
 [0.26 ]
 [0.215]
 [0.201]
 [0.154]
 [0.164]
 [0.156]] [[67.624]
 [68.879]
 [65.331]
 [64.933]
 [65.635]
 [64.604]
 [65.829]] [[0.434]
 [0.577]
 [0.503]
 [0.486]
 [0.445]
 [0.447]
 [0.448]]
printing an ep nov before normalisation:  50.9319107672403
printing an ep nov before normalisation:  27.628815257322323
printing an ep nov before normalisation:  60.20359085841503
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.11871903294377699, 0.08788062932303299, 0.07378307338212152, 0.11065360430450544, 0.09524502421753901, 0.513718635829024]
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.546]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]] [[46.318]
 [61.718]
 [46.318]
 [46.318]
 [46.318]
 [46.318]
 [46.318]] [[1.058]
 [1.613]
 [1.058]
 [1.058]
 [1.058]
 [1.058]
 [1.058]]
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.11871903294377699, 0.08788062932303299, 0.07378307338212152, 0.11065360430450544, 0.09524502421753901, 0.513718635829024]
printing an ep nov before normalisation:  36.74397032339527
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.852]
 [0.422]
 [0.422]
 [0.422]
 [0.422]
 [0.422]] [[41.917]
 [49.087]
 [41.917]
 [41.917]
 [41.917]
 [41.917]
 [41.917]] [[1.223]
 [1.86 ]
 [1.223]
 [1.223]
 [1.223]
 [1.223]
 [1.223]]
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.11243917448141871, 0.08929827870681326, 0.07497296227491462, 0.1044917961345846, 0.09678165296228272, 0.5220161354399862]
printing an ep nov before normalisation:  15.260021686553955
printing an ep nov before normalisation:  34.94044712611608
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.56923391341934
printing an ep nov before normalisation:  17.373165295787203
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.11540574690101718, 0.09096742299845964, 0.07583893677306687, 0.09096742299845964, 0.09887036356396338, 0.5279501067650332]
Printing some Q and Qe and total Qs values:  [[0.096]
 [0.07 ]
 [0.1  ]
 [0.1  ]
 [0.1  ]
 [0.1  ]
 [0.101]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.096]
 [0.07 ]
 [0.1  ]
 [0.1  ]
 [0.1  ]
 [0.1  ]
 [0.101]]
printing an ep nov before normalisation:  39.456293424761746
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.10909887485690141, 0.09257331502505109, 0.07699207289787793, 0.08466978640981829, 0.1007127698676042, 0.5359531809427471]
from probs:  [0.10944277327334187, 0.09269635750595027, 0.07690687978240963, 0.08468720213893692, 0.10094459213765061, 0.5353221951617108]
Printing some Q and Qe and total Qs values:  [[0.096]
 [0.068]
 [0.096]
 [0.098]
 [0.097]
 [0.098]
 [0.099]] [[23.639]
 [32.01 ]
 [23.345]
 [23.56 ]
 [23.246]
 [23.036]
 [23.215]] [[0.096]
 [0.068]
 [0.096]
 [0.098]
 [0.097]
 [0.098]
 [0.099]]
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.10979124740139834, 0.09282103711828772, 0.07682055313706908, 0.08470484959158266, 0.1011794988995213, 0.5346828138521409]
actions average: 
K:  4  action  0 :  tensor([0.2469, 0.0503, 0.1182, 0.1165, 0.1683, 0.1428, 0.1569],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0496, 0.6345, 0.0492, 0.0581, 0.0471, 0.0725, 0.0891],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1788, 0.0574, 0.1211, 0.1451, 0.1670, 0.1746, 0.1559],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1704, 0.0750, 0.1246, 0.1642, 0.1578, 0.1537, 0.1543],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1923, 0.0026, 0.1251, 0.1291, 0.2411, 0.1714, 0.1384],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2088, 0.0368, 0.1153, 0.1286, 0.1813, 0.1785, 0.1506],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2207, 0.0189, 0.1445, 0.1173, 0.1832, 0.1652, 0.1501],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  22.878488757191857
siam score:  -0.85899323
maxi score, test score, baseline:  -0.9935708860759493 -1.0 -0.9935708860759493
Printing some Q and Qe and total Qs values:  [[0.563]
 [0.611]
 [0.543]
 [0.611]
 [0.611]
 [0.611]
 [0.611]] [[40.587]
 [28.219]
 [40.359]
 [28.219]
 [28.219]
 [28.219]
 [28.219]] [[1.988]
 [1.342]
 [1.956]
 [1.342]
 [1.342]
 [1.342]
 [1.342]]
actions average: 
K:  4  action  0 :  tensor([0.2631, 0.0576, 0.1133, 0.1084, 0.1847, 0.1380, 0.1350],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0324, 0.8190, 0.0240, 0.0308, 0.0225, 0.0244, 0.0470],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2207, 0.0069, 0.1317, 0.1300, 0.1766, 0.1789, 0.1552],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1835, 0.0270, 0.0994, 0.2151, 0.1768, 0.1563, 0.1419],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1916, 0.0856, 0.1054, 0.1345, 0.1908, 0.1574, 0.1347],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1618, 0.0719, 0.0997, 0.1320, 0.1949, 0.2000, 0.1398],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1885, 0.0721, 0.1235, 0.1215, 0.1639, 0.1569, 0.1735],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]] [[32.948]
 [29.235]
 [29.235]
 [29.235]
 [29.235]
 [29.235]
 [29.235]] [[2.06]
 [1.81]
 [1.81]
 [1.81]
 [1.81]
 [1.81]
 [1.81]]
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
probs:  [0.111059916180638, 0.08542497230447955, 0.07736827565768678, 0.08542497230447955, 0.1022598608201657, 0.5384620027325505]
actions average: 
K:  0  action  0 :  tensor([0.1932, 0.0591, 0.1116, 0.1233, 0.1749, 0.1779, 0.1599],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0124, 0.8761, 0.0126, 0.0298, 0.0067, 0.0163, 0.0462],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1986, 0.0198, 0.1255, 0.1241, 0.1697, 0.1999, 0.1622],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1957, 0.0190, 0.1044, 0.1466, 0.1681, 0.1684, 0.1978],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1681, 0.0100, 0.0971, 0.1168, 0.3006, 0.1680, 0.1394],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1747, 0.0104, 0.1026, 0.1190, 0.1644, 0.2782, 0.1507],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1759, 0.0325, 0.1137, 0.1293, 0.1737, 0.1789, 0.1960],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  38.74494771896666
printing an ep nov before normalisation:  43.24832945456125
printing an ep nov before normalisation:  33.1361037350845
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.10434801990384326, 0.08698035640145853, 0.0786686888681744, 0.08698035640145853, 0.09553648474454507, 0.5474860936805203]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.10492822904916754, 0.08708028809842423, 0.0785387735005686, 0.08708028809842423, 0.09587302371386403, 0.5464993975395512]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.10492822904916754, 0.08708028809842423, 0.0785387735005686, 0.08708028809842423, 0.09587302371386403, 0.5464993975395512]
Printing some Q and Qe and total Qs values:  [[0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]] [[72.325]
 [72.325]
 [72.325]
 [72.325]
 [72.325]
 [72.325]
 [72.325]] [[2.526]
 [2.526]
 [2.526]
 [2.526]
 [2.526]
 [2.526]
 [2.526]]
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.11330374466078436, 0.09684877374022438, 0.07388037683027607, 0.09684877374022438, 0.104955266767265, 0.5141630642612258]
printing an ep nov before normalisation:  9.445299883853409
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.11422940446572849, 0.09763984279261088, 0.07448357962388424, 0.09763984279261088, 0.09763984279261088, 0.5183674875325546]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
printing an ep nov before normalisation:  46.87570215771938
printing an ep nov before normalisation:  75.7687514204774
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
from probs:  [0.1155468970509416, 0.09050749609392453, 0.07497379364836765, 0.09861203650030197, 0.09861203650030197, 0.5217477402061623]
printing an ep nov before normalisation:  99.55151336483407
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.11595564121938441, 0.09059819701776087, 0.07486718996675353, 0.09880567895741678, 0.09880567895741678, 0.5209676138812676]
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.491]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.474]
 [0.491]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]]
printing an ep nov before normalisation:  76.90534687117226
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.568]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]] [[47.525]
 [53.987]
 [39.742]
 [39.742]
 [39.742]
 [39.742]
 [39.742]] [[1.673]
 [1.967]
 [1.371]
 [1.371]
 [1.371]
 [1.371]
 [1.371]]
using another actor
printing an ep nov before normalisation:  44.74954624395702
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.11979127837679386, 0.095236402966685, 0.07269973649439328, 0.10318411626367674, 0.10318411626367674, 0.5059043496347745]
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.671295767443404
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  68 total reward:  0.0733333333333327  reward:  1.0 rdn_beta:  2.0
from probs:  [0.11979127837679386, 0.095236402966685, 0.07269973649439328, 0.10318411626367674, 0.10318411626367674, 0.5059043496347745]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  2.4205810277777573
from probs:  [0.10779362844689769, 0.09162066560337806, 0.0767769873771339, 0.09685539270731923, 0.09162066560337806, 0.5353326602618931]
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
probs:  [0.10779399184643064, 0.09162078834709968, 0.076776889244974, 0.09685559334446765, 0.09162078834709968, 0.5353319488699285]
printing an ep nov before normalisation:  58.46390279530848
printing an ep nov before normalisation:  23.721515142715354
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
probs:  [0.10779399184643064, 0.09162078834709968, 0.076776889244974, 0.09685559334446765, 0.09162078834709968, 0.5353319488699285]
Printing some Q and Qe and total Qs values:  [[0.079]
 [0.08 ]
 [0.084]
 [0.091]
 [0.079]
 [0.079]
 [0.079]] [[31.91 ]
 [29.919]
 [31.915]
 [32.675]
 [31.91 ]
 [31.91 ]
 [31.91 ]] [[1.989]
 [1.758]
 [1.995]
 [2.091]
 [1.989]
 [1.989]
 [1.989]]
printing an ep nov before normalisation:  69.23935284381288
maxi score, test score, baseline:  -0.9938393939393939 -1.0 -0.9938393939393939
probs:  [0.10779435082204798, 0.09162090959657797, 0.07677679230744805, 0.09685579153912138, 0.09162090959657797, 0.5353312461382268]
printing an ep nov before normalisation:  73.90221223970741
maxi score, test score, baseline:  -0.9938393939393939 -1.0 -0.9938393939393939
probs:  [0.10803963041803882, 0.09170379095048059, 0.07671062321998195, 0.09699123657524578, 0.09170379095048059, 0.5348509278857722]
maxi score, test score, baseline:  -0.9938393939393939 -1.0 -0.9938393939393939
probs:  [0.10803963041803882, 0.09170379095048059, 0.07671062321998195, 0.09699123657524578, 0.09170379095048059, 0.5348509278857722]
Printing some Q and Qe and total Qs values:  [[0.338]
 [0.349]
 [0.349]
 [0.349]
 [0.327]
 [0.327]
 [0.349]] [[20.974]
 [20.779]
 [20.779]
 [20.779]
 [23.847]
 [22.246]
 [20.779]] [[0.527]
 [0.535]
 [0.535]
 [0.535]
 [0.565]
 [0.538]
 [0.535]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  21.665735244750977
printing an ep nov before normalisation:  76.47443451801603
maxi score, test score, baseline:  -0.9938393939393939 -1.0 -0.9938393939393939
probs:  [0.10320923905271713, 0.0923095875135832, 0.07707993741780712, 0.0976804303009825, 0.0923095875135832, 0.5374112182013269]
from probs:  [0.10320923905271713, 0.0923095875135832, 0.07707993741780712, 0.0976804303009825, 0.0923095875135832, 0.5374112182013269]
Starting evaluation
printing an ep nov before normalisation:  45.04152226900077
Printing some Q and Qe and total Qs values:  [[0.115]
 [0.176]
 [0.115]
 [0.115]
 [0.115]
 [0.103]
 [0.103]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.115]
 [0.176]
 [0.115]
 [0.115]
 [0.115]
 [0.103]
 [0.103]]
maxi score, test score, baseline:  -0.9938393939393939 -1.0 -0.9938393939393939
probs:  [0.09822318477585718, 0.09282245851819275, 0.07750807036289752, 0.09822318477585718, 0.09282245851819275, 0.5404006430490027]
printing an ep nov before normalisation:  16.410267507266294
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
printing an ep nov before normalisation:  34.15227069773259
printing an ep nov before normalisation:  74.41002024933017
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
probs:  [0.09822383615469314, 0.09282287302069435, 0.07750781317497164, 0.09822383615469314, 0.09282287302069435, 0.5403987684742534]
printing an ep nov before normalisation:  26.364965484283402
from probs:  [0.09822383615469314, 0.09282287302069435, 0.07750781317497164, 0.09822383615469314, 0.09282287302069435, 0.5403987684742534]
printing an ep nov before normalisation:  41.26553285567041
line 256 mcts: sample exp_bonus 45.716010337400974
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.09822507125467962, 0.09282365897200055, 0.07750732551289709, 0.09822507125467962, 0.09282365897200055, 0.5403952140337427]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9941857142857143 -1.0 -0.9941857142857143
probs:  [0.09837546704682391, 0.09291938204838332, 0.07744801773773664, 0.09837546704682391, 0.09291938204838332, 0.5399622840718489]
printing an ep nov before normalisation:  53.61053211777376
printing an ep nov before normalisation:  59.892797590753304
printing an ep nov before normalisation:  48.28173793909055
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.502]
 [0.518]
 [0.52 ]
 [0.506]
 [0.487]
 [0.514]] [[35.924]
 [43.237]
 [31.453]
 [32.824]
 [33.456]
 [32.936]
 [37.233]] [[0.515]
 [0.502]
 [0.518]
 [0.52 ]
 [0.506]
 [0.487]
 [0.514]]
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]] [[70.558]
 [70.558]
 [70.558]
 [70.558]
 [70.558]
 [70.558]
 [70.558]] [[2.007]
 [2.007]
 [2.007]
 [2.007]
 [2.007]
 [2.007]
 [2.007]]
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.09837662258271998, 0.09292011736998551, 0.07744756149277951, 0.09837662258271998, 0.09292011736998551, 0.5399589586018094]
printing an ep nov before normalisation:  31.92083759478365
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]]
actions average: 
K:  2  action  0 :  tensor([0.2591, 0.0818, 0.1089, 0.1017, 0.1951, 0.1278, 0.1257],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0523, 0.7579, 0.0311, 0.0353, 0.0268, 0.0223, 0.0742],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1580, 0.0205, 0.1545, 0.1322, 0.1657, 0.2135, 0.1555],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1608, 0.0286, 0.1423, 0.1866, 0.1674, 0.1608, 0.1535],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1629, 0.0098, 0.1276, 0.1319, 0.2260, 0.1849, 0.1570],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1777, 0.0252, 0.1284, 0.1351, 0.1724, 0.1900, 0.1712],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1762, 0.0235, 0.1296, 0.1377, 0.1620, 0.1811, 0.1900],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  38.26577612807114
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.09890209158404877, 0.09341601285823889, 0.0778595978412163, 0.09890209158404877, 0.08808447156132504, 0.5428357345711223]
printing an ep nov before normalisation:  51.06787929336898
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.302]
 [0.286]
 [0.286]
 [0.288]
 [0.288]
 [0.289]] [[69.285]
 [70.12 ]
 [65.976]
 [68.048]
 [70.218]
 [71.419]
 [71.357]] [[0.282]
 [0.302]
 [0.286]
 [0.286]
 [0.288]
 [0.288]
 [0.289]]
printing an ep nov before normalisation:  55.525221824645996
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.09890209158404877, 0.09341601285823889, 0.0778595978412163, 0.09890209158404877, 0.08808447156132504, 0.5428357345711223]
printing an ep nov before normalisation:  72.8802490202235
printing an ep nov before normalisation:  23.639144139565435
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
maxi score, test score, baseline:  -0.9946089947089947 -1.0 -0.9946089947089947
siam score:  -0.85652786
maxi score, test score, baseline:  -0.9946089947089947 -1.0 -0.9946089947089947
probs:  [0.09921865863184333, 0.0936207895556055, 0.07774737998325991, 0.09921865863184333, 0.08818060693221942, 0.5420139062652284]
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.524]
 [0.523]
 [0.51 ]
 [0.513]
 [0.477]
 [0.444]] [[40.286]
 [39.991]
 [40.191]
 [39.984]
 [45.408]
 [39.942]
 [39.906]] [[2.499]
 [2.496]
 [2.514]
 [2.481]
 [3.005]
 [2.444]
 [2.407]]
printing an ep nov before normalisation:  48.85321294018227
maxi score, test score, baseline:  -0.9946089947089947 -1.0 -0.9946089947089947
probs:  [0.09937877601210207, 0.09372436417581965, 0.07769062102362138, 0.09937877601210207, 0.088229231546193, 0.5415982312301618]
siam score:  -0.8547781
maxi score, test score, baseline:  -0.9946089947089947 -1.0 -0.9946089947089947
probs:  [0.09991227101879449, 0.09422746385997181, 0.07810753123152957, 0.09991227101879449, 0.0833315834722284, 0.5445088793986811]
maxi score, test score, baseline:  -0.9946089947089947 -1.0 -0.9946089947089947
probs:  [0.09991227101879449, 0.09422746385997181, 0.07810753123152957, 0.09991227101879449, 0.0833315834722284, 0.5445088793986811]
maxi score, test score, baseline:  -0.9946089947089947 -1.0 -0.9946089947089947
probs:  [0.09991227101879449, 0.09422746385997181, 0.07810753123152957, 0.09991227101879449, 0.0833315834722284, 0.5445088793986811]
printing an ep nov before normalisation:  42.707347714319155
printing an ep nov before normalisation:  38.508597046930106
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
probs:  [0.10008016950287889, 0.09433767511354103, 0.07805416362596637, 0.10008016950287889, 0.08333122753397666, 0.5441165947207581]
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
probs:  [0.10024926523150186, 0.09444867227910055, 0.07800041555105834, 0.10024926523150186, 0.08333086912033129, 0.5437215125865061]
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
probs:  [0.10024926523150186, 0.09444867227910055, 0.07800041555105834, 0.10024926523150186, 0.08333086912033129, 0.5437215125865061]
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
probs:  [0.09511756744495664, 0.09511756744495664, 0.07840519658694821, 0.1010113020953654, 0.08382124269833982, 0.5465271237294334]
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
printing an ep nov before normalisation:  63.871335696010675
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
probs:  [0.08990444871692915, 0.09566518491877989, 0.07885646148050311, 0.10159289898155381, 0.08430373296512984, 0.5496772729371041]
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
probs:  [0.09039653079010226, 0.0961888445393135, 0.07928798387380673, 0.10214905144067576, 0.07928798387380673, 0.5526896054822951]
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
probs:  [0.09039653079010226, 0.0961888445393135, 0.07928798387380673, 0.10214905144067576, 0.07928798387380673, 0.5526896054822951]
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
probs:  [0.09039653079010226, 0.0961888445393135, 0.07928798387380673, 0.10214905144067576, 0.07928798387380673, 0.5526896054822951]
Printing some Q and Qe and total Qs values:  [[0.029]
 [0.765]
 [0.029]
 [0.029]
 [0.029]
 [0.029]
 [0.029]] [[35.289]
 [29.119]
 [35.289]
 [35.289]
 [35.289]
 [35.289]
 [35.289]] [[1.283]
 [1.637]
 [1.283]
 [1.283]
 [1.283]
 [1.283]
 [1.283]]
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
probs:  [0.09039653079010226, 0.0961888445393135, 0.07928798387380673, 0.10214905144067576, 0.07928798387380673, 0.5526896054822951]
printing an ep nov before normalisation:  42.663017760218054
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
Printing some Q and Qe and total Qs values:  [[-0.027]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]] [[62.138]
 [60.464]
 [60.464]
 [60.464]
 [60.464]
 [60.464]
 [60.464]] [[1.64 ]
 [1.575]
 [1.575]
 [1.575]
 [1.575]
 [1.575]
 [1.575]]
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
probs:  [0.09093818007260959, 0.09676525216154426, 0.07976297332670754, 0.09676525216154426, 0.07976297332670754, 0.556005368950887]
printing an ep nov before normalisation:  58.66587998316125
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
probs:  [0.09093818007260959, 0.09676525216154426, 0.07976297332670754, 0.09676525216154426, 0.07976297332670754, 0.556005368950887]
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
probs:  [0.09093818007260959, 0.09676525216154426, 0.07976297332670754, 0.09676525216154426, 0.07976297332670754, 0.556005368950887]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.86515203376742
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
probs:  [0.09481174646310667, 0.1005463973007848, 0.08381378595249098, 0.09481174646310667, 0.07853773732915516, 0.5474785864913558]
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.469]
 [0.486]
 [0.469]
 [0.487]
 [0.488]
 [0.486]] [[68.354]
 [75.573]
 [67.627]
 [68.325]
 [67.646]
 [75.338]
 [69.951]] [[1.582]
 [1.794]
 [1.561]
 [1.567]
 [1.563]
 [1.806]
 [1.634]]
printing an ep nov before normalisation:  61.69062923873966
printing an ep nov before normalisation:  54.40773779023723
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
probs:  [0.09863054517236264, 0.09863054517236264, 0.08769070885309008, 0.09863054517236264, 0.07733433047084551, 0.5390833251589764]
printing an ep nov before normalisation:  34.27483708778095
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
probs:  [0.09930534090134023, 0.09930534090134023, 0.08287234690602843, 0.09930534090134023, 0.07768682435639675, 0.5415248060335542]
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
probs:  [0.09961852393174715, 0.09961852393174715, 0.08286263416237154, 0.09961852393174715, 0.07757522005736868, 0.5407065739850182]
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.465]
 [0.465]
 [0.476]
 [0.465]
 [0.465]
 [0.465]] [[30.526]
 [30.526]
 [30.526]
 [62.266]
 [30.526]
 [30.526]
 [30.526]] [[1.065]
 [1.065]
 [1.065]
 [2.025]
 [1.065]
 [1.065]
 [1.065]]
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.09961871263338674, 0.09961871263338674, 0.08286262824241647, 0.09961871263338674, 0.07757515272348812, 0.540706081133935]
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.09961871263338674, 0.09961871263338674, 0.08286262824241647, 0.09961871263338674, 0.07757515272348812, 0.540706081133935]
using another actor
using another actor
from probs:  [0.09961871263338674, 0.09961871263338674, 0.08286262824241647, 0.09961871263338674, 0.07757515272348812, 0.540706081133935]
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.10264042297533964, 0.10264042297533964, 0.08623263851056753, 0.10264042297533964, 0.07601375520356038, 0.5298323373598532]
maxi score, test score, baseline:  -0.9946916666666666 -1.0 -0.9946916666666666
probs:  [0.10264063860560525, 0.10264063860560525, 0.08623267052031021, 0.10264063860560525, 0.07601367285315269, 0.5298317408097214]
printing an ep nov before normalisation:  93.39826812103348
printing an ep nov before normalisation:  74.95819188127376
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
probs:  [0.09821849357556602, 0.10395545463202535, 0.0819255241752215, 0.10395545463202535, 0.07678037594353375, 0.5351646970416281]
printing an ep nov before normalisation:  37.430346148617964
printing an ep nov before normalisation:  43.405985084715226
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
probs:  [0.09835998508549681, 0.10415160140819545, 0.08191179472903251, 0.10415160140819545, 0.07671762935330688, 0.534707388015773]
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
probs:  [0.09864646574936531, 0.10454874369792533, 0.08188399637545499, 0.10454874369792533, 0.07659058499422011, 0.533781465485109]
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
probs:  [0.09879131149550792, 0.10474954038315154, 0.08186994145460007, 0.10474954038315154, 0.07652635091536603, 0.5333133153682229]
actions average: 
K:  1  action  0 :  tensor([0.2960, 0.0120, 0.1180, 0.1225, 0.1712, 0.1467, 0.1337],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0193, 0.8294, 0.0205, 0.0348, 0.0152, 0.0255, 0.0552],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1540, 0.0388, 0.1387, 0.1367, 0.1767, 0.1995, 0.1557],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1472, 0.0884, 0.1060, 0.1811, 0.1785, 0.1504, 0.1485],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1635, 0.0178, 0.1130, 0.1495, 0.2490, 0.1748, 0.1323],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1798, 0.0108, 0.1254, 0.1404, 0.1856, 0.2108, 0.1473],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1712, 0.0263, 0.1213, 0.1383, 0.1873, 0.1597, 0.1959],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.09879148389444338, 0.104749779401524, 0.08186992465433458, 0.104749779401524, 0.07652627436798452, 0.5333127582801894]
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.09879148389444338, 0.104749779401524, 0.08186992465433458, 0.104749779401524, 0.07652627436798452, 0.5333127582801894]
printing an ep nov before normalisation:  29.946477963402067
line 256 mcts: sample exp_bonus 31.397919952869415
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.09908480044568246, 0.10515639814276995, 0.08184146298595397, 0.10515639814276995, 0.07639619852498715, 0.5323647417578364]
printing an ep nov before normalisation:  37.74962483592731
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.192]
 [0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]] [[23.501]
 [19.52 ]
 [19.52 ]
 [19.52 ]
 [19.52 ]
 [19.52 ]
 [19.52 ]] [[1.461]
 [0.93 ]
 [0.93 ]
 [0.93 ]
 [0.93 ]
 [0.93 ]
 [0.93 ]]
Printing some Q and Qe and total Qs values:  [[ 0.144]
 [-0.006]
 [-0.008]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]] [[23.766]
 [16.739]
 [19.067]
 [16.739]
 [16.739]
 [16.739]
 [16.739]] [[0.718]
 [0.31 ]
 [0.393]
 [0.31 ]
 [0.31 ]
 [0.31 ]
 [0.31 ]]
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.1021961636998557, 0.10813432773750427, 0.08533177783293373, 0.10813432773750427, 0.07481891391589156, 0.5213844890763103]
printing an ep nov before normalisation:  68.831791628063
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.10236802070710307, 0.10836038238879439, 0.08534971353109984, 0.10836038238879439, 0.07474089866813684, 0.5208206023160713]
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.10298483781940651, 0.10298483781940651, 0.08586385343652984, 0.10901335344718002, 0.07519103200304836, 0.5239620854744288]
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.399]
 [0.448]
 [0.437]
 [0.403]
 [0.389]
 [0.45 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.341]
 [0.399]
 [0.448]
 [0.437]
 [0.403]
 [0.389]
 [0.45 ]]
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.10441718094873918, 0.09842814882140351, 0.08692920713691907, 0.10441718094873918, 0.07602761307240793, 0.5297806690717911]
printing an ep nov before normalisation:  82.52921983016567
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.45781774597435
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.10441718094873918, 0.09842814882140351, 0.08692920713691907, 0.10441718094873918, 0.07602761307240793, 0.5297806690717911]
actions average: 
K:  0  action  0 :  tensor([0.2961, 0.0067, 0.1218, 0.1066, 0.1538, 0.1596, 0.1554],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0228, 0.8590, 0.0194, 0.0275, 0.0219, 0.0265, 0.0229],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1887, 0.0057, 0.2568, 0.1099, 0.1423, 0.1586, 0.1381],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1802, 0.0464, 0.1205, 0.1795, 0.1500, 0.1646, 0.1588],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2216, 0.0027, 0.1171, 0.1200, 0.2233, 0.1636, 0.1516],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1939, 0.0061, 0.1331, 0.1235, 0.1609, 0.2155, 0.1670],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2137, 0.0025, 0.1274, 0.1239, 0.1699, 0.1968, 0.1657],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.09962077034566034, 0.09962077034566034, 0.08798231092139489, 0.09962077034566034, 0.07694844679189645, 0.5362069312497275]
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.09977320776201704, 0.09977320776201704, 0.08802559834921371, 0.09977320776201704, 0.07688825436045206, 0.5357665240042832]
printing an ep nov before normalisation:  24.11463336819205
siam score:  -0.8621156
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.09977320776201704, 0.09977320776201704, 0.08802559834921371, 0.09977320776201704, 0.07688825436045206, 0.5357665240042832]
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.38 ]
 [0.472]
 [0.462]
 [0.476]
 [0.478]
 [0.475]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.462]
 [0.38 ]
 [0.472]
 [0.462]
 [0.476]
 [0.478]
 [0.475]]
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.10037041521636947, 0.09438155347385499, 0.08855239471114087, 0.10037041521636947, 0.07734829734904107, 0.5389769240332241]
printing an ep nov before normalisation:  18.73022517184357
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.1021566178306174, 0.09012800303952494, 0.0843511025148555, 0.09606103601080704, 0.07872425135446326, 0.5485789892497318]
printing an ep nov before normalisation:  77.07585157729393
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.10215683316331027, 0.09012808048014025, 0.08435111373098621, 0.09606118146575794, 0.07872419806622591, 0.5485785930935795]
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.10215683316331027, 0.09012808048014025, 0.08435111373098621, 0.09606118146575794, 0.07872419806622591, 0.5485785930935795]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  80 total reward:  0.12666666666666604  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  60.12459501003846
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.0965826264206301, 0.08812226717159917, 0.08405906832173558, 0.09229528220659418, 0.08010140710433612, 0.5588393487751049]
printing an ep nov before normalisation:  56.33215494501188
printing an ep nov before normalisation:  51.779308349627705
printing an ep nov before normalisation:  49.0955575045319
printing an ep nov before normalisation:  36.010637283325195
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.09669143364688745, 0.08816142018342082, 0.08406476898057165, 0.09236879168904963, 0.08007452430247197, 0.5586390611975984]
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.09669143364688745, 0.08816142018342082, 0.08406476898057165, 0.09236879168904963, 0.08007452430247197, 0.5586390611975984]
printing an ep nov before normalisation:  69.26013911209888
printing an ep nov before normalisation:  16.637858152389526
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
line 256 mcts: sample exp_bonus 23.44382568240633
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.09669143364688745, 0.08816142018342082, 0.08406476898057165, 0.09236879168904963, 0.08007452430247197, 0.5586390611975984]
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.09891958709012702, 0.09058399688311336, 0.08658072000737646, 0.09469547043116737, 0.0788821106309595, 0.5503381149572564]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.09891958709012702, 0.09058399688311336, 0.08658072000737646, 0.09469547043116737, 0.0788821106309595, 0.5503381149572564]
printing an ep nov before normalisation:  85.73327009190895
printing an ep nov before normalisation:  32.98122234320998
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.09509692909845144, 0.0909680005215513, 0.08694772795983267, 0.09509692909845144, 0.07921643457191233, 0.5526739787498008]
from probs:  [0.09519147266663014, 0.09102926578933794, 0.0869765906719745, 0.09519147266663014, 0.07918298467704488, 0.5524282135283823]
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.09153973223001824, 0.09153973223001824, 0.08740419818829571, 0.09578703746205748, 0.07945124810806027, 0.5542780517815502]
maxi score, test score, baseline:  -0.9948494949494949 -1.0 -0.9948494949494949
actor:  1 policy actor:  1  step number:  72 total reward:  0.0066666666666663765  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  51.90659123825559
printing an ep nov before normalisation:  30.742818110938526
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9948494949494949 -1.0 -0.9948494949494949
probs:  [0.08997310262081284, 0.08997310262081284, 0.0866302747307972, 0.09340627721055857, 0.08020175955769025, 0.5598154832593282]
maxi score, test score, baseline:  -0.9948494949494949 -1.0 -0.9948494949494949
probs:  [0.08997310262081284, 0.08997310262081284, 0.0866302747307972, 0.09340627721055857, 0.08020175955769025, 0.5598154832593282]
printing an ep nov before normalisation:  59.10905527143966
printing an ep nov before normalisation:  29.71850213971057
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.09002233725513749, 0.09002233725513749, 0.08665459688430069, 0.09348109763599696, 0.0801781730942299, 0.5596414578751975]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.275]
 [0.405]
 [0.424]
 [0.405]
 [0.427]
 [0.405]] [[27.88 ]
 [39.844]
 [27.126]
 [30.748]
 [27.126]
 [31.932]
 [27.126]] [[1.26 ]
 [1.785]
 [1.222]
 [1.438]
 [1.222]
 [1.506]
 [1.222]]
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
probs:  [0.09012143950370244, 0.09012143950370244, 0.0867035539064109, 0.09363170038740724, 0.08013069698854257, 0.5592911697102344]
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
probs:  [0.09017130981250383, 0.09017130981250383, 0.08672819010623825, 0.09370748680812795, 0.08010680605572758, 0.5591148974048985]
printing an ep nov before normalisation:  25.74377015162696
line 256 mcts: sample exp_bonus 23.39329242706299
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
probs:  [0.09079646702637034, 0.09079646702637034, 0.08395246733706968, 0.09079646702637034, 0.0806620828710598, 0.5629960487127594]
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
probs:  [0.09155643045240178, 0.09490773903022078, 0.08829216885063006, 0.09490773903022078, 0.07898902328558063, 0.5513468993509459]
printing an ep nov before normalisation:  44.21066170920324
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
printing an ep nov before normalisation:  10.980542886368028
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
printing an ep nov before normalisation:  46.50559387510668
actor:  1 policy actor:  1  step number:  58 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.365]
 [0.444]
 [0.285]
 [0.286]
 [0.287]
 [0.383]
 [0.372]] [[22.492]
 [23.504]
 [22.347]
 [22.568]
 [22.869]
 [25.887]
 [25.689]] [[0.996]
 [1.105]
 [0.911]
 [0.92 ]
 [0.929]
 [1.114]
 [1.097]]
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
probs:  [0.0875914240460375, 0.09270811696641948, 0.08513147552662308, 0.09270811696641948, 0.08039607462675029, 0.5614647918677502]
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
probs:  [0.0875914240460375, 0.09270811696641948, 0.08513147552662308, 0.09270811696641948, 0.08039607462675029, 0.5614647918677502]
printing an ep nov before normalisation:  55.71427782530594
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
printing an ep nov before normalisation:  67.09006928644546
printing an ep nov before normalisation:  63.45625235958252
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
printing an ep nov before normalisation:  97.74746825157594
printing an ep nov before normalisation:  44.759185476851016
printing an ep nov before normalisation:  42.06317663192749
printing an ep nov before normalisation:  42.43969917297363
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
Printing some Q and Qe and total Qs values:  [[0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]] [[106.418]
 [106.418]
 [106.418]
 [106.418]
 [106.418]
 [106.418]
 [106.418]] [[2.099]
 [2.099]
 [2.099]
 [2.099]
 [2.099]
 [2.099]
 [2.099]]
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.567]
 [0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]] [[63.563]
 [83.86 ]
 [63.563]
 [63.563]
 [63.563]
 [63.563]
 [63.563]] [[1.629]
 [2.394]
 [1.629]
 [1.629]
 [1.629]
 [1.629]
 [1.629]]
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
probs:  [0.08980585874373334, 0.09499072324637188, 0.08251464303689789, 0.09236418004437737, 0.08020425781292125, 0.5601203371156982]
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
probs:  [0.08980585874373334, 0.09499072324637188, 0.08251464303689789, 0.09236418004437737, 0.08020425781292125, 0.5601203371156982]
printing an ep nov before normalisation:  30.88203195524084
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
probs:  [0.08980585874373334, 0.09499072324637188, 0.08251464303689789, 0.09236418004437737, 0.08020425781292125, 0.5601203371156982]
Printing some Q and Qe and total Qs values:  [[0.072]
 [0.209]
 [0.311]
 [0.385]
 [0.344]
 [0.287]
 [0.162]] [[18.85 ]
 [19.768]
 [18.967]
 [17.675]
 [18.881]
 [18.239]
 [20.141]] [[1.159]
 [1.44 ]
 [1.416]
 [1.286]
 [1.435]
 [1.277]
 [1.452]]
using explorer policy with actor:  1
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
siam score:  -0.85220903
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
actor:  1 policy actor:  1  step number:  58 total reward:  0.21999999999999964  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
probs:  [0.0900432839826013, 0.094288527687797, 0.08407341002216982, 0.0900432839826013, 0.08033617185181842, 0.5612153224730122]
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.09004334123530586, 0.0942886213595896, 0.08407341606053184, 0.09004334123530586, 0.08033614582916926, 0.5612151342800974]
printing an ep nov before normalisation:  28.53884220123291
actor:  1 policy actor:  1  step number:  41 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.08717540411874536, 0.09215758335712494, 0.08406154209475813, 0.08879299478055692, 0.0810995757792581, 0.5667128998695565]
printing an ep nov before normalisation:  24.94554780615914
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
printing an ep nov before normalisation:  52.81507805274231
printing an ep nov before normalisation:  41.15013598028803
from probs:  [0.08717540411874536, 0.09215758335712494, 0.08406154209475813, 0.08879299478055692, 0.0810995757792581, 0.5667128998695565]
printing an ep nov before normalisation:  67.1059033550027
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.08722001199483108, 0.09226054226347337, 0.08406968057692966, 0.08885654779633832, 0.08107302386234048, 0.5665201935060871]
printing an ep nov before normalisation:  22.27871279740532
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.08724240349532833, 0.09231222383135085, 0.08407376578531424, 0.088888449058972, 0.08105969576847158, 0.5664234620605632]
Printing some Q and Qe and total Qs values:  [[0.152]
 [0.172]
 [0.172]
 [0.172]
 [0.172]
 [0.172]
 [0.172]] [[60.924]
 [48.956]
 [48.956]
 [48.956]
 [48.956]
 [48.956]
 [48.956]] [[1.965]
 [1.47 ]
 [1.47 ]
 [1.47 ]
 [1.47 ]
 [1.47 ]
 [1.47 ]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.13425540924072
printing an ep nov before normalisation:  63.060841246908836
printing an ep nov before normalisation:  44.50425906552418
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.08816854744181031, 0.093194569906669, 0.08502728340127363, 0.08980037291741379, 0.08059923650075806, 0.5632099898320752]
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.48 ]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]] [[52.529]
 [58.995]
 [51.209]
 [51.209]
 [51.209]
 [51.209]
 [51.209]] [[1.265]
 [1.459]
 [1.226]
 [1.226]
 [1.226]
 [1.226]
 [1.226]]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.08819605591484417, 0.09325087081045458, 0.08503679660508763, 0.08983722958225014, 0.08058338287928629, 0.5630956642080773]
Printing some Q and Qe and total Qs values:  [[0.209]
 [0.124]
 [0.209]
 [0.209]
 [0.209]
 [0.209]
 [0.209]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.209]
 [0.124]
 [0.209]
 [0.209]
 [0.209]
 [0.209]
 [0.209]]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.08819605591484417, 0.09325087081045458, 0.08503679660508763, 0.08983722958225014, 0.08058338287928629, 0.5630956642080773]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.08819605591484417, 0.09325087081045458, 0.08503679660508763, 0.08983722958225014, 0.08058338287928629, 0.5630956642080773]
Printing some Q and Qe and total Qs values:  [[0.064]
 [0.066]
 [0.072]
 [0.066]
 [0.067]
 [0.065]
 [0.066]] [[54.012]
 [52.875]
 [54.371]
 [52.875]
 [53.622]
 [53.564]
 [52.875]] [[1.019]
 [0.982]
 [1.039]
 [0.982]
 [1.008]
 [1.004]
 [0.982]]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.08822361737532693, 0.09330728016232669, 0.08504632813345206, 0.08987415724123593, 0.08056749872020677, 0.5629811183674516]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.0883586931914489, 0.0934501511521899, 0.08364438026483684, 0.09001176395792325, 0.08069083481683893, 0.5638441766167621]
Printing some Q and Qe and total Qs values:  [[2.992]
 [2.992]
 [2.992]
 [2.992]
 [2.992]
 [2.992]
 [2.992]] [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]] [[2.992]
 [2.992]
 [2.992]
 [2.992]
 [2.992]
 [2.992]
 [2.992]]
printing an ep nov before normalisation:  60.912625596282496
Printing some Q and Qe and total Qs values:  [[0.128]
 [0.381]
 [0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]] [[48.258]
 [47.39 ]
 [48.258]
 [48.258]
 [48.258]
 [48.258]
 [48.258]] [[1.401]
 [1.623]
 [1.401]
 [1.401]
 [1.401]
 [1.401]
 [1.401]]
printing an ep nov before normalisation:  29.75612056363005
using explorer policy with actor:  1
from probs:  [0.08838717002149094, 0.09350767195763113, 0.08364596452506486, 0.09004967065010787, 0.08067557072007502, 0.5637339521256303]
printing an ep nov before normalisation:  24.02010725065182
maxi score, test score, baseline:  -0.9949980392156863 -1.0 -0.9949980392156863
probs:  [0.08705233413037661, 0.09381546861348633, 0.08392125335115917, 0.08867808761189336, 0.0809410680311811, 0.5655917882619035]
printing an ep nov before normalisation:  63.627315779083794
maxi score, test score, baseline:  -0.9949980392156863 -1.0 -0.9949980392156863
probs:  [0.08707331279630004, 0.0938749426204158, 0.08392441009995015, 0.08870831996555864, 0.08092726175041232, 0.5654917527673631]
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.531]
 [0.507]
 [0.507]
 [0.495]
 [0.488]
 [0.49 ]] [[60.582]
 [61.952]
 [60.582]
 [60.582]
 [56.193]
 [57.827]
 [61.908]] [[0.507]
 [0.531]
 [0.507]
 [0.507]
 [0.495]
 [0.488]
 [0.49 ]]
maxi score, test score, baseline:  -0.9949980392156863 -1.0 -0.9949980392156863
probs:  [0.08707331279630004, 0.0938749426204158, 0.08392441009995015, 0.08870831996555864, 0.08092726175041232, 0.5654917527673631]
maxi score, test score, baseline:  -0.9949980392156863 -1.0 -0.9949980392156863
probs:  [0.08707331279630004, 0.0938749426204158, 0.08392441009995015, 0.08870831996555864, 0.08092726175041232, 0.5654917527673631]
maxi score, test score, baseline:  -0.9949980392156863 -1.0 -0.9949980392156863
probs:  [0.08707331279630004, 0.0938749426204158, 0.08392441009995015, 0.08870831996555864, 0.08092726175041232, 0.5654917527673631]
maxi score, test score, baseline:  -0.9949980392156863 -1.0 -0.9949980392156863
probs:  [0.08707331279630004, 0.0938749426204158, 0.08392441009995015, 0.08870831996555864, 0.08092726175041232, 0.5654917527673631]
from probs:  [0.08707331279630004, 0.0938749426204158, 0.08392441009995015, 0.08870831996555864, 0.08092726175041232, 0.5654917527673631]
printing an ep nov before normalisation:  24.62813876281409
siam score:  -0.85525674
maxi score, test score, baseline:  -0.9950219512195122 -1.0 -0.9950219512195122
probs:  [0.08709437044670558, 0.09393464058251783, 0.08392757871716287, 0.08873866615242969, 0.08091340345651378, 0.5653913406446703]
using explorer policy with actor:  1
from probs:  [0.08713653718163415, 0.09405418224942139, 0.08393392372432526, 0.08879943263062147, 0.08088565308423605, 0.5651902711297618]
line 256 mcts: sample exp_bonus 29.498948185158095
printing an ep nov before normalisation:  27.23739240181823
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.08566685597774287, 0.09426784164520201, 0.08407408085413934, 0.08897492738830409, 0.08100367097731324, 0.5660126231572984]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.08581047785423856, 0.09442590567401198, 0.08421502825798421, 0.08744631857951199, 0.08113946277122888, 0.5669628068630244]
printing an ep nov before normalisation:  53.85075539548147
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.08595862925255499, 0.0946359637094392, 0.08278399469515833, 0.08760622440259631, 0.08125405033014789, 0.5677611376101033]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.08597313568479374, 0.09469903431375884, 0.08278073374736748, 0.08762995188016685, 0.08124222678957169, 0.5676749175843413]
printing an ep nov before normalisation:  77.3115984583477
actions average: 
K:  3  action  0 :  tensor([0.2158, 0.1353, 0.1094, 0.1237, 0.1317, 0.1213, 0.1628],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0350, 0.6649, 0.0465, 0.0484, 0.0318, 0.0477, 0.1258],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1657, 0.0973, 0.1341, 0.1266, 0.1612, 0.1682, 0.1469],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1759, 0.0152, 0.1289, 0.1746, 0.1731, 0.1680, 0.1642],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1750, 0.0591, 0.1287, 0.1148, 0.1975, 0.1835, 0.1415],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1549, 0.0343, 0.1273, 0.1462, 0.1770, 0.2235, 0.1367],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1472, 0.0692, 0.1720, 0.1116, 0.1453, 0.1670, 0.1877],
       grad_fn=<DivBackward0>)
from probs:  [0.08598769790530818, 0.09476234753966431, 0.08277746023420227, 0.08765377062069225, 0.08123035774210303, 0.56758836595803]
actions average: 
K:  3  action  0 :  tensor([0.2042, 0.0751, 0.1278, 0.1182, 0.1675, 0.1607, 0.1464],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0437, 0.6624, 0.0414, 0.0467, 0.0336, 0.0282, 0.1439],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1497, 0.0165, 0.2103, 0.1280, 0.1581, 0.1700, 0.1673],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1430, 0.0983, 0.1163, 0.1674, 0.1635, 0.1528, 0.1587],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1636, 0.0517, 0.0960, 0.1644, 0.2430, 0.1519, 0.1295],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1447, 0.0618, 0.1211, 0.1358, 0.1351, 0.2719, 0.1297],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2066, 0.0083, 0.1371, 0.1230, 0.1739, 0.1843, 0.1667],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
printing an ep nov before normalisation:  38.66724014327513
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
probs:  [0.08696156846691336, 0.09565743018448276, 0.08378015564341236, 0.08861268145126198, 0.08075023866864951, 0.5642379255852801]
printing an ep nov before normalisation:  50.76676525813779
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
probs:  [0.08696156846691336, 0.09565743018448276, 0.08378015564341236, 0.08861268145126198, 0.08075023866864951, 0.5642379255852801]
printing an ep nov before normalisation:  58.25376754599198
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
probs:  [0.08696156846691336, 0.09565743018448276, 0.08378015564341236, 0.08861268145126198, 0.08075023866864951, 0.5642379255852801]
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
probs:  [0.08700120347202737, 0.09579293964319324, 0.08378471462891791, 0.08867052046655255, 0.0807213919211946, 0.5640292298681143]
from probs:  [0.08700120347202737, 0.09579293964319324, 0.08378471462891791, 0.08867052046655255, 0.0807213919211946, 0.5640292298681143]
printing an ep nov before normalisation:  37.870455680042205
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
Printing some Q and Qe and total Qs values:  [[0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]] [[20.996]
 [20.996]
 [20.996]
 [20.996]
 [20.996]
 [20.996]
 [20.996]] [[1.024]
 [1.024]
 [1.024]
 [1.024]
 [1.024]
 [1.024]
 [1.024]]
from probs:  [0.08730347836449354, 0.09617202345961123, 0.08249523102376707, 0.08730347836449354, 0.08096880329655232, 0.5657569854910822]
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
probs:  [0.08748916444751273, 0.0945422469337128, 0.08264548129433918, 0.08748916444751273, 0.0811078041028555, 0.5667261387740671]
maxi score, test score, baseline:  -0.9951153110047847 -1.0 -0.9951153110047847
probs:  [0.08748920456573435, 0.09454235572325574, 0.08264547425273773, 0.08748920456573435, 0.08110778208988169, 0.5667259788026562]
printing an ep nov before normalisation:  39.62190947284313
printing an ep nov before normalisation:  52.61542475660484
maxi score, test score, baseline:  -0.9951153110047847 -1.0 -0.9951153110047847
probs:  [0.08465210627578389, 0.09318536245256948, 0.08306882741888633, 0.08793740990384634, 0.0815232456776292, 0.5696330482712848]
printing an ep nov before normalisation:  49.163360706327374
printing an ep nov before normalisation:  62.933975236220235
maxi score, test score, baseline:  -0.9951380952380953 -1.0 -0.9951380952380953
probs:  [0.08465211871537101, 0.0931854576024559, 0.08306882451222514, 0.08793745418689869, 0.08152322779010654, 0.5696329171929427]
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  55.08702144649629
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.431]
 [0.429]
 [0.429]] [[33.305]
 [33.305]
 [33.305]
 [33.305]
 [34.914]
 [33.305]
 [33.305]] [[0.845]
 [0.845]
 [0.845]
 [0.845]
 [0.885]
 [0.845]
 [0.845]]
maxi score, test score, baseline:  -0.9951380952380953 -1.0 -0.9951380952380953
maxi score, test score, baseline:  -0.9951380952380953 -1.0 -0.9951380952380953
probs:  [0.08465917198706202, 0.09323933279459115, 0.0830671903432554, 0.08796253389796072, 0.08151311302430132, 0.5695586579528295]
actions average: 
K:  1  action  0 :  tensor([0.2693, 0.0320, 0.1190, 0.1218, 0.1507, 0.1500, 0.1572],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0285, 0.8518, 0.0181, 0.0304, 0.0230, 0.0210, 0.0270],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1995, 0.0295, 0.1292, 0.1414, 0.1722, 0.1662, 0.1620],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1460, 0.0518, 0.1098, 0.2291, 0.1586, 0.1615, 0.1432],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1815, 0.0166, 0.1019, 0.1196, 0.2991, 0.1434, 0.1378],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1621, 0.0523, 0.1346, 0.1298, 0.1537, 0.2063, 0.1612],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1800, 0.0140, 0.1278, 0.1319, 0.1578, 0.1506, 0.2378],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.98218038295162
printing an ep nov before normalisation:  43.6192439087354
printing an ep nov before normalisation:  43.48752352882492
Printing some Q and Qe and total Qs values:  [[-0.013]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.013]] [[21.017]
 [21.017]
 [21.017]
 [21.017]
 [21.017]
 [21.017]
 [21.017]] [[0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]]
maxi score, test score, baseline:  -0.9951380952380953 -1.0 -0.9951380952380953
maxi score, test score, baseline:  -0.995160663507109 -1.0 -0.995160663507109
probs:  [0.08480872541787876, 0.09345042876257771, 0.08320532503825992, 0.08645171593032772, 0.08164010085815582, 0.5704437039928001]
Printing some Q and Qe and total Qs values:  [[0.182]
 [0.176]
 [0.155]
 [0.155]
 [0.165]
 [0.155]
 [0.155]] [[43.191]
 [40.368]
 [42.331]
 [42.331]
 [42.666]
 [42.331]
 [42.331]] [[0.515]
 [0.468]
 [0.475]
 [0.475]
 [0.49 ]
 [0.475]
 [0.475]]
printing an ep nov before normalisation:  16.844921587503308
printing an ep nov before normalisation:  41.349908511751195
printing an ep nov before normalisation:  83.18905564901699
maxi score, test score, baseline:  -0.995160663507109 -1.0 -0.995160663507109
using another actor
from probs:  [0.08349134608816042, 0.0919947537688904, 0.08349134608816042, 0.08676673275036753, 0.08191214180459624, 0.572343679499825]
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.08204156996086669, 0.09214033544151867, 0.08362330431325798, 0.08690393852562504, 0.08204156996086669, 0.5732492817978648]
printing an ep nov before normalisation:  38.48029089537838
printing an ep nov before normalisation:  38.720679818413934
printing an ep nov before normalisation:  48.84735298955604
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.634]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.562]
 [0.634]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]]
printing an ep nov before normalisation:  62.053088453541264
Printing some Q and Qe and total Qs values:  [[0.36 ]
 [0.352]
 [0.36 ]
 [0.407]
 [0.416]
 [0.417]
 [0.36 ]] [[54.118]
 [62.595]
 [54.118]
 [61.29 ]
 [61.372]
 [60.404]
 [54.118]] [[1.288]
 [1.506]
 [1.288]
 [1.526]
 [1.538]
 [1.513]
 [1.288]]
from probs:  [0.08218853811028659, 0.09051255798730858, 0.08377311056880404, 0.08705963122350688, 0.08218853811028659, 0.5742776239998072]
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
printing an ep nov before normalisation:  38.25197901044573
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.08349079717754561, 0.09185511368312362, 0.08349079717754561, 0.08671411914798788, 0.08193601834474405, 0.5725131544690532]
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.08349079717754561, 0.09185511368312362, 0.08349079717754561, 0.08671411914798788, 0.08193601834474405, 0.5725131544690532]
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]] [[50.111]
 [50.111]
 [50.111]
 [50.111]
 [50.111]
 [50.111]
 [50.111]] [[2.166]
 [2.166]
 [2.166]
 [2.166]
 [2.166]
 [2.166]
 [2.166]]
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.362]
 [0.354]
 [0.354]] [[58.324]
 [58.324]
 [58.324]
 [58.324]
 [61.535]
 [58.324]
 [58.324]] [[0.615]
 [0.615]
 [0.615]
 [0.615]
 [0.638]
 [0.615]
 [0.615]]
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.08206350371189529, 0.0919980574409532, 0.08362070558609286, 0.08684905093503906, 0.08206350371189529, 0.5734051786141243]
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.08206350371189529, 0.0919980574409532, 0.08362070558609286, 0.08684905093503906, 0.08206350371189529, 0.5734051786141243]
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.08206350371189529, 0.0919980574409532, 0.08362070558609286, 0.08684905093503906, 0.08206350371189529, 0.5734051786141243]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.08205644337619931, 0.09204482153406882, 0.0836220820160241, 0.0868679182205389, 0.08205644337619931, 0.5733522914769696]
printing an ep nov before normalisation:  85.00736113904294
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
using explorer policy with actor:  1
printing an ep nov before normalisation:  57.943046718114275
printing an ep nov before normalisation:  37.700893283780246
from probs:  [0.08204935552647927, 0.09209176771366964, 0.08362346378597936, 0.08688685895811366, 0.08204935552647927, 0.5732991984892787]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.0821721909269507, 0.09228476959055679, 0.0821721909269507, 0.08704349406368778, 0.0821721909269507, 0.5741551635649034]
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.0821721909269507, 0.09228476959055679, 0.0821721909269507, 0.08704349406368778, 0.0821721909269507, 0.5741551635649034]
printing an ep nov before normalisation:  49.651316325738435
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.08320543805236205, 0.0932249532227029, 0.08320543805236205, 0.08803191182344086, 0.08167144251271688, 0.5706608163364151]
printing an ep nov before normalisation:  36.66703925156105
printing an ep nov before normalisation:  48.54857520816176
printing an ep nov before normalisation:  88.57911757894671
printing an ep nov before normalisation:  33.41432691106628
from probs:  [0.08434494120920097, 0.09253117496145559, 0.08434494120920097, 0.0891368829178378, 0.08133391270262455, 0.5683081469996801]
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
printing an ep nov before normalisation:  5.677809674767786
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.77 ]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.576]
 [0.77 ]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]]
printing an ep nov before normalisation:  69.04369428063944
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
probs:  [0.0843709083481537, 0.09277443318378759, 0.0843709083481537, 0.08929004483730524, 0.08127995668447227, 0.5679137485981275]
printing an ep nov before normalisation:  42.163634300231934
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  65.0861406326294
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
probs:  [0.08437613099219422, 0.09282335849148185, 0.08437613099219422, 0.0893208495283626, 0.08126910478555968, 0.5678344252102073]
printing an ep nov before normalisation:  44.62319510481983
siam score:  -0.8516176
printing an ep nov before normalisation:  37.97220820400034
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
probs:  [0.08437613099219422, 0.09282335849148185, 0.08437613099219422, 0.0893208495283626, 0.08126910478555968, 0.5678344252102073]
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
probs:  [0.08437613099219422, 0.09282335849148185, 0.08437613099219422, 0.0893208495283626, 0.08126910478555968, 0.5678344252102073]
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
printing an ep nov before normalisation:  19.981956740019875
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.519]
 [0.54 ]
 [0.541]
 [0.541]
 [0.541]
 [0.54 ]] [[45.254]
 [50.085]
 [45.051]
 [45.036]
 [44.88 ]
 [44.284]
 [44.049]] [[1.644]
 [1.845]
 [1.632]
 [1.633]
 [1.626]
 [1.598]
 [1.586]]
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.356]
 [0.668]
 [0.378]
 [0.485]
 [0.474]
 [0.371]
 [0.438]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.356]
 [0.668]
 [0.378]
 [0.485]
 [0.474]
 [0.371]
 [0.438]]
printing an ep nov before normalisation:  87.30449290141917
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
probs:  [0.08452666676768722, 0.09311925075737747, 0.08292804649053553, 0.08955647202994492, 0.08136617610481264, 0.5685033878496422]
printing an ep nov before normalisation:  58.14808305680245
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
printing an ep nov before normalisation:  32.13033279826254
line 256 mcts: sample exp_bonus 30.00875122320792
printing an ep nov before normalisation:  30.5686480038996
Printing some Q and Qe and total Qs values:  [[0.321]
 [0.317]
 [0.292]
 [0.299]
 [0.324]
 [0.333]
 [0.31 ]] [[28.414]
 [26.37 ]
 [25.901]
 [26.433]
 [27.717]
 [27.985]
 [26.667]] [[1.545]
 [1.451]
 [1.405]
 [1.436]
 [1.517]
 [1.538]
 [1.457]]
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
probs:  [0.08566824496479722, 0.09423959795430371, 0.0825155634054385, 0.09068562232450833, 0.08099296151597547, 0.5658980098349767]
printing an ep nov before normalisation:  62.031805780464914
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
probs:  [0.08566824496479722, 0.09423959795430371, 0.0825155634054385, 0.09068562232450833, 0.08099296151597547, 0.5658980098349767]
Printing some Q and Qe and total Qs values:  [[0.055]
 [0.055]
 [0.055]
 [0.055]
 [1.371]
 [0.055]
 [0.055]] [[77.598]
 [77.598]
 [77.598]
 [77.598]
 [ 0.038]
 [77.598]
 [77.598]] [[0.722]
 [0.722]
 [0.722]
 [0.722]
 [1.372]
 [0.722]
 [0.722]]
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
probs:  [0.0858225169703638, 0.09260715885611083, 0.08266414919596432, 0.09084894373328008, 0.08113880112310093, 0.56691843012118]
printing an ep nov before normalisation:  44.25516916737171
printing an ep nov before normalisation:  72.35309912457159
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
probs:  [0.08598321044895531, 0.09281422672051431, 0.08280325459840197, 0.08931641718387268, 0.08126748046603247, 0.5678154105822232]
deleting a thread, now have 3 threads
Frames:  26141 train batches done:  3057 episodes:  819
printing an ep nov before normalisation:  55.998113857707494
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
probs:  [0.08599652960962074, 0.09286231806439381, 0.08280038670826086, 0.0893467034941787, 0.08125679496612682, 0.567737267157419]
printing an ep nov before normalisation:  58.235927604979565
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
probs:  [0.08600987511307254, 0.09291050452387624, 0.08279751314597426, 0.0893770497050912, 0.08124608833231883, 0.567658969179667]
siam score:  -0.8596797
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
probs:  [0.0861638444092156, 0.09128536696116879, 0.08294572295894617, 0.08953705604985944, 0.08139151657671376, 0.5686764930440964]
actions average: 
K:  1  action  0 :  tensor([0.2004, 0.0096, 0.1330, 0.1271, 0.1819, 0.1858, 0.1621],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0330, 0.8557, 0.0102, 0.0231, 0.0249, 0.0147, 0.0384],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1753, 0.0022, 0.2479, 0.0951, 0.1726, 0.1725, 0.1343],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1783, 0.0609, 0.1244, 0.1381, 0.1851, 0.1551, 0.1581],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1880, 0.0131, 0.1210, 0.1303, 0.2259, 0.1700, 0.1516],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1962, 0.0248, 0.1559, 0.1045, 0.1496, 0.2378, 0.1312],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1786, 0.0500, 0.1235, 0.1396, 0.1621, 0.1525, 0.1937],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9953128440366973 -1.0 -0.9953128440366973
maxi score, test score, baseline:  -0.9953128440366973 -1.0 -0.9953128440366973
maxi score, test score, baseline:  -0.9953128440366973 -1.0 -0.9953128440366973
probs:  [0.08616386978412205, 0.09128543883366379, 0.08294571911697707, 0.08953711204968366, 0.0813914986243218, 0.5686763615912316]
actions average: 
K:  1  action  0 :  tensor([0.3470, 0.0470, 0.0833, 0.1002, 0.1874, 0.1190, 0.1160],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0201, 0.8920, 0.0082, 0.0208, 0.0103, 0.0113, 0.0374],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1475, 0.0179, 0.2421, 0.1204, 0.1771, 0.1723, 0.1228],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2015, 0.0142, 0.1147, 0.1554, 0.2047, 0.1629, 0.1466],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2035, 0.0039, 0.1048, 0.1356, 0.2340, 0.1800, 0.1381],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1550, 0.0555, 0.1088, 0.1228, 0.1907, 0.2482, 0.1189],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1996, 0.0250, 0.1119, 0.1437, 0.2144, 0.1611, 0.1444],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  81.18302672420037
maxi score, test score, baseline:  -0.9953128440366973 -1.0 -0.9953128440366973
probs:  [0.08616386978412205, 0.09128543883366379, 0.08294571911697707, 0.08953711204968366, 0.0813914986243218, 0.5686763615912316]
printing an ep nov before normalisation:  50.334744453430176
printing an ep nov before normalisation:  36.911682736372825
maxi score, test score, baseline:  -0.9953128440366973 -1.0 -0.9953128440366973
probs:  [0.08616386978412205, 0.09128543883366379, 0.08294571911697707, 0.08953711204968366, 0.0813914986243218, 0.5686763615912316]
actions average: 
K:  3  action  0 :  tensor([0.2612, 0.0364, 0.1114, 0.1238, 0.1788, 0.1456, 0.1428],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0370, 0.7788, 0.0287, 0.0357, 0.0327, 0.0439, 0.0431],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1654, 0.0075, 0.2769, 0.1039, 0.1448, 0.1550, 0.1465],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1628, 0.0537, 0.0994, 0.2128, 0.1787, 0.1535, 0.1390],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1658, 0.0211, 0.0980, 0.1205, 0.3105, 0.1568, 0.1273],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1587, 0.0385, 0.1152, 0.1171, 0.1716, 0.2623, 0.1366],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1878, 0.0156, 0.1119, 0.1691, 0.1862, 0.1432, 0.1862],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9953128440366973 -1.0 -0.9953128440366973
probs:  [0.08616386978412205, 0.09128543883366379, 0.08294571911697707, 0.08953711204968366, 0.0813914986243218, 0.5686763615912316]
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.579]
 [0.469]
 [0.464]
 [0.47 ]
 [0.481]
 [0.455]] [[48.93 ]
 [51.745]
 [48.296]
 [48.862]
 [47.856]
 [47.932]
 [50.098]] [[1.09 ]
 [1.283]
 [1.088]
 [1.097]
 [1.078]
 [1.092]
 [1.118]]
using another actor
printing an ep nov before normalisation:  39.41343228022258
using explorer policy with actor:  0
printing an ep nov before normalisation:  20.768766933017307
printing an ep nov before normalisation:  30.472702980041504
maxi score, test score, baseline:  -0.9953545454545455 -1.0 -0.9953545454545455
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.575]
 [0.565]
 [0.555]
 [0.539]
 [0.528]
 [0.517]] [[45.36 ]
 [45.145]
 [49.297]
 [45.849]
 [43.567]
 [45.615]
 [44.269]] [[1.228]
 [1.28 ]
 [1.394]
 [1.28 ]
 [1.197]
 [1.247]
 [1.196]]
maxi score, test score, baseline:  -0.9953545454545455 -1.0 -0.9953545454545455
probs:  [0.08646126363823516, 0.09162582187205873, 0.08164883437489955, 0.08814179449209837, 0.08164883437489955, 0.5704734512478086]
printing an ep nov before normalisation:  41.680521972063694
maxi score, test score, baseline:  -0.9953545454545455 -1.0 -0.9953545454545455
probs:  [0.08646126363823516, 0.09162582187205873, 0.08164883437489955, 0.08814179449209837, 0.08164883437489955, 0.5704734512478086]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9953545454545455 -1.0 -0.9953545454545455
maxi score, test score, baseline:  -0.995375113122172 -1.0 -0.995375113122172
probs:  [0.08662318287672156, 0.09182275751469873, 0.08177812469133372, 0.08662318287672156, 0.08177812469133372, 0.5713746273491908]
maxi score, test score, baseline:  -0.995375113122172 -1.0 -0.995375113122172
probs:  [0.08662318287672156, 0.09182275751469873, 0.08177812469133372, 0.08662318287672156, 0.08177812469133372, 0.5713746273491908]
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.08662321155980561, 0.09182283202683085, 0.08177811067007755, 0.08662321155980561, 0.08177811067007755, 0.5713745235134028]
printing an ep nov before normalisation:  25.85337392788847
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.08662321155980561, 0.09182283202683085, 0.08177811067007755, 0.08662321155980561, 0.08177811067007755, 0.5713745235134028]
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]] [[82.213]
 [82.213]
 [82.213]
 [82.213]
 [82.213]
 [82.213]
 [82.213]] [[2.625]
 [2.625]
 [2.625]
 [2.625]
 [2.625]
 [2.625]
 [2.625]]
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.08662321155980561, 0.09182283202683085, 0.08177811067007755, 0.08662321155980561, 0.08177811067007755, 0.5713745235134028]
actor:  1 policy actor:  1  step number:  72 total reward:  0.16666666666666674  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  53.854308128356934
actions average: 
K:  2  action  0 :  tensor([0.2266, 0.0387, 0.1132, 0.1398, 0.1657, 0.1688, 0.1472],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0309, 0.7807, 0.0291, 0.0293, 0.0198, 0.0284, 0.0817],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1699, 0.0667, 0.1390, 0.1242, 0.1395, 0.1812, 0.1795],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1473, 0.0638, 0.1010, 0.2212, 0.1660, 0.1657, 0.1351],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2177, 0.0220, 0.1033, 0.1094, 0.2717, 0.1434, 0.1325],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1861, 0.0226, 0.1190, 0.1377, 0.1684, 0.2107, 0.1554],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1776, 0.0292, 0.1219, 0.1433, 0.1755, 0.1609, 0.1917],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.07240390578543195, 0.24102134232562353, 0.06835535921597313, 0.07240390578543195, 0.06835535921597313, 0.47746012767156626]
printing an ep nov before normalisation:  33.15680089957102
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.07240390578543195, 0.24102134232562353, 0.06835535921597313, 0.07240390578543195, 0.06835535921597313, 0.47746012767156626]
printing an ep nov before normalisation:  68.4911777667624
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.07240390578543195, 0.24102134232562353, 0.06835535921597313, 0.07240390578543195, 0.06835535921597313, 0.47746012767156626]
printing an ep nov before normalisation:  22.154459953308105
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.07240390578543195, 0.24102134232562353, 0.06835535921597313, 0.07240390578543195, 0.06835535921597313, 0.47746012767156626]
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.0723575683154795, 0.24168771901434366, 0.06829190930189091, 0.0723575683154795, 0.06829190930189091, 0.47701332575091554]
printing an ep nov before normalisation:  39.49480858636766
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.07329827648731822, 0.241323738027731, 0.06926394338476849, 0.07329827648731822, 0.06797960513114783, 0.4748361604817163]
printing an ep nov before normalisation:  40.627676522609235
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.07329827648731822, 0.241323738027731, 0.06926394338476849, 0.07329827648731822, 0.06797960513114783, 0.4748361604817163]
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.563]
 [0.592]
 [0.592]
 [0.555]
 [0.592]
 [0.548]] [[36.836]
 [42.034]
 [39.097]
 [36.836]
 [32.868]
 [36.836]
 [30.947]] [[1.094]
 [1.193]
 [1.15 ]
 [1.094]
 [0.96 ]
 [1.094]
 [0.905]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9954156950672646 -1.0 -0.9954156950672646
probs:  [0.07325620745407978, 0.2419841117603613, 0.06920498534474244, 0.07325620745407978, 0.06791527044102083, 0.47438321754571583]
from probs:  [0.07325620745407978, 0.2419841117603613, 0.06920498534474244, 0.07325620745407978, 0.06791527044102083, 0.47438321754571583]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9954156950672646 -1.0 -0.9954156950672646
probs:  [0.0733574705526602, 0.24231903478695585, 0.06930063817826132, 0.07197374478154743, 0.06800913723510064, 0.47503997446547447]
maxi score, test score, baseline:  -0.9954156950672646 -1.0 -0.9954156950672646
probs:  [0.0733574705526602, 0.24231903478695585, 0.06930063817826132, 0.07197374478154743, 0.06800913723510064, 0.47503997446547447]
maxi score, test score, baseline:  -0.9954156950672646 -1.0 -0.9954156950672646
probs:  [0.0733574705526602, 0.24231903478695585, 0.06930063817826132, 0.07197374478154743, 0.06800913723510064, 0.47503997446547447]
siam score:  -0.84729874
printing an ep nov before normalisation:  53.181785409726814
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.0736095571011567, 0.23971328629138708, 0.06953872514100048, 0.07222105627753751, 0.06824276740087584, 0.4766746077880424]
line 256 mcts: sample exp_bonus 45.18084275554202
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.07356875058403844, 0.2403671654167606, 0.0694808934280633, 0.07217444271688411, 0.06817951568177909, 0.47622923217247437]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.07448452070685532, 0.24001928757895702, 0.07042763274422435, 0.0731007759754153, 0.0678732958788641, 0.47409448711568386]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.07448452070685532, 0.24001928757895702, 0.07042763274422435, 0.0731007759754153, 0.0678732958788641, 0.47409448711568386]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.07444779628954377, 0.24066698976392475, 0.0703741345572899, 0.0730583302723409, 0.06780923642957452, 0.47364351268732624]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.07444779628954377, 0.24066698976392475, 0.0703741345572899, 0.0730583302723409, 0.06780923642957452, 0.47364351268732624]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.07444779628954377, 0.24066698976392475, 0.0703741345572899, 0.0730583302723409, 0.06780923642957452, 0.47364351268732624]
printing an ep nov before normalisation:  26.461167481230135
printing an ep nov before normalisation:  0.8432183333462717
printing an ep nov before normalisation:  26.610992272494265
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.07555405743340816, 0.23841651600567323, 0.0714802574055635, 0.07416454424561617, 0.06767505957735694, 0.47270956533238206]
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.07555405743340816, 0.23841651600567323, 0.0714802574055635, 0.07416454424561617, 0.06767505957735694, 0.47270956533238206]
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.07555405743340816, 0.23841651600567323, 0.0714802574055635, 0.07416454424561617, 0.06767505957735694, 0.47270956533238206]
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.07555405743340816, 0.23841651600567323, 0.0714802574055635, 0.07416454424561617, 0.06767505957735694, 0.47270956533238206]
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.07426763739546756, 0.23874834009955365, 0.0715796125213664, 0.07426763739546756, 0.06776911572181639, 0.47336765686632837]
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.07426763739546756, 0.23874834009955365, 0.0715796125213664, 0.07426763739546756, 0.06776911572181639, 0.47336765686632837]
printing an ep nov before normalisation:  17.322930862275463
using explorer policy with actor:  1
printing an ep nov before normalisation:  67.37212879114611
printing an ep nov before normalisation:  83.860586895898
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.4245],
        [-0.0000],
        [-0.6196],
        [-0.3381],
        [-0.0000],
        [-0.5994],
        [-0.6105],
        [-0.4124],
        [-0.6585]], dtype=torch.float64)
-0.9409620000000001 -0.9409620000000001
-0.09703970119800001 -0.5215089820602925
-0.97049568 -0.97049568
-0.032346567066 -0.651945761339653
-0.083839701198 -0.4219191669480942
-0.4012536 -0.4012536
-0.032346567066 -0.6317688713140042
-0.032346567066 -0.6428866471235503
-0.083839701198 -0.4962712141842455
-0.032346567066 -0.6908513061396571
printing an ep nov before normalisation:  52.98088550567627
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.07300731995820996, 0.23907381986673376, 0.07167693532087721, 0.07436864377315511, 0.06786121674567008, 0.4740120643353538]
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.07300731995820996, 0.23907381986673376, 0.07167693532087721, 0.07436864377315511, 0.06786121674567008, 0.4740120643353538]
printing an ep nov before normalisation:  28.39209960028908
printing an ep nov before normalisation:  24.14592981338501
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.07300731995820996, 0.23907381986673376, 0.07167693532087721, 0.07436864377315511, 0.06786121674567008, 0.4740120643353538]
printing an ep nov before normalisation:  65.79560287107935
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.07300731995820996, 0.23907381986673376, 0.07167693532087721, 0.07436864377315511, 0.06786121674567008, 0.4740120643353538]
Printing some Q and Qe and total Qs values:  [[-0.003]
 [ 0.052]
 [ 0.024]
 [ 0.028]
 [-0.003]
 [-0.003]
 [-0.003]] [[30.838]
 [26.889]
 [22.461]
 [18.025]
 [30.838]
 [30.838]
 [30.838]] [[1.997]
 [1.733]
 [1.347]
 [0.993]
 [1.997]
 [1.997]
 [1.997]]
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.07296487037827575, 0.23971186322040539, 0.07162903420589418, 0.07433177250815454, 0.06779767990928336, 0.4735647797779869]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 81.40014264248623
Printing some Q and Qe and total Qs values:  [[-0.037]
 [-0.005]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]] [[30.471]
 [27.683]
 [30.471]
 [30.471]
 [30.471]
 [30.471]
 [30.471]] [[1.775]
 [1.557]
 [1.775]
 [1.775]
 [1.775]
 [1.775]
 [1.775]]
actions average: 
K:  2  action  0 :  tensor([0.2408, 0.0158, 0.1086, 0.1357, 0.1838, 0.1621, 0.1533],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0442, 0.7157, 0.0311, 0.0499, 0.0467, 0.0293, 0.0831],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2114, 0.0175, 0.1270, 0.1332, 0.1927, 0.1666, 0.1514],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1339, 0.0450, 0.0870, 0.2797, 0.1810, 0.1266, 0.1468],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1598, 0.0658, 0.1118, 0.1318, 0.2220, 0.1656, 0.1433],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2022, 0.0014, 0.1202, 0.1428, 0.2065, 0.1753, 0.1517],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2060, 0.0595, 0.1176, 0.1317, 0.1683, 0.1532, 0.1637],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.07279494924636905, 0.2422658825777082, 0.07143729116179058, 0.07418418077477493, 0.06754334874338419, 0.47177434749597313]
siam score:  -0.85224116
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.07279494924636905, 0.2422658825777082, 0.07143729116179058, 0.07418418077477493, 0.06754334874338419, 0.47177434749597313]
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.47 ]
 [0.481]
 [0.467]
 [0.458]
 [0.451]
 [0.444]] [[45.428]
 [33.017]
 [43.91 ]
 [42.303]
 [42.581]
 [42.234]
 [42.355]] [[2.181]
 [1.687]
 [2.105]
 [2.03 ]
 [2.032]
 [2.012]
 [2.01 ]]
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.07270991491847681, 0.2435440009437703, 0.07134133640501368, 0.07411032083922978, 0.06741607275650952, 0.4708783541369999]
Printing some Q and Qe and total Qs values:  [[0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]]
UNIT TEST: sample policy line 217 mcts : [0.082 0.184 0.143 0.082 0.347 0.102 0.061]
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.639]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]] [[60.468]
 [70.466]
 [60.468]
 [60.468]
 [60.468]
 [60.468]
 [60.468]] [[1.177]
 [1.492]
 [1.177]
 [1.177]
 [1.177]
 [1.177]
 [1.177]]
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.07266737929536508, 0.24418333757943883, 0.07129333819686401, 0.07407337483801736, 0.06735240713413014, 0.4704301629561845]
printing an ep nov before normalisation:  73.06510351468998
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.164]
 [0.164]
 [0.164]
 [0.165]
 [0.21 ]
 [0.164]
 [0.164]] [[38.63 ]
 [38.63 ]
 [38.63 ]
 [39.429]
 [45.13 ]
 [38.63 ]
 [38.63 ]] [[0.581]
 [0.581]
 [0.581]
 [0.596]
 [0.75 ]
 [0.581]
 [0.581]]
printing an ep nov before normalisation:  37.02538188521142
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.07291506224590974, 0.24160471903237474, 0.0715363291982852, 0.07432585885278135, 0.06758194100674672, 0.47203608966390226]
printing an ep nov before normalisation:  85.41444314053642
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.07301336505227493, 0.24193086257062507, 0.0702831991966724, 0.07442606715197624, 0.06767304063142601, 0.4726734653970253]
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.073257652920479, 0.2393921137502627, 0.07051833578700759, 0.07467509027442641, 0.0678994281978646, 0.47425737906995963]
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.073257652920479, 0.2393921137502627, 0.07051833578700759, 0.07467509027442641, 0.0678994281978646, 0.47425737906995963]
printing an ep nov before normalisation:  17.772286511487202
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.594]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]] [[47.62 ]
 [51.599]
 [47.62 ]
 [47.62 ]
 [47.62 ]
 [47.62 ]
 [47.62 ]] [[1.391]
 [1.548]
 [1.391]
 [1.391]
 [1.391]
 [1.391]
 [1.391]]
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.07349797624854715, 0.2368945662047219, 0.0707496563530078, 0.07492007200844832, 0.06812214172760205, 0.47581558745767294]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.07360254023204943, 0.2372320131252441, 0.07085030326082743, 0.07360254023204943, 0.06821904373888991, 0.47649355941093974]
line 256 mcts: sample exp_bonus 68.82450774329384
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.53 ]
 [0.515]
 [0.504]
 [0.488]
 [0.486]
 [0.485]] [[43.104]
 [41.094]
 [40.063]
 [40.35 ]
 [40.522]
 [40.542]
 [40.636]] [[2.233]
 [2.103]
 [2.013]
 [2.023]
 [2.02 ]
 [2.019]
 [2.025]]
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.07366149191625211, 0.23816998971071915, 0.06955707578309178, 0.07366149191625211, 0.06824907503735937, 0.4767008756363255]
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.07362248691939545, 0.2387921435715139, 0.06950157515763047, 0.07362248691939545, 0.06818831756322186, 0.4762729898688428]
printing an ep nov before normalisation:  34.7037148475647
siam score:  -0.8585817
maxi score, test score, baseline:  -0.9954947136563876 -1.0 -0.9954947136563876
probs:  [0.07362248676716023, 0.23879253113775653, 0.06950154199091384, 0.07362248676716023, 0.06818827387540675, 0.4762726794616025]
using explorer policy with actor:  1
printing an ep nov before normalisation:  23.84737897156284
maxi score, test score, baseline:  -0.9954947136563876 -1.0 -0.9954947136563876
probs:  [0.07374731378670946, 0.23815886357767768, 0.06956309009834649, 0.07374731378670946, 0.06822965617568137, 0.4765537625748757]
maxi score, test score, baseline:  -0.9954947136563876 -1.0 -0.9954947136563876
probs:  [0.07374731378670946, 0.23815886357767768, 0.06956309009834649, 0.07374731378670946, 0.06822965617568137, 0.4765537625748757]
actions average: 
K:  3  action  0 :  tensor([0.3163, 0.0086, 0.1084, 0.1083, 0.1698, 0.1413, 0.1473],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0379, 0.5549, 0.0524, 0.1007, 0.0369, 0.0615, 0.1557],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1919, 0.0252, 0.1349, 0.1371, 0.1415, 0.2017, 0.1678],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1596, 0.0377, 0.1097, 0.2115, 0.1499, 0.1365, 0.1950],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1972, 0.0414, 0.1338, 0.1158, 0.1749, 0.2035, 0.1334],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1608, 0.0306, 0.1136, 0.1263, 0.1510, 0.2631, 0.1545],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1674, 0.0166, 0.1174, 0.1298, 0.1622, 0.1841, 0.2225],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
probs:  [0.07374731463318686, 0.23815924415018702, 0.06956305758846099, 0.07374731463318686, 0.06822961303574615, 0.4765534559592321]
printing an ep nov before normalisation:  55.36623886758389
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
probs:  [0.07374731463318686, 0.23815924415018702, 0.06956305758846099, 0.07374731463318686, 0.06822961303574615, 0.4765534559592321]
printing an ep nov before normalisation:  32.52020294270199
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
printing an ep nov before normalisation:  64.81861430003069
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
probs:  [0.07102928795476397, 0.24007283201824625, 0.06965111559654384, 0.07388067904073663, 0.06830323274070219, 0.47706285264900733]
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
probs:  [0.07102928795476397, 0.24007283201824625, 0.06965111559654384, 0.07388067904073663, 0.06830323274070219, 0.47706285264900733]
Printing some Q and Qe and total Qs values:  [[-0.031]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.033]] [[26.76 ]
 [21.456]
 [21.456]
 [21.456]
 [21.456]
 [21.456]
 [21.456]] [[1.035]
 [0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.822]]
printing an ep nov before normalisation:  18.98354172706604
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
probs:  [0.07102928795476397, 0.24007283201824625, 0.06965111559654384, 0.07388067904073663, 0.06830323274070219, 0.47706285264900733]
from probs:  [0.07102928795476397, 0.24007283201824625, 0.06965111559654384, 0.07388067904073663, 0.06830323274070219, 0.47706285264900733]
Printing some Q and Qe and total Qs values:  [[0.111]
 [0.423]
 [0.473]
 [0.398]
 [0.177]
 [0.195]
 [0.215]] [[27.022]
 [27.301]
 [31.304]
 [25.994]
 [27.7  ]
 [27.208]
 [26.23 ]] [[1.227]
 [1.551]
 [1.767]
 [1.471]
 [1.321]
 [1.319]
 [1.298]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.36  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  27.580623626708984
maxi score, test score, baseline:  -0.9955331877729258 -1.0 -0.9955331877729258
probs:  [0.07426061600483937, 0.21949510655467117, 0.0730765467408202, 0.07671041448212039, 0.0696771220796039, 0.486780194137945]
maxi score, test score, baseline:  -0.9955331877729258 -1.0 -0.9955331877729258
probs:  [0.07426061600483937, 0.21949510655467117, 0.0730765467408202, 0.07671041448212039, 0.0696771220796039, 0.486780194137945]
printing an ep nov before normalisation:  27.29231610061845
maxi score, test score, baseline:  -0.9955331877729258 -1.0 -0.9955331877729258
probs:  [0.07426061600483937, 0.21949510655467117, 0.0730765467408202, 0.07671041448212039, 0.0696771220796039, 0.486780194137945]
maxi score, test score, baseline:  -0.9955331877729258 -1.0 -0.9955331877729258
maxi score, test score, baseline:  -0.9955331877729258 -1.0 -0.9955331877729258
probs:  [0.07426061600483937, 0.21949510655467117, 0.0730765467408202, 0.07671041448212039, 0.0696771220796039, 0.486780194137945]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.07446875104230449, 0.2173057673408812, 0.0732813474591859, 0.07692544811082576, 0.06987235007539379, 0.48814633597140883]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.07524677802456449, 0.21715754319195493, 0.07406707436364071, 0.07768754421957925, 0.06959926049886553, 0.4862417997013951]
printing an ep nov before normalisation:  38.29279787353567
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.525]
 [0.454]
 [0.493]
 [0.471]
 [0.437]
 [0.426]] [[74.703]
 [82.205]
 [72.333]
 [69.779]
 [67.666]
 [68.142]
 [70.674]] [[1.7  ]
 [1.89 ]
 [1.601]
 [1.584]
 [1.515]
 [1.492]
 [1.536]]
printing an ep nov before normalisation:  63.79345349606581
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.07408349237442939, 0.21844683910707374, 0.07408349237442939, 0.07773616854096747, 0.06957593455189306, 0.486074073051207]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
printing an ep nov before normalisation:  3.455081578258614
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.07408349237442939, 0.21844683910707374, 0.07408349237442939, 0.07773616854096747, 0.06957593455189306, 0.486074073051207]
printing an ep nov before normalisation:  56.182082982126005
printing an ep nov before normalisation:  53.84191753807856
printing an ep nov before normalisation:  46.73317123845095
printing an ep nov before normalisation:  20.15631382309084
printing an ep nov before normalisation:  25.810995999922145
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]] [[32.904]
 [19.159]
 [19.159]
 [19.159]
 [19.159]
 [19.159]
 [19.159]] [[0.527]
 [0.302]
 [0.302]
 [0.302]
 [0.302]
 [0.302]
 [0.302]]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.07404799064329873, 0.21896342827909226, 0.07404799064329873, 0.07771463579385758, 0.06952319450005591, 0.4857027601403968]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.07502272353775528, 0.216657966405134, 0.07502272353775528, 0.07867597608835823, 0.06944670648683503, 0.48517390394416227]
Printing some Q and Qe and total Qs values:  [[0.042]
 [0.023]
 [0.038]
 [0.02 ]
 [0.046]
 [0.035]
 [0.04 ]] [[36.425]
 [40.496]
 [33.057]
 [33.473]
 [32.688]
 [32.734]
 [32.865]] [[0.523]
 [0.601]
 [0.44 ]
 [0.432]
 [0.439]
 [0.429]
 [0.438]]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.07502272353775528, 0.216657966405134, 0.07502272353775528, 0.07867597608835823, 0.06944670648683503, 0.48517390394416227]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.07502272353775528, 0.216657966405134, 0.07502272353775528, 0.07867597608835823, 0.06944670648683503, 0.48517390394416227]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.07502272353775528, 0.216657966405134, 0.07502272353775528, 0.07867597608835823, 0.06944670648683503, 0.48517390394416227]
printing an ep nov before normalisation:  23.29734184286849
printing an ep nov before normalisation:  36.68284823580138
using another actor
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.07531279562193095, 0.21479513347535817, 0.07414406285500352, 0.07898019844228941, 0.06971518079085755, 0.48705262881456035]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.07531279562193095, 0.21479513347535817, 0.07414406285500352, 0.07898019844228941, 0.06971518079085755, 0.48705262881456035]
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.679]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]] [[31.695]
 [42.998]
 [31.695]
 [31.695]
 [31.695]
 [31.695]
 [31.695]] [[1.238]
 [1.928]
 [1.238]
 [1.238]
 [1.238]
 [1.238]
 [1.238]]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.07531279562193095, 0.21479513347535817, 0.07414406285500352, 0.07898019844228941, 0.06971518079085755, 0.48705262881456035]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.07531279562193095, 0.21479513347535817, 0.07414406285500352, 0.07898019844228941, 0.06971518079085755, 0.48705262881456035]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.07531279562193095, 0.21479513347535817, 0.07414406285500352, 0.07898019844228941, 0.06971518079085755, 0.48705262881456035]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.07531279562193095, 0.21479513347535817, 0.07414406285500352, 0.07898019844228941, 0.06971518079085755, 0.48705262881456035]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.07531279562193095, 0.21479513347535817, 0.07414406285500352, 0.07898019844228941, 0.06971518079085755, 0.48705262881456035]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
Printing some Q and Qe and total Qs values:  [[0.243]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]] [[43.386]
 [29.389]
 [29.389]
 [29.389]
 [29.389]
 [29.389]
 [29.389]] [[1.68 ]
 [1.181]
 [1.181]
 [1.181]
 [1.181]
 [1.181]
 [1.181]]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.07468179956211633, 0.21167153781007836, 0.07468179956211633, 0.07830925730561172, 0.07020157105085185, 0.49045403470922533]
Printing some Q and Qe and total Qs values:  [[0.575]
 [0.564]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]] [[42.47 ]
 [41.894]
 [42.47 ]
 [42.47 ]
 [42.47 ]
 [42.47 ]
 [42.47 ]] [[2.575]
 [2.521]
 [2.575]
 [2.575]
 [2.575]
 [2.575]
 [2.575]]
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
printing an ep nov before normalisation:  59.61920731658545
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.07498934784462528, 0.21112687356925164, 0.07381804957559719, 0.07741090831093052, 0.07045210823407441, 0.4922027124655209]
printing an ep nov before normalisation:  58.30800381358206
printing an ep nov before normalisation:  53.535733177079685
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.07498934784462528, 0.21112687356925164, 0.07381804957559719, 0.07741090831093052, 0.07045210823407441, 0.4922027124655209]
printing an ep nov before normalisation:  0.004174761495505663
printing an ep nov before normalisation:  0.010428031761193779
printing an ep nov before normalisation:  41.40497513657135
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.07518016412947875, 0.2091176493689916, 0.07400588009548063, 0.07760789741325014, 0.07063135860830713, 0.49345705038449167]
printing an ep nov before normalisation:  19.970988349059724
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.0760112321727567, 0.20975836033431283, 0.0748386170761513, 0.07720990538261999, 0.07039245150152254, 0.49178943353263654]
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.0760112321727567, 0.20975836033431283, 0.0748386170761513, 0.07720990538261999, 0.07039245150152254, 0.49178943353263654]
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
printing an ep nov before normalisation:  60.52283791858087
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.07598330508425877, 0.2102382448041558, 0.07480623779860283, 0.07718652942070704, 0.07034319100715747, 0.491442491885118]
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]] [[59.538]
 [59.538]
 [59.538]
 [59.538]
 [59.538]
 [59.538]
 [59.538]] [[2.5]
 [2.5]
 [2.5]
 [2.5]
 [2.5]
 [2.5]
 [2.5]]
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.0759833089292093, 0.21023856216452316, 0.07480623235870683, 0.07718654275683409, 0.0703431503622182, 0.4914422034285083]
printing an ep nov before normalisation:  64.81589794158936
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.07617359826153401, 0.2082589147568621, 0.07499356869337791, 0.07737985070898247, 0.07051928991411932, 0.49267477766512424]
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.542]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]] [[36.696]
 [41.818]
 [36.696]
 [36.696]
 [36.696]
 [36.696]
 [36.696]] [[1.238]
 [1.702]
 [1.238]
 [1.238]
 [1.238]
 [1.238]
 [1.238]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.06169003399591213, 0.17700344151119238, 0.06065984204056576, 0.06274311910582173, 0.21424766615949556, 0.42365589718701235]
printing an ep nov before normalisation:  38.54474473101903
printing an ep nov before normalisation:  40.868112632574984
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.061822388180047864, 0.17523533693709323, 0.0607899811940354, 0.06287773754352727, 0.21470803672139213, 0.4245665194239041]
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.061822388180047864, 0.17523533693709323, 0.0607899811940354, 0.06287773754352727, 0.21470803672139213, 0.4245665194239041]
printing an ep nov before normalisation:  43.865139980133584
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
printing an ep nov before normalisation:  52.10504865868838
printing an ep nov before normalisation:  31.311206817626953
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.061751436243228694, 0.1755378889613255, 0.060715629221303934, 0.06281026119897401, 0.2151405845695969, 0.42404419980557084]
printing an ep nov before normalisation:  29.703139507376655
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.061751436243228694, 0.1755378889613255, 0.060715629221303934, 0.06281026119897401, 0.2151405845695969, 0.42404419980557084]
printing an ep nov before normalisation:  13.258399963378906
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.061751436243228694, 0.1755378889613255, 0.060715629221303934, 0.06281026119897401, 0.2151405845695969, 0.42404419980557084]
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.061751436243228694, 0.1755378889613255, 0.060715629221303934, 0.06281026119897401, 0.2151405845695969, 0.42404419980557084]
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.06188194931830983, 0.17379301277745446, 0.06084394836117113, 0.06294301696338493, 0.215595990068483, 0.42494208251119664]
printing an ep nov before normalisation:  56.903187895188346
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.06188194931830983, 0.17379301277745446, 0.06084394836117113, 0.06294301696338493, 0.215595990068483, 0.42494208251119664]
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.558]
 [0.567]
 [0.583]
 [0.58 ]
 [0.575]
 [0.569]] [[52.295]
 [56.902]
 [52.611]
 [50.678]
 [51.221]
 [51.963]
 [53.716]] [[1.65 ]
 [1.798]
 [1.636]
 [1.576]
 [1.595]
 [1.619]
 [1.683]]
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.464]
 [0.473]
 [0.507]
 [0.503]
 [0.498]
 [0.504]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.498]
 [0.464]
 [0.473]
 [0.507]
 [0.503]
 [0.498]
 [0.504]]
maxi score, test score, baseline:  -0.9956264957264958 -1.0 -0.9956264957264958
probs:  [0.06359297745442727, 0.17440111003975076, 0.060575249761324404, 0.06256520063141398, 0.21579150028503893, 0.4230739618280447]
maxi score, test score, baseline:  -0.9956264957264958 -1.0 -0.9956264957264958
probs:  [0.06359297745442727, 0.17440111003975076, 0.060575249761324404, 0.06256520063141398, 0.21579150028503893, 0.4230739618280447]
maxi score, test score, baseline:  -0.9956264957264958 -1.0 -0.9956264957264958
probs:  [0.06359297745442727, 0.17440111003975076, 0.060575249761324404, 0.06256520063141398, 0.21579150028503893, 0.4230739618280447]
maxi score, test score, baseline:  -0.9956264957264958 -1.0 -0.9956264957264958
probs:  [0.06372367355874302, 0.1727021513082026, 0.06069973081989842, 0.06269378001725245, 0.21623565104047404, 0.4239450132554293]
maxi score, test score, baseline:  -0.9956264957264958 -1.0 -0.9956264957264958
probs:  [0.06372367355874302, 0.1727021513082026, 0.06069973081989842, 0.06269378001725245, 0.21623565104047404, 0.4239450132554293]
maxi score, test score, baseline:  -0.9956264957264958 -1.0 -0.9956264957264958
maxi score, test score, baseline:  -0.9956264957264958 -1.0 -0.9956264957264958
probs:  [0.06372367355874302, 0.1727021513082026, 0.06069973081989842, 0.06269378001725245, 0.21623565104047404, 0.4239450132554293]
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
probs:  [0.06372368527728008, 0.17270243749568534, 0.060699717843122454, 0.06269378332506698, 0.2162354571803556, 0.42394491887848945]
printing an ep nov before normalisation:  41.80214417679148
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.66064391404924
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
probs:  [0.06407873679127322, 0.17163757341706726, 0.061037885311290785, 0.06202970425279042, 0.2149048674959255, 0.4263112327316528]
from probs:  [0.06407873679127322, 0.17163757341706726, 0.061037885311290785, 0.06202970425279042, 0.2149048674959255, 0.4263112327316528]
siam score:  -0.8459151
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
probs:  [0.06423822421579377, 0.1720653248968329, 0.06118978850400844, 0.06218408115552264, 0.21294841174708562, 0.42737416948075657]
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
probs:  [0.0643663803327354, 0.17041148310413448, 0.06131185031162905, 0.06230813071277841, 0.21337386325342994, 0.4282282922852926]
printing an ep nov before normalisation:  43.90739824347812
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
probs:  [0.0643663803327354, 0.17041148310413448, 0.06131185031162905, 0.06230813071277841, 0.21337386325342994, 0.4282282922852926]
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
probs:  [0.0643663803327354, 0.17041148310413448, 0.06131185031162905, 0.06230813071277841, 0.21337386325342994, 0.4282282922852926]
using explorer policy with actor:  1
siam score:  -0.839231
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
probs:  [0.0643663803327354, 0.17041148310413448, 0.06131185031162905, 0.06230813071277841, 0.21337386325342994, 0.4282282922852926]
printing an ep nov before normalisation:  48.37813134059422
printing an ep nov before normalisation:  47.03587532043457
Starting evaluation
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 0.0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.392]
 [0.432]
 [0.408]
 [0.4  ]
 [0.397]
 [0.397]
 [0.395]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.392]
 [0.432]
 [0.408]
 [0.4  ]
 [0.397]
 [0.397]
 [0.395]]
printing an ep nov before normalisation:  21.78472256958503
using explorer policy with actor:  0
printing an ep nov before normalisation:  53.17763119710502
printing an ep nov before normalisation:  46.85677401144295
printing an ep nov before normalisation:  47.78892517089844
printing an ep nov before normalisation:  35.16883436513292
printing an ep nov before normalisation:  34.7790487543562
printing an ep nov before normalisation:  50.21123534798663
printing an ep nov before normalisation:  1.2516710435147615
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.544]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]] [[49.522]
 [47.879]
 [47.395]
 [47.395]
 [47.395]
 [47.395]
 [47.395]] [[0.544]
 [0.544]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]]
printing an ep nov before normalisation:  33.64590825317053
printing an ep nov before normalisation:  39.32283521744982
printing an ep nov before normalisation:  38.234586448648514
Printing some Q and Qe and total Qs values:  [[0.4  ]
 [0.635]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.4  ]] [[44.64 ]
 [30.892]
 [44.64 ]
 [44.64 ]
 [44.64 ]
 [44.64 ]
 [44.64 ]] [[0.4  ]
 [0.635]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.4  ]]
printing an ep nov before normalisation:  3.0686384953810375
printing an ep nov before normalisation:  19.92824912071228
printing an ep nov before normalisation:  32.29701362203822
printing an ep nov before normalisation:  33.690071478195485
Printing some Q and Qe and total Qs values:  [[0.696]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]] [[29.123]
 [22.098]
 [22.098]
 [22.098]
 [22.098]
 [22.098]
 [22.098]] [[0.696]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]]
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.479]
 [0.398]
 [0.401]
 [0.401]
 [0.395]
 [0.401]] [[25.377]
 [30.847]
 [27.267]
 [25.377]
 [25.377]
 [29.466]
 [25.377]] [[0.401]
 [0.479]
 [0.398]
 [0.401]
 [0.401]
 [0.395]
 [0.401]]
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.0645612518349705, 0.16792694526600066, 0.061566890276506955, 0.06354166635807432, 0.21238448555243256, 0.43001876071201495]
printing an ep nov before normalisation:  47.13806902164349
actor:  0 policy actor:  1  step number:  31 total reward:  0.5733333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  36 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[0.691]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [0.67 ]] [[23.05 ]
 [17.602]
 [17.602]
 [17.602]
 [17.602]
 [17.602]
 [17.602]] [[0.691]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [0.67 ]]
Printing some Q and Qe and total Qs values:  [[0.704]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]] [[27.709]
 [19.008]
 [19.008]
 [19.008]
 [19.008]
 [19.008]
 [19.008]] [[0.704]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]]
maxi score, test score, baseline:  -0.9837778220451527 -1.0 -0.9837778220451527
maxi score, test score, baseline:  -0.9837778220451527 -1.0 -0.9837778220451527
probs:  [0.0647066538181195, 0.16815436452572333, 0.061721543018674926, 0.06369021824124768, 0.2106238773510096, 0.4311033430452249]
printing an ep nov before normalisation:  41.36340880551418
actor:  0 policy actor:  1  step number:  41 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.9783920634920635 -1.0 -0.9783920634920635
probs:  [0.06464055662875459, 0.16835411389936955, 0.06165304392986535, 0.06362330320081738, 0.21110586639795365, 0.43062311594323943]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
actor:  0 policy actor:  1  step number:  44 total reward:  0.5  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  0.18704682817656249
maxi score, test score, baseline:  -0.9725482213438735 -1.0 -0.9725482213438735
probs:  [0.0645125508624474, 0.1688243128788372, 0.061513607014972806, 0.06349140510793096, 0.21201348806558815, 0.4296446360702237]
printing an ep nov before normalisation:  32.96947872130021
printing an ep nov before normalisation:  27.122910022735596
siam score:  -0.8388005
maxi score, test score, baseline:  -0.9729739299610894 -0.704 -0.704
probs:  [0.0642556216435735, 0.1636092965465571, 0.0617206140845858, 0.0633924470267139, 0.21586176889488848, 0.43116025180368117]
using explorer policy with actor:  1
printing an ep nov before normalisation:  14.038304097002529
maxi score, test score, baseline:  -0.9729739299610894 -0.704 -0.704
probs:  [0.0642556216435735, 0.1636092965465571, 0.0617206140845858, 0.0633924470267139, 0.21586176889488848, 0.43116025180368117]
printing an ep nov before normalisation:  52.80421157779047
printing an ep nov before normalisation:  19.648798314596768
maxi score, test score, baseline:  -0.9729739299610894 -0.704 -0.704
probs:  [0.0642556216435735, 0.1636092965465571, 0.0617206140845858, 0.0633924470267139, 0.21586176889488848, 0.43116025180368117]
maxi score, test score, baseline:  -0.9730782945736434 -0.704 -0.704
probs:  [0.0642556216435735, 0.1636092965465571, 0.0617206140845858, 0.0633924470267139, 0.21586176889488848, 0.43116025180368117]
printing an ep nov before normalisation:  40.70809658547559
maxi score, test score, baseline:  -0.9730782945736434 -0.704 -0.704
probs:  [0.0642556216435735, 0.1636092965465571, 0.0617206140845858, 0.0633924470267139, 0.21586176889488848, 0.43116025180368117]
printing an ep nov before normalisation:  45.862481425934085
maxi score, test score, baseline:  -0.9730782945736434 -0.704 -0.704
probs:  [0.0642556216435735, 0.1636092965465571, 0.0617206140845858, 0.0633924470267139, 0.21586176889488848, 0.43116025180368117]
Printing some Q and Qe and total Qs values:  [[0.697]
 [0.542]
 [0.663]
 [0.668]
 [0.657]
 [0.642]
 [0.621]] [[35.984]
 [30.45 ]
 [39.503]
 [42.461]
 [39.791]
 [35.986]
 [34.301]] [[0.697]
 [0.542]
 [0.663]
 [0.668]
 [0.657]
 [0.642]
 [0.621]]
from probs:  [0.06419500961241167, 0.1638637919431544, 0.06165196209202302, 0.0633290973742865, 0.21628198681933206, 0.43067815215879246]
printing an ep nov before normalisation:  39.42636691037774
maxi score, test score, baseline:  -0.9731818532818532 -0.704 -0.704
probs:  [0.06407392117238321, 0.16437221329082544, 0.06151481171898773, 0.0632025398172844, 0.21712148241067747, 0.42971503158984176]
printing an ep nov before normalisation:  57.18498242532613
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
maxi score, test score, baseline:  -0.9731818532818532 -0.704 -0.704
probs:  [0.06395301322515379, 0.1648798767924783, 0.06137786578045264, 0.06307617090527347, 0.21795972666119284, 0.4287533466354489]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  61.432117246114174
maxi score, test score, baseline:  -0.9731818532818532 -0.704 -0.704
probs:  [0.06395301322515379, 0.1648798767924783, 0.06137786578045264, 0.06307617090527347, 0.21795972666119284, 0.4287533466354489]
maxi score, test score, baseline:  -0.9731818532818532 -0.704 -0.704
printing an ep nov before normalisation:  29.060438446651226
maxi score, test score, baseline:  -0.9731818532818532 -0.704 -0.704
probs:  [0.06395301322515379, 0.1648798767924783, 0.06137786578045264, 0.06307617090527347, 0.21795972666119284, 0.4287533466354489]
siam score:  -0.8449272
printing an ep nov before normalisation:  29.64742271833245
printing an ep nov before normalisation:  50.492033676374334
printing an ep nov before normalisation:  59.43243849164547
line 256 mcts: sample exp_bonus 54.05117830601871
maxi score, test score, baseline:  -0.9732846153846153 -0.704 -0.704
probs:  [0.06394760571740775, 0.16527570242002085, 0.06136222081514032, 0.062205679719426143, 0.21856657038471797, 0.4286422209432871]
printing an ep nov before normalisation:  64.61540450210802
printing an ep nov before normalisation:  45.58089751637011
maxi score, test score, baseline:  -0.9733865900383142 -0.704 -0.704
probs:  [0.06388738258439128, 0.16552972206373803, 0.06129397978228114, 0.06214005445530997, 0.2189858578156406, 0.42816300329863904]
printing an ep nov before normalisation:  43.81927675499128
Printing some Q and Qe and total Qs values:  [[0.211]
 [0.211]
 [0.211]
 [0.178]
 [0.211]
 [0.211]
 [0.211]] [[ 0.  ]
 [ 0.  ]
 [ 0.  ]
 [48.66]
 [ 0.  ]
 [ 0.  ]
 [ 0.  ]] [[-0.528]
 [-0.528]
 [-0.528]
 [ 1.405]
 [-0.528]
 [-0.528]
 [-0.528]]
printing an ep nov before normalisation:  62.08879267357892
maxi score, test score, baseline:  -0.9734877862595419 -0.704 -0.704
probs:  [0.06458785181905657, 0.16446618715392797, 0.06116647514945938, 0.06284076586011332, 0.2196648312484966, 0.42727388876894623]
maxi score, test score, baseline:  -0.9734877862595419 -0.704 -0.704
probs:  [0.06473820392415361, 0.164849523594215, 0.061308846272833714, 0.06298704257028813, 0.21784611179170157, 0.4282702718468079]
maxi score, test score, baseline:  -0.9734877862595419 -0.704 -0.704
Printing some Q and Qe and total Qs values:  [[0.176]
 [0.245]
 [0.23 ]
 [0.215]
 [0.192]
 [0.197]
 [0.213]] [[51.506]
 [53.262]
 [52.649]
 [50.435]
 [51.976]
 [53.855]
 [51.967]] [[1.515]
 [1.673]
 [1.626]
 [1.5  ]
 [1.555]
 [1.654]
 [1.575]]
maxi score, test score, baseline:  -0.9734877862595419 -0.704 -0.704
probs:  [0.06484966384468765, 0.1634098271096809, 0.0614143896843304, 0.06309548129471798, 0.2182217215655598, 0.42900891650102313]
maxi score, test score, baseline:  -0.9734877862595419 -0.704 -0.704
probs:  [0.06479262889102122, 0.16365637143638317, 0.061346773599069586, 0.06303304321002463, 0.2186370943004478, 0.4285340885630536]
printing an ep nov before normalisation:  39.09591766213122
printing an ep nov before normalisation:  43.78273211460602
maxi score, test score, baseline:  -0.9734877862595419 -0.704 -0.704
printing an ep nov before normalisation:  45.99626233321839
maxi score, test score, baseline:  -0.9734877862595419 -0.704 -0.704
probs:  [0.06484586927222495, 0.1624772076123821, 0.061383542776386704, 0.0630778727637118, 0.21942571192239152, 0.428789795652903]
printing an ep nov before normalisation:  6.808840382745984
siam score:  -0.8344868
maxi score, test score, baseline:  -0.9735882129277567 -0.704 -0.704
probs:  [0.0649544278320775, 0.16107334045249605, 0.06148629311757457, 0.06318346542467174, 0.2197935855781324, 0.42950888759504763]
maxi score, test score, baseline:  -0.9735882129277567 -0.704 -0.704
probs:  [0.0649544278320775, 0.16107334045249605, 0.06148629311757457, 0.06318346542467174, 0.2197935855781324, 0.42950888759504763]
Printing some Q and Qe and total Qs values:  [[0.171]
 [1.07 ]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]] [[56.083]
 [55.762]
 [56.083]
 [56.083]
 [56.083]
 [56.083]
 [56.083]] [[1.998]
 [2.881]
 [1.998]
 [1.998]
 [1.998]
 [1.998]
 [1.998]]
printing an ep nov before normalisation:  57.38088963109732
siam score:  -0.8344552
maxi score, test score, baseline:  -0.9735882129277567 -0.704 -0.704
probs:  [0.0649544278320775, 0.16107334045249605, 0.06148629311757457, 0.06318346542467174, 0.2197935855781324, 0.42950888759504763]
siam score:  -0.83439654
maxi score, test score, baseline:  -0.9735882129277567 -0.704 -0.704
probs:  [0.0649544278320775, 0.16107334045249605, 0.06148629311757457, 0.06318346542467174, 0.2197935855781324, 0.42950888759504763]
from probs:  [0.0649544278320775, 0.16107334045249605, 0.06148629311757457, 0.06318346542467174, 0.2197935855781324, 0.42950888759504763]
printing an ep nov before normalisation:  42.64919805162265
Printing some Q and Qe and total Qs values:  [[0.208]
 [0.239]
 [0.126]
 [0.126]
 [0.126]
 [0.126]
 [0.126]] [[33.895]
 [32.168]
 [27.338]
 [27.338]
 [27.338]
 [27.338]
 [27.338]] [[1.528]
 [1.491]
 [1.19 ]
 [1.19 ]
 [1.19 ]
 [1.19 ]
 [1.19 ]]
actor:  1 policy actor:  1  step number:  62 total reward:  0.15333333333333288  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9735882129277567 -0.704 -0.704
probs:  [0.06651398775769156, 0.1563522381752011, 0.06327247007434457, 0.06485874468534417, 0.20695635624074718, 0.4420462030666714]
maxi score, test score, baseline:  -0.9735882129277567 -0.704 -0.704
probs:  [0.06646183467043454, 0.15657810295345803, 0.06321028562713604, 0.06480146920151615, 0.2073388230100871, 0.441609484537368]
printing an ep nov before normalisation:  42.33680624082349
maxi score, test score, baseline:  -0.9735882129277567 -0.704 -0.704
probs:  [0.06646183467043454, 0.15657810295345803, 0.06321028562713604, 0.06480146920151615, 0.2073388230100871, 0.441609484537368]
maxi score, test score, baseline:  -0.9736878787878788 -0.704 -0.704
probs:  [0.06646183467043454, 0.15657810295345803, 0.06321028562713604, 0.06480146920151615, 0.2073388230100871, 0.441609484537368]
printing an ep nov before normalisation:  32.99005858303384
Printing some Q and Qe and total Qs values:  [[0.061]
 [0.004]
 [0.061]
 [0.061]
 [0.035]
 [0.039]
 [0.061]] [[61.778]
 [79.573]
 [61.778]
 [61.778]
 [61.533]
 [60.42 ]
 [61.778]] [[1.358]
 [1.802]
 [1.358]
 [1.358]
 [1.326]
 [1.298]
 [1.358]]
maxi score, test score, baseline:  -0.9737867924528302 -0.704 -0.704
probs:  [0.06646183467043454, 0.15657810295345803, 0.06321028562713604, 0.06480146920151615, 0.2073388230100871, 0.441609484537368]
printing an ep nov before normalisation:  75.51316992898226
line 256 mcts: sample exp_bonus 41.295885507428665
printing an ep nov before normalisation:  66.9810390200949
printing an ep nov before normalisation:  71.83451263898581
printing an ep nov before normalisation:  41.802317625809934
maxi score, test score, baseline:  -0.973884962406015 -0.704 -0.704
probs:  [0.0664097146625041, 0.1568038244716482, 0.06314814062188095, 0.06474423004601568, 0.2077210471908512, 0.44117304300709986]
printing an ep nov before normalisation:  59.87826931253451
maxi score, test score, baseline:  -0.973884962406015 -0.704 -0.704
probs:  [0.0664097146625041, 0.1568038244716482, 0.06314814062188095, 0.06474423004601568, 0.2077210471908512, 0.44117304300709986]
actions average: 
K:  2  action  0 :  tensor([0.2475, 0.0262, 0.1106, 0.1177, 0.1732, 0.1604, 0.1644],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0300, 0.8528, 0.0184, 0.0184, 0.0161, 0.0153, 0.0489],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1700, 0.2087, 0.1746, 0.0840, 0.1062, 0.1317, 0.1249],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1607, 0.0265, 0.1031, 0.1914, 0.1884, 0.1894, 0.1405],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1727, 0.0015, 0.0924, 0.1057, 0.3606, 0.1477, 0.1194],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1621, 0.0179, 0.1188, 0.1402, 0.1704, 0.2407, 0.1499],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1762, 0.0864, 0.1082, 0.1192, 0.1636, 0.1473, 0.1991],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.973884962406015 -0.704 -0.704
probs:  [0.0664097146625041, 0.1568038244716482, 0.06314814062188095, 0.06474423004601568, 0.2077210471908512, 0.44117304300709986]
maxi score, test score, baseline:  -0.973884962406015 -0.704 -0.704
probs:  [0.06700101314291512, 0.1569776709565374, 0.06298471005979912, 0.06534321995541617, 0.20765975118387398, 0.44003363470145834]
maxi score, test score, baseline:  -0.973884962406015 -0.704 -0.704
probs:  [0.06700101314291512, 0.1569776709565374, 0.06298471005979912, 0.06534321995541617, 0.20765975118387398, 0.44003363470145834]
printing an ep nov before normalisation:  6.2316430126900615
maxi score, test score, baseline:  -0.973884962406015 -0.704 -0.704
probs:  [0.06747887634533724, 0.15759538868925602, 0.06270107705637462, 0.06581850637966938, 0.2083562462205833, 0.4380499053087795]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.973884962406015 -0.704 -0.704
probs:  [0.06747887634533724, 0.15759538868925602, 0.06270107705637462, 0.06581850637966938, 0.2083562462205833, 0.4380499053087795]
siam score:  -0.8498672
printing an ep nov before normalisation:  68.77632633354001
printing an ep nov before normalisation:  22.434324288553853
maxi score, test score, baseline:  -0.9739823970037452 -0.704 -0.704
probs:  [0.06758301412888536, 0.15629379341311628, 0.06279782684335289, 0.06592007670341662, 0.20867822520307253, 0.43872706370815634]
siam score:  -0.8478366
maxi score, test score, baseline:  -0.9739823970037452 -0.704 -0.704
probs:  [0.06753525624076567, 0.15651445521045426, 0.06273559001055024, 0.06586728712530072, 0.20905739099301882, 0.4382900204199102]
actor:  1 policy actor:  1  step number:  53 total reward:  0.26666666666666605  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.059]
 [0.076]
 [0.058]
 [0.057]
 [0.059]
 [0.065]
 [0.061]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.059]
 [0.076]
 [0.058]
 [0.057]
 [0.059]
 [0.065]
 [0.061]]
maxi score, test score, baseline:  -0.9739823970037452 -0.704 -0.704
probs:  [0.05985560684367089, 0.1386966978527565, 0.05560280476800197, 0.058377682718083815, 0.2990998764003269, 0.38836733141715984]
from probs:  [0.05985560684367089, 0.1386966978527565, 0.05560280476800197, 0.058377682718083815, 0.2990998764003269, 0.38836733141715984]
maxi score, test score, baseline:  -0.9740791044776119 -0.704 -0.704
maxi score, test score, baseline:  -0.9740791044776119 -0.704 -0.704
probs:  [0.0598357004261595, 0.1389441223862252, 0.05556847813371443, 0.05763471208584573, 0.2998911890727409, 0.38812579789531426]
maxi score, test score, baseline:  -0.9741750929368029 -0.704 -0.704
probs:  [0.0598357004261595, 0.1389441223862252, 0.05556847813371443, 0.05763471208584573, 0.2998911890727409, 0.38812579789531426]
maxi score, test score, baseline:  -0.9741750929368029 -0.704 -0.704
maxi score, test score, baseline:  -0.9741750929368029 -0.704 -0.704
probs:  [0.06036732696503677, 0.13916332509565674, 0.055458650845912415, 0.05817503101408951, 0.29947476162211645, 0.38736090445718824]
printing an ep nov before normalisation:  49.478508858295655
maxi score, test score, baseline:  -0.9741750929368029 -0.704 -0.704
probs:  [0.060543300920599986, 0.13956950160134585, 0.05562028410160287, 0.058344600175874964, 0.29743012287859927, 0.38849219032197696]
printing an ep nov before normalisation:  49.11899381345176
maxi score, test score, baseline:  -0.9741750929368029 -0.704 -0.704
probs:  [0.059839374833458026, 0.1396740540276125, 0.05566188953375151, 0.05838824836092838, 0.2976530427166769, 0.38878339052757266]
Printing some Q and Qe and total Qs values:  [[0.254]
 [0.383]
 [0.319]
 [0.254]
 [0.254]
 [0.254]
 [0.254]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.254]
 [0.383]
 [0.319]
 [0.254]
 [0.254]
 [0.254]
 [0.254]]
maxi score, test score, baseline:  -0.9743649446494465 -0.704 -0.704
probs:  [0.05914940295913332, 0.13977653386865554, 0.05570267020451515, 0.05843103129027606, 0.2978715435242363, 0.38906881815318367]
maxi score, test score, baseline:  -0.9743649446494465 -0.704 -0.704
probs:  [0.05916509268201511, 0.1387580023043696, 0.055704483014875426, 0.05844382877244285, 0.298848353965611, 0.38908023926068613]
actions average: 
K:  3  action  0 :  tensor([0.2615, 0.0273, 0.1138, 0.1257, 0.2145, 0.1269, 0.1301],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0169, 0.8523, 0.0223, 0.0360, 0.0154, 0.0248, 0.0323],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1769, 0.0365, 0.2689, 0.1110, 0.1387, 0.1412, 0.1267],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2139, 0.0470, 0.1123, 0.1268, 0.2120, 0.1534, 0.1346],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2197, 0.0580, 0.1016, 0.1121, 0.2618, 0.1254, 0.1214],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1903, 0.0628, 0.1165, 0.1439, 0.1894, 0.1616, 0.1355],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2061, 0.0495, 0.1376, 0.1295, 0.1308, 0.1581, 0.1884],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9743649446494465 -0.704 -0.704
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.465]
 [0.391]
 [0.391]
 [0.379]
 [0.374]
 [0.374]] [[34.053]
 [48.821]
 [34.053]
 [34.053]
 [34.011]
 [33.976]
 [33.844]] [[0.391]
 [0.465]
 [0.391]
 [0.391]
 [0.379]
 [0.374]
 [0.374]]
maxi score, test score, baseline:  -0.9743649446494465 -0.704 -0.704
probs:  [0.059335756364857176, 0.13915877253848272, 0.05586514192518713, 0.058612407250062795, 0.2968232199906166, 0.39020470193079354]
maxi score, test score, baseline:  -0.9743649446494465 -0.704 -0.704
maxi score, test score, baseline:  -0.9743649446494465 -0.704 -0.704
actor:  1 policy actor:  1  step number:  46 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9744588235294117 -0.704 -0.704
probs:  [0.06180895705685862, 0.13341678513103739, 0.05869552970193277, 0.06116005325025302, 0.27485479621803444, 0.4100638786418836]
printing an ep nov before normalisation:  55.11987602123652
printing an ep nov before normalisation:  52.136118357341
maxi score, test score, baseline:  -0.9744588235294117 -0.704 -0.704
probs:  [0.06180895705685862, 0.13341678513103739, 0.05869552970193277, 0.06116005325025302, 0.27485479621803444, 0.4100638786418836]
siam score:  -0.84346163
maxi score, test score, baseline:  -0.9744588235294117 -0.704 -0.704
probs:  [0.061848201194708206, 0.13350159317971802, 0.05873279277248575, 0.060563095220541444, 0.27502960097655255, 0.41032471665599396]
printing an ep nov before normalisation:  25.84967226096163
actions average: 
K:  0  action  0 :  tensor([0.2370, 0.0227, 0.1120, 0.1190, 0.1981, 0.1660, 0.1452],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0233, 0.8483, 0.0224, 0.0265, 0.0082, 0.0109, 0.0603],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1890, 0.0285, 0.1534, 0.1353, 0.1668, 0.1761, 0.1509],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1707, 0.0554, 0.1181, 0.1838, 0.1584, 0.1580, 0.1555],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1896, 0.0041, 0.1093, 0.1129, 0.2758, 0.1618, 0.1464],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1396, 0.0151, 0.1354, 0.1134, 0.1416, 0.3299, 0.1249],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1736, 0.0279, 0.1030, 0.1564, 0.1573, 0.1605, 0.2213],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9744588235294117 -0.704 -0.704
probs:  [0.06173100433523192, 0.13377464523105037, 0.05859862833185502, 0.06043889923383896, 0.2760734630253124, 0.40938335984271135]
maxi score, test score, baseline:  -0.9744588235294117 -0.704 -0.704
probs:  [0.06173100433523192, 0.13377464523105037, 0.05859862833185502, 0.06043889923383896, 0.2760734630253124, 0.40938335984271135]
printing an ep nov before normalisation:  39.81764852172061
maxi score, test score, baseline:  -0.9744588235294117 -0.704 -0.704
using another actor
maxi score, test score, baseline:  -0.9744588235294117 -0.704 -0.704
probs:  [0.06127468088861587, 0.13420559993961495, 0.058787162248852644, 0.06063336748930192, 0.274396116931136, 0.4107030725024786]
printing an ep nov before normalisation:  30.68603088282984
maxi score, test score, baseline:  -0.9745520146520146 -0.704 -0.704
probs:  [0.06127468088861587, 0.13420559993961495, 0.058787162248852644, 0.06063336748930192, 0.274396116931136, 0.4107030725024786]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9745520146520146 -0.704 -0.704
probs:  [0.06097209183385254, 0.13254105692045942, 0.05911555017678745, 0.06097209183385254, 0.27339746436256945, 0.4130017448724786]
Printing some Q and Qe and total Qs values:  [[0.15 ]
 [0.126]
 [0.143]
 [0.127]
 [0.112]
 [0.121]
 [0.135]] [[36.028]
 [46.037]
 [41.429]
 [38.332]
 [37.409]
 [34.894]
 [36.942]] [[1.001]
 [1.455]
 [1.252]
 [1.088]
 [1.029]
 [0.918]
 [1.03 ]]
siam score:  -0.84210116
maxi score, test score, baseline:  -0.9745520146520146 -0.704 -0.704
probs:  [0.06052905302733493, 0.1329557560816926, 0.05930028185028996, 0.061162638165498745, 0.27175742854807405, 0.4142948423271097]
printing an ep nov before normalisation:  63.22733402252197
maxi score, test score, baseline:  -0.9745520146520146 -0.704 -0.704
probs:  [0.06067756129327053, 0.13328235552944243, 0.05944576867106968, 0.06131270436409284, 0.2699683791756129, 0.4153132309665115]
maxi score, test score, baseline:  -0.9747363636363636 -0.704 -0.704
probs:  [0.06023795960358864, 0.13368805309374088, 0.0596264904978655, 0.06149911463414259, 0.26837012214987344, 0.4165782600207889]
printing an ep nov before normalisation:  47.83568299130003
maxi score, test score, baseline:  -0.9747363636363636 -0.704 -0.704
probs:  [0.06017432680320829, 0.13382649980509226, 0.05956117539359075, 0.06143895158554448, 0.2688791127973946, 0.41611993361516963]
maxi score, test score, baseline:  -0.9747363636363636 -0.704 -0.704
probs:  [0.06017432680320829, 0.13382649980509226, 0.05956117539359075, 0.06143895158554448, 0.2688791127973946, 0.41611993361516963]
printing an ep nov before normalisation:  24.634394757671885
maxi score, test score, baseline:  -0.9747363636363636 -0.704 -0.704
maxi score, test score, baseline:  -0.9747363636363636 -0.704 -0.704
probs:  [0.06017432680320829, 0.13382649980509226, 0.05956117539359075, 0.06143895158554448, 0.2688791127973946, 0.41611993361516963]
printing an ep nov before normalisation:  34.72707698873712
from probs:  [0.06011076361396681, 0.13396479506258346, 0.05949593174081218, 0.06137885435234826, 0.2693875466343018, 0.41566210859598746]
maxi score, test score, baseline:  -0.9747363636363636 -0.704 -0.704
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9747363636363636 -0.704 -0.704
probs:  [0.05998384561249284, 0.13424093220899747, 0.05936565832126962, 0.06125885690064075, 0.2704027475270207, 0.4147479594295786]
maxi score, test score, baseline:  -0.9747363636363636 -0.704 -0.704
maxi score, test score, baseline:  -0.9747363636363636 -0.704 -0.704
probs:  [0.05998384561249284, 0.13424093220899747, 0.05936565832126962, 0.06125885690064075, 0.2704027475270207, 0.4147479594295786]
maxi score, test score, baseline:  -0.9747363636363636 -0.704 -0.704
probs:  [0.06012817154317046, 0.13456432434377602, 0.05950849353292315, 0.06140625743930555, 0.2686449768621379, 0.41574777627868703]
printing an ep nov before normalisation:  53.45753192196152
printing an ep nov before normalisation:  30.99915115818928
printing an ep nov before normalisation:  42.08475866866228
Printing some Q and Qe and total Qs values:  [[-0.123]
 [-0.123]
 [-0.123]
 [-0.123]
 [-0.123]
 [-0.123]
 [-0.123]] [[33.57]
 [33.57]
 [33.57]
 [33.57]
 [33.57]
 [33.57]
 [33.57]] [[1.185]
 [1.185]
 [1.185]
 [1.185]
 [1.185]
 [1.185]
 [1.185]]
printing an ep nov before normalisation:  53.890581130981445
printing an ep nov before normalisation:  22.18808650970459
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9747363636363636 -0.704 -0.704
probs:  [0.06027064490478459, 0.13488356541393667, 0.05964949531039721, 0.06155176594320855, 0.26690976895877117, 0.4167347594689018]
maxi score, test score, baseline:  -0.9747363636363636 -0.704 -0.704
probs:  [0.06027064490478459, 0.13488356541393667, 0.05964949531039721, 0.06155176594320855, 0.26690976895877117, 0.4167347594689018]
maxi score, test score, baseline:  -0.9747363636363636 -0.704 -0.704
probs:  [0.06027064490478459, 0.13488356541393667, 0.05964949531039721, 0.06155176594320855, 0.26690976895877117, 0.4167347594689018]
siam score:  -0.83259046
maxi score, test score, baseline:  -0.974827536231884 -0.704 -0.704
probs:  [0.06027064490478459, 0.13488356541393667, 0.05964949531039721, 0.06155176594320855, 0.26690976895877117, 0.4167347594689018]
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[ 0.045]
 [-0.   ]
 [ 0.046]
 [ 0.072]
 [ 0.068]
 [ 0.067]
 [ 0.055]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.045]
 [-0.   ]
 [ 0.046]
 [ 0.072]
 [ 0.068]
 [ 0.067]
 [ 0.055]]
printing an ep nov before normalisation:  53.18773995687954
maxi score, test score, baseline:  -0.974827536231884 -0.704 -0.704
probs:  [0.06020776625443729, 0.13502381968820348, 0.059584925586535185, 0.061492375131985404, 0.26740946324247267, 0.41628165009636603]
printing an ep nov before normalisation:  40.3207996677518
maxi score, test score, baseline:  -0.974827536231884 -0.704 -0.704
probs:  [0.06105249266731568, 0.13482205692415097, 0.05981656617346666, 0.062339916098408416, 0.26406244054062783, 0.4179065275960304]
maxi score, test score, baseline:  -0.974827536231884 -0.704 -0.704
probs:  [0.06105249266731568, 0.13482205692415097, 0.05981656617346666, 0.062339916098408416, 0.26406244054062783, 0.4179065275960304]
maxi score, test score, baseline:  -0.974827536231884 -0.704 -0.704
probs:  [0.06105249266731568, 0.13482205692415097, 0.05981656617346666, 0.062339916098408416, 0.26406244054062783, 0.4179065275960304]
maxi score, test score, baseline:  -0.974827536231884 -0.704 -0.704
printing an ep nov before normalisation:  38.661839760584215
printing an ep nov before normalisation:  54.447688741184145
maxi score, test score, baseline:  -0.9750079136690647 -0.704 -0.704
probs:  [0.06157046353195891, 0.13512331904485148, 0.059740321282410955, 0.06220566754297041, 0.263984040378077, 0.41737618821973116]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
printing an ep nov before normalisation:  41.80246004343081
printing an ep nov before normalisation:  48.10316958706651
printing an ep nov before normalisation:  59.46693077890729
maxi score, test score, baseline:  -0.9750971326164874 -0.704 -0.704
probs:  [0.06145238450821787, 0.1354037064250714, 0.059612327617071977, 0.06209102968349531, 0.2649625198701996, 0.4164780318959438]
printing an ep nov before normalisation:  32.328728488091656
maxi score, test score, baseline:  -0.9750971326164874 -0.704 -0.704
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
maxi score, test score, baseline:  -0.9750971326164874 -0.704 -0.704
probs:  [0.06145238450821787, 0.1354037064250714, 0.059612327617071977, 0.06209102968349531, 0.2649625198701996, 0.4164780318959438]
actor:  1 policy actor:  1  step number:  59 total reward:  0.013333333333332642  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9750971326164874 -0.704 -0.704
probs:  [0.05844107293441273, 0.1287607325823307, 0.05669137906871462, 0.05904835499776501, 0.30102663124603735, 0.3960318291707395]
maxi score, test score, baseline:  -0.9750971326164874 -0.704 -0.704
probs:  [0.05837732915139946, 0.128876821768238, 0.05662316068152741, 0.058986164256062956, 0.3015832670465837, 0.3955532570961885]
maxi score, test score, baseline:  -0.9750971326164874 -0.704 -0.704
probs:  [0.05837732915139946, 0.128876821768238, 0.05662316068152741, 0.058986164256062956, 0.3015832670465837, 0.3955532570961885]
printing an ep nov before normalisation:  39.730825424194336
printing an ep nov before normalisation:  29.747369099443233
siam score:  -0.8386473
printing an ep nov before normalisation:  42.29005650645106
printing an ep nov before normalisation:  50.71251484055764
printing an ep nov before normalisation:  47.20099784948817
maxi score, test score, baseline:  -0.9751857142857142 -0.704 -0.704
probs:  [0.05818660645013129, 0.1292241629845515, 0.05641904987314012, 0.05880008828613166, 0.30324873273569497, 0.3941213596703505]
maxi score, test score, baseline:  -0.9751857142857142 -0.704 -0.704
printing an ep nov before normalisation:  40.63279532752517
maxi score, test score, baseline:  -0.9751857142857142 -0.704 -0.704
probs:  [0.05818660645013129, 0.1292241629845515, 0.05641904987314012, 0.05880008828613166, 0.30324873273569497, 0.3941213596703505]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
using explorer policy with actor:  0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[ 0.004]
 [-0.002]
 [-0.008]
 [-0.005]
 [-0.005]
 [-0.001]
 [ 0.001]] [[50.889]
 [46.799]
 [51.681]
 [51.231]
 [50.901]
 [50.107]
 [50.461]] [[0.864]
 [0.724]
 [0.878]
 [0.866]
 [0.855]
 [0.833]
 [0.846]]
printing an ep nov before normalisation:  28.20254819990562
printing an ep nov before normalisation:  65.83128694972068
printing an ep nov before normalisation:  0.022136373372916296
maxi score, test score, baseline:  -0.975273665480427 -0.704 -0.704
probs:  [0.05873700507627544, 0.12989545179207784, 0.05639939695560489, 0.0593515309224311, 0.3016306797953909, 0.3939859354582199]
maxi score, test score, baseline:  -0.9753609929078014 -0.704 -0.704
Printing some Q and Qe and total Qs values:  [[0.28 ]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.28 ]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]]
maxi score, test score, baseline:  -0.975447703180212 -0.704 -0.704
actor:  1 policy actor:  1  step number:  60 total reward:  0.11333333333333284  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  0
using another actor
from probs:  [0.059826282205069095, 0.12797930579422892, 0.057587404478766896, 0.059826282205069095, 0.2924611851127151, 0.40231954020415084]
maxi score, test score, baseline:  -0.975447703180212 -0.704 -0.704
printing an ep nov before normalisation:  42.34319975151027
using explorer policy with actor:  0
printing an ep nov before normalisation:  58.04366545953743
printing an ep nov before normalisation:  16.361758563319402
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.975619298245614 -0.704 -0.704
probs:  [0.0597076923755689, 0.12820405190456985, 0.05745753580721587, 0.0597076923755689, 0.29351454509297137, 0.4014084824441051]
actions average: 
K:  3  action  0 :  tensor([0.3058, 0.0242, 0.1174, 0.0954, 0.1859, 0.1357, 0.1355],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0123, 0.9134, 0.0105, 0.0128, 0.0142, 0.0136, 0.0232],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1985, 0.0317, 0.1654, 0.1052, 0.1756, 0.1697, 0.1541],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1965, 0.0875, 0.1032, 0.1716, 0.1588, 0.1660, 0.1165],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2206, 0.0117, 0.1297, 0.1231, 0.1909, 0.1644, 0.1595],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1513, 0.0374, 0.0867, 0.0915, 0.1355, 0.3944, 0.1033],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1985, 0.1197, 0.1127, 0.0999, 0.1395, 0.1425, 0.1872],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  27.963560107017305
maxi score, test score, baseline:  -0.975619298245614 -0.704 -0.704
probs:  [0.0597076923755689, 0.12820405190456985, 0.05745753580721587, 0.0597076923755689, 0.29351454509297137, 0.4014084824441051]
maxi score, test score, baseline:  -0.975619298245614 -0.704 -0.704
probs:  [0.059773598152296444, 0.12724034226079356, 0.057520952290672486, 0.059773598152296444, 0.2938391056821467, 0.40185240346179435]
maxi score, test score, baseline:  -0.975619298245614 -0.704 -0.704
probs:  [0.059773598152296444, 0.12724034226079356, 0.057520952290672486, 0.059773598152296444, 0.2938391056821467, 0.40185240346179435]
siam score:  -0.83029276
maxi score, test score, baseline:  -0.975619298245614 -0.704 -0.704
probs:  [0.05992017222138978, 0.1275527247408721, 0.05766199018582169, 0.05992017222138978, 0.292105259373832, 0.40283968125669467]
Printing some Q and Qe and total Qs values:  [[0.197]
 [0.371]
 [0.179]
 [0.197]
 [0.19 ]
 [0.152]
 [0.197]] [[39.847]
 [39.895]
 [42.528]
 [39.847]
 [41.26 ]
 [44.252]
 [39.847]] [[1.557]
 [1.734]
 [1.746]
 [1.557]
 [1.658]
 [1.851]
 [1.557]]
printing an ep nov before normalisation:  43.61823081970215
maxi score, test score, baseline:  -0.975619298245614 -0.704 -0.704
probs:  [0.059861311414577435, 0.12766365088793838, 0.057597460365570716, 0.059861311414577435, 0.29262928354200707, 0.40238698237532894]
maxi score, test score, baseline:  -0.975619298245614 -0.704 -0.704
probs:  [0.059861311414577435, 0.12766365088793838, 0.057597460365570716, 0.059861311414577435, 0.29262928354200707, 0.40238698237532894]
maxi score, test score, baseline:  -0.975619298245614 -0.704 -0.704
probs:  [0.059861311414577435, 0.12766365088793838, 0.057597460365570716, 0.059861311414577435, 0.29262928354200707, 0.40238698237532894]
maxi score, test score, baseline:  -0.975619298245614 -0.704 -0.704
probs:  [0.059861311414577435, 0.12766365088793838, 0.057597460365570716, 0.059861311414577435, 0.29262928354200707, 0.40238698237532894]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.975619298245614 -0.704 -0.704
probs:  [0.06000636835531601, 0.12797337408232418, 0.05773701926783523, 0.06000636835531601, 0.290912965241309, 0.4033639046978996]
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.15486614181022
maxi score, test score, baseline:  -0.975619298245614 -0.704 -0.704
probs:  [0.06000636835531601, 0.12797337408232418, 0.05773701926783523, 0.06000636835531601, 0.290912965241309, 0.4033639046978996]
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.516]
 [0.413]
 [0.513]
 [0.488]
 [0.415]
 [0.406]] [[25.567]
 [26.565]
 [27.287]
 [26.981]
 [27.449]
 [26.662]
 [28.212]] [[1.6  ]
 [1.83 ]
 [1.763]
 [1.848]
 [1.847]
 [1.734]
 [1.803]]
printing an ep nov before normalisation:  57.10199220325704
actor:  1 policy actor:  1  step number:  56 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.975619298245614 -0.704 -0.704
maxi score, test score, baseline:  -0.975619298245614 -0.704 -0.704
probs:  [0.06125429004317741, 0.1258570720187239, 0.05909726895983331, 0.06015420929067192, 0.28073149631082867, 0.4129056633767647]
printing an ep nov before normalisation:  64.27353035134817
printing an ep nov before normalisation:  55.065586758193064
maxi score, test score, baseline:  -0.975619298245614 -0.704 -0.704
printing an ep nov before normalisation:  27.91781441173304
maxi score, test score, baseline:  -0.975619298245614 -0.704 -0.704
maxi score, test score, baseline:  -0.9757041958041958 -0.704 -0.704
probs:  [0.0613179767200682, 0.12494693022194059, 0.05915870807267033, 0.06021674970989528, 0.281023872954284, 0.4133357623211415]
Printing some Q and Qe and total Qs values:  [[-0.033]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.033]] [[46.467]
 [46.467]
 [46.467]
 [46.467]
 [46.467]
 [46.467]
 [46.467]] [[1.591]
 [1.591]
 [1.591]
 [1.591]
 [1.591]
 [1.591]
 [1.591]]
maxi score, test score, baseline:  -0.9757041958041958 -0.704 -0.704
probs:  [0.0613179767200682, 0.12494693022194059, 0.05915870807267033, 0.06021674970989528, 0.281023872954284, 0.4133357623211415]
siam score:  -0.8266345
maxi score, test score, baseline:  -0.9757041958041958 -0.704 -0.704
maxi score, test score, baseline:  -0.9757041958041958 -0.704 -0.704
probs:  [0.06149022461226107, 0.12529830695144736, 0.059324877171154214, 0.05985013471874944, 0.2795374425025931, 0.4144990140437948]
printing an ep nov before normalisation:  66.7122749678062
Printing some Q and Qe and total Qs values:  [[0.157]
 [0.266]
 [0.216]
 [0.224]
 [0.157]
 [0.157]
 [0.157]] [[71.771]
 [80.624]
 [81.048]
 [80.03 ]
 [71.771]
 [71.771]
 [71.771]] [[1.214]
 [1.487]
 [1.445]
 [1.433]
 [1.214]
 [1.214]
 [1.214]]
maxi score, test score, baseline:  -0.9757041958041958 -0.704 -0.704
probs:  [0.060966439925653836, 0.125368211516981, 0.05935793564560394, 0.05988348654898657, 0.2796934894629688, 0.41473043689980593]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9757041958041958 -0.704 -0.704
probs:  [0.060966439925653836, 0.125368211516981, 0.05935793564560394, 0.05988348654898657, 0.2796934894629688, 0.41473043689980593]
maxi score, test score, baseline:  -0.9757041958041958 -0.704 -0.704
probs:  [0.060966439925653836, 0.125368211516981, 0.05935793564560394, 0.05988348654898657, 0.2796934894629688, 0.41473043689980593]
printing an ep nov before normalisation:  42.85568272945062
siam score:  -0.8291063
using explorer policy with actor:  1
from probs:  [0.060966439925653836, 0.125368211516981, 0.05935793564560394, 0.05988348654898657, 0.2796934894629688, 0.41473043689980593]
maxi score, test score, baseline:  -0.9757041958041958 -0.704 -0.704
probs:  [0.060941508527442616, 0.12554121116415642, 0.05932806070429147, 0.05932806070429147, 0.2803407891618146, 0.41452036973800344]
actor:  1 policy actor:  1  step number:  60 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9757041958041958 -0.704 -0.704
maxi score, test score, baseline:  -0.9757885017421603 -0.704 -0.704
probs:  [0.06232778929461033, 0.12293405839571478, 0.060814081816723035, 0.060814081816723035, 0.26816421423131653, 0.4249457744449124]
printing an ep nov before normalisation:  41.8804407119751
Printing some Q and Qe and total Qs values:  [[0.207]
 [0.345]
 [0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]] [[52.591]
 [58.978]
 [52.591]
 [52.591]
 [52.591]
 [52.591]
 [52.591]] [[1.546]
 [1.971]
 [1.546]
 [1.546]
 [1.546]
 [1.546]
 [1.546]]
Printing some Q and Qe and total Qs values:  [[0.045]
 [0.096]
 [0.072]
 [0.037]
 [0.037]
 [0.026]
 [0.059]] [[50.219]
 [48.954]
 [48.451]
 [48.139]
 [47.767]
 [47.925]
 [47.634]] [[1.639]
 [1.616]
 [1.562]
 [1.509]
 [1.488]
 [1.485]
 [1.502]]
printing an ep nov before normalisation:  75.63199192128548
maxi score, test score, baseline:  -0.9757885017421603 -0.704 -0.704
probs:  [0.06178971197140687, 0.12309899409203473, 0.060787586336674165, 0.060787586336674165, 0.2687766964514263, 0.4247594248117838]
maxi score, test score, baseline:  -0.9757885017421603 -0.704 -0.704
probs:  [0.06178971197140687, 0.12309899409203473, 0.060787586336674165, 0.060787586336674165, 0.2687766964514263, 0.4247594248117838]
maxi score, test score, baseline:  -0.9757885017421603 -0.704 -0.704
probs:  [0.06178971197140687, 0.12309899409203473, 0.060787586336674165, 0.060787586336674165, 0.2687766964514263, 0.4247594248117838]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.04 ]
 [0.188]
 [0.027]
 [0.038]
 [0.039]
 [0.039]
 [0.015]] [[26.297]
 [22.968]
 [21.814]
 [21.115]
 [20.251]
 [18.763]
 [20.978]] [[1.362]
 [1.276]
 [1.033]
 [0.996]
 [0.936]
 [0.831]
 [0.963]]
actor:  1 policy actor:  1  step number:  61 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9757885017421603 -0.704 -0.704
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9757885017421603 -0.704 -0.704
probs:  [0.06319449293354483, 0.12098417644083151, 0.06224989659735225, 0.06224989659735225, 0.2563048305185719, 0.4350167069123473]
maxi score, test score, baseline:  -0.9757885017421603 -0.704 -0.704
probs:  [0.06280473035189904, 0.12023782684888382, 0.0623364567676553, 0.0623364567676553, 0.2566618080852062, 0.43562272117870027]
maxi score, test score, baseline:  -0.9757885017421603 -0.704 -0.704
probs:  [0.06280473035189904, 0.12023782684888382, 0.0623364567676553, 0.0623364567676553, 0.2566618080852062, 0.43562272117870027]
maxi score, test score, baseline:  -0.9757885017421603 -0.704 -0.704
Printing some Q and Qe and total Qs values:  [[0.133]
 [0.133]
 [0.133]
 [0.133]
 [0.133]
 [0.133]
 [0.133]] [[44.169]
 [44.169]
 [44.169]
 [44.169]
 [44.169]
 [44.169]
 [44.169]] [[2.058]
 [2.058]
 [2.058]
 [2.058]
 [2.058]
 [2.058]
 [2.058]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.98609934877595
actor:  1 policy actor:  1  step number:  68 total reward:  0.006666666666665821  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9757885017421603 -0.704 -0.704
probs:  [0.05991626648645294, 0.11489021637162483, 0.059468043247190246, 0.10471264337806693, 0.24547285190852217, 0.41553997860814285]
printing an ep nov before normalisation:  42.62521765149856
printing an ep nov before normalisation:  39.01396488887991
printing an ep nov before normalisation:  31.132623107305335
maxi score, test score, baseline:  -0.9757885017421603 -0.704 -0.704
probs:  [0.05991626648645294, 0.11489021637162483, 0.059468043247190246, 0.10471264337806693, 0.24547285190852217, 0.41553997860814285]
actor:  1 policy actor:  1  step number:  58 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  51.960458448211114
printing an ep nov before normalisation:  56.55703067779541
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.22960767532609
maxi score, test score, baseline:  -0.9758722222222221 -0.704 -0.704
probs:  [0.06080510107420902, 0.1136953860547697, 0.060373866736690415, 0.10390357125586748, 0.23932857816640457, 0.42189349671205895]
line 256 mcts: sample exp_bonus 29.245613624478526
maxi score, test score, baseline:  -0.9758722222222221 -0.704 -0.704
probs:  [0.06085598793924658, 0.11379063319091585, 0.06042439191579394, 0.10315255373612883, 0.23952919668529823, 0.42224723653261664]
printing an ep nov before normalisation:  28.242019921473904
maxi score, test score, baseline:  -0.9758722222222221 -0.704 -0.704
probs:  [0.06096621640644928, 0.11399695255869556, 0.0605338369178316, 0.10333956207177901, 0.23814994280321333, 0.42301348924203125]
actor:  1 policy actor:  1  step number:  51 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9758722222222221 -0.704 -0.704
probs:  [0.05696705443182359, 0.10651154891304715, 0.0565630995795637, 0.09655477549305541, 0.28819018937985197, 0.3952133322026582]
maxi score, test score, baseline:  -0.9758722222222221 -0.704 -0.704
probs:  [0.057010853318402686, 0.10659352917475068, 0.05660658715906603, 0.0958590504184688, 0.2884121799574623, 0.39551779997184944]
maxi score, test score, baseline:  -0.9758722222222221 -0.704 -0.704
probs:  [0.057010853318402686, 0.10659352917475068, 0.05660658715906603, 0.0958590504184688, 0.2884121799574623, 0.39551779997184944]
Printing some Q and Qe and total Qs values:  [[0.135]
 [0.135]
 [0.135]
 [0.135]
 [0.135]
 [0.135]
 [0.135]] [[62.705]
 [62.705]
 [62.705]
 [62.705]
 [62.705]
 [62.705]
 [62.705]] [[1.42]
 [1.42]
 [1.42]
 [1.42]
 [1.42]
 [1.42]
 [1.42]]
from probs:  [0.05713527948953012, 0.1068264229962265, 0.056730128952743854, 0.09606846136677775, 0.2868569592364947, 0.39638274795822714]
maxi score, test score, baseline:  -0.9758722222222221 -0.704 -0.704
from probs:  [0.057220988754239394, 0.10698684871706282, 0.05681522902615321, 0.09471029600305984, 0.2872880819611284, 0.3969785555383564]
maxi score, test score, baseline:  -0.9759553633217992 -0.704 -0.704
probs:  [0.057160428946405356, 0.10704155891532528, 0.05675372937871097, 0.09473657067730555, 0.28776041428569943, 0.3965472977965534]
maxi score, test score, baseline:  -0.9759553633217992 -0.704 -0.704
probs:  [0.05728393844161393, 0.10727310094493533, 0.0568763580441335, 0.09494146256755506, 0.28621928874432995, 0.39740585125743216]
Printing some Q and Qe and total Qs values:  [[-0.008]
 [-0.037]
 [-0.011]
 [-0.011]
 [ 0.099]
 [-0.011]
 [-0.011]] [[25.725]
 [24.976]
 [40.565]
 [40.565]
 [25.514]
 [40.565]
 [40.565]] [[0.282]
 [0.234]
 [0.648]
 [0.648]
 [0.383]
 [0.648]
 [0.648]]
siam score:  -0.8328651
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
actions average: 
K:  4  action  0 :  tensor([0.2476, 0.0310, 0.1127, 0.1271, 0.1722, 0.1498, 0.1596],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0505, 0.6891, 0.0518, 0.0547, 0.0377, 0.0426, 0.0736],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2336, 0.0006, 0.1327, 0.1113, 0.1975, 0.1675, 0.1569],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1990, 0.0370, 0.1197, 0.1205, 0.1785, 0.1896, 0.1558],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1664, 0.0310, 0.1039, 0.1163, 0.2834, 0.1612, 0.1377],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1930, 0.0509, 0.1116, 0.1150, 0.1466, 0.2330, 0.1499],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2128, 0.0091, 0.1168, 0.1360, 0.2032, 0.1578, 0.1644],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  40.20888328552246
maxi score, test score, baseline:  -0.9760379310344827 -0.704 -0.704
probs:  [0.05700450673751685, 0.10689957691995607, 0.05700450673751685, 0.0952851329789492, 0.2855039111936435, 0.39830236543241765]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.079]
 [0.12 ]
 [0.137]
 [0.156]
 [0.155]
 [0.15 ]
 [0.113]] [[36.423]
 [45.452]
 [37.991]
 [35.626]
 [35.386]
 [35.822]
 [41.924]] [[0.931]
 [1.407]
 [1.066]
 [0.97 ]
 [0.957]
 [0.974]
 [1.23 ]]
siam score:  -0.8319939
maxi score, test score, baseline:  -0.9760379310344827 -0.704 -0.704
probs:  [0.05700450673751685, 0.10689957691995607, 0.05700450673751685, 0.0952851329789492, 0.2855039111936435, 0.39830236543241765]
printing an ep nov before normalisation:  61.15924785350502
maxi score, test score, baseline:  -0.9760379310344827 -0.704 -0.704
probs:  [0.05700450673751685, 0.10689957691995607, 0.05700450673751685, 0.0952851329789492, 0.2855039111936435, 0.39830236543241765]
maxi score, test score, baseline:  -0.9760379310344827 -0.704 -0.704
printing an ep nov before normalisation:  47.9114932991449
printing an ep nov before normalisation:  4.15372214564627
maxi score, test score, baseline:  -0.9760379310344827 -0.704 -0.704
probs:  [0.057151928304681986, 0.10580222406922316, 0.057151928304681986, 0.0956617715015329, 0.28489833814093224, 0.3993338096789477]
from probs:  [0.057151928304681986, 0.10580222406922316, 0.057151928304681986, 0.0956617715015329, 0.28489833814093224, 0.3993338096789477]
maxi score, test score, baseline:  -0.9761199312714777 -0.704 -0.704
probs:  [0.05727134631386103, 0.10602353358606477, 0.05727134631386103, 0.09586184320297658, 0.2834020489212721, 0.4001698816619646]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.69406843185425
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
line 256 mcts: sample exp_bonus 0.009904789406063841
maxi score, test score, baseline:  -0.9761199312714777 -0.704 -0.704
probs:  [0.057314498343135276, 0.10534888496407255, 0.057314498343135276, 0.09593413966794864, 0.28361598008667355, 0.40047199859503485]
maxi score, test score, baseline:  -0.9761199312714777 -0.704 -0.704
probs:  [0.057314498343135276, 0.10534888496407255, 0.057314498343135276, 0.09593413966794864, 0.28361598008667355, 0.40047199859503485]
actor:  1 policy actor:  1  step number:  52 total reward:  0.24666666666666603  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9761199312714777 -0.704 -0.704
probs:  [0.054193060230692584, 0.09960532319236054, 0.054193060230692584, 0.09070451443469552, 0.32268592423846915, 0.3786181176730897]
printing an ep nov before normalisation:  90.28053301021512
Printing some Q and Qe and total Qs values:  [[0.114]
 [0.474]
 [0.114]
 [0.114]
 [0.069]
 [0.025]
 [0.114]] [[28.616]
 [27.51 ]
 [28.616]
 [28.616]
 [29.694]
 [29.989]
 [28.616]] [[0.653]
 [0.992]
 [0.653]
 [0.653]
 [0.627]
 [0.589]
 [0.653]]
maxi score, test score, baseline:  -0.9762013698630136 -0.704 -0.704
probs:  [0.054193060230692584, 0.09960532319236054, 0.054193060230692584, 0.09070451443469552, 0.32268592423846915, 0.3786181176730897]
printing an ep nov before normalisation:  0.7263396657958765
actor:  1 policy actor:  1  step number:  51 total reward:  0.25333333333333297  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9762013698630136 -0.704 -0.704
probs:  [0.05056305774447435, 0.09292598413131885, 0.11765724897979864, 0.0846228456926416, 0.3010271987399427, 0.3532036647118238]
line 256 mcts: sample exp_bonus 64.5739233192481
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.014]
 [ 0.011]
 [-0.01 ]
 [ 0.149]
 [-0.01 ]
 [ 0.035]
 [ 0.317]] [[60.841]
 [59.635]
 [62.331]
 [61.493]
 [62.196]
 [64.438]
 [29.334]] [[0.292]
 [0.31 ]
 [0.304]
 [0.457]
 [0.303]
 [0.36 ]
 [0.455]]
printing an ep nov before normalisation:  52.05919813189146
maxi score, test score, baseline:  -0.9762013698630136 -0.704 -0.704
probs:  [0.050514239495154434, 0.09314332221230075, 0.11782164250755588, 0.08485795908587267, 0.3007990349066283, 0.3528638017924881]
maxi score, test score, baseline:  -0.9762013698630136 -0.704 -0.704
probs:  [0.05062482476695198, 0.09334747743122863, 0.1180799661246314, 0.08504392810560787, 0.2992657655264195, 0.3536380380451607]
printing an ep nov before normalisation:  59.150335986966155
printing an ep nov before normalisation:  39.65373315927167
siam score:  -0.84631294
maxi score, test score, baseline:  -0.9762822525597269 -0.704 -0.704
probs:  [0.05054172944192394, 0.09348198405297407, 0.1173359946239645, 0.08513614174258517, 0.30044908858561525, 0.35305506155293703]
printing an ep nov before normalisation:  44.5251989364624
maxi score, test score, baseline:  -0.9762822525597269 -0.704 -0.704
probs:  [0.05054172944192394, 0.09348198405297407, 0.1173359946239645, 0.08513614174258517, 0.30044908858561525, 0.35305506155293703]
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.352]
 [0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]] [[65.592]
 [71.008]
 [65.592]
 [65.592]
 [65.592]
 [65.592]
 [65.592]] [[0.819]
 [0.946]
 [0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.819]]
maxi score, test score, baseline:  -0.9762822525597269 -0.704 -0.704
probs:  [0.05054172944192394, 0.09348198405297407, 0.1173359946239645, 0.08513614174258517, 0.30044908858561525, 0.35305506155293703]
maxi score, test score, baseline:  -0.9762822525597269 -0.704 -0.704
probs:  [0.05054172944192394, 0.09348198405297407, 0.1173359946239645, 0.08513614174258517, 0.30044908858561525, 0.35305506155293703]
printing an ep nov before normalisation:  57.64291329689891
maxi score, test score, baseline:  -0.9762822525597269 -0.704 -0.704
probs:  [0.05057494092791036, 0.09288516634394489, 0.11741321330484242, 0.08519214565004853, 0.30064695076172726, 0.3532875830115266]
printing an ep nov before normalisation:  49.62206777434442
maxi score, test score, baseline:  -0.9762822525597269 -0.704 -0.704
probs:  [0.05057494092791036, 0.09288516634394489, 0.11741321330484242, 0.08519214565004853, 0.30064695076172726, 0.3532875830115266]
printing an ep nov before normalisation:  28.66009438548005
printing an ep nov before normalisation:  32.70336017087509
maxi score, test score, baseline:  -0.9763625850340135 -0.704 -0.704
probs:  [0.050684848434129945, 0.0930872636466262, 0.11766875479588941, 0.08537748062152097, 0.29912458107697376, 0.3540570714248596]
maxi score, test score, baseline:  -0.9763625850340135 -0.704 -0.704
probs:  [0.05061832016134811, 0.09310705127512661, 0.11773858140336728, 0.08538157393479977, 0.29956378577553144, 0.35359068744982675]
UNIT TEST: sample policy line 217 mcts : [0.    0.735 0.224 0.    0.02  0.02  0.   ]
printing an ep nov before normalisation:  51.69959683527911
printing an ep nov before normalisation:  58.65523397572957
maxi score, test score, baseline:  -0.9763625850340135 -0.704 -0.704
probs:  [0.050551918904688374, 0.09312680112498448, 0.1178082746975254, 0.08538565943310372, 0.30000215194220964, 0.35312519389748837]
maxi score, test score, baseline:  -0.9763625850340135 -0.704 -0.704
probs:  [0.050551918904688374, 0.09312680112498448, 0.1178082746975254, 0.08538565943310372, 0.30000215194220964, 0.35312519389748837]
maxi score, test score, baseline:  -0.9763625850340135 -0.704 -0.704
maxi score, test score, baseline:  -0.9763625850340135 -0.704 -0.704
probs:  [0.05066079899292586, 0.09332762245038854, 0.11806239611671746, 0.08556976362138101, 0.29849193230919424, 0.3538874865093929]
printing an ep nov before normalisation:  35.88433698517029
printing an ep nov before normalisation:  53.30911335804988
printing an ep nov before normalisation:  48.940787148398215
maxi score, test score, baseline:  -0.9763625850340135 -0.704 -0.704
actor:  1 policy actor:  1  step number:  56 total reward:  0.19333333333333258  reward:  1.0 rdn_beta:  1.667
using another actor
maxi score, test score, baseline:  -0.9764423728813559 -0.704 -0.704
probs:  [0.048639356503236045, 0.08959922025510193, 0.11334443673028972, 0.08215172792047443, 0.3265303200838476, 0.33973493850705033]
printing an ep nov before normalisation:  44.33506965637207
maxi score, test score, baseline:  -0.9764423728813559 -0.704 -0.704
probs:  [0.0485718849041341, 0.08961132949753318, 0.11340268050873728, 0.0821493674441856, 0.3270027623103918, 0.3392619753350181]
from probs:  [0.04860235173211274, 0.0890391344555451, 0.11347392445096209, 0.08220095375448536, 0.3272083559063173, 0.3394752797005773]
Printing some Q and Qe and total Qs values:  [[0.199]
 [0.228]
 [0.199]
 [0.199]
 [0.199]
 [0.199]
 [0.199]] [[45.217]
 [51.665]
 [45.217]
 [45.217]
 [45.217]
 [45.217]
 [45.217]] [[1.828]
 [2.228]
 [1.828]
 [1.828]
 [1.828]
 [1.828]
 [1.828]]
maxi score, test score, baseline:  -0.9765216216216216 -0.704 -0.704
probs:  [0.04860235173211274, 0.0890391344555451, 0.11347392445096209, 0.08220095375448536, 0.3272083559063173, 0.3394752797005773]
printing an ep nov before normalisation:  34.27473783493042
printing an ep nov before normalisation:  33.30520152888084
maxi score, test score, baseline:  -0.9765216216216216 -0.704 -0.704
probs:  [0.04860235173211274, 0.0890391344555451, 0.11347392445096209, 0.08220095375448536, 0.3272083559063173, 0.3394752797005773]
Printing some Q and Qe and total Qs values:  [[0.316]
 [0.347]
 [0.317]
 [0.32 ]
 [0.317]
 [0.339]
 [0.328]] [[54.462]
 [62.997]
 [54.592]
 [56.361]
 [55.289]
 [59.98 ]
 [56.76 ]] [[0.316]
 [0.347]
 [0.317]
 [0.32 ]
 [0.317]
 [0.339]
 [0.328]]
Printing some Q and Qe and total Qs values:  [[0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]] [[68.901]
 [68.901]
 [68.901]
 [68.901]
 [68.901]
 [68.901]
 [68.901]] [[1.747]
 [1.747]
 [1.747]
 [1.747]
 [1.747]
 [1.747]
 [1.747]]
maxi score, test score, baseline:  -0.9766003367003366 -0.704 -0.704
probs:  [0.048677594654859026, 0.08869073028873685, 0.11386621988086323, 0.08244040606142924, 0.3263235605089055, 0.3400014886052062]
maxi score, test score, baseline:  -0.9766003367003366 -0.704 -0.704
probs:  [0.048677594654859026, 0.08869073028873685, 0.11386621988086323, 0.08244040606142924, 0.3263235605089055, 0.3400014886052062]
printing an ep nov before normalisation:  64.66436053864707
Printing some Q and Qe and total Qs values:  [[0.398]
 [0.44 ]
 [0.398]
 [0.398]
 [0.398]
 [0.398]
 [0.398]] [[68.005]
 [70.732]
 [68.005]
 [68.005]
 [68.005]
 [68.005]
 [68.005]] [[1.865]
 [1.997]
 [1.865]
 [1.865]
 [1.865]
 [1.865]
 [1.865]]
printing an ep nov before normalisation:  39.75775768339404
maxi score, test score, baseline:  -0.9766785234899329 -0.704 -0.704
probs:  [0.04878899840311809, 0.08889395958714265, 0.11412722403608515, 0.08262929158368063, 0.32477907916298737, 0.3407814472269861]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9766785234899329 -0.704 -0.704
probs:  [0.04889928899396668, 0.08909515819804394, 0.11438562021164668, 0.08281628974287548, 0.3232500304293682, 0.341553612424099]
siam score:  -0.8323027
maxi score, test score, baseline:  -0.9767561872909699 -0.704 -0.704
actor:  1 policy actor:  1  step number:  48 total reward:  0.2999999999999994  reward:  1.0 rdn_beta:  1.667
siam score:  -0.83034134
printing an ep nov before normalisation:  26.35127067565918
maxi score, test score, baseline:  -0.9768333333333333 -0.704 -0.704
probs:  [0.05024316247309504, 0.08729089161086849, 0.11060060489737863, 0.08090756709813367, 0.3509809716450059, 0.3199768022755182]
maxi score, test score, baseline:  -0.9768333333333333 -0.704 -0.704
printing an ep nov before normalisation:  44.41592216491699
maxi score, test score, baseline:  -0.9768333333333333 -0.704 -0.704
probs:  [0.05033277855218447, 0.08744676345693733, 0.10990620694915032, 0.0810520230741529, 0.3516084240414657, 0.31965380392610926]
maxi score, test score, baseline:  -0.9768333333333333 -0.704 -0.704
probs:  [0.05037751859608878, 0.08752458111318188, 0.11000404144721661, 0.08112414145709276, 0.3519216741641078, 0.31904804322231206]
Printing some Q and Qe and total Qs values:  [[0.036]
 [0.187]
 [0.057]
 [0.062]
 [0.063]
 [0.063]
 [0.041]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.036]
 [0.187]
 [0.057]
 [0.062]
 [0.063]
 [0.063]
 [0.041]]
printing an ep nov before normalisation:  87.53650881301307
printing an ep nov before normalisation:  60.22407824936273
printing an ep nov before normalisation:  57.469285311994824
line 256 mcts: sample exp_bonus 48.246283774450234
maxi score, test score, baseline:  -0.9768333333333333 -0.704 -0.704
maxi score, test score, baseline:  -0.9768333333333333 -0.704 -0.704
probs:  [0.05042207535349668, 0.0876020799739062, 0.11010147514678532, 0.08119596439272006, 0.3522336409952628, 0.31844476413782896]
maxi score, test score, baseline:  -0.9768333333333333 -0.704 -0.704
probs:  [0.050480729470249605, 0.08770409878069525, 0.11022973599705, 0.08012509939604985, 0.3526443113402812, 0.3188160250156741]
printing an ep nov before normalisation:  26.04485273361206
maxi score, test score, baseline:  -0.9768333333333333 -0.704 -0.704
probs:  [0.05036686427289523, 0.0878027468046944, 0.11045698591326271, 0.08018047789330533, 0.35184577723201477, 0.3193471478838276]
maxi score, test score, baseline:  -0.9768333333333333 -0.704 -0.704
probs:  [0.05041123500755961, 0.08788018417941747, 0.11055443348995633, 0.08025118261460934, 0.3521564404653705, 0.3187465242430868]
printing an ep nov before normalisation:  81.71609007059779
printing an ep nov before normalisation:  29.8904486672814
printing an ep nov before normalisation:  57.610940058237645
printing an ep nov before normalisation:  54.06037775814634
maxi score, test score, baseline:  -0.9768333333333333 -0.704 -0.704
probs:  [0.05041123500755961, 0.08788018417941747, 0.11055443348995633, 0.08025118261460934, 0.3521564404653705, 0.3187465242430868]
Printing some Q and Qe and total Qs values:  [[-0.013]
 [ 0.084]
 [ 0.084]
 [ 0.084]
 [ 0.084]
 [ 0.084]
 [ 0.084]] [[56.931]
 [36.069]
 [36.069]
 [36.069]
 [36.069]
 [36.069]
 [36.069]] [[1.193]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]]
maxi score, test score, baseline:  -0.9768333333333333 -0.704 -0.704
maxi score, test score, baseline:  -0.9768333333333333 -0.704 -0.704
probs:  [0.05041123500755961, 0.08788018417941747, 0.11055443348995633, 0.08025118261460934, 0.3521564404653705, 0.3187465242430868]
Printing some Q and Qe and total Qs values:  [[ 0.206]
 [ 0.513]
 [ 0.175]
 [ 0.215]
 [ 0.215]
 [-0.009]
 [-0.038]] [[50.247]
 [43.159]
 [49.788]
 [53.04 ]
 [53.04 ]
 [50.536]
 [50.811]] [[1.717]
 [1.629]
 [1.66 ]
 [1.882]
 [1.882]
 [1.519]
 [1.504]]
line 256 mcts: sample exp_bonus 43.19163445715957
printing an ep nov before normalisation:  50.38463852471618
Printing some Q and Qe and total Qs values:  [[0.222]
 [0.664]
 [0.464]
 [0.403]
 [0.403]
 [0.403]
 [0.208]] [[25.532]
 [28.89 ]
 [21.191]
 [27.644]
 [27.644]
 [27.644]
 [26.515]] [[1.2  ]
 [1.775]
 [1.271]
 [1.465]
 [1.465]
 [1.465]
 [1.226]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  42.18371676396545
printing an ep nov before normalisation:  44.44017803357387
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.004]
 [0.605]
 [0.003]
 [0.003]] [[ 0.029]
 [ 0.034]
 [ 0.029]
 [ 0.029]
 [42.277]
 [ 0.029]
 [ 0.029]] [[0.004]
 [0.005]
 [0.005]
 [0.005]
 [2.317]
 [0.004]
 [0.004]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
maxi score, test score, baseline:  -0.9769860927152317 -0.704 -0.704
Printing some Q and Qe and total Qs values:  [[-0.114]
 [-0.11 ]
 [-0.11 ]
 [-0.11 ]
 [-0.11 ]
 [-0.11 ]
 [-0.11 ]] [[24.184]
 [19.006]
 [19.006]
 [19.006]
 [19.006]
 [19.006]
 [19.006]] [[1.003]
 [0.767]
 [0.767]
 [0.767]
 [0.767]
 [0.767]
 [0.767]]
line 256 mcts: sample exp_bonus 66.423169317733
using another actor
printing an ep nov before normalisation:  44.13477891679559
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.811038970947266
maxi score, test score, baseline:  -0.9769860927152317 -0.704 -0.704
probs:  [0.05324248864574565, 0.08667268837345529, 0.10858009071247289, 0.08017305012107177, 0.37200192423625866, 0.2993297579109956]
actor:  1 policy actor:  1  step number:  44 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9769860927152317 -0.704 -0.704
Printing some Q and Qe and total Qs values:  [[0.749]
 [0.753]
 [0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.483]] [[ 1.072]
 [ 1.343]
 [48.428]
 [48.428]
 [48.428]
 [48.428]
 [48.428]] [[0.749]
 [0.753]
 [0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.483]]
Printing some Q and Qe and total Qs values:  [[0.164]
 [0.931]
 [0.164]
 [0.164]
 [0.164]
 [0.164]
 [0.164]] [[52.601]
 [52.463]
 [52.601]
 [52.601]
 [52.601]
 [52.601]
 [52.601]] [[1.624]
 [2.385]
 [1.624]
 [1.624]
 [1.624]
 [1.624]
 [1.624]]
maxi score, test score, baseline:  -0.9770617161716171 -0.704 -0.704
probs:  [0.05197145852551017, 0.08485189686088522, 0.10639903081651204, 0.07845914550132332, 0.3631010966434496, 0.3152173716523198]
printing an ep nov before normalisation:  52.61096719128155
maxi score, test score, baseline:  -0.9770617161716171 -0.704 -0.704
probs:  [0.05197145852551017, 0.08485189686088522, 0.10639903081651204, 0.07845914550132332, 0.3631010966434496, 0.3152173716523198]
printing an ep nov before normalisation:  39.81134150091386
maxi score, test score, baseline:  -0.9770617161716171 -0.704 -0.704
probs:  [0.051822837388173164, 0.08485892782797455, 0.10650806336613325, 0.07843591393991346, 0.3620593319592572, 0.31631492551854834]
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.022]
 [0.042]
 [0.041]
 [0.041]
 [0.044]
 [0.043]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.024]
 [0.022]
 [0.042]
 [0.041]
 [0.041]
 [0.044]
 [0.043]]
printing an ep nov before normalisation:  39.39664602279663
printing an ep nov before normalisation:  51.658815248574264
maxi score, test score, baseline:  -0.9770617161716171 -0.704 -0.704
probs:  [0.05152754417937427, 0.0848728975557668, 0.10672469857710036, 0.07838975548333696, 0.35998946463136194, 0.31849563957305965]
maxi score, test score, baseline:  -0.9770617161716171 -0.704 -0.704
probs:  [0.05152754417937427, 0.0848728975557668, 0.10672469857710036, 0.07838975548333696, 0.35998946463136194, 0.31849563957305965]
maxi score, test score, baseline:  -0.9770617161716171 -0.704 -0.704
probs:  [0.05152754417937427, 0.0848728975557668, 0.10672469857710036, 0.07838975548333696, 0.35998946463136194, 0.31849563957305965]
actor:  1 policy actor:  1  step number:  60 total reward:  0.19333333333333313  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9770617161716171 -0.704 -0.704
maxi score, test score, baseline:  -0.9770617161716171 -0.704 -0.704
probs:  [0.050754686382422136, 0.0838443470801859, 0.10472396542018472, 0.07741091784011739, 0.35457690572755235, 0.32868917754953747]
using explorer policy with actor:  1
printing an ep nov before normalisation:  9.01835951332373
printing an ep nov before normalisation:  57.900352478027344
maxi score, test score, baseline:  -0.9770617161716171 -0.704 -0.704
probs:  [0.050794719840337045, 0.08391054915411134, 0.10401647243052839, 0.07747203210305101, 0.3548572138460667, 0.3289490126259054]
from probs:  [0.050794719840337045, 0.08391054915411134, 0.10401647243052839, 0.07747203210305101, 0.3548572138460667, 0.3289490126259054]
maxi score, test score, baseline:  -0.9770617161716171 -0.704 -0.704
line 256 mcts: sample exp_bonus 38.81268111424174
maxi score, test score, baseline:  -0.9770617161716171 -0.704 -0.704
maxi score, test score, baseline:  -0.9770617161716171 -0.704 -0.704
probs:  [0.05064771008108278, 0.08391300396671533, 0.10410967306700566, 0.07744542739293186, 0.3538267645025211, 0.3300574209897432]
printing an ep nov before normalisation:  51.76475524902344
printing an ep nov before normalisation:  15.074283549111556
maxi score, test score, baseline:  -0.9770617161716171 -0.704 -0.704
probs:  [0.0506871268431095, 0.08397837784215634, 0.10341112412307311, 0.07750575457859406, 0.3541027537453896, 0.3303148628676773]
maxi score, test score, baseline:  -0.9770617161716171 -0.704 -0.704
probs:  [0.050586061524858464, 0.08405635010109727, 0.10359360407421284, 0.07754891760831086, 0.3533940023913644, 0.3308210643001561]
maxi score, test score, baseline:  -0.9770617161716171 -0.704 -0.704
probs:  [0.050586061524858464, 0.08405635010109727, 0.10359360407421284, 0.07754891760831086, 0.3533940023913644, 0.3308210643001561]
maxi score, test score, baseline:  -0.9770617161716171 -0.704 -0.704
probs:  [0.050586061524858464, 0.08405635010109727, 0.10359360407421284, 0.07754891760831086, 0.3533940023913644, 0.3308210643001561]
printing an ep nov before normalisation:  48.103617534333516
maxi score, test score, baseline:  -0.9771368421052631 -0.704 -0.704
probs:  [0.050586061524858464, 0.08405635010109727, 0.10359360407421284, 0.07754891760831086, 0.3533940023913644, 0.3308210643001561]
maxi score, test score, baseline:  -0.9771368421052631 -0.704 -0.704
probs:  [0.050586061524858464, 0.08405635010109727, 0.10359360407421284, 0.07754891760831086, 0.3533940023913644, 0.3308210643001561]
maxi score, test score, baseline:  -0.9771368421052631 -0.704 -0.704
probs:  [0.050586061524858464, 0.08405635010109727, 0.10359360407421284, 0.07754891760831086, 0.3533940023913644, 0.3308210643001561]
maxi score, test score, baseline:  -0.9771368421052631 -0.704 -0.704
probs:  [0.050586061524858464, 0.08405635010109727, 0.10359360407421284, 0.07754891760831086, 0.3533940023913644, 0.3308210643001561]
siam score:  -0.83169615
printing an ep nov before normalisation:  37.8087081420972
actor:  1 policy actor:  1  step number:  55 total reward:  0.18666666666666587  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  44.110739321128676
printing an ep nov before normalisation:  27.573741741025803
from probs:  [0.04864981832481825, 0.11918222310209822, 0.09962307490641756, 0.07457791299238163, 0.3398368102583305, 0.3181301604159539]
printing an ep nov before normalisation:  20.689858967932516
maxi score, test score, baseline:  -0.977211475409836 -0.704 -0.704
probs:  [0.04861411010965887, 0.11931087507054075, 0.09978515930657979, 0.074782807061821, 0.33958829756985986, 0.31791875088153976]
siam score:  -0.831932
line 256 mcts: sample exp_bonus 43.3338294396746
printing an ep nov before normalisation:  39.461977205834486
siam score:  -0.83051485
maxi score, test score, baseline:  -0.977211475409836 -0.704 -0.704
maxi score, test score, baseline:  -0.977211475409836 -0.704 -0.704
probs:  [0.048579200458841953, 0.11943664991745459, 0.09994361891558787, 0.07498311896166418, 0.33934534252045856, 0.31771206922599293]
maxi score, test score, baseline:  -0.977211475409836 -0.704 -0.704
probs:  [0.04843116989259595, 0.11959013807413267, 0.10001415864752712, 0.07494744462398681, 0.33830781273448485, 0.3187092760272726]
using explorer policy with actor:  1
printing an ep nov before normalisation:  52.09220074891374
maxi score, test score, baseline:  -0.977211475409836 -0.704 -0.704
probs:  [0.04847267531392255, 0.11969279481502869, 0.10009999253384286, 0.07501173712656722, 0.3385984271532454, 0.3181243730573933]
printing an ep nov before normalisation:  13.523990947452953
maxi score, test score, baseline:  -0.977211475409836 -0.704 -0.704
probs:  [0.04847267531392255, 0.11969279481502869, 0.10009999253384286, 0.07501173712656722, 0.3385984271532454, 0.3181243730573933]
printing an ep nov before normalisation:  35.442453260831996
printing an ep nov before normalisation:  25.851394200473504
maxi score, test score, baseline:  -0.9773592833876221 -0.704 -0.704
probs:  [0.04847267531392255, 0.11969279481502869, 0.10009999253384286, 0.07501173712656722, 0.3385984271532454, 0.3181243730573933]
printing an ep nov before normalisation:  48.48499244067124
actor:  1 policy actor:  1  step number:  51 total reward:  0.1866666666666661  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9773592833876221 -0.704 -0.704
probs:  [0.0504578151919148, 0.11762617827714966, 0.09914802257201259, 0.07548705345015055, 0.3525120685584742, 0.304768861950298]
maxi score, test score, baseline:  -0.9773592833876221 -0.704 -0.704
probs:  [0.0504578151919148, 0.11762617827714966, 0.09914802257201259, 0.07548705345015055, 0.3525120685584742, 0.304768861950298]
maxi score, test score, baseline:  -0.9774324675324675 -0.704 -0.704
probs:  [0.0504578151919148, 0.11762617827714966, 0.09914802257201259, 0.07548705345015055, 0.3525120685584742, 0.304768861950298]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.2325237033681
printing an ep nov before normalisation:  23.84351516802748
maxi score, test score, baseline:  -0.9775051779935274 -0.704 -0.704
probs:  [0.05021427280545304, 0.11801790996088604, 0.09936498904419157, 0.0754802359938779, 0.35080482758618753, 0.3061177646094038]
printing an ep nov before normalisation:  53.41639041900635
maxi score, test score, baseline:  -0.9775051779935274 -0.704 -0.704
printing an ep nov before normalisation:  53.16179484218773
Printing some Q and Qe and total Qs values:  [[ 0.002]
 [-0.012]
 [ 0.003]
 [-0.   ]
 [-0.001]
 [-0.001]
 [-0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.002]
 [-0.012]
 [ 0.003]
 [-0.   ]
 [-0.001]
 [-0.001]
 [-0.001]]
maxi score, test score, baseline:  -0.9775051779935274 -0.704 -0.704
probs:  [0.050113753349161114, 0.11826135927680842, 0.09951381183895522, 0.07550789111200905, 0.3501000184080269, 0.3065031660150393]
printing an ep nov before normalisation:  36.829085538555816
printing an ep nov before normalisation:  65.89026788887709
maxi score, test score, baseline:  -0.9775051779935274 -0.704 -0.704
probs:  [0.05015418588704292, 0.11835692089887194, 0.09959420733585804, 0.0755688666394705, 0.3503831295820055, 0.3059426896567511]
printing an ep nov before normalisation:  55.0437288324694
Printing some Q and Qe and total Qs values:  [[0.682]
 [0.673]
 [0.681]
 [0.684]
 [0.685]
 [0.684]
 [0.685]] [[13.079]
 [21.979]
 [16.577]
 [12.239]
 [13.958]
 [12.855]
 [12.774]] [[0.682]
 [0.673]
 [0.681]
 [0.684]
 [0.685]
 [0.684]
 [0.685]]
printing an ep nov before normalisation:  29.594715000761383
maxi score, test score, baseline:  -0.9775051779935274 -0.704 -0.704
probs:  [0.05018974596624407, 0.11844096654693641, 0.09895457918629304, 0.07562249410634382, 0.3506321234937614, 0.3061600907004212]
maxi score, test score, baseline:  -0.9775774193548387 -0.704 -0.704
probs:  [0.05018974596624407, 0.11844096654693641, 0.09895457918629304, 0.07562249410634382, 0.3506321234937614, 0.3061600907004212]
maxi score, test score, baseline:  -0.9775774193548387 -0.704 -0.704
probs:  [0.05018974596624407, 0.11844096654693641, 0.09895457918629304, 0.07562249410634382, 0.3506321234937614, 0.3061600907004212]
maxi score, test score, baseline:  -0.9775774193548387 -0.704 -0.704
probs:  [0.05015431826244805, 0.11856101564227506, 0.09910540594118411, 0.07581017258536153, 0.3503854396361244, 0.3059836479326069]
Printing some Q and Qe and total Qs values:  [[0.032]
 [0.086]
 [0.032]
 [0.032]
 [0.032]
 [0.032]
 [0.032]] [[26.475]
 [35.244]
 [26.475]
 [26.475]
 [26.475]
 [26.475]
 [26.475]] [[0.687]
 [1.167]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]]
maxi score, test score, baseline:  -0.9775774193548387 -0.704 -0.704
printing an ep nov before normalisation:  33.82693002970879
maxi score, test score, baseline:  -0.9775774193548387 -0.704 -0.704
probs:  [0.05005435386859949, 0.11880541783193503, 0.09925186652425488, 0.07583936248836665, 0.34968452678306566, 0.3063644725037784]
printing an ep nov before normalisation:  45.12844391710967
maxi score, test score, baseline:  -0.9775774193548387 -0.704 -0.704
probs:  [0.05005435386859949, 0.11880541783193503, 0.09925186652425488, 0.07583936248836665, 0.34968452678306566, 0.3063644725037784]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.022]
 [0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]] [[39.39 ]
 [42.094]
 [39.39 ]
 [39.39 ]
 [39.39 ]
 [39.39 ]
 [39.39 ]] [[1.736]
 [1.862]
 [1.736]
 [1.736]
 [1.736]
 [1.736]
 [1.736]]
maxi score, test score, baseline:  -0.9775774193548387 -0.704 -0.704
probs:  [0.050078261032900286, 0.11886224979334999, 0.0992993343148112, 0.07539710214475526, 0.34985192645619145, 0.3065111262579918]
printing an ep nov before normalisation:  62.22905423853527
printing an ep nov before normalisation:  35.08680635401643
maxi score, test score, baseline:  -0.9775774193548387 -0.704 -0.704
probs:  [0.050118426636623865, 0.11895773121524958, 0.09937908331561825, 0.0754576290627412, 0.350133168884178, 0.3059539608855892]
printing an ep nov before normalisation:  51.38355731964111
actions average: 
K:  0  action  0 :  tensor([0.2351, 0.0175, 0.1374, 0.1302, 0.1640, 0.1579, 0.1580],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0050, 0.9479, 0.0060, 0.0152, 0.0033, 0.0045, 0.0181],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1866, 0.0393, 0.1295, 0.1417, 0.1808, 0.1701, 0.1521],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1795, 0.0249, 0.1406, 0.1501, 0.1877, 0.1618, 0.1553],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1960, 0.0218, 0.1018, 0.1075, 0.3044, 0.1405, 0.1281],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1659, 0.0188, 0.1804, 0.1287, 0.1553, 0.2001, 0.1508],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1756, 0.0163, 0.1208, 0.1898, 0.1800, 0.1691, 0.1484],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9775774193548387 -0.704 -0.704
probs:  [0.05019827229426295, 0.1191475398130825, 0.09953761725573576, 0.07557795120714897, 0.35069225388569164, 0.3048463655440781]
actor:  1 policy actor:  1  step number:  55 total reward:  0.2799999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9775774193548387 -0.704 -0.704
probs:  [0.04773285644901598, 0.11328676868916428, 0.1438474127569651, 0.07186273199387744, 0.33342923592919804, 0.28984099418177917]
maxi score, test score, baseline:  -0.9775774193548387 -0.704 -0.704
probs:  [0.04773285644901598, 0.11328676868916428, 0.1438474127569651, 0.07186273199387744, 0.33342923592919804, 0.28984099418177917]
actor:  1 policy actor:  1  step number:  64 total reward:  0.206666666666666  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  30.319171030506826
printing an ep nov before normalisation:  23.85984956723746
siam score:  -0.8400814
printing an ep nov before normalisation:  28.0875290507787
maxi score, test score, baseline:  -0.9775774193548387 -0.704 -0.704
probs:  [0.047148957211062284, 0.11189872707351786, 0.1420844872000754, 0.07098283431996479, 0.32934073173262995, 0.2985442624627497]
printing an ep nov before normalisation:  12.21981500761558
Printing some Q and Qe and total Qs values:  [[-0.073]
 [-0.067]
 [-0.067]
 [-0.056]
 [-0.057]
 [-0.067]
 [-0.057]] [[13.051]
 [15.054]
 [12.453]
 [12.44 ]
 [12.463]
 [12.567]
 [12.449]] [[0.615]
 [0.726]
 [0.589]
 [0.599]
 [0.599]
 [0.595]
 [0.598]]
maxi score, test score, baseline:  -0.9775774193548387 -0.704 -0.704
probs:  [0.04705750888544578, 0.11213406566295232, 0.1413756585637274, 0.07101167366781284, 0.3286995070635683, 0.29972158615649336]
Printing some Q and Qe and total Qs values:  [[0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]] [[82.393]
 [82.393]
 [82.393]
 [82.393]
 [82.393]
 [82.393]
 [82.393]] [[1.802]
 [1.802]
 [1.802]
 [1.802]
 [1.802]
 [1.802]
 [1.802]]
maxi score, test score, baseline:  -0.9775774193548387 -0.704 -0.704
probs:  [0.04705750888544578, 0.11213406566295232, 0.1413756585637274, 0.07101167366781284, 0.3286995070635683, 0.29972158615649336]
printing an ep nov before normalisation:  48.42170238494873
printing an ep nov before normalisation:  27.29097604751587
maxi score, test score, baseline:  -0.9775774193548387 -0.704 -0.704
probs:  [0.04705750888544578, 0.11213406566295232, 0.1413756585637274, 0.07101167366781284, 0.3286995070635683, 0.29972158615649336]
Printing some Q and Qe and total Qs values:  [[0.147]
 [0.147]
 [0.147]
 [0.147]
 [0.147]
 [0.147]
 [0.147]] [[42.846]
 [42.846]
 [42.846]
 [42.846]
 [42.846]
 [42.846]
 [42.846]] [[1.603]
 [1.603]
 [1.603]
 [1.603]
 [1.603]
 [1.603]
 [1.603]]
from probs:  [0.04705750888544578, 0.11213406566295232, 0.1413756585637274, 0.07101167366781284, 0.3286995070635683, 0.29972158615649336]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
printing an ep nov before normalisation:  43.60945495286694
maxi score, test score, baseline:  -0.977649196141479 -0.704 -0.704
maxi score, test score, baseline:  -0.977649196141479 -0.704 -0.704
probs:  [0.0470290960996554, 0.1122552565560769, 0.14145379710624836, 0.07119340899310435, 0.3285018490026211, 0.2995665922422938]
Printing some Q and Qe and total Qs values:  [[ 0.104]
 [-0.089]
 [-0.088]
 [-0.09 ]
 [ 0.104]
 [ 0.104]
 [-0.09 ]] [[79.761]
 [81.641]
 [87.815]
 [88.985]
 [79.761]
 [79.761]
 [88.488]] [[1.541]
 [1.386]
 [1.513]
 [1.535]
 [1.541]
 [1.541]
 [1.525]]
printing an ep nov before normalisation:  15.376145839691162
maxi score, test score, baseline:  -0.977649196141479 -0.704 -0.704
probs:  [0.0470290960996554, 0.1122552565560769, 0.14145379710624836, 0.07119340899310435, 0.3285018490026211, 0.2995665922422938]
printing an ep nov before normalisation:  21.768008840533213
printing an ep nov before normalisation:  34.04459184015789
maxi score, test score, baseline:  -0.977649196141479 -0.704 -0.704
probs:  [0.0470290960996554, 0.1122552565560769, 0.14145379710624836, 0.07119340899310435, 0.3285018490026211, 0.2995665922422938]
maxi score, test score, baseline:  -0.977649196141479 -0.704 -0.704
probs:  [0.0470290960996554, 0.1122552565560769, 0.14145379710624836, 0.07119340899310435, 0.3285018490026211, 0.2995665922422938]
maxi score, test score, baseline:  -0.977649196141479 -0.704 -0.704
probs:  [0.04674570076180575, 0.1124805577620703, 0.14190681670865204, 0.07109847028626323, 0.32651572647789856, 0.30125272800331016]
maxi score, test score, baseline:  -0.977649196141479 -0.704 -0.704
probs:  [0.046782309680278925, 0.11256879410586013, 0.14201816410881954, 0.07115420559897002, 0.32677206366182504, 0.3007044628442463]
maxi score, test score, baseline:  -0.977649196141479 -0.704 -0.704
probs:  [0.046782309680278925, 0.11256879410586013, 0.14201816410881954, 0.07115420559897002, 0.32677206366182504, 0.3007044628442463]
printing an ep nov before normalisation:  46.620550221395874
maxi score, test score, baseline:  -0.977649196141479 -0.704 -0.704
probs:  [0.046782309680278925, 0.11256879410586013, 0.14201816410881954, 0.07115420559897002, 0.32677206366182504, 0.3007044628442463]
printing an ep nov before normalisation:  16.76411633804674
maxi score, test score, baseline:  -0.9777205128205128 -0.704 -0.704
probs:  [0.04669240807597904, 0.11280381844850437, 0.1413087009906406, 0.07118467919680094, 0.3261416854435898, 0.30186870784448533]
maxi score, test score, baseline:  -0.9777205128205128 -0.704 -0.704
probs:  [0.04669240807597904, 0.11280381844850437, 0.1413087009906406, 0.07118467919680094, 0.3261416854435898, 0.30186870784448533]
siam score:  -0.8384764
maxi score, test score, baseline:  -0.9777205128205128 -0.704 -0.704
maxi score, test score, baseline:  -0.9777205128205128 -0.704 -0.704
maxi score, test score, baseline:  -0.9777205128205128 -0.704 -0.704
probs:  [0.04669240807597904, 0.11280381844850437, 0.1413087009906406, 0.07118467919680094, 0.3261416854435898, 0.30186870784448533]
printing an ep nov before normalisation:  39.329833984375
maxi score, test score, baseline:  -0.9777205128205128 -0.704 -0.704
maxi score, test score, baseline:  -0.9777205128205128 -0.704 -0.704
probs:  [0.04669240807597904, 0.11280381844850437, 0.1413087009906406, 0.07118467919680094, 0.3261416854435898, 0.30186870784448533]
printing an ep nov before normalisation:  35.85717619580464
Printing some Q and Qe and total Qs values:  [[0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.127]] [[22.111]
 [22.111]
 [22.111]
 [22.111]
 [22.111]
 [22.111]
 [28.291]] [[0.685]
 [0.685]
 [0.685]
 [0.685]
 [0.685]
 [0.685]
 [0.964]]
line 256 mcts: sample exp_bonus 19.305958880609406
maxi score, test score, baseline:  -0.9777205128205128 -0.704 -0.704
printing an ep nov before normalisation:  40.109639256644975
maxi score, test score, baseline:  -0.9777913738019169 -0.704 -0.704
probs:  [0.04666471827449112, 0.11292322357348857, 0.14138641762293294, 0.07136495246182999, 0.3259490831748942, 0.3017116048923631]
actor:  1 policy actor:  1  step number:  63 total reward:  0.2666666666666665  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9777913738019169 -0.704 -0.704
probs:  [0.04460299153813705, 0.10792570158501863, 0.17940052667730744, 0.06820880275402329, 0.3115127683194023, 0.2883492091261114]
maxi score, test score, baseline:  -0.9777913738019169 -0.704 -0.704
probs:  [0.04460299153813705, 0.10792570158501863, 0.17940052667730744, 0.06820880275402329, 0.3115127683194023, 0.2883492091261114]
UNIT TEST: sample policy line 217 mcts : [0.02  0.551 0.    0.    0.    0.102 0.327]
maxi score, test score, baseline:  -0.9778617834394904 -0.704 -0.704
printing an ep nov before normalisation:  72.08683095124785
maxi score, test score, baseline:  -0.9778617834394904 -0.704 -0.704
probs:  [0.044340664631436974, 0.1081507969254962, 0.18017579461522, 0.0676953620390938, 0.3096742796629317, 0.28996310212582127]
maxi score, test score, baseline:  -0.9778617834394904 -0.704 -0.704
probs:  [0.04437166713531809, 0.10752581371247842, 0.18030205776632505, 0.06774274310123918, 0.30989135959686487, 0.2901663586877744]
maxi score, test score, baseline:  -0.9778617834394904 -0.704 -0.704
printing an ep nov before normalisation:  64.58184841380822
maxi score, test score, baseline:  -0.9778617834394904 -0.704 -0.704
probs:  [0.044434309555769935, 0.10767788353066125, 0.17914229412038232, 0.0678384793820166, 0.3103299826366441, 0.29057705077452595]
printing an ep nov before normalisation:  27.194485968844344
maxi score, test score, baseline:  -0.9778617834394904 -0.704 -0.704
probs:  [0.044434309555769935, 0.10767788353066125, 0.17914229412038232, 0.0678384793820166, 0.3103299826366441, 0.29057705077452595]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9778617834394904 -0.704 -0.704
probs:  [0.044434309555769935, 0.10767788353066125, 0.17914229412038232, 0.0678384793820166, 0.3103299826366441, 0.29057705077452595]
Printing some Q and Qe and total Qs values:  [[0.238]
 [0.346]
 [0.261]
 [0.261]
 [0.261]
 [0.261]
 [0.232]] [[38.125]
 [37.296]
 [37.579]
 [37.579]
 [37.579]
 [37.579]
 [37.412]] [[1.516]
 [1.571]
 [1.504]
 [1.504]
 [1.504]
 [1.504]
 [1.465]]
maxi score, test score, baseline:  -0.977931746031746 -0.704 -0.704
probs:  [0.044434309555769935, 0.10767788353066125, 0.17914229412038232, 0.0678384793820166, 0.3103299826366441, 0.29057705077452595]
maxi score, test score, baseline:  -0.977931746031746 -0.704 -0.704
probs:  [0.04429471187270676, 0.10776505504610208, 0.17948571186120924, 0.067782800815151, 0.30935168607015895, 0.291320034334672]
maxi score, test score, baseline:  -0.977931746031746 -0.704 -0.704
probs:  [0.04429471187270676, 0.10776505504610208, 0.17948571186120924, 0.067782800815151, 0.30935168607015895, 0.291320034334672]
printing an ep nov before normalisation:  36.84812068939209
printing an ep nov before normalisation:  37.24975682174019
maxi score, test score, baseline:  -0.977931746031746 -0.704 -0.704
probs:  [0.04417474681998609, 0.1078980662248432, 0.17990458286946925, 0.06732709531638098, 0.308510860989324, 0.2921846477799964]
printing an ep nov before normalisation:  13.399822517211268
maxi score, test score, baseline:  -0.977931746031746 -0.704 -0.704
probs:  [0.044239082240396715, 0.10735846815062333, 0.18016719110269414, 0.0674252514603204, 0.308961336011831, 0.2918486710341344]
printing an ep nov before normalisation:  18.05913329902367
maxi score, test score, baseline:  -0.977931746031746 -0.704 -0.704
probs:  [0.044239082240396715, 0.10735846815062333, 0.18016719110269414, 0.0674252514603204, 0.308961336011831, 0.2918486710341344]
printing an ep nov before normalisation:  55.40738344775109
printing an ep nov before normalisation:  19.412219524383545
Printing some Q and Qe and total Qs values:  [[0.127]
 [0.169]
 [0.168]
 [0.168]
 [0.168]
 [0.136]
 [0.154]] [[51.384]
 [47.917]
 [38.233]
 [38.233]
 [38.233]
 [50.282]
 [48.653]] [[1.56 ]
 [1.413]
 [0.884]
 [0.884]
 [0.884]
 [1.51 ]
 [1.439]]
printing an ep nov before normalisation:  32.22207580116829
maxi score, test score, baseline:  -0.977931746031746 -0.704 -0.704
probs:  [0.044239082240396715, 0.10735846815062333, 0.18016719110269414, 0.0674252514603204, 0.308961336011831, 0.2918486710341344]
maxi score, test score, baseline:  -0.977931746031746 -0.704 -0.704
probs:  [0.044239082240396715, 0.10735846815062333, 0.18016719110269414, 0.0674252514603204, 0.308961336011831, 0.2918486710341344]
line 256 mcts: sample exp_bonus 38.01496296754967
maxi score, test score, baseline:  -0.977931746031746 -0.704 -0.704
probs:  [0.044239082240396715, 0.10735846815062333, 0.18016719110269414, 0.0674252514603204, 0.308961336011831, 0.2918486710341344]
actions average: 
K:  3  action  0 :  tensor([0.2139, 0.1093, 0.1023, 0.1010, 0.2206, 0.1346, 0.1183],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0537, 0.6555, 0.0472, 0.0714, 0.0522, 0.0548, 0.0651],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1539, 0.0355, 0.2746, 0.1048, 0.1349, 0.1740, 0.1224],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2076, 0.0090, 0.1257, 0.1675, 0.1834, 0.1734, 0.1333],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1829, 0.0537, 0.1230, 0.1339, 0.2086, 0.1484, 0.1496],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1823, 0.0216, 0.1509, 0.1044, 0.1298, 0.2602, 0.1508],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2192, 0.0300, 0.1358, 0.1352, 0.1463, 0.1446, 0.1890],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.977931746031746 -0.704 -0.704
probs:  [0.044239082240396715, 0.10735846815062333, 0.18016719110269414, 0.0674252514603204, 0.308961336011831, 0.2918486710341344]
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.17723369598389
actor:  0 policy actor:  0  step number:  73 total reward:  0.15999999999999825  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9743303797468355 -0.704 -0.704
probs:  [0.044239082240396715, 0.10735846815062333, 0.18016719110269414, 0.0674252514603204, 0.308961336011831, 0.2918486710341344]
printing an ep nov before normalisation:  22.57907715060588
actor:  1 policy actor:  1  step number:  66 total reward:  0.03333333333333244  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9743303797468355 -0.704 -0.704
probs:  [0.043906097921876705, 0.10654895377798804, 0.17880799545245962, 0.06691721939198844, 0.3066297880492566, 0.2971899454064306]
maxi score, test score, baseline:  -0.9743303797468355 -0.704 -0.704
probs:  [0.043906097921876705, 0.10654895377798804, 0.17880799545245962, 0.06691721939198844, 0.3066297880492566, 0.2971899454064306]
printing an ep nov before normalisation:  57.59946707077642
Printing some Q and Qe and total Qs values:  [[0.26 ]
 [0.296]
 [0.26 ]
 [0.26 ]
 [0.26 ]
 [0.26 ]
 [0.26 ]] [[50.192]
 [62.54 ]
 [50.192]
 [50.192]
 [50.192]
 [50.192]
 [50.192]] [[1.247]
 [1.703]
 [1.247]
 [1.247]
 [1.247]
 [1.247]
 [1.247]]
maxi score, test score, baseline:  -0.9743303797468355 -0.704 -0.704
maxi score, test score, baseline:  -0.9744110410094637 -0.704 -0.704
probs:  [0.043906097921876705, 0.10654895377798804, 0.17880799545245962, 0.06691721939198844, 0.3066297880492566, 0.2971899454064306]
maxi score, test score, baseline:  -0.9744110410094637 -0.704 -0.704
probs:  [0.04392447503793273, 0.10659363017488584, 0.17888300828284823, 0.06652576394312883, 0.3067584641613596, 0.2973146583998447]
printing an ep nov before normalisation:  72.00980700422774
line 256 mcts: sample exp_bonus 45.130866645966144
maxi score, test score, baseline:  -0.9744110410094637 -0.704 -0.704
maxi score, test score, baseline:  -0.9744110410094637 -0.704 -0.704
probs:  [0.04392447503793273, 0.10659363017488584, 0.17888300828284823, 0.06652576394312883, 0.3067584641613596, 0.2973146583998447]
printing an ep nov before normalisation:  0.0025888892344028136
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  51 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  51.4681415481665
actor:  1 policy actor:  1  step number:  48 total reward:  0.24666666666666626  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  51.51170617866002
maxi score, test score, baseline:  -0.9744110410094637 -0.704 -0.704
probs:  [0.045488734850186424, 0.1041629276998811, 0.17184408444791666, 0.06664926240121437, 0.3177236766914187, 0.2941313139093827]
maxi score, test score, baseline:  -0.9744110410094637 -0.704 -0.704
probs:  [0.045488734850186424, 0.1041629276998811, 0.17184408444791666, 0.06664926240121437, 0.3177236766914187, 0.2941313139093827]
printing an ep nov before normalisation:  68.59070443252325
maxi score, test score, baseline:  -0.9744110410094637 -0.704 -0.704
probs:  [0.0454124903368518, 0.10437388286958962, 0.1710758693040486, 0.06667659488673594, 0.3171890322395686, 0.29527213036320543]
maxi score, test score, baseline:  -0.9744911949685535 -0.704 -0.704
probs:  [0.045446669815552614, 0.1044525699109292, 0.17120490696825907, 0.06672682577508901, 0.3174283648013782, 0.294740662728792]
printing an ep nov before normalisation:  63.810862839986385
printing an ep nov before normalisation:  30.744264426191435
printing an ep nov before normalisation:  64.52479321350289
printing an ep nov before normalisation:  47.65307410256314
using another actor
from probs:  [0.04531183879505639, 0.1045275445123238, 0.17151723097990493, 0.06666766000752186, 0.3164834863472847, 0.2954922393579083]
maxi score, test score, baseline:  -0.9745708463949844 -0.704 -0.704
probs:  [0.04534590117066233, 0.10460625251821135, 0.1716464458021596, 0.06671782358612223, 0.3167219983528127, 0.29496157857003186]
siam score:  -0.84267443
printing an ep nov before normalisation:  57.835265221895284
maxi score, test score, baseline:  -0.9745708463949844 -0.704 -0.704
probs:  [0.04534590117066233, 0.10460625251821135, 0.1716464458021596, 0.06671782358612223, 0.3167219983528127, 0.29496157857003186]
actor:  1 policy actor:  1  step number:  77 total reward:  0.09333333333333238  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  69.60092017696878
printing an ep nov before normalisation:  20.99690443607123
maxi score, test score, baseline:  -0.9745708463949844 -0.704 -0.704
probs:  [0.05740444370476683, 0.08982254805443494, 0.16674821360688052, 0.04749329167133406, 0.3316314773798809, 0.3069000255827027]
printing an ep nov before normalisation:  24.419426694851648
maxi score, test score, baseline:  -0.9745708463949844 -0.704 -0.704
probs:  [0.05730028136395317, 0.08984844959036441, 0.1670827467079033, 0.04734936504194404, 0.3306222989846688, 0.30779685831116615]
maxi score, test score, baseline:  -0.9745708463949844 -0.704 -0.704
probs:  [0.05734923609303968, 0.08992527678495749, 0.16722571311707085, 0.04738979835225162, 0.3309053116937721, 0.30720466395890816]
maxi score, test score, baseline:  -0.9745708463949844 -0.704 -0.704
probs:  [0.05734923609303968, 0.08992527678495749, 0.16722571311707085, 0.04738979835225162, 0.3309053116937721, 0.30720466395890816]
printing an ep nov before normalisation:  57.04748709734069
siam score:  -0.842775
maxi score, test score, baseline:  -0.97465 -0.704 -0.704
probs:  [0.05734923609303968, 0.08992527678495749, 0.16722571311707085, 0.04738979835225162, 0.3309053116937721, 0.30720466395890816]
maxi score, test score, baseline:  -0.97465 -0.704 -0.704
actor:  1 policy actor:  1  step number:  43 total reward:  0.3999999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.97465 -0.704 -0.704
probs:  [0.05931378510299502, 0.08943004172467299, 0.16089359985201737, 0.05010637448959861, 0.34995331793516404, 0.2903028808955521]
maxi score, test score, baseline:  -0.97465 -0.704 -0.704
maxi score, test score, baseline:  -0.97465 -0.704 -0.704
printing an ep nov before normalisation:  54.6714483762468
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.97465 -0.704 -0.704
probs:  [0.05872558301787669, 0.08950144700135755, 0.16129591549959824, 0.049995692424836015, 0.34917693879807693, 0.29130442325825445]
actor:  1 policy actor:  1  step number:  46 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.97465 -0.704 -0.704
probs:  [0.057775163570917665, 0.08805181602765769, 0.15868171523062885, 0.04918687946579142, 0.3435151434222144, 0.30278928228279]
using explorer policy with actor:  1
Starting evaluation
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]]
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]]
maxi score, test score, baseline:  -0.9749617283950618 -0.704 -0.704
probs:  [0.058107899242293364, 0.08831084201692643, 0.1587687899969235, 0.049134713089141545, 0.343152234054556, 0.30252552160015916]
maxi score, test score, baseline:  -0.9751147239263804 -0.704 -0.704
probs:  [0.058107899242293364, 0.08831084201692643, 0.1587687899969235, 0.049134713089141545, 0.343152234054556, 0.30252552160015916]
printing an ep nov before normalisation:  18.748525475182788
printing an ep nov before normalisation:  16.147316359217
actor:  1 policy actor:  1  step number:  57 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.975994674556213 -0.704 -0.704
probs:  [0.05937032408629632, 0.0881918057415909, 0.15414125440535612, 0.05080756508521583, 0.3548805924312291, 0.2926084582503117]
Printing some Q and Qe and total Qs values:  [[-0.212]
 [-0.211]
 [-0.211]
 [-0.211]
 [-0.211]
 [-0.211]
 [-0.211]] [[64.231]
 [65.241]
 [65.241]
 [65.241]
 [65.241]
 [65.241]
 [65.241]] [[1.421]
 [1.448]
 [1.448]
 [1.448]
 [1.448]
 [1.448]
 [1.448]]
printing an ep nov before normalisation:  67.72493443764698
Printing some Q and Qe and total Qs values:  [[0.179]
 [0.245]
 [0.211]
 [0.182]
 [0.19 ]
 [0.288]
 [0.16 ]] [[48.053]
 [43.599]
 [51.556]
 [52.316]
 [50.271]
 [48.488]
 [49.183]] [[1.483]
 [1.296]
 [1.712]
 [1.727]
 [1.619]
 [1.616]
 [1.528]]
maxi score, test score, baseline:  -0.9761352941176471 -0.704 -0.704
probs:  [0.05931902030966773, 0.08828056039631334, 0.15455049141552385, 0.05071465045012855, 0.35422869317631733, 0.29290658425204924]
maxi score, test score, baseline:  -0.9761352941176471 -0.704 -0.704
probs:  [0.05936524245455558, 0.08834940054497524, 0.15467108618584663, 0.050754152867081535, 0.3545052300100632, 0.2923548879374778]
Printing some Q and Qe and total Qs values:  [[ 0.   ]
 [ 0.036]
 [-0.016]
 [-0.016]
 [-0.001]
 [-0.015]
 [-0.015]] [[45.643]
 [46.239]
 [40.122]
 [40.376]
 [46.027]
 [39.815]
 [38.593]] [[0.604]
 [0.652]
 [0.461]
 [0.467]
 [0.611]
 [0.455]
 [0.427]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9762049853372434 -1.0 -0.9762049853372434
printing an ep nov before normalisation:  41.640562106029385
actor:  1 policy actor:  1  step number:  54 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9762049853372434 -1.0 -0.9762049853372434
maxi score, test score, baseline:  -0.9762049853372434 -1.0 -0.9762049853372434
maxi score, test score, baseline:  -0.9762049853372434 -1.0 -0.9762049853372434
probs:  [0.060671419984641754, 0.08827037467267153, 0.14807500303147642, 0.05245050156232263, 0.3663605469690819, 0.2841721537798058]
printing an ep nov before normalisation:  59.100016730430845
printing an ep nov before normalisation:  2.0343993125347737
siam score:  -0.82768095
maxi score, test score, baseline:  -0.9762049853372434 -1.0 -0.9762049853372434
probs:  [0.06042249732722335, 0.08861189821990141, 0.14854376184657292, 0.05228544904195801, 0.3652059321852331, 0.28493046137911127]
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.433]
 [0.385]
 [0.383]
 [0.499]
 [0.499]
 [0.499]] [[30.35 ]
 [32.378]
 [31.457]
 [31.854]
 [34.588]
 [34.588]
 [34.588]] [[1.871]
 [2.1  ]
 [1.962]
 [1.999]
 [2.383]
 [2.383]
 [2.383]]
maxi score, test score, baseline:  -0.9762049853372434 -1.0 -0.9762049853372434
probs:  [0.060327350521702586, 0.08863364965387767, 0.14881404390290642, 0.052156558822631774, 0.3643020735318526, 0.2857663235670291]
maxi score, test score, baseline:  -0.9762049853372434 -1.0 -0.9762049853372434
probs:  [0.060327350521702586, 0.08863364965387767, 0.14881404390290642, 0.052156558822631774, 0.3643020735318526, 0.2857663235670291]
maxi score, test score, baseline:  -0.9762049853372434 -1.0 -0.9762049853372434
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.605]
 [0.464]
 [0.464]
 [0.467]
 [0.464]
 [0.464]] [[39.511]
 [63.96 ]
 [39.511]
 [39.511]
 [38.127]
 [39.511]
 [39.511]] [[1.082]
 [1.769]
 [1.082]
 [1.082]
 [1.054]
 [1.082]
 [1.082]]
Printing some Q and Qe and total Qs values:  [[0.406]
 [0.68 ]
 [0.461]
 [0.426]
 [0.415]
 [0.461]
 [0.461]] [[50.341]
 [50.525]
 [41.448]
 [49.267]
 [48.335]
 [41.448]
 [41.448]] [[1.602]
 [1.883]
 [1.332]
 [1.583]
 [1.538]
 [1.332]
 [1.332]]
printing an ep nov before normalisation:  34.462904930114746
maxi score, test score, baseline:  -0.9762049853372434 -1.0 -0.9762049853372434
maxi score, test score, baseline:  -0.9762049853372434 -1.0 -0.9762049853372434
probs:  [0.06057514725847409, 0.08892541841673841, 0.14919929906091978, 0.05196611237355872, 0.3629696975541236, 0.2863643253361854]
maxi score, test score, baseline:  -0.9762049853372434 -1.0 -0.9762049853372434
probs:  [0.06057514725847409, 0.08892541841673841, 0.14919929906091978, 0.05196611237355872, 0.3629696975541236, 0.2863643253361854]
maxi score, test score, baseline:  -0.9762049853372434 -1.0 -0.9762049853372434
probs:  [0.06057514725847409, 0.08892541841673841, 0.14919929906091978, 0.05196611237355872, 0.3629696975541236, 0.2863643253361854]
printing an ep nov before normalisation:  16.08301877975464
maxi score, test score, baseline:  -0.9763431486880467 -1.0 -0.9763431486880467
probs:  [0.06057510617952111, 0.08892528576085272, 0.14919856175523027, 0.05196623833358231, 0.3629705642789798, 0.2863642436918338]
printing an ep nov before normalisation:  48.95811080932617
maxi score, test score, baseline:  -0.9763431486880467 -1.0 -0.9763431486880467
probs:  [0.060482188289026705, 0.08894794751326017, 0.1494669490663056, 0.051838223315446505, 0.3620728558707643, 0.28719183594519687]
maxi score, test score, baseline:  -0.9764116279069768 -1.0 -0.9764116279069768
probs:  [0.06052286263173036, 0.0883339779231375, 0.14956728415969667, 0.05187315172592783, 0.36231735040634966, 0.28738537315315804]
maxi score, test score, baseline:  -0.9764116279069768 -1.0 -0.9764116279069768
Printing some Q and Qe and total Qs values:  [[0.278]
 [0.3  ]
 [0.278]
 [0.278]
 [0.273]
 [0.276]
 [0.268]] [[33.129]
 [45.451]
 [33.129]
 [33.129]
 [33.214]
 [33.316]
 [37.802]] [[0.278]
 [0.3  ]
 [0.278]
 [0.278]
 [0.273]
 [0.276]
 [0.268]]
maxi score, test score, baseline:  -0.9764797101449276 -1.0 -0.9764797101449276
probs:  [0.06059818106129589, 0.0884439357861117, 0.14850680244863645, 0.05193776177666921, 0.3627696191203566, 0.28774369980693004]
maxi score, test score, baseline:  -0.9765473988439307 -1.0 -0.9765473988439307
probs:  [0.06064515425779144, 0.08851249484465598, 0.14862174762936983, 0.05197808618965193, 0.3630518862201356, 0.2871906308583952]
printing an ep nov before normalisation:  49.23036111967959
printing an ep nov before normalisation:  52.6052717482886
printing an ep nov before normalisation:  51.1529750945691
siam score:  -0.8359996
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.511]
 [0.433]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.523]] [[44.896]
 [55.975]
 [46.825]
 [44.896]
 [44.896]
 [44.896]
 [52.425]] [[1.41 ]
 [1.856]
 [1.419]
 [1.41 ]
 [1.41 ]
 [1.41 ]
 [1.728]]
maxi score, test score, baseline:  -0.9766146974063401 -1.0 -0.9766146974063401
probs:  [0.0606451351587199, 0.08851242064953227, 0.14862138137128728, 0.051978148571496574, 0.36305231553352857, 0.2871905987154355]
maxi score, test score, baseline:  -0.9766146974063401 -1.0 -0.9766146974063401
probs:  [0.0606451351587199, 0.08851242064953227, 0.14862138137128728, 0.051978148571496574, 0.36305231553352857, 0.2871905987154355]
printing an ep nov before normalisation:  5.737401424056543
siam score:  -0.8356519
actions average: 
K:  2  action  0 :  tensor([0.2656, 0.0139, 0.1337, 0.1117, 0.1664, 0.1424, 0.1662],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0246, 0.9003, 0.0116, 0.0180, 0.0100, 0.0079, 0.0277],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1755, 0.0116, 0.2737, 0.1025, 0.1504, 0.1409, 0.1454],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1581, 0.0412, 0.1130, 0.1927, 0.1779, 0.1599, 0.1572],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1784, 0.0082, 0.1244, 0.1363, 0.2103, 0.1775, 0.1649],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1645, 0.0174, 0.1269, 0.1187, 0.1838, 0.2360, 0.1528],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1482, 0.0431, 0.1108, 0.1317, 0.1662, 0.1475, 0.2526],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  53 total reward:  0.26666666666666605  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9766146974063401 -1.0 -0.9766146974063401
printing an ep nov before normalisation:  34.501966356608094
actor:  1 policy actor:  1  step number:  58 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9766146974063401 -1.0 -0.9766146974063401
probs:  [0.059438647429563445, 0.1190790630886355, 0.141203933549287, 0.05138353404830066, 0.3589044011716227, 0.2699904207125905]
maxi score, test score, baseline:  -0.9766146974063401 -1.0 -0.9766146974063401
probs:  [0.059438647429563445, 0.1190790630886355, 0.141203933549287, 0.05138353404830066, 0.3589044011716227, 0.2699904207125905]
printing an ep nov before normalisation:  36.285150748220424
printing an ep nov before normalisation:  24.50262846472223
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.488]
 [0.447]
 [0.438]
 [0.441]
 [0.443]
 [0.451]] [[35.993]
 [32.063]
 [35.455]
 [37.528]
 [36.486]
 [36.078]
 [36.142]] [[2.128]
 [1.814]
 [2.084]
 [2.265]
 [2.172]
 [2.137]
 [2.151]]
maxi score, test score, baseline:  -0.9766146974063401 -1.0 -0.9766146974063401
probs:  [0.059438647429563445, 0.1190790630886355, 0.141203933549287, 0.05138353404830066, 0.3589044011716227, 0.2699904207125905]
maxi score, test score, baseline:  -0.9766146974063401 -1.0 -0.9766146974063401
maxi score, test score, baseline:  -0.9766146974063401 -1.0 -0.9766146974063401
probs:  [0.059438647429563445, 0.1190790630886355, 0.141203933549287, 0.05138353404830066, 0.3589044011716227, 0.2699904207125905]
using another actor
actions average: 
K:  2  action  0 :  tensor([0.3360, 0.0110, 0.1065, 0.1135, 0.1468, 0.1482, 0.1379],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0148, 0.9528, 0.0043, 0.0063, 0.0093, 0.0044, 0.0080],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1790, 0.0233, 0.1598, 0.1389, 0.1560, 0.1962, 0.1468],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1486, 0.0845, 0.1108, 0.1655, 0.1539, 0.1842, 0.1525],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1591, 0.0183, 0.1075, 0.1644, 0.2282, 0.1712, 0.1513],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1714, 0.0688, 0.1312, 0.1354, 0.1393, 0.1993, 0.1545],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1367, 0.0858, 0.1148, 0.1405, 0.1279, 0.1731, 0.2212],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  40.993894444578245
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
printing an ep nov before normalisation:  36.95399146317397
line 256 mcts: sample exp_bonus 26.228645124735753
using explorer policy with actor:  0
printing an ep nov before normalisation:  47.29227017740095
Printing some Q and Qe and total Qs values:  [[0.442]
 [0.569]
 [0.443]
 [0.455]
 [0.424]
 [0.441]
 [0.456]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.442]
 [0.569]
 [0.443]
 [0.455]
 [0.424]
 [0.441]
 [0.456]]
maxi score, test score, baseline:  -0.9768142857142857 -1.0 -0.9768142857142857
probs:  [0.05984045393456274, 0.11942629305365647, 0.1415297776132361, 0.05140164336289199, 0.35903331871900523, 0.26876851331664753]
printing an ep nov before normalisation:  45.96684469621786
maxi score, test score, baseline:  -0.9768142857142857 -1.0 -0.9768142857142857
probs:  [0.05984045393456274, 0.11942629305365647, 0.1415297776132361, 0.05140164336289199, 0.35903331871900523, 0.26876851331664753]
maxi score, test score, baseline:  -0.9768142857142857 -1.0 -0.9768142857142857
maxi score, test score, baseline:  -0.9768142857142857 -1.0 -0.9768142857142857
probs:  [0.05990859959247509, 0.11956244582562575, 0.14055073995334597, 0.05146015755195979, 0.3594429422109335, 0.26907511486565994]
printing an ep nov before normalisation:  31.703994274139404
printing an ep nov before normalisation:  28.20681687983898
printing an ep nov before normalisation:  33.6744692621853
maxi score, test score, baseline:  -0.9768142857142857 -1.0 -0.9768142857142857
probs:  [0.05990859959247509, 0.11956244582562575, 0.14055073995334597, 0.05146015755195979, 0.3594429422109335, 0.26907511486565994]
printing an ep nov before normalisation:  51.53693441177784
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.064]
 [0.061]
 [0.064]
 [0.081]
 [0.064]
 [0.099]] [[38.452]
 [38.747]
 [38.066]
 [38.747]
 [37.262]
 [38.747]
 [36.665]] [[0.514]
 [0.579]
 [0.559]
 [0.579]
 [0.558]
 [0.579]
 [0.56 ]]
maxi score, test score, baseline:  -0.9768142857142857 -1.0 -0.9768142857142857
probs:  [0.059868448360739825, 0.11893610437412273, 0.14089869260198948, 0.0513793467702125, 0.35887579518125123, 0.2700416127116843]
maxi score, test score, baseline:  -0.9768142857142857 -1.0 -0.9768142857142857
probs:  [0.059868448360739825, 0.11893610437412273, 0.14089869260198948, 0.0513793467702125, 0.35887579518125123, 0.2700416127116843]
printing an ep nov before normalisation:  26.127233491948566
printing an ep nov before normalisation:  20.1796034972074
maxi score, test score, baseline:  -0.9768142857142857 -1.0 -0.9768142857142857
probs:  [0.05991991515905076, 0.11817756941228558, 0.14101997360915633, 0.05142349951045796, 0.35918488136052096, 0.27027416094852846]
maxi score, test score, baseline:  -0.9768142857142857 -1.0 -0.9768142857142857
probs:  [0.05991991515905076, 0.11817756941228558, 0.14101997360915633, 0.05142349951045796, 0.35918488136052096, 0.27027416094852846]
actor:  1 policy actor:  1  step number:  70 total reward:  0.21999999999999975  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9768142857142857 -1.0 -0.9768142857142857
probs:  [0.09356368175929085, 0.11332055468886906, 0.13620061653756071, 0.04966899708750564, 0.3469026889662314, 0.2603434609605423]
printing an ep nov before normalisation:  43.99941103481706
maxi score, test score, baseline:  -0.9768142857142857 -1.0 -0.9768142857142857
maxi score, test score, baseline:  -0.9768142857142857 -1.0 -0.9768142857142857
maxi score, test score, baseline:  -0.9768142857142857 -1.0 -0.9768142857142857
probs:  [0.09301953049690372, 0.11382879662811428, 0.13700199803692012, 0.04936173469292344, 0.3447475753348007, 0.26204036481033777]
maxi score, test score, baseline:  -0.9768142857142857 -1.0 -0.9768142857142857
probs:  [0.09301953049690372, 0.11382879662811428, 0.13700199803692012, 0.04936173469292344, 0.3447475753348007, 0.26204036481033777]
printing an ep nov before normalisation:  50.65830066391585
siam score:  -0.8361841
maxi score, test score, baseline:  -0.9768142857142857 -1.0 -0.9768142857142857
probs:  [0.09253862127751339, 0.11410105075359589, 0.1372398243700418, 0.049351318803222, 0.34467675973827544, 0.2620924250573515]
printing an ep nov before normalisation:  8.074661501966034
actor:  1 policy actor:  1  step number:  67 total reward:  0.03999999999999937  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9768142857142857 -1.0 -0.9768142857142857
probs:  [0.11462252184000775, 0.11132550235217063, 0.1339007591454724, 0.0481526717472585, 0.33628581904445554, 0.25571272587063515]
printing an ep nov before normalisation:  22.62501098868558
maxi score, test score, baseline:  -0.9768142857142857 -1.0 -0.9768142857142857
probs:  [0.11462252184000775, 0.11132550235217063, 0.1339007591454724, 0.0481526717472585, 0.33628581904445554, 0.25571272587063515]
actions average: 
K:  2  action  0 :  tensor([0.3500, 0.0275, 0.1098, 0.1030, 0.1354, 0.1373, 0.1369],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0501, 0.7926, 0.0345, 0.0304, 0.0215, 0.0392, 0.0318],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1320, 0.0389, 0.2965, 0.0998, 0.1174, 0.1843, 0.1312],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2111, 0.0057, 0.1088, 0.2431, 0.1453, 0.1496, 0.1364],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1901, 0.0243, 0.1200, 0.1583, 0.1726, 0.1744, 0.1603],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1717, 0.0053, 0.1202, 0.1428, 0.1510, 0.2499, 0.1591],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1572, 0.0494, 0.1072, 0.1597, 0.1716, 0.1646, 0.1903],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9768142857142857 -1.0 -0.9768142857142857
maxi score, test score, baseline:  -0.9768142857142857 -1.0 -0.9768142857142857
probs:  [0.11469973758231618, 0.11140049446468937, 0.13399097683178612, 0.04818505780647653, 0.33651253257188884, 0.255211200742843]
maxi score, test score, baseline:  -0.9768142857142857 -1.0 -0.9768142857142857
probs:  [0.11469973758231618, 0.11140049446468937, 0.13399097683178612, 0.04818505780647653, 0.33651253257188884, 0.255211200742843]
printing an ep nov before normalisation:  37.89243010883547
maxi score, test score, baseline:  -0.9768142857142857 -1.0 -0.9768142857142857
maxi score, test score, baseline:  -0.9768142857142857 -1.0 -0.9768142857142857
probs:  [0.11396306077001019, 0.1116087179478064, 0.1343025526649069, 0.04810406816749314, 0.3359442365566651, 0.25607736389311825]
maxi score, test score, baseline:  -0.9768142857142857 -1.0 -0.9768142857142857
probs:  [0.11396306077001019, 0.1116087179478064, 0.1343025526649069, 0.04810406816749314, 0.3359442365566651, 0.25607736389311825]
printing an ep nov before normalisation:  48.152116665648535
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.283]
 [0.235]
 [0.224]
 [0.217]
 [0.2  ]
 [0.209]] [[34.988]
 [35.82 ]
 [34.468]
 [34.767]
 [35.292]
 [36.31 ]
 [35.976]] [[1.798]
 [1.887]
 [1.719]
 [1.734]
 [1.773]
 [1.847]
 [1.826]]
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.414]
 [0.367]
 [0.343]
 [0.326]
 [0.296]
 [0.416]] [[25.598]
 [29.233]
 [24.769]
 [23.806]
 [23.631]
 [23.657]
 [28.547]] [[1.438]
 [1.793]
 [1.408]
 [1.312]
 [1.282]
 [1.253]
 [1.743]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.684]
 [0.676]
 [0.658]
 [0.667]
 [0.666]
 [0.659]
 [0.641]] [[62.67 ]
 [64.992]
 [65.152]
 [68.469]
 [68.354]
 [67.916]
 [64.319]] [[2.111]
 [2.192]
 [2.181]
 [2.317]
 [2.312]
 [2.287]
 [2.132]]
printing an ep nov before normalisation:  46.96480239920601
maxi score, test score, baseline:  -0.9768142857142857 -1.0 -0.9768142857142857
probs:  [0.11333956663443409, 0.11191734941295735, 0.13479551181943014, 0.047896891462525695, 0.33449125772617805, 0.2575594229444747]
printing an ep nov before normalisation:  50.941036291274735
printing an ep nov before normalisation:  47.556951092076645
printing an ep nov before normalisation:  32.12869644165039
using explorer policy with actor:  1
printing an ep nov before normalisation:  20.686946183660694
printing an ep nov before normalisation:  48.93295214100206
maxi score, test score, baseline:  -0.9768800569800571 -1.0 -0.9768800569800571
probs:  [0.11344641949582206, 0.11201833375628029, 0.13497713942136957, 0.04777133015835216, 0.33361095162471355, 0.2581758255434623]
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.565]
 [0.496]
 [0.52 ]
 [0.493]
 [0.5  ]
 [0.496]] [[50.461]
 [51.787]
 [50.855]
 [51.108]
 [51.519]
 [51.799]
 [50.946]] [[1.951]
 [2.055]
 [1.934]
 [1.972]
 [1.968]
 [1.99 ]
 [1.939]]
line 256 mcts: sample exp_bonus 0.0
siam score:  -0.83990324
maxi score, test score, baseline:  -0.9769454545454546 -1.0 -0.9769454545454546
probs:  [0.11352445956084506, 0.11209457052595455, 0.13506868588222534, 0.04780384951545635, 0.33383858835215086, 0.2576698461633678]
maxi score, test score, baseline:  -0.9769454545454546 -1.0 -0.9769454545454546
probs:  [0.11352445956084506, 0.11209457052595455, 0.13506868588222534, 0.04780384951545635, 0.33383858835215086, 0.2576698461633678]
maxi score, test score, baseline:  -0.9769454545454546 -1.0 -0.9769454545454546
probs:  [0.11352445956084506, 0.11209457052595455, 0.13506868588222534, 0.04780384951545635, 0.33383858835215086, 0.2576698461633678]
actions average: 
K:  0  action  0 :  tensor([0.2352, 0.0463, 0.1073, 0.1210, 0.2349, 0.1380, 0.1173],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0190, 0.9114, 0.0075, 0.0136, 0.0134, 0.0045, 0.0307],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1756, 0.0389, 0.1529, 0.1344, 0.1564, 0.1876, 0.1542],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1687, 0.0406, 0.1237, 0.1783, 0.1874, 0.1593, 0.1420],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1845, 0.0175, 0.1154, 0.1377, 0.2453, 0.1620, 0.1377],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1589, 0.0156, 0.1541, 0.1251, 0.1579, 0.2608, 0.1276],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2109, 0.0041, 0.1362, 0.1396, 0.1905, 0.1615, 0.1572],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  33.10127985812996
maxi score, test score, baseline:  -0.9769454545454546 -1.0 -0.9769454545454546
probs:  [0.11352445956084506, 0.11209457052595455, 0.13506868588222534, 0.04780384951545635, 0.33383858835215086, 0.2576698461633678]
maxi score, test score, baseline:  -0.9769454545454546 -1.0 -0.9769454545454546
probs:  [0.11363054708480491, 0.11219562806245005, 0.13525056053769027, 0.04767874857890683, 0.332961515087708, 0.25828300064843995]
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.436]
 [1.166]
 [0.401]
 [0.4  ]
 [0.403]
 [0.406]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.402]
 [0.436]
 [1.166]
 [0.401]
 [0.4  ]
 [0.403]
 [0.406]]
printing an ep nov before normalisation:  37.921397723851086
printing an ep nov before normalisation:  55.37627163685037
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
printing an ep nov before normalisation:  60.734705165831386
maxi score, test score, baseline:  -0.9769454545454546 -1.0 -0.9769454545454546
probs:  [0.11408663593975588, 0.1118324054948769, 0.13576627328246874, 0.04758100399575049, 0.33227801638569615, 0.25845566490145183]
using explorer policy with actor:  1
siam score:  -0.83808166
maxi score, test score, baseline:  -0.9769454545454546 -1.0 -0.9769454545454546
probs:  [0.11419310338177299, 0.11193106036250883, 0.13594787668929773, 0.047456980274906135, 0.33140850157286605, 0.2590624777186483]
maxi score, test score, baseline:  -0.9769454545454546 -1.0 -0.9769454545454546
probs:  [0.11427090257359611, 0.11200731663561267, 0.13604051461172117, 0.04748925937152546, 0.3316344613881277, 0.25855754541941683]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9770104815864024 -1.0 -0.9770104815864024
probs:  [0.11427164595596569, 0.112007217977274, 0.13604006930826187, 0.04748927518104098, 0.3316345657917535, 0.258557225785704]
siam score:  -0.83558446
maxi score, test score, baseline:  -0.9770104815864024 -1.0 -0.9770104815864024
probs:  [0.11427164595596569, 0.112007217977274, 0.13604006930826187, 0.04748927518104098, 0.3316345657917535, 0.258557225785704]
printing an ep nov before normalisation:  13.687911033630371
line 256 mcts: sample exp_bonus 26.25462668039499
printing an ep nov before normalisation:  28.805680295863425
maxi score, test score, baseline:  -0.9770104815864024 -1.0 -0.9770104815864024
probs:  [0.11445489425333348, 0.11058262230968706, 0.1362582666310631, 0.04756530492900156, 0.33216678847696174, 0.2589721233999531]
maxi score, test score, baseline:  -0.9770751412429379 -1.0 -0.9770751412429379
probs:  [0.11453338026608832, 0.1106576178584826, 0.1363503956077204, 0.0475975771841606, 0.33239269415213124, 0.25846833493141674]
Printing some Q and Qe and total Qs values:  [[0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]]
printing an ep nov before normalisation:  41.87239094049251
printing an ep nov before normalisation:  27.66615867614746
siam score:  -0.83456457
maxi score, test score, baseline:  -0.9771394366197184 -1.0 -0.9771394366197184
probs:  [0.11453411917613052, 0.11065750449268565, 0.13634995718223006, 0.047597593824395185, 0.33239280442834446, 0.2584680208962142]
maxi score, test score, baseline:  -0.9771394366197184 -1.0 -0.9771394366197184
maxi score, test score, baseline:  -0.9771394366197184 -1.0 -0.9771394366197184
probs:  [0.11462409142340461, 0.10995859203080263, 0.136457086812895, 0.047634922866856826, 0.33265411481018176, 0.2586711920558592]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.79045123448503
printing an ep nov before normalisation:  63.90797680228372
maxi score, test score, baseline:  -0.9772033707865169 -1.0 -0.9772033707865169
probs:  [0.11491008799462393, 0.10867465774332781, 0.13685121902903888, 0.04758479697204554, 0.3323018902068839, 0.2596773480540799]
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.44012718378218
printing an ep nov before normalisation:  28.27155007695563
maxi score, test score, baseline:  -0.9772033707865169 -1.0 -0.9772033707865169
probs:  [0.11491008799462393, 0.10867465774332781, 0.13685121902903888, 0.04758479697204554, 0.3323018902068839, 0.2596773480540799]
maxi score, test score, baseline:  -0.9772033707865169 -1.0 -0.9772033707865169
probs:  [0.11491008799462393, 0.10867465774332781, 0.13685121902903888, 0.04758479697204554, 0.3323018902068839, 0.2596773480540799]
maxi score, test score, baseline:  -0.9772033707865169 -1.0 -0.9772033707865169
maxi score, test score, baseline:  -0.9772033707865169 -1.0 -0.9772033707865169
probs:  [0.11501846293092374, 0.10876160938303178, 0.13703497791461802, 0.047461859934696364, 0.3314399846570499, 0.2602831051796803]
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.427]
 [0.427]
 [0.389]
 [0.427]
 [0.427]
 [0.427]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.427]
 [0.427]
 [0.427]
 [0.389]
 [0.427]
 [0.427]
 [0.427]]
printing an ep nov before normalisation:  49.28116088108652
maxi score, test score, baseline:  -0.9772033707865169 -1.0 -0.9772033707865169
probs:  [0.11510579812275068, 0.10808460073096103, 0.13713904995291373, 0.047497838928998844, 0.3316918424258768, 0.26048086983849894]
maxi score, test score, baseline:  -0.9772033707865169 -1.0 -0.9772033707865169
probs:  [0.11519203333040418, 0.10741611898295902, 0.1372418112069469, 0.04753336476892586, 0.33194052805351, 0.26067614365725406]
printing an ep nov before normalisation:  55.89692486514514
actor:  0 policy actor:  1  step number:  48 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9732893557422969 -1.0 -0.9732893557422969
printing an ep nov before normalisation:  53.146708786475834
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.469589317778414
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  58.71715632438955
siam score:  -0.83979905
maxi score, test score, baseline:  -0.9732893557422969 -1.0 -0.9732893557422969
probs:  [0.11522475648982146, 0.1074987957356518, 0.1373622022784714, 0.047564702759981, 0.3321602820587447, 0.2601892606773297]
printing an ep nov before normalisation:  31.309266090393066
maxi score, test score, baseline:  -0.9732893557422969 -1.0 -0.9732893557422969
probs:  [0.11522475648982146, 0.1074987957356518, 0.1373622022784714, 0.047564702759981, 0.3321602820587447, 0.2601892606773297]
printing an ep nov before normalisation:  55.14383692873082
actor:  1 policy actor:  1  step number:  60 total reward:  0.16666666666666596  reward:  1.0 rdn_beta:  0.333
from probs:  [0.11522475648982146, 0.1074987957356518, 0.1373622022784714, 0.047564702759981, 0.3321602820587447, 0.2601892606773297]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
actor:  1 policy actor:  1  step number:  47 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9732893557422969 -1.0 -0.9732893557422969
probs:  [0.13925892028508208, 0.10325321650250409, 0.1308386608126826, 0.04789085917428595, 0.33446179627420286, 0.24429654695124237]
from probs:  [0.13926005950903012, 0.10325302429697249, 0.13083816486527, 0.04789084756517836, 0.33446170810794207, 0.24429619565560684]
maxi score, test score, baseline:  -0.9734376044568245 -1.0 -0.9734376044568245
probs:  [0.13926119226278277, 0.10325283317557507, 0.13083767172899186, 0.04789083602252473, 0.3344616204459883, 0.24429584636413737]
actions average: 
K:  0  action  0 :  tensor([0.2468, 0.0393, 0.1204, 0.1206, 0.1698, 0.1604, 0.1427],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0032, 0.9806, 0.0043, 0.0018, 0.0024, 0.0023, 0.0055],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1722, 0.0275, 0.1744, 0.1062, 0.1294, 0.2738, 0.1165],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2377, 0.0058, 0.1424, 0.1295, 0.1741, 0.1604, 0.1501],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2020, 0.0197, 0.1177, 0.1319, 0.2202, 0.1624, 0.1460],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1640, 0.0030, 0.1809, 0.1065, 0.1222, 0.3035, 0.1200],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1706, 0.0616, 0.1117, 0.1432, 0.1763, 0.1671, 0.1695],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9734376044568245 -1.0 -0.9734376044568245
probs:  [0.13926119226278277, 0.10325283317557507, 0.13083767172899186, 0.04789083602252473, 0.3344616204459883, 0.24429584636413737]
maxi score, test score, baseline:  -0.9734376044568245 -1.0 -0.9734376044568245
probs:  [0.13926119226278277, 0.10325283317557507, 0.13083767172899186, 0.04789083602252473, 0.3344616204459883, 0.24429584636413737]
printing an ep nov before normalisation:  39.20896827589422
maxi score, test score, baseline:  -0.9735111111111111 -1.0 -0.9735111111111111
probs:  [0.1395394643877357, 0.10338508250541348, 0.13108047039471407, 0.04780079010286041, 0.333830039775668, 0.24436415283360843]
using explorer policy with actor:  1
siam score:  -0.83483857
maxi score, test score, baseline:  -0.9735111111111111 -1.0 -0.9735111111111111
probs:  [0.1395394643877357, 0.10338508250541348, 0.13108047039471407, 0.04780079010286041, 0.333830039775668, 0.24436415283360843]
printing an ep nov before normalisation:  33.94096887830341
maxi score, test score, baseline:  -0.9735111111111111 -1.0 -0.9735111111111111
probs:  [0.13963515123659037, 0.10277011349610173, 0.13117035108673242, 0.047833508573762305, 0.3340590854557795, 0.24453179015103368]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9735111111111111 -1.0 -0.9735111111111111
actor:  1 policy actor:  1  step number:  78 total reward:  0.033333333333332216  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9735111111111111 -1.0 -0.9735111111111111
probs:  [0.15839407275657452, 0.10061657837665451, 0.12849677926679673, 0.04668591753026454, 0.32602419484960576, 0.23978245722010386]
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.254557848024675
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.672]
 [0.287]
 [0.551]
 [0.478]
 [0.595]
 [0.595]] [[63.335]
 [65.795]
 [69.1  ]
 [66.593]
 [68.975]
 [63.335]
 [63.335]] [[2.027]
 [2.213]
 [1.976]
 [2.128]
 [2.161]
 [2.027]
 [2.027]]
Printing some Q and Qe and total Qs values:  [[0.665]
 [0.905]
 [0.665]
 [0.665]
 [0.665]
 [0.665]
 [0.665]] [[43.616]
 [46.439]
 [43.616]
 [43.616]
 [43.616]
 [43.616]
 [43.616]] [[1.799]
 [2.164]
 [1.799]
 [1.799]
 [1.799]
 [1.799]
 [1.799]]
UNIT TEST: sample policy line 217 mcts : [0.224 0.306 0.224 0.061 0.184 0.    0.   ]
printing an ep nov before normalisation:  31.707051729053447
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.681]
 [0.631]
 [0.63 ]
 [0.647]
 [0.624]
 [0.653]] [[38.108]
 [37.005]
 [37.639]
 [39.601]
 [40.45 ]
 [39.612]
 [37.908]] [[1.988]
 [1.936]
 [1.932]
 [2.07 ]
 [2.148]
 [2.065]
 [1.973]]
maxi score, test score, baseline:  -0.9735111111111111 -1.0 -0.9735111111111111
probs:  [0.15922671260383878, 0.10084795406907039, 0.1290182915531429, 0.04635606131807422, 0.3237115230058358, 0.2408394574500379]
printing an ep nov before normalisation:  26.478182710576732
actor:  1 policy actor:  1  step number:  52 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  63.908880609277475
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.436]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]] [[36.278]
 [44.143]
 [36.278]
 [36.278]
 [36.278]
 [36.278]
 [36.278]] [[1.307]
 [1.716]
 [1.307]
 [1.307]
 [1.307]
 [1.307]
 [1.307]]
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
probs:  [0.15463742563467495, 0.10015283260397714, 0.12703766507308767, 0.04814722790843483, 0.33626756085418974, 0.2337572879256356]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
probs:  [0.15487110707517168, 0.10020787085352584, 0.12718085287085232, 0.04803175103337277, 0.3354580318927731, 0.23425038627430428]
printing an ep nov before normalisation:  34.32566772511137
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
probs:  [0.15487110707517168, 0.10020787085352584, 0.12718085287085232, 0.04803175103337277, 0.3354580318927731, 0.23425038627430428]
printing an ep nov before normalisation:  46.07274191985622
actor:  1 policy actor:  1  step number:  45 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
probs:  [0.1521560703456913, 0.09956840685187761, 0.12551721998106022, 0.049373423510796176, 0.3448635750080164, 0.22852130430255824]
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
probs:  [0.1521560703456913, 0.09956840685187761, 0.12551721998106022, 0.049373423510796176, 0.3448635750080164, 0.22852130430255824]
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
probs:  [0.1521560703456913, 0.09956840685187761, 0.12551721998106022, 0.049373423510796176, 0.3448635750080164, 0.22852130430255824]
printing an ep nov before normalisation:  36.99022152802343
printing an ep nov before normalisation:  37.57193759031452
printing an ep nov before normalisation:  19.671424627304077
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.4707],
        [-0.0000],
        [-0.6327],
        [-0.4868],
        [-0.6245],
        [-0.6279],
        [-0.4761],
        [-0.0000],
        [-0.6453],
        [-0.5515]], dtype=torch.float64)
-0.058614567066 -0.5293111187809562
-0.957 -0.957
-0.032346567066 -0.6650332716057781
-0.09703970119800001 -0.5838271419595846
-0.032346567066 -0.656836294702459
-0.032346567066 -0.6602657041797848
-0.057834381198 -0.5339333290578651
-0.8645999999999999 -0.8645999999999999
-0.032346567066 -0.6776487194511839
-0.09703970119800001 -0.6485486967762489
actor:  1 policy actor:  1  step number:  46 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  59.719152311818235
printing an ep nov before normalisation:  70.16035232991881
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
Printing some Q and Qe and total Qs values:  [[0.95]
 [0.95]
 [0.95]
 [0.95]
 [0.95]
 [0.95]
 [0.95]] [[72.477]
 [72.477]
 [72.477]
 [72.477]
 [72.477]
 [72.477]
 [72.477]] [[2.95]
 [2.95]
 [2.95]
 [2.95]
 [2.95]
 [2.95]
 [2.95]]
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
probs:  [0.14851480727596017, 0.09904759137385824, 0.12401208796232895, 0.05075665474649027, 0.3545599411307608, 0.2231089175106016]
Printing some Q and Qe and total Qs values:  [[0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]] [[37.155]
 [37.155]
 [37.155]
 [37.155]
 [37.155]
 [37.155]
 [37.155]] [[0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]]
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
probs:  [0.14859636039376653, 0.09910195644263747, 0.1240801739560644, 0.050784478269952034, 0.35475474026613446, 0.2226822906714451]
maxi score, test score, baseline:  -0.9736569060773481 -1.0 -0.9736569060773481
probs:  [0.14876761326628113, 0.09861938024801183, 0.12422143922047119, 0.050842323537978906, 0.35515972255926787, 0.22238952116798913]
maxi score, test score, baseline:  -0.9736569060773481 -1.0 -0.9736569060773481
probs:  [0.1489850331438183, 0.098763444134804, 0.12348622456608294, 0.05091649995079829, 0.3556790493385622, 0.22216974886593427]
from probs:  [0.1489850331438183, 0.098763444134804, 0.12348622456608294, 0.05091649995079829, 0.3556790493385622, 0.22216974886593427]
maxi score, test score, baseline:  -0.9737292011019284 -1.0 -0.9737292011019284
probs:  [0.14804021935649442, 0.09887299061920114, 0.12362304243995141, 0.05097294747743353, 0.35607424572231045, 0.2224165543846091]
printing an ep nov before normalisation:  65.96760797991772
printing an ep nov before normalisation:  59.455735740926976
from probs:  [0.1471871867736763, 0.09903497738572309, 0.12382563271521865, 0.05105635219351399, 0.35665818204621164, 0.2222376688856563]
maxi score, test score, baseline:  -0.9737292011019284 -1.0 -0.9737292011019284
maxi score, test score, baseline:  -0.9737292011019284 -1.0 -0.9737292011019284
probs:  [0.14535927217036954, 0.0992471286017429, 0.12409096163173798, 0.05116558588166137, 0.3574229531682837, 0.22271409854620436]
maxi score, test score, baseline:  -0.9737292011019284 -1.0 -0.9737292011019284
printing an ep nov before normalisation:  39.35856035189903
maxi score, test score, baseline:  -0.9737292011019284 -1.0 -0.9737292011019284
probs:  [0.14535927217036954, 0.0992471286017429, 0.12409096163173798, 0.05116558588166137, 0.3574229531682837, 0.22271409854620436]
printing an ep nov before normalisation:  40.519819259643555
maxi score, test score, baseline:  -0.9738010989010989 -1.0 -0.9738010989010989
actor:  1 policy actor:  1  step number:  59 total reward:  0.21333333333333315  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9738010989010989 -1.0 -0.9738010989010989
probs:  [0.14365423406230643, 0.09895676966829908, 0.1230373350794529, 0.05205561753312449, 0.36366444614711924, 0.2186315975096978]
Printing some Q and Qe and total Qs values:  [[0.442]
 [0.501]
 [0.442]
 [0.442]
 [0.445]
 [0.442]
 [0.442]] [[34.233]
 [40.965]
 [34.233]
 [34.233]
 [35.434]
 [34.233]
 [34.233]] [[0.619]
 [0.753]
 [0.619]
 [0.619]
 [0.635]
 [0.619]
 [0.619]]
printing an ep nov before normalisation:  13.714527152576572
maxi score, test score, baseline:  -0.9738010989010989 -1.0 -0.9738010989010989
probs:  [0.14365423406230643, 0.09895676966829908, 0.1230373350794529, 0.05205561753312449, 0.36366444614711924, 0.2186315975096978]
printing an ep nov before normalisation:  23.142627756327183
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]]
from probs:  [0.14278495417338896, 0.09905716392238474, 0.12316219263783901, 0.05210836518352225, 0.3640337553676853, 0.2188535687151799]
maxi score, test score, baseline:  -0.9740144414168938 -1.0 -0.9740144414168938
probs:  [0.14340326636349146, 0.09873031339068748, 0.12272609759951646, 0.05197126017838547, 0.3630718959606614, 0.22009716650725764]
printing an ep nov before normalisation:  51.79503172448719
using another actor
Printing some Q and Qe and total Qs values:  [[0.184]
 [0.282]
 [0.183]
 [0.183]
 [0.183]
 [0.182]
 [0.185]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.184]
 [0.282]
 [0.183]
 [0.183]
 [0.183]
 [0.182]
 [0.185]]
maxi score, test score, baseline:  -0.9740847826086957 -1.0 -0.9740847826086957
line 256 mcts: sample exp_bonus 18.13604561517196
maxi score, test score, baseline:  -0.9740847826086957 -1.0 -0.9740847826086957
probs:  [0.14355640153887428, 0.09883458222116166, 0.12285555665195787, 0.05202613235716545, 0.36345607205735214, 0.21927125517348856]
actor:  1 policy actor:  1  step number:  52 total reward:  0.23333333333333295  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9741547425474255 -1.0 -0.9741547425474255
probs:  [0.1418163176877067, 0.09838688230874526, 0.12171267004287384, 0.05293280793131774, 0.36981225636453824, 0.2153390656648181]
Printing some Q and Qe and total Qs values:  [[0.596]
 [0.62 ]
 [0.554]
 [0.575]
 [0.583]
 [0.554]
 [0.554]] [[39.688]
 [47.26 ]
 [39.132]
 [42.393]
 [42.82 ]
 [39.132]
 [39.132]] [[1.012]
 [1.18 ]
 [0.959]
 [1.042]
 [1.058]
 [0.959]
 [0.959]]
actions average: 
K:  2  action  0 :  tensor([0.2991, 0.0418, 0.1159, 0.0983, 0.1638, 0.1180, 0.1631],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0172, 0.9000, 0.0136, 0.0186, 0.0149, 0.0121, 0.0237],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1856, 0.0154, 0.2248, 0.1248, 0.1497, 0.1473, 0.1526],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1691, 0.0988, 0.1148, 0.1261, 0.1700, 0.1401, 0.1811],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1697, 0.0026, 0.1181, 0.1167, 0.3090, 0.1447, 0.1392],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1397, 0.0468, 0.1489, 0.1108, 0.1359, 0.2876, 0.1303],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2024, 0.0444, 0.1029, 0.1177, 0.1722, 0.1389, 0.2215],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  50.239853812500144
using explorer policy with actor:  0
siam score:  -0.82334757
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.397]
 [0.506]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]] [[51.621]
 [56.779]
 [51.621]
 [51.621]
 [51.621]
 [51.621]
 [51.621]] [[0.397]
 [0.506]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]]
printing an ep nov before normalisation:  51.86286725735538
actions average: 
K:  2  action  0 :  tensor([0.2435, 0.0705, 0.1109, 0.1174, 0.1650, 0.1298, 0.1630],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0229, 0.8527, 0.0257, 0.0267, 0.0148, 0.0136, 0.0437],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1497, 0.0342, 0.2830, 0.1172, 0.1367, 0.1256, 0.1537],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1481, 0.0223, 0.1114, 0.2388, 0.1572, 0.1481, 0.1741],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1998, 0.0777, 0.0942, 0.1107, 0.2729, 0.1067, 0.1380],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1766, 0.0028, 0.1337, 0.1144, 0.1568, 0.2410, 0.1747],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1572, 0.1055, 0.1207, 0.1349, 0.1501, 0.1328, 0.1989],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  46.401037331348334
maxi score, test score, baseline:  -0.9742243243243244 -1.0 -0.9742243243243244
using explorer policy with actor:  0
printing an ep nov before normalisation:  48.32110485168599
maxi score, test score, baseline:  -0.9742243243243244 -1.0 -0.9742243243243244
probs:  [0.14209543277919864, 0.09793588624314407, 0.12191047109835215, 0.05285861108357356, 0.36929183779662034, 0.21590776099911133]
printing an ep nov before normalisation:  17.938056846247918
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9743623655913979 -1.0 -0.9743623655913979
probs:  [0.14209797211546962, 0.09793549106595834, 0.12190963499473255, 0.05285852251682405, 0.36929120664292364, 0.2159071726640918]
printing an ep nov before normalisation:  47.34287306634968
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  44 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  53.15919672339227
printing an ep nov before normalisation:  45.032665464613174
Printing some Q and Qe and total Qs values:  [[0.81 ]
 [1.107]
 [1.003]
 [0.95 ]
 [1.003]
 [1.003]
 [1.003]] [[50.993]
 [48.47 ]
 [47.131]
 [50.306]
 [47.131]
 [47.131]
 [47.131]] [[2.656]
 [2.797]
 [2.61 ]
 [2.754]
 [2.61 ]
 [2.61 ]
 [2.61 ]]
maxi score, test score, baseline:  -0.9743623655913979 -1.0 -0.9743623655913979
maxi score, test score, baseline:  -0.9743623655913979 -1.0 -0.9743623655913979
probs:  [0.1378446045856566, 0.09502170877991754, 0.15017247612009654, 0.05103336489872889, 0.35651290282691117, 0.20941494278868916]
printing an ep nov before normalisation:  26.104238035672463
maxi score, test score, baseline:  -0.9743623655913979 -1.0 -0.9743623655913979
probs:  [0.1378446045856566, 0.09502170877991754, 0.15017247612009654, 0.05103336489872889, 0.35651290282691117, 0.20941494278868916]
maxi score, test score, baseline:  -0.9743623655913979 -1.0 -0.9743623655913979
probs:  [0.13809030802052316, 0.0951069101400031, 0.1504643849533063, 0.05095369603399025, 0.355954217643656, 0.20943048320852123]
actor:  1 policy actor:  1  step number:  46 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9743623655913979 -1.0 -0.9743623655913979
probs:  [0.13685645239243724, 0.09425748789439703, 0.14911985849683315, 0.0504991697315415, 0.3527718385431001, 0.21649519294169095]
line 256 mcts: sample exp_bonus 47.24943753634118
maxi score, test score, baseline:  -0.9743623655913979 -1.0 -0.9743623655913979
probs:  [0.1370276120503408, 0.09429233699740022, 0.14933025928913415, 0.050393998514788756, 0.3520346080418074, 0.21692118510652875]
maxi score, test score, baseline:  -0.9743623655913979 -1.0 -0.9743623655913979
probs:  [0.1370276120503408, 0.09429233699740022, 0.14933025928913415, 0.050393998514788756, 0.3520346080418074, 0.21692118510652875]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9743623655913979 -1.0 -0.9743623655913979
probs:  [0.1370276120503408, 0.09429233699740022, 0.14933025928913415, 0.050393998514788756, 0.3520346080418074, 0.21692118510652875]
printing an ep nov before normalisation:  27.46054571048028
maxi score, test score, baseline:  -0.9743623655913979 -1.0 -0.9743623655913979
probs:  [0.13717370017128802, 0.09439281977071363, 0.14842317000663974, 0.05044763476647421, 0.35241014288525113, 0.2171525323996333]
printing an ep nov before normalisation:  44.53285356047059
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.7301078534845
maxi score, test score, baseline:  -0.9743623655913979 -1.0 -0.9743623655913979
probs:  [0.13724669134220052, 0.09391082562007702, 0.1485021529255004, 0.050474433473915274, 0.3525977743488066, 0.2172681222895001]
actor:  1 policy actor:  1  step number:  62 total reward:  0.1533333333333331  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9743623655913979 -1.0 -0.9743623655913979
probs:  [0.13632918912218855, 0.09328330770704032, 0.1475093341709614, 0.0501375725458983, 0.35023923862175743, 0.2225013578321539]
maxi score, test score, baseline:  -0.9743623655913979 -1.0 -0.9743623655913979
probs:  [0.13632918912218855, 0.09328330770704032, 0.1475093341709614, 0.0501375725458983, 0.35023923862175743, 0.2225013578321539]
actions average: 
K:  0  action  0 :  tensor([0.2895, 0.0027, 0.1228, 0.1198, 0.1606, 0.1484, 0.1564],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0164, 0.8809, 0.0169, 0.0338, 0.0102, 0.0156, 0.0262],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1709, 0.0556, 0.2153, 0.1085, 0.1305, 0.1700, 0.1492],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1853, 0.0062, 0.1301, 0.1464, 0.1918, 0.1762, 0.1640],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2097, 0.0079, 0.1164, 0.1189, 0.2661, 0.1409, 0.1400],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1824, 0.0024, 0.1511, 0.1298, 0.1714, 0.1997, 0.1633],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1928, 0.0075, 0.1936, 0.1300, 0.1510, 0.1717, 0.1533],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9743623655913979 -1.0 -0.9743623655913979
probs:  [0.13642397053913494, 0.09344828910120478, 0.14758588281157894, 0.05010107759668105, 0.3499851717840528, 0.22245560816734752]
printing an ep nov before normalisation:  50.60880661010742
maxi score, test score, baseline:  -0.9744308310991957 -1.0 -0.9744308310991957
printing an ep nov before normalisation:  35.15615463256836
Printing some Q and Qe and total Qs values:  [[0.353]
 [0.321]
 [0.419]
 [0.41 ]
 [0.388]
 [0.419]
 [0.419]] [[27.621]
 [28.896]
 [26.577]
 [27.066]
 [27.238]
 [26.577]
 [26.577]] [[1.189]
 [1.196]
 [1.224]
 [1.229]
 [1.213]
 [1.224]
 [1.224]]
printing an ep nov before normalisation:  22.549784020581285
maxi score, test score, baseline:  -0.9744989304812834 -1.0 -0.9744989304812834
probs:  [0.13687767720290417, 0.09367382710744776, 0.14601744874084113, 0.05009958708751574, 0.3499738669793188, 0.22335759288197238]
maxi score, test score, baseline:  -0.9744989304812834 -1.0 -0.9744989304812834
maxi score, test score, baseline:  -0.9744989304812834 -1.0 -0.9744989304812834
probs:  [0.13687767720290417, 0.09367382710744776, 0.14601744874084113, 0.05009958708751574, 0.3499738669793188, 0.22335759288197238]
printing an ep nov before normalisation:  72.45639381659142
printing an ep nov before normalisation:  76.19922573111705
siam score:  -0.8300094
maxi score, test score, baseline:  -0.9744989304812834 -1.0 -0.9744989304812834
probs:  [0.13687767720290417, 0.09367382710744776, 0.14601744874084113, 0.05009958708751574, 0.3499738669793188, 0.22335759288197238]
siam score:  -0.8311159
maxi score, test score, baseline:  -0.9745666666666667 -1.0 -0.9745666666666667
probs:  [0.13538597418574852, 0.09394589559895188, 0.14542431305415843, 0.050244969765582445, 0.35099176203048676, 0.22400708536507205]
printing an ep nov before normalisation:  53.834078047010635
printing an ep nov before normalisation:  67.80997256761673
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  2.1528841975082003
maxi score, test score, baseline:  -0.9747010610079576 -1.0 -0.9747010610079576
probs:  [0.13592477023161134, 0.09362768573499985, 0.1450290196012804, 0.050113105489356824, 0.350066780401432, 0.22523863854131948]
maxi score, test score, baseline:  -0.9747010610079576 -1.0 -0.9747010610079576
printing an ep nov before normalisation:  24.724224554405705
maxi score, test score, baseline:  -0.9747010610079576 -1.0 -0.9747010610079576
probs:  [0.13592477023161134, 0.09362768573499985, 0.1450290196012804, 0.050113105489356824, 0.350066780401432, 0.22523863854131948]
printing an ep nov before normalisation:  32.18704000649366
printing an ep nov before normalisation:  19.47683877751757
maxi score, test score, baseline:  -0.974834036939314 -1.0 -0.974834036939314
probs:  [0.13592717590888076, 0.09362736140799201, 0.1450277422797565, 0.0501130416393113, 0.3500663231214689, 0.22523835564259043]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.974834036939314 -1.0 -0.974834036939314
probs:  [0.13606234771098727, 0.0937204271944008, 0.14417735020379657, 0.050162792469073736, 0.35041465298113234, 0.22546242944060924]
printing an ep nov before normalisation:  38.84516102927072
maxi score, test score, baseline:  -0.974834036939314 -1.0 -0.974834036939314
maxi score, test score, baseline:  -0.974834036939314 -1.0 -0.974834036939314
probs:  [0.13630019382749364, 0.09380316992950341, 0.14444492251862784, 0.05008597852017568, 0.34987597974250256, 0.22548975546169678]
maxi score, test score, baseline:  -0.974834036939314 -1.0 -0.974834036939314
maxi score, test score, baseline:  -0.974834036939314 -1.0 -0.974834036939314
probs:  [0.13630019382749364, 0.09380316992950341, 0.14444492251862784, 0.05008597852017568, 0.34987597974250256, 0.22548975546169678]
maxi score, test score, baseline:  -0.974834036939314 -1.0 -0.974834036939314
probs:  [0.1363953252732265, 0.09396782752341562, 0.14452672899329408, 0.05004945146907885, 0.34962169609096455, 0.22543897065002036]
maxi score, test score, baseline:  -0.974834036939314 -1.0 -0.974834036939314
probs:  [0.1363953252732265, 0.09396782752341562, 0.14452672899329408, 0.05004945146907885, 0.34962169609096455, 0.22543897065002036]
Printing some Q and Qe and total Qs values:  [[0.186]
 [0.359]
 [0.186]
 [0.186]
 [0.138]
 [0.186]
 [0.111]] [[23.867]
 [28.224]
 [23.867]
 [23.867]
 [16.206]
 [23.867]
 [21.069]] [[0.972]
 [1.359]
 [0.972]
 [0.972]
 [0.547]
 [0.972]
 [0.759]]
maxi score, test score, baseline:  -0.9749656167979003 -1.0 -0.9749656167979003
probs:  [0.13653169258431205, 0.09405977261448452, 0.14368503605074381, 0.05009846223627837, 0.34996483491949415, 0.22566020159468708]
printing an ep nov before normalisation:  32.79220461845398
Printing some Q and Qe and total Qs values:  [[0.353]
 [0.357]
 [0.359]
 [0.471]
 [0.351]
 [0.354]
 [0.36 ]] [[11.555]
 [22.714]
 [12.639]
 [42.906]
 [11.8  ]
 [10.845]
 [10.673]] [[0.595]
 [1.024]
 [0.642]
 [1.907]
 [0.602]
 [0.569]
 [0.569]]
Printing some Q and Qe and total Qs values:  [[0.322]
 [0.613]
 [0.422]
 [0.624]
 [0.391]
 [0.428]
 [0.473]] [[53.752]
 [69.003]
 [54.216]
 [55.519]
 [47.005]
 [46.201]
 [50.882]] [[0.631]
 [1.075]
 [0.736]
 [0.951]
 [0.632]
 [0.662]
 [0.753]]
Printing some Q and Qe and total Qs values:  [[0.363]
 [0.378]
 [0.354]
 [0.342]
 [0.363]
 [0.362]
 [0.367]] [[39.678]
 [37.738]
 [39.141]
 [41.433]
 [39.022]
 [32.801]
 [36.814]] [[1.828]
 [1.723]
 [1.785]
 [1.915]
 [1.787]
 [1.401]
 [1.655]]
maxi score, test score, baseline:  -0.9749656167979003 -1.0 -0.9749656167979003
probs:  [0.13653169258431205, 0.09405977261448452, 0.14368503605074381, 0.05009846223627837, 0.34996483491949415, 0.22566020159468708]
printing an ep nov before normalisation:  28.586119243938278
printing an ep nov before normalisation:  22.45125385538394
maxi score, test score, baseline:  -0.9749656167979003 -1.0 -0.9749656167979003
probs:  [0.13653169258431205, 0.09405977261448452, 0.14368503605074381, 0.05009846223627837, 0.34996483491949415, 0.22566020159468708]
maxi score, test score, baseline:  -0.9749656167979003 -1.0 -0.9749656167979003
probs:  [0.13653169258431205, 0.09405977261448452, 0.14368503605074381, 0.05009846223627837, 0.34996483491949415, 0.22566020159468708]
actions average: 
K:  3  action  0 :  tensor([0.2162, 0.0176, 0.1330, 0.1422, 0.1649, 0.1565, 0.1695],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0370, 0.8064, 0.0239, 0.0432, 0.0276, 0.0203, 0.0416],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1750, 0.0330, 0.1891, 0.1332, 0.1534, 0.1662, 0.1502],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1860, 0.0296, 0.1223, 0.1844, 0.1640, 0.1515, 0.1622],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1908, 0.0018, 0.1145, 0.1275, 0.2813, 0.1435, 0.1406],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1452, 0.0158, 0.1523, 0.1188, 0.1544, 0.2682, 0.1453],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1527, 0.0583, 0.1080, 0.1454, 0.1380, 0.1176, 0.2801],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9749656167979003 -1.0 -0.9749656167979003
line 256 mcts: sample exp_bonus 39.308704351385735
printing an ep nov before normalisation:  72.53351332883352
printing an ep nov before normalisation:  31.172280036547086
maxi score, test score, baseline:  -0.9749656167979003 -1.0 -0.9749656167979003
probs:  [0.13674998829248108, 0.0942100951398869, 0.1439147801587775, 0.050178427919157026, 0.3505247146672524, 0.22442199382244526]
actor:  1 policy actor:  1  step number:  54 total reward:  0.40666666666666673  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9749656167979003 -1.0 -0.9749656167979003
printing an ep nov before normalisation:  32.45244957019133
actions average: 
K:  2  action  0 :  tensor([0.2528, 0.0542, 0.1094, 0.1230, 0.1744, 0.1414, 0.1447],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0183, 0.8589, 0.0163, 0.0378, 0.0158, 0.0205, 0.0324],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1995, 0.0165, 0.1444, 0.1288, 0.1604, 0.1948, 0.1556],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2000, 0.0143, 0.1098, 0.2311, 0.1467, 0.1442, 0.1539],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2084, 0.0033, 0.1019, 0.1081, 0.3233, 0.1354, 0.1196],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1792, 0.0139, 0.1460, 0.1212, 0.1638, 0.2407, 0.1354],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1809, 0.0191, 0.1557, 0.1251, 0.1535, 0.1728, 0.1929],
       grad_fn=<DivBackward0>)
siam score:  -0.8288925
using explorer policy with actor:  1
siam score:  -0.8300958
siam score:  -0.82919955
printing an ep nov before normalisation:  43.57252692733029
printing an ep nov before normalisation:  44.7560743007994
maxi score, test score, baseline:  -0.9749656167979003 -1.0 -0.9749656167979003
maxi score, test score, baseline:  -0.9749656167979003 -1.0 -0.9749656167979003
probs:  [0.13349252371288656, 0.11953826334678551, 0.14051286445199584, 0.04866635381449107, 0.33993626735092575, 0.21785372732291525]
maxi score, test score, baseline:  -0.9749656167979003 -1.0 -0.9749656167979003
probs:  [0.13349252371288656, 0.11953826334678551, 0.14051286445199584, 0.04866635381449107, 0.33993626735092575, 0.21785372732291525]
printing an ep nov before normalisation:  39.09647747119499
maxi score, test score, baseline:  -0.9749656167979003 -1.0 -0.9749656167979003
probs:  [0.13349252371288656, 0.11953826334678551, 0.14051286445199584, 0.04866635381449107, 0.33993626735092575, 0.21785372732291525]
maxi score, test score, baseline:  -0.9749656167979003 -1.0 -0.9749656167979003
from probs:  [0.13371065805330568, 0.1197075227966452, 0.14075558758295723, 0.04858738392569338, 0.3393825323407995, 0.2178563153005991]
actor:  1 policy actor:  1  step number:  55 total reward:  0.17333333333333278  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9749656167979003 -1.0 -0.9749656167979003
probs:  [0.13176516404440294, 0.11897361202590039, 0.13955319038832675, 0.049436460468232135, 0.34533375782860226, 0.2149378152445355]
maxi score, test score, baseline:  -0.9749656167979003 -1.0 -0.9749656167979003
maxi score, test score, baseline:  -0.9749656167979003 -1.0 -0.9749656167979003
probs:  [0.13176516404440294, 0.11897361202590039, 0.13955319038832675, 0.049436460468232135, 0.34533375782860226, 0.2149378152445355]
printing an ep nov before normalisation:  47.34183010040329
maxi score, test score, baseline:  -0.975030890052356 -1.0 -0.975030890052356
probs:  [0.1317663164212393, 0.11897324843949116, 0.13955260570464864, 0.04943645123202174, 0.3453336884162478, 0.21493768978635136]
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.150747858074205
maxi score, test score, baseline:  -0.975030890052356 -1.0 -0.975030890052356
probs:  [0.1317663164212393, 0.11897324843949116, 0.13955260570464864, 0.04943645123202174, 0.3453336884162478, 0.21493768978635136]
actions average: 
K:  2  action  0 :  tensor([0.2877, 0.0619, 0.1214, 0.1001, 0.1406, 0.1596, 0.1287],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0326, 0.7885, 0.0286, 0.0381, 0.0200, 0.0203, 0.0719],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1865, 0.0122, 0.1594, 0.1157, 0.1770, 0.1841, 0.1650],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1891, 0.0087, 0.1251, 0.1225, 0.2109, 0.2092, 0.1345],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1931, 0.0063, 0.1331, 0.1170, 0.2253, 0.1723, 0.1529],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1769, 0.0069, 0.1304, 0.1138, 0.1737, 0.2416, 0.1567],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1705, 0.0308, 0.1244, 0.1388, 0.1588, 0.1864, 0.1903],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.975030890052356 -1.0 -0.975030890052356
probs:  [0.1317663164212393, 0.11897324843949116, 0.13955260570464864, 0.04943645123202174, 0.3453336884162478, 0.21493768978635136]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.0928806565717
maxi score, test score, baseline:  -0.975030890052356 -1.0 -0.975030890052356
probs:  [0.1311392510666038, 0.11918591824528955, 0.13984536555871813, 0.049378500076572136, 0.34492713428560756, 0.21552383076720874]
maxi score, test score, baseline:  -0.975030890052356 -1.0 -0.975030890052356
probs:  [0.1311392510666038, 0.11918591824528955, 0.13984536555871813, 0.049378500076572136, 0.34492713428560756, 0.21552383076720874]
Printing some Q and Qe and total Qs values:  [[0.148]
 [0.254]
 [0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]] [[33.542]
 [36.893]
 [33.542]
 [33.542]
 [33.542]
 [33.542]
 [33.542]] [[0.597]
 [0.78 ]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]]
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]] [[70.829]
 [70.829]
 [70.829]
 [70.829]
 [70.829]
 [70.829]
 [70.829]] [[2.225]
 [2.225]
 [2.225]
 [2.225]
 [2.225]
 [2.225]
 [2.225]]
maxi score, test score, baseline:  -0.975030890052356 -1.0 -0.975030890052356
probs:  [0.13137204681243844, 0.11868698318309169, 0.1401101446442252, 0.04931093429222519, 0.34445326395963305, 0.2160666271083864]
maxi score, test score, baseline:  -0.975030890052356 -1.0 -0.975030890052356
probs:  [0.13137204681243844, 0.11868698318309169, 0.1401101446442252, 0.04931093429222519, 0.34445326395963305, 0.2160666271083864]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.592]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]] [[37.626]
 [45.629]
 [44.714]
 [44.714]
 [44.714]
 [44.714]
 [44.714]] [[0.571]
 [0.592]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.805]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]] [[45.465]
 [37.289]
 [45.465]
 [45.465]
 [45.465]
 [45.465]
 [45.465]] [[0.517]
 [0.805]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]]
printing an ep nov before normalisation:  24.778908962737
printing an ep nov before normalisation:  42.022813688397775
from probs:  [0.13143787172621887, 0.11874644570181772, 0.14018035228970135, 0.04933560015219914, 0.3446259631408024, 0.21567376698926058]
Printing some Q and Qe and total Qs values:  [[0.389]
 [0.448]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]] [[55.822]
 [62.454]
 [55.822]
 [55.822]
 [55.822]
 [55.822]
 [55.822]] [[1.738]
 [2.11 ]
 [1.738]
 [1.738]
 [1.738]
 [1.738]
 [1.738]]
printing an ep nov before normalisation:  0.0004657499499671758
printing an ep nov before normalisation:  27.703731827044038
actor:  0 policy actor:  1  step number:  48 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9714926892950392 -1.0 -0.9714926892950392
maxi score, test score, baseline:  -0.9714926892950392 -1.0 -0.9714926892950392
siam score:  -0.8408743
maxi score, test score, baseline:  -0.9714926892950392 -1.0 -0.9714926892950392
probs:  [0.13151794286242774, 0.11887148708762174, 0.1403808773691311, 0.04923506595121044, 0.34392152544686666, 0.21607310128274237]
printing an ep nov before normalisation:  56.479950233767084
maxi score, test score, baseline:  -0.9714926892950392 -1.0 -0.9714926892950392
probs:  [0.13151794286242774, 0.11887148708762174, 0.1403808773691311, 0.04923506595121044, 0.34392152544686666, 0.21607310128274237]
actions average: 
K:  2  action  0 :  tensor([0.3270, 0.0182, 0.1033, 0.1041, 0.1635, 0.1490, 0.1348],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0504, 0.7290, 0.0303, 0.0704, 0.0317, 0.0306, 0.0576],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1622, 0.0071, 0.2406, 0.0978, 0.1506, 0.2158, 0.1260],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2217, 0.0058, 0.1137, 0.1737, 0.1708, 0.1555, 0.1588],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1786, 0.0182, 0.1145, 0.1270, 0.2122, 0.1969, 0.1527],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2054, 0.0026, 0.1192, 0.1312, 0.2028, 0.1943, 0.1444],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1944, 0.0606, 0.1057, 0.1591, 0.1594, 0.1595, 0.1613],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9714926892950392 -1.0 -0.9714926892950392
probs:  [0.13151794286242774, 0.11887148708762174, 0.1403808773691311, 0.04923506595121044, 0.34392152544686666, 0.21607310128274237]
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]] [[15.763]
 [15.763]
 [15.763]
 [15.763]
 [15.763]
 [15.763]
 [15.763]] [[0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]]
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.464]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]] [[34.306]
 [53.601]
 [34.306]
 [34.306]
 [34.306]
 [34.306]
 [34.306]] [[1.079]
 [1.683]
 [1.079]
 [1.079]
 [1.079]
 [1.079]
 [1.079]]
maxi score, test score, baseline:  -0.9714926892950392 -1.0 -0.9714926892950392
maxi score, test score, baseline:  -0.9714926892950392 -1.0 -0.9714926892950392
maxi score, test score, baseline:  -0.9714926892950392 -1.0 -0.9714926892950392
probs:  [0.13164924018067642, 0.11899014636259005, 0.14052103172621197, 0.049284135127586476, 0.34426508505562625, 0.21529036154730888]
actor:  1 policy actor:  1  step number:  41 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9714926892950392 -1.0 -0.9714926892950392
probs:  [0.13008492440983194, 0.11783597473271777, 0.13866927726705242, 0.05038838045151953, 0.35200530032749466, 0.21101614281138362]
printing an ep nov before normalisation:  40.23176193237305
printing an ep nov before normalisation:  30.016368001374758
maxi score, test score, baseline:  -0.9715666666666667 -1.0 -0.9715666666666667
probs:  [0.1293429694307116, 0.11793621919668013, 0.13878710026174243, 0.05043133128606512, 0.3523060242927215, 0.21119635553207922]
printing an ep nov before normalisation:  26.672431858709373
Printing some Q and Qe and total Qs values:  [[0.874]
 [1.333]
 [0.874]
 [0.874]
 [0.874]
 [0.874]
 [0.874]] [[47.308]
 [40.856]
 [47.308]
 [47.308]
 [47.308]
 [47.308]
 [47.308]] [[1.823]
 [2.039]
 [1.823]
 [1.823]
 [1.823]
 [1.823]
 [1.823]]
maxi score, test score, baseline:  -0.9715666666666667 -1.0 -0.9715666666666667
maxi score, test score, baseline:  -0.9715666666666667 -1.0 -0.9715666666666667
probs:  [0.1293429694307116, 0.11793621919668013, 0.13878710026174243, 0.05043133128606512, 0.3523060242927215, 0.21119635553207922]
actor:  1 policy actor:  1  step number:  49 total reward:  0.23999999999999966  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9715666666666667 -1.0 -0.9715666666666667
probs:  [0.1265211485916945, 0.11536351119426912, 0.15805006179722553, 0.049332867373089714, 0.34461489730467315, 0.20611751373904802]
maxi score, test score, baseline:  -0.9715666666666667 -1.0 -0.9715666666666667
probs:  [0.1265211485916945, 0.11536351119426912, 0.15805006179722553, 0.049332867373089714, 0.34461489730467315, 0.20611751373904802]
maxi score, test score, baseline:  -0.9715666666666667 -1.0 -0.9715666666666667
printing an ep nov before normalisation:  34.47874799745628
actions average: 
K:  1  action  0 :  tensor([0.3464, 0.0037, 0.1055, 0.1273, 0.1569, 0.1326, 0.1276],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0449, 0.8106, 0.0277, 0.0304, 0.0204, 0.0223, 0.0436],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2172, 0.0251, 0.2186, 0.1173, 0.1286, 0.1530, 0.1400],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2294, 0.0281, 0.1115, 0.2603, 0.1337, 0.1235, 0.1134],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1990, 0.0048, 0.0939, 0.1174, 0.3327, 0.1276, 0.1246],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1909, 0.0054, 0.1141, 0.1404, 0.1565, 0.2402, 0.1525],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2248, 0.0226, 0.1099, 0.1335, 0.1339, 0.1332, 0.2421],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9715666666666667 -1.0 -0.9715666666666667
probs:  [0.12601590957379322, 0.11500793443449682, 0.15739316473161905, 0.04945738831723673, 0.3454867569832604, 0.2066388459595939]
printing an ep nov before normalisation:  54.16535339323371
maxi score, test score, baseline:  -0.9715666666666667 -1.0 -0.9715666666666667
probs:  [0.12601590957379322, 0.11500793443449682, 0.15739316473161905, 0.04945738831723673, 0.3454867569832604, 0.2066388459595939]
line 256 mcts: sample exp_bonus 40.0565601194171
maxi score, test score, baseline:  -0.9715666666666667 -1.0 -0.9715666666666667
probs:  [0.126141059987977, 0.11510078293372077, 0.1576103888846109, 0.04935788462275954, 0.34478930642712224, 0.20700057714380957]
maxi score, test score, baseline:  -0.9715666666666667 -1.0 -0.9715666666666667
probs:  [0.1262728371949484, 0.11522101437534536, 0.15673012639584627, 0.04940936307264438, 0.3451497428728488, 0.20721691608836668]
printing an ep nov before normalisation:  18.090505599975586
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9715666666666667 -1.0 -0.9715666666666667
probs:  [0.12633221544595177, 0.11527519015341323, 0.15680384193992278, 0.04943255904400225, 0.34531215400780146, 0.20684403940890853]
maxi score, test score, baseline:  -0.9715666666666667 -1.0 -0.9715666666666667
probs:  [0.12645772848212525, 0.11536840288330774, 0.1570183701354663, 0.04933342915960587, 0.3446173219387825, 0.20720474740071235]
printing an ep nov before normalisation:  0.009401595218037073
actor:  1 policy actor:  1  step number:  46 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9715666666666667 -1.0 -0.9715666666666667
maxi score, test score, baseline:  -0.9715666666666667 -1.0 -0.9715666666666667
probs:  [0.1245916530874548, 0.11459164647240042, 0.15514425090787765, 0.05029651384692087, 0.3513675814045739, 0.20400835428077224]
maxi score, test score, baseline:  -0.9716402597402598 -1.0 -0.9716402597402598
probs:  [0.12459282813224953, 0.11459125910817328, 0.15514375759395965, 0.05029649061119291, 0.35136741355486406, 0.20400825099956058]
printing an ep nov before normalisation:  42.73537635803223
maxi score, test score, baseline:  -0.9716402597402598 -1.0 -0.9716402597402598
Printing some Q and Qe and total Qs values:  [[0.303]
 [0.303]
 [0.303]
 [0.303]
 [0.303]
 [0.303]
 [0.303]] [[37.971]
 [37.971]
 [37.971]
 [37.971]
 [37.971]
 [37.971]
 [37.971]] [[0.806]
 [0.806]
 [0.806]
 [0.806]
 [0.806]
 [0.806]
 [0.806]]
maxi score, test score, baseline:  -0.9716402597402598 -1.0 -0.9716402597402598
probs:  [0.12483445657029525, 0.11477426790679346, 0.15556444638392108, 0.050102664921427474, 0.3500088274659988, 0.20471533675156392]
actor:  1 policy actor:  1  step number:  57 total reward:  0.11999999999999966  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.38 ]
 [0.434]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]] [[45.221]
 [48.6  ]
 [45.221]
 [45.221]
 [45.221]
 [45.221]
 [45.221]] [[1.938]
 [2.218]
 [1.938]
 [1.938]
 [1.938]
 [1.938]
 [1.938]]
maxi score, test score, baseline:  -0.9716402597402598 -1.0 -0.9716402597402598
probs:  [0.12407822082196554, 0.11420149676737457, 0.15424779713326375, 0.05070929018760391, 0.35426085746666014, 0.20250233762313213]
maxi score, test score, baseline:  -0.9716402597402598 -1.0 -0.9716402597402598
maxi score, test score, baseline:  -0.9716402597402598 -1.0 -0.9716402597402598
probs:  [0.12407822082196554, 0.11420149676737457, 0.15424779713326375, 0.05070929018760391, 0.35426085746666014, 0.20250233762313213]
maxi score, test score, baseline:  -0.9716402597402598 -1.0 -0.9716402597402598
printing an ep nov before normalisation:  56.15979194641113
printing an ep nov before normalisation:  54.119839090933525
maxi score, test score, baseline:  -0.9717134715025907 -1.0 -0.9717134715025907
probs:  [0.12421222903783442, 0.11370406366541311, 0.15441249503635796, 0.05076349658750895, 0.3546403993055495, 0.20226731636733614]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9717863049095607 -1.0 -0.9717863049095607
probs:  [0.12445139853796473, 0.11388045876879112, 0.15482600422247747, 0.050573491211501106, 0.35330858373091134, 0.2029600635283541]
actions average: 
K:  2  action  0 :  tensor([0.3356, 0.0259, 0.1223, 0.1046, 0.1330, 0.1396, 0.1390],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0288, 0.8314, 0.0169, 0.0528, 0.0210, 0.0203, 0.0286],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2084, 0.0262, 0.1473, 0.1497, 0.1491, 0.1771, 0.1422],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2003, 0.0078, 0.1260, 0.1356, 0.1947, 0.1785, 0.1571],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2308, 0.0084, 0.1498, 0.1246, 0.1686, 0.1771, 0.1407],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2179, 0.0393, 0.1628, 0.1201, 0.1432, 0.1707, 0.1460],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1975, 0.0501, 0.1194, 0.1504, 0.1595, 0.1581, 0.1650],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.319]
 [0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]] [[36.783]
 [40.599]
 [36.783]
 [36.783]
 [36.783]
 [36.783]
 [36.783]] [[1.954]
 [2.255]
 [1.954]
 [1.954]
 [1.954]
 [1.954]
 [1.954]]
maxi score, test score, baseline:  -0.9717863049095607 -1.0 -0.9717863049095607
probs:  [0.12445139853796473, 0.11388045876879112, 0.15482600422247747, 0.050573491211501106, 0.35330858373091134, 0.2029600635283541]
printing an ep nov before normalisation:  31.913854007447114
line 256 mcts: sample exp_bonus 13.763188744468478
printing an ep nov before normalisation:  11.946659328338782
actor:  1 policy actor:  1  step number:  52 total reward:  0.37999999999999945  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8245996
printing an ep nov before normalisation:  41.34816386439059
maxi score, test score, baseline:  -0.9717863049095607 -1.0 -0.9717863049095607
probs:  [0.12343099186953929, 0.11294681863881313, 0.15355628202965216, 0.05015947625719279, 0.3504097115531162, 0.20949671965168642]
printing an ep nov before normalisation:  57.11697682639174
printing an ep nov before normalisation:  32.30492026438342
maxi score, test score, baseline:  -0.9717863049095607 -1.0 -0.9717863049095607
probs:  [0.12343099186953929, 0.11294681863881313, 0.15355628202965216, 0.05015947625719279, 0.3504097115531162, 0.20949671965168642]
maxi score, test score, baseline:  -0.9717863049095607 -1.0 -0.9717863049095607
actor:  1 policy actor:  1  step number:  66 total reward:  0.23333333333333295  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  45.51093963401511
printing an ep nov before normalisation:  41.24161252763772
maxi score, test score, baseline:  -0.9717863049095607 -1.0 -0.9717863049095607
probs:  [0.12255088350781775, 0.11229702004544052, 0.15201440011215928, 0.050888950779679885, 0.35552277240988905, 0.2067259731450135]
maxi score, test score, baseline:  -0.9717863049095607 -1.0 -0.9717863049095607
probs:  [0.12255088350781775, 0.11229702004544052, 0.15201440011215928, 0.050888950779679885, 0.35552277240988905, 0.2067259731450135]
printing an ep nov before normalisation:  39.997788023351596
actor:  1 policy actor:  1  step number:  56 total reward:  0.0999999999999992  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9717863049095607 -1.0 -0.9717863049095607
probs:  [0.12063942520771417, 0.11054566799914618, 0.1652446470331548, 0.05009643887787091, 0.349973635832536, 0.203500185049578]
printing an ep nov before normalisation:  27.617881096734244
printing an ep nov before normalisation:  36.00266793774973
Printing some Q and Qe and total Qs values:  [[0.369]
 [0.433]
 [0.369]
 [0.369]
 [0.369]
 [0.369]
 [0.369]] [[30.556]
 [34.032]
 [30.556]
 [30.556]
 [30.556]
 [30.556]
 [30.556]] [[0.971]
 [1.116]
 [0.971]
 [0.971]
 [0.971]
 [0.971]
 [0.971]]
printing an ep nov before normalisation:  60.01839781608549
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.694]
 [0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.476]] [[40.019]
 [46.629]
 [40.019]
 [40.019]
 [40.019]
 [40.019]
 [40.019]] [[1.492]
 [2.031]
 [1.492]
 [1.492]
 [1.492]
 [1.492]
 [1.492]]
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.947]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.611]] [[35.237]
 [31.876]
 [35.237]
 [35.237]
 [35.237]
 [35.237]
 [37.41 ]] [[2.058]
 [2.228]
 [2.058]
 [2.058]
 [2.058]
 [2.058]
 [2.25 ]]
printing an ep nov before normalisation:  56.00279439397984
actor:  1 policy actor:  1  step number:  58 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9717863049095607 -1.0 -0.9717863049095607
probs:  [0.11731911402296426, 0.10883868293611931, 0.17959700808377962, 0.0493240036718703, 0.3445650754746798, 0.20035611581058677]
actor:  1 policy actor:  1  step number:  48 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8243324
maxi score, test score, baseline:  -0.9717863049095607 -1.0 -0.9717863049095607
probs:  [0.1164160906415601, 0.10802002413817094, 0.17807443499586423, 0.04887200338960058, 0.34140137563903017, 0.20721607119577407]
maxi score, test score, baseline:  -0.9717863049095607 -1.0 -0.9717863049095607
probs:  [0.11650742217318436, 0.10808815328948394, 0.17833615826443946, 0.04877667803855702, 0.34073326137714105, 0.2075583268571941]
maxi score, test score, baseline:  -0.9717863049095607 -1.0 -0.9717863049095607
printing an ep nov before normalisation:  43.40014046726637
printing an ep nov before normalisation:  35.02515390104214
printing an ep nov before normalisation:  39.2800654636507
printing an ep nov before normalisation:  49.51776761822028
maxi score, test score, baseline:  -0.971858762886598 -1.0 -0.971858762886598
probs:  [0.1165994690543012, 0.10815567162651246, 0.17859704352630576, 0.048681623397772884, 0.34006704004302396, 0.20789915235208367]
printing an ep nov before normalisation:  49.635628697477294
maxi score, test score, baseline:  -0.971858762886598 -1.0 -0.971858762886598
probs:  [0.1165994690543012, 0.10815567162651246, 0.17859704352630576, 0.048681623397772884, 0.34006704004302396, 0.20789915235208367]
maxi score, test score, baseline:  -0.971858762886598 -1.0 -0.971858762886598
probs:  [0.1165994690543012, 0.10815567162651246, 0.17859704352630576, 0.048681623397772884, 0.34006704004302396, 0.20789915235208367]
printing an ep nov before normalisation:  41.37786216437809
printing an ep nov before normalisation:  47.89971470674795
printing an ep nov before normalisation:  25.553347851529896
printing an ep nov before normalisation:  38.647694844539956
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.971858762886598 -1.0 -0.971858762886598
probs:  [0.11698724463474991, 0.10793804241414551, 0.17690345014464956, 0.048843265795750476, 0.34119885325479454, 0.20812914375591002]
Printing some Q and Qe and total Qs values:  [[0.382]
 [0.416]
 [0.575]
 [0.396]
 [0.575]
 [0.575]
 [0.365]] [[35.685]
 [30.786]
 [33.434]
 [33.844]
 [33.434]
 [33.434]
 [33.237]] [[2.049]
 [1.637]
 [2.036]
 [1.895]
 [2.036]
 [2.036]
 [1.809]]
printing an ep nov before normalisation:  63.34577572433915
printing an ep nov before normalisation:  38.549525430415706
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
maxi score, test score, baseline:  -0.971858762886598 -1.0 -0.971858762886598
probs:  [0.11698724463474991, 0.10793804241414551, 0.17690345014464956, 0.048843265795750476, 0.34119885325479454, 0.20812914375591002]
maxi score, test score, baseline:  -0.971858762886598 -1.0 -0.971858762886598
maxi score, test score, baseline:  -0.971858762886598 -1.0 -0.971858762886598
probs:  [0.11698724463474991, 0.10793804241414551, 0.17690345014464956, 0.048843265795750476, 0.34119885325479454, 0.20812914375591002]
printing an ep nov before normalisation:  28.835935834177207
actor:  1 policy actor:  1  step number:  54 total reward:  0.2733333333333331  reward:  1.0 rdn_beta:  1.667
using another actor
printing an ep nov before normalisation:  47.30988911220006
Printing some Q and Qe and total Qs values:  [[0.177]
 [0.25 ]
 [0.177]
 [0.177]
 [0.177]
 [0.177]
 [0.177]] [[29.796]
 [56.207]
 [29.796]
 [29.796]
 [29.796]
 [29.796]
 [29.796]] [[0.581]
 [1.357]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]]
maxi score, test score, baseline:  -0.9719308483290489 -1.0 -0.9719308483290489
probs:  [0.11642904480423481, 0.10752897395354251, 0.17534769767352007, 0.04941743980427612, 0.3452231257129633, 0.20605371805146316]
printing an ep nov before normalisation:  50.6045679846418
maxi score, test score, baseline:  -0.9719308483290489 -1.0 -0.9719308483290489
probs:  [0.11651934078005943, 0.1075949631400894, 0.17559890509643844, 0.0493247218480521, 0.3445732831188098, 0.20638878601655083]
printing an ep nov before normalisation:  36.255013942718506
printing an ep nov before normalisation:  27.821313610455796
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  56 total reward:  0.13999999999999968  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9719308483290489 -1.0 -0.9719308483290489
probs:  [0.11591111577049219, 0.10703337625157963, 0.17468193476212504, 0.049067650823511895, 0.3427732654936105, 0.21053265689868084]
Printing some Q and Qe and total Qs values:  [[0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]] [[41.61]
 [41.61]
 [41.61]
 [41.61]
 [41.61]
 [41.61]
 [41.61]] [[1.505]
 [1.505]
 [1.505]
 [1.505]
 [1.505]
 [1.505]
 [1.505]]
maxi score, test score, baseline:  -0.9719308483290489 -1.0 -0.9719308483290489
probs:  [0.11626042090409892, 0.10681617683159621, 0.17397961542509013, 0.04914343067700546, 0.34330503384220196, 0.2104953223200074]
printing an ep nov before normalisation:  37.18197834516587
maxi score, test score, baseline:  -0.9719308483290489 -1.0 -0.9719308483290489
probs:  [0.11631396390993566, 0.10686536536026514, 0.1740597711394461, 0.0491660279131996, 0.34346326077779776, 0.2101316108993557]
maxi score, test score, baseline:  -0.9719308483290489 -1.0 -0.9719308483290489
probs:  [0.11631396390993566, 0.10686536536026514, 0.1740597711394461, 0.0491660279131996, 0.34346326077779776, 0.2101316108993557]
printing an ep nov before normalisation:  37.168455423650215
maxi score, test score, baseline:  -0.9719308483290489 -1.0 -0.9719308483290489
probs:  [0.11578326700292962, 0.10700389406668992, 0.17442734338111807, 0.04910787863613682, 0.3430554767847466, 0.21062214012837907]
printing an ep nov before normalisation:  44.19606982149759
maxi score, test score, baseline:  -0.9719308483290489 -1.0 -0.9719308483290489
maxi score, test score, baseline:  -0.9719308483290489 -1.0 -0.9719308483290489
printing an ep nov before normalisation:  49.28576527699186
maxi score, test score, baseline:  -0.9719308483290489 -1.0 -0.9719308483290489
probs:  [0.11615679840833364, 0.10734906948099943, 0.17176302530442897, 0.0492660590440967, 0.3441630616132609, 0.2113019861488805]
maxi score, test score, baseline:  -0.9719308483290489 -1.0 -0.9719308483290489
probs:  [0.11615679840833364, 0.10734906948099943, 0.17176302530442897, 0.0492660590440967, 0.3441630616132609, 0.2113019861488805]
maxi score, test score, baseline:  -0.9719308483290489 -1.0 -0.9719308483290489
probs:  [0.11615679840833364, 0.10734906948099943, 0.17176302530442897, 0.0492660590440967, 0.3441630616132609, 0.2113019861488805]
Printing some Q and Qe and total Qs values:  [[0.1]
 [0.1]
 [0.1]
 [0.1]
 [0.1]
 [0.1]
 [0.1]] [[37.98]
 [37.98]
 [37.98]
 [37.98]
 [37.98]
 [37.98]
 [37.98]] [[1.676]
 [1.676]
 [1.676]
 [1.676]
 [1.676]
 [1.676]
 [1.676]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9719308483290489 -1.0 -0.9719308483290489
probs:  [0.11636732545302528, 0.10752653641598259, 0.17113247500327844, 0.04922550942644935, 0.34387850981362955, 0.21186964388763482]
maxi score, test score, baseline:  -0.9719308483290489 -1.0 -0.9719308483290489
probs:  [0.11636732545302528, 0.10752653641598259, 0.17113247500327844, 0.04922550942644935, 0.34387850981362955, 0.21186964388763482]
actor:  1 policy actor:  1  step number:  57 total reward:  0.1999999999999993  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  36.10923061235024
printing an ep nov before normalisation:  39.54036004379868
maxi score, test score, baseline:  -0.9720025641025641 -1.0 -0.9720025641025641
probs:  [0.11514957357703684, 0.10714961273455523, 0.16961466504420533, 0.04989497782984483, 0.34857044391728276, 0.20962072689707503]
maxi score, test score, baseline:  -0.9720025641025641 -1.0 -0.9720025641025641
probs:  [0.11514957357703684, 0.10714961273455523, 0.16961466504420533, 0.04989497782984483, 0.34857044391728276, 0.20962072689707503]
maxi score, test score, baseline:  -0.9720739130434782 -1.0 -0.9720739130434782
probs:  [0.11515050274219825, 0.10714921895175189, 0.16961457514179612, 0.04989496095937812, 0.34857032146525313, 0.20962042073962256]
maxi score, test score, baseline:  -0.9720739130434782 -1.0 -0.9720739130434782
maxi score, test score, baseline:  -0.9720739130434782 -1.0 -0.9720739130434782
probs:  [0.11515050274219825, 0.10714921895175189, 0.16961457514179612, 0.04989496095937812, 0.34857032146525313, 0.20962042073962256]
maxi score, test score, baseline:  -0.9720739130434782 -1.0 -0.9720739130434782
probs:  [0.11529985329598799, 0.10672256415022058, 0.16994135038795571, 0.04983173202881898, 0.34812697925014924, 0.21007752088686762]
printing an ep nov before normalisation:  42.55412370152008
printing an ep nov before normalisation:  51.921110828888175
printing an ep nov before normalisation:  46.73910182274466
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.30857086822782
maxi score, test score, baseline:  -0.9720739130434782 -1.0 -0.9720739130434782
probs:  [0.11535248392952079, 0.1067712750009545, 0.17001895194141065, 0.049854444023576895, 0.3482860108394998, 0.20971683426503732]
siam score:  -0.8258901
maxi score, test score, baseline:  -0.9720739130434782 -1.0 -0.9720739130434782
probs:  [0.11535248392952079, 0.1067712750009545, 0.17001895194141065, 0.049854444023576895, 0.3482860108394998, 0.20971683426503732]
maxi score, test score, baseline:  -0.9721448979591837 -1.0 -0.9721448979591837
probs:  [0.11540583661396975, 0.10681939820827381, 0.17009616001220054, 0.04987704989643095, 0.3484442950326258, 0.20935726023649906]
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.363]
 [0.332]
 [0.332]
 [0.332]
 [0.332]
 [0.332]] [[43.728]
 [47.179]
 [43.728]
 [43.728]
 [43.728]
 [43.728]
 [43.728]] [[1.282]
 [1.437]
 [1.282]
 [1.282]
 [1.282]
 [1.282]
 [1.282]]
Printing some Q and Qe and total Qs values:  [[0.243]
 [0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.228]] [[53.655]
 [52.324]
 [52.324]
 [52.324]
 [52.324]
 [52.324]
 [52.324]] [[2.175]
 [2.097]
 [2.097]
 [2.097]
 [2.097]
 [2.097]
 [2.097]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
siam score:  -0.82791626
maxi score, test score, baseline:  -0.9721448979591837 -1.0 -0.9721448979591837
probs:  [0.11554457028116225, 0.10693108351815588, 0.17040717505367256, 0.049809359671258414, 0.34796971496462387, 0.20933809651112692]
maxi score, test score, baseline:  -0.9721448979591837 -1.0 -0.9721448979591837
probs:  [0.11554457028116225, 0.10693108351815588, 0.17040717505367256, 0.049809359671258414, 0.34796971496462387, 0.20933809651112692]
actor:  1 policy actor:  1  step number:  39 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9721448979591837 -1.0 -0.9721448979591837
probs:  [0.11285628978721439, 0.10444343727506919, 0.18971539369741, 0.048652251755709676, 0.33986754738082237, 0.20446508010377432]
maxi score, test score, baseline:  -0.9721448979591837 -1.0 -0.9721448979591837
probs:  [0.1129060791227079, 0.10448951068655475, 0.18979913139810292, 0.048673682419042304, 0.34001760669976727, 0.20411398967382502]
maxi score, test score, baseline:  -0.9721448979591837 -1.0 -0.9721448979591837
probs:  [0.1129060791227079, 0.10448951068655475, 0.18979913139810292, 0.048673682419042304, 0.34001760669976727, 0.20411398967382502]
printing an ep nov before normalisation:  49.02945036337192
UNIT TEST: sample policy line 217 mcts : [0.102 0.245 0.163 0.204 0.082 0.122 0.082]
maxi score, test score, baseline:  -0.9721448979591837 -1.0 -0.9721448979591837
probs:  [0.1129060791227079, 0.10448951068655475, 0.18979913139810292, 0.048673682419042304, 0.34001760669976727, 0.20411398967382502]
actor:  1 policy actor:  1  step number:  59 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.107]
 [0.073]
 [0.107]
 [0.107]
 [0.101]
 [0.102]
 [0.105]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.107]
 [0.073]
 [0.107]
 [0.107]
 [0.101]
 [0.102]
 [0.105]]
maxi score, test score, baseline:  -0.9722155216284988 -1.0 -0.9722155216284988
probs:  [0.11089730565382414, 0.10262945511010561, 0.20422532786806188, 0.04780865314524432, 0.33396059435297104, 0.20047866386979302]
maxi score, test score, baseline:  -0.9722155216284988 -1.0 -0.9722155216284988
probs:  [0.11103563471550171, 0.10275745876647904, 0.20323234693836636, 0.04786819317901201, 0.3343774987048182, 0.20072886769582263]
printing an ep nov before normalisation:  52.44845720833271
maxi score, test score, baseline:  -0.9722155216284988 -1.0 -0.9722155216284988
maxi score, test score, baseline:  -0.9722155216284988 -1.0 -0.9722155216284988
probs:  [0.11045552308605927, 0.10282447849965137, 0.20336498275522194, 0.04789936695395903, 0.33459578011341845, 0.20085986859168983]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.3030],
        [-0.6170],
        [-0.6415],
        [-0.0000],
        [-0.0000],
        [-0.5951],
        [-0.0089],
        [-0.6013],
        [-0.6415],
        [-0.6369]], dtype=torch.float64)
-0.071551887066 -0.3745905790485985
-0.032346567066 -0.6493944325560527
-0.032346567066 -0.6738269471182332
-0.9042 -0.9042
0.99 0.99
-0.032346567066 -0.6274653929552811
-0.09703970119800001 -0.1059142435463708
-0.032346567066 -0.633657602005242
-0.032346567066 -0.6738382175102959
-0.032346567066 -0.6692477436021036
printing an ep nov before normalisation:  11.024595039609377
maxi score, test score, baseline:  -0.9722155216284988 -1.0 -0.9722155216284988
printing an ep nov before normalisation:  11.501736013165544
maxi score, test score, baseline:  -0.9722155216284988 -1.0 -0.9722155216284988
probs:  [0.11045552308605927, 0.10282447849965137, 0.20336498275522194, 0.04789936695395903, 0.33459578011341845, 0.20085986859168983]
maxi score, test score, baseline:  -0.9722155216284988 -1.0 -0.9722155216284988
probs:  [0.11045552308605927, 0.10282447849965137, 0.20336498275522194, 0.04789936695395903, 0.33459578011341845, 0.20085986859168983]
maxi score, test score, baseline:  -0.9722155216284988 -1.0 -0.9722155216284988
probs:  [0.11045552308605927, 0.10282447849965137, 0.20336498275522194, 0.04789936695395903, 0.33459578011341845, 0.20085986859168983]
Printing some Q and Qe and total Qs values:  [[0.271]
 [0.271]
 [0.271]
 [0.271]
 [0.275]
 [0.271]
 [0.271]] [[12.089]
 [12.089]
 [12.089]
 [12.089]
 [11.781]
 [12.089]
 [12.089]] [[0.271]
 [0.271]
 [0.271]
 [0.271]
 [0.275]
 [0.271]
 [0.271]]
maxi score, test score, baseline:  -0.9722857868020305 -1.0 -0.9722857868020305
probs:  [0.11045642368502237, 0.1028241251150671, 0.20336472523139296, 0.047899362129409824, 0.33459574220557875, 0.2008596216335289]
printing an ep nov before normalisation:  12.044340296545267
actor:  1 policy actor:  1  step number:  62 total reward:  0.2199999999999993  reward:  1.0 rdn_beta:  1.667
from probs:  [0.10996522180356408, 0.10247131493532796, 0.2011888751621065, 0.04854246836285036, 0.3391029246934226, 0.19872919504272846]
actions average: 
K:  2  action  0 :  tensor([0.2376, 0.1133, 0.1062, 0.1197, 0.1445, 0.1437, 0.1350],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0335, 0.8583, 0.0180, 0.0289, 0.0196, 0.0122, 0.0296],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1499, 0.0172, 0.2573, 0.0973, 0.1324, 0.1968, 0.1492],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1852, 0.0292, 0.0964, 0.1631, 0.2002, 0.1841, 0.1417],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2147, 0.0575, 0.0863, 0.1124, 0.2743, 0.1356, 0.1191],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1708, 0.0623, 0.1163, 0.1365, 0.1670, 0.2133, 0.1337],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1660, 0.1520, 0.0931, 0.1396, 0.1516, 0.1421, 0.1556],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9723556962025317 -1.0 -0.9723556962025317
probs:  [0.10996609813425677, 0.10247096732748012, 0.20118860868443086, 0.04854246806270736, 0.33910291858967806, 0.19872893920144685]
using explorer policy with actor:  0
actions average: 
K:  1  action  0 :  tensor([0.3582, 0.0032, 0.0933, 0.1002, 0.1857, 0.1435, 0.1159],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0370, 0.8100, 0.0231, 0.0365, 0.0269, 0.0247, 0.0418],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1881, 0.0147, 0.2256, 0.1145, 0.1603, 0.1601, 0.1367],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1932, 0.0658, 0.1025, 0.1892, 0.1544, 0.1547, 0.1401],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2036, 0.0040, 0.0796, 0.0943, 0.3877, 0.1276, 0.1033],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1728, 0.0085, 0.1245, 0.1218, 0.1697, 0.2591, 0.1435],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1768, 0.1277, 0.0923, 0.1164, 0.1409, 0.1373, 0.2086],
       grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 39.20720574751748
maxi score, test score, baseline:  -0.9723556962025317 -1.0 -0.9723556962025317
probs:  [0.11010393368967233, 0.10256996389487909, 0.20179914979835367, 0.04836201275785826, 0.3378382052952138, 0.19932673456402294]
line 256 mcts: sample exp_bonus 25.421817177451338
UNIT TEST: sample policy line 217 mcts : [0.02  0.531 0.02  0.367 0.02  0.02  0.02 ]
maxi score, test score, baseline:  -0.9723556962025317 -1.0 -0.9723556962025317
probs:  [0.11010393368967233, 0.10256996389487909, 0.20179914979835367, 0.04836201275785826, 0.3378382052952138, 0.19932673456402294]
Printing some Q and Qe and total Qs values:  [[0.88 ]
 [1.137]
 [0.88 ]
 [0.88 ]
 [0.88 ]
 [0.88 ]
 [0.88 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.88 ]
 [1.137]
 [0.88 ]
 [0.88 ]
 [0.88 ]
 [0.88 ]
 [0.88 ]]
maxi score, test score, baseline:  -0.9724252525252526 -1.0 -0.9724252525252526
probs:  [0.11010481039115366, 0.10256961654820222, 0.20179888495207476, 0.048362011975117615, 0.33783819580915536, 0.1993264803242965]
printing an ep nov before normalisation:  50.82760369622211
printing an ep nov before normalisation:  51.513798868832374
Printing some Q and Qe and total Qs values:  [[-0.009]
 [ 0.008]
 [ 0.008]
 [ 0.008]
 [ 0.008]
 [ 0.008]
 [ 0.008]] [[42.221]
 [38.734]
 [38.734]
 [38.734]
 [38.734]
 [38.734]
 [38.734]] [[1.342]
 [1.194]
 [1.194]
 [1.194]
 [1.194]
 [1.194]
 [1.194]]
siam score:  -0.8221195
Printing some Q and Qe and total Qs values:  [[0.545]
 [0.554]
 [0.537]
 [0.529]
 [0.551]
 [0.551]
 [0.554]] [[33.001]
 [30.438]
 [32.865]
 [36.241]
 [32.904]
 [32.614]
 [32.405]] [[1.799]
 [1.617]
 [1.782]
 [2.025]
 [1.798]
 [1.777]
 [1.764]]
Printing some Q and Qe and total Qs values:  [[-0.07 ]
 [-0.067]
 [-0.067]
 [-0.067]
 [-0.067]
 [-0.067]
 [-0.067]] [[19.644]
 [15.875]
 [15.875]
 [15.875]
 [15.875]
 [15.875]
 [15.875]] [[1.185]
 [0.801]
 [0.801]
 [0.801]
 [0.801]
 [0.801]
 [0.801]]
Printing some Q and Qe and total Qs values:  [[0.57]
 [0.57]
 [0.57]
 [0.57]
 [0.57]
 [0.57]
 [0.57]] [[45.246]
 [45.246]
 [45.246]
 [45.246]
 [45.246]
 [45.246]
 [45.246]] [[2.57]
 [2.57]
 [2.57]
 [2.57]
 [2.57]
 [2.57]
 [2.57]]
maxi score, test score, baseline:  -0.9724944584382872 -1.0 -0.9724944584382872
probs:  [0.11010568258689454, 0.10256927098403616, 0.2017986214663922, 0.048362011196354845, 0.337838186371496, 0.19932622739482636]
printing an ep nov before normalisation:  35.373700266182546
siam score:  -0.82479364
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.464]
 [0.5  ]
 [0.469]
 [0.473]
 [0.468]
 [0.467]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.468]
 [0.464]
 [0.5  ]
 [0.469]
 [0.473]
 [0.468]
 [0.467]]
maxi score, test score, baseline:  -0.9725633165829146 -1.0 -0.9725633165829146
probs:  [0.11029084873228957, 0.10271134670443209, 0.2024920958725575, 0.048203257235851296, 0.3367254313141617, 0.1995770201407077]
printing an ep nov before normalisation:  32.75638871107818
printing an ep nov before normalisation:  43.46557653418152
maxi score, test score, baseline:  -0.9725633165829146 -1.0 -0.9725633165829146
probs:  [0.11029084873228957, 0.10271134670443209, 0.2024920958725575, 0.048203257235851296, 0.3367254313141617, 0.1995770201407077]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9725633165829146 -1.0 -0.9725633165829146
probs:  [0.11033793257131654, 0.1027551908942458, 0.2025785885931525, 0.04822380344533401, 0.33686929837385704, 0.19923518612209426]
actor:  1 policy actor:  1  step number:  56 total reward:  0.13999999999999935  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9725633165829146 -1.0 -0.9725633165829146
probs:  [0.10884120983227409, 0.10136145161501092, 0.2133995579214591, 0.04757067108464656, 0.33229598608464883, 0.19653112346196044]
maxi score, test score, baseline:  -0.9725633165829146 -1.0 -0.9725633165829146
probs:  [0.10884120983227409, 0.10136145161501092, 0.2133995579214591, 0.04757067108464656, 0.33229598608464883, 0.19653112346196044]
printing an ep nov before normalisation:  45.633873527271376
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
maxi score, test score, baseline:  -0.9725633165829146 -1.0 -0.9725633165829146
probs:  [0.10909182345366092, 0.10158044860413096, 0.2127972817562978, 0.0475622966430367, 0.33223678775782106, 0.19673136178505252]
printing an ep nov before normalisation:  32.28220968863022
printing an ep nov before normalisation:  35.53731451254643
maxi score, test score, baseline:  -0.9725633165829146 -1.0 -0.9725633165829146
probs:  [0.10909182345366092, 0.10158044860413096, 0.2127972817562978, 0.0475622966430367, 0.33223678775782106, 0.19673136178505252]
line 256 mcts: sample exp_bonus 35.21791825593757
maxi score, test score, baseline:  -0.9725633165829146 -1.0 -0.9725633165829146
probs:  [0.10909182345366092, 0.10158044860413096, 0.2127972817562978, 0.0475622966430367, 0.33223678775782106, 0.19673136178505252]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9725633165829146 -1.0 -0.9725633165829146
probs:  [0.10909182345366092, 0.10158044860413096, 0.2127972817562978, 0.0475622966430367, 0.33223678775782106, 0.19673136178505252]
actor:  1 policy actor:  1  step number:  57 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  27.125040757475716
printing an ep nov before normalisation:  22.122925414899477
maxi score, test score, baseline:  -0.9725633165829146 -1.0 -0.9725633165829146
probs:  [0.1086935631619132, 0.1012984745147923, 0.210793521252314, 0.04811659632228846, 0.33612152138157525, 0.1949763233671168]
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.654]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]] [[50.133]
 [43.239]
 [50.133]
 [50.133]
 [50.133]
 [50.133]
 [50.133]] [[1.141]
 [1.066]
 [1.141]
 [1.141]
 [1.141]
 [1.141]
 [1.141]]
actor:  1 policy actor:  1  step number:  68 total reward:  0.05999999999999872  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]] [[49.714]
 [49.714]
 [49.714]
 [49.714]
 [49.714]
 [49.714]
 [49.714]] [[1.109]
 [1.109]
 [1.109]
 [1.109]
 [1.109]
 [1.109]
 [1.109]]
printing an ep nov before normalisation:  61.573345134427456
siam score:  -0.81770736
maxi score, test score, baseline:  -0.9725633165829146 -1.0 -0.9725633165829146
probs:  [0.12023842516968433, 0.09973434342787622, 0.20756081271816804, 0.04750482172433765, 0.33183722342854544, 0.19312437353138845]
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.26979182084955
using explorer policy with actor:  1
siam score:  -0.8145001
maxi score, test score, baseline:  -0.9726318295739349 -1.0 -0.9726318295739349
probs:  [0.11967948818116735, 0.09935631984802867, 0.20781014315165897, 0.04756178740396656, 0.33223610304348783, 0.19335615837169062]
actor:  1 policy actor:  1  step number:  62 total reward:  0.03333333333333233  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.158]
 [0.445]
 [0.558]
 [0.4  ]
 [0.409]
 [0.558]
 [0.39 ]] [[44.09 ]
 [45.173]
 [48.793]
 [42.587]
 [44.406]
 [48.793]
 [42.248]] [[0.796]
 [1.112]
 [1.324]
 [0.997]
 [1.055]
 [1.324]
 [0.977]]
maxi score, test score, baseline:  -0.9726318295739349 -1.0 -0.9726318295739349
probs:  [0.12990374382687056, 0.09820304613613036, 0.20539630660637062, 0.04701052347450821, 0.32837605892551297, 0.19111032103060743]
maxi score, test score, baseline:  -0.9726318295739349 -1.0 -0.9726318295739349
probs:  [0.12990374382687056, 0.09820304613613036, 0.20539630660637062, 0.04701052347450821, 0.32837605892551297, 0.19111032103060743]
maxi score, test score, baseline:  -0.9726318295739349 -1.0 -0.9726318295739349
probs:  [0.12935421456701313, 0.0978793698803528, 0.20573656639285964, 0.04708823086828461, 0.3289201793590129, 0.1910214389324768]
printing an ep nov before normalisation:  44.995508456727464
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.90421967753197
printing an ep nov before normalisation:  24.877309678783774
from probs:  [0.12935421456701313, 0.0978793698803528, 0.20573656639285964, 0.04708823086828461, 0.3289201793590129, 0.1910214389324768]
actor:  1 policy actor:  1  step number:  57 total reward:  0.2133333333333326  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9726318295739349 -1.0 -0.9726318295739349
probs:  [0.1268312245589352, 0.11103629478086571, 0.20290533399590172, 0.04644164346807175, 0.32439266403001127, 0.18839283916621427]
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.489]
 [0.363]
 [0.471]
 [0.473]
 [0.474]
 [0.472]] [[42.079]
 [38.723]
 [64.569]
 [42.732]
 [43.09 ]
 [43.454]
 [43.929]] [[1.33 ]
 [1.216]
 [2.03 ]
 [1.344]
 [1.358]
 [1.373]
 [1.388]]
maxi score, test score, baseline:  -0.9726318295739349 -1.0 -0.9726318295739349
probs:  [0.12693609193503352, 0.11110305358267054, 0.2031937460848938, 0.0463525541525943, 0.3237683174211419, 0.18864623682366582]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.185]
 [0.197]
 [0.206]
 [0.209]
 [0.213]
 [0.208]
 [0.21 ]] [[41.597]
 [43.684]
 [43.711]
 [45.237]
 [46.106]
 [46.114]
 [45.472]] [[0.651]
 [0.716]
 [0.726]
 [0.768]
 [0.794]
 [0.789]
 [0.775]]
maxi score, test score, baseline:  -0.9727678304239402 -1.0 -0.9727678304239402
probs:  [0.12693864171479177, 0.11110238903816165, 0.20319324661223365, 0.046352487016309424, 0.3237678399363777, 0.18864539568212582]
printing an ep nov before normalisation:  47.47773753361979
printing an ep nov before normalisation:  54.47171968677683
printing an ep nov before normalisation:  43.98963451385498
using explorer policy with actor:  0
printing an ep nov before normalisation:  49.85427760401044
printing an ep nov before normalisation:  35.15137284162562
maxi score, test score, baseline:  -0.9727678304239402 -1.0 -0.9727678304239402
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]] [[35.726]
 [35.726]
 [35.726]
 [35.726]
 [35.726]
 [35.726]
 [35.726]] [[0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]]
printing an ep nov before normalisation:  33.91349716579495
printing an ep nov before normalisation:  41.73622131347656
printing an ep nov before normalisation:  31.321469316060107
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.713]
 [0.874]
 [0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]] [[40.454]
 [35.728]
 [40.454]
 [40.454]
 [40.454]
 [40.454]
 [40.454]] [[0.713]
 [0.874]
 [0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.741]
 [0.443]
 [0.443]
 [0.509]
 [0.429]
 [0.445]] [[34.04 ]
 [32.7  ]
 [38.292]
 [39.657]
 [34.04 ]
 [38.207]
 [33.685]] [[0.509]
 [0.741]
 [0.443]
 [0.443]
 [0.509]
 [0.429]
 [0.445]]
printing an ep nov before normalisation:  49.87916946411133
line 256 mcts: sample exp_bonus 37.30081932502838
line 256 mcts: sample exp_bonus 31.528304670567046
printing an ep nov before normalisation:  34.51857137781196
printing an ep nov before normalisation:  0.011663598507993811
printing an ep nov before normalisation:  39.94474895280528
actor:  0 policy actor:  1  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  26 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  27 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.5337],
        [-0.6224],
        [-0.4249],
        [-0.0000],
        [-0.5963],
        [-0.0000],
        [-0.7355],
        [-0.0000],
        [-0.0000]], dtype=torch.float64)
-0.2524744199999997 -0.2524744199999997
-0.070771701198 -0.6044995223235025
-0.032346567066 -0.6547929450541273
-0.084359833866 -0.5092560714950275
-0.4091999999999998 -0.4091999999999998
-0.032346567066 -0.6286398983208158
-0.5874000000000004 -0.5874000000000004
-0.032346567066 -0.7678061987664879
-0.973632 -0.973632
-0.7841460000000001 -0.7841460000000001
maxi score, test score, baseline:  -0.9495314102564103 -1.0 -0.9495314102564103
maxi score, test score, baseline:  -0.9495314102564103 -1.0 -0.9495314102564103
probs:  [0.12671922745202918, 0.11085191307600198, 0.2024329263984993, 0.046446015194003626, 0.32442402341497556, 0.18912589446449032]
actor:  0 policy actor:  1  step number:  27 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  28 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  28 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.9380543357199683 -1.0 -0.9380543357199683
probs:  [0.12649405411518966, 0.11091075577213877, 0.2024773709840073, 0.04645187969371559, 0.32446573723302, 0.18920020220192868]
actor:  0 policy actor:  1  step number:  30 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  2
using another actor
printing an ep nov before normalisation:  23.14518981034693
maxi score, test score, baseline:  -0.9342492063492064 -1.0 -0.9342492063492064
probs:  [0.12578215138807644, 0.11101124725147764, 0.20263979907917914, 0.046487615237454816, 0.32471618244492984, 0.1893630045988821]
Printing some Q and Qe and total Qs values:  [[0.365]
 [0.391]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]] [[43.147]
 [44.64 ]
 [43.147]
 [43.147]
 [43.147]
 [43.147]
 [43.147]] [[0.365]
 [0.391]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]]
Printing some Q and Qe and total Qs values:  [[0.564]
 [1.143]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.564]
 [1.143]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]]
maxi score, test score, baseline:  -0.9342492063492064 -1.0 -0.9342492063492064
probs:  [0.12583239623605505, 0.11105558546846038, 0.20272077798107357, 0.04650615156165608, 0.32484597759687767, 0.18903911115587727]
maxi score, test score, baseline:  -0.934405146476643 -0.16533333333333328 -0.16533333333333328
printing an ep nov before normalisation:  38.22800439159063
maxi score, test score, baseline:  -0.934405146476643 -0.16533333333333328 -0.16533333333333328
probs:  [0.09059252617094421, 0.11891874799781467, 0.20933486931151485, 0.047368386538073216, 0.33097075816478444, 0.20281471181686855]
printing an ep nov before normalisation:  39.138794207014776
maxi score, test score, baseline:  -0.934405146476643 -0.16533333333333328 -0.16533333333333328
probs:  [0.09059252617094421, 0.11891874799781467, 0.20933486931151485, 0.047368386538073216, 0.33097075816478444, 0.20281471181686855]
printing an ep nov before normalisation:  17.421783627717097
printing an ep nov before normalisation:  18.31753468452671
from probs:  [0.09029951971438488, 0.11904085734052144, 0.20969541009527695, 0.04730181431002683, 0.3305043398922371, 0.20315805864755274]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[-0.175]
 [-0.175]
 [-0.172]
 [-0.172]
 [-0.174]
 [-0.172]
 [-0.172]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.175]
 [-0.175]
 [-0.172]
 [-0.172]
 [-0.174]
 [-0.172]
 [-0.172]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  36.22123526152958
maxi score, test score, baseline:  -0.9345603475513429 -0.16533333333333328 -0.16533333333333328
probs:  [0.0903793725122483, 0.11918936467370264, 0.2100604642807577, 0.04723216058525378, 0.33001656952857916, 0.20312206841945854]
printing an ep nov before normalisation:  23.561978733027722
printing an ep nov before normalisation:  27.384114265441895
maxi score, test score, baseline:  -0.9345603475513429 -0.16533333333333328 -0.16533333333333328
probs:  [0.09017322603417793, 0.11936205120215988, 0.20925269755555093, 0.0473004758482859, 0.3304950531267739, 0.2034164962330515]
printing an ep nov before normalisation:  14.62624690312251
printing an ep nov before normalisation:  52.67227744262887
printing an ep nov before normalisation:  56.979763795225296
maxi score, test score, baseline:  -0.9345603475513429 -0.16533333333333328 -0.16533333333333328
probs:  [0.09017322603417793, 0.11936205120215988, 0.20925269755555093, 0.0473004758482859, 0.3304950531267739, 0.2034164962330515]
printing an ep nov before normalisation:  32.11789071120529
printing an ep nov before normalisation:  33.65742977465056
printing an ep nov before normalisation:  48.03258938487705
printing an ep nov before normalisation:  57.211041421731686
maxi score, test score, baseline:  -0.9347148148148149 -0.16533333333333328 -0.16533333333333328
probs:  [0.09017322603417793, 0.11936205120215988, 0.20925269755555093, 0.0473004758482859, 0.3304950531267739, 0.2034164962330515]
maxi score, test score, baseline:  -0.9347148148148149 -0.16533333333333328 -0.16533333333333328
probs:  [0.09017322603417793, 0.11936205120215988, 0.20925269755555093, 0.0473004758482859, 0.3304950531267739, 0.2034164962330515]
maxi score, test score, baseline:  -0.9348685534591196 -0.16533333333333328 -0.16533333333333328
printing an ep nov before normalisation:  37.906795902224495
maxi score, test score, baseline:  -0.9348685534591196 -0.16533333333333328 -0.16533333333333328
maxi score, test score, baseline:  -0.9348685534591196 -0.16533333333333328 -0.16533333333333328
probs:  [0.09017322603417793, 0.11936205120215988, 0.20925269755555093, 0.0473004758482859, 0.3304950531267739, 0.2034164962330515]
printing an ep nov before normalisation:  52.307121619382855
printing an ep nov before normalisation:  62.57942715690166
maxi score, test score, baseline:  -0.9348685534591196 -0.16533333333333328 -0.16533333333333328
probs:  [0.09018870588969145, 0.11944391377573474, 0.20953899403144574, 0.04721845230406601, 0.32992041427745644, 0.20368951972160573]
printing an ep nov before normalisation:  43.44278318297512
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9348685534591196 -0.16533333333333328 -0.16533333333333328
probs:  [0.09018870588969145, 0.11944391377573474, 0.20953899403144574, 0.04721845230406601, 0.32992041427745644, 0.20368951972160573]
maxi score, test score, baseline:  -0.9348685534591196 -0.16533333333333328 -0.16533333333333328
probs:  [0.09028797490072323, 0.11957543108062026, 0.20866839563329373, 0.04727035479594679, 0.330283941830361, 0.20391390175905488]
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.881278038024902
maxi score, test score, baseline:  -0.9348685534591196 -0.16533333333333328 -0.16533333333333328
probs:  [0.09045576703554739, 0.11979773176764943, 0.20796736489842468, 0.047358084388745746, 0.3308984041236316, 0.20352264778600127]
siam score:  -0.81779116
printing an ep nov before normalisation:  47.37264738458988
maxi score, test score, baseline:  -0.9348685534591196 -0.16533333333333328 -0.16533333333333328
probs:  [0.09049035945242286, 0.11984356179445889, 0.2080469628052721, 0.04737617092569574, 0.3310250830995285, 0.20321786192262187]
maxi score, test score, baseline:  -0.935021568627451 -0.16533333333333328 -0.16533333333333328
probs:  [0.09052481540167977, 0.11988921102110517, 0.20812624669714827, 0.04739418611097828, 0.3311512623249099, 0.20291427844417853]
printing an ep nov before normalisation:  68.2431099959939
maxi score, test score, baseline:  -0.935021568627451 -0.16533333333333328 -0.16533333333333328
probs:  [0.09052481540167977, 0.11988921102110517, 0.20812624669714827, 0.04739418611097828, 0.3311512623249099, 0.20291427844417853]
printing an ep nov before normalisation:  49.98531216684367
maxi score, test score, baseline:  -0.935021568627451 -0.16533333333333328 -0.16533333333333328
probs:  [0.09052481540167977, 0.11988921102110517, 0.20812624669714827, 0.04739418611097828, 0.3311512623249099, 0.20291427844417853]
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.3649197230923
maxi score, test score, baseline:  -0.935021568627451 -0.16533333333333328 -0.16533333333333328
probs:  [0.09052481540167977, 0.11988921102110517, 0.20812624669714827, 0.04739418611097828, 0.3311512623249099, 0.20291427844417853]
Printing some Q and Qe and total Qs values:  [[-0.125]
 [-0.12 ]
 [-0.125]
 [-0.125]
 [-0.125]
 [-0.125]
 [-0.125]] [[38.763]
 [47.49 ]
 [38.763]
 [38.763]
 [38.763]
 [38.763]
 [38.763]] [[0.314]
 [0.526]
 [0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]]
maxi score, test score, baseline:  -0.935021568627451 -0.16533333333333328 -0.16533333333333328
probs:  [0.09052481540167977, 0.11988921102110517, 0.20812624669714827, 0.04739418611097828, 0.3311512623249099, 0.20291427844417853]
printing an ep nov before normalisation:  24.710177173115156
siam score:  -0.8251431
maxi score, test score, baseline:  -0.935021568627451 -0.16533333333333328 -0.16533333333333328
probs:  [0.09052481540167977, 0.11988921102110517, 0.20812624669714827, 0.04739418611097828, 0.3311512623249099, 0.20291427844417853]
maxi score, test score, baseline:  -0.935021568627451 -0.16533333333333328 -0.16533333333333328
probs:  [0.09054106476402221, 0.11997211893353157, 0.20840945680336398, 0.04731252692958676, 0.3305791754305606, 0.20318565713893497]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[ 0.1506],
        [-0.0000],
        [-0.6328],
        [-0.6604],
        [-0.6336],
        [-0.4726],
        [-0.6395],
        [-0.4889],
        [-0.2887],
        [-0.6296]], dtype=torch.float64)
-0.045026434398 0.10552356670628418
-0.9481788029999999 -0.9481788029999999
-0.032346567066 -0.6651495289982232
-0.032346567066 -0.6927838723438399
-0.032346567066 -0.6659450602754724
-0.09703970119800001 -0.5695966236386861
-0.071551887066 -0.7110055293992182
-0.09703970119800001 -0.585983818437167
-0.058351887066 -0.3470970511417397
-0.032346567066 -0.6619725292801334
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9351738654147105 -0.16533333333333328 -0.16533333333333328
printing an ep nov before normalisation:  57.96431634040932
maxi score, test score, baseline:  -0.9351738654147105 -0.16533333333333328 -0.16533333333333328
probs:  [0.09054106476402221, 0.11997211893353157, 0.20840945680336398, 0.04731252692958676, 0.3305791754305606, 0.20318565713893497]
printing an ep nov before normalisation:  46.84489264548384
Printing some Q and Qe and total Qs values:  [[-0.144]
 [-0.138]
 [-0.147]
 [-0.144]
 [-0.144]
 [-0.151]
 [-0.153]] [[50.562]
 [47.273]
 [51.428]
 [50.562]
 [50.562]
 [50.777]
 [50.716]] [[1.47 ]
 [1.274]
 [1.52 ]
 [1.47 ]
 [1.47 ]
 [1.476]
 [1.47 ]]
printing an ep nov before normalisation:  37.020333789252426
printing an ep nov before normalisation:  40.5842040850145
maxi score, test score, baseline:  -0.9351738654147105 -0.16533333333333328 -0.16533333333333328
probs:  [0.09062907045402227, 0.11911612194219692, 0.20861219910690007, 0.04735845266962381, 0.3309008413874139, 0.20338331443984298]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9351738654147105 -0.16533333333333328 -0.16533333333333328
probs:  [0.09062907045402227, 0.11911612194219692, 0.20861219910690007, 0.04735845266962381, 0.3309008413874139, 0.20338331443984298]
printing an ep nov before normalisation:  31.823959350585938
maxi score, test score, baseline:  -0.9351738654147105 -0.16533333333333328 -0.16533333333333328
probs:  [0.09062907045402227, 0.11911612194219692, 0.20861219910690007, 0.04735845266962381, 0.3309008413874139, 0.20338331443984298]
printing an ep nov before normalisation:  30.119994952878812
maxi score, test score, baseline:  -0.9351738654147105 -0.16533333333333328 -0.16533333333333328
probs:  [0.09062907045402227, 0.11911612194219692, 0.20861219910690007, 0.04735845266962381, 0.3309008413874139, 0.20338331443984298]
maxi score, test score, baseline:  -0.9351738654147105 -0.16533333333333328 -0.16533333333333328
probs:  [0.09064548110640681, 0.11919690387648482, 0.20889521247306522, 0.047277086086737495, 0.33033080419525035, 0.20365451226205536]
maxi score, test score, baseline:  -0.9351738654147105 -0.16533333333333328 -0.16533333333333328
probs:  [0.09064548110640681, 0.11919690387648482, 0.20889521247306522, 0.047277086086737495, 0.33033080419525035, 0.20365451226205536]
printing an ep nov before normalisation:  33.44852265964408
printing an ep nov before normalisation:  40.41095205120566
line 256 mcts: sample exp_bonus 43.99529868538519
actions average: 
K:  0  action  0 :  tensor([0.2771, 0.0420, 0.1381, 0.1326, 0.1376, 0.1292, 0.1435],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0334, 0.8333, 0.0241, 0.0275, 0.0152, 0.0095, 0.0570],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1976, 0.0010, 0.2230, 0.1353, 0.1591, 0.1477, 0.1363],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2079, 0.0140, 0.1502, 0.1409, 0.1688, 0.1560, 0.1621],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2538, 0.0331, 0.1201, 0.1122, 0.2518, 0.1071, 0.1218],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.2030, 0.0183, 0.1706, 0.1312, 0.1491, 0.1691, 0.1586],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2219, 0.0093, 0.1784, 0.1509, 0.1395, 0.1412, 0.1590],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  52 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9351738654147105 -0.16533333333333328 -0.16533333333333328
probs:  [0.09051480665277468, 0.11855365491361179, 0.2066416386280005, 0.04792499047916797, 0.3348698863042714, 0.20149502302217367]
printing an ep nov before normalisation:  27.193647629599724
actor:  1 policy actor:  1  step number:  64 total reward:  0.0599999999999995  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  24.393033981323242
maxi score, test score, baseline:  -0.9353254488680719 -0.16533333333333328 -0.16533333333333328
probs:  [0.09060543571332456, 0.11865253311333045, 0.20570369518353257, 0.04800308944869837, 0.33541694364330366, 0.20161830289781044]
Printing some Q and Qe and total Qs values:  [[-0.126]
 [-0.117]
 [-0.126]
 [-0.126]
 [-0.126]
 [-0.126]
 [-0.126]] [[29.432]
 [31.575]
 [29.432]
 [29.432]
 [29.432]
 [29.432]
 [29.432]] [[0.39 ]
 [0.471]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]]
actions average: 
K:  3  action  0 :  tensor([0.2924, 0.0250, 0.1269, 0.1157, 0.1407, 0.1504, 0.1488],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0318, 0.7757, 0.0364, 0.0524, 0.0303, 0.0327, 0.0406],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1763, 0.0775, 0.2383, 0.1060, 0.1117, 0.1568, 0.1335],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1775, 0.0413, 0.1070, 0.2117, 0.1744, 0.1380, 0.1501],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1877, 0.1038, 0.1229, 0.1372, 0.1560, 0.1375, 0.1550],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1830, 0.0588, 0.1590, 0.1541, 0.1358, 0.1560, 0.1533],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1859, 0.0558, 0.1350, 0.1495, 0.1486, 0.1633, 0.1619],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  28.421825181394443
actor:  1 policy actor:  1  step number:  53 total reward:  0.2933333333333331  reward:  1.0 rdn_beta:  1.0
from probs:  [0.08907708256258687, 0.1166503784474928, 0.21911054339006597, 0.047194420617486285, 0.32975297311594925, 0.19821460186641876]
maxi score, test score, baseline:  -0.9353254488680719 -0.16533333333333328 -0.16533333333333328
probs:  [0.0891747633958036, 0.116283981951862, 0.21919856159030363, 0.04720575063478656, 0.3298327696078393, 0.19830417281940485]
printing an ep nov before normalisation:  44.63911712666558
printing an ep nov before normalisation:  42.110713009248734
printing an ep nov before normalisation:  55.625922883667755
actions average: 
K:  3  action  0 :  tensor([0.2340, 0.0138, 0.1153, 0.1197, 0.1878, 0.1700, 0.1593],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0224, 0.8561, 0.0247, 0.0281, 0.0141, 0.0202, 0.0344],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1404, 0.0445, 0.2750, 0.1023, 0.1288, 0.1731, 0.1358],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2052, 0.0917, 0.1087, 0.1503, 0.1487, 0.1435, 0.1519],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1833, 0.0080, 0.0860, 0.0979, 0.3754, 0.1308, 0.1186],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1924, 0.0598, 0.1431, 0.1048, 0.1562, 0.2017, 0.1420],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1546, 0.0113, 0.1007, 0.1068, 0.1369, 0.1398, 0.3500],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  28.032726092191254
printing an ep nov before normalisation:  27.86153258435366
maxi score, test score, baseline:  -0.9353254488680719 -0.16533333333333328 -0.16533333333333328
probs:  [0.08921545250525555, 0.11588045664504854, 0.21929866738050377, 0.04722726125947523, 0.3299834316616534, 0.19839473054806347]
maxi score, test score, baseline:  -0.9353254488680719 -0.16533333333333328 -0.16533333333333328
probs:  [0.08932976164005295, 0.11608427324121587, 0.2187130625748873, 0.04720062699196821, 0.32979674952006927, 0.19887552603180644]
Printing some Q and Qe and total Qs values:  [[-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]] [[34.348]
 [34.348]
 [34.348]
 [34.348]
 [34.348]
 [34.348]
 [34.348]] [[1.055]
 [1.055]
 [1.055]
 [1.055]
 [1.055]
 [1.055]
 [1.055]]
maxi score, test score, baseline:  -0.935476323987539 -0.16533333333333328 -0.16533333333333328
probs:  [0.08932976164005295, 0.11608427324121587, 0.2187130625748873, 0.04720062699196821, 0.32979674952006927, 0.19887552603180644]
actor:  1 policy actor:  1  step number:  57 total reward:  0.33333333333333315  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  69.66253786786689
maxi score, test score, baseline:  -0.935476323987539 -0.16533333333333328 -0.16533333333333328
printing an ep nov before normalisation:  48.095085360475416
maxi score, test score, baseline:  -0.935476323987539 -0.16533333333333328 -0.16533333333333328
probs:  [0.08919504654933406, 0.1153451357125126, 0.21565539484906607, 0.04801766892278507, 0.33552073751862793, 0.19626601644767414]
Printing some Q and Qe and total Qs values:  [[-0.145]
 [-0.115]
 [-0.145]
 [-0.145]
 [-0.145]
 [-0.145]
 [-0.145]] [[26.343]
 [34.92 ]
 [26.343]
 [26.343]
 [26.343]
 [26.343]
 [26.343]] [[0.508]
 [0.939]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.938515351284465
actor:  1 policy actor:  1  step number:  49 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  33.72827237420587
siam score:  -0.8248618
printing an ep nov before normalisation:  46.46212067338235
maxi score, test score, baseline:  -0.9356264957264958 -0.16533333333333328 -0.16533333333333328
probs:  [0.0888357793984839, 0.1142391676289652, 0.2125801373055094, 0.04895504181866796, 0.34208764736497993, 0.19330222648339368]
printing an ep nov before normalisation:  76.981951873907
printing an ep nov before normalisation:  54.82840061187744
printing an ep nov before normalisation:  61.48229743385432
maxi score, test score, baseline:  -0.9356264957264958 -0.16533333333333328 -0.16533333333333328
Printing some Q and Qe and total Qs values:  [[-0.006]
 [-0.014]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]] [[32.093]
 [42.74 ]
 [32.093]
 [32.093]
 [32.093]
 [32.093]
 [32.093]] [[1.009]
 [1.63 ]
 [1.009]
 [1.009]
 [1.009]
 [1.009]
 [1.009]]
actor:  1 policy actor:  1  step number:  69 total reward:  0.1999999999999993  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  34.93930650797941
maxi score, test score, baseline:  -0.9356264957264958 -0.16533333333333328 -0.16533333333333328
probs:  [0.08804894992960578, 0.11279060378363027, 0.22021006333733817, 0.04850223676568509, 0.33891634480475763, 0.19153180137898307]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
maxi score, test score, baseline:  -0.9356264957264958 -0.16533333333333328 -0.16533333333333328
maxi score, test score, baseline:  -0.9356264957264958 -0.16533333333333328 -0.16533333333333328
probs:  [0.0880594321793024, 0.11285647513321995, 0.22051641445480372, 0.04842418585349998, 0.33836954186072343, 0.19177395051845056]
maxi score, test score, baseline:  -0.9356264957264958 -0.16533333333333328 -0.16533333333333328
probs:  [0.0880594321793024, 0.11285647513321995, 0.22051641445480372, 0.04842418585349998, 0.33836954186072343, 0.19177395051845056]
actor:  1 policy actor:  1  step number:  53 total reward:  0.26666666666666605  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9356264957264958 -0.16533333333333328 -0.16533333333333328
probs:  [0.08770417689213987, 0.11240103036815881, 0.21851390976557064, 0.04822907202526647, 0.3370029385975812, 0.1961488723512831]
printing an ep nov before normalisation:  57.170504554522154
printing an ep nov before normalisation:  19.18928861618042
siam score:  -0.820236
printing an ep nov before normalisation:  46.43618106842041
printing an ep nov before normalisation:  27.50108462675191
printing an ep nov before normalisation:  38.68681490421295
maxi score, test score, baseline:  -0.9356264957264958 -0.16533333333333328 -0.16533333333333328
probs:  [0.08796907702407952, 0.11279284929046705, 0.21725713086134038, 0.04829110693940656, 0.3374373159793165, 0.19625251990538992]
printing an ep nov before normalisation:  38.753692889290505
printing an ep nov before normalisation:  28.628252190770606
printing an ep nov before normalisation:  29.44539521279539
maxi score, test score, baseline:  -0.9357759689922481 -0.16533333333333328 -0.16533333333333328
probs:  [0.08808999985904112, 0.11293443401203596, 0.2164081240725792, 0.04833798848687462, 0.33776588189165185, 0.19646357167781728]
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.040798769748605
maxi score, test score, baseline:  -0.9357759689922481 -0.16533333333333328 -0.16533333333333328
probs:  [0.08812144285247792, 0.11297475845203359, 0.2164854385298606, 0.04835522083810025, 0.3378865796046828, 0.19617655972284484]
printing an ep nov before normalisation:  35.13538360595703
siam score:  -0.8152268
Printing some Q and Qe and total Qs values:  [[-0.115]
 [-0.147]
 [-0.139]
 [-0.138]
 [-0.136]
 [-0.123]
 [-0.117]] [[15.183]
 [14.657]
 [13.774]
 [13.768]
 [13.614]
 [15.098]
 [15.83 ]] [[0.81 ]
 [0.746]
 [0.699]
 [0.7  ]
 [0.692]
 [0.797]
 [0.848]]
printing an ep nov before normalisation:  13.905680957533676
printing an ep nov before normalisation:  48.831514632123856
printing an ep nov before normalisation:  44.58268442246155
maxi score, test score, baseline:  -0.9357759689922481 -0.16533333333333328 -0.16533333333333328
probs:  [0.08788417669530067, 0.11311523055272571, 0.2169240058553434, 0.048309598367909344, 0.337566910964895, 0.19620007756382582]
maxi score, test score, baseline:  -0.9357759689922481 -0.16533333333333328 -0.16533333333333328
probs:  [0.08788417669530067, 0.11311523055272571, 0.2169240058553434, 0.048309598367909344, 0.337566910964895, 0.19620007756382582]
actions average: 
K:  0  action  0 :  tensor([0.3511, 0.0628, 0.1169, 0.1014, 0.1424, 0.1054, 0.1199],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0160, 0.9095, 0.0146, 0.0157, 0.0116, 0.0094, 0.0232],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1777, 0.0053, 0.2586, 0.1140, 0.1333, 0.1846, 0.1264],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1867, 0.0490, 0.1208, 0.2172, 0.1353, 0.1316, 0.1594],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2067, 0.0103, 0.1170, 0.1733, 0.2412, 0.1191, 0.1323],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1422, 0.0100, 0.1893, 0.0994, 0.1192, 0.3307, 0.1093],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1846, 0.0427, 0.1341, 0.1878, 0.1428, 0.1532, 0.1548],
       grad_fn=<DivBackward0>)
UNIT TEST: sample policy line 217 mcts : [0.265 0.367 0.02  0.02  0.122 0.184 0.02 ]
printing an ep nov before normalisation:  37.226247787475586
printing an ep nov before normalisation:  37.89976093346421
maxi score, test score, baseline:  -0.9357759689922481 -0.16533333333333328 -0.16533333333333328
probs:  [0.08788417669530067, 0.11311523055272571, 0.2169240058553434, 0.048309598367909344, 0.337566910964895, 0.19620007756382582]
maxi score, test score, baseline:  -0.9357759689922481 -0.16533333333333328 -0.16533333333333328
probs:  [0.08788417669530067, 0.11311523055272571, 0.2169240058553434, 0.048309598367909344, 0.337566910964895, 0.19620007756382582]
maxi score, test score, baseline:  -0.9359247486465584 -0.16533333333333328 -0.16533333333333328
Printing some Q and Qe and total Qs values:  [[-0.012]
 [-0.01 ]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.013]] [[30.283]
 [32.063]
 [36.759]
 [36.32 ]
 [36.34 ]
 [35.892]
 [29.296]] [[1.144]
 [1.286]
 [1.661]
 [1.627]
 [1.628]
 [1.593]
 [1.065]]
maxi score, test score, baseline:  -0.9359247486465584 -0.16533333333333328 -0.16533333333333328
probs:  [0.08797804952598297, 0.11323609466325422, 0.21608701174422015, 0.04836113572879846, 0.3379278854628927, 0.19640982287485154]
maxi score, test score, baseline:  -0.9359247486465584 -0.16533333333333328 -0.16533333333333328
probs:  [0.08808136965424648, 0.11342200302017816, 0.21554989374607078, 0.048334917299437265, 0.33774412532826026, 0.1968676909518071]
maxi score, test score, baseline:  -0.9359247486465584 -0.16533333333333328 -0.16533333333333328
probs:  [0.08808136965424648, 0.11342200302017816, 0.21554989374607078, 0.048334917299437265, 0.33774412532826026, 0.1968676909518071]
printing an ep nov before normalisation:  21.517693520527168
maxi score, test score, baseline:  -0.9359247486465584 -0.16533333333333328 -0.16533333333333328
probs:  [0.08808136965424648, 0.11342200302017816, 0.21554989374607078, 0.048334917299437265, 0.33774412532826026, 0.1968676909518071]
siam score:  -0.81582963
maxi score, test score, baseline:  -0.9359247486465584 -0.16533333333333328 -0.16533333333333328
probs:  [0.08808136965424648, 0.11342200302017816, 0.21554989374607078, 0.048334917299437265, 0.33774412532826026, 0.1968676909518071]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9359247486465584 -0.16533333333333328 -0.16533333333333328
probs:  [0.08780462082617685, 0.11345640900115832, 0.21561531612939552, 0.04834955622928379, 0.3378466582934643, 0.19692743952052125]
printing an ep nov before normalisation:  25.44407734897177
actor:  1 policy actor:  1  step number:  41 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  51.01203623766091
printing an ep nov before normalisation:  49.76820233224926
maxi score, test score, baseline:  -0.9360728395061729 -0.16533333333333328 -0.16533333333333328
probs:  [0.08556255666489242, 0.13643191305598115, 0.21062370825254995, 0.04695627412533877, 0.32808769095314433, 0.19233785694809336]
Printing some Q and Qe and total Qs values:  [[0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]] [[35.841]
 [35.841]
 [35.841]
 [35.841]
 [35.841]
 [35.841]
 [35.841]] [[1.665]
 [1.665]
 [1.665]
 [1.665]
 [1.665]
 [1.665]
 [1.665]]
printing an ep nov before normalisation:  34.746646652350904
printing an ep nov before normalisation:  34.97783909467515
maxi score, test score, baseline:  -0.9360728395061729 -0.16533333333333328 -0.16533333333333328
probs:  [0.08556255666489242, 0.13643191305598115, 0.21062370825254995, 0.04695627412533877, 0.32808769095314433, 0.19233785694809336]
printing an ep nov before normalisation:  48.80862236022949
actor:  1 policy actor:  1  step number:  60 total reward:  0.12666666666666604  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9360728395061729 -0.16533333333333328 -0.16533333333333328
probs:  [0.08528355069205129, 0.1361687700366874, 0.20995872300144588, 0.04717775464622602, 0.3296392889574571, 0.1917719126661324]
from probs:  [0.08537045442182592, 0.13630760680848494, 0.20915309313458724, 0.0472257679097149, 0.32997558017120693, 0.1919674975541801]
Printing some Q and Qe and total Qs values:  [[0.145]
 [0.128]
 [0.132]
 [0.137]
 [0.133]
 [0.147]
 [0.136]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.145]
 [0.128]
 [0.132]
 [0.137]
 [0.133]
 [0.147]
 [0.136]]
line 256 mcts: sample exp_bonus 42.672498534784246
Printing some Q and Qe and total Qs values:  [[-0.138]
 [-0.125]
 [-0.138]
 [-0.136]
 [-0.136]
 [-0.137]
 [-0.136]] [[30.924]
 [33.272]
 [30.497]
 [30.593]
 [30.861]
 [31.112]
 [30.777]] [[1.263]
 [1.502]
 [1.221]
 [1.233]
 [1.259]
 [1.282]
 [1.25 ]]
maxi score, test score, baseline:  -0.9362202463433411 -0.16533333333333328 -0.16533333333333328
probs:  [0.0854564854881055, 0.13644504941937785, 0.20835555318259374, 0.04727329903713858, 0.33030849444052973, 0.19216111843225456]
maxi score, test score, baseline:  -0.9362202463433411 -0.16533333333333328 -0.16533333333333328
probs:  [0.0854564854881055, 0.13644504941937785, 0.20835555318259374, 0.04727329903713858, 0.33030849444052973, 0.19216111843225456]
using explorer policy with actor:  1
printing an ep nov before normalisation:  29.583325392799864
maxi score, test score, baseline:  -0.9362202463433411 -0.16533333333333328 -0.16533333333333328
probs:  [0.0855157718547918, 0.13653976489112665, 0.20850023524677996, 0.04730605404031727, 0.33053791479180183, 0.19160025917518259]
actions average: 
K:  4  action  0 :  tensor([0.2820, 0.1121, 0.0955, 0.1210, 0.1416, 0.1155, 0.1323],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0910, 0.6362, 0.0653, 0.0528, 0.0400, 0.0407, 0.0741],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2116, 0.1290, 0.2475, 0.0902, 0.0825, 0.0978, 0.1414],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2058, 0.1486, 0.0958, 0.1529, 0.1362, 0.1336, 0.1272],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2067, 0.0167, 0.1091, 0.1715, 0.2106, 0.1501, 0.1353],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1131, 0.1619, 0.1263, 0.1343, 0.1334, 0.2252, 0.1059],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2047, 0.0873, 0.1318, 0.1297, 0.1452, 0.1495, 0.1517],
       grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 18.121323585510254
maxi score, test score, baseline:  -0.9363669738863287 -0.16533333333333328 -0.16533333333333328
probs:  [0.0855157718547918, 0.13653976489112665, 0.20850023524677996, 0.04730605404031727, 0.33053791479180183, 0.19160025917518259]
maxi score, test score, baseline:  -0.9363669738863287 -0.16533333333333328 -0.16533333333333328
probs:  [0.08560106179857115, 0.13667602349005936, 0.20771025931147605, 0.04735317570649028, 0.3308679611406592, 0.19179151855274393]
maxi score, test score, baseline:  -0.9363669738863287 -0.16533333333333328 -0.16533333333333328
maxi score, test score, baseline:  -0.9363669738863287 -0.16533333333333328 -0.16533333333333328
probs:  [0.08560106179857115, 0.13667602349005936, 0.20771025931147605, 0.04735317570649028, 0.3308679611406592, 0.19179151855274393]
maxi score, test score, baseline:  -0.9363669738863287 -0.16533333333333328 -0.16533333333333328
maxi score, test score, baseline:  -0.9363669738863287 -0.16533333333333328 -0.16533333333333328
printing an ep nov before normalisation:  31.21881579376992
actions average: 
K:  3  action  0 :  tensor([0.1777, 0.0630, 0.1423, 0.1586, 0.1356, 0.1656, 0.1572],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0171, 0.8241, 0.0156, 0.0517, 0.0264, 0.0080, 0.0572],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1789, 0.0303, 0.1643, 0.1282, 0.1268, 0.2131, 0.1583],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1436, 0.0616, 0.1415, 0.1959, 0.1331, 0.1697, 0.1545],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2092, 0.1235, 0.1353, 0.1087, 0.1818, 0.1153, 0.1262],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1690, 0.1171, 0.1386, 0.1368, 0.1354, 0.1576, 0.1453],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2424, 0.0443, 0.1491, 0.1177, 0.1316, 0.1474, 0.1675],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  24.46988765712873
maxi score, test score, baseline:  -0.9363669738863287 -0.16533333333333328 -0.16533333333333328
maxi score, test score, baseline:  -0.9363669738863287 -0.16533333333333328 -0.16533333333333328
probs:  [0.08565414409398184, 0.13630186410569392, 0.20809252616114918, 0.0473031485425716, 0.33051744577181374, 0.19213087132478965]
printing an ep nov before normalisation:  0.1526276660385406
actions average: 
K:  1  action  0 :  tensor([0.3171, 0.0131, 0.1372, 0.1348, 0.1308, 0.1442, 0.1227],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0249, 0.8647, 0.0213, 0.0236, 0.0135, 0.0136, 0.0384],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1515, 0.0296, 0.3829, 0.0933, 0.1204, 0.1250, 0.0973],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1734, 0.0379, 0.0921, 0.2936, 0.1597, 0.1236, 0.1198],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2149, 0.0072, 0.1275, 0.1501, 0.1923, 0.1592, 0.1488],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1684, 0.0058, 0.1243, 0.1474, 0.1359, 0.2716, 0.1465],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2360, 0.0182, 0.1161, 0.1361, 0.1110, 0.1345, 0.2482],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9366584097859327 -0.16533333333333328 -0.16533333333333328
probs:  [0.08538945941729911, 0.13634131091717358, 0.208152770072219, 0.0473168132384116, 0.33061315506476713, 0.19218649129012966]
from probs:  [0.08538945941729911, 0.13634131091717358, 0.208152770072219, 0.0473168132384116, 0.33061315506476713, 0.19218649129012966]
printing an ep nov before normalisation:  35.33020014693118
maxi score, test score, baseline:  -0.9366584097859327 -0.16533333333333328 -0.16533333333333328
probs:  [0.08539374762088302, 0.13645361338594916, 0.20841730767602182, 0.04724039016694157, 0.33007775962039565, 0.19241718152980877]
maxi score, test score, baseline:  -0.9366584097859327 -0.16533333333333328 -0.16533333333333328
probs:  [0.08539374762088302, 0.13645361338594916, 0.20841730767602182, 0.04724039016694157, 0.33007775962039565, 0.19241718152980877]
maxi score, test score, baseline:  -0.9366584097859327 -0.16533333333333328 -0.16533333333333328
probs:  [0.08547841600677673, 0.13658898702644623, 0.20763188403905888, 0.04728717017074321, 0.33040541268593765, 0.1926081300710372]
maxi score, test score, baseline:  -0.9366584097859327 -0.16533333333333328 -0.16533333333333328
probs:  [0.08547841600677673, 0.13658898702644623, 0.20763188403905888, 0.04728717017074321, 0.33040541268593765, 0.1926081300710372]
maxi score, test score, baseline:  -0.9366584097859327 -0.16533333333333328 -0.16533333333333328
probs:  [0.08547841600677673, 0.13658898702644623, 0.20763188403905888, 0.04728717017074321, 0.33040541268593765, 0.1926081300710372]
maxi score, test score, baseline:  -0.9366584097859327 -0.16533333333333328 -0.16533333333333328
probs:  [0.08547841600677673, 0.13658898702644623, 0.20763188403905888, 0.04728717017074321, 0.33040541268593765, 0.1926081300710372]
maxi score, test score, baseline:  -0.9366584097859327 -0.16533333333333328 -0.16533333333333328
probs:  [0.08547841600677673, 0.13658898702644623, 0.20763188403905888, 0.04728717017074321, 0.33040541268593765, 0.1926081300710372]
printing an ep nov before normalisation:  37.99624295114667
printing an ep nov before normalisation:  30.9722900390625
maxi score, test score, baseline:  -0.9368031273836767 -0.16533333333333328 -0.16533333333333328
probs:  [0.08547841600677673, 0.13658898702644623, 0.20763188403905888, 0.04728717017074321, 0.33040541268593765, 0.1926081300710372]
printing an ep nov before normalisation:  40.43252336960914
printing an ep nov before normalisation:  40.38767987638974
printing an ep nov before normalisation:  33.4859913080626
printing an ep nov before normalisation:  14.707880331094186
maxi score, test score, baseline:  -0.9368031273836767 -0.16533333333333328 -0.16533333333333328
probs:  [0.08550457314150692, 0.13660287556089662, 0.20762871941036168, 0.04728248268322762, 0.3303727774542349, 0.19260857174977222]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9368031273836767 -0.16533333333333328 -0.16533333333333328
probs:  [0.08550457314150692, 0.13660287556089662, 0.20762871941036168, 0.04728248268322762, 0.3303727774542349, 0.19260857174977222]
Printing some Q and Qe and total Qs values:  [[-0.186]
 [-0.177]
 [-0.185]
 [-0.185]
 [-0.186]
 [-0.173]
 [-0.173]] [[28.854]
 [40.602]
 [27.585]
 [15.654]
 [15.159]
 [24.458]
 [24.458]] [[-0.019]
 [ 0.084]
 [-0.029]
 [-0.125]
 [-0.129]
 [-0.042]
 [-0.042]]
printing an ep nov before normalisation:  41.6157341003418
actor:  0 policy actor:  1  step number:  57 total reward:  0.1733333333333329  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.504]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]] [[23.67 ]
 [31.519]
 [23.67 ]
 [23.67 ]
 [23.67 ]
 [23.67 ]
 [23.67 ]] [[0.462]
 [0.504]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]]
maxi score, test score, baseline:  -0.9342683409436834 -0.16533333333333328 -0.16533333333333328
probs:  [0.08550457314150692, 0.13660287556089662, 0.20762871941036168, 0.04728248268322762, 0.3303727774542349, 0.19260857174977222]
siam score:  -0.8104062
printing an ep nov before normalisation:  33.89223308349741
maxi score, test score, baseline:  -0.9342683409436834 -0.16533333333333328 -0.16533333333333328
probs:  [0.0852425023669771, 0.13664201274640084, 0.20768822560575628, 0.04729600429181878, 0.33046748456801045, 0.19266377042103655]
maxi score, test score, baseline:  -0.9342683409436834 -0.16533333333333328 -0.16533333333333328
probs:  [0.0852425023669771, 0.13664201274640084, 0.20768822560575628, 0.04729600429181878, 0.33046748456801045, 0.19266377042103655]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9342683409436834 -0.16533333333333328 -0.16533333333333328
printing an ep nov before normalisation:  40.101113800914945
actions average: 
K:  1  action  0 :  tensor([0.2953, 0.0058, 0.1094, 0.1191, 0.1818, 0.1437, 0.1449],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0288, 0.8417, 0.0205, 0.0249, 0.0221, 0.0183, 0.0437],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1256, 0.0240, 0.2906, 0.0930, 0.1268, 0.2076, 0.1324],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1823, 0.0343, 0.1167, 0.2088, 0.1508, 0.1423, 0.1648],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2032, 0.0264, 0.1098, 0.1330, 0.2351, 0.1497, 0.1428],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2044, 0.0351, 0.1347, 0.1121, 0.1480, 0.2264, 0.1393],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2095, 0.0071, 0.1208, 0.1270, 0.1573, 0.1392, 0.2392],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9345666666666667 -0.16533333333333328 -0.16533333333333328
probs:  [0.0852425023669771, 0.13664201274640084, 0.20768822560575628, 0.04729600429181878, 0.33046748456801045, 0.19266377042103655]
maxi score, test score, baseline:  -0.9345666666666667 -0.16533333333333328 -0.16533333333333328
probs:  [0.0852425023669771, 0.13664201274640084, 0.20768822560575628, 0.04729600429181878, 0.33046748456801045, 0.19266377042103655]
Printing some Q and Qe and total Qs values:  [[-0.169]
 [-0.169]
 [-0.169]
 [-0.169]
 [-0.169]
 [-0.169]
 [-0.169]] [[47.419]
 [47.419]
 [47.419]
 [47.419]
 [47.419]
 [47.419]
 [47.419]] [[1.396]
 [1.396]
 [1.396]
 [1.396]
 [1.396]
 [1.396]
 [1.396]]
maxi score, test score, baseline:  -0.9345666666666667 -0.16533333333333328 -0.16533333333333328
probs:  [0.08524646245255814, 0.1367544818094387, 0.20795067959769017, 0.047219855915785924, 0.3299340137842942, 0.19289450644023287]
maxi score, test score, baseline:  -0.9345666666666667 -0.16533333333333328 -0.16533333333333328
probs:  [0.08524646245255814, 0.1367544818094387, 0.20795067959769017, 0.047219855915785924, 0.3299340137842942, 0.19289450644023287]
maxi score, test score, baseline:  -0.9345666666666667 -0.16533333333333328 -0.16533333333333328
probs:  [0.08524646245255814, 0.1367544818094387, 0.20795067959769017, 0.047219855915785924, 0.3299340137842942, 0.19289450644023287]
maxi score, test score, baseline:  -0.9345666666666667 -0.16533333333333328 -0.16533333333333328
Printing some Q and Qe and total Qs values:  [[-0.12 ]
 [-0.076]
 [-0.111]
 [-0.111]
 [-0.111]
 [-0.111]
 [-0.111]] [[14.465]
 [16.883]
 [14.139]
 [14.139]
 [14.139]
 [14.139]
 [14.139]] [[0.571]
 [0.731]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]]
maxi score, test score, baseline:  -0.9345666666666667 -0.16533333333333328 -0.16533333333333328
probs:  [0.08525041031048124, 0.1368666035988634, 0.20821232320369568, 0.04714394266504856, 0.3294021902120117, 0.19312453000989943]
from probs:  [0.08536031412012146, 0.1370151419364947, 0.2074288285650941, 0.047185697465373544, 0.32969484071748, 0.19331517719543637]
printing an ep nov before normalisation:  47.478478491712366
printing an ep nov before normalisation:  47.77139398862822
actions average: 
K:  4  action  0 :  tensor([0.3095, 0.1116, 0.1156, 0.1062, 0.1394, 0.0998, 0.1180],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0433, 0.7437, 0.0358, 0.0686, 0.0311, 0.0303, 0.0472],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2056, 0.0488, 0.2406, 0.1415, 0.1111, 0.1173, 0.1350],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1585, 0.1807, 0.1301, 0.1794, 0.1097, 0.1180, 0.1237],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1842, 0.0310, 0.1153, 0.1533, 0.2393, 0.1306, 0.1462],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1910, 0.0288, 0.1596, 0.1661, 0.1515, 0.1554, 0.1476],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1930, 0.1286, 0.1291, 0.1229, 0.1116, 0.1000, 0.2147],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  38.12308311462402
maxi score, test score, baseline:  -0.9347148148148148 -0.16533333333333328 -0.16533333333333328
probs:  [0.08547483738459673, 0.1371714679717342, 0.20764213837755702, 0.04723025179195376, 0.3300070969071613, 0.19247420756699693]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9347148148148148 -0.16533333333333328 -0.16533333333333328
probs:  [0.08547483738459673, 0.1371714679717342, 0.20764213837755702, 0.04723025179195376, 0.3300070969071613, 0.19247420756699693]
maxi score, test score, baseline:  -0.9347148148148148 -0.16533333333333328 -0.16533333333333328
probs:  [0.08547483738459673, 0.1371714679717342, 0.20764213837755702, 0.04723025179195376, 0.3300070969071613, 0.19247420756699693]
maxi score, test score, baseline:  -0.9347148148148148 -0.16533333333333328 -0.16533333333333328
maxi score, test score, baseline:  -0.9347148148148148 -0.16533333333333328 -0.16533333333333328
probs:  [0.08547483738459673, 0.1371714679717342, 0.20764213837755702, 0.04723025179195376, 0.3300070969071613, 0.19247420756699693]
printing an ep nov before normalisation:  35.20396371739115
maxi score, test score, baseline:  -0.9347148148148148 -0.16533333333333328 -0.16533333333333328
probs:  [0.08547483738459673, 0.1371714679717342, 0.20764213837755702, 0.04723025179195376, 0.3300070969071613, 0.19247420756699693]
maxi score, test score, baseline:  -0.9347148148148148 -0.16533333333333328 -0.16533333333333328
probs:  [0.08550874046434007, 0.1373314495493475, 0.20797398485560473, 0.04717088341766973, 0.32959115681130113, 0.19242378490173692]
printing an ep nov before normalisation:  28.761643335020075
maxi score, test score, baseline:  -0.9347148148148148 -0.16533333333333328 -0.16533333333333328
probs:  [0.08558643556090574, 0.13689100677722849, 0.20816312817163266, 0.04721368954694443, 0.3298909765673089, 0.1922547633759798]
printing an ep nov before normalisation:  60.438727924666395
Printing some Q and Qe and total Qs values:  [[-0.113]
 [-0.101]
 [-0.113]
 [-0.113]
 [-0.113]
 [-0.113]
 [-0.113]] [[58.063]
 [59.068]
 [58.063]
 [58.063]
 [58.063]
 [58.063]
 [58.063]] [[0.502]
 [0.53 ]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]]
maxi score, test score, baseline:  -0.9347148148148148 -0.16533333333333328 -0.16533333333333328
printing an ep nov before normalisation:  34.85071859159078
line 256 mcts: sample exp_bonus 51.456396717748284
printing an ep nov before normalisation:  34.66679364580922
printing an ep nov before normalisation:  42.204923452187195
maxi score, test score, baseline:  -0.9347148148148148 -0.16533333333333328 -0.16533333333333328
probs:  [0.08561573214579908, 0.13693789250531493, 0.20823444867266747, 0.047229830506357366, 0.33000402996865386, 0.19197806620120733]
Printing some Q and Qe and total Qs values:  [[-0.119]
 [-0.119]
 [-0.119]
 [-0.119]
 [-0.119]
 [-0.119]
 [-0.119]] [[39.297]
 [39.297]
 [39.297]
 [39.297]
 [39.297]
 [39.297]
 [39.297]] [[0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]]
maxi score, test score, baseline:  -0.9347148148148148 -0.16533333333333328 -0.16533333333333328
maxi score, test score, baseline:  -0.9347148148148148 -0.16533333333333328 -0.16533333333333328
probs:  [0.08568879830285947, 0.13646874623646554, 0.2083476775937823, 0.04725166386405125, 0.3301571435264793, 0.1920859704763621]
printing an ep nov before normalisation:  43.964576721191406
actor:  1 policy actor:  1  step number:  66 total reward:  0.19333333333333313  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.563]
 [0.492]
 [0.607]
 [0.495]
 [0.49 ]
 [0.521]] [[29.125]
 [29.949]
 [29.153]
 [35.296]
 [31.168]
 [31.363]
 [29.268]] [[0.5  ]
 [0.563]
 [0.492]
 [0.607]
 [0.495]
 [0.49 ]
 [0.521]]
printing an ep nov before normalisation:  29.082918167114258
maxi score, test score, baseline:  -0.9347148148148148 -0.16533333333333328 -0.16533333333333328
maxi score, test score, baseline:  -0.9347148148148148 -0.16533333333333328 -0.16533333333333328
probs:  [0.08566654119430743, 0.13595977756130045, 0.20714976951022848, 0.04759781591462808, 0.3325821695223911, 0.19104392629714453]
Printing some Q and Qe and total Qs values:  [[0.261]
 [0.284]
 [0.261]
 [0.261]
 [0.261]
 [0.261]
 [0.261]] [[44.492]
 [44.072]
 [44.492]
 [44.492]
 [44.492]
 [44.492]
 [44.492]] [[0.261]
 [0.284]
 [0.261]
 [0.261]
 [0.261]
 [0.261]
 [0.261]]
maxi score, test score, baseline:  -0.9347148148148148 -0.16533333333333328 -0.16533333333333328
probs:  [0.08566654119430743, 0.13595977756130045, 0.20714976951022848, 0.04759781591462808, 0.3325821695223911, 0.19104392629714453]
Printing some Q and Qe and total Qs values:  [[0.186]
 [0.24 ]
 [0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]] [[18.68 ]
 [19.789]
 [18.68 ]
 [18.68 ]
 [18.68 ]
 [18.68 ]
 [18.68 ]] [[2.073]
 [2.24 ]
 [2.073]
 [2.073]
 [2.073]
 [2.073]
 [2.073]]
line 256 mcts: sample exp_bonus 20.048046669033155
maxi score, test score, baseline:  -0.9350091045899173 -0.16533333333333328 -0.16533333333333328
probs:  [0.08566654119430743, 0.13595977756130045, 0.20714976951022848, 0.04759781591462808, 0.3325821695223911, 0.19104392629714453]
actor:  1 policy actor:  1  step number:  70 total reward:  0.12666666666666526  reward:  1.0 rdn_beta:  2.0
from probs:  [0.08567135466699916, 0.136069850566207, 0.20740883720691106, 0.047522954733143385, 0.33205771701407133, 0.19126928581266803]
maxi score, test score, baseline:  -0.9351552552552552 -0.16533333333333328 -0.16533333333333328
printing an ep nov before normalisation:  29.008538701401694
maxi score, test score, baseline:  -0.9351552552552552 -0.16533333333333328 -0.16533333333333328
probs:  [0.08552156742934998, 0.13583180968463995, 0.20704587344000555, 0.04743996979273997, 0.3314764785217601, 0.1926843011315044]
actor:  1 policy actor:  1  step number:  52 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
Printing some Q and Qe and total Qs values:  [[-0.155]
 [-0.214]
 [-0.135]
 [-0.116]
 [-0.148]
 [-0.155]
 [-0.145]] [[56.266]
 [65.059]
 [68.104]
 [68.42 ]
 [66.277]
 [56.266]
 [69.73 ]] [[1.069]
 [1.203]
 [1.349]
 [1.375]
 [1.297]
 [1.069]
 [1.375]]
maxi score, test score, baseline:  -0.9351552552552552 -0.16533333333333328 -0.16533333333333328
probs:  [0.07520151182545808, 0.13039506017225233, 0.2085214355315396, 0.04922371483637605, 0.3438924039926579, 0.192765873641716]
maxi score, test score, baseline:  -0.9351552552552552 -0.16533333333333328 -0.16533333333333328
probs:  [0.07518302207817928, 0.13050155475729672, 0.2088048452299185, 0.04914639905400513, 0.34335057359423016, 0.19301360528637015]
maxi score, test score, baseline:  -0.9351552552552552 -0.16533333333333328 -0.16533333333333328
Printing some Q and Qe and total Qs values:  [[-0.122]
 [-0.042]
 [-0.136]
 [-0.136]
 [-0.136]
 [-0.136]
 [-0.136]] [[32.028]
 [37.865]
 [27.336]
 [27.336]
 [27.336]
 [27.336]
 [27.336]] [[0.629]
 [0.961]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]]
maxi score, test score, baseline:  -0.9351552552552552 -0.16533333333333328 -0.16533333333333328
probs:  [0.07518302207817928, 0.13050155475729672, 0.2088048452299185, 0.04914639905400513, 0.34335057359423016, 0.19301360528637015]
using explorer policy with actor:  1
siam score:  -0.8052889
siam score:  -0.8060518
printing an ep nov before normalisation:  0.0070462232981429096
maxi score, test score, baseline:  -0.9351552552552552 -0.16533333333333328 -0.16533333333333328
probs:  [0.07539422827102452, 0.12967289364122211, 0.20940333000953704, 0.04912655950227747, 0.3432120291321664, 0.1931909594437725]
printing an ep nov before normalisation:  35.13772020492007
printing an ep nov before normalisation:  24.466916279223195
printing an ep nov before normalisation:  23.45170021057129
using explorer policy with actor:  1
siam score:  -0.8039755
maxi score, test score, baseline:  -0.9351552552552552 -0.16533333333333328 -0.16533333333333328
printing an ep nov before normalisation:  46.90980652290189
maxi score, test score, baseline:  -0.9351552552552552 -0.16533333333333328 -0.16533333333333328
probs:  [0.07543867086533519, 0.12915934759462505, 0.20952690671078616, 0.04915549069861292, 0.3434146216000187, 0.19330496253062202]
printing an ep nov before normalisation:  21.73510935426436
siam score:  -0.8076035
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.661]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.667]] [[27.232]
 [28.103]
 [27.232]
 [27.232]
 [27.232]
 [27.232]
 [25.705]] [[2.47 ]
 [2.626]
 [2.47 ]
 [2.47 ]
 [2.47 ]
 [2.47 ]
 [2.46 ]]
maxi score, test score, baseline:  -0.9351552552552552 -0.16533333333333328 -0.16533333333333328
actor:  1 policy actor:  1  step number:  62 total reward:  0.08666666666666623  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9351552552552552 -0.16533333333333328 -0.16533333333333328
probs:  [0.07535401741730748, 0.12869491740643807, 0.20995572481527022, 0.04897018960435389, 0.3421162041862265, 0.19490894657040386]
maxi score, test score, baseline:  -0.9351552552552552 -0.16533333333333328 -0.16533333333333328
probs:  [0.07535401741730748, 0.12869491740643807, 0.20995572481527022, 0.04897018960435389, 0.3421162041862265, 0.19490894657040386]
maxi score, test score, baseline:  -0.9351552552552552 -0.16533333333333328 -0.16533333333333328
probs:  [0.07535401741730748, 0.12869491740643807, 0.20995572481527022, 0.04897018960435389, 0.3421162041862265, 0.19490894657040386]
printing an ep nov before normalisation:  46.114540100097656
printing an ep nov before normalisation:  41.527949930049246
maxi score, test score, baseline:  -0.9351552552552552 -0.16533333333333328 -0.16533333333333328
maxi score, test score, baseline:  -0.9351552552552552 -0.16533333333333328 -0.16533333333333328
probs:  [0.07535401741730748, 0.12869491740643807, 0.20995572481527022, 0.04897018960435389, 0.3421162041862265, 0.19490894657040386]
actions average: 
K:  0  action  0 :  tensor([0.2865, 0.0127, 0.1163, 0.1283, 0.1552, 0.1458, 0.1552],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0136, 0.8967, 0.0140, 0.0223, 0.0101, 0.0120, 0.0312],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1824, 0.1135, 0.1818, 0.0986, 0.1237, 0.1527, 0.1474],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1196, 0.1407, 0.1146, 0.2288, 0.1195, 0.1399, 0.1369],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2008, 0.0334, 0.0966, 0.1027, 0.3270, 0.1171, 0.1224],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1873, 0.0053, 0.1543, 0.1335, 0.1588, 0.2194, 0.1414],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1453, 0.0671, 0.1303, 0.1343, 0.1384, 0.1321, 0.2525],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9351552552552552 -0.16533333333333328 -0.16533333333333328
probs:  [0.07533621261011833, 0.1287956409398504, 0.21023701726784338, 0.04889375752747072, 0.3415805679441763, 0.19515680371054087]
siam score:  -0.7997878
maxi score, test score, baseline:  -0.9353007490636704 -0.16533333333333328 -0.16533333333333328
probs:  [0.07536461638380293, 0.12884423605948037, 0.2103163723685484, 0.04891217412443274, 0.3417095308157135, 0.19485307024802212]
actor:  1 policy actor:  1  step number:  64 total reward:  0.01999999999999902  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9353007490636704 -0.16533333333333328 -0.16533333333333328
probs:  [0.07537380891905367, 0.12879199257639756, 0.2101705357804601, 0.04895175454833095, 0.3419869106701134, 0.19472499750564431]
maxi score, test score, baseline:  -0.9353007490636704 -0.16533333333333328 -0.16533333333333328
probs:  [0.07535617703737656, 0.1288921989815939, 0.21045025986040442, 0.04887583671588666, 0.3414548780328705, 0.19497064937186795]
maxi score, test score, baseline:  -0.9353007490636704 -0.16533333333333328 -0.16533333333333328
probs:  [0.07535617703737656, 0.1288921989815939, 0.21045025986040442, 0.04887583671588666, 0.3414548780328705, 0.19497064937186795]
actor:  1 policy actor:  1  step number:  54 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  24.77249518733089
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9353007490636704 -0.16533333333333328 -0.16533333333333328
maxi score, test score, baseline:  -0.9353007490636704 -0.16533333333333328 -0.16533333333333328
probs:  [0.07412140091972838, 0.1267786351923673, 0.22339969395021023, 0.04807573235322945, 0.33585212220052596, 0.19177241538393872]
printing an ep nov before normalisation:  42.52951422822462
maxi score, test score, baseline:  -0.9353007490636704 -0.16533333333333328 -0.16533333333333328
probs:  [0.07424906865275585, 0.1264247953263961, 0.2226331760631585, 0.04815845788436491, 0.33643141032053286, 0.19210309175279172]
actions average: 
K:  4  action  0 :  tensor([0.2186, 0.0291, 0.1305, 0.1467, 0.1519, 0.1868, 0.1363],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0461, 0.6842, 0.0511, 0.0656, 0.0324, 0.0373, 0.0833],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1758, 0.0888, 0.2141, 0.1160, 0.1454, 0.1425, 0.1175],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1797, 0.0939, 0.1030, 0.2048, 0.1318, 0.1494, 0.1373],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2099, 0.0466, 0.0901, 0.1023, 0.3555, 0.1112, 0.0843],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2279, 0.0487, 0.1401, 0.1288, 0.1441, 0.1698, 0.1407],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1581, 0.1117, 0.1317, 0.1443, 0.1383, 0.1780, 0.1379],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  60.937151320705404
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9353007490636704 -0.16533333333333328 -0.16533333333333328
probs:  [0.07424906865275585, 0.1264247953263961, 0.2226331760631585, 0.04815845788436491, 0.33643141032053286, 0.19210309175279172]
printing an ep nov before normalisation:  28.590469360351562
printing an ep nov before normalisation:  43.86618622986335
printing an ep nov before normalisation:  24.908685487706972
maxi score, test score, baseline:  -0.9353007490636704 -0.16533333333333328 -0.16533333333333328
probs:  [0.07424906865275585, 0.1264247953263961, 0.2226331760631585, 0.04815845788436491, 0.33643141032053286, 0.19210309175279172]
Printing some Q and Qe and total Qs values:  [[0.273]
 [0.554]
 [0.273]
 [0.273]
 [0.273]
 [0.273]
 [0.273]] [[28.048]
 [30.434]
 [28.048]
 [28.048]
 [28.048]
 [28.048]
 [28.048]] [[0.865]
 [1.244]
 [0.865]
 [0.865]
 [0.865]
 [0.865]
 [0.865]]
actor:  1 policy actor:  1  step number:  59 total reward:  0.053333333333332456  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9353007490636704 -0.16533333333333328 -0.16533333333333328
probs:  [0.07403076854866349, 0.1261792179104163, 0.22501517291453713, 0.04795379787256519, 0.3349978648314668, 0.19182317792235107]
siam score:  -0.7985692
actor:  1 policy actor:  1  step number:  55 total reward:  0.11999999999999933  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9353007490636704 -0.16533333333333328 -0.16533333333333328
printing an ep nov before normalisation:  47.73037358349015
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9353007490636704 -0.16533333333333328 -0.16533333333333328
probs:  [0.06781907639873623, 0.12290212579366537, 0.22668353577736353, 0.048984017035831405, 0.34216943198250205, 0.19144181301190147]
printing an ep nov before normalisation:  31.606552280017503
Printing some Q and Qe and total Qs values:  [[-0.084]
 [-0.076]
 [-0.077]
 [-0.076]
 [-0.076]
 [-0.076]
 [-0.076]] [[16.508]
 [16.333]
 [16.037]
 [16.333]
 [16.333]
 [16.333]
 [16.333]] [[1.916]
 [1.883]
 [1.811]
 [1.883]
 [1.883]
 [1.883]
 [1.883]]
maxi score, test score, baseline:  -0.9353007490636704 -0.16533333333333328 -0.16533333333333328
probs:  [0.06781907639873623, 0.12290212579366537, 0.22668353577736353, 0.048984017035831405, 0.34216943198250205, 0.19144181301190147]
from probs:  [0.06781907639873623, 0.12290212579366537, 0.22668353577736353, 0.048984017035831405, 0.34216943198250205, 0.19144181301190147]
printing an ep nov before normalisation:  57.9015759690455
maxi score, test score, baseline:  -0.9353007490636704 -0.16533333333333328 -0.16533333333333328
maxi score, test score, baseline:  -0.9353007490636704 -0.16533333333333328 -0.16533333333333328
probs:  [0.06784531276707277, 0.12294971343675971, 0.2267713511388514, 0.04900295256421891, 0.34230201195175247, 0.19112865814134478]
siam score:  -0.7934878
printing an ep nov before normalisation:  36.704206929185645
Printing some Q and Qe and total Qs values:  [[-0.02 ]
 [ 0.097]
 [-0.038]
 [-0.034]
 [-0.032]
 [-0.025]
 [-0.029]] [[32.453]
 [41.404]
 [33.409]
 [34.045]
 [32.508]
 [31.963]
 [31.003]] [[0.244]
 [0.502]
 [0.241]
 [0.254]
 [0.232]
 [0.23 ]
 [0.211]]
maxi score, test score, baseline:  -0.9353007490636704 -0.16533333333333328 -0.16533333333333328
probs:  [0.06733108299852399, 0.12317662420588915, 0.2259833277872988, 0.04909324229136007, 0.3429341891481334, 0.19148153356879444]
printing an ep nov before normalisation:  44.99864449096522
maxi score, test score, baseline:  -0.9353007490636704 -0.16533333333333328 -0.16533333333333328
maxi score, test score, baseline:  -0.9353007490636704 -0.16533333333333328 -0.16533333333333328
probs:  [0.06741140199723904, 0.1233236932202001, 0.22505897179778137, 0.04915176230442075, 0.3433439258807633, 0.19171024479959548]
printing an ep nov before normalisation:  41.83507269905582
maxi score, test score, baseline:  -0.9353007490636704 -0.16533333333333328 -0.16533333333333328
probs:  [0.06741140199723904, 0.1233236932202001, 0.22505897179778137, 0.04915176230442075, 0.3433439258807633, 0.19171024479959548]
printing an ep nov before normalisation:  51.736572150697114
printing an ep nov before normalisation:  50.48216966637911
maxi score, test score, baseline:  -0.9353007490636704 -0.16533333333333328 -0.16533333333333328
probs:  [0.06741140199723904, 0.1233236932202001, 0.22505897179778137, 0.04915176230442075, 0.3433439258807633, 0.19171024479959548]
maxi score, test score, baseline:  -0.9353007490636704 -0.16533333333333328 -0.16533333333333328
probs:  [0.06741140199723904, 0.1233236932202001, 0.22505897179778137, 0.04915176230442075, 0.3433439258807633, 0.19171024479959548]
actor:  0 policy actor:  0  step number:  69 total reward:  0.21333333333333238  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  0.047728572025107496
maxi score, test score, baseline:  -0.9327251121076232 -0.16533333333333328 -0.16533333333333328
maxi score, test score, baseline:  -0.9327251121076232 -0.16533333333333328 -0.16533333333333328
actor:  1 policy actor:  1  step number:  60 total reward:  0.07333333333333247  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9327251121076232 -0.16533333333333328 -0.16533333333333328
probs:  [0.06710093030603272, 0.126988218254154, 0.22446041398424166, 0.04887465838920339, 0.34140315041493935, 0.19117262865142884]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  0.36666666666666603  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.147]
 [-0.139]
 [-0.146]
 [-0.148]
 [-0.148]
 [-0.145]
 [-0.148]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.147]
 [-0.139]
 [-0.146]
 [-0.148]
 [-0.148]
 [-0.145]
 [-0.148]]
maxi score, test score, baseline:  -0.9327251121076232 -0.16533333333333328 -0.16533333333333328
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  51 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  47 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9327251121076232 -0.16533333333333328 -0.16533333333333328
probs:  [0.06734643223558437, 0.1238046608145881, 0.21569573979271575, 0.05016377025037574, 0.3504419422418587, 0.19254745466487724]
maxi score, test score, baseline:  -0.9327251121076232 -0.16533333333333328 -0.16533333333333328
printing an ep nov before normalisation:  7.506571634169177
maxi score, test score, baseline:  -0.9327251121076232 -0.16533333333333328 -0.16533333333333328
maxi score, test score, baseline:  -0.9327251121076232 -0.16533333333333328 -0.16533333333333328
probs:  [0.06734643223558437, 0.1238046608145881, 0.21569573979271575, 0.05016377025037574, 0.3504419422418587, 0.19254745466487724]
printing an ep nov before normalisation:  40.500832068239525
printing an ep nov before normalisation:  33.02979581077006
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9327251121076232 -0.16533333333333328 -0.16533333333333328
probs:  [0.06729959547551399, 0.12403542234068322, 0.21637831864783907, 0.05003244841808634, 0.34952134496510734, 0.19273287015277007]
maxi score, test score, baseline:  -0.9327251121076232 -0.16533333333333328 -0.16533333333333328
probs:  [0.06729959547551399, 0.12403542234068322, 0.21637831864783907, 0.05003244841808634, 0.34952134496510734, 0.19273287015277007]
Printing some Q and Qe and total Qs values:  [[-0.094]
 [-0.081]
 [-0.094]
 [-0.094]
 [-0.094]
 [-0.094]
 [-0.094]] [[44.72 ]
 [45.658]
 [44.72 ]
 [44.72 ]
 [44.72 ]
 [44.72 ]
 [44.72 ]] [[0.898]
 [0.952]
 [0.898]
 [0.898]
 [0.898]
 [0.898]
 [0.898]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.03393149305274
maxi score, test score, baseline:  -0.9327251121076232 -0.16533333333333328 -0.16533333333333328
probs:  [0.06753171279921735, 0.1242608374163903, 0.21547647434634176, 0.05006068228244088, 0.34972012321084994, 0.1929501699447598]
actions average: 
K:  4  action  0 :  tensor([0.2988, 0.0758, 0.1152, 0.1189, 0.1422, 0.1029, 0.1461],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0664, 0.7491, 0.0300, 0.0450, 0.0378, 0.0248, 0.0469],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2598, 0.0237, 0.1262, 0.1330, 0.1702, 0.1277, 0.1593],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2545, 0.0285, 0.1075, 0.1511, 0.1797, 0.1221, 0.1564],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2754, 0.0088, 0.0893, 0.1222, 0.2216, 0.1381, 0.1445],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1933, 0.1094, 0.1027, 0.0934, 0.0888, 0.3282, 0.0842],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1298, 0.1811, 0.0950, 0.1624, 0.1124, 0.1096, 0.2097],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9327251121076232 -0.16533333333333328 -0.16533333333333328
probs:  [0.06753171279921735, 0.1242608374163903, 0.21547647434634176, 0.05006068228244088, 0.34972012321084994, 0.1929501699447598]
printing an ep nov before normalisation:  34.06650419024169
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.42688126440282304
printing an ep nov before normalisation:  68.3743629369102
printing an ep nov before normalisation:  57.40451571161155
maxi score, test score, baseline:  -0.9327251121076232 -0.16533333333333328 -0.16533333333333328
probs:  [0.06751672169561765, 0.1248622118933897, 0.21595325614957112, 0.04985586731075213, 0.34828386047085264, 0.19352808247981673]
maxi score, test score, baseline:  -0.9327251121076232 -0.16533333333333328 -0.16533333333333328
probs:  [0.06751672169561765, 0.1248622118933897, 0.21595325614957112, 0.04985586731075213, 0.34828386047085264, 0.19352808247981673]
maxi score, test score, baseline:  -0.9327251121076232 -0.16533333333333328 -0.16533333333333328
printing an ep nov before normalisation:  51.832661400160475
maxi score, test score, baseline:  -0.9327251121076232 -0.16533333333333328 -0.16533333333333328
probs:  [0.06751672169561765, 0.1248622118933897, 0.21595325614957112, 0.04985586731075213, 0.34828386047085264, 0.19352808247981673]
printing an ep nov before normalisation:  0.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9327251121076232 -0.16533333333333328 -0.16533333333333328
probs:  [0.06748166759620236, 0.12495398449327405, 0.2162464879477102, 0.04978175403055873, 0.3477643877210157, 0.19377171821123898]
printing an ep nov before normalisation:  31.550907342578125
actor:  1 policy actor:  1  step number:  60 total reward:  0.13999999999999935  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  67 total reward:  0.10666666666666558  reward:  1.0 rdn_beta:  0.667
from probs:  [0.06734133332645152, 0.12469389488068926, 0.21579617170707882, 0.04967830115574072, 0.3470400270007723, 0.1954502719292675]
printing an ep nov before normalisation:  58.640308993241064
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9328753914988813 -0.16533333333333328 -0.16533333333333328
probs:  [0.06706756650971948, 0.12935536287448648, 0.21381899363027626, 0.04947648327995749, 0.3456269300509049, 0.1946546636546553]
printing an ep nov before normalisation:  47.60002233258882
maxi score, test score, baseline:  -0.9328753914988813 -0.16533333333333328 -0.16533333333333328
probs:  [0.06714041784255541, 0.12949600824250682, 0.2129640392729291, 0.04953018847847799, 0.3460029653843798, 0.1948663807791508]
Printing some Q and Qe and total Qs values:  [[-0.122]
 [-0.11 ]
 [-0.122]
 [-0.122]
 [-0.122]
 [-0.122]
 [-0.122]] [[41.794]
 [46.004]
 [41.794]
 [41.794]
 [41.794]
 [41.794]
 [41.794]] [[1.066]
 [1.304]
 [1.066]
 [1.066]
 [1.066]
 [1.066]
 [1.066]]
actor:  1 policy actor:  1  step number:  63 total reward:  0.013333333333332864  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  49.9950423706876
Printing some Q and Qe and total Qs values:  [[0.175]
 [0.56 ]
 [0.209]
 [0.248]
 [0.083]
 [0.165]
 [0.272]] [[28.434]
 [33.845]
 [28.564]
 [29.52 ]
 [30.717]
 [28.928]
 [29.239]] [[1.128]
 [1.853]
 [1.17 ]
 [1.269]
 [1.179]
 [1.149]
 [1.275]]
maxi score, test score, baseline:  -0.9328753914988813 -0.16533333333333328 -0.16533333333333328
probs:  [0.0671012105368896, 0.12942031537979864, 0.21283950752363776, 0.049501285285464644, 0.345800589783457, 0.19533709149075235]
actor:  1 policy actor:  1  step number:  61 total reward:  0.18666666666666576  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  20.562947669407095
actor:  1 policy actor:  1  step number:  43 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9328753914988813 -0.16533333333333328 -0.16533333333333328
probs:  [0.06626525130258495, 0.12780642872073658, 0.21018430171399766, 0.048885025364256304, 0.3414856349678568, 0.2053733579305677]
maxi score, test score, baseline:  -0.9328753914988813 -0.16533333333333328 -0.16533333333333328
siam score:  -0.77402055
actor:  1 policy actor:  1  step number:  55 total reward:  0.2533333333333331  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9328753914988813 -0.16533333333333328 -0.16533333333333328
probs:  [0.06649890119812471, 0.12719926511778024, 0.20845164083432355, 0.04935613461934917, 0.344787631100201, 0.2037064271302213]
Printing some Q and Qe and total Qs values:  [[-0.113]
 [-0.113]
 [-0.113]
 [-0.113]
 [-0.113]
 [-0.113]
 [-0.113]] [[35.931]
 [35.931]
 [35.931]
 [35.931]
 [35.931]
 [35.931]
 [35.931]] [[1.243]
 [1.243]
 [1.243]
 [1.243]
 [1.243]
 [1.243]
 [1.243]]
maxi score, test score, baseline:  -0.9328753914988813 -0.16533333333333328 -0.16533333333333328
probs:  [0.06649890119812471, 0.12719926511778024, 0.20845164083432355, 0.04935613461934917, 0.344787631100201, 0.2037064271302213]
printing an ep nov before normalisation:  46.10441551023572
maxi score, test score, baseline:  -0.9328753914988813 -0.16533333333333328 -0.16533333333333328
probs:  [0.06649890119812471, 0.12719926511778024, 0.20845164083432355, 0.04935613461934917, 0.344787631100201, 0.2037064271302213]
printing an ep nov before normalisation:  41.06083930655351
maxi score, test score, baseline:  -0.9328753914988813 -0.16533333333333328 -0.16533333333333328
probs:  [0.06649890119812471, 0.12719926511778024, 0.20845164083432355, 0.04935613461934917, 0.344787631100201, 0.2037064271302213]
Printing some Q and Qe and total Qs values:  [[0.49]
 [0.98]
 [0.49]
 [0.49]
 [0.49]
 [0.49]
 [0.49]] [[31.048]
 [34.282]
 [31.048]
 [31.048]
 [31.048]
 [31.048]
 [31.048]] [[1.903]
 [2.54 ]
 [1.903]
 [1.903]
 [1.903]
 [1.903]
 [1.903]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  49 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  32.74665831108618
actor:  1 policy actor:  1  step number:  60 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  50.49961739582734
maxi score, test score, baseline:  -0.9328753914988813 -0.16533333333333328 -0.16533333333333328
probs:  [0.06721413353306238, 0.12564271752211942, 0.2028464769937603, 0.05071295442681247, 0.3542972052577455, 0.19928651226649988]
Printing some Q and Qe and total Qs values:  [[0.55 ]
 [0.636]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]] [[33.473]
 [35.66 ]
 [33.473]
 [33.473]
 [33.473]
 [33.473]
 [33.473]] [[1.452]
 [1.598]
 [1.452]
 [1.452]
 [1.452]
 [1.452]
 [1.452]]
maxi score, test score, baseline:  -0.9328753914988813 -0.16533333333333328 -0.16533333333333328
probs:  [0.06721413353306238, 0.12564271752211942, 0.2028464769937603, 0.05071295442681247, 0.3542972052577455, 0.19928651226649988]
siam score:  -0.7769116
actor:  1 policy actor:  1  step number:  65 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9328753914988813 -0.16533333333333328 -0.16533333333333328
probs:  [0.06602263819368981, 0.1239307409654405, 0.21398723426231211, 0.05002303601911889, 0.349466332112604, 0.1965700184468348]
maxi score, test score, baseline:  -0.9328753914988813 -0.16533333333333328 -0.16533333333333328
probs:  [0.06602263819368981, 0.1239307409654405, 0.21398723426231211, 0.05002303601911889, 0.349466332112604, 0.1965700184468348]
actor:  0 policy actor:  1  step number:  55 total reward:  0.26666666666666616  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.930197619047619 -0.16533333333333328 -0.16533333333333328
probs:  [0.0656781214829817, 0.12427623760896711, 0.21497825021801778, 0.049838766855482566, 0.3481745955843098, 0.19705402825024107]
printing an ep nov before normalisation:  0.19403632928202796
printing an ep nov before normalisation:  0.18740535390406876
printing an ep nov before normalisation:  34.12479877471924
printing an ep nov before normalisation:  32.2737699879026
printing an ep nov before normalisation:  35.06257772445679
maxi score, test score, baseline:  -0.930197619047619 -0.16533333333333328 -0.16533333333333328
probs:  [0.06570267635714493, 0.1239106731613959, 0.21546601697297005, 0.049814179258640214, 0.34800194244315086, 0.1971045118066981]
actor:  1 policy actor:  1  step number:  63 total reward:  0.0799999999999994  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.930197619047619 -0.16533333333333328 -0.16533333333333328
printing an ep nov before normalisation:  5.079250595895672
Printing some Q and Qe and total Qs values:  [[-0.012]
 [-0.005]
 [-0.019]
 [-0.017]
 [-0.016]
 [-0.015]
 [-0.016]] [[32.227]
 [32.544]
 [32.078]
 [31.872]
 [31.994]
 [32.014]
 [32.325]] [[1.684]
 [1.724]
 [1.662]
 [1.642]
 [1.656]
 [1.659]
 [1.691]]
printing an ep nov before normalisation:  50.357433748413236
maxi score, test score, baseline:  -0.930197619047619 -0.16533333333333328 -0.16533333333333328
probs:  [0.06526336559506736, 0.12359407529134342, 0.21812162816770667, 0.04968710535138112, 0.34711216407366174, 0.19622166152083975]
printing an ep nov before normalisation:  37.28892038514703
printing an ep nov before normalisation:  41.053942906563165
printing an ep nov before normalisation:  44.639320373535156
printing an ep nov before normalisation:  48.226013713167646
printing an ep nov before normalisation:  26.571279532794158
printing an ep nov before normalisation:  27.127128161837994
maxi score, test score, baseline:  -0.930197619047619 -0.16533333333333328 -0.16533333333333328
Printing some Q and Qe and total Qs values:  [[0.098]
 [0.222]
 [0.098]
 [0.098]
 [0.098]
 [0.098]
 [0.098]] [[24.544]
 [38.753]
 [24.544]
 [24.544]
 [24.544]
 [24.544]
 [24.544]] [[0.44]
 [0.95]
 [0.44]
 [0.44]
 [0.44]
 [0.44]
 [0.44]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.930352858203415 -0.16533333333333328 -0.16533333333333328
probs:  [0.06518621891423274, 0.12376550338364002, 0.21869588292083497, 0.04954358085041595, 0.34610622378424605, 0.1967025901466301]
maxi score, test score, baseline:  -0.930352858203415 -0.16533333333333328 -0.16533333333333328
probs:  [0.06525652674928793, 0.12389912241366456, 0.21785218665499514, 0.04959698246824327, 0.34648014374703906, 0.19691503796677]
printing an ep nov before normalisation:  43.6982351832393
maxi score, test score, baseline:  -0.930352858203415 -0.16533333333333328 -0.16533333333333328
probs:  [0.0652181936532488, 0.12398491612814176, 0.21813684766158986, 0.049525503341341676, 0.3459791572009765, 0.19715538201470126]
maxi score, test score, baseline:  -0.930352858203415 -0.16533333333333328 -0.16533333333333328
Printing some Q and Qe and total Qs values:  [[-0.132]
 [-0.084]
 [-0.113]
 [-0.132]
 [-0.103]
 [-0.112]
 [-0.098]] [[36.992]
 [37.948]
 [43.036]
 [36.992]
 [43.298]
 [42.896]
 [40.897]] [[1.18 ]
 [1.268]
 [1.45 ]
 [1.18 ]
 [1.471]
 [1.445]
 [1.376]]
printing an ep nov before normalisation:  20.96594648151129
maxi score, test score, baseline:  -0.930352858203415 -0.16533333333333328 -0.16533333333333328
probs:  [0.0652181936532488, 0.12398491612814176, 0.21813684766158986, 0.049525503341341676, 0.3459791572009765, 0.19715538201470126]
maxi score, test score, baseline:  -0.930352858203415 -0.16533333333333328 -0.16533333333333328
maxi score, test score, baseline:  -0.930352858203415 -0.16533333333333328 -0.16533333333333328
probs:  [0.0652181936532488, 0.12398491612814176, 0.21813684766158986, 0.049525503341341676, 0.3459791572009765, 0.19715538201470126]
maxi score, test score, baseline:  -0.930352858203415 -0.16533333333333328 -0.16533333333333328
probs:  [0.0652181936532488, 0.12398491612814176, 0.21813684766158986, 0.049525503341341676, 0.3459791572009765, 0.19715538201470126]
maxi score, test score, baseline:  -0.930352858203415 -0.16533333333333328 -0.16533333333333328
probs:  [0.0652181936532488, 0.12398491612814176, 0.21813684766158986, 0.049525503341341676, 0.3459791572009765, 0.19715538201470126]
printing an ep nov before normalisation:  37.80283516814518
maxi score, test score, baseline:  -0.9305074074074073 -0.16533333333333328 -0.16533333333333328
probs:  [0.06528798930783693, 0.12411773189410759, 0.21729910954985868, 0.04957847050780552, 0.3463500345978865, 0.19736666414250478]
maxi score, test score, baseline:  -0.9305074074074073 -0.16533333333333328 -0.16533333333333328
printing an ep nov before normalisation:  11.73624689685548
actor:  1 policy actor:  1  step number:  53 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.192]
 [0.192]
 [0.192]
 [0.192]
 [0.192]
 [0.192]
 [0.192]] [[10.864]
 [10.864]
 [10.864]
 [10.864]
 [10.864]
 [10.864]
 [10.864]] [[0.192]
 [0.192]
 [0.192]
 [0.192]
 [0.192]
 [0.192]
 [0.192]]
printing an ep nov before normalisation:  41.60322711033599
maxi score, test score, baseline:  -0.9305074074074073 -0.16533333333333328 -0.16533333333333328
probs:  [0.06563389301652502, 0.1236212131341647, 0.21442356796556197, 0.05014932933318684, 0.35035075166656615, 0.19582124488399535]
printing an ep nov before normalisation:  30.220411519728057
maxi score, test score, baseline:  -0.9305074074074073 -0.16533333333333328 -0.16533333333333328
probs:  [0.06563389301652502, 0.1236212131341647, 0.21442356796556197, 0.05014932933318684, 0.35035075166656615, 0.19582124488399535]
maxi score, test score, baseline:  -0.9305074074074073 -0.16533333333333328 -0.16533333333333328
probs:  [0.06563389301652502, 0.1236212131341647, 0.21442356796556197, 0.05014932933318684, 0.35035075166656615, 0.19582124488399535]
from probs:  [0.06563389301652502, 0.1236212131341647, 0.21442356796556197, 0.05014932933318684, 0.35035075166656615, 0.19582124488399535]
maxi score, test score, baseline:  -0.930661271249076 -0.16533333333333328 -0.16533333333333328
probs:  [0.06563389301652502, 0.1236212131341647, 0.21442356796556197, 0.05014932933318684, 0.35035075166656615, 0.19582124488399535]
printing an ep nov before normalisation:  40.22232825852232
actions average: 
K:  1  action  0 :  tensor([0.2843, 0.0253, 0.1300, 0.1142, 0.1755, 0.1417, 0.1290],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0213, 0.8339, 0.0155, 0.0263, 0.0147, 0.0164, 0.0717],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1983, 0.0273, 0.1432, 0.1412, 0.1526, 0.1744, 0.1630],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1800, 0.0844, 0.1283, 0.1525, 0.1500, 0.1660, 0.1387],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1642, 0.0436, 0.1236, 0.1548, 0.2390, 0.1411, 0.1337],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1803, 0.0369, 0.1236, 0.1335, 0.1307, 0.2476, 0.1474],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1746, 0.0146, 0.1138, 0.1394, 0.1298, 0.1404, 0.2874],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]] [[43.311]
 [43.311]
 [43.311]
 [43.311]
 [43.311]
 [43.311]
 [43.311]] [[1.218]
 [1.218]
 [1.218]
 [1.218]
 [1.218]
 [1.218]
 [1.218]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.930661271249076 -0.16533333333333328 -0.16533333333333328
probs:  [0.06563389301652502, 0.1236212131341647, 0.21442356796556197, 0.05014932933318684, 0.35035075166656615, 0.19582124488399535]
maxi score, test score, baseline:  -0.930661271249076 -0.16533333333333328 -0.16533333333333328
probs:  [0.06563389301652502, 0.1236212131341647, 0.21442356796556197, 0.05014932933318684, 0.35035075166656615, 0.19582124488399535]
maxi score, test score, baseline:  -0.930661271249076 -0.16533333333333328 -0.16533333333333328
maxi score, test score, baseline:  -0.930661271249076 -0.16533333333333328 -0.16533333333333328
probs:  [0.06578014828064972, 0.12370162979387014, 0.21440088794295906, 0.050123907721598075, 0.3501737403856922, 0.19581968587523066]
actor:  1 policy actor:  1  step number:  57 total reward:  0.3199999999999994  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  30.007106590074237
Printing some Q and Qe and total Qs values:  [[-0.127]
 [-0.12 ]
 [-0.124]
 [-0.12 ]
 [-0.123]
 [-0.129]
 [-0.129]] [[43.975]
 [40.861]
 [42.325]
 [43.712]
 [44.731]
 [46.249]
 [44.43 ]] [[0.359]
 [0.298]
 [0.326]
 [0.36 ]
 [0.38 ]
 [0.406]
 [0.367]]
printing an ep nov before normalisation:  0.0015179887680005777
printing an ep nov before normalisation:  0.0
actor:  0 policy actor:  0  step number:  54 total reward:  0.2733333333333331  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  66.19156761040105
maxi score, test score, baseline:  -0.9279973451327433 -0.16533333333333328 -0.16533333333333328
probs:  [0.06483234430862822, 0.12136551905550237, 0.22628571714038226, 0.0494021470527999, 0.34511988894125867, 0.19299438350142853]
printing an ep nov before normalisation:  39.16788785879739
maxi score, test score, baseline:  -0.9279973451327433 -0.16533333333333328 -0.16533333333333328
probs:  [0.06483234430862822, 0.12136551905550237, 0.22628571714038226, 0.0494021470527999, 0.34511988894125867, 0.19299438350142853]
Printing some Q and Qe and total Qs values:  [[-0.111]
 [-0.088]
 [-0.111]
 [-0.111]
 [-0.111]
 [-0.111]
 [-0.111]] [[41.044]
 [41.932]
 [41.044]
 [41.044]
 [41.044]
 [41.044]
 [41.044]] [[1.387]
 [1.465]
 [1.387]
 [1.387]
 [1.387]
 [1.387]
 [1.387]]
printing an ep nov before normalisation:  46.16004728391872
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9279973451327433 -0.16533333333333328 -0.16533333333333328
maxi score, test score, baseline:  -0.9279973451327433 -0.16533333333333328 -0.16533333333333328
probs:  [0.06483234430862822, 0.12136551905550237, 0.22628571714038226, 0.0494021470527999, 0.34511988894125867, 0.19299438350142853]
line 256 mcts: sample exp_bonus 42.448467333602395
printing an ep nov before normalisation:  40.584035160128124
maxi score, test score, baseline:  -0.9279973451327433 -0.16533333333333328 -0.16533333333333328
probs:  [0.06481757684376176, 0.12148986608938624, 0.2266682474592261, 0.04934940959657039, 0.344750148121509, 0.19292475188954647]
maxi score, test score, baseline:  -0.9279973451327433 -0.16533333333333328 -0.16533333333333328
probs:  [0.06481757684376176, 0.12148986608938624, 0.2266682474592261, 0.04934940959657039, 0.344750148121509, 0.19292475188954647]
actor:  1 policy actor:  1  step number:  67 total reward:  0.013333333333332642  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9279973451327433 -0.16533333333333328 -0.16533333333333328
printing an ep nov before normalisation:  20.79029689924879
maxi score, test score, baseline:  -0.9279973451327433 -0.16533333333333328 -0.16533333333333328
probs:  [0.06482718014053544, 0.12147012879809238, 0.22659405682569994, 0.04936702113033963, 0.34487358204146357, 0.192868031063869]
line 256 mcts: sample exp_bonus 32.1052258561064
printing an ep nov before normalisation:  57.45285199918619
printing an ep nov before normalisation:  31.56756231868423
printing an ep nov before normalisation:  34.263296127319336
actor:  1 policy actor:  1  step number:  52 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 49.19375752678023
printing an ep nov before normalisation:  20.772352266959935
maxi score, test score, baseline:  -0.9279973451327433 -0.16533333333333328 -0.16533333333333328
probs:  [0.06511358396956204, 0.12125334751582498, 0.22544341181144303, 0.04960812770068778, 0.34656453930530823, 0.19201698969717407]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9279973451327433 -0.16533333333333328 -0.16533333333333328
probs:  [0.06507583124448044, 0.12133170836136811, 0.2257372684058953, 0.04953830512390362, 0.3460751762615806, 0.19224171060277193]
maxi score, test score, baseline:  -0.9279973451327433 -0.16533333333333328 -0.16533333333333328
probs:  [0.06479339426028821, 0.1214416642526367, 0.22608964102395435, 0.04948156566091564, 0.3456774241011631, 0.1925163107010419]
Printing some Q and Qe and total Qs values:  [[-0.138]
 [-0.13 ]
 [-0.142]
 [-0.14 ]
 [-0.138]
 [-0.132]
 [-0.14 ]] [[32.036]
 [39.076]
 [30.593]
 [30.583]
 [31.059]
 [36.374]
 [31.3  ]] [[0.456]
 [0.713]
 [0.401]
 [0.403]
 [0.421]
 [0.615]
 [0.429]]
maxi score, test score, baseline:  -0.9279973451327433 -0.16533333333333328 -0.16533333333333328
probs:  [0.06479339426028821, 0.1214416642526367, 0.22608964102395435, 0.04948156566091564, 0.3456774241011631, 0.1925163107010419]
maxi score, test score, baseline:  -0.9279973451327433 -0.16533333333333328 -0.16533333333333328
probs:  [0.06479339426028821, 0.1214416642526367, 0.22608964102395435, 0.04948156566091564, 0.3456774241011631, 0.1925163107010419]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
maxi score, test score, baseline:  -0.9279973451327433 -0.16533333333333328 -0.16533333333333328
probs:  [0.06479339426028821, 0.1214416642526367, 0.22608964102395435, 0.04948156566091564, 0.3456774241011631, 0.1925163107010419]
siam score:  -0.79641175
from probs:  [0.06479339426028821, 0.1214416642526367, 0.22608964102395435, 0.04948156566091564, 0.3456774241011631, 0.1925163107010419]
printing an ep nov before normalisation:  38.35071470983902
printing an ep nov before normalisation:  35.666418527962726
maxi score, test score, baseline:  -0.9279973451327433 -0.16533333333333328 -0.16533333333333328
probs:  [0.0648286485658815, 0.120963015941637, 0.22621283799028577, 0.04950847158286819, 0.34586582325320314, 0.19262120266612445]
maxi score, test score, baseline:  -0.9279973451327433 -0.16533333333333328 -0.16533333333333328
probs:  [0.06486355114972384, 0.12048914296287516, 0.22633480585702265, 0.04953510907241735, 0.34605234280277986, 0.192725048155181]
maxi score, test score, baseline:  -0.9279973451327433 -0.16533333333333328 -0.16533333333333328
probs:  [0.06486355114972384, 0.12048914296287516, 0.22633480585702265, 0.04953510907241735, 0.34605234280277986, 0.192725048155181]
maxi score, test score, baseline:  -0.9279973451327433 -0.16533333333333328 -0.16533333333333328
probs:  [0.06486355114972384, 0.12048914296287516, 0.22633480585702265, 0.04953510907241735, 0.34605234280277986, 0.192725048155181]
actor:  1 policy actor:  1  step number:  55 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  40.02151330240259
siam score:  -0.79223204
maxi score, test score, baseline:  -0.9279973451327433 -0.16533333333333328 -0.16533333333333328
probs:  [0.06400689833304746, 0.11837025081706935, 0.23709089537424877, 0.04888131562965804, 0.34147438667689844, 0.1901762531690778]
actor:  0 policy actor:  0  step number:  56 total reward:  0.17999999999999905  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  41.47872897737415
Printing some Q and Qe and total Qs values:  [[ 0.025]
 [ 0.304]
 [ 0.001]
 [ 0.01 ]
 [-0.004]
 [-0.003]
 [ 0.009]] [[43.783]
 [40.436]
 [44.387]
 [42.291]
 [41.418]
 [40.929]
 [38.741]] [[1.721]
 [1.76 ]
 [1.741]
 [1.599]
 [1.522]
 [1.488]
 [1.343]]
printing an ep nov before normalisation:  47.01084907397437
printing an ep nov before normalisation:  32.508134841918945
actions average: 
K:  2  action  0 :  tensor([0.3430, 0.1315, 0.1108, 0.0921, 0.1343, 0.0849, 0.1034],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0488, 0.7436, 0.0415, 0.0465, 0.0336, 0.0339, 0.0520],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1945, 0.0259, 0.2958, 0.0929, 0.0991, 0.1623, 0.1295],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0918, 0.0784, 0.0859, 0.4341, 0.0902, 0.1132, 0.1064],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1954, 0.0191, 0.0983, 0.1423, 0.2855, 0.1339, 0.1256],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2006, 0.0249, 0.1522, 0.1541, 0.1446, 0.1736, 0.1500],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1850, 0.2684, 0.0972, 0.0845, 0.0828, 0.0798, 0.2023],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  0.14666666666666617  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  43.5142445646713
using explorer policy with actor:  1
siam score:  -0.7845059
maxi score, test score, baseline:  -0.9255512141280353 -0.16533333333333328 -0.16533333333333328
printing an ep nov before normalisation:  27.05201678943417
maxi score, test score, baseline:  -0.9255512141280353 -0.16533333333333328 -0.16533333333333328
probs:  [0.0638184941325905, 0.11845059562579462, 0.2366084177761835, 0.04861813706045636, 0.33963023347523086, 0.19287412192974437]
maxi score, test score, baseline:  -0.9257149779735683 -0.16533333333333328 -0.16533333333333328
maxi score, test score, baseline:  -0.9257149779735683 -0.16533333333333328 -0.16533333333333328
probs:  [0.06377944319429278, 0.11852068206467876, 0.2369145455616489, 0.0485487206949146, 0.339143725833877, 0.19309288265058802]
maxi score, test score, baseline:  -0.9258780219780219 -0.16533333333333328 -0.16533333333333328
maxi score, test score, baseline:  -0.9258780219780219 -0.16533333333333328 -0.16533333333333328
probs:  [0.06377944319429278, 0.11852068206467876, 0.2369145455616489, 0.0485487206949146, 0.339143725833877, 0.19309288265058802]
maxi score, test score, baseline:  -0.9258780219780219 -0.16533333333333328 -0.16533333333333328
printing an ep nov before normalisation:  48.47153385445276
maxi score, test score, baseline:  -0.9258780219780219 -0.16533333333333328 -0.16533333333333328
printing an ep nov before normalisation:  43.75968933105469
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
siam score:  -0.78165454
printing an ep nov before normalisation:  44.354346973441395
printing an ep nov before normalisation:  44.91420606135128
printing an ep nov before normalisation:  32.45398942656613
maxi score, test score, baseline:  -0.9258780219780219 -0.16533333333333328 -0.16533333333333328
probs:  [0.06380295289175014, 0.1185644121308591, 0.2370020080610704, 0.0485666044542089, 0.3392689498411124, 0.19279507262099915]
Printing some Q and Qe and total Qs values:  [[0.691]
 [0.804]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]] [[31.752]
 [24.475]
 [29.444]
 [29.444]
 [29.444]
 [29.444]
 [29.444]] [[1.605]
 [1.509]
 [1.506]
 [1.506]
 [1.506]
 [1.506]
 [1.506]]
printing an ep nov before normalisation:  24.0794140936654
siam score:  -0.77924734
maxi score, test score, baseline:  -0.9258780219780219 -0.16533333333333328 -0.16533333333333328
probs:  [0.06380295289175014, 0.1185644121308591, 0.2370020080610704, 0.0485666044542089, 0.3392689498411124, 0.19279507262099915]
maxi score, test score, baseline:  -0.9258780219780219 -0.16533333333333328 -0.16533333333333328
probs:  [0.0638368611791378, 0.11877009596964247, 0.2364354416043196, 0.04855271941777937, 0.3391712788964112, 0.19323360293270955]
maxi score, test score, baseline:  -0.9260403508771929 -0.16533333333333328 -0.16533333333333328
probs:  [0.06359521550777394, 0.11880074073467702, 0.23649648018279057, 0.048565226524428615, 0.33925885488787366, 0.1932834821624563]
from probs:  [0.06359521550777394, 0.11880074073467702, 0.23649648018279057, 0.048565226524428615, 0.33925885488787366, 0.1932834821624563]
printing an ep nov before normalisation:  61.62414341423906
printing an ep nov before normalisation:  32.78531074523926
Printing some Q and Qe and total Qs values:  [[-0.125]
 [-0.125]
 [-0.125]
 [-0.125]
 [-0.125]
 [-0.125]
 [-0.125]] [[41.907]
 [41.907]
 [41.907]
 [41.907]
 [41.907]
 [41.907]
 [41.907]] [[0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]]
printing an ep nov before normalisation:  41.16947626450013
printing an ep nov before normalisation:  52.34393031166935
printing an ep nov before normalisation:  39.3128674881841
siam score:  -0.78365874
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9260403508771929 -0.16533333333333328 -0.16533333333333328
probs:  [0.0638424362866893, 0.1185535364313215, 0.2357106877518632, 0.04862307450024894, 0.3396648604555071, 0.19360540457437006]
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.331]
 [0.309]
 [0.309]
 [0.307]
 [0.301]
 [0.302]] [[53.603]
 [56.35 ]
 [54.638]
 [53.135]
 [52.363]
 [52.157]
 [51.813]] [[0.305]
 [0.331]
 [0.309]
 [0.309]
 [0.307]
 [0.301]
 [0.302]]
printing an ep nov before normalisation:  53.08979921285564
maxi score, test score, baseline:  -0.9260403508771929 -0.16533333333333328 -0.16533333333333328
printing an ep nov before normalisation:  52.29633835663198
siam score:  -0.78774506
maxi score, test score, baseline:  -0.9262019693654266 -0.16533333333333328 -0.16533333333333328
probs:  [0.0638424362866893, 0.1185535364313215, 0.2357106877518632, 0.04862307450024894, 0.3396648604555071, 0.19360540457437006]
maxi score, test score, baseline:  -0.9262019693654266 -0.16533333333333328 -0.16533333333333328
probs:  [0.0638424362866893, 0.1185535364313215, 0.2357106877518632, 0.04862307450024894, 0.3396648604555071, 0.19360540457437006]
maxi score, test score, baseline:  -0.9262019693654266 -0.16533333333333328 -0.16533333333333328
probs:  [0.0639139715139191, 0.1186865034516848, 0.23485326136107884, 0.04867752082445144, 0.3400461000399574, 0.19382264280890854]
Printing some Q and Qe and total Qs values:  [[-0.17 ]
 [-0.172]
 [-0.169]
 [-0.171]
 [-0.172]
 [-0.174]
 [-0.169]] [[43.152]
 [40.088]
 [44.279]
 [44.114]
 [44.38 ]
 [44.015]
 [43.267]] [[1.305]
 [1.102]
 [1.38 ]
 [1.367]
 [1.384]
 [1.357]
 [1.313]]
printing an ep nov before normalisation:  50.0853420026233
printing an ep nov before normalisation:  82.81322858042559
Printing some Q and Qe and total Qs values:  [[-0.085]
 [-0.07 ]
 [-0.085]
 [-0.085]
 [-0.085]
 [-0.085]
 [-0.085]] [[33.998]
 [40.121]
 [33.998]
 [33.998]
 [33.998]
 [33.998]
 [33.998]] [[0.589]
 [0.841]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]]
maxi score, test score, baseline:  -0.9262019693654266 -0.16533333333333328 -0.16533333333333328
probs:  [0.06394635214832768, 0.11888891068113772, 0.23430263672784496, 0.048662603995338796, 0.3399412064712373, 0.19425828997611347]
printing an ep nov before normalisation:  39.50841794438735
maxi score, test score, baseline:  -0.9262019693654266 -0.16533333333333328 -0.16533333333333328
probs:  [0.06394635214832768, 0.11888891068113772, 0.23430263672784496, 0.048662603995338796, 0.3399412064712373, 0.19425828997611347]
using another actor
printing an ep nov before normalisation:  55.65861370206653
Printing some Q and Qe and total Qs values:  [[-0.164]
 [-0.12 ]
 [-0.211]
 [-0.145]
 [-0.158]
 [-0.192]
 [-0.136]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.164]
 [-0.12 ]
 [-0.211]
 [-0.145]
 [-0.158]
 [-0.192]
 [-0.136]]
maxi score, test score, baseline:  -0.9263628820960699 -0.16533333333333328 -0.16533333333333328
probs:  [0.06401681072870055, 0.1190200335002895, 0.23345793567241893, 0.04871618718854098, 0.34031640182018275, 0.19447263108986723]
printing an ep nov before normalisation:  1.0018638931796886e-05
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.753]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]] [[31.852]
 [39.638]
 [31.852]
 [31.852]
 [31.852]
 [31.852]
 [31.852]] [[1.932]
 [2.317]
 [1.932]
 [1.932]
 [1.932]
 [1.932]
 [1.932]]
Printing some Q and Qe and total Qs values:  [[-0.145]
 [-0.144]
 [-0.146]
 [-0.145]
 [-0.148]
 [-0.145]
 [-0.145]] [[40.072]
 [44.533]
 [41.103]
 [41.547]
 [49.514]
 [41.853]
 [42.942]] [[0.862]
 [1.091]
 [0.913]
 [0.937]
 [1.34 ]
 [0.954]
 [1.008]]
maxi score, test score, baseline:  -0.9263628820960699 -0.16533333333333328 -0.16533333333333328
probs:  [0.06401681072870055, 0.1190200335002895, 0.23345793567241893, 0.04871618718854098, 0.34031640182018275, 0.19447263108986723]
printing an ep nov before normalisation:  56.512633567866914
maxi score, test score, baseline:  -0.9263628820960699 -0.16533333333333328 -0.16533333333333328
probs:  [0.06401681072870055, 0.1190200335002895, 0.23345793567241893, 0.04871618718854098, 0.34031640182018275, 0.19447263108986723]
Printing some Q and Qe and total Qs values:  [[-0.17 ]
 [-0.172]
 [-0.172]
 [-0.172]
 [-0.172]
 [-0.172]
 [-0.172]] [[40.71 ]
 [31.302]
 [31.302]
 [31.302]
 [31.302]
 [31.302]
 [31.302]] [[1.268]
 [0.732]
 [0.732]
 [0.732]
 [0.732]
 [0.732]
 [0.732]]
Printing some Q and Qe and total Qs values:  [[0.461]
 [0.594]
 [0.512]
 [0.479]
 [0.471]
 [0.467]
 [0.479]] [[14.848]
 [11.026]
 [14.439]
 [14.656]
 [14.696]
 [14.833]
 [16.091]] [[1.749]
 [1.55 ]
 [1.764]
 [1.749]
 [1.746]
 [1.753]
 [1.874]]
actor:  1 policy actor:  1  step number:  65 total reward:  0.38666666666666594  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9263628820960699 -0.16533333333333328 -0.16533333333333328
probs:  [0.06443190673461718, 0.11825519132555797, 0.23023815279999563, 0.04945951468695999, 0.3455260670700524, 0.1920891673828168]
printing an ep nov before normalisation:  44.49236896549105
maxi score, test score, baseline:  -0.9263628820960699 -0.16533333333333328 -0.16533333333333328
probs:  [0.06447855350315025, 0.1183408852766876, 0.2304050870391833, 0.0494952994330313, 0.3457766392416683, 0.19150353550627933]
printing an ep nov before normalisation:  0.09493950705300828
printing an ep nov before normalisation:  27.92049091875974
maxi score, test score, baseline:  -0.9263628820960699 -0.16533333333333328 -0.16533333333333328
probs:  [0.06444071910040927, 0.11841094151253038, 0.23069961700077143, 0.04942745235173632, 0.3453011275436262, 0.19172014249092634]
actions average: 
K:  1  action  0 :  tensor([0.3882, 0.0037, 0.1201, 0.1225, 0.1441, 0.1081, 0.1133],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0208, 0.8769, 0.0162, 0.0281, 0.0146, 0.0122, 0.0311],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2439, 0.0231, 0.2056, 0.1371, 0.1455, 0.1244, 0.1204],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1954, 0.0313, 0.1277, 0.2794, 0.1421, 0.1148, 0.1094],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2585, 0.0320, 0.1124, 0.1346, 0.1888, 0.1320, 0.1416],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1534, 0.0145, 0.1724, 0.1426, 0.1348, 0.2635, 0.1188],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1939, 0.0186, 0.1391, 0.1570, 0.1414, 0.1331, 0.2168],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9263628820960699 -0.16533333333333328 -0.16533333333333328
probs:  [0.06444071910040927, 0.11841094151253038, 0.23069961700077143, 0.04942745235173632, 0.3453011275436262, 0.19172014249092634]
maxi score, test score, baseline:  -0.9263628820960699 -0.16533333333333328 -0.16533333333333328
probs:  [0.06444071910040927, 0.11841094151253038, 0.23069961700077143, 0.04942745235173632, 0.3453011275436262, 0.19172014249092634]
printing an ep nov before normalisation:  0.004899043964314842
actor:  1 policy actor:  1  step number:  51 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9263628820960699 -0.16533333333333328 -0.16533333333333328
actions average: 
K:  3  action  0 :  tensor([0.1815, 0.0084, 0.1704, 0.2002, 0.1290, 0.1260, 0.1845],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0436, 0.8139, 0.0269, 0.0336, 0.0235, 0.0201, 0.0383],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1075, 0.0791, 0.2885, 0.1154, 0.1249, 0.1687, 0.1159],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1630, 0.3150, 0.0992, 0.1228, 0.1064, 0.0876, 0.1060],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2539, 0.0469, 0.1098, 0.1349, 0.2307, 0.0911, 0.1328],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1767, 0.0181, 0.1690, 0.0986, 0.1294, 0.3118, 0.0963],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1458, 0.1685, 0.0833, 0.1207, 0.1427, 0.0846, 0.2545],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  46.136958876813985
printing an ep nov before normalisation:  0.516988666593079
printing an ep nov before normalisation:  11.647064685821533
actions average: 
K:  2  action  0 :  tensor([0.4105, 0.0714, 0.0836, 0.1388, 0.1147, 0.0805, 0.1006],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0382, 0.8283, 0.0263, 0.0297, 0.0225, 0.0200, 0.0350],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1790, 0.0468, 0.2403, 0.1136, 0.1373, 0.1728, 0.1102],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1113, 0.0381, 0.1234, 0.2657, 0.1393, 0.1686, 0.1535],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1932, 0.0952, 0.1079, 0.1086, 0.2423, 0.1217, 0.1310],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1734, 0.0046, 0.1485, 0.0984, 0.1110, 0.3605, 0.1036],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1746, 0.0870, 0.1019, 0.1428, 0.1284, 0.1104, 0.2549],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  18.492750607497065
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9265230936819172 -0.16533333333333328 -0.16533333333333328
probs:  [0.0633661235157808, 0.11592961323280031, 0.24404857881530415, 0.048603732513771224, 0.3395332785947624, 0.18851867332758104]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9265230936819172 -0.16533333333333328 -0.16533333333333328
probs:  [0.0634381369709055, 0.11606148917567519, 0.24318841415519926, 0.048658933663334035, 0.33991980794346494, 0.1887332180914211]
actor:  0 policy actor:  1  step number:  38 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  53.671637310713585
maxi score, test score, baseline:  -0.923508695652174 -0.16533333333333328 -0.16533333333333328
printing an ep nov before normalisation:  47.5435693252017
maxi score, test score, baseline:  -0.923508695652174 -0.16533333333333328 -0.16533333333333328
probs:  [0.06354150252118163, 0.11638693772363008, 0.2417998453008075, 0.04869992747869907, 0.3402064284451981, 0.18936535853048367]
maxi score, test score, baseline:  -0.923508695652174 -0.16533333333333328 -0.16533333333333328
probs:  [0.06354150252118163, 0.11638693772363008, 0.2417998453008075, 0.04869992747869907, 0.3402064284451981, 0.18936535853048367]
actor:  0 policy actor:  0  step number:  42 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 32.82279008236352
printing an ep nov before normalisation:  71.6393266451009
maxi score, test score, baseline:  -0.9207098336948663 -0.16533333333333328 -0.16533333333333328
maxi score, test score, baseline:  -0.9207098336948663 -0.16533333333333328 -0.16533333333333328
probs:  [0.06350265319546043, 0.11645164420658638, 0.24211031066554797, 0.04863199463233537, 0.339730323919801, 0.18957307338026877]
printing an ep nov before normalisation:  46.25709077484157
siam score:  -0.78679293
maxi score, test score, baseline:  -0.9207098336948663 -0.16533333333333328 -0.16533333333333328
probs:  [0.06350265319546043, 0.11645164420658638, 0.24211031066554797, 0.04863199463233537, 0.339730323919801, 0.18957307338026877]
printing an ep nov before normalisation:  56.735478531987695
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.465]
 [0.464]
 [0.464]
 [0.465]
 [0.465]
 [0.465]] [[ 4.329]
 [ 6.423]
 [ 6.375]
 [ 3.934]
 [ 6.322]
 [ 4.499]
 [10.31 ]] [[0.509]
 [0.531]
 [0.53 ]
 [0.505]
 [0.53 ]
 [0.512]
 [0.572]]
actor:  1 policy actor:  1  step number:  67 total reward:  0.039999999999998814  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9207098336948663 -0.16533333333333328 -0.16533333333333328
probs:  [0.06626106292668477, 0.11610878980062618, 0.24139707099829125, 0.04848904206630756, 0.33872934433666074, 0.1890146898714296]
Printing some Q and Qe and total Qs values:  [[0.351]
 [0.436]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.261]
 [0.307]] [[40.271]
 [39.782]
 [39.836]
 [39.836]
 [39.836]
 [44.855]
 [43.589]] [[1.571]
 [1.628]
 [1.425]
 [1.425]
 [1.425]
 [1.741]
 [1.715]]
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.47 ]
 [0.34 ]
 [0.353]
 [0.395]
 [0.395]
 [0.43 ]] [[27.984]
 [28.206]
 [24.316]
 [24.354]
 [27.984]
 [27.984]
 [27.632]] [[1.961]
 [2.056]
 [1.576]
 [1.592]
 [1.961]
 [1.961]
 [1.964]]
maxi score, test score, baseline:  -0.9207098336948663 -0.16533333333333328 -0.16533333333333328
printing an ep nov before normalisation:  40.5306876042776
maxi score, test score, baseline:  -0.920881240981241 -0.16533333333333328 -0.16533333333333328
probs:  [0.0662277826941028, 0.11617249212778591, 0.24170453124538416, 0.0484211850098078, 0.33825377272757123, 0.18922023619534814]
actor:  1 policy actor:  1  step number:  73 total reward:  0.039999999999999036  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  61 total reward:  0.2533333333333325  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.920881240981241 -0.16533333333333328 -0.16533333333333328
probs:  [0.06640489639812845, 0.1158334754129679, 0.240068261935392, 0.0487823127330396, 0.340784712047416, 0.18812634147305596]
maxi score, test score, baseline:  -0.920881240981241 -0.16533333333333328 -0.16533333333333328
probs:  [0.06640489639812845, 0.1158334754129679, 0.240068261935392, 0.0487823127330396, 0.340784712047416, 0.18812634147305596]
printing an ep nov before normalisation:  23.435916900634766
actions average: 
K:  2  action  0 :  tensor([0.3533, 0.0796, 0.0925, 0.1125, 0.1576, 0.0946, 0.1099],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0238, 0.8735, 0.0129, 0.0262, 0.0130, 0.0145, 0.0360],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2133, 0.1077, 0.2289, 0.0988, 0.0865, 0.1384, 0.1263],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2267, 0.0826, 0.1145, 0.1535, 0.1263, 0.1198, 0.1767],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2516, 0.0172, 0.0864, 0.1090, 0.3221, 0.1124, 0.1012],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1941, 0.0118, 0.1230, 0.1512, 0.1379, 0.2332, 0.1488],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2446, 0.0122, 0.0905, 0.0897, 0.1800, 0.1006, 0.2824],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.493]
 [0.286]
 [0.286]
 [0.286]
 [0.385]
 [0.286]] [[48.039]
 [44.999]
 [48.039]
 [48.039]
 [48.039]
 [51.56 ]
 [48.039]] [[1.69 ]
 [1.738]
 [1.69 ]
 [1.69 ]
 [1.69 ]
 [1.974]
 [1.69 ]]
maxi score, test score, baseline:  -0.920881240981241 -0.16533333333333328 -0.16533333333333328
probs:  [0.06637213745169139, 0.11589617993638743, 0.24037090626761654, 0.04871551855803564, 0.34031658957168565, 0.18832866821458338]
UNIT TEST: sample policy line 217 mcts : [0.122 0.245 0.122 0.02  0.306 0.163 0.02 ]
printing an ep nov before normalisation:  39.88449584056165
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.920881240981241 -0.16533333333333328 -0.16533333333333328
probs:  [0.06640518986551712, 0.11545535523549073, 0.24049078195874726, 0.048739760532977766, 0.3404863369477485, 0.18842257545951882]
maxi score, test score, baseline:  -0.920881240981241 -0.16533333333333328 -0.16533333333333328
probs:  [0.06640518986551712, 0.11545535523549073, 0.24049078195874726, 0.048739760532977766, 0.3404863369477485, 0.18842257545951882]
printing an ep nov before normalisation:  63.18036715958014
actor:  1 policy actor:  1  step number:  59 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.920881240981241 -0.16533333333333328 -0.16533333333333328
maxi score, test score, baseline:  -0.920881240981241 -0.16533333333333328 -0.16533333333333328
probs:  [0.06666209151558002, 0.1149693334674562, 0.23811094911171138, 0.04926422623798109, 0.34416202179423777, 0.18683137787303364]
line 256 mcts: sample exp_bonus 26.366495808581647
maxi score, test score, baseline:  -0.920881240981241 -0.16533333333333328 -0.16533333333333328
probs:  [0.06672649335883107, 0.11411324331442582, 0.23834131835109504, 0.04931178645021241, 0.34449505150413706, 0.18701210702129858]
printing an ep nov before normalisation:  44.01770484366631
maxi score, test score, baseline:  -0.9210519078473722 -0.16533333333333328 -0.16533333333333328
printing an ep nov before normalisation:  30.848920345306396
printing an ep nov before normalisation:  54.51402551967452
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]] [[12.237]
 [12.237]
 [12.237]
 [38.051]
 [12.237]
 [12.237]
 [12.237]] [[0.561]
 [0.561]
 [0.561]
 [1.583]
 [0.561]
 [0.561]
 [0.561]]
actor:  1 policy actor:  1  step number:  61 total reward:  0.27999999999999914  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  51.63548953801224
Printing some Q and Qe and total Qs values:  [[0.316]
 [0.29 ]
 [0.316]
 [0.316]
 [0.316]
 [0.316]
 [0.316]] [[26.772]
 [33.945]
 [26.772]
 [26.772]
 [26.772]
 [26.772]
 [26.772]] [[0.316]
 [0.29 ]
 [0.316]
 [0.316]
 [0.316]
 [0.316]
 [0.316]]
maxi score, test score, baseline:  -0.9210519078473722 -0.16533333333333328 -0.16533333333333328
probs:  [0.0664115816054374, 0.11357424517397695, 0.23721486020269292, 0.04907922680487499, 0.342866604792329, 0.19085348142068867]
maxi score, test score, baseline:  -0.9210519078473722 -0.16533333333333328 -0.16533333333333328
probs:  [0.0664115816054374, 0.11357424517397695, 0.23721486020269292, 0.04907922680487499, 0.342866604792329, 0.19085348142068867]
Printing some Q and Qe and total Qs values:  [[-0.037]
 [ 0.298]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]] [[32.884]
 [36.271]
 [32.884]
 [32.884]
 [32.884]
 [32.884]
 [32.884]] [[0.309]
 [0.702]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]]
maxi score, test score, baseline:  -0.9210519078473722 -0.16533333333333328 -0.16533333333333328
probs:  [0.0664115816054374, 0.11357424517397695, 0.23721486020269292, 0.04907922680487499, 0.342866604792329, 0.19085348142068867]
printing an ep nov before normalisation:  51.778591196911364
maxi score, test score, baseline:  -0.9212218390804598 -0.16533333333333328 -0.16533333333333328
probs:  [0.06644301498742085, 0.113154162017477, 0.2373272992872476, 0.04910244008950805, 0.343029150607086, 0.19094393301126047]
maxi score, test score, baseline:  -0.9212218390804598 -0.16533333333333328 -0.16533333333333328
printing an ep nov before normalisation:  29.463372230529785
printing an ep nov before normalisation:  21.697509288787842
actor:  1 policy actor:  1  step number:  52 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.245]
 [0.291]
 [0.245]
 [0.245]
 [0.245]
 [0.245]
 [0.245]] [[46.031]
 [43.981]
 [46.031]
 [46.031]
 [46.031]
 [46.031]
 [46.031]] [[1.77 ]
 [1.705]
 [1.77 ]
 [1.77 ]
 [1.77 ]
 [1.77 ]
 [1.77 ]]
maxi score, test score, baseline:  -0.9212218390804598 -0.16533333333333328 -0.16533333333333328
probs:  [0.06662804010513491, 0.11282849698107537, 0.23564405682786968, 0.049477048667062484, 0.34565456032008063, 0.189767797098777]
maxi score, test score, baseline:  -0.9212218390804598 -0.16533333333333328 -0.16533333333333328
maxi score, test score, baseline:  -0.9212218390804598 -0.16533333333333328 -0.16533333333333328
probs:  [0.06662804010513491, 0.11282849698107537, 0.23564405682786968, 0.049477048667062484, 0.34565456032008063, 0.189767797098777]
printing an ep nov before normalisation:  38.991561997243025
printing an ep nov before normalisation:  23.4511661529541
printing an ep nov before normalisation:  35.269140554703284
printing an ep nov before normalisation:  28.14825325510043
Printing some Q and Qe and total Qs values:  [[0.318]
 [0.396]
 [0.318]
 [0.286]
 [0.197]
 [0.187]
 [0.318]] [[23.567]
 [17.256]
 [23.567]
 [14.401]
 [14.418]
 [14.574]
 [23.567]] [[3.174]
 [2.063]
 [3.174]
 [1.415]
 [1.329]
 [1.349]
 [3.174]]
actor:  1 policy actor:  1  step number:  60 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9212218390804598 -0.16533333333333328 -0.16533333333333328
probs:  [0.06675858270376359, 0.11259872731164013, 0.234456462151622, 0.049741349956458875, 0.34750689156371706, 0.1889379863127983]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.7289400100708
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  82 total reward:  0.09999999999999865  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  58 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9212218390804598 -0.16533333333333328 -0.16533333333333328
printing an ep nov before normalisation:  46.655965575897014
Starting evaluation
printing an ep nov before normalisation:  38.73925174707988
Printing some Q and Qe and total Qs values:  [[0.241]
 [0.263]
 [0.242]
 [0.299]
 [0.299]
 [0.299]
 [0.299]] [[40.553]
 [33.192]
 [43.675]
 [39.157]
 [39.157]
 [39.157]
 [39.157]] [[0.241]
 [0.263]
 [0.242]
 [0.299]
 [0.299]
 [0.299]
 [0.299]]
printing an ep nov before normalisation:  44.58660992454548
using explorer policy with actor:  0
printing an ep nov before normalisation:  29.783646265665688
Printing some Q and Qe and total Qs values:  [[0.744]
 [0.891]
 [0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]] [[43.78 ]
 [43.991]
 [43.78 ]
 [43.78 ]
 [43.78 ]
 [43.78 ]
 [43.78 ]] [[0.744]
 [0.891]
 [0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]]
printing an ep nov before normalisation:  41.843075287165696
maxi score, test score, baseline:  -0.9212218390804598 -0.16533333333333328 -0.16533333333333328
probs:  [0.06606381405035683, 0.11080312180337094, 0.24300314665215253, 0.049286843779049086, 0.3443248795341049, 0.18651819418096563]
printing an ep nov before normalisation:  52.22731789039364
printing an ep nov before normalisation:  49.3726856543984
maxi score, test score, baseline:  -0.9212218390804598 -0.16533333333333328 -0.16533333333333328
probs:  [0.06606381405035683, 0.11080312180337094, 0.24300314665215253, 0.049286843779049086, 0.3443248795341049, 0.18651819418096563]
maxi score, test score, baseline:  -0.9212218390804598 -0.16533333333333328 -0.16533333333333328
probs:  [0.06606381405035683, 0.11080312180337094, 0.24300314665215253, 0.049286843779049086, 0.3443248795341049, 0.18651819418096563]
printing an ep nov before normalisation:  46.2502871955377
printing an ep nov before normalisation:  35.18028687464167
printing an ep nov before normalisation:  44.43660286657266
maxi score, test score, baseline:  -0.9212218390804598 -0.16533333333333328 -0.16533333333333328
printing an ep nov before normalisation:  29.203458583717218
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.743]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]] [[29.107]
 [28.538]
 [29.107]
 [29.107]
 [29.107]
 [29.107]
 [29.107]] [[0.421]
 [0.743]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]]
actor:  0 policy actor:  1  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  34.08066286857179
actor:  0 policy actor:  1  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]]
actor:  0 policy actor:  1  step number:  26 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  26 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  27 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  27 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  27 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  27 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  27 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  28 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  28 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  32 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  32 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.8559881542699725 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  44 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  56 total reward:  0.206666666666666  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8513951989026063 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.112]
 [-0.112]
 [-0.112]
 [-0.112]
 [-0.112]
 [-0.112]
 [-0.112]] [[41.078]
 [41.078]
 [41.078]
 [41.078]
 [41.078]
 [41.078]
 [41.078]] [[0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]]
maxi score, test score, baseline:  -0.8513951989026063 0.6573333333333335 0.6573333333333335
maxi score, test score, baseline:  -0.8513951989026063 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.128]
 [-0.105]
 [-0.128]
 [-0.128]
 [-0.128]
 [-0.128]
 [-0.128]] [[44.474]
 [48.146]
 [44.474]
 [44.474]
 [44.474]
 [44.474]
 [44.474]] [[0.658]
 [0.794]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]]
printing an ep nov before normalisation:  45.30604758643931
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.098]
 [-0.098]
 [-0.098]
 [-0.098]
 [-0.098]
 [-0.098]
 [-0.098]] [[43.613]
 [43.613]
 [43.613]
 [43.613]
 [43.613]
 [43.613]
 [43.613]] [[1.66]
 [1.66]
 [1.66]
 [1.66]
 [1.66]
 [1.66]
 [1.66]]
printing an ep nov before normalisation:  23.94304900184178
Printing some Q and Qe and total Qs values:  [[0.36]
 [0.36]
 [0.36]
 [0.36]
 [0.36]
 [0.36]
 [0.36]] [[33.528]
 [33.528]
 [33.528]
 [33.528]
 [33.528]
 [33.528]
 [33.528]] [[1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]]
maxi score, test score, baseline:  -0.8513951989026063 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8513951989026063 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8513951989026063 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8513951989026063 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.564406996877935
maxi score, test score, baseline:  -0.8517001368925393 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.21847713704256
maxi score, test score, baseline:  -0.8517001368925393 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.837]
 [0.872]
 [0.756]
 [0.662]
 [0.837]
 [0.837]
 [0.772]] [[48.848]
 [48.575]
 [53.314]
 [55.161]
 [48.848]
 [48.848]
 [54.639]] [[0.837]
 [0.872]
 [0.756]
 [0.662]
 [0.837]
 [0.837]
 [0.772]]
printing an ep nov before normalisation:  40.276007424951494
printing an ep nov before normalisation:  34.59619002828663
using explorer policy with actor:  1
printing an ep nov before normalisation:  27.53289416465129
maxi score, test score, baseline:  -0.8517001368925393 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.083]
 [-0.09 ]
 [-0.101]
 [-0.089]
 [-0.081]
 [-0.083]
 [-0.092]] [[ 6.994]
 [12.22 ]
 [11.008]
 [11.745]
 [12.348]
 [12.76 ]
 [11.651]] [[0.106]
 [0.244]
 [0.199]
 [0.232]
 [0.257]
 [0.266]
 [0.227]]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  68 total reward:  0.0733333333333328  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.8498043715846995 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.07084666044387
printing an ep nov before normalisation:  17.694703472263598
using explorer policy with actor:  1
using explorer policy with actor:  1
actions average: 
K:  0  action  0 :  tensor([0.3569, 0.0159, 0.0849, 0.1237, 0.2059, 0.0857, 0.1269],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0120, 0.9185, 0.0154, 0.0173, 0.0072, 0.0094, 0.0202],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1797, 0.0470, 0.2280, 0.1348, 0.1081, 0.1501, 0.1524],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1822, 0.0555, 0.1006, 0.2611, 0.1259, 0.1054, 0.1694],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2400, 0.0391, 0.1177, 0.1200, 0.2232, 0.1110, 0.1490],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1727, 0.0057, 0.1492, 0.1176, 0.1363, 0.2659, 0.1526],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1913, 0.1031, 0.1127, 0.1848, 0.1312, 0.1268, 0.1500],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  19.225659271063208
maxi score, test score, baseline:  -0.8498043715846995 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
printing an ep nov before normalisation:  40.76988515515918
maxi score, test score, baseline:  -0.8498043715846995 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8498043715846995 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8498043715846995 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8498043715846995 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.66494471432082
maxi score, test score, baseline:  -0.8498043715846995 0.6573333333333335 0.6573333333333335
printing an ep nov before normalisation:  36.8414831161499
maxi score, test score, baseline:  -0.8498043715846995 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.114]
 [-0.088]
 [-0.111]
 [-0.114]
 [-0.111]
 [-0.114]
 [-0.114]] [[23.726]
 [26.966]
 [25.245]
 [23.726]
 [25.518]
 [23.726]
 [23.726]] [[0.097]
 [0.181]
 [0.127]
 [0.097]
 [0.133]
 [0.097]
 [0.097]]
actor:  0 policy actor:  0  step number:  45 total reward:  0.2933333333333328  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.8474664621676892 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.23504081996167
printing an ep nov before normalisation:  55.42928503423648
printing an ep nov before normalisation:  40.5467356741699
Printing some Q and Qe and total Qs values:  [[-0.198]
 [-0.198]
 [-0.198]
 [-0.197]
 [-0.197]
 [-0.198]
 [-0.197]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.198]
 [-0.198]
 [-0.198]
 [-0.197]
 [-0.197]
 [-0.198]
 [-0.197]]
Printing some Q and Qe and total Qs values:  [[-0.189]
 [-0.189]
 [-0.189]
 [-0.205]
 [-0.189]
 [-0.189]
 [-0.189]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.189]
 [-0.189]
 [-0.189]
 [-0.205]
 [-0.189]
 [-0.189]
 [-0.189]]
line 256 mcts: sample exp_bonus 26.985127689501883
maxi score, test score, baseline:  -0.8477775510204081 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.074]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.074]] [[35.148]
 [24.341]
 [24.341]
 [24.341]
 [24.341]
 [24.341]
 [24.341]] [[1.101]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]]
printing an ep nov before normalisation:  12.299751584743186
maxi score, test score, baseline:  -0.8477775510204081 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8477775510204081 0.6573333333333335 0.6573333333333335
maxi score, test score, baseline:  -0.8477775510204081 0.6573333333333335 0.6573333333333335
maxi score, test score, baseline:  -0.8477775510204081 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.818850123931185
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.333]
 [0.325]
 [0.328]
 [0.328]
 [0.331]
 [0.327]] [[47.972]
 [46.106]
 [47.742]
 [47.773]
 [47.86 ]
 [47.981]
 [47.816]] [[0.327]
 [0.333]
 [0.325]
 [0.328]
 [0.328]
 [0.331]
 [0.327]]
printing an ep nov before normalisation:  38.98729923465161
printing an ep nov before normalisation:  21.408627033233643
printing an ep nov before normalisation:  28.433925156142873
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
printing an ep nov before normalisation:  23.85702201730654
printing an ep nov before normalisation:  23.800898136619708
printing an ep nov before normalisation:  16.431219919122032
maxi score, test score, baseline:  -0.8477775510204081 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  49 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  46.69352452688691
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.58237644217183
printing an ep nov before normalisation:  34.81904145324594
maxi score, test score, baseline:  -0.8452632043448743 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.07455435097453
maxi score, test score, baseline:  -0.8452632043448743 0.6573333333333335 0.6573333333333335
printing an ep nov before normalisation:  0.3483621709563067
maxi score, test score, baseline:  -0.8452632043448743 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.399368472594652
printing an ep nov before normalisation:  25.776379795544813
printing an ep nov before normalisation:  51.28811569851083
maxi score, test score, baseline:  -0.8452632043448743 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.21293927489519
Printing some Q and Qe and total Qs values:  [[-0.118]
 [-0.108]
 [-0.132]
 [-0.132]
 [-0.132]
 [-0.132]
 [-0.132]] [[22.525]
 [26.627]
 [22.509]
 [22.509]
 [22.509]
 [22.509]
 [22.509]] [[0.18 ]
 [0.282]
 [0.166]
 [0.166]
 [0.166]
 [0.166]
 [0.166]]
maxi score, test score, baseline:  -0.8452632043448743 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8452632043448743 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8452632043448743 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8452632043448743 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8452632043448743 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.38400173253498
maxi score, test score, baseline:  -0.8452632043448743 0.6573333333333335 0.6573333333333335
Printing some Q and Qe and total Qs values:  [[0.764]
 [0.825]
 [0.764]
 [0.764]
 [0.764]
 [0.706]
 [0.867]] [[56.71 ]
 [52.318]
 [56.71 ]
 [56.71 ]
 [56.71 ]
 [59.132]
 [55.812]] [[1.08 ]
 [1.111]
 [1.08 ]
 [1.08 ]
 [1.08 ]
 [1.039]
 [1.176]]
maxi score, test score, baseline:  -0.8452632043448743 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.687901084260247
maxi score, test score, baseline:  -0.8452632043448743 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.0000],
        [-0.4799],
        [-0.6982],
        [ 0.3714],
        [-0.5086],
        [-0.6396],
        [-0.3929],
        [-0.4468],
        [ 0.7120]], dtype=torch.float64)
0.99 0.99
-0.95403 -0.95403
-0.057834381198 -0.5377665845548872
-0.032346567066 -0.730516284200814
-0.057834381198 0.3135838818876234
-0.09703970119800001 -0.6056426607058968
-0.032346567066 -0.671972789667848
-0.084359833866 -0.47723026467261864
-0.08410238119800001 -0.5308845556107313
-0.084359833866 0.6276550004248055
printing an ep nov before normalisation:  41.639333369159374
maxi score, test score, baseline:  -0.8452632043448743 0.6573333333333335 0.6573333333333335
printing an ep nov before normalisation:  25.31421749440728
maxi score, test score, baseline:  -0.8452632043448743 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8452632043448743 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.98503830340001
Printing some Q and Qe and total Qs values:  [[-0.1  ]
 [-0.094]
 [-0.1  ]
 [-0.113]
 [-0.1  ]
 [-0.1  ]
 [-0.1  ]] [[33.742]
 [36.193]
 [33.742]
 [33.626]
 [33.742]
 [33.742]
 [33.742]] [[1.482]
 [1.605]
 [1.482]
 [1.464]
 [1.482]
 [1.482]
 [1.482]]
maxi score, test score, baseline:  -0.8452632043448743 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  53.53744822415081
printing an ep nov before normalisation:  19.50300552593209
Printing some Q and Qe and total Qs values:  [[-0.097]
 [-0.112]
 [-0.112]
 [-0.112]
 [-0.112]
 [-0.112]
 [-0.112]] [[26.998]
 [18.103]
 [18.103]
 [18.103]
 [18.103]
 [18.103]
 [18.103]] [[1.459]
 [0.927]
 [0.927]
 [0.927]
 [0.927]
 [0.927]
 [0.927]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  29.11067922572948
maxi score, test score, baseline:  -0.8452632043448743 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.056]
 [-0.053]
 [-0.073]
 [-0.061]
 [ 0.23 ]
 [-0.076]
 [-0.068]] [[26.565]
 [27.113]
 [26.736]
 [28.826]
 [40.372]
 [29.449]
 [27.297]] [[0.497]
 [0.524]
 [0.488]
 [0.586]
 [1.356]
 [0.598]
 [0.516]]
actions average: 
K:  2  action  0 :  tensor([0.4017, 0.1776, 0.0807, 0.0828, 0.0987, 0.0613, 0.0972],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0183, 0.8876, 0.0197, 0.0190, 0.0099, 0.0132, 0.0323],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1544, 0.0969, 0.1780, 0.1346, 0.1316, 0.1773, 0.1272],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1593, 0.0069, 0.1181, 0.4106, 0.1059, 0.1048, 0.0944],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2583, 0.0897, 0.1094, 0.1164, 0.2120, 0.1052, 0.1089],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1437, 0.0252, 0.2092, 0.1142, 0.0989, 0.2737, 0.1351],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1779, 0.1266, 0.1910, 0.1296, 0.0935, 0.1285, 0.1529],
       grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 46.840668181108484
printing an ep nov before normalisation:  37.75784902721426
maxi score, test score, baseline:  -0.8452632043448743 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.64 ]
 [0.697]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]] [[27.28]
 [26.87]
 [27.28]
 [27.28]
 [27.28]
 [27.28]
 [27.28]] [[0.64 ]
 [0.697]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  34.736265895631924
siam score:  -0.77442706
actor:  0 policy actor:  0  step number:  48 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.8433888438133875 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  11.956811851626087
siam score:  -0.7832375
maxi score, test score, baseline:  -0.8437056680161944 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  57.17434506321121
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8437056680161944 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.976335461672335
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.78728675842285
maxi score, test score, baseline:  -0.8437056680161944 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8437056680161944 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.114347001879416
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8437056680161944 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.213078475185355
printing an ep nov before normalisation:  25.096385723152114
Printing some Q and Qe and total Qs values:  [[0.125]
 [0.429]
 [0.125]
 [0.125]
 [0.125]
 [0.125]
 [0.125]] [[34.316]
 [35.688]
 [34.316]
 [34.316]
 [34.316]
 [34.316]
 [34.316]] [[0.494]
 [0.827]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]]
Printing some Q and Qe and total Qs values:  [[0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]] [[29.035]
 [29.035]
 [29.035]
 [29.035]
 [29.035]
 [29.035]
 [29.035]] [[2.707]
 [2.707]
 [2.707]
 [2.707]
 [2.707]
 [2.707]
 [2.707]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  59 total reward:  0.05333333333333279  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  31.224638386179
printing an ep nov before normalisation:  34.34341907501221
actor:  1 policy actor:  1  step number:  65 total reward:  0.0799999999999994  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  48 total reward:  0.37999999999999945  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  1  action  0 :  tensor([0.3804, 0.0301, 0.1249, 0.0999, 0.1546, 0.1051, 0.1050],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0072, 0.9526, 0.0061, 0.0119, 0.0055, 0.0035, 0.0132],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2044, 0.0038, 0.1916, 0.1635, 0.1402, 0.1463, 0.1502],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1263, 0.1235, 0.1115, 0.2687, 0.1191, 0.1016, 0.1493],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1720, 0.0202, 0.1341, 0.1134, 0.2796, 0.1371, 0.1434],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1792, 0.0233, 0.1233, 0.0990, 0.1017, 0.3460, 0.1275],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1151, 0.2037, 0.1412, 0.1213, 0.1016, 0.0928, 0.2244],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.138]
 [0.382]
 [0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]] [[34.902]
 [44.961]
 [34.902]
 [34.902]
 [34.902]
 [34.902]
 [34.902]] [[0.443]
 [0.857]
 [0.443]
 [0.443]
 [0.443]
 [0.443]
 [0.443]]
printing an ep nov before normalisation:  41.14054376489925
maxi score, test score, baseline:  -0.8412333333333333 0.6573333333333335 0.6573333333333335
maxi score, test score, baseline:  -0.8412333333333333 0.6573333333333335 0.6573333333333335
maxi score, test score, baseline:  -0.8412333333333333 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.492]
 [0.368]
 [0.384]
 [0.387]
 [0.371]
 [0.376]] [[21.501]
 [22.947]
 [21.718]
 [25.288]
 [21.705]
 [22.18 ]
 [22.723]] [[0.686]
 [0.896]
 [0.732]
 [0.865]
 [0.75 ]
 [0.75 ]
 [0.773]]
actor:  1 policy actor:  1  step number:  61 total reward:  0.25333333333333297  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  70 total reward:  0.08666666666666611  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8412333333333333 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8412333333333333 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.288]
 [0.28 ]
 [0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]] [[44.822]
 [48.416]
 [44.822]
 [44.822]
 [44.822]
 [44.822]
 [44.822]] [[1.166]
 [1.28 ]
 [1.166]
 [1.166]
 [1.166]
 [1.166]
 [1.166]]
printing an ep nov before normalisation:  35.156261137244044
printing an ep nov before normalisation:  30.244171241615163
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.235]
 [0.446]
 [0.235]
 [0.235]
 [0.235]
 [0.235]
 [0.235]] [[39.047]
 [42.663]
 [39.047]
 [39.047]
 [39.047]
 [39.047]
 [39.047]] [[0.85 ]
 [1.164]
 [0.85 ]
 [0.85 ]
 [0.85 ]
 [0.85 ]
 [0.85 ]]
maxi score, test score, baseline:  -0.8415532258064516 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8415532258064516 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8415532258064516 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8415532258064516 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.458]
 [0.676]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]] [[27.563]
 [34.283]
 [27.563]
 [27.563]
 [27.563]
 [27.563]
 [27.563]] [[1.02 ]
 [1.512]
 [1.02 ]
 [1.02 ]
 [1.02 ]
 [1.02 ]
 [1.02 ]]
maxi score, test score, baseline:  -0.8415532258064516 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8415532258064516 0.6573333333333335 0.6573333333333335
maxi score, test score, baseline:  -0.8415532258064516 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.2266666666666659  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.189]
 [-0.167]
 [-0.182]
 [-0.184]
 [-0.189]
 [-0.191]
 [-0.183]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.189]
 [-0.167]
 [-0.182]
 [-0.184]
 [-0.189]
 [-0.191]
 [-0.183]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8415532258064516 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8415532258064516 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]] [[36.014]
 [36.014]
 [36.014]
 [36.014]
 [36.014]
 [36.014]
 [36.014]] [[0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]]
siam score:  -0.79350495
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8418718309859156 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8418718309859156 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
maxi score, test score, baseline:  -0.8418718309859156 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8418718309859156 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.489989939154203
maxi score, test score, baseline:  -0.8418718309859156 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8418718309859156 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8418718309859156 0.6573333333333335 0.6573333333333335
printing an ep nov before normalisation:  1.1568978530230822
printing an ep nov before normalisation:  9.962882709766632
printing an ep nov before normalisation:  21.92475608644233
actor:  1 policy actor:  1  step number:  51 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  1.333
deleting a thread, now have 2 threads
Frames:  63150 train batches done:  7394 episodes:  1903
maxi score, test score, baseline:  -0.8418718309859156 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8418718309859156 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8418718309859156 0.6573333333333335 0.6573333333333335
maxi score, test score, baseline:  -0.8418718309859156 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8418718309859156 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8421891566265061 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.78666753895465
siam score:  -0.7998286
line 256 mcts: sample exp_bonus 18.480838419204986
maxi score, test score, baseline:  -0.8421891566265061 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8421891566265061 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.342854499816895
maxi score, test score, baseline:  -0.8421891566265061 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.021910753089735
using explorer policy with actor:  1
printing an ep nov before normalisation:  19.525442692815407
maxi score, test score, baseline:  -0.8421891566265061 0.6573333333333335 0.6573333333333335
actor:  1 policy actor:  1  step number:  68 total reward:  0.20666666666666644  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  50.57236628635762
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8421891566265061 0.6573333333333335 0.6573333333333335
maxi score, test score, baseline:  -0.8421891566265061 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8421891566265061 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8425052104208418 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8425052104208418 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8425052104208418 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.367]
 [0.339]
 [0.349]
 [0.343]
 [0.334]
 [0.344]] [[36.03 ]
 [34.852]
 [35.556]
 [35.806]
 [35.809]
 [36.16 ]
 [34.871]] [[0.341]
 [0.367]
 [0.339]
 [0.349]
 [0.343]
 [0.334]
 [0.344]]
maxi score, test score, baseline:  -0.8425052104208418 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.185]
 [-0.192]
 [-0.19 ]
 [-0.19 ]
 [-0.186]
 [-0.189]
 [-0.19 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.185]
 [-0.192]
 [-0.19 ]
 [-0.19 ]
 [-0.186]
 [-0.189]
 [-0.19 ]]
maxi score, test score, baseline:  -0.8425052104208418 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  13.169421957140132
maxi score, test score, baseline:  -0.8425052104208418 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.721]
 [0.888]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]] [[20.388]
 [28.642]
 [20.388]
 [20.388]
 [20.388]
 [20.388]
 [20.388]] [[0.721]
 [0.888]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]]
siam score:  -0.7993573
maxi score, test score, baseline:  -0.8425052104208418 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.354]
 [0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[29.937]
 [35.846]
 [29.937]
 [29.937]
 [29.937]
 [29.937]
 [29.937]] [[0.467]
 [0.995]
 [0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]]
maxi score, test score, baseline:  -0.8428200000000001 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8428200000000001 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8428200000000001 0.6573333333333335 0.6573333333333335
Printing some Q and Qe and total Qs values:  [[-0.144]
 [-0.144]
 [-0.149]
 [-0.149]
 [-0.147]
 [-0.145]
 [-0.147]] [[23.471]
 [22.411]
 [19.564]
 [19.601]
 [19.454]
 [19.319]
 [19.185]] [[0.747]
 [0.679]
 [0.492]
 [0.495]
 [0.487]
 [0.48 ]
 [0.47 ]]
maxi score, test score, baseline:  -0.8428200000000001 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.676928422321268
maxi score, test score, baseline:  -0.8428200000000001 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.968338037619468
actor:  1 policy actor:  1  step number:  57 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8428200000000001 0.6573333333333335 0.6573333333333335
maxi score, test score, baseline:  -0.8428200000000001 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
maxi score, test score, baseline:  -0.8428200000000001 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.79439217
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.66714314121143
printing an ep nov before normalisation:  14.805501197659044
actions average: 
K:  4  action  0 :  tensor([0.3592, 0.0128, 0.1167, 0.1420, 0.1426, 0.1278, 0.0989],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0274, 0.8566, 0.0259, 0.0265, 0.0174, 0.0168, 0.0294],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1873, 0.0864, 0.1310, 0.1192, 0.1599, 0.1434, 0.1729],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1357, 0.2121, 0.0864, 0.2353, 0.0818, 0.1076, 0.1410],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1805, 0.0531, 0.0692, 0.1166, 0.3746, 0.1267, 0.0793],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1627, 0.1875, 0.1141, 0.1327, 0.1384, 0.1509, 0.1137],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1531, 0.2658, 0.1200, 0.1055, 0.1338, 0.1050, 0.1168],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.8428200000000001 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8428200000000001 0.6573333333333335 0.6573333333333335
printing an ep nov before normalisation:  39.17077591582055
printing an ep nov before normalisation:  29.985371752991586
maxi score, test score, baseline:  -0.8428200000000001 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8428200000000001 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.28109252202515
maxi score, test score, baseline:  -0.8428200000000001 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8428200000000001 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7955502
maxi score, test score, baseline:  -0.8428200000000001 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.890435682510386
maxi score, test score, baseline:  -0.8428200000000001 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 30.3760530303776
actions average: 
K:  1  action  0 :  tensor([0.3061, 0.0508, 0.1137, 0.1119, 0.1436, 0.1359, 0.1380],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0277, 0.8327, 0.0223, 0.0278, 0.0167, 0.0214, 0.0514],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1141, 0.0155, 0.4652, 0.0900, 0.0678, 0.1230, 0.1244],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1814, 0.0203, 0.1375, 0.2036, 0.1170, 0.1396, 0.2006],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2666, 0.0209, 0.0742, 0.0852, 0.3136, 0.0883, 0.1511],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1331, 0.0041, 0.1914, 0.1500, 0.1498, 0.1988, 0.1728],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1140, 0.2339, 0.0893, 0.1075, 0.1040, 0.1063, 0.2450],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]] [[32.127]
 [32.127]
 [32.127]
 [32.127]
 [32.127]
 [32.127]
 [32.127]] [[0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]]
maxi score, test score, baseline:  -0.8428200000000001 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8428200000000001 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8004985
maxi score, test score, baseline:  -0.8428200000000001 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.14168050639694
maxi score, test score, baseline:  -0.8428200000000001 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.591711136873066
printing an ep nov before normalisation:  30.50389459546679
printing an ep nov before normalisation:  24.488015151783788
printing an ep nov before normalisation:  41.38313674687481
maxi score, test score, baseline:  -0.8448200000000001 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8448200000000001 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8448200000000001 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8076744
maxi score, test score, baseline:  -0.8448200000000001 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.451]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]] [[44.227]
 [44.215]
 [44.227]
 [44.227]
 [44.227]
 [44.227]
 [44.227]] [[0.421]
 [0.451]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]]
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  55 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8448200000000001 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8066185
printing an ep nov before normalisation:  53.2255489184293
maxi score, test score, baseline:  -0.8448200000000001 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.802829421995135
maxi score, test score, baseline:  -0.8448200000000001 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.84482 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.84482 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.84482 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.02589164886444
maxi score, test score, baseline:  -0.84482 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.645]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]] [[39.128]
 [40.003]
 [39.128]
 [39.128]
 [39.128]
 [39.128]
 [39.128]] [[0.73 ]
 [0.919]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]]
maxi score, test score, baseline:  -0.84482 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.84482 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.18325996398926
actor:  1 policy actor:  1  step number:  63 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.566]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]] [[37.004]
 [44.506]
 [37.004]
 [37.004]
 [37.004]
 [37.004]
 [37.004]] [[1.482]
 [1.868]
 [1.482]
 [1.482]
 [1.482]
 [1.482]
 [1.482]]
maxi score, test score, baseline:  -0.84482 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.579]
 [0.579]
 [0.579]
 [0.369]
 [0.579]
 [0.334]
 [0.579]] [[42.975]
 [36.841]
 [42.975]
 [38.778]
 [42.975]
 [39.071]
 [42.975]] [[1.934]
 [1.654]
 [1.934]
 [1.532]
 [1.934]
 [1.51 ]
 [1.934]]
printing an ep nov before normalisation:  39.37504135493134
maxi score, test score, baseline:  -0.84482 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.84482 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.344]
 [0.446]
 [0.344]
 [0.344]
 [0.344]
 [0.344]
 [0.344]] [[37.654]
 [37.384]
 [37.654]
 [37.654]
 [37.654]
 [37.654]
 [37.654]] [[1.832]
 [1.923]
 [1.832]
 [1.832]
 [1.832]
 [1.832]
 [1.832]]
printing an ep nov before normalisation:  0.02016726490637666
maxi score, test score, baseline:  -0.84482 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.84482 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.84482 0.6573333333333335 0.6573333333333335
actor:  1 policy actor:  1  step number:  53 total reward:  0.3733333333333333  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.84482 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.84482 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.56644588224623
siam score:  -0.8075248
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.685]
 [0.614]
 [0.635]
 [0.503]
 [0.445]
 [0.607]] [[31.866]
 [32.342]
 [32.231]
 [32.378]
 [31.789]
 [31.908]
 [32.061]] [[0.506]
 [0.685]
 [0.614]
 [0.635]
 [0.503]
 [0.445]
 [0.607]]
maxi score, test score, baseline:  -0.84482 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.884]
 [0.952]
 [0.884]
 [0.884]
 [0.884]
 [0.884]
 [0.884]] [[43.735]
 [46.756]
 [43.735]
 [43.735]
 [43.735]
 [43.735]
 [43.735]] [[0.884]
 [0.952]
 [0.884]
 [0.884]
 [0.884]
 [0.884]
 [0.884]]
maxi score, test score, baseline:  -0.84482 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  50 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.052]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]] [[32.961]
 [32.961]
 [32.961]
 [32.961]
 [32.961]
 [32.961]
 [32.961]] [[0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.224]]
printing an ep nov before normalisation:  52.95192241668701
Printing some Q and Qe and total Qs values:  [[ 0.213]
 [-0.044]
 [ 0.213]
 [ 0.213]
 [ 0.213]
 [ 0.213]
 [ 0.213]] [[50.475]
 [52.008]
 [50.475]
 [50.475]
 [50.475]
 [50.475]
 [50.475]] [[2.139]
 [1.956]
 [2.139]
 [2.139]
 [2.139]
 [2.139]
 [2.139]]
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.825238028324925
printing an ep nov before normalisation:  42.46903972740987
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.54409353477199
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
actor:  1 policy actor:  1  step number:  68 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.58239254144173
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.51864721658082
printing an ep nov before normalisation:  46.566084794631436
actor:  1 policy actor:  1  step number:  42 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
printing an ep nov before normalisation:  53.60445448351077
printing an ep nov before normalisation:  63.94052182440372
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.48458174618443
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.81291497
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.20708754084134
printing an ep nov before normalisation:  46.50175732203463
actor:  1 policy actor:  1  step number:  65 total reward:  0.22666666666666602  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  61 total reward:  0.09333333333333216  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.80928147
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.07577299039599
using explorer policy with actor:  1
printing an ep nov before normalisation:  23.601984977722168
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.66605863496576
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.07162855300003
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.184]
 [0.455]
 [0.202]
 [0.194]
 [0.178]
 [0.174]
 [0.169]] [[23.034]
 [39.268]
 [24.304]
 [24.041]
 [24.47 ]
 [24.396]
 [23.005]] [[0.396]
 [0.957]
 [0.437]
 [0.424]
 [0.416]
 [0.41 ]
 [0.38 ]]
printing an ep nov before normalisation:  46.12090834944527
Printing some Q and Qe and total Qs values:  [[0.54 ]
 [0.588]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]] [[23.306]
 [35.926]
 [23.306]
 [23.306]
 [23.306]
 [23.306]
 [23.306]] [[0.937]
 [1.201]
 [0.937]
 [0.937]
 [0.937]
 [0.937]
 [0.937]]
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 18.97439711428063
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  66 total reward:  0.07333333333333225  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.086666666666666  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]] [[35.937]
 [35.937]
 [35.937]
 [35.937]
 [35.937]
 [35.937]
 [35.937]] [[0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]]
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.30121248178894
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8421133333333334 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  60 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  40.56951332798964
Printing some Q and Qe and total Qs values:  [[0.254]
 [0.315]
 [0.254]
 [0.254]
 [0.254]
 [0.254]
 [0.254]] [[32.693]
 [34.871]
 [32.693]
 [32.693]
 [32.693]
 [32.693]
 [32.693]] [[0.254]
 [0.315]
 [0.254]
 [0.254]
 [0.254]
 [0.254]
 [0.254]]
maxi score, test score, baseline:  -0.8392466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8392466666666666 0.6573333333333335 0.6573333333333335
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.551]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]] [[36.42 ]
 [37.678]
 [30.039]
 [30.039]
 [30.039]
 [30.039]
 [30.039]] [[0.493]
 [0.551]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]]
Printing some Q and Qe and total Qs values:  [[-0.07 ]
 [-0.023]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]] [[30.318]
 [34.804]
 [30.318]
 [30.318]
 [30.318]
 [30.318]
 [30.318]] [[0.824]
 [1.109]
 [0.824]
 [0.824]
 [0.824]
 [0.824]
 [0.824]]
actor:  0 policy actor:  0  step number:  42 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  42.60940326641912
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8364866666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.9298711086993
maxi score, test score, baseline:  -0.8364866666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8364866666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8364866666666666 0.6573333333333335 0.6573333333333335
maxi score, test score, baseline:  -0.8364866666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8364866666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8364866666666666 0.6573333333333335 0.6573333333333335
maxi score, test score, baseline:  -0.8364866666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8364866666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.01917111786440273
maxi score, test score, baseline:  -0.8364866666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.757729302274775
Printing some Q and Qe and total Qs values:  [[-0.033]
 [ 0.098]
 [-0.075]
 [-0.048]
 [-0.026]
 [-0.087]
 [-0.049]] [[25.782]
 [36.92 ]
 [26.971]
 [29.234]
 [28.722]
 [28.738]
 [28.475]] [[0.287]
 [0.755]
 [0.28 ]
 [0.376]
 [0.382]
 [0.323]
 [0.352]]
actor:  0 policy actor:  0  step number:  59 total reward:  0.11999999999999911  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.65209464339529
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.35643084563791
printing an ep nov before normalisation:  47.37735400251826
actor:  1 policy actor:  1  step number:  57 total reward:  0.13333333333333264  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.022]
 [ 0.301]
 [-0.023]
 [-0.023]
 [-0.023]
 [-0.023]
 [-0.023]] [[33.911]
 [35.398]
 [32.357]
 [32.357]
 [32.357]
 [32.357]
 [32.357]] [[0.413]
 [0.772]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]]
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([0.3406, 0.0981, 0.1119, 0.1259, 0.1206, 0.1009, 0.1018],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0136, 0.8936, 0.0213, 0.0144, 0.0090, 0.0112, 0.0370],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1924, 0.0292, 0.1890, 0.1521, 0.1247, 0.1768, 0.1358],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1344, 0.1395, 0.1223, 0.1905, 0.1484, 0.1360, 0.1288],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1734, 0.0973, 0.0992, 0.1120, 0.2656, 0.1575, 0.0950],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1166, 0.0357, 0.2151, 0.1379, 0.1006, 0.2277, 0.1664],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1367, 0.2144, 0.1320, 0.1310, 0.1129, 0.1085, 0.1645],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  25.130751594384346
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.52342756940612
printing an ep nov before normalisation:  41.204724343173865
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.427649499867826
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.25333333333333274  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.187]
 [-0.138]
 [-0.187]
 [-0.187]
 [-0.187]
 [-0.187]
 [-0.187]] [[25.461]
 [49.879]
 [25.461]
 [25.461]
 [25.461]
 [25.461]
 [25.461]] [[0.417]
 [1.246]
 [0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]]
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
actions average: 
K:  3  action  0 :  tensor([0.2548, 0.0163, 0.1248, 0.1932, 0.1524, 0.1459, 0.1126],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0115, 0.9122, 0.0105, 0.0119, 0.0066, 0.0064, 0.0408],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1323, 0.0571, 0.2984, 0.1483, 0.1234, 0.1265, 0.1141],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1067, 0.2292, 0.0807, 0.2536, 0.1131, 0.1165, 0.1002],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2932, 0.0033, 0.1316, 0.1451, 0.1409, 0.1588, 0.1271],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1284, 0.0969, 0.1350, 0.1744, 0.1145, 0.2118, 0.1389],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1165, 0.1420, 0.0771, 0.1293, 0.1304, 0.0940, 0.3106],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 22.906190204216383
printing an ep nov before normalisation:  38.3758046190338
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.941406664124166
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.36714491290651
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.047047891549056
actor:  1 policy actor:  1  step number:  58 total reward:  0.23333333333333306  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  42.484786232675205
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.80247253
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  0.3533333333333326  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  35.61261224185099
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.151]
 [-0.116]
 [-0.147]
 [-0.151]
 [-0.151]
 [-0.151]
 [-0.14 ]] [[47.269]
 [54.487]
 [54.611]
 [47.269]
 [47.269]
 [47.269]
 [62.106]] [[1.038]
 [1.302]
 [1.275]
 [1.038]
 [1.038]
 [1.038]
 [1.52 ]]
printing an ep nov before normalisation:  29.425656754778345
siam score:  -0.8016466
printing an ep nov before normalisation:  34.38088942370438
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.1733333333333329  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
printing an ep nov before normalisation:  50.97649056185373
printing an ep nov before normalisation:  50.03456880881351
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.661]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]] [[42.444]
 [42.433]
 [42.444]
 [42.444]
 [42.444]
 [42.444]
 [42.444]] [[1.116]
 [1.251]
 [1.116]
 [1.116]
 [1.116]
 [1.116]
 [1.116]]
Printing some Q and Qe and total Qs values:  [[0.152]
 [0.479]
 [0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.152]] [[41.1  ]
 [41.913]
 [41.1  ]
 [41.1  ]
 [41.1  ]
 [41.1  ]
 [41.1  ]] [[0.549]
 [0.892]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]]
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.694481006373667
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 46.23317002757118
printing an ep nov before normalisation:  57.70523272473726
printing an ep nov before normalisation:  40.617428725791115
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.822687248781314
siam score:  -0.8067575
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.104]
 [-0.096]
 [-0.106]
 [-0.106]
 [-0.107]
 [-0.106]
 [-0.106]] [[30.85 ]
 [37.496]
 [27.122]
 [27.122]
 [30.654]
 [27.122]
 [27.122]] [[0.785]
 [1.121]
 [0.599]
 [0.599]
 [0.773]
 [0.599]
 [0.599]]
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.085]
 [-0.026]
 [-0.085]
 [-0.085]
 [-0.082]
 [-0.085]
 [-0.085]] [[35.743]
 [41.562]
 [35.743]
 [35.743]
 [44.682]
 [35.743]
 [35.743]] [[0.497]
 [0.729]
 [0.497]
 [0.497]
 [0.766]
 [0.497]
 [0.497]]
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.82022080466192
Printing some Q and Qe and total Qs values:  [[0.058]
 [0.208]
 [0.058]
 [0.058]
 [0.058]
 [0.058]
 [0.058]] [[31.492]
 [36.203]
 [31.492]
 [31.492]
 [31.492]
 [31.492]
 [31.492]] [[1.054]
 [1.502]
 [1.054]
 [1.054]
 [1.054]
 [1.054]
 [1.054]]
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.72262251640503
actor:  1 policy actor:  1  step number:  58 total reward:  0.07333333333333292  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.58856852872834
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.602]
 [0.765]
 [0.546]
 [0.517]
 [0.343]
 [0.576]] [[24.531]
 [22.983]
 [20.688]
 [26.285]
 [26.704]
 [27.435]
 [24.5  ]] [[1.533]
 [1.549]
 [1.594]
 [1.662]
 [1.654]
 [1.518]
 [1.6  ]]
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.487]
 [0.268]
 [0.268]
 [0.268]
 [0.268]
 [0.268]] [[27.663]
 [37.113]
 [27.663]
 [27.663]
 [27.663]
 [27.663]
 [27.663]] [[0.688]
 [1.186]
 [0.688]
 [0.688]
 [0.688]
 [0.688]
 [0.688]]
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
actor:  1 policy actor:  1  step number:  61 total reward:  0.3333333333333326  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  16.074544099034416
actor:  1 policy actor:  1  step number:  57 total reward:  0.22666666666666613  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.145]
 [-0.145]
 [-0.145]
 [-0.145]
 [-0.145]
 [-0.145]
 [-0.145]] [[46.721]
 [46.721]
 [46.721]
 [46.721]
 [46.721]
 [46.721]
 [46.721]] [[1.037]
 [1.037]
 [1.037]
 [1.037]
 [1.037]
 [1.037]
 [1.037]]
line 256 mcts: sample exp_bonus 64.75712730341077
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8342466666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.66904878616333
actor:  0 policy actor:  0  step number:  49 total reward:  0.36  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8315266666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8315266666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8315266666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.083693981170654
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.433]
 [0.434]
 [0.434]] [[18.073]
 [18.073]
 [18.073]
 [18.073]
 [26.533]
 [18.073]
 [18.073]] [[1.025]
 [1.025]
 [1.025]
 [1.025]
 [1.887]
 [1.025]
 [1.025]]
maxi score, test score, baseline:  -0.8315266666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8315266666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.208]
 [-0.208]
 [-0.208]
 [-0.214]
 [-0.208]
 [-0.208]
 [-0.208]] [[37.625]
 [37.625]
 [37.625]
 [45.742]
 [37.625]
 [37.625]
 [37.625]] [[0.826]
 [0.826]
 [0.826]
 [1.364]
 [0.826]
 [0.826]
 [0.826]]
maxi score, test score, baseline:  -0.8315266666666666 0.6573333333333335 0.6573333333333335
siam score:  -0.8211824
maxi score, test score, baseline:  -0.8315266666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  14.20601143968111
maxi score, test score, baseline:  -0.8315266666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8315266666666666 0.6573333333333335 0.6573333333333335
maxi score, test score, baseline:  -0.8315266666666666 0.6573333333333335 0.6573333333333335
Printing some Q and Qe and total Qs values:  [[0.545]
 [0.612]
 [0.514]
 [0.54 ]
 [0.547]
 [0.522]
 [0.595]] [[33.699]
 [32.592]
 [36.115]
 [35.531]
 [33.245]
 [35.077]
 [33.575]] [[0.545]
 [0.612]
 [0.514]
 [0.54 ]
 [0.547]
 [0.522]
 [0.595]]
siam score:  -0.82154876
maxi score, test score, baseline:  -0.8315266666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8315266666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.212650775909424
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.8315266666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8315266666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.432]
 [0.457]
 [0.459]
 [0.432]
 [0.432]
 [0.432]] [[15.763]
 [15.763]
 [19.775]
 [27.212]
 [15.763]
 [15.763]
 [15.763]] [[1.233]
 [1.233]
 [1.463]
 [1.845]
 [1.233]
 [1.233]
 [1.233]]
printing an ep nov before normalisation:  23.655416875096797
maxi score, test score, baseline:  -0.8315266666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8315266666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8315266666666666 0.6573333333333335 0.6573333333333335
maxi score, test score, baseline:  -0.8315266666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.137]
 [-0.111]
 [-0.132]
 [-0.132]
 [-0.133]
 [-0.134]
 [-0.13 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.137]
 [-0.111]
 [-0.132]
 [-0.132]
 [-0.133]
 [-0.134]
 [-0.13 ]]
actor:  1 policy actor:  1  step number:  78 total reward:  0.16666666666666485  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  47.56882732201333
actions average: 
K:  3  action  0 :  tensor([0.4124, 0.1210, 0.0727, 0.0823, 0.1035, 0.0737, 0.1343],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0145, 0.9052, 0.0122, 0.0175, 0.0076, 0.0074, 0.0355],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1427, 0.0605, 0.2493, 0.1196, 0.1385, 0.1480, 0.1415],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1557, 0.0620, 0.1635, 0.2069, 0.1337, 0.1332, 0.1450],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1531, 0.0388, 0.1209, 0.0858, 0.3822, 0.1218, 0.0974],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1019, 0.1003, 0.1762, 0.0943, 0.0869, 0.3413, 0.0992],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1976, 0.1421, 0.0953, 0.1168, 0.2151, 0.1183, 0.1148],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.8315266666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  63.85840252310539
maxi score, test score, baseline:  -0.8315266666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8251628
Printing some Q and Qe and total Qs values:  [[-0.158]
 [-0.163]
 [-0.186]
 [-0.155]
 [-0.182]
 [-0.198]
 [-0.158]] [[48.817]
 [48.791]
 [53.659]
 [53.55 ]
 [68.918]
 [52.322]
 [51.337]] [[0.58 ]
 [0.575]
 [0.695]
 [0.723]
 [1.149]
 [0.644]
 [0.654]]
actor:  1 policy actor:  1  step number:  60 total reward:  0.13999999999999957  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8315266666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8315266666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.55213953293916
maxi score, test score, baseline:  -0.8315266666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8315266666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  68 total reward:  0.11333333333333262  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  29.45608139038086
maxi score, test score, baseline:  -0.8315266666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8315266666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8315266666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
actor:  1 policy actor:  1  step number:  49 total reward:  0.5466666666666669  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8315266666666666 0.6573333333333335 0.6573333333333335
maxi score, test score, baseline:  -0.8315266666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.688]
 [0.632]
 [0.265]
 [0.534]
 [0.615]
 [0.424]
 [0.569]] [[35.573]
 [34.712]
 [24.919]
 [24.483]
 [28.192]
 [26.075]
 [25.853]] [[0.688]
 [0.632]
 [0.265]
 [0.534]
 [0.615]
 [0.424]
 [0.569]]
Printing some Q and Qe and total Qs values:  [[0.158]
 [0.193]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]] [[41.85 ]
 [49.645]
 [41.85 ]
 [41.85 ]
 [41.85 ]
 [41.85 ]
 [41.85 ]] [[0.765]
 [0.97 ]
 [0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]]
maxi score, test score, baseline:  -0.8315266666666666 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.357705344347025
siam score:  -0.8110426
actor:  1 policy actor:  1  step number:  64 total reward:  0.11333333333333284  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  62 total reward:  0.03333333333333266  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  41.375813348901524
printing an ep nov before normalisation:  37.294975917729644
maxi score, test score, baseline:  -0.82946 0.6573333333333335 0.6573333333333335
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
actor:  1 policy actor:  1  step number:  53 total reward:  0.21333333333333304  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.168]
 [-0.168]
 [-0.168]
 [-0.168]
 [-0.168]
 [-0.168]
 [-0.168]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.168]
 [-0.168]
 [-0.168]
 [-0.168]
 [-0.168]
 [-0.168]
 [-0.168]]
line 256 mcts: sample exp_bonus 31.999917030334476
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 48.85254801448712
maxi score, test score, baseline:  -0.82946 0.6573333333333335 0.6573333333333335
maxi score, test score, baseline:  -0.82946 0.6573333333333335 0.6573333333333335
maxi score, test score, baseline:  -0.82946 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.82946 0.6573333333333335 0.6573333333333335
printing an ep nov before normalisation:  36.71130657196045
printing an ep nov before normalisation:  43.55200631181236
printing an ep nov before normalisation:  42.78486136941972
printing an ep nov before normalisation:  50.868605033110725
maxi score, test score, baseline:  -0.82946 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.12666666666666637  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.82946 0.6573333333333335 0.6573333333333335
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  77 total reward:  0.06666666666666632  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  39.977604486916036
printing an ep nov before normalisation:  15.68418025970459
printing an ep nov before normalisation:  50.78649512860379
actions average: 
K:  0  action  0 :  tensor([0.3449, 0.0063, 0.1292, 0.1171, 0.1720, 0.1159, 0.1146],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0069, 0.9489, 0.0046, 0.0071, 0.0018, 0.0014, 0.0292],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0988, 0.0013, 0.2864, 0.2153, 0.1061, 0.2188, 0.0733],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1890, 0.0565, 0.1332, 0.1866, 0.1748, 0.1380, 0.1218],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1734, 0.0520, 0.1443, 0.1134, 0.2958, 0.1016, 0.1196],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1289, 0.0118, 0.1701, 0.1399, 0.1296, 0.3151, 0.1046],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1332, 0.0896, 0.1414, 0.1482, 0.1205, 0.1188, 0.2484],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.493667743570487
printing an ep nov before normalisation:  22.89498325613888
maxi score, test score, baseline:  -0.82946 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.82946 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  81 total reward:  0.15999999999999825  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  30.428007670811247
Printing some Q and Qe and total Qs values:  [[-0.124]
 [-0.076]
 [-0.135]
 [-0.141]
 [-0.14 ]
 [-0.14 ]
 [-0.13 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.124]
 [-0.076]
 [-0.135]
 [-0.141]
 [-0.14 ]
 [-0.14 ]
 [-0.13 ]]
maxi score, test score, baseline:  -0.82946 0.6573333333333335 0.6573333333333335
maxi score, test score, baseline:  -0.82946 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.78297920344954
maxi score, test score, baseline:  -0.82946 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.82946 0.6573333333333335 0.6573333333333335
maxi score, test score, baseline:  -0.82946 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.82946 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.82946 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.51893618318675
printing an ep nov before normalisation:  43.16393988473075
maxi score, test score, baseline:  -0.82946 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.82946 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.82946 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.16567588894742
maxi score, test score, baseline:  -0.82946 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.82946 0.6573333333333335 0.6573333333333335
printing an ep nov before normalisation:  45.303283365091616
maxi score, test score, baseline:  -0.82946 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.731572151184082
printing an ep nov before normalisation:  53.99101531268083
printing an ep nov before normalisation:  42.80232526392315
Printing some Q and Qe and total Qs values:  [[0.594]
 [0.616]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.57 ]] [[27.663]
 [28.513]
 [27.663]
 [27.663]
 [27.663]
 [27.663]
 [27.115]] [[2.351]
 [2.428]
 [2.351]
 [2.351]
 [2.351]
 [2.351]
 [2.292]]
printing an ep nov before normalisation:  13.717694800413259
printing an ep nov before normalisation:  58.32609053706783
Printing some Q and Qe and total Qs values:  [[0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]] [[28.039]
 [28.039]
 [28.039]
 [28.039]
 [28.039]
 [28.039]
 [28.039]] [[2.383]
 [2.383]
 [2.383]
 [2.383]
 [2.383]
 [2.383]
 [2.383]]
maxi score, test score, baseline:  -0.82946 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8099071
maxi score, test score, baseline:  -0.82946 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.82946 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.82946 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  89 total reward:  0.09333333333333127  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  63.660421052894684
printing an ep nov before normalisation:  63.7396528445613
maxi score, test score, baseline:  -0.82946 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.60867962555337
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.536]
 [0.473]
 [0.468]
 [0.458]
 [0.457]
 [0.459]] [[42.562]
 [44.142]
 [42.168]
 [42.753]
 [43.807]
 [43.012]
 [42.866]] [[0.451]
 [0.536]
 [0.473]
 [0.468]
 [0.458]
 [0.457]
 [0.459]]
maxi score, test score, baseline:  -0.82946 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8139942
maxi score, test score, baseline:  -0.82946 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.82946 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.03487709664327099
actor:  0 policy actor:  0  step number:  55 total reward:  0.3066666666666661  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.14419515555073
printing an ep nov before normalisation:  53.31915831130301
line 256 mcts: sample exp_bonus 57.63073430901354
printing an ep nov before normalisation:  53.29104337476046
Printing some Q and Qe and total Qs values:  [[0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.537]] [[21.199]
 [21.199]
 [21.199]
 [21.199]
 [21.199]
 [21.199]
 [21.199]] [[0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.537]]
maxi score, test score, baseline:  -0.8268466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.15333333333333277  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  51 total reward:  0.1999999999999994  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.96586952909906
printing an ep nov before normalisation:  28.37915255484886
Printing some Q and Qe and total Qs values:  [[-0.064]
 [-0.104]
 [-0.071]
 [-0.071]
 [-0.071]
 [-0.064]
 [-0.071]] [[37.729]
 [47.869]
 [30.045]
 [30.045]
 [30.045]
 [37.124]
 [30.045]] [[0.789]
 [1.147]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.765]
 [0.48 ]]
printing an ep nov before normalisation:  38.61995336092253
printing an ep nov before normalisation:  33.80512718087354
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 28.77368460598512
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.05999999999999939  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.313]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.288]] [[41.245]
 [33.953]
 [41.245]
 [41.245]
 [41.245]
 [41.245]
 [23.536]] [[0.317]
 [0.313]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.288]]
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.14028626545945
actions average: 
K:  1  action  0 :  tensor([0.4082, 0.0100, 0.1229, 0.1126, 0.1302, 0.0982, 0.1179],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0105, 0.9337, 0.0123, 0.0100, 0.0033, 0.0043, 0.0259],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0769, 0.1238, 0.2962, 0.1361, 0.0640, 0.1410, 0.1620],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1361, 0.0662, 0.1243, 0.2806, 0.1469, 0.1201, 0.1258],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1559, 0.0332, 0.1233, 0.1149, 0.3161, 0.1093, 0.1474],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1359, 0.0144, 0.1747, 0.1559, 0.1589, 0.2296, 0.1305],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0970, 0.1101, 0.1030, 0.1201, 0.1127, 0.1012, 0.3558],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.304]
 [0.375]
 [0.284]
 [0.297]
 [0.259]
 [0.272]
 [0.335]] [[50.096]
 [49.734]
 [49.155]
 [49.623]
 [56.16 ]
 [54.503]
 [46.534]] [[0.558]
 [0.625]
 [0.53 ]
 [0.547]
 [0.569]
 [0.566]
 [0.556]]
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.096044158997785
printing an ep nov before normalisation:  53.19031159365791
printing an ep nov before normalisation:  52.69147494423137
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
actor:  1 policy actor:  1  step number:  48 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
printing an ep nov before normalisation:  41.713292457924496
Printing some Q and Qe and total Qs values:  [[-0.089]
 [-0.082]
 [-0.097]
 [-0.093]
 [-0.097]
 [-0.094]
 [-0.099]] [[38.036]
 [38.249]
 [39.7  ]
 [38.789]
 [39.374]
 [39.051]
 [39.106]] [[0.952]
 [0.972]
 [1.041]
 [0.992]
 [1.021]
 [1.007]
 [1.005]]
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.085]
 [-0.063]
 [-0.085]
 [-0.085]
 [-0.082]
 [-0.085]
 [-0.07 ]] [[31.718]
 [43.111]
 [31.718]
 [31.718]
 [36.129]
 [31.718]
 [42.612]] [[0.621]
 [1.055]
 [0.621]
 [0.621]
 [0.783]
 [0.621]
 [1.03 ]]
siam score:  -0.8150423
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.212974071502686
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.03721794988181
printing an ep nov before normalisation:  18.24086237860169
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
actor:  1 policy actor:  1  step number:  82 total reward:  0.09999999999999909  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.30500304915292
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.208775997161865
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.81528014
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.2754, 0.0406, 0.1042, 0.1361, 0.1698, 0.1275, 0.1464],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0482, 0.7904, 0.0322, 0.0422, 0.0222, 0.0151, 0.0497],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1752, 0.1412, 0.1959, 0.1175, 0.1002, 0.1109, 0.1590],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1531, 0.1159, 0.0997, 0.1783, 0.1240, 0.1434, 0.1857],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1944, 0.0469, 0.1069, 0.1565, 0.2436, 0.1309, 0.1208],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1484, 0.0316, 0.1058, 0.1856, 0.1235, 0.2527, 0.1524],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2004, 0.0304, 0.1260, 0.1584, 0.1777, 0.1599, 0.1472],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  68 total reward:  0.17999999999999972  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.235763197977885
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.21999999999999975  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  49.512237702802715
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.139]
 [-0.112]
 [-0.139]
 [-0.139]
 [-0.139]
 [-0.139]
 [-0.139]] [[32.046]
 [35.925]
 [32.046]
 [32.046]
 [32.046]
 [32.046]
 [32.046]] [[0.812]
 [1.064]
 [0.812]
 [0.812]
 [0.812]
 [0.812]
 [0.812]]
printing an ep nov before normalisation:  39.344617094901785
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.949603953084214
siam score:  -0.8154127
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.69555069850849
printing an ep nov before normalisation:  41.6779899597168
siam score:  -0.8165367
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.30128198033341
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
Printing some Q and Qe and total Qs values:  [[-0.118]
 [-0.096]
 [-0.118]
 [-0.118]
 [-0.127]
 [-0.118]
 [-0.118]] [[28.781]
 [43.578]
 [28.781]
 [28.781]
 [26.781]
 [28.781]
 [28.781]] [[0.034]
 [0.172]
 [0.034]
 [0.034]
 [0.009]
 [0.034]
 [0.034]]
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.658834343631334
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.0709824977215
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.041]
 [-0.044]
 [-0.054]
 [-0.049]
 [-0.046]
 [-0.048]
 [-0.044]] [[25.19 ]
 [25.931]
 [24.963]
 [19.542]
 [26.855]
 [19.426]
 [19.534]] [[0.128]
 [0.133]
 [0.113]
 [0.06 ]
 [0.141]
 [0.06 ]
 [0.065]]
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.119]
 [0.315]
 [0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]] [[36.868]
 [51.827]
 [36.868]
 [36.868]
 [36.868]
 [36.868]
 [36.868]] [[0.259]
 [0.557]
 [0.259]
 [0.259]
 [0.259]
 [0.259]
 [0.259]]
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.299]
 [0.467]
 [0.304]
 [0.301]
 [0.349]
 [0.299]
 [0.3  ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.299]
 [0.467]
 [0.304]
 [0.301]
 [0.349]
 [0.299]
 [0.3  ]]
Printing some Q and Qe and total Qs values:  [[0.229]
 [0.285]
 [0.239]
 [0.225]
 [0.221]
 [0.332]
 [0.332]] [[38.863]
 [38.393]
 [39.999]
 [40.057]
 [40.117]
 [43.377]
 [43.377]] [[1.483]
 [1.51 ]
 [1.565]
 [1.554]
 [1.554]
 [1.87 ]
 [1.87 ]]
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[ 0.184]
 [ 0.367]
 [-0.008]
 [ 0.342]
 [ 0.182]
 [ 0.342]
 [ 0.204]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.184]
 [ 0.367]
 [-0.008]
 [ 0.342]
 [ 0.182]
 [ 0.342]
 [ 0.204]]
Printing some Q and Qe and total Qs values:  [[0.204]
 [0.265]
 [0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.204]
 [0.265]
 [0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]]
maxi score, test score, baseline:  -0.8244466666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.20842827302157
Printing some Q and Qe and total Qs values:  [[0.741]
 [0.791]
 [0.801]
 [0.801]
 [0.801]
 [0.801]
 [0.801]] [[48.097]
 [29.498]
 [30.827]
 [30.827]
 [30.827]
 [30.827]
 [30.827]] [[0.741]
 [0.791]
 [0.801]
 [0.801]
 [0.801]
 [0.801]
 [0.801]]
Printing some Q and Qe and total Qs values:  [[0.849]
 [0.896]
 [0.825]
 [0.825]
 [0.825]
 [0.825]
 [0.825]] [[41.125]
 [32.993]
 [29.666]
 [29.666]
 [29.666]
 [29.666]
 [29.666]] [[0.849]
 [0.896]
 [0.825]
 [0.825]
 [0.825]
 [0.825]
 [0.825]]
printing an ep nov before normalisation:  48.532729740465236
Printing some Q and Qe and total Qs values:  [[-0.029]
 [-0.029]
 [-0.029]
 [-0.029]
 [-0.029]
 [-0.029]
 [-0.029]] [[30.783]
 [30.783]
 [30.783]
 [30.783]
 [30.783]
 [30.783]
 [30.783]] [[0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]]
printing an ep nov before normalisation:  45.138814797745326
printing an ep nov before normalisation:  35.56359713305696
siam score:  -0.82679665
printing an ep nov before normalisation:  48.760090577702236
printing an ep nov before normalisation:  50.233525722067974
printing an ep nov before normalisation:  46.39450421599561
siam score:  -0.82767504
printing an ep nov before normalisation:  47.84843852135662
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]] [[37.138]
 [37.138]
 [37.138]
 [37.138]
 [37.138]
 [37.138]
 [37.138]] [[0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]]
Printing some Q and Qe and total Qs values:  [[0.91]
 [0.91]
 [0.91]
 [0.91]
 [0.91]
 [0.91]
 [0.91]] [[28.979]
 [28.979]
 [28.979]
 [28.979]
 [28.979]
 [28.979]
 [28.979]] [[0.91]
 [0.91]
 [0.91]
 [0.91]
 [0.91]
 [0.91]
 [0.91]]
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  27.95865976127338
actions average: 
K:  2  action  0 :  tensor([0.3288, 0.0993, 0.1041, 0.1137, 0.1127, 0.1105, 0.1310],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0227, 0.9142, 0.0114, 0.0119, 0.0092, 0.0093, 0.0213],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1805, 0.0693, 0.2368, 0.1144, 0.1189, 0.1365, 0.1436],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1254, 0.0826, 0.0900, 0.3688, 0.1187, 0.0921, 0.1225],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1735, 0.0316, 0.1228, 0.1223, 0.2863, 0.1306, 0.1329],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1169, 0.0872, 0.1237, 0.0982, 0.1179, 0.3506, 0.1055],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1770, 0.0130, 0.1411, 0.1301, 0.2035, 0.1391, 0.1962],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.7906066666666667 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  26 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  29 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.76402 0.6573333333333335 0.6573333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  33 total reward:  0.52  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.76098 0.5866666666666668 0.5866666666666668
actor:  0 policy actor:  0  step number:  70 total reward:  0.086666666666666  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.7588066666666666 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.82542634
actor:  1 policy actor:  1  step number:  45 total reward:  0.38666666666666616  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  40.70176380016615
Printing some Q and Qe and total Qs values:  [[-0.157]
 [-0.122]
 [-0.151]
 [-0.151]
 [-0.154]
 [-0.154]
 [-0.153]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.157]
 [-0.122]
 [-0.151]
 [-0.151]
 [-0.154]
 [-0.154]
 [-0.153]]
siam score:  -0.8243958
printing an ep nov before normalisation:  31.37832117995397
siam score:  -0.82418734
maxi score, test score, baseline:  -0.7588066666666666 0.5866666666666668 0.5866666666666668
maxi score, test score, baseline:  -0.7588066666666666 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]] [[43.81]
 [43.81]
 [43.81]
 [43.81]
 [43.81]
 [43.81]
 [43.81]] [[0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]]
Printing some Q and Qe and total Qs values:  [[0.175]
 [0.211]
 [0.139]
 [0.225]
 [0.175]
 [0.241]
 [0.175]] [[62.877]
 [47.605]
 [65.609]
 [65.509]
 [62.877]
 [59.693]
 [62.877]] [[1.415]
 [1.058]
 [1.449]
 [1.533]
 [1.415]
 [1.399]
 [1.415]]
maxi score, test score, baseline:  -0.7588066666666666 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.13424908413353
Printing some Q and Qe and total Qs values:  [[0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]] [[36.083]
 [36.083]
 [36.083]
 [36.083]
 [36.083]
 [36.083]
 [36.083]] [[0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]]
printing an ep nov before normalisation:  47.523269716929654
printing an ep nov before normalisation:  48.56408667084938
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7588066666666666 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7588066666666666 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7588066666666666 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7588066666666666 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  5.287338839640418
Printing some Q and Qe and total Qs values:  [[0.551]
 [0.72 ]
 [0.554]
 [0.54 ]
 [0.541]
 [0.544]
 [0.569]] [[41.668]
 [45.673]
 [39.88 ]
 [43.811]
 [44.172]
 [45.067]
 [46.619]] [[0.551]
 [0.72 ]
 [0.554]
 [0.54 ]
 [0.541]
 [0.544]
 [0.569]]
actions average: 
K:  1  action  0 :  tensor([0.3819, 0.1140, 0.0829, 0.0966, 0.1234, 0.1060, 0.0952],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0101, 0.9328, 0.0098, 0.0073, 0.0046, 0.0065, 0.0289],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1619, 0.0121, 0.2000, 0.1676, 0.1568, 0.1498, 0.1519],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1220, 0.0357, 0.0807, 0.3525, 0.1296, 0.1619, 0.1177],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1560, 0.0422, 0.1170, 0.1312, 0.2699, 0.1372, 0.1466],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1873, 0.0320, 0.1728, 0.1188, 0.1152, 0.2585, 0.1153],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1460, 0.1938, 0.0937, 0.1429, 0.1162, 0.1161, 0.1914],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.7588066666666666 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]] [[53.116]
 [53.116]
 [53.116]
 [53.116]
 [53.116]
 [53.116]
 [53.116]] [[0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]]
siam score:  -0.82581955
printing an ep nov before normalisation:  51.615321642755866
maxi score, test score, baseline:  -0.7588066666666666 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]] [[46.697]
 [46.697]
 [46.697]
 [46.697]
 [46.697]
 [46.697]
 [46.697]] [[0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]]
printing an ep nov before normalisation:  51.122633616129555
UNIT TEST: sample policy line 217 mcts : [0.102 0.245 0.122 0.061 0.163 0.143 0.163]
maxi score, test score, baseline:  -0.7588066666666666 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7588066666666666 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.486]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]] [[32.017]
 [34.242]
 [32.017]
 [32.017]
 [32.017]
 [32.017]
 [32.017]] [[0.371]
 [0.486]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  70.3741095461835
maxi score, test score, baseline:  -0.7588066666666666 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7588066666666666 0.5866666666666668 0.5866666666666668
printing an ep nov before normalisation:  49.578889807051056
printing an ep nov before normalisation:  42.834252655676686
maxi score, test score, baseline:  -0.7588066666666666 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  72 total reward:  0.0999999999999992  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.171]
 [-0.17 ]
 [-0.171]
 [-0.171]
 [-0.171]
 [-0.171]
 [-0.171]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.171]
 [-0.17 ]
 [-0.171]
 [-0.171]
 [-0.171]
 [-0.171]
 [-0.171]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  60.03917884954788
maxi score, test score, baseline:  -0.7566066666666668 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7566066666666668 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7566066666666668 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.874256134033203
maxi score, test score, baseline:  -0.7566066666666668 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.98628330230713
maxi score, test score, baseline:  -0.7566066666666668 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  41 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  1.0
siam score:  -0.82469916
printing an ep nov before normalisation:  49.183207514244046
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  0.37999999999999934  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.84407782879497
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.022]
 [-0.004]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]] [[44.619]
 [50.768]
 [44.619]
 [44.619]
 [44.619]
 [44.619]
 [44.619]] [[1.218]
 [1.54 ]
 [1.218]
 [1.218]
 [1.218]
 [1.218]
 [1.218]]
printing an ep nov before normalisation:  49.09275079762265
printing an ep nov before normalisation:  43.39755477572179
printing an ep nov before normalisation:  56.870404792268666
line 256 mcts: sample exp_bonus 41.54439736953774
actions average: 
K:  3  action  0 :  tensor([0.2106, 0.0285, 0.1342, 0.1640, 0.1686, 0.1527, 0.1414],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0265, 0.8475, 0.0221, 0.0277, 0.0226, 0.0166, 0.0369],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0723, 0.0215, 0.3656, 0.0541, 0.0537, 0.3009, 0.1319],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1720, 0.0050, 0.1254, 0.1925, 0.1537, 0.1500, 0.2015],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1980, 0.0125, 0.1088, 0.1292, 0.3010, 0.1157, 0.1349],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1629, 0.0910, 0.1861, 0.1128, 0.1207, 0.1853, 0.1412],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1058, 0.1258, 0.0934, 0.1461, 0.1061, 0.1032, 0.3196],
       grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.623338039284903
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.92136774325085
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.1666666666666664  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.40450140957693
printing an ep nov before normalisation:  16.293873835364728
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 48.76949930231333
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.3181227055144
siam score:  -0.83349335
printing an ep nov before normalisation:  47.86987682284958
printing an ep nov before normalisation:  39.014753381451484
printing an ep nov before normalisation:  41.56867147038681
printing an ep nov before normalisation:  60.45894316213706
Printing some Q and Qe and total Qs values:  [[-0.138]
 [-0.121]
 [-0.138]
 [-0.138]
 [-0.134]
 [-0.138]
 [-0.138]] [[30.048]
 [44.039]
 [30.048]
 [30.048]
 [31.959]
 [30.048]
 [30.048]] [[0.016]
 [0.143]
 [0.016]
 [0.016]
 [0.035]
 [0.016]
 [0.016]]
printing an ep nov before normalisation:  31.53876210196009
actor:  1 policy actor:  1  step number:  34 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  4  action  0 :  tensor([0.3687, 0.0443, 0.1124, 0.1026, 0.1391, 0.1030, 0.1300],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0207, 0.9179, 0.0077, 0.0099, 0.0082, 0.0052, 0.0304],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1403, 0.1639, 0.1910, 0.1131, 0.1196, 0.0978, 0.1745],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1479, 0.2694, 0.0853, 0.2095, 0.0919, 0.0836, 0.1123],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1712, 0.0973, 0.0892, 0.1308, 0.2734, 0.1231, 0.1150],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1791, 0.0868, 0.1111, 0.1419, 0.1137, 0.2308, 0.1366],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1115, 0.1843, 0.0952, 0.1260, 0.1013, 0.0890, 0.2928],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.460544809207036
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.562687990995016
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.5698137604496
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8284854
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.00201911418046
printing an ep nov before normalisation:  60.89151841836471
printing an ep nov before normalisation:  52.58996791744739
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  49 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  14.607245922088623
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.592783572506647
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.000185129769647574
using explorer policy with actor:  1
printing an ep nov before normalisation:  54.368621999127114
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.506]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]] [[39.676]
 [52.01 ]
 [39.676]
 [39.676]
 [39.676]
 [39.676]
 [39.676]] [[1.497]
 [2.093]
 [1.497]
 [1.497]
 [1.497]
 [1.497]
 [1.497]]
printing an ep nov before normalisation:  28.729872703552246
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.26666666666666616  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.117]
 [-0.094]
 [-0.11 ]
 [-0.111]
 [-0.113]
 [-0.12 ]
 [-0.117]] [[36.609]
 [54.035]
 [38.639]
 [38.272]
 [36.611]
 [37.671]
 [38.889]] [[0.454]
 [1.04 ]
 [0.526]
 [0.513]
 [0.458]
 [0.485]
 [0.527]]
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.641694491752325
actor:  1 policy actor:  1  step number:  53 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8359715
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.155]
 [-0.139]
 [-0.151]
 [-0.151]
 [-0.151]
 [-0.151]
 [-0.15 ]] [[33.879]
 [35.287]
 [30.67 ]
 [30.802]
 [31.072]
 [31.44 ]
 [31.774]] [[0.081]
 [0.116]
 [0.045]
 [0.047]
 [0.05 ]
 [0.055]
 [0.06 ]]
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.74407965394228
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.217]
 [-0.217]
 [-0.217]
 [-0.217]
 [-0.217]
 [-0.217]
 [-0.217]] [[47.799]
 [47.799]
 [47.799]
 [47.799]
 [47.799]
 [47.799]
 [47.799]] [[0.882]
 [0.882]
 [0.882]
 [0.882]
 [0.882]
 [0.882]
 [0.882]]
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.412]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]] [[51.257]
 [64.122]
 [51.257]
 [51.257]
 [51.257]
 [51.257]
 [51.257]] [[1.312]
 [1.778]
 [1.312]
 [1.312]
 [1.312]
 [1.312]
 [1.312]]
printing an ep nov before normalisation:  53.44026955403321
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.033]
 [-0.084]
 [-0.084]
 [-0.084]
 [-0.084]
 [-0.084]
 [-0.084]] [[30.662]
 [13.964]
 [13.964]
 [13.964]
 [13.964]
 [13.964]
 [13.964]] [[0.896]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]]
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  68 total reward:  0.39333333333333287  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.103893795251576
actor:  1 policy actor:  1  step number:  44 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.044190269262117
actor:  1 policy actor:  1  step number:  43 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  1.4006050108412182
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
siam score:  -0.8272824
printing an ep nov before normalisation:  51.401878219318675
maxi score, test score, baseline:  -0.7538333333333332 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.82740235
printing an ep nov before normalisation:  33.923713828482796
using explorer policy with actor:  1
printing an ep nov before normalisation:  19.429414724191208
siam score:  -0.8297607
printing an ep nov before normalisation:  3.6127654966549017
actor:  0 policy actor:  0  step number:  73 total reward:  0.15999999999999914  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  59.32934442742253
maxi score, test score, baseline:  -0.7515133333333333 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.14689610500697
maxi score, test score, baseline:  -0.7515133333333333 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7515133333333333 0.5866666666666668 0.5866666666666668
Printing some Q and Qe and total Qs values:  [[-0.154]
 [-0.161]
 [-0.154]
 [-0.154]
 [-0.154]
 [-0.154]
 [-0.154]] [[ 0.   ]
 [44.569]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-1.618]
 [ 1.565]
 [-1.618]
 [-1.618]
 [-1.618]
 [-1.618]
 [-1.618]]
siam score:  -0.8294759
printing an ep nov before normalisation:  53.036261651350685
Printing some Q and Qe and total Qs values:  [[0.311]
 [0.325]
 [0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]] [[40.563]
 [56.214]
 [40.563]
 [40.563]
 [40.563]
 [40.563]
 [40.563]] [[1.207]
 [1.855]
 [1.207]
 [1.207]
 [1.207]
 [1.207]
 [1.207]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.074]
 [-0.048]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.074]] [[43.67 ]
 [47.624]
 [43.67 ]
 [43.67 ]
 [43.67 ]
 [43.67 ]
 [43.67 ]] [[1.079]
 [1.285]
 [1.079]
 [1.079]
 [1.079]
 [1.079]
 [1.079]]
maxi score, test score, baseline:  -0.7515133333333333 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4333333333333329  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.258]
 [0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[32.951]
 [42.618]
 [32.951]
 [32.951]
 [32.951]
 [32.951]
 [32.951]] [[0.613]
 [1.203]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]]
printing an ep nov before normalisation:  47.478934559535524
printing an ep nov before normalisation:  48.8094668667448
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7515133333333333 0.5866666666666668 0.5866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5933333333333334  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.7515133333333333 0.5866666666666668 0.5866666666666668
probs:  [0.12509976057461936, 0.12509976057461936, 0.12509976057461936, 0.37450119712690333, 0.12509976057461936, 0.12509976057461936]
line 256 mcts: sample exp_bonus 52.03060023937366
Printing some Q and Qe and total Qs values:  [[0.31 ]
 [0.352]
 [0.201]
 [0.274]
 [0.281]
 [0.283]
 [0.37 ]] [[40.22 ]
 [47.633]
 [42.008]
 [43.333]
 [44.607]
 [44.933]
 [39.755]] [[1.247]
 [1.624]
 [1.219]
 [1.351]
 [1.416]
 [1.432]
 [1.286]]
siam score:  -0.8231476
actor:  1 policy actor:  1  step number:  41 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8261838
maxi score, test score, baseline:  -0.7515133333333333 0.5866666666666668 0.5866666666666668
probs:  [0.12509976057461936, 0.12509976057461936, 0.12509976057461936, 0.37450119712690333, 0.12509976057461936, 0.12509976057461936]
maxi score, test score, baseline:  -0.7515133333333333 0.5866666666666668 0.5866666666666668
probs:  [0.12509976057461936, 0.12509976057461936, 0.12509976057461936, 0.37450119712690333, 0.12509976057461936, 0.12509976057461936]
printing an ep nov before normalisation:  42.17455122205946
printing an ep nov before normalisation:  42.860353329558464
printing an ep nov before normalisation:  41.47040691543225
Printing some Q and Qe and total Qs values:  [[0.279]
 [0.274]
 [0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]] [[37.86]
 [41.62]
 [37.86]
 [37.86]
 [37.86]
 [37.86]
 [37.86]] [[1.986]
 [2.274]
 [1.986]
 [1.986]
 [1.986]
 [1.986]
 [1.986]]
maxi score, test score, baseline:  -0.7515133333333333 0.5866666666666668 0.5866666666666668
actor:  1 policy actor:  1  step number:  58 total reward:  0.2199999999999993  reward:  1.0 rdn_beta:  2.0
from probs:  [0.12509976057461936, 0.12509976057461936, 0.12509976057461936, 0.37450119712690333, 0.12509976057461936, 0.12509976057461936]
Printing some Q and Qe and total Qs values:  [[0.28 ]
 [0.395]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]] [[38.769]
 [52.717]
 [38.769]
 [38.769]
 [38.769]
 [38.769]
 [38.769]] [[0.799]
 [1.322]
 [0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]]
maxi score, test score, baseline:  -0.7515133333333333 0.5866666666666668 0.5866666666666668
probs:  [0.12509976057461936, 0.12509976057461936, 0.12509976057461936, 0.37450119712690333, 0.12509976057461936, 0.12509976057461936]
printing an ep nov before normalisation:  38.14271422785105
actions average: 
K:  3  action  0 :  tensor([0.3820, 0.0536, 0.0857, 0.1173, 0.1372, 0.1049, 0.1193],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0140, 0.9099, 0.0108, 0.0152, 0.0051, 0.0050, 0.0400],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1964, 0.0259, 0.2536, 0.1185, 0.1145, 0.1263, 0.1648],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1770, 0.0434, 0.1262, 0.1324, 0.1808, 0.1383, 0.2019],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1426, 0.0107, 0.1079, 0.1501, 0.2918, 0.1480, 0.1490],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0720, 0.2560, 0.0660, 0.0976, 0.0825, 0.3182, 0.1077],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1366, 0.1383, 0.1348, 0.1377, 0.1014, 0.1200, 0.2313],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.7515133333333333 0.5866666666666668 0.5866666666666668
probs:  [0.12509976057461936, 0.12509976057461936, 0.12509976057461936, 0.37450119712690333, 0.12509976057461936, 0.12509976057461936]
actor:  1 policy actor:  1  step number:  38 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.7515133333333333 0.5866666666666668 0.5866666666666668
probs:  [0.12521142174907846, 0.12521142174907846, 0.12521142174907846, 0.37394289125460756, 0.12521142174907846, 0.12521142174907846]
maxi score, test score, baseline:  -0.7515133333333333 0.5866666666666668 0.5866666666666668
probs:  [0.12521142174907846, 0.12521142174907846, 0.12521142174907846, 0.37394289125460756, 0.12521142174907846, 0.12521142174907846]
maxi score, test score, baseline:  -0.7515133333333333 0.5866666666666668 0.5866666666666668
probs:  [0.12521142174907846, 0.12521142174907846, 0.12521142174907846, 0.37394289125460756, 0.12521142174907846, 0.12521142174907846]
maxi score, test score, baseline:  -0.7515133333333333 0.5866666666666668 0.5866666666666668
probs:  [0.12521142174907846, 0.12521142174907846, 0.12521142174907846, 0.37394289125460756, 0.12521142174907846, 0.12521142174907846]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7515133333333333 0.5866666666666668 0.5866666666666668
from probs:  [0.12521142174907846, 0.12521142174907846, 0.12521142174907846, 0.37394289125460756, 0.12521142174907846, 0.12521142174907846]
actor:  1 policy actor:  1  step number:  42 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7515133333333334 0.5866666666666668 0.5866666666666668
probs:  [0.12521142174907846, 0.12521142174907846, 0.12521142174907846, 0.37394289125460756, 0.12521142174907846, 0.12521142174907846]
printing an ep nov before normalisation:  38.61715055425583
printing an ep nov before normalisation:  35.8215007087304
printing an ep nov before normalisation:  36.36866362630253
printing an ep nov before normalisation:  22.67476984546947
maxi score, test score, baseline:  -0.7515133333333334 0.5866666666666668 0.5866666666666668
probs:  [0.12532248461996273, 0.12532248461996273, 0.12532248461996273, 0.37338757690018604, 0.12532248461996273, 0.12532248461996273]
actor:  0 policy actor:  0  step number:  45 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  51.41537530081613
printing an ep nov before normalisation:  44.1426342244467
maxi score, test score, baseline:  -0.7487400000000001 0.5866666666666668 0.5866666666666668
printing an ep nov before normalisation:  23.855018615722656
printing an ep nov before normalisation:  28.73466295521823
printing an ep nov before normalisation:  37.90258667959616
maxi score, test score, baseline:  -0.7487400000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12532248461996273, 0.12532248461996273, 0.12532248461996273, 0.37338757690018604, 0.12532248461996273, 0.12532248461996273]
maxi score, test score, baseline:  -0.7487400000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12532248461996273, 0.12532248461996273, 0.12532248461996273, 0.37338757690018604, 0.12532248461996273, 0.12532248461996273]
maxi score, test score, baseline:  -0.7487400000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12532248461996273, 0.12532248461996273, 0.12532248461996273, 0.37338757690018604, 0.12532248461996273, 0.12532248461996273]
printing an ep nov before normalisation:  40.37444539461344
maxi score, test score, baseline:  -0.7487400000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12532248461996273, 0.12532248461996273, 0.12532248461996273, 0.37338757690018604, 0.12532248461996273, 0.12532248461996273]
maxi score, test score, baseline:  -0.7487400000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12532248461996273, 0.12532248461996273, 0.12532248461996273, 0.37338757690018604, 0.12532248461996273, 0.12532248461996273]
printing an ep nov before normalisation:  28.210410070259254
printing an ep nov before normalisation:  1.6403480174176366e-05
actor:  1 policy actor:  1  step number:  57 total reward:  0.26666666666666583  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
siam score:  -0.8269995
maxi score, test score, baseline:  -0.7487400000000001 0.5866666666666668 0.5866666666666668
actor:  0 policy actor:  0  step number:  53 total reward:  0.2799999999999998  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  30.532541275024414
maxi score, test score, baseline:  -0.74618 0.5866666666666668 0.5866666666666668
probs:  [0.12532248461996273, 0.12532248461996273, 0.12532248461996273, 0.37338757690018604, 0.12532248461996273, 0.12532248461996273]
printing an ep nov before normalisation:  37.59740496402468
printing an ep nov before normalisation:  45.56482447440219
actor:  1 policy actor:  1  step number:  44 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.74618 0.5866666666666668 0.5866666666666668
maxi score, test score, baseline:  -0.74618 0.5866666666666668 0.5866666666666668
probs:  [0.12532248461996273, 0.12532248461996273, 0.12532248461996273, 0.37338757690018604, 0.12532248461996273, 0.12532248461996273]
printing an ep nov before normalisation:  53.53263914259729
maxi score, test score, baseline:  -0.74618 0.5866666666666668 0.5866666666666668
probs:  [0.1254329539831741, 0.1254329539831741, 0.1254329539831741, 0.37283523008412933, 0.1254329539831741, 0.1254329539831741]
maxi score, test score, baseline:  -0.74618 0.5866666666666668 0.5866666666666668
probs:  [0.1254329539831741, 0.1254329539831741, 0.1254329539831741, 0.37283523008412933, 0.1254329539831741, 0.1254329539831741]
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  47 total reward:  0.4399999999999996  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.845]
 [0.778]
 [0.845]
 [0.752]
 [0.845]
 [0.845]
 [0.845]] [[36.364]
 [36.031]
 [36.364]
 [37.674]
 [36.364]
 [36.364]
 [36.364]] [[0.845]
 [0.778]
 [0.845]
 [0.752]
 [0.845]
 [0.845]
 [0.845]]
maxi score, test score, baseline:  -0.74618 0.5866666666666668 0.5866666666666668
actor:  0 policy actor:  1  step number:  67 total reward:  0.09333333333333271  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.7439933333333334 0.5866666666666668 0.5866666666666668
probs:  [0.1254329539831741, 0.1254329539831741, 0.1254329539831741, 0.37283523008412933, 0.1254329539831741, 0.1254329539831741]
printing an ep nov before normalisation:  27.290965762604134
maxi score, test score, baseline:  -0.7439933333333334 0.5866666666666668 0.5866666666666668
probs:  [0.1254329539831741, 0.1254329539831741, 0.1254329539831741, 0.37283523008412933, 0.1254329539831741, 0.1254329539831741]
from probs:  [0.1254329539831741, 0.1254329539831741, 0.1254329539831741, 0.37283523008412933, 0.1254329539831741, 0.1254329539831741]
maxi score, test score, baseline:  -0.7439933333333334 0.5866666666666668 0.5866666666666668
probs:  [0.1254329539831741, 0.1254329539831741, 0.1254329539831741, 0.37283523008412933, 0.1254329539831741, 0.1254329539831741]
maxi score, test score, baseline:  -0.7439933333333334 0.5866666666666668 0.5866666666666668
probs:  [0.1254329539831741, 0.1254329539831741, 0.1254329539831741, 0.37283523008412933, 0.1254329539831741, 0.1254329539831741]
maxi score, test score, baseline:  -0.7439933333333334 0.5866666666666668 0.5866666666666668
probs:  [0.1254329539831741, 0.1254329539831741, 0.1254329539831741, 0.37283523008412933, 0.1254329539831741, 0.1254329539831741]
printing an ep nov before normalisation:  23.825812339782715
maxi score, test score, baseline:  -0.7439933333333334 0.5866666666666668 0.5866666666666668
probs:  [0.1254329539831741, 0.1254329539831741, 0.1254329539831741, 0.37283523008412933, 0.1254329539831741, 0.1254329539831741]
actor:  1 policy actor:  1  step number:  57 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7439933333333334 0.5866666666666668 0.5866666666666668
maxi score, test score, baseline:  -0.7439933333333334 0.5866666666666668 0.5866666666666668
probs:  [0.1254329539831741, 0.1254329539831741, 0.1254329539831741, 0.37283523008412933, 0.1254329539831741, 0.1254329539831741]
printing an ep nov before normalisation:  39.97044033474392
maxi score, test score, baseline:  -0.7439933333333334 0.5866666666666668 0.5866666666666668
probs:  [0.1254329539831741, 0.1254329539831741, 0.1254329539831741, 0.37283523008412933, 0.1254329539831741, 0.1254329539831741]
actor:  0 policy actor:  1  step number:  61 total reward:  0.06666666666666576  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.00013603582772248046
printing an ep nov before normalisation:  49.534968700752685
maxi score, test score, baseline:  -0.7418600000000001 0.5866666666666668 0.5866666666666668
probs:  [0.1254329539831741, 0.1254329539831741, 0.1254329539831741, 0.37283523008412933, 0.1254329539831741, 0.1254329539831741]
printing an ep nov before normalisation:  49.7022302614788
printing an ep nov before normalisation:  43.73847848599347
Printing some Q and Qe and total Qs values:  [[0.565]
 [0.622]
 [0.532]
 [0.482]
 [0.532]
 [0.532]
 [0.508]] [[38.975]
 [32.8  ]
 [36.058]
 [42.888]
 [36.058]
 [36.058]
 [38.964]] [[0.565]
 [0.622]
 [0.532]
 [0.482]
 [0.532]
 [0.532]
 [0.508]]
maxi score, test score, baseline:  -0.7418600000000001 0.5866666666666668 0.5866666666666668
probs:  [0.1254329539831741, 0.1254329539831741, 0.1254329539831741, 0.37283523008412933, 0.1254329539831741, 0.1254329539831741]
printing an ep nov before normalisation:  38.546635835574286
printing an ep nov before normalisation:  43.22556192824248
actor:  0 policy actor:  0  step number:  55 total reward:  0.13333333333333286  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.934159755706787
actor:  1 policy actor:  1  step number:  73 total reward:  0.22666666666666513  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.073]
 [-0.042]
 [-0.073]
 [-0.073]
 [-0.075]
 [-0.073]
 [-0.073]] [[39.076]
 [47.686]
 [39.076]
 [39.076]
 [44.177]
 [39.076]
 [39.076]] [[0.296]
 [0.459]
 [0.296]
 [0.296]
 [0.372]
 [0.296]
 [0.296]]
maxi score, test score, baseline:  -0.7395933333333334 0.5866666666666668 0.5866666666666668
probs:  [0.1254329539831741, 0.1254329539831741, 0.1254329539831741, 0.37283523008412933, 0.1254329539831741, 0.1254329539831741]
maxi score, test score, baseline:  -0.7395933333333334 0.5866666666666668 0.5866666666666668
probs:  [0.1254329539831741, 0.1254329539831741, 0.1254329539831741, 0.37283523008412933, 0.1254329539831741, 0.1254329539831741]
Printing some Q and Qe and total Qs values:  [[-0.168]
 [-0.138]
 [-0.168]
 [-0.155]
 [-0.17 ]
 [-0.166]
 [-0.169]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.168]
 [-0.138]
 [-0.168]
 [-0.155]
 [-0.17 ]
 [-0.166]
 [-0.169]]
maxi score, test score, baseline:  -0.7395933333333334 0.5866666666666668 0.5866666666666668
probs:  [0.12554283458349638, 0.12554283458349638, 0.12554283458349638, 0.37228582708251784, 0.12554283458349638, 0.12554283458349638]
maxi score, test score, baseline:  -0.7395933333333334 0.5866666666666668 0.5866666666666668
probs:  [0.12554283458349638, 0.12554283458349638, 0.12554283458349638, 0.37228582708251784, 0.12554283458349638, 0.12554283458349638]
actor:  1 policy actor:  1  step number:  48 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]] [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]] [[1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]]
maxi score, test score, baseline:  -0.7395933333333334 0.5866666666666668 0.5866666666666668
probs:  [0.12554283458349638, 0.12554283458349638, 0.12554283458349638, 0.37228582708251784, 0.12554283458349638, 0.12554283458349638]
Printing some Q and Qe and total Qs values:  [[0.369]
 [0.498]
 [0.356]
 [0.391]
 [0.337]
 [0.334]
 [0.484]] [[15.071]
 [24.908]
 [14.82 ]
 [18.239]
 [14.701]
 [14.923]
 [27.283]] [[0.83 ]
 [1.261]
 [0.81 ]
 [0.95 ]
 [0.787]
 [0.791]
 [1.32 ]]
maxi score, test score, baseline:  -0.7395933333333334 0.5866666666666668 0.5866666666666668
actor:  1 policy actor:  1  step number:  48 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  39 total reward:  0.48  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  34.65914196438259
siam score:  -0.82995677
printing an ep nov before normalisation:  39.90756454909437
maxi score, test score, baseline:  -0.7366333333333334 0.5866666666666668 0.5866666666666668
probs:  [0.12565213111526485, 0.12565213111526485, 0.12565213111526485, 0.37173934442367584, 0.12565213111526485, 0.12565213111526485]
printing an ep nov before normalisation:  40.81400754131464
actions average: 
K:  4  action  0 :  tensor([0.4065, 0.0785, 0.0729, 0.1027, 0.1335, 0.1020, 0.1038],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0294, 0.8956, 0.0158, 0.0134, 0.0127, 0.0106, 0.0226],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0988, 0.1253, 0.2750, 0.0904, 0.0989, 0.2172, 0.0944],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1377, 0.2621, 0.0784, 0.2366, 0.0902, 0.0920, 0.1029],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1494, 0.2096, 0.0962, 0.1214, 0.1772, 0.1243, 0.1218],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1501, 0.0688, 0.1372, 0.1548, 0.1369, 0.2305, 0.1217],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1890, 0.1118, 0.1101, 0.1417, 0.1470, 0.1638, 0.1366],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.7366333333333334 0.5866666666666668 0.5866666666666668
probs:  [0.12565213111526485, 0.12565213111526485, 0.12565213111526485, 0.37173934442367584, 0.12565213111526485, 0.12565213111526485]
printing an ep nov before normalisation:  46.410409314887545
printing an ep nov before normalisation:  38.822871940836414
printing an ep nov before normalisation:  42.80510395172243
maxi score, test score, baseline:  -0.7366333333333334 0.5866666666666668 0.5866666666666668
probs:  [0.12565213111526485, 0.12565213111526485, 0.12565213111526485, 0.37173934442367584, 0.12565213111526485, 0.12565213111526485]
actions average: 
K:  4  action  0 :  tensor([0.2350, 0.1010, 0.1066, 0.1312, 0.1749, 0.1328, 0.1184],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0151, 0.9180, 0.0206, 0.0135, 0.0058, 0.0071, 0.0199],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1699, 0.0502, 0.1795, 0.1362, 0.1434, 0.1789, 0.1420],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1837, 0.0460, 0.1128, 0.2727, 0.1356, 0.1227, 0.1264],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1716, 0.0293, 0.1004, 0.2006, 0.3157, 0.0882, 0.0942],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1860, 0.0472, 0.1426, 0.1512, 0.1425, 0.1641, 0.1665],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1423, 0.1167, 0.1174, 0.1281, 0.1415, 0.1414, 0.2126],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.7366333333333334 0.5866666666666668 0.5866666666666668
actor:  1 policy actor:  1  step number:  67 total reward:  0.03999999999999948  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.071289435896073
maxi score, test score, baseline:  -0.7366333333333334 0.5866666666666668 0.5866666666666668
probs:  [0.12576084822305028, 0.12576084822305028, 0.12576084822305028, 0.3711957588847487, 0.12576084822305028, 0.12576084822305028]
maxi score, test score, baseline:  -0.7366333333333334 0.5866666666666668 0.5866666666666668
probs:  [0.12576084822305028, 0.12576084822305028, 0.12576084822305028, 0.3711957588847487, 0.12576084822305028, 0.12576084822305028]
printing an ep nov before normalisation:  63.342282417697746
maxi score, test score, baseline:  -0.7366333333333334 0.5866666666666668 0.5866666666666668
probs:  [0.12576084822305028, 0.12576084822305028, 0.12576084822305028, 0.3711957588847487, 0.12576084822305028, 0.12576084822305028]
printing an ep nov before normalisation:  45.80282362607418
printing an ep nov before normalisation:  56.7913942285233
maxi score, test score, baseline:  -0.7366333333333334 0.5866666666666668 0.5866666666666668
probs:  [0.12576084822305028, 0.12576084822305028, 0.12576084822305028, 0.3711957588847487, 0.12576084822305028, 0.12576084822305028]
maxi score, test score, baseline:  -0.7366333333333334 0.5866666666666668 0.5866666666666668
maxi score, test score, baseline:  -0.7366333333333334 0.5866666666666668 0.5866666666666668
probs:  [0.12576084822305028, 0.12576084822305028, 0.12576084822305028, 0.3711957588847487, 0.12576084822305028, 0.12576084822305028]
actor:  1 policy actor:  1  step number:  53 total reward:  0.35999999999999943  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.7366333333333334 0.5866666666666668 0.5866666666666668
probs:  [0.12586899050229966, 0.12586899050229966, 0.12586899050229966, 0.3706550474885018, 0.12586899050229966, 0.12586899050229966]
siam score:  -0.8421094
maxi score, test score, baseline:  -0.7366333333333334 0.5866666666666668 0.5866666666666668
probs:  [0.12586899050229966, 0.12586899050229966, 0.12586899050229966, 0.3706550474885018, 0.12586899050229966, 0.12586899050229966]
maxi score, test score, baseline:  -0.7366333333333334 0.5866666666666668 0.5866666666666668
probs:  [0.12586899050229966, 0.12586899050229966, 0.12586899050229966, 0.3706550474885018, 0.12586899050229966, 0.12586899050229966]
Printing some Q and Qe and total Qs values:  [[0.844]
 [0.844]
 [0.844]
 [0.844]
 [0.844]
 [0.844]
 [0.844]] [[44.637]
 [44.637]
 [44.637]
 [44.637]
 [44.637]
 [44.637]
 [44.637]] [[0.844]
 [0.844]
 [0.844]
 [0.844]
 [0.844]
 [0.844]
 [0.844]]
maxi score, test score, baseline:  -0.7366333333333334 0.5866666666666668 0.5866666666666668
probs:  [0.12586899050229966, 0.12586899050229966, 0.12586899050229966, 0.3706550474885018, 0.12586899050229966, 0.12586899050229966]
maxi score, test score, baseline:  -0.7366333333333334 0.5866666666666668 0.5866666666666668
probs:  [0.12586899050229966, 0.12586899050229966, 0.12586899050229966, 0.3706550474885018, 0.12586899050229966, 0.12586899050229966]
maxi score, test score, baseline:  -0.7366333333333334 0.5866666666666668 0.5866666666666668
probs:  [0.12586899050229966, 0.12586899050229966, 0.12586899050229966, 0.3706550474885018, 0.12586899050229966, 0.12586899050229966]
maxi score, test score, baseline:  -0.7366333333333334 0.5866666666666668 0.5866666666666668
probs:  [0.12586899050229966, 0.12586899050229966, 0.12586899050229966, 0.3706550474885018, 0.12586899050229966, 0.12586899050229966]
actions average: 
K:  4  action  0 :  tensor([0.2706, 0.1030, 0.1143, 0.1267, 0.1461, 0.1191, 0.1203],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0202, 0.8705, 0.0139, 0.0346, 0.0192, 0.0122, 0.0293],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1407, 0.0184, 0.1512, 0.2070, 0.1412, 0.1887, 0.1529],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1514, 0.1837, 0.1177, 0.1952, 0.1146, 0.1071, 0.1302],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2471, 0.0482, 0.0917, 0.1031, 0.2975, 0.1066, 0.1059],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1821, 0.0004, 0.1225, 0.2544, 0.1451, 0.1495, 0.1460],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1140, 0.2182, 0.0895, 0.1178, 0.1377, 0.0983, 0.2244],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  1  step number:  51 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7342066666666668 0.5866666666666668 0.5866666666666668
probs:  [0.12586899050229966, 0.12586899050229966, 0.12586899050229966, 0.3706550474885018, 0.12586899050229966, 0.12586899050229966]
maxi score, test score, baseline:  -0.7342066666666668 0.5866666666666668 0.5866666666666668
probs:  [0.12586899050229966, 0.12586899050229966, 0.12586899050229966, 0.3706550474885018, 0.12586899050229966, 0.12586899050229966]
printing an ep nov before normalisation:  44.28459777109013
printing an ep nov before normalisation:  40.996118150768794
printing an ep nov before normalisation:  50.88203971149294
from probs:  [0.12586899050229966, 0.12586899050229966, 0.12586899050229966, 0.3706550474885018, 0.12586899050229966, 0.12586899050229966]
printing an ep nov before normalisation:  48.164495729382395
maxi score, test score, baseline:  -0.7342066666666668 0.5866666666666668 0.5866666666666668
probs:  [0.12586899050229966, 0.12586899050229966, 0.12586899050229966, 0.3706550474885018, 0.12586899050229966, 0.12586899050229966]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  53.71387014774814
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  54 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.7342066666666668 0.5866666666666668 0.5866666666666668
probs:  [0.12597656249999978, 0.12597656249999978, 0.12597656249999978, 0.370117187500001, 0.12597656249999978, 0.12597656249999978]
Printing some Q and Qe and total Qs values:  [[-0.098]
 [-0.081]
 [-0.098]
 [-0.098]
 [-0.098]
 [-0.098]
 [-0.098]] [[38.772]
 [40.469]
 [38.772]
 [38.772]
 [38.772]
 [38.772]
 [38.772]] [[0.924]
 [1.027]
 [0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.924]]
maxi score, test score, baseline:  -0.7342066666666668 0.5866666666666668 0.5866666666666668
probs:  [0.12597656249999978, 0.12597656249999978, 0.12597656249999978, 0.370117187500001, 0.12597656249999978, 0.12597656249999978]
printing an ep nov before normalisation:  39.239091353238656
printing an ep nov before normalisation:  31.896885306467144
maxi score, test score, baseline:  -0.7342066666666668 0.5866666666666668 0.5866666666666668
probs:  [0.1260835687152985, 0.1260835687152985, 0.1260835687152985, 0.3695821564235074, 0.1260835687152985, 0.1260835687152985]
maxi score, test score, baseline:  -0.7342066666666668 0.5866666666666668 0.5866666666666668
probs:  [0.1260835687152985, 0.1260835687152985, 0.1260835687152985, 0.3695821564235074, 0.1260835687152985, 0.1260835687152985]
actor:  1 policy actor:  1  step number:  45 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.7342066666666668 0.5866666666666668 0.5866666666666668
probs:  [0.1261900136001522, 0.1261900136001522, 0.1261900136001522, 0.3690499319992391, 0.1261900136001522, 0.1261900136001522]
maxi score, test score, baseline:  -0.7342066666666668 0.5866666666666668 0.5866666666666668
actor:  0 policy actor:  1  step number:  40 total reward:  0.5266666666666667  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.7311533333333333 0.5866666666666668 0.5866666666666668
probs:  [0.12629590155992473, 0.12629590155992473, 0.12629590155992473, 0.3685204922003765, 0.12629590155992473, 0.12629590155992473]
maxi score, test score, baseline:  -0.7311533333333333 0.5866666666666668 0.5866666666666668
probs:  [0.12629590155992473, 0.12629590155992473, 0.12629590155992473, 0.3685204922003765, 0.12629590155992473, 0.12629590155992473]
printing an ep nov before normalisation:  34.46088684334471
printing an ep nov before normalisation:  26.56376361846924
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7311533333333333 0.5866666666666668 0.5866666666666668
probs:  [0.12629590155992473, 0.12629590155992473, 0.12629590155992473, 0.3685204922003765, 0.12629590155992473, 0.12629590155992473]
maxi score, test score, baseline:  -0.7311533333333333 0.5866666666666668 0.5866666666666668
maxi score, test score, baseline:  -0.7311533333333333 0.5866666666666668 0.5866666666666668
probs:  [0.12629590155992473, 0.12629590155992473, 0.12629590155992473, 0.3685204922003765, 0.12629590155992473, 0.12629590155992473]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7311533333333333 0.5866666666666668 0.5866666666666668
probs:  [0.12629590155992473, 0.12629590155992473, 0.12629590155992473, 0.3685204922003765, 0.12629590155992473, 0.12629590155992473]
Printing some Q and Qe and total Qs values:  [[0.774]
 [0.871]
 [0.774]
 [0.774]
 [0.774]
 [0.774]
 [0.774]] [[30.258]
 [33.635]
 [30.258]
 [30.258]
 [30.258]
 [30.258]
 [30.258]] [[0.774]
 [0.871]
 [0.774]
 [0.774]
 [0.774]
 [0.774]
 [0.774]]
maxi score, test score, baseline:  -0.7311533333333333 0.5866666666666668 0.5866666666666668
probs:  [0.12629590155992473, 0.12629590155992473, 0.12629590155992473, 0.3685204922003765, 0.12629590155992473, 0.12629590155992473]
printing an ep nov before normalisation:  18.9804208278656
actor:  0 policy actor:  0  step number:  75 total reward:  0.11999999999999922  reward:  1.0 rdn_beta:  1.0
siam score:  -0.823162
actor:  1 policy actor:  1  step number:  59 total reward:  0.19999999999999907  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7289133333333333 0.5866666666666668 0.5866666666666668
probs:  [0.12640123695399758, 0.12640123695399758, 0.12640123695399758, 0.36799381523001207, 0.12640123695399758, 0.12640123695399758]
maxi score, test score, baseline:  -0.7289133333333333 0.5866666666666668 0.5866666666666668
probs:  [0.12640123695399758, 0.12640123695399758, 0.12640123695399758, 0.36799381523001207, 0.12640123695399758, 0.12640123695399758]
printing an ep nov before normalisation:  66.60207921525642
printing an ep nov before normalisation:  38.8446307182312
maxi score, test score, baseline:  -0.7289133333333333 0.5866666666666668 0.5866666666666668
maxi score, test score, baseline:  -0.7289133333333333 0.5866666666666668 0.5866666666666668
maxi score, test score, baseline:  -0.7289133333333333 0.5866666666666668 0.5866666666666668
probs:  [0.12650602409638226, 0.12650602409638226, 0.12650602409638226, 0.36746987951808874, 0.12650602409638226, 0.12650602409638226]
printing an ep nov before normalisation:  50.025550967578795
Printing some Q and Qe and total Qs values:  [[0.368]
 [0.404]
 [0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]] [[31.751]
 [30.606]
 [31.751]
 [31.751]
 [31.751]
 [31.751]
 [31.751]] [[1.701]
 [1.648]
 [1.701]
 [1.701]
 [1.701]
 [1.701]
 [1.701]]
maxi score, test score, baseline:  -0.7289133333333333 0.5866666666666668 0.5866666666666668
probs:  [0.12650602409638226, 0.12650602409638226, 0.12650602409638226, 0.36746987951808874, 0.12650602409638226, 0.12650602409638226]
actor:  1 policy actor:  1  step number:  42 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.7289133333333333 0.5866666666666668 0.5866666666666668
probs:  [0.12661026725629473, 0.12661026725629473, 0.12661026725629473, 0.3669486637185263, 0.12661026725629473, 0.12661026725629473]
maxi score, test score, baseline:  -0.7289133333333333 0.5866666666666668 0.5866666666666668
probs:  [0.12661026725629473, 0.12661026725629473, 0.12661026725629473, 0.3669486637185263, 0.12661026725629473, 0.12661026725629473]
printing an ep nov before normalisation:  39.67616211950627
maxi score, test score, baseline:  -0.7289133333333333 0.5866666666666668 0.5866666666666668
probs:  [0.12661026725629473, 0.12661026725629473, 0.12661026725629473, 0.3669486637185263, 0.12661026725629473, 0.12661026725629473]
siam score:  -0.8246225
actor:  1 policy actor:  1  step number:  71 total reward:  0.09333333333333238  reward:  1.0 rdn_beta:  1.333
using another actor
actions average: 
K:  1  action  0 :  tensor([0.4820, 0.0096, 0.0845, 0.0955, 0.1168, 0.1059, 0.1057],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0108, 0.9371, 0.0056, 0.0086, 0.0034, 0.0024, 0.0320],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1335, 0.0417, 0.3466, 0.1203, 0.1005, 0.1565, 0.1009],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1357, 0.0141, 0.0938, 0.4354, 0.1099, 0.1052, 0.1060],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2387, 0.0044, 0.1397, 0.1564, 0.1690, 0.1406, 0.1513],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0712, 0.0189, 0.1336, 0.0818, 0.0782, 0.5076, 0.1087],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1654, 0.0276, 0.0920, 0.1126, 0.1610, 0.0927, 0.3487],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.216]
 [0.39 ]
 [0.368]
 [0.464]
 [0.316]
 [0.305]
 [0.404]] [[49.531]
 [51.696]
 [44.726]
 [51.425]
 [55.758]
 [55.583]
 [48.922]] [[0.97 ]
 [1.203]
 [0.991]
 [1.269]
 [1.239]
 [1.223]
 [1.142]]
printing an ep nov before normalisation:  26.72055284321928
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]] [[41.238]
 [41.238]
 [41.238]
 [41.238]
 [41.238]
 [41.238]
 [41.238]] [[1.604]
 [1.604]
 [1.604]
 [1.604]
 [1.604]
 [1.604]
 [1.604]]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5666666666666668  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.107]
 [-0.053]
 [-0.107]
 [-0.107]
 [-0.107]
 [-0.104]
 [-0.107]] [[58.579]
 [61.292]
 [58.579]
 [58.579]
 [58.579]
 [65.223]
 [58.579]] [[0.928]
 [1.042]
 [0.928]
 [0.928]
 [0.928]
 [1.08 ]
 [0.928]]
maxi score, test score, baseline:  -0.7289133333333333 0.5866666666666668 0.5866666666666668
probs:  [0.12681713848507808, 0.12681713848507808, 0.12681713848507808, 0.3659143075746097, 0.12681713848507808, 0.12681713848507808]
Printing some Q and Qe and total Qs values:  [[-0.113]
 [-0.099]
 [-0.113]
 [-0.113]
 [-0.113]
 [-0.113]
 [-0.113]] [[44.822]
 [74.165]
 [44.822]
 [44.822]
 [44.822]
 [44.822]
 [44.822]] [[0.403]
 [1.075]
 [0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]]
maxi score, test score, baseline:  -0.7289133333333333 0.5866666666666668 0.5866666666666668
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  53 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  1.333
from probs:  [0.12691977487360198, 0.12691977487360198, 0.12691977487360198, 0.3654011256319899, 0.12691977487360198, 0.12691977487360198]
printing an ep nov before normalisation:  42.787723742103346
printing an ep nov before normalisation:  45.12034449280556
printing an ep nov before normalisation:  36.41106054545295
printing an ep nov before normalisation:  24.95668266528747
printing an ep nov before normalisation:  20.48717534212341
line 256 mcts: sample exp_bonus 22.831563453917976
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.677]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]] [[52.603]
 [53.976]
 [52.603]
 [52.603]
 [52.603]
 [52.603]
 [52.603]] [[0.634]
 [0.677]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]]
maxi score, test score, baseline:  -0.7289133333333333 0.5866666666666668 0.5866666666666668
probs:  [0.12702188392007407, 0.12702188392007407, 0.12702188392007407, 0.3648905803996299, 0.12702188392007407, 0.12702188392007407]
actor:  1 policy actor:  1  step number:  75 total reward:  0.31999999999999895  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.7289133333333333 0.5866666666666668 0.5866666666666668
probs:  [0.12712346967827506, 0.12712346967827506, 0.12712346967827506, 0.3643826516086248, 0.12712346967827506, 0.12712346967827506]
using explorer policy with actor:  0
printing an ep nov before normalisation:  40.906932235656676
Printing some Q and Qe and total Qs values:  [[0.714]
 [0.71 ]
 [0.582]
 [0.616]
 [0.588]
 [0.578]
 [0.544]] [[41.444]
 [43.249]
 [35.676]
 [40.142]
 [38.621]
 [35.738]
 [36.62 ]] [[0.714]
 [0.71 ]
 [0.582]
 [0.616]
 [0.588]
 [0.578]
 [0.544]]
printing an ep nov before normalisation:  54.01991073004175
actor:  0 policy actor:  1  step number:  55 total reward:  0.26666666666666616  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  37.25202280839226
actor:  1 policy actor:  1  step number:  42 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  42.71963131047385
maxi score, test score, baseline:  -0.72638 0.5866666666666668 0.5866666666666668
probs:  [0.12722453616054408, 0.12722453616054408, 0.12722453616054408, 0.36387731919727967, 0.12722453616054408, 0.12722453616054408]
maxi score, test score, baseline:  -0.72638 0.5866666666666668 0.5866666666666668
probs:  [0.12722453616054408, 0.12722453616054408, 0.12722453616054408, 0.36387731919727967, 0.12722453616054408, 0.12722453616054408]
siam score:  -0.83015794
printing an ep nov before normalisation:  59.88432088687189
printing an ep nov before normalisation:  37.187230587005615
maxi score, test score, baseline:  -0.72638 0.5866666666666668 0.5866666666666668
probs:  [0.12722453616054408, 0.12722453616054408, 0.12722453616054408, 0.36387731919727967, 0.12722453616054408, 0.12722453616054408]
maxi score, test score, baseline:  -0.72638 0.5866666666666668 0.5866666666666668
probs:  [0.12742512714258647, 0.12742512714258647, 0.12742512714258647, 0.3628743642870676, 0.12742512714258647, 0.12742512714258647]
printing an ep nov before normalisation:  19.078290462493896
Printing some Q and Qe and total Qs values:  [[0.23 ]
 [0.385]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]] [[30.936]
 [41.316]
 [30.936]
 [30.936]
 [30.936]
 [30.936]
 [30.936]] [[0.352]
 [0.605]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]]
maxi score, test score, baseline:  -0.72638 0.5866666666666668 0.5866666666666668
probs:  [0.127524659464536, 0.127524659464536, 0.127524659464536, 0.3623767026773201, 0.127524659464536, 0.127524659464536]
printing an ep nov before normalisation:  39.155662957312835
maxi score, test score, baseline:  -0.72638 0.5866666666666668 0.5866666666666668
probs:  [0.127524659464536, 0.127524659464536, 0.127524659464536, 0.3623767026773201, 0.127524659464536, 0.127524659464536]
maxi score, test score, baseline:  -0.72638 0.5866666666666668 0.5866666666666668
probs:  [0.127524659464536, 0.127524659464536, 0.127524659464536, 0.3623767026773201, 0.127524659464536, 0.127524659464536]
maxi score, test score, baseline:  -0.72638 0.5866666666666668 0.5866666666666668
printing an ep nov before normalisation:  43.709416321072325
maxi score, test score, baseline:  -0.72638 0.5866666666666668 0.5866666666666668
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.431]
 [0.413]
 [0.452]
 [0.413]
 [0.309]
 [0.377]] [[30.584]
 [29.933]
 [30.953]
 [30.462]
 [30.702]
 [31.042]
 [30.028]] [[1.944]
 [1.844]
 [1.92 ]
 [1.915]
 [1.897]
 [1.825]
 [1.799]]
maxi score, test score, baseline:  -0.72638 0.5866666666666668 0.5866666666666668
actor:  1 policy actor:  1  step number:  57 total reward:  0.2799999999999989  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.72638 0.5866666666666668 0.5866666666666668
probs:  [0.127524659464536, 0.127524659464536, 0.127524659464536, 0.3623767026773201, 0.127524659464536, 0.127524659464536]
printing an ep nov before normalisation:  50.44428191077102
Printing some Q and Qe and total Qs values:  [[-0.087]
 [-0.07 ]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]] [[30.496]
 [44.113]
 [30.496]
 [30.496]
 [30.496]
 [30.496]
 [30.496]] [[0.392]
 [0.848]
 [0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.392]]
maxi score, test score, baseline:  -0.72638 0.5866666666666668 0.5866666666666668
probs:  [0.127524659464536, 0.127524659464536, 0.127524659464536, 0.3623767026773201, 0.127524659464536, 0.127524659464536]
line 256 mcts: sample exp_bonus 40.9743385984958
maxi score, test score, baseline:  -0.72638 0.5866666666666668 0.5866666666666668
probs:  [0.12762368815591815, 0.12762368815591815, 0.12762368815591815, 0.36188155922040927, 0.12762368815591815, 0.12762368815591815]
maxi score, test score, baseline:  -0.72638 0.5866666666666668 0.5866666666666668
actor:  1 policy actor:  1  step number:  66 total reward:  0.19333333333333325  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.72638 0.5866666666666668 0.5866666666666668
probs:  [0.12762368815591815, 0.12762368815591815, 0.12762368815591815, 0.36188155922040927, 0.12762368815591815, 0.12762368815591815]
using another actor
printing an ep nov before normalisation:  42.23727168684139
maxi score, test score, baseline:  -0.72638 0.5866666666666668 0.5866666666666668
probs:  [0.12772221702962713, 0.12772221702962713, 0.12772221702962713, 0.36138891485186425, 0.12772221702962713, 0.12772221702962713]
maxi score, test score, baseline:  -0.72638 0.5866666666666668 0.5866666666666668
probs:  [0.12772221702962713, 0.12772221702962713, 0.12772221702962713, 0.36138891485186425, 0.12772221702962713, 0.12772221702962713]
printing an ep nov before normalisation:  56.04305290001221
printing an ep nov before normalisation:  52.668793979785846
maxi score, test score, baseline:  -0.72638 0.5866666666666668 0.5866666666666668
probs:  [0.12782024986015217, 0.12782024986015217, 0.12782024986015217, 0.36089875069923916, 0.12782024986015217, 0.12782024986015217]
maxi score, test score, baseline:  -0.72638 0.5866666666666668 0.5866666666666668
probs:  [0.12782024986015217, 0.12782024986015217, 0.12782024986015217, 0.36089875069923916, 0.12782024986015217, 0.12782024986015217]
maxi score, test score, baseline:  -0.72638 0.5866666666666668 0.5866666666666668
probs:  [0.12782024986015217, 0.12782024986015217, 0.12782024986015217, 0.36089875069923916, 0.12782024986015217, 0.12782024986015217]
printing an ep nov before normalisation:  22.659269645271266
maxi score, test score, baseline:  -0.72638 0.5866666666666668 0.5866666666666668
probs:  [0.12782024986015217, 0.12782024986015217, 0.12782024986015217, 0.36089875069923916, 0.12782024986015217, 0.12782024986015217]
printing an ep nov before normalisation:  25.777013792493975
printing an ep nov before normalisation:  34.624759974930754
printing an ep nov before normalisation:  22.886799554723893
maxi score, test score, baseline:  -0.72638 0.5866666666666668 0.5866666666666668
probs:  [0.12782024986015217, 0.12782024986015217, 0.12782024986015217, 0.36089875069923916, 0.12782024986015217, 0.12782024986015217]
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.234]
 [0.309]
 [0.249]
 [0.309]
 [0.309]
 [0.309]] [[34.8  ]
 [35.328]
 [34.8  ]
 [37.871]
 [34.8  ]
 [34.8  ]
 [34.8  ]] [[1.401]
 [1.354]
 [1.401]
 [1.504]
 [1.401]
 [1.401]
 [1.401]]
actor:  1 policy actor:  1  step number:  58 total reward:  0.1666666666666663  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.72638 0.5866666666666668 0.5866666666666668
probs:  [0.12791779038407722, 0.12791779038407722, 0.12791779038407722, 0.36041104807961394, 0.12791779038407722, 0.12791779038407722]
maxi score, test score, baseline:  -0.72638 0.5866666666666668 0.5866666666666668
probs:  [0.12811140927176706, 0.12811140927176706, 0.12811140927176706, 0.3594429536411646, 0.12811140927176706, 0.12811140927176706]
maxi score, test score, baseline:  -0.72638 0.5866666666666668 0.5866666666666668
probs:  [0.12811140927176706, 0.12811140927176706, 0.12811140927176706, 0.3594429536411646, 0.12811140927176706, 0.12811140927176706]
printing an ep nov before normalisation:  50.51061667664219
maxi score, test score, baseline:  -0.72638 0.5866666666666668 0.5866666666666668
probs:  [0.12811140927176706, 0.12811140927176706, 0.12811140927176706, 0.3594429536411646, 0.12811140927176706, 0.12811140927176706]
printing an ep nov before normalisation:  53.844740762695714
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.77796712079884
actions average: 
K:  1  action  0 :  tensor([0.4206, 0.0074, 0.1123, 0.0944, 0.1374, 0.1115, 0.1163],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0083, 0.9563, 0.0074, 0.0061, 0.0061, 0.0053, 0.0105],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1222, 0.1191, 0.2805, 0.0984, 0.0991, 0.1685, 0.1122],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1411, 0.0703, 0.1032, 0.2698, 0.1522, 0.1459, 0.1176],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1210, 0.0016, 0.0818, 0.0887, 0.5172, 0.1148, 0.0749],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1103, 0.0044, 0.1029, 0.1411, 0.1187, 0.4284, 0.0942],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1777, 0.0947, 0.1277, 0.1462, 0.1554, 0.1498, 0.1485],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.331]
 [0.331]
 [0.331]
 [0.331]
 [0.331]
 [0.331]
 [0.331]] [[51.039]
 [51.039]
 [51.039]
 [51.039]
 [51.039]
 [51.039]
 [51.039]] [[2.279]
 [2.279]
 [2.279]
 [2.279]
 [2.279]
 [2.279]
 [2.279]]
maxi score, test score, baseline:  -0.72638 0.5866666666666668 0.5866666666666668
probs:  [0.12811140927176706, 0.12811140927176706, 0.12811140927176706, 0.3594429536411646, 0.12811140927176706, 0.12811140927176706]
actor:  1 policy actor:  1  step number:  50 total reward:  0.33999999999999986  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  41 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  0.333
using another actor
maxi score, test score, baseline:  -0.72638 0.5866666666666668 0.5866666666666668
probs:  [0.12811140927176706, 0.12811140927176706, 0.12811140927176706, 0.3594429536411646, 0.12811140927176706, 0.12811140927176706]
siam score:  -0.8068515
Printing some Q and Qe and total Qs values:  [[0.173]
 [0.493]
 [0.226]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.345]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.173]
 [0.493]
 [0.226]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.345]]
siam score:  -0.8055768
actions average: 
K:  3  action  0 :  tensor([0.3180, 0.0502, 0.1409, 0.1149, 0.1529, 0.1158, 0.1073],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0228, 0.8828, 0.0212, 0.0198, 0.0113, 0.0134, 0.0287],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1269, 0.0277, 0.3018, 0.1341, 0.1177, 0.1651, 0.1267],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1398, 0.0880, 0.1227, 0.2419, 0.1360, 0.1276, 0.1440],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0958, 0.0202, 0.1387, 0.1144, 0.4011, 0.1271, 0.1027],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1322, 0.0238, 0.1773, 0.1605, 0.1410, 0.2354, 0.1298],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1015, 0.1667, 0.1487, 0.1238, 0.0856, 0.1138, 0.2599],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.72638 0.5866666666666668 0.5866666666666668
probs:  [0.12811140927176706, 0.12811140927176706, 0.12811140927176706, 0.3594429536411646, 0.12811140927176706, 0.12811140927176706]
maxi score, test score, baseline:  -0.72638 0.5866666666666668 0.5866666666666668
actor:  0 policy actor:  1  step number:  38 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.22 ]
 [0.391]
 [0.22 ]
 [0.22 ]
 [0.22 ]
 [0.22 ]
 [0.22 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.22 ]
 [0.391]
 [0.22 ]
 [0.22 ]
 [0.22 ]
 [0.22 ]
 [0.22 ]]
Printing some Q and Qe and total Qs values:  [[0.406]
 [0.488]
 [0.406]
 [0.404]
 [0.494]
 [0.403]
 [0.484]] [[37.416]
 [39.81 ]
 [36.775]
 [37.071]
 [39.956]
 [37.328]
 [38.985]] [[0.406]
 [0.488]
 [0.406]
 [0.404]
 [0.494]
 [0.403]
 [0.484]]
maxi score, test score, baseline:  -0.7234600000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12811140927176706, 0.12811140927176706, 0.12811140927176706, 0.3594429536411646, 0.12811140927176706, 0.12811140927176706]
maxi score, test score, baseline:  -0.7234600000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12811140927176706, 0.12811140927176706, 0.12811140927176706, 0.3594429536411646, 0.12811140927176706, 0.12811140927176706]
maxi score, test score, baseline:  -0.7234600000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12811140927176706, 0.12811140927176706, 0.12811140927176706, 0.3594429536411646, 0.12811140927176706, 0.12811140927176706]
printing an ep nov before normalisation:  33.66886624659541
maxi score, test score, baseline:  -0.7234600000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12811140927176706, 0.12811140927176706, 0.12811140927176706, 0.3594429536411646, 0.12811140927176706, 0.12811140927176706]
printing an ep nov before normalisation:  48.75862754099908
printing an ep nov before normalisation:  17.242913793655173
siam score:  -0.819064
printing an ep nov before normalisation:  0.05286011757164033
maxi score, test score, baseline:  -0.7234600000000001 0.5866666666666668 0.5866666666666668
using another actor
maxi score, test score, baseline:  -0.7234600000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12811140927176706, 0.12811140927176706, 0.12811140927176706, 0.3594429536411646, 0.12811140927176706, 0.12811140927176706]
maxi score, test score, baseline:  -0.7234600000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12811140927176706, 0.12811140927176706, 0.12811140927176706, 0.3594429536411646, 0.12811140927176706, 0.12811140927176706]
Printing some Q and Qe and total Qs values:  [[0.633]
 [0.799]
 [0.605]
 [0.629]
 [0.662]
 [0.592]
 [0.618]] [[54.306]
 [45.082]
 [48.401]
 [49.036]
 [47.706]
 [52.261]
 [45.772]] [[0.633]
 [0.799]
 [0.605]
 [0.629]
 [0.662]
 [0.592]
 [0.618]]
maxi score, test score, baseline:  -0.7234600000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12811140927176706, 0.12811140927176706, 0.12811140927176706, 0.3594429536411646, 0.12811140927176706, 0.12811140927176706]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.7234600000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12811140927176706, 0.12811140927176706, 0.12811140927176706, 0.3594429536411646, 0.12811140927176706, 0.12811140927176706]
printing an ep nov before normalisation:  20.769005274919863
actions average: 
K:  0  action  0 :  tensor([0.3398, 0.0853, 0.1034, 0.1279, 0.1254, 0.0995, 0.1186],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0040, 0.9575, 0.0031, 0.0084, 0.0025, 0.0021, 0.0224],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1105, 0.0368, 0.4226, 0.0860, 0.0920, 0.1483, 0.1038],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1673, 0.0051, 0.1247, 0.2848, 0.1559, 0.1330, 0.1291],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1958, 0.0031, 0.1131, 0.1418, 0.3208, 0.1163, 0.1091],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1419, 0.0042, 0.1275, 0.1363, 0.1400, 0.3357, 0.1144],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1431, 0.1513, 0.1119, 0.1271, 0.1276, 0.1180, 0.2210],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.7234600000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12811140927176706, 0.12811140927176706, 0.12811140927176706, 0.3594429536411646, 0.12811140927176706, 0.12811140927176706]
maxi score, test score, baseline:  -0.7234600000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12811140927176706, 0.12811140927176706, 0.12811140927176706, 0.3594429536411646, 0.12811140927176706, 0.12811140927176706]
maxi score, test score, baseline:  -0.7234600000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12811140927176706, 0.12811140927176706, 0.12811140927176706, 0.3594429536411646, 0.12811140927176706, 0.12811140927176706]
printing an ep nov before normalisation:  34.729404110024724
Printing some Q and Qe and total Qs values:  [[-0.148]
 [-0.117]
 [-0.156]
 [-0.133]
 [-0.155]
 [-0.154]
 [-0.149]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.148]
 [-0.117]
 [-0.156]
 [-0.133]
 [-0.155]
 [-0.154]
 [-0.149]]
siam score:  -0.8233112
maxi score, test score, baseline:  -0.7234600000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12811140927176706, 0.12811140927176706, 0.12811140927176706, 0.3594429536411646, 0.12811140927176706, 0.12811140927176706]
maxi score, test score, baseline:  -0.7234600000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12811140927176706, 0.12811140927176706, 0.12811140927176706, 0.3594429536411646, 0.12811140927176706, 0.12811140927176706]
Printing some Q and Qe and total Qs values:  [[0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]] [[50.786]
 [50.786]
 [50.786]
 [50.786]
 [50.786]
 [50.786]
 [50.786]] [[2.142]
 [2.142]
 [2.142]
 [2.142]
 [2.142]
 [2.142]
 [2.142]]
maxi score, test score, baseline:  -0.7234600000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12820749492338773, 0.12820749492338773, 0.12820749492338773, 0.35896252538306145, 0.12820749492338773, 0.12820749492338773]
maxi score, test score, baseline:  -0.7234600000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12820749492338773, 0.12820749492338773, 0.12820749492338773, 0.35896252538306145, 0.12820749492338773, 0.12820749492338773]
printing an ep nov before normalisation:  34.19088108106707
printing an ep nov before normalisation:  5.214030231688298
printing an ep nov before normalisation:  28.354881672698752
printing an ep nov before normalisation:  12.828856706619263
actor:  1 policy actor:  1  step number:  62 total reward:  0.3399999999999992  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
printing an ep nov before normalisation:  30.74719566468784
actor:  1 policy actor:  1  step number:  65 total reward:  0.1599999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.7234600000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12820749492338773, 0.12820749492338773, 0.12820749492338773, 0.35896252538306145, 0.12820749492338773, 0.12820749492338773]
maxi score, test score, baseline:  -0.7234600000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12820749492338773, 0.12820749492338773, 0.12820749492338773, 0.35896252538306145, 0.12820749492338773, 0.12820749492338773]
printing an ep nov before normalisation:  29.64107081918371
maxi score, test score, baseline:  -0.7234600000000001 0.5866666666666668 0.5866666666666668
probs:  [0.1283031028450394, 0.1283031028450394, 0.1283031028450394, 0.35848448577480274, 0.1283031028450394, 0.1283031028450394]
printing an ep nov before normalisation:  60.57680158725784
maxi score, test score, baseline:  -0.7234600000000001 0.5866666666666668 0.5866666666666668
maxi score, test score, baseline:  -0.7234600000000001 0.5866666666666668 0.5866666666666668
probs:  [0.1283031028450394, 0.1283031028450394, 0.1283031028450394, 0.35848448577480274, 0.1283031028450394, 0.1283031028450394]
actor:  1 policy actor:  1  step number:  41 total reward:  0.5200000000000002  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  52.65516048300746
maxi score, test score, baseline:  -0.7234600000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12849289967933805, 0.12849289967933805, 0.12849289967933805, 0.35753550160331, 0.12849289967933805, 0.12849289967933805]
printing an ep nov before normalisation:  47.20962622146729
maxi score, test score, baseline:  -0.7234600000000001 0.5866666666666668 0.5866666666666668
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.519]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]] [[50.622]
 [43.325]
 [50.622]
 [50.622]
 [50.622]
 [50.622]
 [50.622]] [[1.158]
 [1.041]
 [1.158]
 [1.158]
 [1.158]
 [1.158]
 [1.158]]
Printing some Q and Qe and total Qs values:  [[0.287]
 [0.294]
 [0.287]
 [0.287]
 [0.177]
 [0.176]
 [0.287]] [[45.388]
 [50.443]
 [45.388]
 [45.388]
 [44.672]
 [44.95 ]
 [45.388]] [[1.042]
 [1.2  ]
 [1.042]
 [1.042]
 [0.912]
 [0.918]
 [1.042]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.39333333333333287  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  54 total reward:  0.38  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.7234600000000001 0.5866666666666668 0.5866666666666668
maxi score, test score, baseline:  -0.7234600000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12858709559495296, 0.12858709559495296, 0.12858709559495296, 0.3570645220252353, 0.12858709559495296, 0.12858709559495296]
printing an ep nov before normalisation:  54.76250169734284
maxi score, test score, baseline:  -0.7234600000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12858709559495296, 0.12858709559495296, 0.12858709559495296, 0.3570645220252353, 0.12858709559495296, 0.12858709559495296]
maxi score, test score, baseline:  -0.7234600000000001 0.5866666666666668 0.5866666666666668
printing an ep nov before normalisation:  38.36054801940918
printing an ep nov before normalisation:  51.40546482652965
printing an ep nov before normalisation:  38.355836878509315
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  56 total reward:  0.1933333333333328  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
printing an ep nov before normalisation:  55.70243563569107
maxi score, test score, baseline:  -0.7234600000000001 0.5866666666666668 0.5866666666666668
maxi score, test score, baseline:  -0.7234600000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12886691463303943, 0.12886691463303943, 0.12886691463303943, 0.355665426834803, 0.12886691463303943, 0.12886691463303943]
actor:  0 policy actor:  1  step number:  49 total reward:  0.2799999999999997  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  45.5554530091401
maxi score, test score, baseline:  -0.7209 0.5866666666666668 0.5866666666666668
probs:  [0.12886691463303943, 0.12886691463303943, 0.12886691463303943, 0.355665426834803, 0.12886691463303943, 0.12886691463303943]
printing an ep nov before normalisation:  0.00011015345279474786
actor:  1 policy actor:  1  step number:  44 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.7209 0.5866666666666668 0.5866666666666668
probs:  [0.12895927601809823, 0.12895927601809823, 0.12895927601809823, 0.3552036199095087, 0.12895927601809823, 0.12895927601809823]
printing an ep nov before normalisation:  18.048325606754847
Printing some Q and Qe and total Qs values:  [[0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]] [[17.312]
 [17.312]
 [17.312]
 [17.312]
 [17.312]
 [17.312]
 [17.312]] [[0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]]
printing an ep nov before normalisation:  52.47325452901984
from probs:  [0.12895927601809823, 0.12895927601809823, 0.12895927601809823, 0.3552036199095087, 0.12895927601809823, 0.12895927601809823]
maxi score, test score, baseline:  -0.7209 0.5866666666666668 0.5866666666666668
probs:  [0.12895927601809823, 0.12895927601809823, 0.12895927601809823, 0.3552036199095087, 0.12895927601809823, 0.12895927601809823]
maxi score, test score, baseline:  -0.7209 0.5866666666666668 0.5866666666666668
maxi score, test score, baseline:  -0.7209 0.5866666666666668 0.5866666666666668
maxi score, test score, baseline:  -0.7209 0.5866666666666668 0.5866666666666668
probs:  [0.12895927601809823, 0.12895927601809823, 0.12895927601809823, 0.3552036199095087, 0.12895927601809823, 0.12895927601809823]
printing an ep nov before normalisation:  41.17806409245674
printing an ep nov before normalisation:  4.090586951474506
maxi score, test score, baseline:  -0.7209 0.5866666666666668 0.5866666666666668
probs:  [0.12895927601809823, 0.12895927601809823, 0.12895927601809823, 0.3552036199095087, 0.12895927601809823, 0.12895927601809823]
maxi score, test score, baseline:  -0.7209 0.5866666666666668 0.5866666666666668
probs:  [0.12895927601809823, 0.12895927601809823, 0.12895927601809823, 0.3552036199095087, 0.12895927601809823, 0.12895927601809823]
printing an ep nov before normalisation:  51.29090025374436
maxi score, test score, baseline:  -0.7209 0.5866666666666668 0.5866666666666668
probs:  [0.12895927601809823, 0.12895927601809823, 0.12895927601809823, 0.3552036199095087, 0.12895927601809823, 0.12895927601809823]
Printing some Q and Qe and total Qs values:  [[0.941]
 [0.997]
 [0.952]
 [0.898]
 [0.87 ]
 [0.85 ]
 [1.   ]] [[24.922]
 [16.398]
 [19.744]
 [25.741]
 [26.929]
 [26.627]
 [17.347]] [[0.941]
 [0.997]
 [0.952]
 [0.898]
 [0.87 ]
 [0.85 ]
 [1.   ]]
maxi score, test score, baseline:  -0.7209 0.5866666666666668 0.5866666666666668
probs:  [0.12895927601809823, 0.12895927601809823, 0.12895927601809823, 0.3552036199095087, 0.12895927601809823, 0.12895927601809823]
actor:  0 policy actor:  0  step number:  39 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  32.78927974582639
printing an ep nov before normalisation:  45.78269642106247
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.71802 0.5866666666666668 0.5866666666666668
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  50.9675446155444
using another actor
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.488]
 [0.44 ]
 [0.479]
 [0.435]
 [0.433]
 [0.439]] [[40.831]
 [42.137]
 [38.784]
 [46.022]
 [46.76 ]
 [46.209]
 [43.175]] [[1.3  ]
 [1.342]
 [1.179]
 [1.468]
 [1.45 ]
 [1.429]
 [1.33 ]]
maxi score, test score, baseline:  -0.7180200000000001 0.5866666666666668 0.5866666666666668
maxi score, test score, baseline:  -0.7180200000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12895927601809823, 0.12895927601809823, 0.12895927601809823, 0.3552036199095087, 0.12895927601809823, 0.12895927601809823]
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.588]
 [0.429]
 [0.401]
 [0.381]
 [0.433]
 [0.433]] [[24.341]
 [17.948]
 [22.489]
 [22.676]
 [22.797]
 [24.341]
 [24.341]] [[1.511]
 [1.383]
 [1.425]
 [1.405]
 [1.391]
 [1.511]
 [1.511]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.11999999999999944  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.7180200000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12905118714453173, 0.12905118714453173, 0.12905118714453173, 0.3547440642773414, 0.12905118714453173, 0.12905118714453173]
maxi score, test score, baseline:  -0.7180200000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12905118714453173, 0.12905118714453173, 0.12905118714453173, 0.3547440642773414, 0.12905118714453173, 0.12905118714453173]
printing an ep nov before normalisation:  56.151748220338256
printing an ep nov before normalisation:  34.848478894304456
maxi score, test score, baseline:  -0.7180200000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12905118714453173, 0.12905118714453173, 0.12905118714453173, 0.3547440642773414, 0.12905118714453173, 0.12905118714453173]
maxi score, test score, baseline:  -0.7180200000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12905118714453173, 0.12905118714453173, 0.12905118714453173, 0.3547440642773414, 0.12905118714453173, 0.12905118714453173]
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.34769248962402
Printing some Q and Qe and total Qs values:  [[0.164]
 [0.229]
 [0.026]
 [0.164]
 [0.164]
 [0.164]
 [0.164]] [[34.788]
 [42.132]
 [47.669]
 [34.788]
 [34.788]
 [34.788]
 [34.788]] [[0.496]
 [0.698]
 [0.598]
 [0.496]
 [0.496]
 [0.496]
 [0.496]]
maxi score, test score, baseline:  -0.7180200000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12905118714453173, 0.12905118714453173, 0.12905118714453173, 0.3547440642773414, 0.12905118714453173, 0.12905118714453173]
maxi score, test score, baseline:  -0.7180200000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12905118714453173, 0.12905118714453173, 0.12905118714453173, 0.3547440642773414, 0.12905118714453173, 0.12905118714453173]
UNIT TEST: sample policy line 217 mcts : [0.184 0.245 0.102 0.184 0.02  0.02  0.245]
Printing some Q and Qe and total Qs values:  [[0.312]
 [0.469]
 [0.354]
 [0.399]
 [0.343]
 [0.349]
 [0.346]] [[18.739]
 [25.299]
 [18.762]
 [24.776]
 [18.879]
 [19.161]
 [18.983]] [[0.582]
 [0.924]
 [0.625]
 [0.839]
 [0.617]
 [0.631]
 [0.623]]
printing an ep nov before normalisation:  39.83500031039547
maxi score, test score, baseline:  -0.71802 0.5866666666666668 0.5866666666666668
probs:  [0.12905118714453173, 0.12905118714453173, 0.12905118714453173, 0.3547440642773414, 0.12905118714453173, 0.12905118714453173]
maxi score, test score, baseline:  -0.71802 0.5866666666666668 0.5866666666666668
probs:  [0.12905118714453173, 0.12905118714453173, 0.12905118714453173, 0.3547440642773414, 0.12905118714453173, 0.12905118714453173]
actor:  1 policy actor:  1  step number:  69 total reward:  0.07999999999999952  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.08 ]
 [-0.052]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.08 ]
 [-0.052]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]]
Printing some Q and Qe and total Qs values:  [[-0.104]
 [-0.076]
 [-0.111]
 [-0.113]
 [-0.102]
 [-0.071]
 [-0.101]] [[40.88 ]
 [40.044]
 [39.01 ]
 [39.522]
 [39.915]
 [40.916]
 [40.116]] [[0.896]
 [0.887]
 [0.805]
 [0.826]
 [0.855]
 [0.931]
 [0.865]]
printing an ep nov before normalisation:  43.052298680731326
printing an ep nov before normalisation:  22.223441564990303
siam score:  -0.8217009
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.471]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]] [[37.68]
 [52.4 ]
 [37.68]
 [37.68]
 [37.68]
 [37.68]
 [37.68]] [[0.912]
 [1.488]
 [0.912]
 [0.912]
 [0.912]
 [0.912]
 [0.912]]
printing an ep nov before normalisation:  50.285679070726516
siam score:  -0.82105535
maxi score, test score, baseline:  -0.71802 0.5866666666666668 0.5866666666666668
probs:  [0.12914265129682734, 0.12914265129682734, 0.12914265129682734, 0.35428674351586337, 0.12914265129682734, 0.12914265129682734]
Printing some Q and Qe and total Qs values:  [[0.33]
 [0.33]
 [0.33]
 [0.33]
 [0.33]
 [0.33]
 [0.33]] [[14.811]
 [14.811]
 [14.811]
 [14.811]
 [14.811]
 [14.811]
 [14.811]] [[0.981]
 [0.981]
 [0.981]
 [0.981]
 [0.981]
 [0.981]
 [0.981]]
printing an ep nov before normalisation:  55.21917565692698
maxi score, test score, baseline:  -0.71802 0.5866666666666668 0.5866666666666668
probs:  [0.12914265129682734, 0.12914265129682734, 0.12914265129682734, 0.35428674351586337, 0.12914265129682734, 0.12914265129682734]
actor:  1 policy actor:  1  step number:  83 total reward:  0.0799999999999984  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  28.385676608200683
printing an ep nov before normalisation:  35.11274807347409
maxi score, test score, baseline:  -0.71802 0.5866666666666668 0.5866666666666668
probs:  [0.1292336717276059, 0.1292336717276059, 0.1292336717276059, 0.35383164136197065, 0.1292336717276059, 0.1292336717276059]
printing an ep nov before normalisation:  0.1343649053794138
actor:  0 policy actor:  0  step number:  39 total reward:  0.4933333333333333  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.7150333333333333 0.5866666666666668 0.5866666666666668
probs:  [0.1292336717276059, 0.1292336717276059, 0.1292336717276059, 0.35383164136197065, 0.1292336717276059, 0.1292336717276059]
printing an ep nov before normalisation:  0.026175099517615763
actor:  0 policy actor:  0  step number:  51 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.7125 0.5866666666666668 0.5866666666666668
probs:  [0.1292336717276059, 0.1292336717276059, 0.1292336717276059, 0.35383164136197065, 0.1292336717276059, 0.1292336717276059]
printing an ep nov before normalisation:  36.97340939931058
printing an ep nov before normalisation:  29.63271141052246
Printing some Q and Qe and total Qs values:  [[-0.165]
 [-0.122]
 [-0.223]
 [-0.148]
 [-0.152]
 [-0.282]
 [-0.139]] [[49.747]
 [56.546]
 [42.846]
 [43.898]
 [48.479]
 [41.148]
 [54.114]] [[ 0.055]
 [ 0.128]
 [-0.034]
 [ 0.046]
 [ 0.062]
 [-0.101]
 [ 0.1  ]]
maxi score, test score, baseline:  -0.7125 0.5866666666666668 0.5866666666666668
probs:  [0.1292336717276059, 0.1292336717276059, 0.1292336717276059, 0.35383164136197065, 0.1292336717276059, 0.1292336717276059]
Printing some Q and Qe and total Qs values:  [[0.669]
 [0.778]
 [0.709]
 [0.632]
 [0.672]
 [0.638]
 [0.641]] [[34.838]
 [34.317]
 [37.958]
 [35.325]
 [33.894]
 [36.255]
 [35.408]] [[0.669]
 [0.778]
 [0.709]
 [0.632]
 [0.672]
 [0.638]
 [0.641]]
printing an ep nov before normalisation:  47.37087366606011
Printing some Q and Qe and total Qs values:  [[0.614]
 [0.812]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]] [[30.202]
 [42.203]
 [30.202]
 [30.202]
 [30.202]
 [30.202]
 [30.202]] [[0.614]
 [0.812]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]]
actor:  0 policy actor:  1  step number:  51 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.7098866666666667 0.5866666666666668 0.5866666666666668
probs:  [0.1292336717276059, 0.1292336717276059, 0.1292336717276059, 0.35383164136197065, 0.1292336717276059, 0.1292336717276059]
maxi score, test score, baseline:  -0.7098866666666667 0.5866666666666668 0.5866666666666668
probs:  [0.1292336717276059, 0.1292336717276059, 0.1292336717276059, 0.35383164136197065, 0.1292336717276059, 0.1292336717276059]
Printing some Q and Qe and total Qs values:  [[-0.004]
 [ 0.255]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]] [[28.188]
 [52.776]
 [28.188]
 [28.188]
 [28.188]
 [28.188]
 [28.188]] [[0.273]
 [1.109]
 [0.273]
 [0.273]
 [0.273]
 [0.273]
 [0.273]]
Printing some Q and Qe and total Qs values:  [[-0.024]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.024]] [[30.388]
 [30.388]
 [30.388]
 [30.388]
 [30.388]
 [30.388]
 [30.388]] [[1.309]
 [1.309]
 [1.309]
 [1.309]
 [1.309]
 [1.309]
 [1.309]]
maxi score, test score, baseline:  -0.7098866666666667 0.5866666666666668 0.5866666666666668
probs:  [0.1292336717276059, 0.1292336717276059, 0.1292336717276059, 0.35383164136197065, 0.1292336717276059, 0.1292336717276059]
maxi score, test score, baseline:  -0.7098866666666667 0.5866666666666668 0.5866666666666668
printing an ep nov before normalisation:  28.933229699515856
maxi score, test score, baseline:  -0.7098866666666667 0.5866666666666668 0.5866666666666668
probs:  [0.1292336717276059, 0.1292336717276059, 0.1292336717276059, 0.35383164136197065, 0.1292336717276059, 0.1292336717276059]
maxi score, test score, baseline:  -0.7098866666666667 0.5866666666666668 0.5866666666666668
probs:  [0.1292336717276059, 0.1292336717276059, 0.1292336717276059, 0.35383164136197065, 0.1292336717276059, 0.1292336717276059]
maxi score, test score, baseline:  -0.7098866666666667 0.5866666666666668 0.5866666666666668
probs:  [0.12932425165800132, 0.12932425165800132, 0.12932425165800132, 0.3533787417099933, 0.12932425165800132, 0.12932425165800132]
printing an ep nov before normalisation:  75.86549098041968
maxi score, test score, baseline:  -0.7098866666666667 0.5866666666666668 0.5866666666666668
probs:  [0.12932425165800132, 0.12932425165800132, 0.12932425165800132, 0.3533787417099933, 0.12932425165800132, 0.12932425165800132]
maxi score, test score, baseline:  -0.7098866666666667 0.5866666666666668 0.5866666666666668
probs:  [0.12932425165800132, 0.12932425165800132, 0.12932425165800132, 0.3533787417099933, 0.12932425165800132, 0.12932425165800132]
maxi score, test score, baseline:  -0.7098866666666667 0.5866666666666668 0.5866666666666668
maxi score, test score, baseline:  -0.7098866666666667 0.5866666666666668 0.5866666666666668
probs:  [0.1294143942780485, 0.1294143942780485, 0.1294143942780485, 0.35292802860975764, 0.1294143942780485, 0.1294143942780485]
maxi score, test score, baseline:  -0.7098866666666667 0.5866666666666668 0.5866666666666668
probs:  [0.1294143942780485, 0.1294143942780485, 0.1294143942780485, 0.35292802860975764, 0.1294143942780485, 0.1294143942780485]
siam score:  -0.84132737
maxi score, test score, baseline:  -0.7098866666666667 0.5866666666666668 0.5866666666666668
probs:  [0.1294143942780485, 0.1294143942780485, 0.1294143942780485, 0.35292802860975764, 0.1294143942780485, 0.1294143942780485]
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.90701266291654
printing an ep nov before normalisation:  47.73633702479123
Printing some Q and Qe and total Qs values:  [[0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]] [[43.3]
 [43.3]
 [43.3]
 [43.3]
 [43.3]
 [43.3]
 [43.3]] [[1.587]
 [1.587]
 [1.587]
 [1.587]
 [1.587]
 [1.587]
 [1.587]]
printing an ep nov before normalisation:  39.86627841373336
maxi score, test score, baseline:  -0.7098866666666667 0.5866666666666668 0.5866666666666668
probs:  [0.12959338019396446, 0.12959338019396446, 0.12959338019396446, 0.3520330990301778, 0.12959338019396446, 0.12959338019396446]
maxi score, test score, baseline:  -0.7098866666666667 0.5866666666666668 0.5866666666666668
probs:  [0.12959338019396446, 0.12959338019396446, 0.12959338019396446, 0.3520330990301778, 0.12959338019396446, 0.12959338019396446]
actor:  1 policy actor:  1  step number:  65 total reward:  0.21333333333333238  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.7098866666666667 0.5866666666666668 0.5866666666666668
probs:  [0.12959338019396446, 0.12959338019396446, 0.12959338019396446, 0.3520330990301778, 0.12959338019396446, 0.12959338019396446]
maxi score, test score, baseline:  -0.7098866666666667 0.5866666666666668 0.5866666666666668
probs:  [0.12959338019396446, 0.12959338019396446, 0.12959338019396446, 0.3520330990301778, 0.12959338019396446, 0.12959338019396446]
maxi score, test score, baseline:  -0.7098866666666667 0.5866666666666668 0.5866666666666668
probs:  [0.12959338019396446, 0.12959338019396446, 0.12959338019396446, 0.3520330990301778, 0.12959338019396446, 0.12959338019396446]
actor:  1 policy actor:  1  step number:  59 total reward:  0.13333333333333264  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.145]
 [-0.145]
 [-0.166]
 [-0.14 ]
 [-0.144]
 [-0.2  ]
 [-0.144]] [[47.867]
 [49.253]
 [51.217]
 [48.394]
 [48.773]
 [49.386]
 [51.158]] [[1.38 ]
 [1.457]
 [1.546]
 [1.414]
 [1.432]
 [1.409]
 [1.564]]
maxi score, test score, baseline:  -0.7098866666666667 0.5866666666666668 0.5866666666666668
probs:  [0.1296822297177326, 0.1296822297177326, 0.1296822297177326, 0.3515888514113371, 0.1296822297177326, 0.1296822297177326]
maxi score, test score, baseline:  -0.7098866666666667 0.5866666666666668 0.5866666666666668
probs:  [0.1296822297177326, 0.1296822297177326, 0.1296822297177326, 0.3515888514113371, 0.1296822297177326, 0.1296822297177326]
maxi score, test score, baseline:  -0.7098866666666667 0.5866666666666668 0.5866666666666668
probs:  [0.1296822297177326, 0.1296822297177326, 0.1296822297177326, 0.3515888514113371, 0.1296822297177326, 0.1296822297177326]
maxi score, test score, baseline:  -0.7098866666666667 0.5866666666666668 0.5866666666666668
probs:  [0.1296822297177326, 0.1296822297177326, 0.1296822297177326, 0.3515888514113371, 0.1296822297177326, 0.1296822297177326]
printing an ep nov before normalisation:  38.68342107596907
Printing some Q and Qe and total Qs values:  [[0.14 ]
 [0.176]
 [0.14 ]
 [0.14 ]
 [0.14 ]
 [0.14 ]
 [0.14 ]] [[27.942]
 [30.539]
 [27.942]
 [27.942]
 [27.942]
 [27.942]
 [27.942]] [[0.969]
 [1.155]
 [0.969]
 [0.969]
 [0.969]
 [0.969]
 [0.969]]
printing an ep nov before normalisation:  25.3173376012827
line 256 mcts: sample exp_bonus 27.25512268175782
actor:  0 policy actor:  1  step number:  57 total reward:  0.21333333333333304  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  69 total reward:  0.07999999999999952  reward:  1.0 rdn_beta:  2.0
using another actor
maxi score, test score, baseline:  -0.7074600000000001 0.5866666666666668 0.5866666666666668
maxi score, test score, baseline:  -0.7074600000000001 0.5866666666666668 0.5866666666666668
maxi score, test score, baseline:  -0.7074600000000001 0.5866666666666668 0.5866666666666668
probs:  [0.1296822297177326, 0.1296822297177326, 0.1296822297177326, 0.3515888514113371, 0.1296822297177326, 0.1296822297177326]
maxi score, test score, baseline:  -0.7074600000000001 0.5866666666666668 0.5866666666666668
probs:  [0.1296822297177326, 0.1296822297177326, 0.1296822297177326, 0.3515888514113371, 0.1296822297177326, 0.1296822297177326]
maxi score, test score, baseline:  -0.7074600000000001 0.5866666666666668 0.5866666666666668
probs:  [0.1296822297177326, 0.1296822297177326, 0.1296822297177326, 0.3515888514113371, 0.1296822297177326, 0.1296822297177326]
maxi score, test score, baseline:  -0.7074600000000001 0.5866666666666668 0.5866666666666668
probs:  [0.1296822297177326, 0.1296822297177326, 0.1296822297177326, 0.3515888514113371, 0.1296822297177326, 0.1296822297177326]
Printing some Q and Qe and total Qs values:  [[0.727]
 [0.726]
 [0.726]
 [0.764]
 [0.698]
 [0.69 ]
 [0.723]] [[25.34 ]
 [17.665]
 [25.853]
 [18.859]
 [26.811]
 [26.092]
 [19.874]] [[0.727]
 [0.726]
 [0.726]
 [0.764]
 [0.698]
 [0.69 ]
 [0.723]]
maxi score, test score, baseline:  -0.7074600000000001 0.5866666666666668 0.5866666666666668
probs:  [0.1296822297177326, 0.1296822297177326, 0.1296822297177326, 0.3515888514113371, 0.1296822297177326, 0.1296822297177326]
printing an ep nov before normalisation:  26.17356300354004
actor:  0 policy actor:  0  step number:  38 total reward:  0.5266666666666667  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.7044066666666666 0.5866666666666668 0.5866666666666668
probs:  [0.1296822297177326, 0.1296822297177326, 0.1296822297177326, 0.3515888514113371, 0.1296822297177326, 0.1296822297177326]
printing an ep nov before normalisation:  41.39421215240168
maxi score, test score, baseline:  -0.7044066666666666 0.5866666666666668 0.5866666666666668
probs:  [0.1296822297177326, 0.1296822297177326, 0.1296822297177326, 0.3515888514113371, 0.1296822297177326, 0.1296822297177326]
printing an ep nov before normalisation:  27.680878550823614
actor:  0 policy actor:  0  step number:  54 total reward:  0.15333333333333277  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
maxi score, test score, baseline:  -0.7021000000000001 0.5866666666666668 0.5866666666666668
probs:  [0.1296822297177326, 0.1296822297177326, 0.1296822297177326, 0.3515888514113371, 0.1296822297177326, 0.1296822297177326]
maxi score, test score, baseline:  -0.7021000000000001 0.5866666666666668 0.5866666666666668
probs:  [0.1296822297177326, 0.1296822297177326, 0.1296822297177326, 0.3515888514113371, 0.1296822297177326, 0.1296822297177326]
Printing some Q and Qe and total Qs values:  [[-0.026]
 [-0.026]
 [-0.003]
 [ 0.013]
 [-0.026]
 [-0.026]
 [-0.026]] [[32.236]
 [32.236]
 [35.554]
 [45.204]
 [32.236]
 [32.236]
 [32.236]] [[0.712]
 [0.712]
 [0.815]
 [1.062]
 [0.712]
 [0.712]
 [0.712]]
printing an ep nov before normalisation:  43.92859457493909
maxi score, test score, baseline:  -0.7021000000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12977065438767243, 0.12977065438767243, 0.12977065438767243, 0.351146728061638, 0.12977065438767243, 0.12977065438767243]
maxi score, test score, baseline:  -0.7021000000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12977065438767243, 0.12977065438767243, 0.12977065438767243, 0.351146728061638, 0.12977065438767243, 0.12977065438767243]
actions average: 
K:  0  action  0 :  tensor([0.3530, 0.0123, 0.1246, 0.1287, 0.1313, 0.1225, 0.1276],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0062, 0.9739, 0.0021, 0.0069, 0.0010, 0.0010, 0.0088],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1443, 0.0652, 0.2750, 0.1020, 0.1114, 0.1013, 0.2008],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1127, 0.0120, 0.1000, 0.4373, 0.1171, 0.1129, 0.1080],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2590, 0.0058, 0.1323, 0.1262, 0.2382, 0.1193, 0.1191],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1452, 0.0521, 0.1675, 0.1204, 0.1127, 0.2643, 0.1377],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1974, 0.0129, 0.1037, 0.1098, 0.1067, 0.1082, 0.3612],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  49.982237910312364
siam score:  -0.84005094
maxi score, test score, baseline:  -0.7021000000000001 0.5866666666666668 0.5866666666666668
probs:  [0.1298586572438157, 0.1298586572438157, 0.1298586572438157, 0.35070671378092155, 0.1298586572438157, 0.1298586572438157]
printing an ep nov before normalisation:  13.910660336775885
maxi score, test score, baseline:  -0.7021000000000001 0.5866666666666668 0.5866666666666668
probs:  [0.1298586572438157, 0.1298586572438157, 0.1298586572438157, 0.35070671378092155, 0.1298586572438157, 0.1298586572438157]
maxi score, test score, baseline:  -0.7021000000000001 0.5866666666666668 0.5866666666666668
probs:  [0.1298586572438157, 0.1298586572438157, 0.1298586572438157, 0.35070671378092155, 0.1298586572438157, 0.1298586572438157]
printing an ep nov before normalisation:  40.94044262712309
Printing some Q and Qe and total Qs values:  [[0.677]
 [0.765]
 [0.645]
 [0.653]
 [0.69 ]
 [0.512]
 [0.535]] [[29.155]
 [22.252]
 [21.42 ]
 [27.108]
 [23.286]
 [24.664]
 [24.382]] [[1.757]
 [1.589]
 [1.439]
 [1.658]
 [1.553]
 [1.426]
 [1.439]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  51 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  33.60347602534417
maxi score, test score, baseline:  -0.7021000000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12994624129725693, 0.12994624129725693, 0.12994624129725693, 0.3502687935137153, 0.12994624129725693, 0.12994624129725693]
printing an ep nov before normalisation:  36.97863189481154
printing an ep nov before normalisation:  0.0046917418302427905
Printing some Q and Qe and total Qs values:  [[-0.179]
 [-0.155]
 [-0.179]
 [-0.137]
 [-0.179]
 [-0.179]
 [-0.179]] [[32.039]
 [50.295]
 [32.039]
 [42.778]
 [32.039]
 [32.039]
 [32.039]] [[0.399]
 [0.979]
 [0.399]
 [0.769]
 [0.399]
 [0.399]
 [0.399]]
printing an ep nov before normalisation:  14.097571813579663
printing an ep nov before normalisation:  12.392014291976416
actor:  1 policy actor:  1  step number:  55 total reward:  0.45333333333333337  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.7021000000000001 0.5866666666666668 0.5866666666666668
probs:  [0.12994624129725693, 0.12994624129725693, 0.12994624129725693, 0.3502687935137153, 0.12994624129725693, 0.12994624129725693]
printing an ep nov before normalisation:  29.94690993173611
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.65 ]
 [0.568]
 [0.564]
 [0.51 ]
 [0.579]
 [0.521]] [[16.784]
 [14.186]
 [16.078]
 [15.641]
 [16.518]
 [15.008]
 [16.492]] [[1.496]
 [1.482]
 [1.511]
 [1.481]
 [1.479]
 [1.459]
 [1.488]]
actor:  1 policy actor:  1  step number:  63 total reward:  0.3333333333333327  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.7021000000000001 0.5866666666666668 0.5866666666666668
printing an ep nov before normalisation:  37.73502769221229
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7021000000000001 0.5866666666666668 0.5866666666666668
probs:  [0.13003340953050502, 0.13003340953050502, 0.13003340953050502, 0.3498329523474751, 0.13003340953050502, 0.13003340953050502]
actions average: 
K:  2  action  0 :  tensor([0.3260, 0.0587, 0.1057, 0.1232, 0.1666, 0.1008, 0.1191],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0091, 0.9293, 0.0069, 0.0107, 0.0040, 0.0042, 0.0358],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1644, 0.1225, 0.2376, 0.0853, 0.0976, 0.1531, 0.1395],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1585, 0.1074, 0.1164, 0.1816, 0.1257, 0.2078, 0.1028],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0940, 0.0286, 0.1244, 0.1223, 0.3754, 0.1309, 0.1244],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1364, 0.0168, 0.1460, 0.1307, 0.1392, 0.3080, 0.1229],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1498, 0.1000, 0.0980, 0.1166, 0.0993, 0.0916, 0.3447],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  28.4049129486084
actor:  1 policy actor:  1  step number:  69 total reward:  0.11999999999999955  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.7021000000000001 0.5866666666666668 0.5866666666666668
probs:  [0.13003340953050502, 0.13003340953050502, 0.13003340953050502, 0.3498329523474751, 0.13003340953050502, 0.13003340953050502]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.7021000000000001 0.5866666666666668 0.5866666666666668
probs:  [0.13003340953050502, 0.13003340953050502, 0.13003340953050502, 0.3498329523474751, 0.13003340953050502, 0.13003340953050502]
maxi score, test score, baseline:  -0.7021000000000001 0.5866666666666668 0.5866666666666668
maxi score, test score, baseline:  -0.7021000000000001 0.5866666666666668 0.5866666666666668
probs:  [0.13003340953050502, 0.13003340953050502, 0.13003340953050502, 0.3498329523474751, 0.13003340953050502, 0.13003340953050502]
actor:  0 policy actor:  1  step number:  39 total reward:  0.6000000000000002  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.6989000000000001 0.5866666666666668 0.5866666666666668
probs:  [0.13003340953050502, 0.13003340953050502, 0.13003340953050502, 0.3498329523474751, 0.13003340953050502, 0.13003340953050502]
printing an ep nov before normalisation:  22.118172645568848
maxi score, test score, baseline:  -0.6989000000000001 0.5866666666666668 0.5866666666666668
probs:  [0.13003340953050502, 0.13003340953050502, 0.13003340953050502, 0.3498329523474751, 0.13003340953050502, 0.13003340953050502]
maxi score, test score, baseline:  -0.6989000000000001 0.5866666666666668 0.5866666666666668
actor:  1 policy actor:  1  step number:  70 total reward:  0.046666666666665635  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  35.48907703823513
maxi score, test score, baseline:  -0.6989000000000001 0.5866666666666668 0.5866666666666668
probs:  [0.13003340953050502, 0.13003340953050502, 0.13003340953050502, 0.3498329523474751, 0.13003340953050502, 0.13003340953050502]
actor:  1 policy actor:  1  step number:  47 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  2.0
from probs:  [0.13003340953050502, 0.13003340953050502, 0.13003340953050502, 0.3498329523474751, 0.13003340953050502, 0.13003340953050502]
maxi score, test score, baseline:  -0.6989000000000001 0.5866666666666668 0.5866666666666668
probs:  [0.13003340953050502, 0.13003340953050502, 0.13003340953050502, 0.3498329523474751, 0.13003340953050502, 0.13003340953050502]
printing an ep nov before normalisation:  35.223236083984375
maxi score, test score, baseline:  -0.6989000000000001 0.5866666666666668 0.5866666666666668
probs:  [0.13003340953050502, 0.13003340953050502, 0.13003340953050502, 0.3498329523474751, 0.13003340953050502, 0.13003340953050502]
maxi score, test score, baseline:  -0.6989 0.5866666666666668 0.5866666666666668
probs:  [0.13012016489781328, 0.13012016489781328, 0.13012016489781328, 0.3493991755109335, 0.13012016489781328, 0.13012016489781328]
printing an ep nov before normalisation:  63.43432545735743
maxi score, test score, baseline:  -0.6989 0.5866666666666668 0.5866666666666668
probs:  [0.13012016489781328, 0.13012016489781328, 0.13012016489781328, 0.3493991755109335, 0.13012016489781328, 0.13012016489781328]
printing an ep nov before normalisation:  0.000151437831164003
actor:  1 policy actor:  1  step number:  40 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  0.00024016110955926706
printing an ep nov before normalisation:  42.72567005976693
maxi score, test score, baseline:  -0.6989 0.5866666666666668 0.5866666666666668
probs:  [0.13012016489781328, 0.13012016489781328, 0.13012016489781328, 0.3493991755109335, 0.13012016489781328, 0.13012016489781328]
maxi score, test score, baseline:  -0.6989 0.5866666666666668 0.5866666666666668
probs:  [0.13012016489781328, 0.13012016489781328, 0.13012016489781328, 0.3493991755109335, 0.13012016489781328, 0.13012016489781328]
from probs:  [0.13012016489781328, 0.13012016489781328, 0.13012016489781328, 0.3493991755109335, 0.13012016489781328, 0.13012016489781328]
maxi score, test score, baseline:  -0.6989 0.5866666666666668 0.5866666666666668
probs:  [0.13012016489781328, 0.13012016489781328, 0.13012016489781328, 0.3493991755109335, 0.13012016489781328, 0.13012016489781328]
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]] [[35.253]
 [35.253]
 [35.253]
 [35.253]
 [35.253]
 [35.253]
 [35.253]] [[0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]]
maxi score, test score, baseline:  -0.6989 0.5866666666666668 0.5866666666666668
probs:  [0.13012016489781328, 0.13012016489781328, 0.13012016489781328, 0.3493991755109335, 0.13012016489781328, 0.13012016489781328]
maxi score, test score, baseline:  -0.6989 0.5866666666666668 0.5866666666666668
probs:  [0.13012016489781328, 0.13012016489781328, 0.13012016489781328, 0.3493991755109335, 0.13012016489781328, 0.13012016489781328]
maxi score, test score, baseline:  -0.6989 0.5866666666666668 0.5866666666666668
probs:  [0.13012016489781328, 0.13012016489781328, 0.13012016489781328, 0.3493991755109335, 0.13012016489781328, 0.13012016489781328]
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.595]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]] [[22.879]
 [29.431]
 [22.879]
 [22.879]
 [22.879]
 [22.879]
 [22.879]] [[0.561]
 [0.595]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]]
maxi score, test score, baseline:  -0.6989 0.5866666666666668 0.5866666666666668
probs:  [0.13012016489781328, 0.13012016489781328, 0.13012016489781328, 0.3493991755109335, 0.13012016489781328, 0.13012016489781328]
printing an ep nov before normalisation:  44.68169285084976
maxi score, test score, baseline:  -0.6989 0.5866666666666668 0.5866666666666668
probs:  [0.13012016489781328, 0.13012016489781328, 0.13012016489781328, 0.3493991755109335, 0.13012016489781328, 0.13012016489781328]
maxi score, test score, baseline:  -0.6989 0.5866666666666668 0.5866666666666668
actor:  0 policy actor:  0  step number:  56 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  30.33576011657715
maxi score, test score, baseline:  -0.6965400000000002 0.5866666666666668 0.5866666666666668
probs:  [0.13012016489781328, 0.13012016489781328, 0.13012016489781328, 0.3493991755109335, 0.13012016489781328, 0.13012016489781328]
maxi score, test score, baseline:  -0.6965400000000002 0.5866666666666668 0.5866666666666668
probs:  [0.13012016489781328, 0.13012016489781328, 0.13012016489781328, 0.3493991755109335, 0.13012016489781328, 0.13012016489781328]
maxi score, test score, baseline:  -0.6965400000000002 0.5866666666666668 0.5866666666666668
probs:  [0.13012016489781328, 0.13012016489781328, 0.13012016489781328, 0.3493991755109335, 0.13012016489781328, 0.13012016489781328]
printing an ep nov before normalisation:  34.68852291316102
printing an ep nov before normalisation:  25.850413187493082
maxi score, test score, baseline:  -0.6965400000000002 0.5866666666666668 0.5866666666666668
probs:  [0.13012016489781328, 0.13012016489781328, 0.13012016489781328, 0.3493991755109335, 0.13012016489781328, 0.13012016489781328]
printing an ep nov before normalisation:  24.692299456839116
actor:  1 policy actor:  1  step number:  48 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6965400000000002 0.5866666666666668 0.5866666666666668
probs:  [0.13012016489781328, 0.13012016489781328, 0.13012016489781328, 0.3493991755109335, 0.13012016489781328, 0.13012016489781328]
maxi score, test score, baseline:  -0.6965400000000002 0.5866666666666668 0.5866666666666668
maxi score, test score, baseline:  -0.6965400000000002 0.5866666666666668 0.5866666666666668
probs:  [0.13012016489781328, 0.13012016489781328, 0.13012016489781328, 0.3493991755109335, 0.13012016489781328, 0.13012016489781328]
printing an ep nov before normalisation:  48.12754153659668
printing an ep nov before normalisation:  34.39457057862575
actions average: 
K:  1  action  0 :  tensor([0.3368, 0.0788, 0.0964, 0.1276, 0.1327, 0.1152, 0.1126],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0077, 0.9289, 0.0063, 0.0106, 0.0051, 0.0042, 0.0372],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1208, 0.0510, 0.4781, 0.0804, 0.0801, 0.0977, 0.0920],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1494, 0.0761, 0.0893, 0.3106, 0.1480, 0.1082, 0.1184],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1523, 0.0700, 0.0788, 0.1159, 0.3550, 0.1182, 0.1098],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0881, 0.0089, 0.1763, 0.0974, 0.0974, 0.4382, 0.0936],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1420, 0.1197, 0.1126, 0.1323, 0.1201, 0.1210, 0.2523],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  40.93664438923206
maxi score, test score, baseline:  -0.6965400000000002 0.5866666666666668 0.5866666666666668
probs:  [0.1302065103255133, 0.1302065103255133, 0.1302065103255133, 0.3489674483724334, 0.1302065103255133, 0.1302065103255133]
printing an ep nov before normalisation:  38.90268802642822
maxi score, test score, baseline:  -0.6965400000000002 0.5866666666666668 0.5866666666666668
probs:  [0.1302065103255133, 0.1302065103255133, 0.1302065103255133, 0.3489674483724334, 0.1302065103255133, 0.1302065103255133]
siam score:  -0.8455263
maxi score, test score, baseline:  -0.6965400000000002 0.5866666666666668 0.5866666666666668
probs:  [0.1302065103255133, 0.1302065103255133, 0.1302065103255133, 0.3489674483724334, 0.1302065103255133, 0.1302065103255133]
maxi score, test score, baseline:  -0.6965400000000002 0.5866666666666668 0.5866666666666668
actor:  1 policy actor:  1  step number:  58 total reward:  0.3133333333333327  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6965400000000002 0.5866666666666668 0.5866666666666668
using explorer policy with actor:  1
printing an ep nov before normalisation:  25.232629863751335
maxi score, test score, baseline:  -0.6965400000000002 0.5866666666666668 0.5866666666666668
probs:  [0.1302065103255133, 0.1302065103255133, 0.1302065103255133, 0.3489674483724334, 0.1302065103255133, 0.1302065103255133]
printing an ep nov before normalisation:  40.560046573075006
maxi score, test score, baseline:  -0.6965400000000002 0.5866666666666668 0.5866666666666668
probs:  [0.13029244871234896, 0.13029244871234896, 0.13029244871234896, 0.34853775643825535, 0.13029244871234896, 0.13029244871234896]
maxi score, test score, baseline:  -0.6965400000000002 0.5866666666666668 0.5866666666666668
probs:  [0.13029244871234896, 0.13029244871234896, 0.13029244871234896, 0.34853775643825535, 0.13029244871234896, 0.13029244871234896]
Printing some Q and Qe and total Qs values:  [[-0.003]
 [-0.026]
 [ 0.229]
 [ 0.229]
 [ 0.229]
 [ 0.229]
 [ 0.003]] [[40.918]
 [39.837]
 [37.732]
 [37.732]
 [37.732]
 [37.732]
 [40.172]] [[1.88 ]
 [1.767]
 [1.847]
 [1.847]
 [1.847]
 [1.847]
 [1.823]]
maxi score, test score, baseline:  -0.6965400000000002 0.5866666666666668 0.5866666666666668
probs:  [0.13029244871234896, 0.13029244871234896, 0.13029244871234896, 0.34853775643825535, 0.13029244871234896, 0.13029244871234896]
maxi score, test score, baseline:  -0.6965400000000002 0.5866666666666668 0.5866666666666668
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.542]
 [0.498]
 [0.457]
 [0.393]
 [0.379]
 [0.387]] [[41.468]
 [44.906]
 [42.863]
 [41.468]
 [35.083]
 [35.133]
 [33.429]] [[0.457]
 [0.542]
 [0.498]
 [0.457]
 [0.393]
 [0.379]
 [0.387]]
printing an ep nov before normalisation:  45.40710674786251
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.96446864414042
actor:  0 policy actor:  1  step number:  46 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.6937266666666667 0.5866666666666668 0.5866666666666668
maxi score, test score, baseline:  -0.6937266666666667 0.5866666666666668 0.5866666666666668
probs:  [0.13029244871234896, 0.13029244871234896, 0.13029244871234896, 0.34853775643825535, 0.13029244871234896, 0.13029244871234896]
printing an ep nov before normalisation:  54.4047448938224
Starting evaluation
maxi score, test score, baseline:  -0.6937266666666667 0.5866666666666668 0.5866666666666668
probs:  [0.13029244871234896, 0.13029244871234896, 0.13029244871234896, 0.34853775643825535, 0.13029244871234896, 0.13029244871234896]
maxi score, test score, baseline:  -0.6937266666666667 0.5866666666666668 0.5866666666666668
Printing some Q and Qe and total Qs values:  [[0.563]
 [0.673]
 [0.55 ]
 [0.531]
 [0.524]
 [0.517]
 [0.534]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.563]
 [0.673]
 [0.55 ]
 [0.531]
 [0.524]
 [0.517]
 [0.534]]
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.712]
 [0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.552]
 [0.712]
 [0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]]
printing an ep nov before normalisation:  49.824533200286915
printing an ep nov before normalisation:  45.13181746030252
printing an ep nov before normalisation:  40.397766168196604
printing an ep nov before normalisation:  49.50566486185282
printing an ep nov before normalisation:  40.264347311226636
printing an ep nov before normalisation:  41.19569957151429
printing an ep nov before normalisation:  54.533419609069824
Printing some Q and Qe and total Qs values:  [[0.761]
 [0.788]
 [0.761]
 [0.765]
 [0.761]
 [0.761]
 [0.761]] [[33.549]
 [39.037]
 [33.549]
 [39.714]
 [33.549]
 [33.549]
 [33.549]] [[0.761]
 [0.788]
 [0.761]
 [0.765]
 [0.761]
 [0.761]
 [0.761]]
maxi score, test score, baseline:  -0.6937266666666667 0.5866666666666668 0.5866666666666668
probs:  [0.13029244871234896, 0.13029244871234896, 0.13029244871234896, 0.34853775643825535, 0.13029244871234896, 0.13029244871234896]
Printing some Q and Qe and total Qs values:  [[0.631]
 [0.809]
 [0.694]
 [0.726]
 [0.691]
 [0.695]
 [0.699]] [[35.475]
 [30.875]
 [36.614]
 [37.94 ]
 [34.147]
 [34.918]
 [34.2  ]] [[0.631]
 [0.809]
 [0.694]
 [0.726]
 [0.691]
 [0.695]
 [0.699]]
printing an ep nov before normalisation:  31.687581539154053
maxi score, test score, baseline:  -0.6937266666666667 0.5866666666666668 0.5866666666666668
probs:  [0.13037798292980188, 0.13037798292980188, 0.13037798292980188, 0.3481100853509908, 0.13037798292980188, 0.13037798292980188]
Printing some Q and Qe and total Qs values:  [[0.968]
 [0.968]
 [0.968]
 [0.968]
 [0.968]
 [0.968]
 [0.968]] [[27.396]
 [27.396]
 [27.396]
 [27.396]
 [27.396]
 [27.396]
 [27.396]] [[0.968]
 [0.968]
 [0.968]
 [0.968]
 [0.968]
 [0.968]
 [0.968]]
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  30.651355584632217
printing an ep nov before normalisation:  32.946839332580566
printing an ep nov before normalisation:  39.996739795472976
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[-0.116]
 [-0.117]
 [-0.117]
 [-0.117]
 [-0.116]
 [-0.116]
 [-0.117]] [[36.321]
 [33.47 ]
 [37.119]
 [37.119]
 [36.974]
 [37.508]
 [37.119]] [[1.143]
 [0.966]
 [1.192]
 [1.192]
 [1.184]
 [1.217]
 [1.192]]
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  28 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  2
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  58 total reward:  0.2999999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.6263933333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85152453
maxi score, test score, baseline:  -0.6263933333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6263933333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6263933333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  0.239999999999999  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  45.876087335605256
maxi score, test score, baseline:  -0.6263933333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6263933333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.27393032891751
maxi score, test score, baseline:  -0.6263933333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.24870538711548
Printing some Q and Qe and total Qs values:  [[-0.127]
 [-0.099]
 [-0.127]
 [-0.127]
 [-0.127]
 [-0.127]
 [-0.127]] [[18.136]
 [37.097]
 [18.136]
 [18.136]
 [18.136]
 [18.136]
 [18.136]] [[0.022]
 [0.353]
 [0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]]
maxi score, test score, baseline:  -0.6263933333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8463516
actor:  0 policy actor:  0  step number:  54 total reward:  0.16666666666666607  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6240600000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6240600000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.122]
 [-0.1  ]
 [-0.122]
 [-0.122]
 [-0.122]
 [-0.122]
 [-0.122]] [[53.835]
 [53.324]
 [53.835]
 [53.835]
 [53.835]
 [53.835]
 [53.835]] [[0.8  ]
 [0.809]
 [0.8  ]
 [0.8  ]
 [0.8  ]
 [0.8  ]
 [0.8  ]]
maxi score, test score, baseline:  -0.6240600000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6240600000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.57856304674816
Printing some Q and Qe and total Qs values:  [[0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]] [[36.154]
 [36.154]
 [36.154]
 [36.154]
 [36.154]
 [36.154]
 [36.154]] [[1.881]
 [1.881]
 [1.881]
 [1.881]
 [1.881]
 [1.881]
 [1.881]]
maxi score, test score, baseline:  -0.6240600000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.2599999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.6240600000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.06390521674737
printing an ep nov before normalisation:  29.8976932398415
printing an ep nov before normalisation:  27.875216007232666
maxi score, test score, baseline:  -0.6240600000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6240600000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.11178092986631
actor:  1 policy actor:  1  step number:  62 total reward:  0.03333333333333277  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.6240600000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.14097488334433
siam score:  -0.8442013
maxi score, test score, baseline:  -0.6240600000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.446]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]] [[48.936]
 [42.52 ]
 [48.463]
 [48.463]
 [48.463]
 [48.463]
 [48.463]] [[1.972]
 [1.763]
 [1.963]
 [1.963]
 [1.963]
 [1.963]
 [1.963]]
printing an ep nov before normalisation:  45.78496961782176
printing an ep nov before normalisation:  36.423914325294426
maxi score, test score, baseline:  -0.6240600000000001 0.6833333333333336 0.6833333333333336
maxi score, test score, baseline:  -0.6240600000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  1.667
siam score:  -0.84299713
maxi score, test score, baseline:  -0.6240600000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6240600000000001 0.6833333333333336 0.6833333333333336
maxi score, test score, baseline:  -0.6240600000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.123476793689335
siam score:  -0.8448828
maxi score, test score, baseline:  -0.6240600000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 41.616103155486606
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]] [[35.63]
 [35.63]
 [35.63]
 [35.63]
 [35.63]
 [35.63]
 [35.63]] [[0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6240600000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6240600000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.174545926474615
actor:  0 policy actor:  0  step number:  52 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  45 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.6186866666666666 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85434395
maxi score, test score, baseline:  -0.6186866666666666 0.6833333333333336 0.6833333333333336
maxi score, test score, baseline:  -0.6186866666666666 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6186866666666666 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.094474825409513
maxi score, test score, baseline:  -0.6186866666666666 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.785232341568353
actor:  1 policy actor:  1  step number:  49 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  51 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6186866666666666 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.115]
 [-0.118]
 [-0.115]
 [-0.115]
 [-0.115]
 [-0.125]
 [-0.115]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.115]
 [-0.118]
 [-0.115]
 [-0.115]
 [-0.115]
 [-0.125]
 [-0.115]]
maxi score, test score, baseline:  -0.6186866666666666 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6186866666666666 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.06478894823527
maxi score, test score, baseline:  -0.6186866666666666 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6186866666666666 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.070149421691895
actions average: 
K:  1  action  0 :  tensor([0.2748, 0.0169, 0.1100, 0.1559, 0.1807, 0.1306, 0.1312],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0050, 0.9492, 0.0046, 0.0194, 0.0049, 0.0044, 0.0126],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0498, 0.0046, 0.6583, 0.0431, 0.0493, 0.1444, 0.0506],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1227, 0.1420, 0.0891, 0.3042, 0.1263, 0.1060, 0.1096],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1548, 0.0061, 0.1228, 0.1571, 0.2450, 0.1691, 0.1452],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1352, 0.0040, 0.1657, 0.1410, 0.1194, 0.2821, 0.1526],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1127, 0.1578, 0.0865, 0.1278, 0.1085, 0.1116, 0.2951],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.6186866666666666 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.38453892398732
line 256 mcts: sample exp_bonus 13.508296741510904
actions average: 
K:  1  action  0 :  tensor([0.3668, 0.0065, 0.1027, 0.1293, 0.1508, 0.1273, 0.1166],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0033, 0.9670, 0.0026, 0.0086, 0.0012, 0.0012, 0.0161],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1494, 0.0109, 0.2419, 0.1779, 0.1538, 0.1359, 0.1301],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1199, 0.0779, 0.0888, 0.3294, 0.1654, 0.1046, 0.1140],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1202, 0.0812, 0.1030, 0.1656, 0.3010, 0.1151, 0.1138],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1280, 0.0169, 0.1639, 0.1761, 0.1476, 0.2189, 0.1486],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1211, 0.1298, 0.1001, 0.1179, 0.1098, 0.1111, 0.3103],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.6186866666666666 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.450766563415527
actor:  0 policy actor:  0  step number:  62 total reward:  0.046666666666665635  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.6165933333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6165933333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8631284
maxi score, test score, baseline:  -0.6165933333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[ 1.5  ]
 [ 1.5  ]
 [ 1.5  ]
 [ 1.5  ]
 [ 1.5  ]
 [-0.135]
 [-0.134]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 1.5  ]
 [ 1.5  ]
 [ 1.5  ]
 [ 1.5  ]
 [ 1.5  ]
 [-0.135]
 [-0.134]]
printing an ep nov before normalisation:  31.489312444871203
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.421]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]] [[40.623]
 [36.028]
 [40.623]
 [40.623]
 [40.623]
 [40.623]
 [40.623]] [[1.667]
 [1.501]
 [1.667]
 [1.667]
 [1.667]
 [1.667]
 [1.667]]
maxi score, test score, baseline:  -0.6165933333333334 0.6833333333333336 0.6833333333333336
maxi score, test score, baseline:  -0.6165933333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86205614
printing an ep nov before normalisation:  43.00119981732391
Printing some Q and Qe and total Qs values:  [[-0.099]
 [-0.086]
 [-0.098]
 [-0.098]
 [-0.098]
 [-0.098]
 [-0.098]] [[48.491]
 [41.906]
 [43.723]
 [43.723]
 [43.723]
 [43.723]
 [43.723]] [[0.228]
 [0.167]
 [0.175]
 [0.175]
 [0.175]
 [0.175]
 [0.175]]
printing an ep nov before normalisation:  34.02294405374002
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6165933333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6165933333333334 0.6833333333333336 0.6833333333333336
printing an ep nov before normalisation:  38.47958163188336
printing an ep nov before normalisation:  17.316535878531116
maxi score, test score, baseline:  -0.6165933333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.93972463544198
maxi score, test score, baseline:  -0.6165933333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6165933333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6165933333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6165933333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6165933333333334 0.6833333333333336 0.6833333333333336
printing an ep nov before normalisation:  29.577031135559082
maxi score, test score, baseline:  -0.6165933333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.542768907874017
Printing some Q and Qe and total Qs values:  [[-0.145]
 [-0.163]
 [-0.145]
 [-0.157]
 [-0.141]
 [-0.145]
 [-0.142]] [[17.131]
 [22.63 ]
 [17.131]
 [19.011]
 [12.715]
 [17.131]
 [12.709]] [[ 0.065]
 [ 0.152]
 [ 0.065]
 [ 0.089]
 [-0.016]
 [ 0.065]
 [-0.018]]
printing an ep nov before normalisation:  24.753618794183343
maxi score, test score, baseline:  -0.6165933333333334 0.6833333333333336 0.6833333333333336
Printing some Q and Qe and total Qs values:  [[-0.043]
 [-0.02 ]
 [-0.059]
 [-0.059]
 [-0.064]
 [-0.079]
 [-0.059]] [[37.472]
 [31.486]
 [43.894]
 [43.894]
 [43.468]
 [46.131]
 [43.894]] [[0.901]
 [0.714]
 [1.111]
 [1.111]
 [1.091]
 [1.169]
 [1.111]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.026]
 [ 0.051]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]
 [-0.026]] [[34.869]
 [34.149]
 [34.869]
 [34.869]
 [34.869]
 [34.869]
 [34.869]] [[0.279]
 [0.344]
 [0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]]
printing an ep nov before normalisation:  29.510323313059533
maxi score, test score, baseline:  -0.6165933333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.70318603515625
actor:  1 policy actor:  1  step number:  58 total reward:  0.27333333333333276  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6165933333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6165933333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.37296825520069
maxi score, test score, baseline:  -0.6165933333333334 0.6833333333333336 0.6833333333333336
printing an ep nov before normalisation:  35.34546150815167
maxi score, test score, baseline:  -0.6165933333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 42.707994643655624
maxi score, test score, baseline:  -0.6165933333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  73 total reward:  0.03999999999999926  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6165933333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.14259089027074
maxi score, test score, baseline:  -0.6165933333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6165933333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6165933333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6165933333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6165933333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  59 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  41 total reward:  0.5733333333333337  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  57.14186598172805
Printing some Q and Qe and total Qs values:  [[0.302]
 [0.457]
 [0.302]
 [0.302]
 [0.437]
 [0.31 ]
 [0.327]] [[43.324]
 [46.094]
 [43.324]
 [43.324]
 [46.564]
 [46.166]
 [45.433]] [[1.177]
 [1.438]
 [1.177]
 [1.177]
 [1.436]
 [1.294]
 [1.282]]
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.311]
 [0.457]
 [0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]] [[21.337]
 [26.91 ]
 [21.337]
 [21.337]
 [21.337]
 [21.337]
 [21.337]] [[1.163]
 [1.537]
 [1.163]
 [1.163]
 [1.163]
 [1.163]
 [1.163]]
printing an ep nov before normalisation:  49.30275355285872
actor:  1 policy actor:  1  step number:  48 total reward:  0.20666666666666633  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  53 total reward:  0.22666666666666646  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.074471950531006
printing an ep nov before normalisation:  32.87061749743959
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.72067529089941
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.092200756073
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.64196884850301
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
Printing some Q and Qe and total Qs values:  [[0.673]
 [0.702]
 [0.673]
 [0.468]
 [0.441]
 [0.349]
 [0.576]] [[44.427]
 [37.095]
 [44.427]
 [45.962]
 [49.2  ]
 [50.7  ]
 [42.998]] [[1.219]
 [1.108]
 [1.219]
 [1.044]
 [1.079]
 [1.016]
 [1.095]]
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.724]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]] [[40.655]
 [38.955]
 [36.931]
 [36.931]
 [36.931]
 [36.931]
 [36.931]] [[1.248]
 [1.363]
 [1.278]
 [1.278]
 [1.278]
 [1.278]
 [1.278]]
actor:  1 policy actor:  1  step number:  64 total reward:  0.1933333333333327  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.67784496860175
printing an ep nov before normalisation:  18.252527713775635
actor:  1 policy actor:  1  step number:  58 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  33.86283874511719
using explorer policy with actor:  1
printing an ep nov before normalisation:  69.55169555494099
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
line 256 mcts: sample exp_bonus 34.55393384568234
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.33864912714807
using explorer policy with actor:  1
printing an ep nov before normalisation:  61.03936928429978
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.94361733584142
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.3188376590821
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.02  0.306 0.02  0.02  0.163 0.02  0.449]
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.224632872263456
printing an ep nov before normalisation:  32.62747583895509
printing an ep nov before normalisation:  0.00011697013064804196
actor:  1 policy actor:  1  step number:  72 total reward:  0.13999999999999913  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  44.765033034498735
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.3738, 0.0235, 0.1067, 0.1036, 0.1659, 0.1288, 0.0977],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0200, 0.9032, 0.0141, 0.0162, 0.0142, 0.0100, 0.0223],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1135, 0.1211, 0.3268, 0.0963, 0.0999, 0.1658, 0.0768],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1511, 0.0817, 0.1309, 0.2237, 0.1408, 0.1519, 0.1198],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1533, 0.0357, 0.1216, 0.1348, 0.2947, 0.1476, 0.1123],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1662, 0.0231, 0.1749, 0.1366, 0.1426, 0.2259, 0.1306],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1622, 0.2036, 0.1144, 0.1218, 0.1022, 0.1144, 0.1814],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.49580636325703
printing an ep nov before normalisation:  32.64532804489136
siam score:  -0.8667433
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.603]
 [0.638]
 [0.611]
 [0.604]
 [0.615]
 [0.598]
 [0.6  ]] [[34.507]
 [39.668]
 [34.602]
 [34.864]
 [38.517]
 [34.814]
 [34.819]] [[0.603]
 [0.638]
 [0.611]
 [0.604]
 [0.615]
 [0.598]
 [0.6  ]]
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.585]
 [0.427]
 [0.427]
 [0.427]
 [0.495]
 [0.427]] [[45.103]
 [41.419]
 [42.458]
 [42.458]
 [42.458]
 [46.082]
 [42.458]] [[2.133]
 [1.985]
 [1.887]
 [1.887]
 [1.887]
 [2.166]
 [1.887]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
Printing some Q and Qe and total Qs values:  [[0.392]
 [0.417]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]] [[34.902]
 [32.287]
 [30.433]
 [30.433]
 [30.433]
 [30.433]
 [30.433]] [[0.392]
 [0.417]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]]
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.6546368598938
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.7064231832702
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.514167610000136
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  66 total reward:  0.08666666666666567  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.102]
 [-0.086]
 [-0.113]
 [-0.107]
 [-0.107]
 [-0.101]
 [-0.103]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.102]
 [-0.086]
 [-0.113]
 [-0.107]
 [-0.107]
 [-0.101]
 [-0.103]]
maxi score, test score, baseline:  -0.6139533333333335 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.152254581451416
siam score:  -0.8675234
actor:  0 policy actor:  0  step number:  45 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6112333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6112333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.570271886020336
printing an ep nov before normalisation:  41.641661875964836
Printing some Q and Qe and total Qs values:  [[-0.152]
 [-0.166]
 [-0.152]
 [-0.152]
 [-0.152]
 [-0.152]
 [-0.152]] [[42.991]
 [47.312]
 [42.991]
 [42.991]
 [42.991]
 [42.991]
 [42.991]] [[1.199]
 [1.321]
 [1.199]
 [1.199]
 [1.199]
 [1.199]
 [1.199]]
maxi score, test score, baseline:  -0.6112333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  13.94335789373584
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6112333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.880253314971924
maxi score, test score, baseline:  -0.6112333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6112333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86674005
siam score:  -0.86650604
maxi score, test score, baseline:  -0.6112333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86469024
actor:  1 policy actor:  1  step number:  61 total reward:  0.30666666666666564  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.055]
 [-0.009]
 [-0.055]
 [-0.055]
 [-0.049]
 [-0.036]
 [-0.055]] [[45.185]
 [43.602]
 [45.185]
 [45.185]
 [48.034]
 [47.433]
 [45.185]] [[1.426]
 [1.377]
 [1.426]
 [1.426]
 [1.604]
 [1.581]
 [1.426]]
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.123]
 [0.123]
 [0.123]
 [0.303]
 [0.123]
 [0.123]] [[47.995]
 [40.506]
 [40.506]
 [40.506]
 [44.993]
 [40.506]
 [40.506]] [[1.958]
 [1.379]
 [1.379]
 [1.379]
 [1.8  ]
 [1.379]
 [1.379]]
printing an ep nov before normalisation:  49.69065285735345
maxi score, test score, baseline:  -0.6112333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.13863754272461
maxi score, test score, baseline:  -0.6112333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6112333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.104152734818925
maxi score, test score, baseline:  -0.6112333333333334 0.6833333333333336 0.6833333333333336
actor:  1 policy actor:  1  step number:  61 total reward:  0.18666666666666654  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6112333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6112333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.591364649669686
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.75222929946836
maxi score, test score, baseline:  -0.6112333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.117049618157566
maxi score, test score, baseline:  -0.6112333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6112333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  34.40585072274324
actions average: 
K:  4  action  0 :  tensor([0.2991, 0.0066, 0.1276, 0.1218, 0.1585, 0.1333, 0.1531],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0331, 0.8490, 0.0175, 0.0305, 0.0199, 0.0148, 0.0353],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1378, 0.1231, 0.2047, 0.1234, 0.1150, 0.1467, 0.1493],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1744, 0.1032, 0.1098, 0.1588, 0.1780, 0.1272, 0.1487],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1812, 0.0721, 0.0901, 0.0964, 0.3559, 0.0988, 0.1054],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1348, 0.0783, 0.1414, 0.1640, 0.1708, 0.2128, 0.0979],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1921, 0.0043, 0.1337, 0.1428, 0.2282, 0.1454, 0.1535],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  44.44449242425427
maxi score, test score, baseline:  -0.6112333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6112333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.264]
 [0.367]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]] [[24.544]
 [29.952]
 [24.544]
 [24.544]
 [24.544]
 [24.544]
 [24.544]] [[1.096]
 [1.557]
 [1.096]
 [1.096]
 [1.096]
 [1.096]
 [1.096]]
printing an ep nov before normalisation:  39.69539422529854
actor:  1 policy actor:  1  step number:  48 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[ 0.109]
 [ 0.24 ]
 [ 0.109]
 [ 0.06 ]
 [-0.009]
 [ 0.109]
 [ 0.182]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.109]
 [ 0.24 ]
 [ 0.109]
 [ 0.06 ]
 [-0.009]
 [ 0.109]
 [ 0.182]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6112333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6112333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.31543463391004
maxi score, test score, baseline:  -0.6112333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.865]
 [0.865]
 [0.865]
 [0.865]
 [0.865]
 [0.865]
 [0.865]] [[44.148]
 [44.148]
 [44.148]
 [44.148]
 [44.148]
 [44.148]
 [44.148]] [[0.865]
 [0.865]
 [0.865]
 [0.865]
 [0.865]
 [0.865]
 [0.865]]
maxi score, test score, baseline:  -0.6112333333333334 0.6833333333333336 0.6833333333333336
maxi score, test score, baseline:  -0.6112333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6112333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  50 total reward:  0.37999999999999945  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  50 total reward:  0.4466666666666663  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.02550805480534
printing an ep nov before normalisation:  42.96366181043898
printing an ep nov before normalisation:  40.45138573681389
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.2239, 0.1756, 0.1340, 0.1083, 0.1223, 0.1287, 0.1072],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0075, 0.9468, 0.0065, 0.0124, 0.0038, 0.0049, 0.0180],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1154, 0.1461, 0.3709, 0.0826, 0.0842, 0.1138, 0.0870],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1471, 0.1658, 0.0814, 0.3043, 0.1114, 0.0811, 0.1089],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2299, 0.0121, 0.0905, 0.1132, 0.3584, 0.0946, 0.1013],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1887, 0.1049, 0.1449, 0.1184, 0.1227, 0.2054, 0.1150],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2174, 0.1475, 0.1100, 0.1573, 0.1369, 0.1040, 0.1270],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  66 total reward:  0.11333333333333284  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  33.41695600063663
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.30456045296512
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.003]
 [ 0.225]
 [ 0.114]
 [ 0.114]
 [ 0.114]
 [ 0.114]
 [ 0.114]] [[48.872]
 [48.311]
 [40.72 ]
 [40.72 ]
 [40.72 ]
 [40.72 ]
 [40.72 ]] [[0.852]
 [1.066]
 [0.774]
 [0.774]
 [0.774]
 [0.774]
 [0.774]]
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 62.44273312542491
line 256 mcts: sample exp_bonus 45.36356848722025
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.048719549041408
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.513]
 [0.513]
 [0.509]
 [0.513]
 [0.513]
 [0.513]] [[10.389]
 [10.389]
 [10.389]
 [11.913]
 [10.389]
 [10.389]
 [10.389]] [[1.335]
 [1.335]
 [1.335]
 [1.452]
 [1.335]
 [1.335]
 [1.335]]
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.053424886539048
printing an ep nov before normalisation:  18.420113062968156
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
actor:  1 policy actor:  1  step number:  81 total reward:  0.17333333333333156  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
printing an ep nov before normalisation:  41.245490806545725
printing an ep nov before normalisation:  32.55256182941258
printing an ep nov before normalisation:  36.74232327167872
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
printing an ep nov before normalisation:  62.83583226463383
Printing some Q and Qe and total Qs values:  [[-0.075]
 [-0.064]
 [-0.094]
 [-0.083]
 [-0.094]
 [-0.095]
 [-0.095]] [[35.656]
 [32.819]
 [20.259]
 [26.607]
 [20.197]
 [19.982]
 [19.796]] [[0.22 ]
 [0.197]
 [0.021]
 [0.106]
 [0.021]
 [0.018]
 [0.015]]
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.35318322148719
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.29333333333333234  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.823765094322404
printing an ep nov before normalisation:  39.31787453067198
siam score:  -0.8612825
printing an ep nov before normalisation:  40.73977874875206
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.163]
 [-0.168]
 [-0.168]
 [-0.168]
 [-0.168]
 [-0.168]
 [-0.168]] [[55.774]
 [47.88 ]
 [47.88 ]
 [47.88 ]
 [47.88 ]
 [47.88 ]
 [47.88 ]] [[1.042]
 [0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.775]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
Printing some Q and Qe and total Qs values:  [[-0.024]
 [-0.016]
 [-0.023]
 [-0.016]
 [-0.026]
 [-0.014]
 [-0.028]] [[37.992]
 [36.793]
 [37.851]
 [37.993]
 [38.145]
 [38.269]
 [37.796]] [[0.578]
 [0.548]
 [0.574]
 [0.586]
 [0.581]
 [0.596]
 [0.568]]
actions average: 
K:  0  action  0 :  tensor([0.3550, 0.0133, 0.1338, 0.1090, 0.1433, 0.1186, 0.1271],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0024,     0.9752,     0.0013,     0.0040,     0.0009,     0.0007,
            0.0155], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1640, 0.0030, 0.2538, 0.1290, 0.1481, 0.1785, 0.1236],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1456, 0.0292, 0.1271, 0.2879, 0.1481, 0.1402, 0.1219],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1349, 0.1031, 0.0892, 0.1003, 0.3993, 0.0877, 0.0855],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1169, 0.0018, 0.2057, 0.1021, 0.1247, 0.3549, 0.0939],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1450, 0.0206, 0.1615, 0.1237, 0.0971, 0.0984, 0.3536],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.352]
 [0.355]
 [0.355]
 [0.355]
 [0.355]
 [0.355]] [[47.945]
 [48.141]
 [47.945]
 [47.945]
 [47.945]
 [47.945]
 [47.945]] [[0.855]
 [0.855]
 [0.855]
 [0.855]
 [0.855]
 [0.855]
 [0.855]]
printing an ep nov before normalisation:  45.26544452125306
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.435]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]] [[31.955]
 [35.427]
 [31.955]
 [31.955]
 [31.955]
 [31.955]
 [31.955]] [[1.308]
 [1.528]
 [1.308]
 [1.308]
 [1.308]
 [1.308]
 [1.308]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  50 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  1.333
UNIT TEST: sample policy line 217 mcts : [0.041 0.388 0.347 0.082 0.041 0.061 0.041]
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.647657070052155
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  40 total reward:  0.44666666666666666  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.504209462440066
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6055800000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.14304248311825
line 256 mcts: sample exp_bonus 49.115349970941075
actor:  1 policy actor:  1  step number:  38 total reward:  0.5  reward:  1.0 rdn_beta:  1.667
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  38 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  39.664383033953584
maxi score, test score, baseline:  -0.6025533333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.66207988741358
maxi score, test score, baseline:  -0.6025533333333334 0.6833333333333336 0.6833333333333336
actor:  1 policy actor:  1  step number:  70 total reward:  0.28666666666666674  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6025533333333334 0.6833333333333336 0.6833333333333336
maxi score, test score, baseline:  -0.6025533333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  -0.6025533333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  64 total reward:  0.059999999999999165  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6004333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6004333333333334 0.6833333333333336 0.6833333333333336
Printing some Q and Qe and total Qs values:  [[-0.067]
 [-0.042]
 [-0.06 ]
 [-0.061]
 [-0.064]
 [-0.061]
 [-0.05 ]] [[31.131]
 [38.175]
 [33.51 ]
 [33.56 ]
 [31.97 ]
 [33.573]
 [35.607]] [[0.079]
 [0.168]
 [0.107]
 [0.107]
 [0.089]
 [0.107]
 [0.136]]
maxi score, test score, baseline:  -0.6004333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.5920608602355
printing an ep nov before normalisation:  31.754369735717773
UNIT TEST: sample policy line 217 mcts : [0.02  0.265 0.02  0.02  0.02  0.02  0.633]
actor:  1 policy actor:  1  step number:  42 total reward:  0.5000000000000001  reward:  1.0 rdn_beta:  2.0
siam score:  -0.87134796
maxi score, test score, baseline:  -0.6004333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6004333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.84596718740574
maxi score, test score, baseline:  -0.6004333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.70699016937261
maxi score, test score, baseline:  -0.6004333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  18.71912205857585
actor:  1 policy actor:  1  step number:  57 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6004333333333334 0.6833333333333336 0.6833333333333336
maxi score, test score, baseline:  -0.6004333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.921]
 [0.921]
 [0.921]
 [0.921]
 [0.921]
 [0.921]
 [0.921]] [[39.061]
 [39.061]
 [39.061]
 [39.061]
 [39.061]
 [39.061]
 [39.061]] [[0.921]
 [0.921]
 [0.921]
 [0.921]
 [0.921]
 [0.921]
 [0.921]]
maxi score, test score, baseline:  -0.6004333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6004333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6004333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.173333333333333  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6004333333333334 0.6833333333333336 0.6833333333333336
printing an ep nov before normalisation:  50.285652996705714
maxi score, test score, baseline:  -0.6004333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6004333333333334 0.6833333333333336 0.6833333333333336
maxi score, test score, baseline:  -0.6004333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6004333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  58 total reward:  0.11333333333333284  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.5982066666666668 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5982066666666668 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.698]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]] [[50.159]
 [48.668]
 [48.668]
 [48.668]
 [48.668]
 [48.668]
 [48.668]] [[0.698]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]]
actor:  1 policy actor:  1  step number:  58 total reward:  0.19333333333333258  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.5982066666666668 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5982066666666668 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0009816340582347038
actor:  1 policy actor:  1  step number:  62 total reward:  0.03333333333333288  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.083]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.083]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]]
printing an ep nov before normalisation:  50.257183959865046
printing an ep nov before normalisation:  43.52316650098092
Printing some Q and Qe and total Qs values:  [[-0.126]
 [-0.126]
 [-0.126]
 [-0.126]
 [-0.126]
 [-0.126]
 [-0.126]] [[39.092]
 [39.092]
 [39.092]
 [39.092]
 [39.092]
 [39.092]
 [39.092]] [[1.142]
 [1.142]
 [1.142]
 [1.142]
 [1.142]
 [1.142]
 [1.142]]
maxi score, test score, baseline:  -0.5982066666666668 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.24066011381202
printing an ep nov before normalisation:  31.943076235697863
actor:  1 policy actor:  1  step number:  39 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.5982066666666668 0.6833333333333336 0.6833333333333336
Printing some Q and Qe and total Qs values:  [[-0.172]
 [-0.202]
 [-0.082]
 [-0.137]
 [-0.127]
 [-0.081]
 [-0.161]] [[47.594]
 [50.233]
 [45.892]
 [43.7  ]
 [43.33 ]
 [48.914]
 [46.036]] [[0.899]
 [0.963]
 [0.929]
 [0.795]
 [0.791]
 [1.037]
 [0.854]]
maxi score, test score, baseline:  -0.5982066666666668 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.14015697132945
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.718]
 [0.71 ]
 [0.607]
 [0.534]
 [0.557]
 [0.603]] [[30.161]
 [31.585]
 [29.188]
 [30.708]
 [29.704]
 [29.884]
 [29.839]] [[1.86 ]
 [2.131]
 [1.936]
 [1.952]
 [1.801]
 [1.838]
 [1.88 ]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5982066666666668 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5982066666666668 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5982066666666668 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.75360296479534
printing an ep nov before normalisation:  16.790969371795654
maxi score, test score, baseline:  -0.5982066666666668 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  0.19333333333333258  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.5982066666666668 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.101717269589287
printing an ep nov before normalisation:  31.55106544461459
actor:  0 policy actor:  0  step number:  40 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  62 total reward:  0.16666666666666585  reward:  1.0 rdn_beta:  0.667
siam score:  -0.863632
actions average: 
K:  0  action  0 :  tensor([0.3924, 0.0106, 0.1207, 0.0906, 0.1851, 0.1080, 0.0925],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0035, 0.9629, 0.0054, 0.0052, 0.0016, 0.0020, 0.0195],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1434, 0.0042, 0.2762, 0.1142, 0.1396, 0.2012, 0.1212],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1466, 0.0059, 0.1476, 0.2358, 0.1392, 0.1874, 0.1374],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1410, 0.0049, 0.1404, 0.1167, 0.3069, 0.1690, 0.1210],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1494, 0.0066, 0.1803, 0.1323, 0.1377, 0.2516, 0.1422],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1248, 0.0567, 0.1384, 0.1058, 0.1289, 0.1459, 0.2996],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.011]
 [ 0.307]
 [-0.011]
 [-0.008]
 [-0.009]
 [-0.009]
 [-0.009]] [[49.673]
 [49.283]
 [48.785]
 [47.283]
 [47.975]
 [47.939]
 [48.319]] [[1.261]
 [1.562]
 [1.222]
 [1.156]
 [1.187]
 [1.185]
 [1.202]]
maxi score, test score, baseline:  -0.5952866666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]] [[60.527]
 [60.527]
 [60.527]
 [60.527]
 [60.527]
 [60.527]
 [60.527]] [[2.288]
 [2.288]
 [2.288]
 [2.288]
 [2.288]
 [2.288]
 [2.288]]
line 256 mcts: sample exp_bonus 51.63859223465217
maxi score, test score, baseline:  -0.5952866666666667 0.6833333333333336 0.6833333333333336
actor:  1 policy actor:  1  step number:  47 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.5952866666666667 0.6833333333333336 0.6833333333333336
printing an ep nov before normalisation:  46.28651497460193
maxi score, test score, baseline:  -0.5952866666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5952866666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  40 total reward:  0.4466666666666663  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.5952866666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.435494422912598
actor:  1 policy actor:  1  step number:  40 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.5952866666666667 0.6833333333333336 0.6833333333333336
Printing some Q and Qe and total Qs values:  [[-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]] [[35.51]
 [35.51]
 [35.51]
 [35.51]
 [35.51]
 [35.51]
 [35.51]] [[1.916]
 [1.916]
 [1.916]
 [1.916]
 [1.916]
 [1.916]
 [1.916]]
printing an ep nov before normalisation:  62.822812362547786
printing an ep nov before normalisation:  51.839764872659885
actor:  1 policy actor:  1  step number:  62 total reward:  0.05999999999999939  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  0.4199999999999996  reward:  1.0 rdn_beta:  2.0
siam score:  -0.86104095
maxi score, test score, baseline:  -0.5952866666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.96466119376312
Printing some Q and Qe and total Qs values:  [[0.585]
 [0.585]
 [0.559]
 [0.534]
 [0.581]
 [0.585]
 [0.585]] [[28.912]
 [28.912]
 [31.632]
 [32.067]
 [30.245]
 [28.912]
 [28.912]] [[1.861]
 [1.861]
 [2.059]
 [2.07 ]
 [1.967]
 [1.861]
 [1.861]]
maxi score, test score, baseline:  -0.5952866666666667 0.6833333333333336 0.6833333333333336
Printing some Q and Qe and total Qs values:  [[0.164]
 [0.164]
 [0.164]
 [0.164]
 [0.164]
 [0.164]
 [0.164]] [[57.794]
 [57.794]
 [57.794]
 [57.794]
 [57.794]
 [57.794]
 [57.794]] [[0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]]
maxi score, test score, baseline:  -0.5952866666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5952866666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  0.3199999999999992  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.5952866666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5952866666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.31632267312291
maxi score, test score, baseline:  -0.5952866666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.64577537125599
printing an ep nov before normalisation:  39.71525092924258
maxi score, test score, baseline:  -0.5952866666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5952866666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5952866666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.17372918128967
printing an ep nov before normalisation:  45.03901272777468
actions average: 
K:  0  action  0 :  tensor([0.3543, 0.0063, 0.1199, 0.1122, 0.1586, 0.1270, 0.1218],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0100, 0.9424, 0.0053, 0.0156, 0.0023, 0.0017, 0.0228],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1364, 0.0062, 0.2635, 0.1315, 0.1137, 0.2317, 0.1170],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1531, 0.0240, 0.1374, 0.2347, 0.1393, 0.1586, 0.1529],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1935, 0.0033, 0.1211, 0.1319, 0.2967, 0.1462, 0.1074],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1826, 0.0093, 0.1670, 0.1285, 0.1453, 0.2291, 0.1381],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1487, 0.0045, 0.1201, 0.1261, 0.1095, 0.1136, 0.3775],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.5952866666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5952866666666667 0.6833333333333336 0.6833333333333336
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.12136590141089
actor:  1 policy actor:  1  step number:  38 total reward:  0.5666666666666667  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  0.7243654497959338
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.75109455785077
maxi score, test score, baseline:  -0.5952866666666667 0.6833333333333336 0.6833333333333336
printing an ep nov before normalisation:  50.281483907287104
maxi score, test score, baseline:  -0.5952866666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.16360036916048
maxi score, test score, baseline:  -0.5952866666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.89183259326893
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  43 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.59238 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.01984262466431
printing an ep nov before normalisation:  54.801413703458664
maxi score, test score, baseline:  -0.59238 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
printing an ep nov before normalisation:  52.84837474806084
Printing some Q and Qe and total Qs values:  [[0.781]
 [0.808]
 [0.796]
 [0.757]
 [0.759]
 [0.767]
 [0.726]] [[26.563]
 [33.827]
 [29.842]
 [27.242]
 [28.73 ]
 [26.261]
 [26.544]] [[0.781]
 [0.808]
 [0.796]
 [0.757]
 [0.759]
 [0.767]
 [0.726]]
UNIT TEST: sample policy line 217 mcts : [0.02  0.571 0.02  0.204 0.02  0.02  0.143]
printing an ep nov before normalisation:  45.97740298483447
actor:  0 policy actor:  0  step number:  48 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  46.86151102639847
printing an ep nov before normalisation:  45.52081967390954
printing an ep nov before normalisation:  35.08878064531791
actor:  0 policy actor:  0  step number:  55 total reward:  0.22666666666666635  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.9042113294966
actor:  0 policy actor:  0  step number:  53 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.5849000000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.3013, 0.0346, 0.1158, 0.1340, 0.1664, 0.1342, 0.1138],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0093, 0.9340, 0.0078, 0.0101, 0.0064, 0.0060, 0.0264],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1454, 0.0089, 0.2288, 0.1827, 0.1470, 0.1556, 0.1317],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1409, 0.0245, 0.1336, 0.2500, 0.1581, 0.1545, 0.1383],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1277, 0.0149, 0.0748, 0.0952, 0.5268, 0.0883, 0.0723],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1141, 0.0259, 0.1722, 0.1145, 0.1023, 0.3731, 0.0979],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1445, 0.0167, 0.1387, 0.1628, 0.1396, 0.1495, 0.2482],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  62.99464404217498
Printing some Q and Qe and total Qs values:  [[-0.14 ]
 [-0.151]
 [-0.145]
 [-0.145]
 [-0.128]
 [-0.145]
 [-0.145]] [[49.419]
 [43.224]
 [43.278]
 [43.278]
 [51.378]
 [43.278]
 [43.278]] [[1.384]
 [1.045]
 [1.053]
 [1.053]
 [1.5  ]
 [1.053]
 [1.053]]
printing an ep nov before normalisation:  37.881569761071006
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5849000000000001 0.6833333333333336 0.6833333333333336
printing an ep nov before normalisation:  18.3661990532038
printing an ep nov before normalisation:  0.00043421855252745445
Printing some Q and Qe and total Qs values:  [[0.686]
 [0.686]
 [0.686]
 [0.685]
 [0.685]
 [0.685]
 [0.686]] [[2.572]
 [3.025]
 [3.196]
 [6.2  ]
 [3.015]
 [5.899]
 [5.599]] [[0.686]
 [0.686]
 [0.686]
 [0.685]
 [0.685]
 [0.685]
 [0.686]]
actor:  0 policy actor:  0  step number:  39 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5818333333333334 0.6833333333333336 0.6833333333333336
maxi score, test score, baseline:  -0.5818333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5818333333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.628]] [[28.139]
 [28.139]
 [28.139]
 [28.139]
 [28.139]
 [28.139]
 [28.139]] [[2.187]
 [2.187]
 [2.187]
 [2.187]
 [2.187]
 [2.187]
 [2.187]]
printing an ep nov before normalisation:  52.019694678801386
printing an ep nov before normalisation:  0.0015566015463264193
actor:  1 policy actor:  1  step number:  53 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8600507
siam score:  -0.8637715
printing an ep nov before normalisation:  50.73787659266615
printing an ep nov before normalisation:  42.378274352649115
printing an ep nov before normalisation:  26.919553935560856
actor:  0 policy actor:  0  step number:  55 total reward:  0.10666666666666613  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.57962 0.6833333333333336 0.6833333333333336
maxi score, test score, baseline:  -0.57962 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  40 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  43.373739025111895
maxi score, test score, baseline:  -0.5767000000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5767000000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5767000000000001 0.6833333333333336 0.6833333333333336
Printing some Q and Qe and total Qs values:  [[-0.136]
 [-0.136]
 [-0.136]
 [-0.136]
 [-0.136]
 [-0.136]
 [-0.136]] [[34.317]
 [34.317]
 [34.317]
 [34.317]
 [34.317]
 [34.317]
 [34.317]] [[0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]]
maxi score, test score, baseline:  -0.5767000000000001 0.6833333333333336 0.6833333333333336
actor:  0 policy actor:  0  step number:  39 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.067]
 [-0.024]
 [-0.067]
 [-0.067]
 [-0.086]
 [-0.084]
 [-0.067]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.067]
 [-0.024]
 [-0.067]
 [-0.067]
 [-0.086]
 [-0.084]
 [-0.067]]
printing an ep nov before normalisation:  27.62014635084128
printing an ep nov before normalisation:  22.72289752960205
maxi score, test score, baseline:  -0.5739266666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5739266666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]] [[50.589]
 [50.589]
 [50.589]
 [50.589]
 [50.589]
 [50.589]
 [50.589]] [[1.114]
 [1.114]
 [1.114]
 [1.114]
 [1.114]
 [1.114]
 [1.114]]
printing an ep nov before normalisation:  32.18419313430786
maxi score, test score, baseline:  -0.5739266666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.039]
 [0.155]
 [0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]] [[31.36 ]
 [38.944]
 [31.36 ]
 [31.36 ]
 [31.36 ]
 [31.36 ]
 [31.36 ]] [[0.756]
 [1.133]
 [0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]]
maxi score, test score, baseline:  -0.5739266666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.125]
 [-0.125]
 [-0.125]
 [-0.125]
 [-0.125]
 [-0.125]
 [-0.125]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.125]
 [-0.125]
 [-0.125]
 [-0.125]
 [-0.125]
 [-0.125]
 [-0.125]]
maxi score, test score, baseline:  -0.5739266666666667 0.6833333333333336 0.6833333333333336
maxi score, test score, baseline:  -0.5739266666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.482330319197466
line 256 mcts: sample exp_bonus 24.891753887272703
actor:  1 policy actor:  1  step number:  70 total reward:  0.09999999999999909  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.5739266666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.058]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]] [[55.961]
 [45.704]
 [45.704]
 [45.704]
 [45.704]
 [45.704]
 [45.704]] [[1.073]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]]
maxi score, test score, baseline:  -0.5739266666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  56 total reward:  0.206666666666666  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  37.077035143143306
maxi score, test score, baseline:  -0.5739266666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.046]
 [-0.059]
 [-0.045]
 [-0.059]
 [-0.059]
 [-0.059]
 [-0.059]] [[40.65 ]
 [31.384]
 [33.199]
 [31.384]
 [31.384]
 [31.384]
 [31.384]] [[0.87 ]
 [0.529]
 [0.607]
 [0.529]
 [0.529]
 [0.529]
 [0.529]]
maxi score, test score, baseline:  -0.5739266666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5739266666666667 0.6833333333333336 0.6833333333333336
maxi score, test score, baseline:  -0.5739266666666667 0.6833333333333336 0.6833333333333336
maxi score, test score, baseline:  -0.5739266666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.25333333333333274  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.5739266666666667 0.6833333333333336 0.6833333333333336
printing an ep nov before normalisation:  50.55685912801905
printing an ep nov before normalisation:  49.07325925476652
maxi score, test score, baseline:  -0.5739266666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.68010052327108
printing an ep nov before normalisation:  27.71158010262786
maxi score, test score, baseline:  -0.5739266666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.708957990594033
actor:  1 policy actor:  1  step number:  61 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.165]
 [-0.189]
 [-0.176]
 [-0.147]
 [-0.179]
 [-0.159]
 [-0.179]] [[23.172]
 [19.485]
 [20.563]
 [19.972]
 [20.006]
 [20.657]
 [19.728]] [[0.46 ]
 [0.337]
 [0.379]
 [0.392]
 [0.36 ]
 [0.398]
 [0.353]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.097]
 [-0.064]
 [-0.097]
 [-0.097]
 [-0.097]
 [-0.097]
 [-0.097]] [[37.721]
 [40.635]
 [37.721]
 [37.721]
 [37.721]
 [37.721]
 [37.721]] [[0.135]
 [0.197]
 [0.135]
 [0.135]
 [0.135]
 [0.135]
 [0.135]]
printing an ep nov before normalisation:  32.9980536379823
maxi score, test score, baseline:  -0.5739266666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.15351663896233
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.29219024897211
maxi score, test score, baseline:  -0.5739266666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.222]
 [0.378]
 [0.222]
 [0.17 ]
 [0.044]
 [0.222]
 [0.196]] [[37.707]
 [37.546]
 [37.707]
 [34.239]
 [36.233]
 [37.707]
 [33.757]] [[0.466]
 [0.62 ]
 [0.466]
 [0.374]
 [0.271]
 [0.466]
 [0.394]]
maxi score, test score, baseline:  -0.5739266666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.072008239144
printing an ep nov before normalisation:  42.57167335817443
Printing some Q and Qe and total Qs values:  [[-0.037]
 [-0.035]
 [-0.057]
 [-0.053]
 [-0.037]
 [-0.057]
 [-0.046]] [[38.011]
 [36.496]
 [36.617]
 [38.504]
 [39.86 ]
 [39.77 ]
 [37.158]] [[0.226]
 [0.208]
 [0.188]
 [0.217]
 [0.251]
 [0.23 ]
 [0.206]]
printing an ep nov before normalisation:  29.922112760175565
printing an ep nov before normalisation:  50.436171582354916
maxi score, test score, baseline:  -0.5739266666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5739266666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.511]
 [0.576]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]] [[31.043]
 [38.108]
 [31.043]
 [31.043]
 [31.043]
 [31.043]
 [31.043]] [[1.081]
 [1.399]
 [1.081]
 [1.081]
 [1.081]
 [1.081]
 [1.081]]
printing an ep nov before normalisation:  22.146979935270977
printing an ep nov before normalisation:  31.636263166504722
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  61 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  0.333
siam score:  -0.86295336
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.525]
 [0.51 ]
 [0.479]
 [0.461]
 [0.505]
 [0.542]] [[36.001]
 [37.72 ]
 [34.818]
 [33.138]
 [32.34 ]
 [34.381]
 [35.119]] [[0.777]
 [0.758]
 [0.725]
 [0.684]
 [0.66 ]
 [0.717]
 [0.759]]
maxi score, test score, baseline:  -0.5739266666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  72 total reward:  0.03333333333333277  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.3532, 0.0974, 0.1030, 0.1211, 0.1228, 0.1078, 0.0947],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0125, 0.8974, 0.0091, 0.0149, 0.0075, 0.0082, 0.0504],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1465, 0.0617, 0.3494, 0.1097, 0.0890, 0.1289, 0.1149],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0855, 0.1197, 0.0762, 0.4101, 0.1122, 0.1112, 0.0849],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1765, 0.0383, 0.1450, 0.1680, 0.1567, 0.1786, 0.1369],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1352, 0.0709, 0.1731, 0.1301, 0.1082, 0.2590, 0.1236],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1495, 0.1352, 0.1477, 0.1405, 0.1337, 0.1467, 0.1466],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  51.664072291445166
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.400987227384256
maxi score, test score, baseline:  -0.5739266666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.65485223947454
maxi score, test score, baseline:  -0.5739266666666667 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8593263
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.707]
 [0.69 ]
 [0.941]
 [0.619]
 [0.67 ]
 [0.692]] [[30.708]
 [33.742]
 [34.779]
 [35.214]
 [34.396]
 [34.263]
 [31.271]] [[0.668]
 [0.707]
 [0.69 ]
 [0.941]
 [0.619]
 [0.67 ]
 [0.692]]
actor:  0 policy actor:  0  step number:  43 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5711 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.39333333333333287  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  38.04422749413384
printing an ep nov before normalisation:  57.458523695362956
printing an ep nov before normalisation:  38.92138305303836
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.91334629058838
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.06724136258039
maxi score, test score, baseline:  -0.5711 0.6833333333333336 0.6833333333333336
Printing some Q and Qe and total Qs values:  [[-0.12 ]
 [-0.085]
 [-0.115]
 [-0.089]
 [-0.131]
 [-0.12 ]
 [-0.12 ]] [[42.964]
 [46.918]
 [48.056]
 [48.937]
 [54.336]
 [42.964]
 [42.964]] [[0.074]
 [0.152]
 [0.134]
 [0.169]
 [0.187]
 [0.074]
 [0.074]]
maxi score, test score, baseline:  -0.5711 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.4046, 0.0264, 0.0988, 0.1249, 0.1267, 0.1145, 0.1041],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0054, 0.9520, 0.0066, 0.0101, 0.0023, 0.0025, 0.0212],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1116, 0.0452, 0.3791, 0.1012, 0.1212, 0.1355, 0.1061],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1334, 0.0244, 0.1122, 0.3085, 0.1463, 0.1380, 0.1372],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1942, 0.0020, 0.1450, 0.1394, 0.2109, 0.1653, 0.1432],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1068, 0.0205, 0.1503, 0.1179, 0.1332, 0.3495, 0.1218],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1468, 0.0885, 0.1211, 0.1701, 0.1817, 0.1442, 0.1478],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.5711 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.35811753227589
printing an ep nov before normalisation:  62.5408159165441
printing an ep nov before normalisation:  62.947406069592674
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([0.4348, 0.0031, 0.0897, 0.0980, 0.1118, 0.1133, 0.1492],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0195, 0.8959, 0.0122, 0.0345, 0.0094, 0.0106, 0.0179],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1869, 0.0042, 0.2113, 0.1464, 0.1504, 0.1764, 0.1244],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1322, 0.0303, 0.1116, 0.3432, 0.1135, 0.1413, 0.1278],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2042, 0.0156, 0.1109, 0.1300, 0.2768, 0.1349, 0.1277],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1579, 0.0042, 0.1585, 0.1285, 0.1016, 0.3337, 0.1155],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0978, 0.1020, 0.0863, 0.1091, 0.0877, 0.1062, 0.4108],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.5711 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.93390404530216
maxi score, test score, baseline:  -0.5711 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.2733333333333331  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.5711 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.78446575328781
printing an ep nov before normalisation:  36.67838891965218
printing an ep nov before normalisation:  34.471291736347574
maxi score, test score, baseline:  -0.5711 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5711 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.45 ]
 [0.635]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]] [[39.125]
 [41.437]
 [39.125]
 [39.125]
 [39.125]
 [39.125]
 [39.125]] [[0.45 ]
 [0.635]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]]
Printing some Q and Qe and total Qs values:  [[0.537]
 [0.674]
 [0.521]
 [0.525]
 [0.681]
 [0.608]
 [0.657]] [[30.341]
 [32.032]
 [30.875]
 [30.538]
 [31.471]
 [31.421]
 [31.726]] [[0.767]
 [0.929]
 [0.759]
 [0.758]
 [0.928]
 [0.854]
 [0.908]]
actor:  1 policy actor:  1  step number:  63 total reward:  0.18666666666666565  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5711 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  1  action  0 :  tensor([0.3373, 0.0805, 0.0974, 0.1000, 0.1496, 0.1166, 0.1186],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0051, 0.9564, 0.0044, 0.0102, 0.0046, 0.0049, 0.0144],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1070, 0.1012, 0.2949, 0.1084, 0.1130, 0.1644, 0.1111],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1357, 0.0243, 0.1277, 0.2156, 0.1618, 0.1520, 0.1827],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1372, 0.0141, 0.1040, 0.1963, 0.2700, 0.1341, 0.1444],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1775, 0.0075, 0.1397, 0.1434, 0.1493, 0.2522, 0.1303],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0944, 0.2425, 0.0887, 0.1123, 0.0950, 0.1008, 0.2662],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.5711 0.6833333333333336 0.6833333333333336
maxi score, test score, baseline:  -0.5711 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.352434824246494
maxi score, test score, baseline:  -0.5711 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5711 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  57 total reward:  0.173333333333333  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  23.40535153994097
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.05894189442047
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.946657419204712
printing an ep nov before normalisation:  32.187482491954675
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.194201469421387
Printing some Q and Qe and total Qs values:  [[0.393]
 [0.485]
 [0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.449]] [[30.526]
 [31.794]
 [30.526]
 [30.526]
 [30.526]
 [30.526]
 [31.966]] [[1.069]
 [1.215]
 [1.069]
 [1.069]
 [1.069]
 [1.069]
 [1.186]]
actor:  1 policy actor:  1  step number:  37 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.123]
 [-0.099]
 [-0.123]
 [-0.123]
 [-0.132]
 [-0.123]
 [-0.123]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.123]
 [-0.099]
 [-0.123]
 [-0.123]
 [-0.132]
 [-0.123]
 [-0.123]]
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  72 total reward:  0.08666666666666534  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.38962318053123
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.644]
 [0.528]
 [0.644]
 [0.56 ]
 [0.375]
 [0.388]
 [0.644]] [[33.891]
 [40.768]
 [33.891]
 [38.536]
 [41.784]
 [42.083]
 [33.891]] [[1.238]
 [1.352]
 [1.238]
 [1.309]
 [1.234]
 [1.256]
 [1.238]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[ 0.104]
 [ 0.162]
 [-0.072]
 [ 0.059]
 [ 0.197]
 [-0.065]
 [ 0.094]] [[28.491]
 [33.662]
 [29.455]
 [29.147]
 [39.279]
 [30.959]
 [28.257]] [[0.764]
 [1.082]
 [0.637]
 [0.752]
 [1.4  ]
 [0.719]
 [0.743]]
Printing some Q and Qe and total Qs values:  [[0.672]
 [0.59 ]
 [0.672]
 [0.672]
 [0.578]
 [0.672]
 [0.672]] [[35.05 ]
 [41.573]
 [35.05 ]
 [35.05 ]
 [40.262]
 [35.05 ]
 [35.05 ]] [[1.417]
 [1.574]
 [1.417]
 [1.417]
 [1.514]
 [1.417]
 [1.417]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  64 total reward:  0.19333333333333247  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
Printing some Q and Qe and total Qs values:  [[-0.119]
 [-0.106]
 [-0.119]
 [-0.123]
 [-0.117]
 [-0.123]
 [-0.123]] [[34.769]
 [49.105]
 [33.34 ]
 [29.772]
 [32.972]
 [29.772]
 [29.772]] [[ 0.012]
 [ 0.148]
 [-0.001]
 [-0.035]
 [-0.001]
 [-0.035]
 [-0.035]]
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
actions average: 
K:  2  action  0 :  tensor([0.4988, 0.0024, 0.0949, 0.0889, 0.1292, 0.1037, 0.0821],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0145, 0.8708, 0.0191, 0.0155, 0.0069, 0.0147, 0.0583],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1598, 0.1204, 0.1818, 0.1385, 0.1136, 0.1660, 0.1200],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1150, 0.0221, 0.0995, 0.4395, 0.1048, 0.1223, 0.0968],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2385, 0.0161, 0.1171, 0.1138, 0.2699, 0.1319, 0.1128],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1244, 0.0085, 0.2033, 0.1435, 0.1236, 0.2917, 0.1052],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1843, 0.1128, 0.1180, 0.1434, 0.1447, 0.1353, 0.1614],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
Printing some Q and Qe and total Qs values:  [[0.26 ]
 [0.429]
 [0.233]
 [0.321]
 [0.259]
 [0.26 ]
 [0.32 ]] [[32.378]
 [31.044]
 [35.886]
 [34.106]
 [35.677]
 [32.378]
 [33.666]] [[0.734]
 [0.864]
 [0.811]
 [0.846]
 [0.83 ]
 [0.734]
 [0.833]]
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.59023487782979
actor:  1 policy actor:  1  step number:  64 total reward:  0.046666666666665746  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.472]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]] [[27.341]
 [30.547]
 [27.341]
 [27.341]
 [27.341]
 [27.341]
 [27.341]] [[0.574]
 [0.729]
 [0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
actor:  1 policy actor:  1  step number:  63 total reward:  0.17333333333333256  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.63692359217894
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[ 0.137]
 [ 0.249]
 [ 0.206]
 [ 0.022]
 [-0.006]
 [ 0.206]
 [ 0.206]] [[28.707]
 [34.897]
 [33.504]
 [27.617]
 [28.705]
 [33.504]
 [33.504]] [[0.651]
 [1.015]
 [0.915]
 [0.491]
 [0.508]
 [0.915]
 [0.915]]
actor:  1 policy actor:  1  step number:  59 total reward:  0.10666666666666613  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.675893056137596
printing an ep nov before normalisation:  29.320737246425093
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.30610839723131
actor:  1 policy actor:  1  step number:  60 total reward:  0.25999999999999945  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  50.33277828700708
actor:  1 policy actor:  1  step number:  59 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.280465788156654
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.588293932909096
Printing some Q and Qe and total Qs values:  [[0.222]
 [0.319]
 [0.155]
 [0.23 ]
 [0.22 ]
 [0.203]
 [0.196]] [[35.36 ]
 [37.707]
 [34.679]
 [38.168]
 [35.882]
 [37.543]
 [33.904]] [[1.233]
 [1.452]
 [1.131]
 [1.388]
 [1.259]
 [1.328]
 [1.131]]
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  16.601574420928955
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.41163562474175
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  0.19333333333333247  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  25.89123337733892
printing an ep nov before normalisation:  40.33754399815687
Printing some Q and Qe and total Qs values:  [[0.541]
 [0.687]
 [0.558]
 [0.494]
 [0.28 ]
 [0.613]
 [0.604]] [[13.311]
 [12.414]
 [12.93 ]
 [19.944]
 [12.921]
 [12.575]
 [10.415]] [[1.115]
 [1.188]
 [1.101]
 [1.602]
 [0.822]
 [1.128]
 [0.945]]
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.3666666666666658  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.589]
 [0.731]
 [0.589]
 [0.543]
 [0.589]
 [0.589]
 [0.807]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.589]
 [0.731]
 [0.589]
 [0.543]
 [0.589]
 [0.589]
 [0.807]]
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
maxi score, test score, baseline:  -0.5687533333333333 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  41 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.5659266666666668 0.6833333333333336 0.6833333333333336
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.658]
 [0.5  ]
 [0.564]
 [0.509]
 [0.479]
 [0.639]] [[32.828]
 [31.288]
 [32.066]
 [35.268]
 [38.846]
 [37.53 ]
 [30.721]] [[0.958]
 [1.031]
 [0.893]
 [1.038]
 [1.073]
 [1.01 ]
 [0.997]]
printing an ep nov before normalisation:  31.920372624746705
maxi score, test score, baseline:  -0.5659266666666668 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.022]
 [ 0.037]
 [-0.047]
 [-0.049]
 [-0.019]
 [-0.024]
 [-0.048]] [[31.333]
 [31.981]
 [31.409]
 [31.256]
 [31.234]
 [31.505]
 [30.771]] [[0.208]
 [0.276]
 [0.183]
 [0.18 ]
 [0.209]
 [0.208]
 [0.174]]
maxi score, test score, baseline:  -0.5659266666666668 0.6833333333333336 0.6833333333333336
actor:  1 policy actor:  1  step number:  41 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.789]
 [0.756]
 [0.556]
 [0.553]
 [0.552]
 [0.552]] [[39.147]
 [33.227]
 [32.784]
 [37.451]
 [37.416]
 [37.146]
 [36.904]] [[0.555]
 [0.789]
 [0.756]
 [0.556]
 [0.553]
 [0.552]
 [0.552]]
maxi score, test score, baseline:  -0.5659266666666668 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5659266666666668 0.6833333333333336 0.6833333333333336
actor:  0 policy actor:  0  step number:  53 total reward:  0.22666666666666635  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  46 total reward:  0.3399999999999995  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5634733333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5634733333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.699]
 [0.805]
 [0.698]
 [0.701]
 [0.697]
 [0.699]
 [0.69 ]] [[30.805]
 [28.351]
 [31.374]
 [31.414]
 [33.159]
 [33.388]
 [33.083]] [[0.699]
 [0.805]
 [0.698]
 [0.701]
 [0.697]
 [0.699]
 [0.69 ]]
maxi score, test score, baseline:  -0.5634733333333334 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.60736800648995
printing an ep nov before normalisation:  42.315969467163086
maxi score, test score, baseline:  -0.5634733333333334 0.6833333333333336 0.6833333333333336
printing an ep nov before normalisation:  51.8289191667079
actor:  0 policy actor:  0  step number:  59 total reward:  0.06666666666666576  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  35.54664337203408
printing an ep nov before normalisation:  33.65102940153612
maxi score, test score, baseline:  -0.5613400000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.962803777735054
maxi score, test score, baseline:  -0.5613400000000001 0.6833333333333336 0.6833333333333336
printing an ep nov before normalisation:  30.851078033447266
maxi score, test score, baseline:  -0.5613400000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.5275, 0.0110, 0.0775, 0.0870, 0.1383, 0.0816, 0.0771],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0064, 0.9525, 0.0071, 0.0090, 0.0031, 0.0032, 0.0188],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1865, 0.0046, 0.2301, 0.1348, 0.1639, 0.1547, 0.1254],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1442, 0.0077, 0.1286, 0.2888, 0.1776, 0.1380, 0.1150],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1637, 0.0063, 0.1418, 0.1835, 0.2643, 0.1238, 0.1166],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1752, 0.0050, 0.1530, 0.1274, 0.1707, 0.2366, 0.1321],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1306, 0.1102, 0.1409, 0.1439, 0.1013, 0.0984, 0.2747],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  39.12318381755196
printing an ep nov before normalisation:  27.550511653726574
maxi score, test score, baseline:  -0.5613400000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.70592763465245
maxi score, test score, baseline:  -0.5613400000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.26482823429128
printing an ep nov before normalisation:  33.98312256847201
actor:  1 policy actor:  1  step number:  51 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  34.988143617457936
printing an ep nov before normalisation:  32.6710164038574
printing an ep nov before normalisation:  44.40311419714855
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.079]
 [0.171]
 [0.078]
 [0.084]
 [0.064]
 [0.091]
 [0.078]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.079]
 [0.171]
 [0.078]
 [0.084]
 [0.064]
 [0.091]
 [0.078]]
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.673]
 [0.745]
 [0.441]
 [0.637]
 [0.54 ]
 [0.673]
 [0.737]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.673]
 [0.745]
 [0.441]
 [0.637]
 [0.54 ]
 [0.673]
 [0.737]]
printing an ep nov before normalisation:  36.827969130057355
Printing some Q and Qe and total Qs values:  [[0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.697]]
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.673]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.531]
 [0.673]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]]
maxi score, test score, baseline:  -0.5613400000000001 0.6833333333333336 0.6833333333333336
printing an ep nov before normalisation:  39.80509812242388
printing an ep nov before normalisation:  46.675181688847864
maxi score, test score, baseline:  -0.5613400000000001 0.6833333333333336 0.6833333333333336
printing an ep nov before normalisation:  50.73391536147877
printing an ep nov before normalisation:  56.861212113981814
printing an ep nov before normalisation:  49.9649701034082
printing an ep nov before normalisation:  47.30471677525817
printing an ep nov before normalisation:  40.588363597118914
maxi score, test score, baseline:  -0.5613400000000001 0.6833333333333336 0.6833333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.960182709787205
printing an ep nov before normalisation:  43.543157825096046
printing an ep nov before normalisation:  40.30891075459239
printing an ep nov before normalisation:  40.95836919297096
printing an ep nov before normalisation:  36.912355550065456
actor:  0 policy actor:  0  step number:  60 total reward:  0.15333333333333277  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  44.92599659126963
printing an ep nov before normalisation:  45.09437561035156
printing an ep nov before normalisation:  31.966944492646853
printing an ep nov before normalisation:  35.552899251654665
actions average: 
K:  4  action  0 :  tensor([0.2574, 0.0795, 0.1294, 0.1099, 0.1640, 0.1222, 0.1376],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0150, 0.9064, 0.0115, 0.0141, 0.0122, 0.0115, 0.0293],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1669, 0.0664, 0.2455, 0.1130, 0.1254, 0.1508, 0.1319],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1839, 0.0056, 0.1266, 0.2581, 0.1325, 0.1486, 0.1447],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2057, 0.0417, 0.0901, 0.0868, 0.3637, 0.1004, 0.1115],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1748, 0.1092, 0.1177, 0.1041, 0.1221, 0.2455, 0.1265],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1297, 0.2411, 0.0944, 0.0809, 0.0910, 0.0752, 0.2879],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  39.19951104289929
printing an ep nov before normalisation:  29.440315775466797
Printing some Q and Qe and total Qs values:  [[0.861]
 [0.947]
 [0.861]
 [0.759]
 [0.861]
 [0.861]
 [0.861]] [[30.07 ]
 [29.914]
 [30.07 ]
 [45.622]
 [30.07 ]
 [30.07 ]
 [30.07 ]] [[0.861]
 [0.947]
 [0.861]
 [0.759]
 [0.861]
 [0.861]
 [0.861]]
line 256 mcts: sample exp_bonus 31.337889317574888
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333333  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  26.187915802001953
printing an ep nov before normalisation:  38.62517649173954
actor:  1 policy actor:  1  step number:  62 total reward:  0.17999999999999972  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.49136666666666673 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.38941335285276
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.639]] [[40.699]
 [40.699]
 [40.699]
 [40.699]
 [40.699]
 [40.699]
 [40.699]] [[0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.639]]
maxi score, test score, baseline:  -0.49136666666666673 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.47820238044448
printing an ep nov before normalisation:  43.63425334529744
printing an ep nov before normalisation:  36.636647296758255
Printing some Q and Qe and total Qs values:  [[-0.029]
 [ 0.253]
 [ 0.226]
 [ 0.226]
 [-0.003]
 [ 0.226]
 [ 0.226]] [[40.844]
 [40.934]
 [36.426]
 [36.426]
 [46.907]
 [36.426]
 [36.426]] [[1.061]
 [1.348]
 [1.075]
 [1.075]
 [1.419]
 [1.075]
 [1.075]]
printing an ep nov before normalisation:  44.43362866582088
actor:  0 policy actor:  0  step number:  58 total reward:  0.13999999999999957  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  60 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  37.7258166912851
printing an ep nov before normalisation:  26.462338886751287
maxi score, test score, baseline:  -0.4890866666666667 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.4201, 0.1220, 0.0878, 0.0785, 0.1146, 0.0898, 0.0872],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0100, 0.9143, 0.0091, 0.0167, 0.0054, 0.0071, 0.0373],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1461, 0.0753, 0.2360, 0.1320, 0.1575, 0.1267, 0.1264],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1363, 0.0341, 0.1144, 0.2932, 0.1628, 0.1292, 0.1301],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1609, 0.0907, 0.0755, 0.1209, 0.3506, 0.1089, 0.0925],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1216, 0.0654, 0.2024, 0.1209, 0.1224, 0.2510, 0.1162],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1424, 0.1317, 0.1101, 0.1144, 0.1423, 0.1163, 0.2427],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  53 total reward:  0.2533333333333331  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  47.81232099896312
maxi score, test score, baseline:  -0.4890866666666667 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.63986173448282
printing an ep nov before normalisation:  36.09603254133506
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.772]
 [0.754]
 [0.595]
 [0.536]
 [0.541]
 [0.636]] [[31.627]
 [38.698]
 [34.889]
 [35.541]
 [37.384]
 [33.857]
 [33.365]] [[0.543]
 [0.772]
 [0.754]
 [0.595]
 [0.536]
 [0.541]
 [0.636]]
actor:  1 policy actor:  1  step number:  56 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  44 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  1.333
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[ 0.4769],
        [ 0.3909],
        [ 0.1859],
        [ 0.0686],
        [-0.3923],
        [-0.0479],
        [-0.4585],
        [-0.4030],
        [ 0.1810],
        [-0.3099]], dtype=torch.float64)
-0.084359833866 0.39254068349623317
-0.07129443439800001 0.3196545573399385
-0.083839701198 0.10208129782353023
-0.07129183386599999 -0.0026586492872476275
-0.032346567066 -0.4246266372369808
-0.09703970119800001 -0.14497065811600707
-0.09703970119800001 -0.5555061907391469
-0.032346567066 -0.43536094583081636
-0.045026434398 0.13595969105042421
-0.032346567066 -0.34221259887507055
printing an ep nov before normalisation:  40.431871273651694
line 256 mcts: sample exp_bonus 29.63022077575521
maxi score, test score, baseline:  -0.4890866666666667 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0167],
        [ 0.4831],
        [ 0.6546],
        [-0.2684],
        [-0.0161],
        [-0.0000],
        [ 0.4772],
        [ 0.1817],
        [ 0.0392],
        [ 0.5563]], dtype=torch.float64)
-0.09703970119800001 -0.11377616472777366
-0.070771701198 0.41234775944322477
-0.058614567066 0.5960167898830911
-0.09703970119800001 -0.3654570887441514
-0.032346567066 -0.04842761065092743
0.957165 0.957165
-0.09703970119800001 0.38011087816565925
-0.058354513866000005 0.12330083664931439
-0.045026434398 -0.005789146322394523
-0.071422513866 0.48488061155018464
actor:  0 policy actor:  0  step number:  44 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  38.893058241865724
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.70912100341541
actions average: 
K:  2  action  0 :  tensor([0.3262, 0.0238, 0.0993, 0.1328, 0.1469, 0.1342, 0.1369],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0044, 0.9347, 0.0035, 0.0161, 0.0032, 0.0015, 0.0365],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1540, 0.0469, 0.2021, 0.1389, 0.1567, 0.1493, 0.1520],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1429, 0.1619, 0.1195, 0.1331, 0.1415, 0.1294, 0.1718],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1660, 0.0147, 0.1204, 0.1304, 0.2897, 0.1351, 0.1438],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1397, 0.0282, 0.1426, 0.1626, 0.1883, 0.1897, 0.1490],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1132, 0.0577, 0.1495, 0.1706, 0.1356, 0.1622, 0.2112],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  33.57533534885608
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  51 total reward:  0.26666666666666616  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.48384666666666676 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.48384666666666676 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  42 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.48384666666666676 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.48384666666666676 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.48384666666666665 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.48384666666666665 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.570249259410346
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.48384666666666665 0.6916666666666668 0.6916666666666668
maxi score, test score, baseline:  -0.48384666666666665 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.48384666666666665 0.6916666666666668 0.6916666666666668
maxi score, test score, baseline:  -0.48384666666666665 0.6916666666666668 0.6916666666666668
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.03552656780334
printing an ep nov before normalisation:  46.86360772620134
printing an ep nov before normalisation:  41.69789708318408
Printing some Q and Qe and total Qs values:  [[0.243]
 [0.354]
 [0.265]
 [0.261]
 [0.263]
 [0.262]
 [0.255]] [[54.442]
 [46.126]
 [50.726]
 [51.592]
 [51.882]
 [53.195]
 [54.476]] [[1.089]
 [0.952]
 [1.001]
 [1.022]
 [1.033]
 [1.071]
 [1.102]]
printing an ep nov before normalisation:  34.148541940867666
maxi score, test score, baseline:  -0.48384666666666665 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.41333333333333333  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  44.18493862738133
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  62 total reward:  0.2599999999999991  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.48384666666666665 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.48384666666666665 0.6916666666666668 0.6916666666666668
printing an ep nov before normalisation:  40.92814509395907
actions average: 
K:  1  action  0 :  tensor([0.3470, 0.0063, 0.1175, 0.1261, 0.1472, 0.1313, 0.1246],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0052, 0.9540, 0.0065, 0.0071, 0.0051, 0.0037, 0.0185],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1275, 0.0161, 0.3152, 0.1381, 0.1380, 0.1396, 0.1255],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1434, 0.0110, 0.1204, 0.3230, 0.1295, 0.1166, 0.1561],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1582, 0.0489, 0.1325, 0.1176, 0.2987, 0.1267, 0.1173],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1541, 0.0040, 0.2139, 0.1220, 0.1183, 0.2716, 0.1161],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0633, 0.0606, 0.0582, 0.0818, 0.0669, 0.0644, 0.6048],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.48384666666666665 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.0101050673187
actor:  1 policy actor:  1  step number:  63 total reward:  0.25333333333333263  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.099]
 [-0.057]
 [-0.099]
 [-0.099]
 [-0.099]
 [-0.099]
 [-0.099]] [[17.704]
 [33.884]
 [17.704]
 [17.704]
 [17.704]
 [17.704]
 [17.704]] [[0.733]
 [1.811]
 [0.733]
 [0.733]
 [0.733]
 [0.733]
 [0.733]]
printing an ep nov before normalisation:  54.70539173112886
maxi score, test score, baseline:  -0.48384666666666665 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.48384666666666665 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.290334564879885
actor:  1 policy actor:  1  step number:  64 total reward:  0.3133333333333328  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.48384666666666665 0.6916666666666668 0.6916666666666668
actor:  1 policy actor:  1  step number:  61 total reward:  0.013333333333332642  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.48384666666666665 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.32616812158228
siam score:  -0.8747237
actor:  1 policy actor:  1  step number:  55 total reward:  0.26666666666666594  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  17.972787891067277
printing an ep nov before normalisation:  38.451115434308214
line 256 mcts: sample exp_bonus 0.0
siam score:  -0.8702338
maxi score, test score, baseline:  -0.48384666666666665 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  0.5  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  32.252328395843506
Printing some Q and Qe and total Qs values:  [[0.193]
 [0.193]
 [0.193]
 [0.193]
 [0.193]
 [0.193]
 [0.193]] [[56.817]
 [56.817]
 [56.817]
 [56.817]
 [56.817]
 [56.817]
 [56.817]] [[1.402]
 [1.402]
 [1.402]
 [1.402]
 [1.402]
 [1.402]
 [1.402]]
printing an ep nov before normalisation:  31.86957597732544
printing an ep nov before normalisation:  29.116743324707322
printing an ep nov before normalisation:  29.33115810154118
printing an ep nov before normalisation:  35.98426932997861
Printing some Q and Qe and total Qs values:  [[-0.08 ]
 [-0.033]
 [-0.121]
 [-0.072]
 [-0.08 ]
 [-0.078]
 [-0.078]] [[50.34 ]
 [44.005]
 [50.874]
 [53.21 ]
 [70.858]
 [54.05 ]
 [54.05 ]] [[0.493]
 [0.411]
 [0.463]
 [0.56 ]
 [0.911]
 [0.57 ]
 [0.57 ]]
maxi score, test score, baseline:  -0.48384666666666665 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.33837191964699
printing an ep nov before normalisation:  57.127861976623535
printing an ep nov before normalisation:  32.8444766998291
printing an ep nov before normalisation:  19.67280387878418
maxi score, test score, baseline:  -0.48384666666666665 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.6207971852644
actor:  1 policy actor:  1  step number:  42 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.48384666666666665 0.6916666666666668 0.6916666666666668
actor:  1 policy actor:  1  step number:  44 total reward:  0.4466666666666663  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  17.812660188410817
maxi score, test score, baseline:  -0.48384666666666665 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  14.03780221939087
maxi score, test score, baseline:  -0.48384666666666665 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.68355019310802
printing an ep nov before normalisation:  35.987201114550615
actor:  1 policy actor:  1  step number:  51 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  71 total reward:  0.013333333333332531  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.834716796875
Printing some Q and Qe and total Qs values:  [[0.717]
 [0.793]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]] [[18.423]
 [25.763]
 [18.423]
 [18.423]
 [18.423]
 [18.423]
 [18.423]] [[0.717]
 [0.793]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]]
actor:  1 policy actor:  1  step number:  59 total reward:  0.18666666666666631  reward:  1.0 rdn_beta:  0.667
siam score:  -0.87058365
maxi score, test score, baseline:  -0.48384666666666665 0.6916666666666668 0.6916666666666668
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.76 ]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]] [[46.974]
 [40.371]
 [47.187]
 [47.187]
 [47.187]
 [47.187]
 [47.187]] [[0.617]
 [0.76 ]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]]
actor:  0 policy actor:  0  step number:  59 total reward:  0.10666666666666558  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.48163333333333336 0.6916666666666668 0.6916666666666668
printing an ep nov before normalisation:  34.36206713164793
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.835]
 [0.943]
 [0.822]
 [0.805]
 [0.87 ]
 [0.87 ]
 [0.848]] [[34.259]
 [34.911]
 [34.56 ]
 [34.085]
 [40.676]
 [40.676]
 [36.908]] [[0.835]
 [0.943]
 [0.822]
 [0.805]
 [0.87 ]
 [0.87 ]
 [0.848]]
actor:  0 policy actor:  0  step number:  52 total reward:  0.21999999999999964  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.4791933333333334 0.6916666666666668 0.6916666666666668
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([0.3404, 0.0145, 0.1204, 0.1291, 0.1499, 0.1279, 0.1177],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0163, 0.9293, 0.0099, 0.0086, 0.0043, 0.0044, 0.0272],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1516, 0.0248, 0.3043, 0.0961, 0.1058, 0.1841, 0.1334],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1526, 0.1442, 0.1143, 0.1481, 0.1493, 0.1359, 0.1555],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1834, 0.0708, 0.1294, 0.1249, 0.2154, 0.1409, 0.1352],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0996, 0.0028, 0.1130, 0.0885, 0.1096, 0.4990, 0.0874],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1491, 0.0844, 0.1156, 0.1669, 0.1454, 0.1467, 0.1920],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.47919333333333336 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.47919333333333336 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
printing an ep nov before normalisation:  45.85865020751953
printing an ep nov before normalisation:  12.705196142196655
printing an ep nov before normalisation:  61.27185685416285
maxi score, test score, baseline:  -0.47919333333333336 0.6916666666666668 0.6916666666666668
maxi score, test score, baseline:  -0.47919333333333336 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.136]
 [-0.147]
 [-0.144]
 [-0.141]
 [-0.141]
 [-0.143]
 [-0.141]] [[45.335]
 [38.158]
 [45.447]
 [45.243]
 [46.811]
 [46.957]
 [45.243]] [[0.168]
 [0.075]
 [0.161]
 [0.161]
 [0.179]
 [0.179]
 [0.161]]
printing an ep nov before normalisation:  35.22534594422008
maxi score, test score, baseline:  -0.47919333333333336 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8743187
maxi score, test score, baseline:  -0.47919333333333336 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.8465387125174
printing an ep nov before normalisation:  39.90097942197073
printing an ep nov before normalisation:  42.478405742058854
maxi score, test score, baseline:  -0.47919333333333336 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.330057621016216
actor:  0 policy actor:  0  step number:  46 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  46 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.47640666666666676 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.81107692088861
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.47640666666666664 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.236924171447754
maxi score, test score, baseline:  -0.47640666666666664 0.6916666666666668 0.6916666666666668
Printing some Q and Qe and total Qs values:  [[0.778]
 [0.775]
 [0.785]
 [0.779]
 [0.761]
 [0.76 ]
 [0.701]] [[31.564]
 [31.886]
 [31.823]
 [31.255]
 [27.614]
 [27.776]
 [23.553]] [[0.778]
 [0.775]
 [0.785]
 [0.779]
 [0.761]
 [0.76 ]
 [0.701]]
maxi score, test score, baseline:  -0.47640666666666664 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.47640666666666664 0.6916666666666668 0.6916666666666668
printing an ep nov before normalisation:  61.18373744529433
printing an ep nov before normalisation:  44.467805710904024
line 256 mcts: sample exp_bonus 49.38111034678073
maxi score, test score, baseline:  -0.47640666666666664 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.47640666666666664 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.081]
 [ 0.226]
 [ 0.081]
 [ 0.081]
 [ 0.081]
 [-0.001]
 [ 0.162]] [[33.93 ]
 [39.862]
 [33.93 ]
 [33.93 ]
 [33.93 ]
 [34.413]
 [36.303]] [[0.255]
 [0.46 ]
 [0.255]
 [0.255]
 [0.255]
 [0.178]
 [0.36 ]]
maxi score, test score, baseline:  -0.47640666666666664 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.47640666666666676 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.241483942592495
printing an ep nov before normalisation:  41.44613124053254
siam score:  -0.8646308
printing an ep nov before normalisation:  55.45142615980048
Printing some Q and Qe and total Qs values:  [[0.131]
 [0.302]
 [0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]] [[32.303]
 [53.86 ]
 [32.303]
 [32.303]
 [32.303]
 [32.303]
 [32.303]] [[0.43 ]
 [1.003]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  66 total reward:  0.07333333333333247  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  70.63232389725806
Printing some Q and Qe and total Qs values:  [[0.73 ]
 [0.755]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]] [[34.889]
 [37.45 ]
 [34.889]
 [34.889]
 [34.889]
 [34.889]
 [34.889]] [[1.842]
 [1.95 ]
 [1.842]
 [1.842]
 [1.842]
 [1.842]
 [1.842]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  44.25629041184869
maxi score, test score, baseline:  -0.47640666666666676 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.300645031548836
printing an ep nov before normalisation:  31.214015802952385
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.84177083028958
actor:  1 policy actor:  1  step number:  65 total reward:  0.17333333333333234  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.47640666666666676 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.47640666666666676 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.47640666666666676 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.3199999999999994  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.47640666666666676 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.27192148477067
maxi score, test score, baseline:  -0.47640666666666676 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.0799999999999993  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  62.883788956499025
actor:  1 policy actor:  1  step number:  55 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  36.679049004126135
printing an ep nov before normalisation:  58.9170744241216
Printing some Q and Qe and total Qs values:  [[0.4  ]
 [0.182]
 [0.04 ]
 [0.04 ]
 [0.04 ]
 [0.04 ]
 [0.04 ]] [[43.992]
 [32.268]
 [34.074]
 [34.074]
 [34.074]
 [34.074]
 [34.074]] [[0.86 ]
 [0.441]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.47640666666666676 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.52650005106329
maxi score, test score, baseline:  -0.47640666666666676 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.35822186903142
printing an ep nov before normalisation:  29.932258129119873
maxi score, test score, baseline:  -0.47640666666666676 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  46 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.446]
 [0.45 ]
 [0.451]
 [0.454]
 [0.454]
 [0.454]] [[33.587]
 [41.183]
 [39.424]
 [39.726]
 [33.587]
 [33.587]
 [33.587]] [[1.478]
 [1.702]
 [1.653]
 [1.663]
 [1.478]
 [1.478]
 [1.478]]
printing an ep nov before normalisation:  17.517557740211487
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.521]
 [0.504]
 [0.52 ]
 [0.504]
 [0.504]
 [0.504]] [[35.936]
 [38.605]
 [35.936]
 [31.419]
 [35.936]
 [35.936]
 [35.936]] [[1.655]
 [1.758]
 [1.655]
 [1.527]
 [1.655]
 [1.655]
 [1.655]]
maxi score, test score, baseline:  -0.47640666666666676 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.10666666666666613  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  43.2971975086256
maxi score, test score, baseline:  -0.47640666666666676 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.5869722366333
printing an ep nov before normalisation:  55.69711984936467
maxi score, test score, baseline:  -0.47640666666666676 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.58073356139215
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.47640666666666676 0.6916666666666668 0.6916666666666668
printing an ep nov before normalisation:  62.88934375087793
actions average: 
K:  4  action  0 :  tensor([0.4769, 0.0842, 0.0783, 0.0856, 0.1253, 0.0850, 0.0647],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0088, 0.9174, 0.0106, 0.0146, 0.0063, 0.0185, 0.0238],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0945, 0.1902, 0.4798, 0.0407, 0.0269, 0.0941, 0.0737],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1155, 0.1876, 0.0887, 0.2220, 0.1413, 0.1432, 0.1017],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2290, 0.0281, 0.0773, 0.1195, 0.3110, 0.1273, 0.1077],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2065, 0.0537, 0.1164, 0.1494, 0.1299, 0.2283, 0.1158],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1817, 0.0564, 0.0985, 0.1625, 0.1329, 0.1305, 0.2375],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  53.72689954184003
printing an ep nov before normalisation:  57.24265758152085
actor:  1 policy actor:  1  step number:  41 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  59 total reward:  0.30666666666666587  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.16476988053235
printing an ep nov before normalisation:  42.853359364358994
printing an ep nov before normalisation:  43.90554428100586
actor:  1 policy actor:  1  step number:  61 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.47640666666666676 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.47640666666666676 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.36975936530368
maxi score, test score, baseline:  -0.47640666666666676 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.47640666666666676 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0504958955464073
maxi score, test score, baseline:  -0.47640666666666676 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.2982, 0.0106, 0.1160, 0.1103, 0.1517, 0.1583, 0.1549],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0101, 0.9381, 0.0057, 0.0087, 0.0074, 0.0057, 0.0242],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1339, 0.0116, 0.3085, 0.1014, 0.1369, 0.1854, 0.1224],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1139, 0.1404, 0.0812, 0.3040, 0.1040, 0.0909, 0.1655],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1566, 0.0325, 0.1033, 0.1111, 0.3377, 0.1389, 0.1200],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1840, 0.0238, 0.1445, 0.1291, 0.1643, 0.1947, 0.1596],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1392, 0.0184, 0.1015, 0.1153, 0.1452, 0.1471, 0.3335],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.47640666666666676 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.18 ]
 [ 0.133]
 [-0.095]
 [ 0.173]
 [ 0.213]
 [ 0.173]
 [ 0.106]] [[32.177]
 [38.619]
 [28.962]
 [33.215]
 [42.72 ]
 [33.215]
 [32.807]] [[0.618]
 [0.754]
 [0.251]
 [0.64 ]
 [0.95 ]
 [0.64 ]
 [0.562]]
maxi score, test score, baseline:  -0.47640666666666676 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  54 total reward:  0.3266666666666661  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  35.967297388966735
printing an ep nov before normalisation:  35.82582704081336
printing an ep nov before normalisation:  53.17138942125936
maxi score, test score, baseline:  -0.47375333333333336 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86614245
maxi score, test score, baseline:  -0.47375333333333336 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  47 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  65 total reward:  0.14666666666666628  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  33.45510366853189
siam score:  -0.8643434
printing an ep nov before normalisation:  41.35513616129801
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  66 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4710866666666667 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.025644302368164
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.056631088256836
actor:  1 policy actor:  1  step number:  58 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.4710866666666667 0.6916666666666668 0.6916666666666668
printing an ep nov before normalisation:  55.469834644478425
printing an ep nov before normalisation:  31.467667860557476
actor:  0 policy actor:  0  step number:  58 total reward:  0.12666666666666626  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.4688333333333334 0.6916666666666668 0.6916666666666668
printing an ep nov before normalisation:  42.70862707408436
actor:  0 policy actor:  0  step number:  49 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  35.8137857441308
maxi score, test score, baseline:  -0.46608666666666676 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  62 total reward:  0.16666666666666563  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4637533333333334 0.6916666666666668 0.6916666666666668
maxi score, test score, baseline:  -0.4637533333333334 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.59316510850404
maxi score, test score, baseline:  -0.4637533333333334 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.15667154158207
printing an ep nov before normalisation:  36.65202731583676
actor:  1 policy actor:  1  step number:  61 total reward:  0.09333333333333282  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4637533333333334 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4637533333333334 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4637533333333334 0.6916666666666668 0.6916666666666668
maxi score, test score, baseline:  -0.4637533333333334 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.6890260904593
maxi score, test score, baseline:  -0.4637533333333334 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  91.0659699089456
maxi score, test score, baseline:  -0.4637533333333334 0.6916666666666668 0.6916666666666668
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.089]
 [0.089]
 [0.089]
 [0.089]
 [0.089]
 [0.089]] [[40.677]
 [40.677]
 [40.677]
 [40.677]
 [40.677]
 [40.677]
 [40.677]] [[0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.4133333333333328  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4637533333333334 0.6916666666666668 0.6916666666666668
maxi score, test score, baseline:  -0.4637533333333334 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.826]
 [0.574]
 [0.592]
 [0.577]
 [0.654]
 [0.853]] [[41.556]
 [43.193]
 [40.925]
 [42.804]
 [50.54 ]
 [45.802]
 [33.368]] [[1.487]
 [1.829]
 [1.494]
 [1.581]
 [1.847]
 [1.751]
 [1.498]]
printing an ep nov before normalisation:  37.56706587480795
maxi score, test score, baseline:  -0.4637533333333334 0.6916666666666668 0.6916666666666668
maxi score, test score, baseline:  -0.4637533333333334 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.31207449939813
siam score:  -0.8647986
maxi score, test score, baseline:  -0.4637533333333334 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.86056580563443
actor:  1 policy actor:  1  step number:  61 total reward:  0.053333333333332233  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  26.632181075245622
maxi score, test score, baseline:  -0.4637533333333334 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.19333333333333313  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  40.89848600659877
maxi score, test score, baseline:  -0.4637533333333334 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.2545389754487
actor:  0 policy actor:  0  step number:  55 total reward:  0.18666666666666587  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4613800000000001 0.6916666666666668 0.6916666666666668
printing an ep nov before normalisation:  45.27756205512106
printing an ep nov before normalisation:  27.53162384033203
maxi score, test score, baseline:  -0.4613800000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4613800000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.80745144574798
printing an ep nov before normalisation:  33.46138776467871
printing an ep nov before normalisation:  21.14679758123204
siam score:  -0.86516184
maxi score, test score, baseline:  -0.4613800000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.90515762536587
maxi score, test score, baseline:  -0.4613800000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.999198022269535
actor:  1 policy actor:  1  step number:  64 total reward:  0.12666666666666626  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  36.60917058578985
printing an ep nov before normalisation:  35.09505299202103
siam score:  -0.8667729
printing an ep nov before normalisation:  45.47372364239047
maxi score, test score, baseline:  -0.46138 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.031]
 [0.292]
 [0.192]
 [0.176]
 [0.053]
 [0.165]
 [0.224]] [[25.363]
 [33.666]
 [27.808]
 [27.738]
 [29.897]
 [29.609]
 [26.384]] [[0.991]
 [1.889]
 [1.339]
 [1.318]
 [1.36 ]
 [1.45 ]
 [1.263]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.46138 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.46138 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  44 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  28.214869499206543
actor:  1 policy actor:  1  step number:  53 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.45875333333333335 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.45875333333333335 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.45875333333333335 0.6916666666666668 0.6916666666666668
maxi score, test score, baseline:  -0.45875333333333335 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.45875333333333335 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.45875333333333335 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.761175632476807
printing an ep nov before normalisation:  23.574222642598997
actor:  1 policy actor:  1  step number:  72 total reward:  0.03333333333333255  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  46 total reward:  0.35333333333333317  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.643]
 [0.712]
 [0.643]
 [0.643]
 [0.643]
 [0.643]
 [0.643]] [[45.488]
 [45.572]
 [45.488]
 [45.488]
 [45.488]
 [45.488]
 [45.488]] [[1.118]
 [1.188]
 [1.118]
 [1.118]
 [1.118]
 [1.118]
 [1.118]]
actor:  1 policy actor:  1  step number:  66 total reward:  0.24666666666666603  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.45875333333333335 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.45875333333333335 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.93245348381232
printing an ep nov before normalisation:  46.934421488713134
printing an ep nov before normalisation:  27.11991756493602
maxi score, test score, baseline:  -0.45875333333333335 0.6916666666666668 0.6916666666666668
printing an ep nov before normalisation:  50.59015251970892
actor:  1 policy actor:  1  step number:  57 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.45875333333333335 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.45875333333333335 0.6916666666666668 0.6916666666666668
maxi score, test score, baseline:  -0.45875333333333335 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.730373674885726
maxi score, test score, baseline:  -0.45875333333333335 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.4207, 0.0293, 0.0978, 0.1030, 0.1280, 0.1144, 0.1067],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0032, 0.9787, 0.0024, 0.0024, 0.0011, 0.0014, 0.0109],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1347, 0.0253, 0.2650, 0.1601, 0.1244, 0.1615, 0.1290],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1143, 0.1252, 0.1040, 0.2731, 0.1302, 0.1365, 0.1166],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1362, 0.0238, 0.0921, 0.1267, 0.3681, 0.1278, 0.1254],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1546, 0.0148, 0.1716, 0.1312, 0.1206, 0.2771, 0.1302],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2020, 0.0704, 0.1231, 0.1472, 0.1463, 0.1594, 0.1516],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.45875333333333335 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.45875333333333335 0.6916666666666668 0.6916666666666668
actor:  1 policy actor:  1  step number:  73 total reward:  0.053333333333332456  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  47.99458484042808
printing an ep nov before normalisation:  0.22730572100158497
maxi score, test score, baseline:  -0.45875333333333335 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.88443702148036
actor:  1 policy actor:  1  step number:  49 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  61.03858683483942
printing an ep nov before normalisation:  36.39476710994259
printing an ep nov before normalisation:  43.20074271047856
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.45875333333333335 0.6916666666666668 0.6916666666666668
maxi score, test score, baseline:  -0.45875333333333335 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.45875333333333335 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.45875333333333335 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.206666666666666  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.45875333333333346 0.6916666666666668 0.6916666666666668
maxi score, test score, baseline:  -0.45875333333333346 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.45875333333333346 0.6916666666666668 0.6916666666666668
actor:  1 policy actor:  1  step number:  44 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.868779143528776
Printing some Q and Qe and total Qs values:  [[0.259]
 [0.259]
 [0.259]
 [0.259]
 [0.259]
 [0.259]
 [0.259]] [[28.092]
 [28.092]
 [28.092]
 [28.092]
 [28.092]
 [28.092]
 [28.092]] [[0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  42 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  48 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.048]
 [-0.027]
 [-0.047]
 [-0.051]
 [-0.05 ]
 [-0.051]
 [-0.051]] [[60.787]
 [45.468]
 [55.407]
 [54.903]
 [56.822]
 [54.903]
 [54.903]] [[0.255]
 [0.152]
 [0.212]
 [0.204]
 [0.22 ]
 [0.204]
 [0.204]]
printing an ep nov before normalisation:  40.91434374896012
maxi score, test score, baseline:  -0.45875333333333346 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.71235159386197
printing an ep nov before normalisation:  36.95812128681858
maxi score, test score, baseline:  -0.45875333333333346 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.45875333333333346 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.608170174133704
maxi score, test score, baseline:  -0.45875333333333346 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.454882497247667
maxi score, test score, baseline:  -0.45875333333333346 0.6916666666666668 0.6916666666666668
maxi score, test score, baseline:  -0.45875333333333346 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.2133333333333325  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.45875333333333346 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.45875333333333346 0.6916666666666668 0.6916666666666668
siam score:  -0.8727194
maxi score, test score, baseline:  -0.45875333333333346 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.38037841295815
line 256 mcts: sample exp_bonus 55.62773421873215
actor:  1 policy actor:  1  step number:  49 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  52 total reward:  0.3133333333333328  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.458]
 [0.473]
 [0.483]
 [0.486]
 [0.488]
 [0.493]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.466]
 [0.458]
 [0.473]
 [0.483]
 [0.486]
 [0.488]
 [0.493]]
actions average: 
K:  2  action  0 :  tensor([0.2926, 0.0273, 0.1380, 0.1185, 0.1392, 0.1467, 0.1377],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0223, 0.9187, 0.0093, 0.0114, 0.0071, 0.0064, 0.0250],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1150, 0.0126, 0.2999, 0.1061, 0.1223, 0.2229, 0.1213],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1233, 0.1488, 0.0816, 0.3016, 0.1295, 0.1151, 0.1001],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1345, 0.0680, 0.1022, 0.1018, 0.3524, 0.1166, 0.1245],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1320, 0.0168, 0.1949, 0.1147, 0.1182, 0.2928, 0.1305],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0926, 0.3114, 0.1020, 0.0968, 0.0928, 0.1217, 0.1828],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  51 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.45875333333333346 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.40875458228301
maxi score, test score, baseline:  -0.45875333333333346 0.6916666666666668 0.6916666666666668
printing an ep nov before normalisation:  58.073268617981576
maxi score, test score, baseline:  -0.45875333333333346 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.6115082996661
printing an ep nov before normalisation:  23.2602858543396
printing an ep nov before normalisation:  0.8300078013728296
actor:  0 policy actor:  0  step number:  53 total reward:  0.26666666666666594  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4562200000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.45622000000000007 0.6916666666666668 0.6916666666666668
maxi score, test score, baseline:  -0.45622000000000007 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.8644777
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.45622000000000007 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.45622000000000007 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.42145703029167
actor:  1 policy actor:  1  step number:  56 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  33.83710031909465
maxi score, test score, baseline:  -0.45622000000000007 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.041 0.082 0.143 0.143 0.082 0.061 0.449]
maxi score, test score, baseline:  -0.45622000000000007 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.22239446887510894
maxi score, test score, baseline:  -0.45622000000000007 0.6916666666666668 0.6916666666666668
actor:  1 policy actor:  1  step number:  89 total reward:  0.11999999999999822  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.45622000000000007 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]] [[38.489]
 [38.489]
 [38.489]
 [38.489]
 [38.489]
 [38.489]
 [38.489]] [[0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.734]
 [0.892]
 [0.734]
 [0.734]
 [0.734]
 [0.734]
 [0.734]] [[36.288]
 [38.722]
 [36.288]
 [36.288]
 [36.288]
 [36.288]
 [36.288]] [[0.734]
 [0.892]
 [0.734]
 [0.734]
 [0.734]
 [0.734]
 [0.734]]
siam score:  -0.86804795
maxi score, test score, baseline:  -0.45622000000000007 0.6916666666666668 0.6916666666666668
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.45622000000000007 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.46071703834524
actor:  0 policy actor:  0  step number:  45 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.4534466666666668 0.6916666666666668 0.6916666666666668
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.3760844426828
line 256 mcts: sample exp_bonus 55.94369888305664
maxi score, test score, baseline:  -0.4534466666666668 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4534466666666668 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.57882480439564
actor:  1 policy actor:  1  step number:  55 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  64.18124156607885
printing an ep nov before normalisation:  65.5680509073516
printing an ep nov before normalisation:  35.574052049210835
maxi score, test score, baseline:  -0.4534466666666668 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4534466666666668 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.43598997839182
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  40.96934435179721
maxi score, test score, baseline:  -0.4534466666666668 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  44 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  53.4289351960007
printing an ep nov before normalisation:  53.79582402446559
using explorer policy with actor:  1
printing an ep nov before normalisation:  60.13849095718026
maxi score, test score, baseline:  -0.4506600000000001 0.6916666666666668 0.6916666666666668
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.381]
 [0.342]
 [0.427]
 [0.345]
 [0.35 ]
 [0.427]] [[44.159]
 [46.995]
 [48.836]
 [44.159]
 [49.312]
 [48.33 ]
 [44.159]] [[1.364]
 [1.411]
 [1.433]
 [1.364]
 [1.451]
 [1.424]
 [1.364]]
printing an ep nov before normalisation:  0.035118752348353155
actor:  1 policy actor:  1  step number:  33 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.4506600000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 12.337169854249021
actions average: 
K:  2  action  0 :  tensor([0.4062, 0.0813, 0.0933, 0.1004, 0.1192, 0.0960, 0.1035],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0069, 0.9313, 0.0070, 0.0108, 0.0051, 0.0053, 0.0336],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1181, 0.0095, 0.3915, 0.0956, 0.1039, 0.1729, 0.1086],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1124, 0.0513, 0.1110, 0.3723, 0.1124, 0.1280, 0.1126],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1583, 0.0167, 0.1216, 0.1190, 0.3104, 0.1488, 0.1253],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1328, 0.0674, 0.1387, 0.1123, 0.1436, 0.2874, 0.1178],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1265, 0.1723, 0.1087, 0.1276, 0.1226, 0.1262, 0.2162],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  16.307239651290075
siam score:  -0.8735373
actor:  1 policy actor:  1  step number:  57 total reward:  0.38666666666666594  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  60 total reward:  0.5266666666666671  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  13.315423882420147
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4506600000000001 0.6916666666666668 0.6916666666666668
maxi score, test score, baseline:  -0.4506600000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.90303204339193
printing an ep nov before normalisation:  46.99376496412372
maxi score, test score, baseline:  -0.4506600000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.45066000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.45066000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.45066000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.49140219155136
printing an ep nov before normalisation:  48.309041295097046
Printing some Q and Qe and total Qs values:  [[0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]] [[45.125]
 [45.125]
 [45.125]
 [45.125]
 [45.125]
 [45.125]
 [45.125]] [[2.183]
 [2.183]
 [2.183]
 [2.183]
 [2.183]
 [2.183]
 [2.183]]
printing an ep nov before normalisation:  44.38658934101166
printing an ep nov before normalisation:  41.02185531758901
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  34 total reward:  0.5  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.4476600000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.44766000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.44766000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.44766000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.143]
 [-0.038]
 [-0.03 ]
 [-0.034]
 [ 0.143]
 [-0.006]
 [ 0.042]] [[38.322]
 [43.533]
 [44.115]
 [45.558]
 [38.322]
 [44.767]
 [40.743]] [[0.9  ]
 [0.963]
 [0.998]
 [1.061]
 [0.9  ]
 [1.053]
 [0.912]]
maxi score, test score, baseline:  -0.44766000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.006240753747086
Printing some Q and Qe and total Qs values:  [[0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]] [[45.934]
 [45.934]
 [45.934]
 [45.934]
 [45.934]
 [45.934]
 [45.934]] [[1.452]
 [1.452]
 [1.452]
 [1.452]
 [1.452]
 [1.452]
 [1.452]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.0914],
        [ 0.0973],
        [-0.2541],
        [-0.1173],
        [ 0.6492],
        [-0.0000],
        [-0.3683],
        [ 0.1391],
        [ 0.2702]], dtype=torch.float64)
0.9117834164999999 0.9117834164999999
-0.070771701198 -0.1621709203605455
-0.08423175439800001 0.013057111378164746
-0.09703970119800001 -0.3511344165689092
-0.057834381198 -0.17513494824570486
-0.058094434398 0.5910598148691184
-0.7376239200000001 -0.7376239200000001
-0.032346567066 -0.4006087430824628
-0.045414567066 0.09364621010803184
-0.09703970119800001 0.173189992796453
maxi score, test score, baseline:  -0.44766000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.44766000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  47 total reward:  0.2933333333333331  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]]
maxi score, test score, baseline:  -0.44766000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.243]
 [0.043]
 [0.043]
 [0.21 ]
 [0.043]
 [0.043]] [[36.361]
 [37.208]
 [37.764]
 [37.764]
 [36.067]
 [37.764]
 [37.764]] [[1.309]
 [1.264]
 [1.093]
 [1.093]
 [1.174]
 [1.093]
 [1.093]]
siam score:  -0.8644101
maxi score, test score, baseline:  -0.44766000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.44766000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.845841659961394
printing an ep nov before normalisation:  49.44182645824199
actor:  1 policy actor:  1  step number:  59 total reward:  0.11999999999999966  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.44766000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  52 total reward:  0.21999999999999975  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.077]
 [-0.042]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]] [[27.729]
 [54.451]
 [27.729]
 [27.729]
 [27.729]
 [27.729]
 [27.729]] [[0.143]
 [0.45 ]
 [0.143]
 [0.143]
 [0.143]
 [0.143]
 [0.143]]
Printing some Q and Qe and total Qs values:  [[-0.064]
 [-0.035]
 [-0.064]
 [-0.064]
 [-0.064]
 [-0.064]
 [-0.064]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.064]
 [-0.035]
 [-0.064]
 [-0.064]
 [-0.064]
 [-0.064]
 [-0.064]]
maxi score, test score, baseline:  -0.44522000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.44522000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.66398196147694
printing an ep nov before normalisation:  55.6091085182275
Printing some Q and Qe and total Qs values:  [[-0.05 ]
 [-0.031]
 [-0.051]
 [-0.045]
 [-0.052]
 [-0.05 ]
 [-0.052]] [[32.203]
 [31.108]
 [31.22 ]
 [34.216]
 [31.5  ]
 [31.224]
 [31.756]] [[0.354]
 [0.348]
 [0.33 ]
 [0.407]
 [0.336]
 [0.331]
 [0.342]]
Printing some Q and Qe and total Qs values:  [[0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]] [[27.654]
 [27.654]
 [27.654]
 [27.654]
 [27.654]
 [27.654]
 [27.654]] [[0.685]
 [0.685]
 [0.685]
 [0.685]
 [0.685]
 [0.685]
 [0.685]]
Printing some Q and Qe and total Qs values:  [[ 0.064]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.03 ]
 [-0.028]
 [-0.028]] [[46.796]
 [43.404]
 [43.404]
 [43.404]
 [48.163]
 [43.404]
 [43.404]] [[0.693]
 [0.531]
 [0.531]
 [0.531]
 [0.627]
 [0.531]
 [0.531]]
printing an ep nov before normalisation:  49.52776194434055
printing an ep nov before normalisation:  33.279131132695674
actor:  1 policy actor:  1  step number:  55 total reward:  0.23999999999999966  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.44522000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.44522000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.44522000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.40125655169848
maxi score, test score, baseline:  -0.44522000000000006 0.6916666666666668 0.6916666666666668
maxi score, test score, baseline:  -0.44522000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.44522000000000006 0.6916666666666668 0.6916666666666668
maxi score, test score, baseline:  -0.44522000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  38 total reward:  0.5000000000000001  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.565]
 [0.355]
 [0.355]
 [0.355]
 [0.355]
 [0.355]] [[28.857]
 [37.095]
 [28.857]
 [28.857]
 [28.857]
 [28.857]
 [28.857]] [[1.569]
 [2.128]
 [1.569]
 [1.569]
 [1.569]
 [1.569]
 [1.569]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.3599999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.44222000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.44222000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.44222000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.61801583925862
actor:  1 policy actor:  1  step number:  53 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.44222000000000006 0.6916666666666668 0.6916666666666668
maxi score, test score, baseline:  -0.44222000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.44222000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.950047413549946
Printing some Q and Qe and total Qs values:  [[0.169]
 [0.134]
 [0.169]
 [0.169]
 [0.216]
 [0.169]
 [0.169]] [[70.19 ]
 [67.149]
 [70.19 ]
 [70.19 ]
 [64.084]
 [70.19 ]
 [70.19 ]] [[1.83 ]
 [1.696]
 [1.83 ]
 [1.83 ]
 [1.68 ]
 [1.83 ]
 [1.83 ]]
actor:  1 policy actor:  1  step number:  61 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  54 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.44222000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.44222000000000006 0.6916666666666668 0.6916666666666668
actions average: 
K:  4  action  0 :  tensor([0.2926, 0.0951, 0.1248, 0.1051, 0.1479, 0.0998, 0.1348],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0245, 0.8971, 0.0265, 0.0136, 0.0108, 0.0077, 0.0197],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1510, 0.0787, 0.1970, 0.1261, 0.1431, 0.1956, 0.1085],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1231, 0.1040, 0.0936, 0.2284, 0.1798, 0.1156, 0.1555],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2198, 0.0223, 0.1280, 0.1226, 0.2133, 0.1491, 0.1450],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1561, 0.1758, 0.1143, 0.1130, 0.1757, 0.1439, 0.1214],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1091, 0.1199, 0.0853, 0.1313, 0.1524, 0.1100, 0.2922],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  39.35691833496094
maxi score, test score, baseline:  -0.44222000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.22722300400663
actor:  1 policy actor:  1  step number:  44 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  23.8026762008667
printing an ep nov before normalisation:  47.05188665198163
maxi score, test score, baseline:  -0.44222000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.2733333333333331  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  25.19665130221167
using explorer policy with actor:  1
siam score:  -0.8556549
maxi score, test score, baseline:  -0.44222000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  66 total reward:  0.29999999999999893  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.44222000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.025]
 [-0.002]
 [-0.025]
 [-0.027]
 [-0.027]
 [-0.026]
 [ 0.155]] [[34.15 ]
 [33.937]
 [33.993]
 [33.489]
 [33.683]
 [33.908]
 [51.69 ]] [[0.473]
 [0.491]
 [0.469]
 [0.453]
 [0.458]
 [0.466]
 [1.146]]
printing an ep nov before normalisation:  41.42918109995464
printing an ep nov before normalisation:  47.462493103768665
maxi score, test score, baseline:  -0.44222000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.548]
 [0.642]
 [0.498]
 [0.498]
 [0.552]
 [0.293]
 [0.435]] [[42.248]
 [51.763]
 [39.945]
 [39.945]
 [47.734]
 [38.334]
 [36.008]] [[0.548]
 [0.642]
 [0.498]
 [0.498]
 [0.552]
 [0.293]
 [0.435]]
maxi score, test score, baseline:  -0.44222000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.44222000000000006 0.6916666666666668 0.6916666666666668
maxi score, test score, baseline:  -0.44222000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.033120997852904566
Printing some Q and Qe and total Qs values:  [[-0.105]
 [-0.087]
 [-0.105]
 [-0.105]
 [-0.105]
 [-0.105]
 [-0.105]] [[29.38 ]
 [45.975]
 [29.38 ]
 [29.38 ]
 [29.38 ]
 [29.38 ]
 [29.38 ]] [[0.345]
 [0.859]
 [0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]]
maxi score, test score, baseline:  -0.44222000000000006 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.18666666666666643  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  50 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  35.6543755531311
maxi score, test score, baseline:  -0.4397000000000001 0.6916666666666668 0.6916666666666668
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4397000000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4397000000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4397000000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4397000000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.4397000000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.075]
 [-0.061]
 [-0.081]
 [-0.102]
 [-0.101]
 [-0.1  ]
 [-0.098]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.075]
 [-0.061]
 [-0.081]
 [-0.102]
 [-0.101]
 [-0.1  ]
 [-0.098]]
maxi score, test score, baseline:  -0.4397000000000001 0.6916666666666668 0.6916666666666668
maxi score, test score, baseline:  -0.4397000000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.741]
 [0.594]
 [0.597]
 [0.594]
 [0.594]
 [0.579]] [[38.02 ]
 [31.299]
 [32.534]
 [36.693]
 [32.534]
 [32.534]
 [36.818]] [[0.561]
 [0.741]
 [0.594]
 [0.597]
 [0.594]
 [0.594]
 [0.579]]
Printing some Q and Qe and total Qs values:  [[0.361]
 [0.668]
 [0.461]
 [0.511]
 [0.468]
 [0.45 ]
 [0.593]] [[31.067]
 [28.804]
 [32.782]
 [32.868]
 [40.25 ]
 [37.163]
 [30.766]] [[0.523]
 [0.805]
 [0.642]
 [0.693]
 [0.73 ]
 [0.679]
 [0.752]]
printing an ep nov before normalisation:  33.70555400848389
printing an ep nov before normalisation:  38.1160306930542
printing an ep nov before normalisation:  35.57324043290613
actor:  1 policy actor:  1  step number:  67 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  48 total reward:  0.25999999999999934  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.43718 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.43718 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.43718 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.13289204858210724
printing an ep nov before normalisation:  47.77666907327225
printing an ep nov before normalisation:  51.219998593418396
printing an ep nov before normalisation:  57.2712300881203
maxi score, test score, baseline:  -0.4371800000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.8524962
maxi score, test score, baseline:  -0.4371800000000001 0.6916666666666668 0.6916666666666668
printing an ep nov before normalisation:  67.2589501352675
actor:  1 policy actor:  1  step number:  47 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.245]
 [0.288]
 [0.245]
 [0.245]
 [0.245]
 [0.245]
 [0.245]] [[46.055]
 [46.182]
 [46.055]
 [46.055]
 [46.055]
 [46.055]
 [46.055]] [[0.546]
 [0.591]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]]
line 256 mcts: sample exp_bonus 46.22100163199334
Printing some Q and Qe and total Qs values:  [[0.338]
 [0.519]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]] [[27.97 ]
 [39.586]
 [27.97 ]
 [27.97 ]
 [27.97 ]
 [27.97 ]
 [27.97 ]] [[0.484]
 [0.784]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]]
UNIT TEST: sample policy line 217 mcts : [0.02  0.592 0.02  0.02  0.02  0.102 0.224]
Printing some Q and Qe and total Qs values:  [[-0.081]
 [-0.079]
 [-0.081]
 [-0.085]
 [-0.084]
 [-0.089]
 [-0.085]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.081]
 [-0.079]
 [-0.081]
 [-0.085]
 [-0.084]
 [-0.089]
 [-0.085]]
maxi score, test score, baseline:  -0.4371800000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.976884551909052
actor:  1 policy actor:  1  step number:  58 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  30.257956957012915
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4371800000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4371800000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.82529164269085
actor:  1 policy actor:  1  step number:  58 total reward:  0.31333333333333246  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4371800000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([0.4429, 0.0238, 0.1245, 0.0961, 0.1082, 0.0928, 0.1116],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0090, 0.9078, 0.0119, 0.0175, 0.0074, 0.0070, 0.0394],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1541, 0.0715, 0.2353, 0.1305, 0.1318, 0.1325, 0.1442],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2005, 0.0309, 0.1419, 0.1847, 0.1547, 0.1410, 0.1464],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1814, 0.0284, 0.1159, 0.1302, 0.3184, 0.1171, 0.1086],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1420, 0.0201, 0.1434, 0.1549, 0.1560, 0.2239, 0.1598],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1215, 0.1028, 0.0998, 0.1216, 0.1163, 0.1073, 0.3307],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.4371800000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.11999999999999933  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.4371800000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.047]
 [-0.012]
 [-0.044]
 [-0.046]
 [-0.038]
 [-0.04 ]
 [-0.047]] [[31.131]
 [36.428]
 [30.133]
 [30.488]
 [31.786]
 [32.083]
 [30.25 ]] [[0.285]
 [0.424]
 [0.269]
 [0.273]
 [0.307]
 [0.311]
 [0.268]]
siam score:  -0.855648
maxi score, test score, baseline:  -0.4371800000000001 0.6916666666666668 0.6916666666666668
maxi score, test score, baseline:  -0.4371800000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4371800000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.416680563327105
printing an ep nov before normalisation:  62.06881615235413
actions average: 
K:  0  action  0 :  tensor([0.4715, 0.0074, 0.0672, 0.0599, 0.2302, 0.0759, 0.0879],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0046, 0.9673, 0.0039, 0.0061, 0.0016, 0.0019, 0.0146],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1262, 0.0187, 0.2613, 0.1128, 0.1279, 0.2290, 0.1241],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1641, 0.0059, 0.1691, 0.1486, 0.1801, 0.1801, 0.1521],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2097, 0.0029, 0.1226, 0.0979, 0.3349, 0.1144, 0.1175],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1378, 0.0018, 0.1639, 0.1185, 0.1613, 0.2542, 0.1624],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1328, 0.1529, 0.1221, 0.1042, 0.1281, 0.1364, 0.2235],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  50 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  57 total reward:  0.17333333333333256  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
siam score:  -0.86071754
maxi score, test score, baseline:  -0.4371800000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.07689359446544586
printing an ep nov before normalisation:  33.34751405302258
Printing some Q and Qe and total Qs values:  [[0.774]
 [0.815]
 [0.592]
 [0.631]
 [0.796]
 [0.646]
 [0.708]] [[28.211]
 [32.399]
 [25.539]
 [26.924]
 [34.323]
 [27.435]
 [25.221]] [[0.774]
 [0.815]
 [0.592]
 [0.631]
 [0.796]
 [0.646]
 [0.708]]
printing an ep nov before normalisation:  57.571135802422454
printing an ep nov before normalisation:  56.36231059635305
actor:  1 policy actor:  1  step number:  62 total reward:  0.28666666666666574  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4371800000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  43 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.43435333333333337 0.6916666666666668 0.6916666666666668
maxi score, test score, baseline:  -0.43435333333333337 0.6916666666666668 0.6916666666666668
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.495]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]] [[54.045]
 [51.701]
 [54.045]
 [54.045]
 [54.045]
 [54.045]
 [54.045]] [[1.008]
 [1.055]
 [1.008]
 [1.008]
 [1.008]
 [1.008]
 [1.008]]
Printing some Q and Qe and total Qs values:  [[-0.008]
 [ 0.093]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.019]
 [-0.008]] [[37.933]
 [62.318]
 [37.933]
 [37.933]
 [37.933]
 [43.535]
 [37.933]] [[0.52 ]
 [1.282]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.66 ]
 [0.52 ]]
maxi score, test score, baseline:  -0.43435333333333337 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.62187184379332
maxi score, test score, baseline:  -0.43435333333333337 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.43435333333333337 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.43435333333333337 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.38666666666666605  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  57 total reward:  0.3333333333333328  reward:  1.0 rdn_beta:  1.667
siam score:  -0.86547524
actor:  0 policy actor:  0  step number:  49 total reward:  0.42666666666666664  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.4315000000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4315000000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4315000000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  36 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.4315000000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.5400000000000001  reward:  1.0 rdn_beta:  2.0
siam score:  -0.86192316
maxi score, test score, baseline:  -0.4315000000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4315000000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4315000000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.201518745070427
actor:  1 policy actor:  1  step number:  34 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.4315000000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4315000000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4315000000000001 0.6916666666666668 0.6916666666666668
Printing some Q and Qe and total Qs values:  [[ 0.034]
 [ 0.234]
 [-0.001]
 [ 0.082]
 [ 0.039]
 [ 0.048]
 [ 0.02 ]] [[29.046]
 [30.323]
 [28.824]
 [28.574]
 [28.561]
 [28.39 ]
 [28.336]] [[1.269]
 [1.573]
 [1.216]
 [1.279]
 [1.235]
 [1.23 ]
 [1.198]]
actor:  0 policy actor:  0  step number:  55 total reward:  0.3199999999999992  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  47.376370226292956
maxi score, test score, baseline:  -0.4288600000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
printing an ep nov before normalisation:  50.49225565251556
printing an ep nov before normalisation:  30.444281101226807
printing an ep nov before normalisation:  36.43656563861634
printing an ep nov before normalisation:  21.643367622565428
actor:  1 policy actor:  1  step number:  49 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4288600000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4288600000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.879600524902344
printing an ep nov before normalisation:  21.110589504241943
maxi score, test score, baseline:  -0.4288600000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.951998055788557
printing an ep nov before normalisation:  54.610579836076056
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4288600000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.032]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.039]] [[37.291]
 [36.359]
 [36.359]
 [36.359]
 [36.359]
 [36.359]
 [36.359]] [[1.968]
 [1.896]
 [1.896]
 [1.896]
 [1.896]
 [1.896]
 [1.896]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.009]
 [ 0.232]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]] [[40.026]
 [52.083]
 [40.026]
 [40.026]
 [40.026]
 [40.026]
 [40.026]] [[0.553]
 [1.029]
 [0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.553]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  71 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  0.667
siam score:  -0.86525285
maxi score, test score, baseline:  -0.4288600000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4288600000000001 0.6916666666666668 0.6916666666666668
printing an ep nov before normalisation:  47.022955952930545
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]] [[41.099]
 [41.099]
 [41.099]
 [41.099]
 [41.099]
 [41.099]
 [41.099]] [[1.509]
 [1.509]
 [1.509]
 [1.509]
 [1.509]
 [1.509]
 [1.509]]
line 256 mcts: sample exp_bonus 43.53661478191248
actor:  1 policy actor:  1  step number:  55 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.4288600000000001 0.6916666666666668 0.6916666666666668
maxi score, test score, baseline:  -0.4288600000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.76655261877322
actor:  1 policy actor:  1  step number:  56 total reward:  0.20666666666666633  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.013]
 [-0.019]
 [-0.037]
 [-0.038]
 [-0.037]
 [-0.037]
 [-0.037]] [[45.359]
 [41.23 ]
 [38.673]
 [39.719]
 [39.463]
 [38.49 ]
 [38.004]] [[0.548]
 [0.455]
 [0.384]
 [0.405]
 [0.4  ]
 [0.38 ]
 [0.37 ]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.013]
 [ 0.004]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.013]] [[33.748]
 [44.418]
 [33.748]
 [33.748]
 [33.748]
 [33.748]
 [33.748]] [[0.07 ]
 [0.138]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]]
maxi score, test score, baseline:  -0.4288600000000001 0.6916666666666668 0.6916666666666668
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.226]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]] [[57.549]
 [58.706]
 [57.549]
 [57.549]
 [57.549]
 [57.549]
 [57.549]] [[1.255]
 [1.226]
 [1.255]
 [1.255]
 [1.255]
 [1.255]
 [1.255]]
Starting evaluation
maxi score, test score, baseline:  -0.4288600000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.698]
 [0.535]
 [0.509]
 [0.56 ]
 [0.56 ]
 [0.535]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.524]
 [0.698]
 [0.535]
 [0.509]
 [0.56 ]
 [0.56 ]
 [0.535]]
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.784]
 [0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.695]
 [0.784]
 [0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  53.955334077010654
printing an ep nov before normalisation:  51.01062005652529
printing an ep nov before normalisation:  60.212715657892936
printing an ep nov before normalisation:  53.21537968766072
printing an ep nov before normalisation:  31.96634883348251
maxi score, test score, baseline:  -0.4288600000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 40.32544056789349
Printing some Q and Qe and total Qs values:  [[1.008]
 [1.008]
 [1.008]
 [1.008]
 [1.008]
 [1.008]
 [1.008]] [[48.018]
 [48.018]
 [48.018]
 [48.018]
 [48.018]
 [48.018]
 [48.018]] [[1.008]
 [1.008]
 [1.008]
 [1.008]
 [1.008]
 [1.008]
 [1.008]]
printing an ep nov before normalisation:  41.7367302129438
printing an ep nov before normalisation:  39.81742670707124
printing an ep nov before normalisation:  30.511947368166304
maxi score, test score, baseline:  -0.4288600000000001 0.6916666666666668 0.6916666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  25.749933129847165
actor:  0 policy actor:  0  step number:  28 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  2
actor:  1 policy actor:  1  step number:  52 total reward:  0.37999999999999945  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3728733333333335 0.6956666666666668 0.6956666666666668
Printing some Q and Qe and total Qs values:  [[-0.142]
 [-0.146]
 [-0.135]
 [-0.157]
 [-0.137]
 [-0.138]
 [-0.141]] [[28.758]
 [25.205]
 [28.441]
 [29.648]
 [28.782]
 [29.041]
 [28.81 ]] [[1.24 ]
 [0.904]
 [1.218]
 [1.309]
 [1.247]
 [1.271]
 [1.246]]
printing an ep nov before normalisation:  30.003843307495117
printing an ep nov before normalisation:  39.8290928241806
printing an ep nov before normalisation:  53.869384080744574
maxi score, test score, baseline:  -0.3728733333333335 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  49 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.37023333333333336 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.266663491167506
printing an ep nov before normalisation:  55.405623294731924
printing an ep nov before normalisation:  52.54533800692502
maxi score, test score, baseline:  -0.37023333333333336 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  39 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.36743333333333345 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.53627567465948
printing an ep nov before normalisation:  35.90210346071678
printing an ep nov before normalisation:  44.25483128740924
maxi score, test score, baseline:  -0.36743333333333345 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.14415338110298
maxi score, test score, baseline:  -0.36743333333333345 0.6956666666666668 0.6956666666666668
actor:  1 policy actor:  1  step number:  52 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  44.524264335694546
maxi score, test score, baseline:  -0.36743333333333345 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.36743333333333345 0.6956666666666668 0.6956666666666668
printing an ep nov before normalisation:  57.06553921262768
maxi score, test score, baseline:  -0.36743333333333345 0.6956666666666668 0.6956666666666668
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.36743333333333345 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.36743333333333345 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.36743333333333345 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.36743333333333345 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.3320569657357453
actor:  1 policy actor:  1  step number:  37 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.088]
 [-0.095]
 [-0.088]
 [-0.089]
 [-0.092]
 [-0.088]
 [-0.088]] [[40.305]
 [48.071]
 [47.903]
 [47.991]
 [48.431]
 [48.189]
 [47.875]] [[1.022]
 [1.431]
 [1.429]
 [1.433]
 [1.454]
 [1.445]
 [1.428]]
printing an ep nov before normalisation:  31.78027630033624
printing an ep nov before normalisation:  31.96241347391169
maxi score, test score, baseline:  -0.36743333333333345 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  48 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3648066666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.50587033886722
maxi score, test score, baseline:  -0.3648066666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3648066666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.039456040824874
maxi score, test score, baseline:  -0.3648066666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.15333333333333288  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  45.530929906104056
printing an ep nov before normalisation:  10.826251544627477
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.06666009819476
actor:  1 policy actor:  1  step number:  50 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  51 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.3648066666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.928650856018066
printing an ep nov before normalisation:  30.45492342156613
printing an ep nov before normalisation:  41.99679118419929
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.397]
 [0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.358]] [[53.402]
 [52.283]
 [53.402]
 [53.402]
 [53.402]
 [53.402]
 [53.402]] [[1.935]
 [1.926]
 [1.935]
 [1.935]
 [1.935]
 [1.935]
 [1.935]]
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.521]
 [0.411]
 [0.411]
 [0.283]
 [0.297]
 [0.411]] [[35.744]
 [37.41 ]
 [35.744]
 [35.744]
 [36.949]
 [37.196]
 [35.744]] [[1.657]
 [1.861]
 [1.657]
 [1.657]
 [1.597]
 [1.625]
 [1.657]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3648066666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3648066666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.206]
 [0.209]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]] [[42.069]
 [50.844]
 [42.069]
 [42.069]
 [42.069]
 [42.069]
 [42.069]] [[1.177]
 [1.509]
 [1.177]
 [1.177]
 [1.177]
 [1.177]
 [1.177]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.22666666666666613  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  30.19686222076416
maxi score, test score, baseline:  -0.3648066666666668 0.6956666666666668 0.6956666666666668
actor:  1 policy actor:  1  step number:  57 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3648066666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.986155463114095
maxi score, test score, baseline:  -0.3648066666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.72316343106531
actor:  1 policy actor:  1  step number:  57 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.3648066666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.495806217193604
maxi score, test score, baseline:  -0.3648066666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.87809266299468
printing an ep nov before normalisation:  34.23362381017314
printing an ep nov before normalisation:  41.79508838774744
printing an ep nov before normalisation:  35.038464319103475
actor:  1 policy actor:  1  step number:  57 total reward:  0.09333333333333282  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  48 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.3648066666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3648066666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.576685251354885
printing an ep nov before normalisation:  24.10478834004527
maxi score, test score, baseline:  -0.3648066666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  60 total reward:  0.2333333333333325  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  40.664814082984186
printing an ep nov before normalisation:  31.6587653791225
printing an ep nov before normalisation:  55.98217163069318
maxi score, test score, baseline:  -0.36234000000000005 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.989415056749504
printing an ep nov before normalisation:  41.16900159914268
printing an ep nov before normalisation:  42.13733088686837
printing an ep nov before normalisation:  27.36963618833855
printing an ep nov before normalisation:  36.48076618540444
maxi score, test score, baseline:  -0.36234000000000005 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.88316122154676
actor:  0 policy actor:  0  step number:  71 total reward:  0.02666666666666584  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.08 ]
 [-0.071]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.08 ]
 [-0.071]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]]
maxi score, test score, baseline:  -0.36028666666666675 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  65 total reward:  0.053333333333332344  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  42.672565046485865
printing an ep nov before normalisation:  29.897810148362588
maxi score, test score, baseline:  -0.3581800000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.36622941080712
actor:  0 policy actor:  0  step number:  49 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  33.57187793442909
maxi score, test score, baseline:  -0.3555133333333334 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.865157
Printing some Q and Qe and total Qs values:  [[0.032]
 [0.032]
 [0.032]
 [0.032]
 [0.032]
 [0.032]
 [0.032]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.032]
 [0.032]
 [0.032]
 [0.032]
 [0.032]
 [0.032]
 [0.032]]
maxi score, test score, baseline:  -0.3555133333333334 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  51 total reward:  0.4533333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.3526066666666668 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.3526066666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.69972224631862
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.486020850424595
maxi score, test score, baseline:  -0.3526066666666668 0.6956666666666668 0.6956666666666668
printing an ep nov before normalisation:  37.142641229768685
maxi score, test score, baseline:  -0.3526066666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.4266666666666663  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.3526066666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.3526066666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.314464989934464
Printing some Q and Qe and total Qs values:  [[-0.06 ]
 [-0.039]
 [-0.065]
 [-0.062]
 [-0.074]
 [-0.064]
 [-0.064]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.06 ]
 [-0.039]
 [-0.065]
 [-0.062]
 [-0.074]
 [-0.064]
 [-0.064]]
printing an ep nov before normalisation:  41.69509902555501
printing an ep nov before normalisation:  50.763747600183784
Printing some Q and Qe and total Qs values:  [[0.711]
 [0.804]
 [0.708]
 [0.655]
 [0.708]
 [0.708]
 [0.715]] [[36.826]
 [42.451]
 [32.697]
 [36.975]
 [32.697]
 [32.697]
 [34.734]] [[0.711]
 [0.804]
 [0.708]
 [0.655]
 [0.708]
 [0.708]
 [0.715]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]] [[53.345]
 [53.345]
 [53.345]
 [53.345]
 [53.345]
 [53.345]
 [53.345]] [[1.601]
 [1.601]
 [1.601]
 [1.601]
 [1.601]
 [1.601]
 [1.601]]
maxi score, test score, baseline:  -0.3526066666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.0766053444755
printing an ep nov before normalisation:  47.134434412636224
maxi score, test score, baseline:  -0.3526066666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  62 total reward:  0.31333333333333335  reward:  1.0 rdn_beta:  1.333
siam score:  -0.8589811
maxi score, test score, baseline:  -0.34998000000000007 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.34998000000000007 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]] [[34.826]
 [34.826]
 [34.826]
 [34.826]
 [34.826]
 [34.826]
 [34.826]] [[2.501]
 [2.501]
 [2.501]
 [2.501]
 [2.501]
 [2.501]
 [2.501]]
printing an ep nov before normalisation:  48.728956571117905
maxi score, test score, baseline:  -0.34998000000000007 0.6956666666666668 0.6956666666666668
line 256 mcts: sample exp_bonus 55.07242287721794
maxi score, test score, baseline:  -0.34998000000000007 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.569536715956566
printing an ep nov before normalisation:  48.7893427985706
actor:  1 policy actor:  1  step number:  80 total reward:  0.08666666666666534  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.34998000000000007 0.6956666666666668 0.6956666666666668
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.34998000000000007 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.051]
 [-0.037]
 [-0.042]
 [-0.044]
 [-0.044]
 [-0.047]
 [-0.047]] [[20.312]
 [21.965]
 [19.744]
 [20.023]
 [20.011]
 [19.503]
 [19.588]] [[0.8  ]
 [0.946]
 [0.764]
 [0.785]
 [0.783]
 [0.74 ]
 [0.747]]
printing an ep nov before normalisation:  14.470729331128338
maxi score, test score, baseline:  -0.3499800000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3499800000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3499800000000001 0.6956666666666668 0.6956666666666668
actor:  1 policy actor:  1  step number:  52 total reward:  0.25999999999999945  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.62 ]
 [0.376]
 [0.396]
 [0.381]
 [0.43 ]
 [0.409]] [[41.846]
 [36.152]
 [41.136]
 [41.212]
 [41.332]
 [40.26 ]
 [41.846]] [[1.256]
 [1.241]
 [1.195]
 [1.218]
 [1.207]
 [1.214]
 [1.256]]
printing an ep nov before normalisation:  29.931099304146805
maxi score, test score, baseline:  -0.3499800000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.778223893971287
maxi score, test score, baseline:  -0.3499800000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.60002374649048
maxi score, test score, baseline:  -0.3499800000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.932588591879096
actor:  1 policy actor:  1  step number:  58 total reward:  0.25999999999999923  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.3499800000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.4979, 0.0018, 0.0807, 0.0863, 0.1557, 0.0928, 0.0848],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0045, 0.9699, 0.0036, 0.0080, 0.0012, 0.0011, 0.0118],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0826, 0.0141, 0.3815, 0.0943, 0.0973, 0.2072, 0.1230],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1725, 0.0268, 0.1249, 0.1776, 0.1704, 0.1802, 0.1475],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1233, 0.0078, 0.0930, 0.1000, 0.4523, 0.1193, 0.1044],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1398, 0.0025, 0.1244, 0.1392, 0.1181, 0.3676, 0.1084],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1381, 0.0111, 0.1298, 0.1418, 0.1648, 0.1522, 0.2621],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  38.91273605132109
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.08884829327297
maxi score, test score, baseline:  -0.3499800000000001 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.3499800000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3499800000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.783597006389073
siam score:  -0.86302185
Printing some Q and Qe and total Qs values:  [[0.541]
 [0.736]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]] [[35.377]
 [34.561]
 [35.377]
 [35.377]
 [35.377]
 [35.377]
 [35.377]] [[0.793]
 [0.978]
 [0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 35.29833082420433
maxi score, test score, baseline:  -0.3499800000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
actor:  1 policy actor:  1  step number:  60 total reward:  0.31333333333333324  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  33.88236920114138
actor:  1 policy actor:  1  step number:  48 total reward:  0.3133333333333328  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3499800000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3499800000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3499800000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3499800000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 37.94102438782499
maxi score, test score, baseline:  -0.3499800000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.806276604921386
printing an ep nov before normalisation:  35.24864196777344
actor:  0 policy actor:  0  step number:  62 total reward:  0.11333333333333284  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.058]
 [-0.037]
 [-0.065]
 [-0.065]
 [-0.07 ]
 [-0.061]
 [-0.067]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.058]
 [-0.037]
 [-0.065]
 [-0.065]
 [-0.07 ]
 [-0.061]
 [-0.067]]
maxi score, test score, baseline:  -0.3477533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.50149094242688
maxi score, test score, baseline:  -0.3477533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  37 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  1.667
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  27.69252157939144
maxi score, test score, baseline:  -0.34479333333333345 0.6956666666666668 0.6956666666666668
actor:  0 policy actor:  0  step number:  47 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3421266666666667 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3421266666666667 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.482140838405492
maxi score, test score, baseline:  -0.3421266666666667 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.650767088374895
maxi score, test score, baseline:  -0.3421266666666667 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.562096300942834
maxi score, test score, baseline:  -0.34212666666666675 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.34212666666666675 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  29.761773347854618
printing an ep nov before normalisation:  41.586785316467285
maxi score, test score, baseline:  -0.34212666666666675 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  53 total reward:  0.25333333333333263  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.069]
 [-0.023]
 [-0.069]
 [-0.069]
 [-0.069]
 [-0.069]
 [-0.069]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.069]
 [-0.023]
 [-0.069]
 [-0.069]
 [-0.069]
 [-0.069]
 [-0.069]]
maxi score, test score, baseline:  -0.3396200000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3396200000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3396200000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.044]
 [ 0.056]
 [-0.042]
 [-0.044]
 [-0.042]
 [-0.044]
 [-0.037]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.044]
 [ 0.056]
 [-0.042]
 [-0.044]
 [-0.042]
 [-0.044]
 [-0.037]]
printing an ep nov before normalisation:  35.916146095962716
maxi score, test score, baseline:  -0.3396200000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3396200000000001 0.6956666666666668 0.6956666666666668
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[ 0.5517],
        [-0.0259],
        [-0.2452],
        [ 0.8606],
        [ 0.6626],
        [-0.0263],
        [ 0.3824],
        [ 0.8038],
        [ 0.6626],
        [-0.0000]], dtype=torch.float64)
-0.09703970119800001 0.4546625954788086
-0.09703970119800001 -0.12290206796678257
-0.045026434398 -0.29025981826434744
-0.071551887066 0.789001848902921
-0.071422513866 0.5911359259983561
-0.09703970119800001 -0.12337308727371292
-0.04528388706599999 0.33712702166678415
-0.045026434398 0.7587851665242366
-0.071422513866 0.5911359259983561
0.94099335 0.94099335
maxi score, test score, baseline:  -0.3396200000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.03152361234534
UNIT TEST: sample policy line 217 mcts : [0.245 0.306 0.122 0.082 0.061 0.143 0.041]
actor:  0 policy actor:  0  step number:  56 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.33726000000000006 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.33726000000000006 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.33726000000000006 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.33726000000000006 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.48076455215022
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.33726000000000006 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  44 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.3345000000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.39046016104079
printing an ep nov before normalisation:  45.8791842232484
printing an ep nov before normalisation:  34.076223373413086
printing an ep nov before normalisation:  35.76850652694702
actor:  1 policy actor:  1  step number:  71 total reward:  0.05333333333333257  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3345000000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.642]
 [0.726]
 [0.642]
 [0.642]
 [0.496]
 [0.514]
 [0.642]] [[35.627]
 [33.901]
 [35.627]
 [35.627]
 [35.558]
 [36.759]
 [35.627]] [[1.581]
 [1.586]
 [1.581]
 [1.581]
 [1.431]
 [1.504]
 [1.581]]
maxi score, test score, baseline:  -0.3345000000000001 0.6956666666666668 0.6956666666666668
printing an ep nov before normalisation:  30.220934979030922
printing an ep nov before normalisation:  31.246398998670276
printing an ep nov before normalisation:  37.90547737936295
Printing some Q and Qe and total Qs values:  [[0.818]
 [0.818]
 [0.818]
 [0.818]
 [0.818]
 [0.818]
 [0.818]] [[34.834]
 [34.834]
 [34.834]
 [34.834]
 [34.834]
 [34.834]
 [34.834]] [[1.424]
 [1.424]
 [1.424]
 [1.424]
 [1.424]
 [1.424]
 [1.424]]
printing an ep nov before normalisation:  44.33401279090508
maxi score, test score, baseline:  -0.3345000000000001 0.6956666666666668 0.6956666666666668
actor:  1 policy actor:  1  step number:  58 total reward:  0.23333333333333295  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  57 total reward:  0.3199999999999993  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  43.74427439843647
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[ 0.4838],
        [-0.0950],
        [ 0.1530],
        [ 0.4741],
        [ 0.4838],
        [ 0.3973],
        [-0.0352],
        [ 0.7569],
        [ 0.0020],
        [-0.0000]], dtype=torch.float64)
-0.070771701198 0.4130391532205636
-0.057834381198 -0.15285820234289105
-0.071551887066 0.08147189982648566
-0.09703970119800001 0.37706420145401365
-0.083839701198 0.3999711532205636
-0.058094434398 0.3392128247948062
-0.070771701198 -0.10596555602805197
-0.045414567066 0.7114588436292548
-0.09703970119800001 -0.09501025227613975
0.957165 0.957165
maxi score, test score, baseline:  -0.3345000000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.54117344840519
maxi score, test score, baseline:  -0.3345000000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  0.13999999999999957  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.312]
 [0.365]
 [0.265]
 [0.265]
 [0.246]
 [0.265]
 [0.265]] [[32.559]
 [33.616]
 [31.028]
 [31.028]
 [32.665]
 [31.028]
 [31.028]] [[1.655]
 [1.79 ]
 [1.489]
 [1.489]
 [1.597]
 [1.489]
 [1.489]]
maxi score, test score, baseline:  -0.3345000000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  38 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  33.76481400081167
maxi score, test score, baseline:  -0.33152666666666675 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.33152666666666675 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.2870, 0.0176, 0.1187, 0.1268, 0.1577, 0.1533, 0.1389],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0092, 0.9429, 0.0111, 0.0097, 0.0046, 0.0054, 0.0171],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1344, 0.0252, 0.3479, 0.1216, 0.1116, 0.1650, 0.0943],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1283, 0.1233, 0.1156, 0.1792, 0.1156, 0.1886, 0.1492],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1978, 0.0271, 0.0953, 0.1052, 0.3513, 0.1209, 0.1024],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1096, 0.0168, 0.1167, 0.1227, 0.1311, 0.3982, 0.1050],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1543, 0.0859, 0.1073, 0.1637, 0.0958, 0.1174, 0.2756],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.33152666666666675 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.572]
 [0.583]
 [0.536]
 [0.583]
 [0.583]
 [0.583]] [[47.646]
 [47.646]
 [47.646]
 [47.819]
 [47.646]
 [47.646]
 [47.646]] [[2.295]
 [2.284]
 [2.295]
 [2.259]
 [2.295]
 [2.295]
 [2.295]]
actions average: 
K:  1  action  0 :  tensor([0.4516, 0.0043, 0.1044, 0.0992, 0.1062, 0.1217, 0.1124],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0061, 0.9641, 0.0067, 0.0048, 0.0022, 0.0037, 0.0123],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1655, 0.0113, 0.1639, 0.1813, 0.1659, 0.1742, 0.1378],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1298, 0.0374, 0.1127, 0.3651, 0.1088, 0.1304, 0.1159],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1557, 0.0256, 0.1332, 0.1349, 0.2999, 0.1322, 0.1185],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1615, 0.0084, 0.1688, 0.1404, 0.1455, 0.2547, 0.1207],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1375, 0.1027, 0.1184, 0.1386, 0.1065, 0.1212, 0.2751],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.33152666666666675 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]] [[39.317]
 [39.317]
 [39.317]
 [39.317]
 [39.317]
 [39.317]
 [39.317]] [[0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]]
maxi score, test score, baseline:  -0.33152666666666675 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.865717
maxi score, test score, baseline:  -0.33152666666666675 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.08666666666666611  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.33152666666666675 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.016644737510415
printing an ep nov before normalisation:  26.25502264426604
printing an ep nov before normalisation:  29.243803024291992
Printing some Q and Qe and total Qs values:  [[0.734]
 [0.546]
 [0.546]
 [0.546]
 [0.731]
 [0.546]
 [0.546]] [[15.318]
 [19.034]
 [19.034]
 [19.034]
 [15.661]
 [19.034]
 [19.034]] [[2.65 ]
 [3.37 ]
 [3.37 ]
 [3.37 ]
 [2.731]
 [3.37 ]
 [3.37 ]]
actor:  1 policy actor:  1  step number:  81 total reward:  0.10666666666666524  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  60 total reward:  0.04666666666666608  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.163]
 [0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]] [[47.989]
 [47.274]
 [47.274]
 [47.274]
 [47.274]
 [47.274]
 [47.274]] [[0.83 ]
 [0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]]
actor:  0 policy actor:  0  step number:  43 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8678826
actions average: 
K:  2  action  0 :  tensor([0.4407, 0.0426, 0.0755, 0.0887, 0.1724, 0.0997, 0.0804],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0108, 0.9202, 0.0109, 0.0216, 0.0061, 0.0076, 0.0227],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1759, 0.0145, 0.1378, 0.1532, 0.1714, 0.1917, 0.1555],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0921, 0.0258, 0.0768, 0.5072, 0.0976, 0.1108, 0.0897],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1395, 0.0806, 0.0835, 0.1461, 0.2964, 0.1451, 0.1088],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1251, 0.1259, 0.1569, 0.1481, 0.1075, 0.2145, 0.1220],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0924, 0.1625, 0.0866, 0.1502, 0.0998, 0.1087, 0.2999],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  33.33524541463435
actions average: 
K:  1  action  0 :  tensor([0.4379, 0.0038, 0.0905, 0.1033, 0.1361, 0.1222, 0.1062],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0061, 0.9588, 0.0046, 0.0096, 0.0042, 0.0039, 0.0127],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1545, 0.0238, 0.2247, 0.1417, 0.1693, 0.1493, 0.1367],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1729, 0.0261, 0.1237, 0.2289, 0.1719, 0.1408, 0.1357],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1914, 0.0043, 0.1294, 0.1544, 0.1998, 0.1731, 0.1476],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0646, 0.0012, 0.1810, 0.0771, 0.0827, 0.5258, 0.0676],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1351, 0.1611, 0.1064, 0.1156, 0.1131, 0.1109, 0.2578],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.3266600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.89157009124756
maxi score, test score, baseline:  -0.3266600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3266600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3266600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.025]
 [ 0.542]
 [ 0.472]
 [ 0.491]
 [ 0.456]
 [ 0.39 ]
 [ 0.472]] [[36.05 ]
 [35.309]
 [29.337]
 [36.173]
 [38.83 ]
 [38.132]
 [29.337]] [[0.587]
 [1.129]
 [0.858]
 [1.107]
 [1.162]
 [1.072]
 [0.858]]
maxi score, test score, baseline:  -0.3266600000000001 0.6956666666666668 0.6956666666666668
actor:  1 policy actor:  1  step number:  52 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  45.387850922555096
printing an ep nov before normalisation:  21.196166909714407
actor:  1 policy actor:  1  step number:  58 total reward:  0.086666666666666  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3266600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.946129322052
printing an ep nov before normalisation:  25.601344108581543
actor:  1 policy actor:  1  step number:  51 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.3266600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.456344769067876
maxi score, test score, baseline:  -0.3266600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3266600000000001 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.3266600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.11999999999999933  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.3266600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3266600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.925811072446365
Printing some Q and Qe and total Qs values:  [[0.871]
 [0.753]
 [0.753]
 [0.753]
 [0.795]
 [0.753]
 [0.753]] [[40.113]
 [40.266]
 [40.266]
 [40.266]
 [38.824]
 [40.266]
 [40.266]] [[0.871]
 [0.753]
 [0.753]
 [0.753]
 [0.795]
 [0.753]
 [0.753]]
printing an ep nov before normalisation:  44.126582233672565
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.74872506742426
siam score:  -0.86598665
maxi score, test score, baseline:  -0.3266600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.708]
 [0.728]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]] [[40.558]
 [38.673]
 [40.558]
 [40.558]
 [40.558]
 [40.558]
 [40.558]] [[0.708]
 [0.728]
 [0.708]
 [0.708]
 [0.708]
 [0.708]
 [0.708]]
Printing some Q and Qe and total Qs values:  [[0.579]
 [0.536]
 [0.503]
 [0.579]
 [0.579]
 [0.579]
 [0.579]] [[29.891]
 [33.513]
 [32.802]
 [29.891]
 [29.891]
 [29.891]
 [29.891]] [[1.104]
 [1.201]
 [1.142]
 [1.104]
 [1.104]
 [1.104]
 [1.104]]
maxi score, test score, baseline:  -0.3266600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  0.15333333333333243  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  44 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.32390000000000013 0.6956666666666668 0.6956666666666668
actor:  0 policy actor:  0  step number:  43 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  46.499217799480505
printing an ep nov before normalisation:  30.71173084991522
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  35.508455599623325
maxi score, test score, baseline:  -0.32096666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.32696485251451
siam score:  -0.87130356
Printing some Q and Qe and total Qs values:  [[-0.015]
 [ 0.103]
 [-0.015]
 [-0.015]
 [-0.011]
 [-0.015]
 [-0.015]] [[35.243]
 [47.815]
 [35.243]
 [35.243]
 [38.842]
 [35.243]
 [35.243]] [[0.436]
 [0.823]
 [0.436]
 [0.436]
 [0.517]
 [0.436]
 [0.436]]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  37.44559338940517
printing an ep nov before normalisation:  48.39422504740164
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.32096666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.3872, 0.0726, 0.0910, 0.1204, 0.1040, 0.1083, 0.1166],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0036, 0.9711, 0.0038, 0.0046, 0.0015, 0.0019, 0.0136],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1526, 0.0158, 0.2653, 0.1209, 0.1184, 0.1640, 0.1630],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1001, 0.0945, 0.0813, 0.4158, 0.0998, 0.1105, 0.0981],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1253, 0.0219, 0.0735, 0.0939, 0.5058, 0.0929, 0.0867],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1091, 0.0238, 0.1709, 0.0942, 0.0778, 0.4297, 0.0945],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1366, 0.0519, 0.1138, 0.1308, 0.1236, 0.1192, 0.3241],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  52 total reward:  0.1933333333333328  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.32096666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.32096666666666673 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8700044
actor:  0 policy actor:  0  step number:  45 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.31816666666666676 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.31816666666666676 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.31816666666666676 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.328902744675023
maxi score, test score, baseline:  -0.31816666666666676 0.6956666666666668 0.6956666666666668
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.693315309552275
printing an ep nov before normalisation:  36.11260816687563
maxi score, test score, baseline:  -0.31816666666666676 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.34269611338376
printing an ep nov before normalisation:  47.14169203387494
maxi score, test score, baseline:  -0.31816666666666676 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.01533814349637
actor:  0 policy actor:  0  step number:  30 total reward:  0.6066666666666667  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3149533333333334 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.3149533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.76682901277498
actor:  1 policy actor:  1  step number:  55 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  44.06212376395788
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.06 ]
 [-0.088]
 [-0.062]
 [-0.087]
 [-0.063]
 [-0.06 ]
 [-0.064]] [[22.569]
 [30.731]
 [22.754]
 [29.121]
 [22.432]
 [22.74 ]
 [21.826]] [[0.638]
 [1.108]
 [0.647]
 [1.01 ]
 [0.627]
 [0.648]
 [0.589]]
printing an ep nov before normalisation:  38.37907358773431
maxi score, test score, baseline:  -0.3149533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3149533333333334 0.6956666666666668 0.6956666666666668
printing an ep nov before normalisation:  49.728828717513906
maxi score, test score, baseline:  -0.3149533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8616345
maxi score, test score, baseline:  -0.3149533333333334 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.3149533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.38004732120836
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.53 ]
 [0.384]
 [0.384]
 [0.384]
 [0.384]
 [0.384]] [[46.126]
 [42.214]
 [46.126]
 [46.126]
 [46.126]
 [46.126]
 [46.126]] [[1.005]
 [1.056]
 [1.005]
 [1.005]
 [1.005]
 [1.005]
 [1.005]]
maxi score, test score, baseline:  -0.3149533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  50 total reward:  0.39333333333333287  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 31.619418237980543
printing an ep nov before normalisation:  38.37012801415842
maxi score, test score, baseline:  -0.3121666666666667 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.50424581937613
actor:  0 policy actor:  0  step number:  39 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.204526094728095
Printing some Q and Qe and total Qs values:  [[0.081]
 [0.199]
 [0.081]
 [0.033]
 [0.081]
 [0.081]
 [0.124]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.081]
 [0.199]
 [0.081]
 [0.033]
 [0.081]
 [0.081]
 [0.124]]
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.639]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]] [[46.181]
 [46.575]
 [46.181]
 [46.181]
 [46.181]
 [46.181]
 [46.181]] [[1.096]
 [1.211]
 [1.096]
 [1.096]
 [1.096]
 [1.096]
 [1.096]]
maxi score, test score, baseline:  -0.30923333333333336 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.869011
maxi score, test score, baseline:  -0.30923333333333336 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.30923333333333336 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  36 total reward:  0.5  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  41 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  27.82978420662965
printing an ep nov before normalisation:  39.764412704782025
printing an ep nov before normalisation:  42.19414580529609
maxi score, test score, baseline:  -0.30332666666666674 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.083]
 [0.171]
 [0.047]
 [0.079]
 [0.074]
 [0.06 ]
 [0.074]] [[23.593]
 [37.794]
 [18.523]
 [16.519]
 [16.993]
 [17.287]
 [20.901]] [[0.261]
 [0.544]
 [0.155]
 [0.16 ]
 [0.162]
 [0.152]
 [0.215]]
maxi score, test score, baseline:  -0.30332666666666674 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.30332666666666674 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.30332666666666674 0.6956666666666668 0.6956666666666668
printing an ep nov before normalisation:  35.00441133419854
maxi score, test score, baseline:  -0.30332666666666674 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.643]
 [0.6  ]
 [0.589]
 [0.586]
 [0.586]
 [0.586]] [[12.51 ]
 [14.812]
 [12.853]
 [12.643]
 [12.51 ]
 [12.51 ]
 [12.51 ]] [[2.113]
 [2.45 ]
 [2.168]
 [2.131]
 [2.113]
 [2.113]
 [2.113]]
actor:  1 policy actor:  1  step number:  73 total reward:  0.35999999999999943  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  54.3724639637658
printing an ep nov before normalisation:  34.97689243649959
printing an ep nov before normalisation:  34.712786776500636
maxi score, test score, baseline:  -0.30332666666666674 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.99048403736985
actor:  1 policy actor:  1  step number:  58 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  1  action  0 :  tensor([0.4832, 0.0043, 0.0954, 0.1128, 0.1057, 0.1044, 0.0942],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0036, 0.9701, 0.0041, 0.0052, 0.0035, 0.0034, 0.0101],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1446, 0.0143, 0.3388, 0.1197, 0.1400, 0.1278, 0.1147],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1215, 0.0095, 0.1307, 0.3034, 0.1754, 0.1434, 0.1161],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2081, 0.0025, 0.1438, 0.1122, 0.2631, 0.1553, 0.1151],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1330, 0.0580, 0.1575, 0.1024, 0.1150, 0.3198, 0.1142],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1487, 0.1687, 0.1358, 0.1223, 0.1183, 0.1293, 0.1769],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  51 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  46.65804031298429
actor:  1 policy actor:  1  step number:  52 total reward:  0.28666666666666596  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8636381
maxi score, test score, baseline:  -0.30332666666666674 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.721]
 [0.784]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]] [[41.478]
 [38.231]
 [41.478]
 [41.478]
 [41.478]
 [41.478]
 [41.478]] [[0.721]
 [0.784]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]]
printing an ep nov before normalisation:  29.690269256858322
printing an ep nov before normalisation:  42.32013157291848
printing an ep nov before normalisation:  37.61424100256271
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.686]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]] [[36.689]
 [38.09 ]
 [36.689]
 [36.689]
 [36.689]
 [36.689]
 [36.689]] [[1.963]
 [2.18 ]
 [1.963]
 [1.963]
 [1.963]
 [1.963]
 [1.963]]
Printing some Q and Qe and total Qs values:  [[0.58 ]
 [0.659]
 [0.58 ]
 [0.58 ]
 [0.58 ]
 [0.58 ]
 [0.58 ]] [[33.584]
 [31.773]
 [33.584]
 [33.584]
 [33.584]
 [33.584]
 [33.584]] [[2.247]
 [2.151]
 [2.247]
 [2.247]
 [2.247]
 [2.247]
 [2.247]]
printing an ep nov before normalisation:  27.41072785422114
printing an ep nov before normalisation:  0.008558798199373996
actor:  1 policy actor:  1  step number:  57 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  37 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.30332666666666674 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.4394, 0.0319, 0.0924, 0.0851, 0.1111, 0.1302, 0.1099],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0048, 0.9731, 0.0024, 0.0062, 0.0014, 0.0017, 0.0105],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1192, 0.0059, 0.2787, 0.1370, 0.1229, 0.1994, 0.1369],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1553, 0.0089, 0.1363, 0.2234, 0.1588, 0.1505, 0.1670],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1565, 0.0070, 0.1271, 0.1449, 0.2985, 0.1251, 0.1410],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1469, 0.0127, 0.1483, 0.1713, 0.1587, 0.2064, 0.1557],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1276, 0.0052, 0.1117, 0.1246, 0.1415, 0.1345, 0.3549],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  55 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.30095333333333335 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.4475861089879
maxi score, test score, baseline:  -0.30095333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.30095333333333346 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.97862712985874
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.920418590272277
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
actions average: 
K:  1  action  0 :  tensor([0.3341, 0.0844, 0.0873, 0.1109, 0.1347, 0.1313, 0.1173],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0041, 0.9442, 0.0096, 0.0057, 0.0021, 0.0033, 0.0310],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1370, 0.0126, 0.2254, 0.1510, 0.1625, 0.1840, 0.1274],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0944, 0.0584, 0.0975, 0.3380, 0.1226, 0.1550, 0.1342],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1308, 0.0061, 0.1056, 0.1482, 0.2971, 0.1655, 0.1468],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1158, 0.0050, 0.1387, 0.1269, 0.1304, 0.3539, 0.1293],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1067, 0.1089, 0.1020, 0.1549, 0.1179, 0.1152, 0.2944],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  39.62896355506352
actor:  1 policy actor:  1  step number:  42 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  47.83509860034666
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  0.23999999999999921  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  41.67721961034036
actions average: 
K:  4  action  0 :  tensor([0.5575, 0.0212, 0.0685, 0.0845, 0.0929, 0.0870, 0.0885],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0095, 0.9170, 0.0084, 0.0150, 0.0066, 0.0074, 0.0362],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1671, 0.0139, 0.1257, 0.1659, 0.1777, 0.1983, 0.1515],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1554, 0.0173, 0.1307, 0.1582, 0.1701, 0.1957, 0.1727],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1835, 0.1997, 0.0571, 0.0852, 0.2843, 0.0865, 0.1038],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1209, 0.2312, 0.0900, 0.1431, 0.1480, 0.1431, 0.1237],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0831, 0.0459, 0.0745, 0.2149, 0.1292, 0.2005, 0.2518],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.075]
 [0.212]
 [0.075]
 [0.075]
 [0.075]
 [0.044]
 [0.075]] [[44.586]
 [49.677]
 [44.586]
 [44.586]
 [44.586]
 [50.892]
 [44.586]] [[1.329]
 [1.716]
 [1.329]
 [1.329]
 [1.329]
 [1.607]
 [1.329]]
actor:  1 policy actor:  1  step number:  58 total reward:  0.19333333333333258  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.176]
 [0.21 ]
 [0.176]
 [0.245]
 [0.176]
 [0.3  ]
 [0.176]] [[34.861]
 [40.052]
 [34.861]
 [41.193]
 [34.861]
 [41.645]
 [34.861]] [[1.351]
 [1.792]
 [1.351]
 [1.918]
 [1.351]
 [2.008]
 [1.351]]
printing an ep nov before normalisation:  44.751525538814306
Printing some Q and Qe and total Qs values:  [[ 0.024]
 [ 0.14 ]
 [ 0.024]
 [ 0.024]
 [ 0.024]
 [-0.002]
 [ 0.024]] [[30.699]
 [39.851]
 [30.699]
 [30.699]
 [30.699]
 [36.791]
 [30.699]] [[0.943]
 [1.576]
 [0.943]
 [0.943]
 [0.943]
 [1.262]
 [0.943]]
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
printing an ep nov before normalisation:  49.16656698299762
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0742],
        [-0.2224],
        [ 0.7775],
        [ 0.4827],
        [ 0.4649],
        [ 0.3555],
        [ 0.5844],
        [ 0.5314],
        [-0.0000],
        [ 0.5020]], dtype=torch.float64)
-0.057834381198 -0.13207463755513177
-0.032346567066 -0.2547484997588352
-0.032346567066 0.7451885921429562
-0.045026434398 0.4376630533256791
-0.057834381198 0.40710746912290197
-0.045154513866 0.31031432237833295
-0.071031754398 0.5133524471145016
-0.070771701198 0.4606646543505395
-0.9609867497999999 -0.9609867497999999
-0.09703970119800001 0.40500395665413247
actor:  1 policy actor:  1  step number:  57 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.313323688754885
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
siam score:  -0.8718564
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]] [[40.287]
 [40.287]
 [40.287]
 [40.287]
 [40.287]
 [40.287]
 [40.287]] [[0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.712]]
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.733]
 [0.632]
 [0.672]
 [0.571]
 [0.496]
 [0.704]] [[37.737]
 [28.595]
 [38.775]
 [39.915]
 [41.112]
 [40.803]
 [37.75 ]] [[2.312]
 [2.009]
 [2.365]
 [2.457]
 [2.41 ]
 [2.32 ]
 [2.391]]
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.3374, 0.0088, 0.1007, 0.1428, 0.1405, 0.1553, 0.1145],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0164, 0.9052, 0.0278, 0.0125, 0.0092, 0.0103, 0.0186],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1230, 0.0031, 0.2354, 0.1842, 0.1411, 0.1892, 0.1239],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0777, 0.1847, 0.0630, 0.3219, 0.1020, 0.1170, 0.1338],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1737, 0.0094, 0.0940, 0.1336, 0.3668, 0.1319, 0.0906],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1284, 0.0443, 0.1479, 0.1978, 0.1400, 0.1945, 0.1471],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1653, 0.0451, 0.1039, 0.1882, 0.1955, 0.1654, 0.1366],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.306666666666666  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  57 total reward:  0.5466666666666671  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.644]
 [0.73 ]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]] [[25.981]
 [29.859]
 [25.981]
 [25.981]
 [25.981]
 [25.981]
 [25.981]] [[1.131]
 [1.355]
 [1.131]
 [1.131]
 [1.131]
 [1.131]
 [1.131]]
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  72 total reward:  0.0599999999999995  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  59 total reward:  0.13333333333333264  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.37560147362377
siam score:  -0.8735468
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.74996723584603
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.194]
 [0.387]
 [0.17 ]
 [0.176]
 [0.166]
 [0.165]
 [0.168]] [[25.145]
 [33.692]
 [25.969]
 [26.004]
 [25.454]
 [25.925]
 [28.238]] [[0.337]
 [0.629]
 [0.323]
 [0.329]
 [0.313]
 [0.318]
 [0.347]]
line 256 mcts: sample exp_bonus 41.83700003040349
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.2099656559768
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.36666666666666614  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
actor:  1 policy actor:  1  step number:  50 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
printing an ep nov before normalisation:  32.40603303270513
printing an ep nov before normalisation:  44.06402219719514
printing an ep nov before normalisation:  43.803323410402015
printing an ep nov before normalisation:  42.086168692195294
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
Printing some Q and Qe and total Qs values:  [[0.287]
 [0.412]
 [0.323]
 [0.323]
 [0.323]
 [0.323]
 [0.323]] [[43.31 ]
 [50.478]
 [38.817]
 [38.817]
 [38.817]
 [38.817]
 [38.817]] [[1.45 ]
 [1.917]
 [1.272]
 [1.272]
 [1.272]
 [1.272]
 [1.272]]
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3009533333333334 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  45 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.399]
 [0.399]
 [0.399]
 [0.353]
 [0.399]
 [0.399]
 [0.399]] [[29.51 ]
 [29.51 ]
 [29.51 ]
 [29.858]
 [29.51 ]
 [29.51 ]
 [29.51 ]] [[2.29 ]
 [2.29 ]
 [2.29 ]
 [2.266]
 [2.29 ]
 [2.29 ]
 [2.29 ]]
maxi score, test score, baseline:  -0.2982600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2982600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8755107
printing an ep nov before normalisation:  39.928090327394195
actor:  1 policy actor:  1  step number:  66 total reward:  0.2466666666666657  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.2982600000000001 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.2982600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.043]
 [0.007]
 [0.006]
 [0.027]
 [0.004]
 [0.016]] [[38.61 ]
 [39.042]
 [38.558]
 [38.22 ]
 [38.559]
 [38.02 ]
 [38.907]] [[1.51 ]
 [1.579]
 [1.506]
 [1.479]
 [1.526]
 [1.463]
 [1.541]]
printing an ep nov before normalisation:  12.39186537392799
actor:  1 policy actor:  1  step number:  75 total reward:  0.23999999999999844  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.2982600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2982600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.1896576521692
maxi score, test score, baseline:  -0.2982600000000001 0.6956666666666668 0.6956666666666668
printing an ep nov before normalisation:  48.76323051153284
printing an ep nov before normalisation:  36.1378924845975
Printing some Q and Qe and total Qs values:  [[0.698]
 [0.792]
 [0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]] [[38.491]
 [35.66 ]
 [38.491]
 [38.491]
 [38.491]
 [38.491]
 [38.491]] [[0.698]
 [0.792]
 [0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.2982600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.21171020594054
maxi score, test score, baseline:  -0.2982600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2982600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  53.35478159036676
printing an ep nov before normalisation:  52.6610983329172
maxi score, test score, baseline:  -0.2982600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.041]
 [-0.037]
 [-0.041]
 [-0.04 ]
 [-0.036]
 [-0.035]
 [-0.036]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.041]
 [-0.037]
 [-0.041]
 [-0.04 ]
 [-0.036]
 [-0.035]
 [-0.036]]
maxi score, test score, baseline:  -0.2982600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  49 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.29578000000000004 0.6956666666666668 0.6956666666666668
printing an ep nov before normalisation:  40.7466598513223
maxi score, test score, baseline:  -0.29578000000000004 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.731930232402696
actions average: 
K:  4  action  0 :  tensor([0.4647, 0.0453, 0.0852, 0.1029, 0.1312, 0.0918, 0.0788],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0074, 0.9254, 0.0069, 0.0317, 0.0025, 0.0036, 0.0224],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1516, 0.1638, 0.2240, 0.1068, 0.1189, 0.1308, 0.1041],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0901, 0.1349, 0.0585, 0.4910, 0.0735, 0.0679, 0.0841],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1591, 0.0087, 0.1158, 0.1231, 0.3642, 0.1109, 0.1184],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1446, 0.0154, 0.1910, 0.1416, 0.1384, 0.2416, 0.1274],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1380, 0.0780, 0.1257, 0.1601, 0.1356, 0.1221, 0.2404],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  33.53849548932012
maxi score, test score, baseline:  -0.29578000000000004 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  59 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  36 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.29578000000000004 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.29578000000000004 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.83903475404707
maxi score, test score, baseline:  -0.29578000000000004 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.134]
 [0.257]
 [0.134]
 [0.134]
 [0.134]
 [0.134]
 [0.134]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.134]
 [0.257]
 [0.134]
 [0.134]
 [0.134]
 [0.134]
 [0.134]]
printing an ep nov before normalisation:  42.14836597442627
maxi score, test score, baseline:  -0.29578000000000004 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.74 ]
 [0.635]
 [0.657]
 [0.631]
 [0.689]
 [0.695]] [[42.061]
 [42.539]
 [38.345]
 [38.339]
 [39.92 ]
 [37.737]
 [40.023]] [[0.653]
 [0.74 ]
 [0.635]
 [0.657]
 [0.631]
 [0.689]
 [0.695]]
actions average: 
K:  3  action  0 :  tensor([0.4011, 0.0816, 0.0966, 0.0857, 0.1081, 0.1259, 0.1009],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0079, 0.9288, 0.0102, 0.0148, 0.0047, 0.0075, 0.0262],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1546, 0.0040, 0.2921, 0.1277, 0.1306, 0.1594, 0.1316],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0833, 0.0082, 0.0686, 0.5119, 0.0969, 0.1165, 0.1147],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1589, 0.0119, 0.1107, 0.1521, 0.2359, 0.1819, 0.1486],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1325, 0.0053, 0.1514, 0.1453, 0.1350, 0.2901, 0.1403],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0982, 0.2798, 0.1052, 0.0831, 0.1129, 0.1141, 0.2067],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  36.92042190349131
Printing some Q and Qe and total Qs values:  [[0.795]
 [0.795]
 [0.795]
 [0.795]
 [0.795]
 [0.795]
 [0.795]] [[33.084]
 [33.084]
 [33.084]
 [33.084]
 [33.084]
 [33.084]
 [33.084]] [[0.795]
 [0.795]
 [0.795]
 [0.795]
 [0.795]
 [0.795]
 [0.795]]
printing an ep nov before normalisation:  46.066753351375205
actor:  0 policy actor:  0  step number:  42 total reward:  0.39333333333333287  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  0  action  0 :  tensor([0.4687, 0.0054, 0.0980, 0.0930, 0.1213, 0.1148, 0.0989],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0029, 0.9609, 0.0050, 0.0089, 0.0021, 0.0041, 0.0161],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0871, 0.0224, 0.4609, 0.0842, 0.0788, 0.1898, 0.0766],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1540, 0.0264, 0.1287, 0.2967, 0.1115, 0.1525, 0.1301],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1356, 0.0009, 0.0804, 0.1003, 0.5275, 0.0880, 0.0673],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1284, 0.0156, 0.1608, 0.1090, 0.1048, 0.3642, 0.1172],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1440, 0.1118, 0.1441, 0.1706, 0.1359, 0.1535, 0.1400],
       grad_fn=<DivBackward0>)
actions average: 
K:  2  action  0 :  tensor([0.2759, 0.0250, 0.1144, 0.1190, 0.1908, 0.1496, 0.1253],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0260, 0.9288, 0.0093, 0.0092, 0.0060, 0.0044, 0.0163],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1629, 0.0254, 0.3798, 0.0964, 0.0911, 0.1536, 0.0908],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1153, 0.0528, 0.1091, 0.3521, 0.1240, 0.1339, 0.1127],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2345, 0.0138, 0.0677, 0.0732, 0.4905, 0.0632, 0.0570],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1319, 0.0087, 0.1409, 0.1528, 0.1688, 0.2721, 0.1248],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1328, 0.1367, 0.1038, 0.1148, 0.0949, 0.0999, 0.3172],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.711]
 [0.768]
 [0.694]
 [0.7  ]
 [0.687]
 [0.675]
 [0.688]] [[30.321]
 [30.679]
 [29.656]
 [30.153]
 [29.926]
 [29.364]
 [29.98 ]] [[0.711]
 [0.768]
 [0.694]
 [0.7  ]
 [0.687]
 [0.675]
 [0.688]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
actor:  0 policy actor:  0  step number:  49 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.29056666666666675 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  57 total reward:  0.17333333333333256  reward:  1.0 rdn_beta:  1.333
siam score:  -0.8687248
maxi score, test score, baseline:  -0.2882200000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.09057667646616
printing an ep nov before normalisation:  33.202121699051034
maxi score, test score, baseline:  -0.2882200000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.24666666666666603  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.2882200000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.    0.694 0.02  0.02  0.143 0.    0.122]
Printing some Q and Qe and total Qs values:  [[-0.036]
 [ 0.44 ]
 [ 0.136]
 [-0.009]
 [-0.036]
 [ 0.146]
 [-0.036]] [[48.664]
 [45.591]
 [49.619]
 [50.367]
 [48.664]
 [50.317]
 [48.664]] [[1.657]
 [1.946]
 [1.888]
 [1.789]
 [1.657]
 [1.941]
 [1.657]]
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.352]
 [0.324]
 [0.182]
 [0.324]
 [0.205]
 [0.248]] [[42.667]
 [42.812]
 [42.667]
 [45.566]
 [42.667]
 [43.967]
 [42.129]] [[1.735]
 [1.771]
 [1.735]
 [1.759]
 [1.735]
 [1.69 ]
 [1.628]]
Printing some Q and Qe and total Qs values:  [[0.545]
 [0.314]
 [0.545]
 [0.332]
 [0.318]
 [0.545]
 [0.311]] [[34.097]
 [34.316]
 [34.097]
 [34.079]
 [34.325]
 [34.097]
 [34.52 ]] [[1.847]
 [1.632]
 [1.847]
 [1.632]
 [1.636]
 [1.847]
 [1.644]]
maxi score, test score, baseline:  -0.2882200000000001 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.2882200000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2882200000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.623974654440342
maxi score, test score, baseline:  -0.2882200000000001 0.6956666666666668 0.6956666666666668
actor:  1 policy actor:  1  step number:  60 total reward:  0.1799999999999996  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  54 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.2882200000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2882200000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2882200000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.28822000000000014 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.83342354438998
maxi score, test score, baseline:  -0.28822000000000014 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  12.466607527953062
maxi score, test score, baseline:  -0.28822000000000014 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.28822000000000014 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.94457721710205
line 256 mcts: sample exp_bonus 49.53427617747489
maxi score, test score, baseline:  -0.28822000000000014 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.28822000000000014 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]] [[38.374]
 [38.374]
 [38.374]
 [38.374]
 [38.374]
 [38.374]
 [38.374]] [[1.203]
 [1.203]
 [1.203]
 [1.203]
 [1.203]
 [1.203]
 [1.203]]
actor:  1 policy actor:  1  step number:  95 total reward:  0.18666666666666476  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  2  action  0 :  tensor([0.1826, 0.0053, 0.1437, 0.1617, 0.1635, 0.1720, 0.1712],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0067, 0.9374, 0.0072, 0.0108, 0.0056, 0.0074, 0.0250],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1092, 0.0115, 0.2573, 0.1481, 0.1403, 0.1860, 0.1476],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1400, 0.1440, 0.1078, 0.2066, 0.1107, 0.1343, 0.1566],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1735, 0.0041, 0.1289, 0.1653, 0.1840, 0.1896, 0.1547],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1644, 0.0472, 0.1418, 0.1159, 0.1651, 0.2299, 0.1356],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1262, 0.1420, 0.1093, 0.1179, 0.1246, 0.1259, 0.2541],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.28822000000000014 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.28822000000000014 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.28822000000000014 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  67 total reward:  0.07999999999999907  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.431]
 [0.528]
 [0.338]
 [0.428]
 [0.343]
 [0.369]
 [0.461]] [[44.201]
 [43.031]
 [39.005]
 [43.864]
 [45.785]
 [49.163]
 [39.895]] [[0.851]
 [0.928]
 [0.667]
 [0.842]
 [0.79 ]
 [0.875]
 [0.806]]
maxi score, test score, baseline:  -0.2860600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.74295797409461
maxi score, test score, baseline:  -0.2860600000000001 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.2860600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2860600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.24800605127994
printing an ep nov before normalisation:  31.917598110019973
actor:  1 policy actor:  1  step number:  57 total reward:  0.13333333333333297  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.2860600000000001 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.2860600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2860600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.2999999999999998  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  32.34593868255615
maxi score, test score, baseline:  -0.2860600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  69 total reward:  0.013333333333332642  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.2860600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  72.44273271342425
maxi score, test score, baseline:  -0.2860600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.857965
Printing some Q and Qe and total Qs values:  [[0.77 ]
 [0.77 ]
 [0.759]
 [0.758]
 [0.77 ]
 [0.77 ]
 [0.77 ]] [[25.715]
 [25.715]
 [27.136]
 [26.778]
 [25.715]
 [25.715]
 [25.715]] [[2.557]
 [2.557]
 [2.702]
 [2.662]
 [2.557]
 [2.557]
 [2.557]]
printing an ep nov before normalisation:  0.016774235676990656
actor:  0 policy actor:  0  step number:  45 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  90 total reward:  0.206666666666665  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.08 ]
 [-0.025]
 [-0.08 ]
 [-0.055]
 [-0.08 ]
 [-0.181]
 [-0.08 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.08 ]
 [-0.025]
 [-0.08 ]
 [-0.055]
 [-0.08 ]
 [-0.181]
 [-0.08 ]]
maxi score, test score, baseline:  -0.2832600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2832600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.93413427775284
printing an ep nov before normalisation:  20.70641146650581
actions average: 
K:  3  action  0 :  tensor([0.4443, 0.0145, 0.1041, 0.1008, 0.1240, 0.1285, 0.0838],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0111, 0.9129, 0.0071, 0.0223, 0.0044, 0.0059, 0.0363],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1391, 0.0326, 0.2683, 0.1379, 0.1109, 0.1733, 0.1379],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1357, 0.1609, 0.1366, 0.1633, 0.1351, 0.1505, 0.1178],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1513, 0.0024, 0.1270, 0.1365, 0.2839, 0.1777, 0.1211],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1622, 0.1842, 0.1324, 0.1217, 0.1353, 0.1468, 0.1173],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1490, 0.0666, 0.0819, 0.2277, 0.1028, 0.1215, 0.2504],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  65.74499504201754
maxi score, test score, baseline:  -0.2832600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.203818297687015
actor:  1 policy actor:  1  step number:  71 total reward:  0.239999999999999  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.07 ]
 [0.122]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.07 ]
 [0.122]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]]
maxi score, test score, baseline:  -0.2832600000000001 0.6956666666666668 0.6956666666666668
actor:  1 policy actor:  1  step number:  62 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  48.58176966304947
maxi score, test score, baseline:  -0.2832600000000001 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  53 total reward:  0.3066666666666664  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.2829666666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.378548911074834
maxi score, test score, baseline:  -0.2829666666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2829666666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.38666666666666594  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.834184657841824
maxi score, test score, baseline:  -0.2829666666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85473406
printing an ep nov before normalisation:  32.771679527645006
actor:  0 policy actor:  0  step number:  50 total reward:  0.3133333333333328  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.28034000000000014 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.18577011585365
actor:  1 policy actor:  1  step number:  55 total reward:  0.33333333333333315  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.28034000000000014 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  49 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.2779666666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.63 ]
 [0.523]
 [0.523]
 [0.434]
 [0.523]
 [0.523]] [[43.431]
 [43.953]
 [43.431]
 [43.431]
 [58.028]
 [43.431]
 [43.431]] [[1.571]
 [1.701]
 [1.571]
 [1.571]
 [2.115]
 [1.571]
 [1.571]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.2779666666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2779666666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.40666666666666595  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  44.82677777608235
maxi score, test score, baseline:  -0.2779666666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.111374855041504
line 256 mcts: sample exp_bonus 46.677466195954324
maxi score, test score, baseline:  -0.2779666666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8455408
maxi score, test score, baseline:  -0.2779666666666668 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.2779666666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  40 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.2779666666666668 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.2779666666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.879262410930096
maxi score, test score, baseline:  -0.2779666666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]] [[36.525]
 [36.525]
 [36.525]
 [36.525]
 [36.525]
 [36.525]
 [36.525]] [[0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]]
printing an ep nov before normalisation:  0.2757188652822151
actor:  1 policy actor:  1  step number:  55 total reward:  0.3199999999999994  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.2779666666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.35486500630489
Printing some Q and Qe and total Qs values:  [[0.541]
 [0.601]
 [0.541]
 [0.423]
 [0.541]
 [0.541]
 [0.541]] [[16.156]
 [16.95 ]
 [16.156]
 [15.684]
 [16.156]
 [16.156]
 [16.156]] [[1.982]
 [2.178]
 [1.982]
 [1.785]
 [1.982]
 [1.982]
 [1.982]]
maxi score, test score, baseline:  -0.27796666666666675 0.6956666666666668 0.6956666666666668
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  54 total reward:  0.33999999999999986  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  29.242811884198872
line 256 mcts: sample exp_bonus 60.252490037149585
maxi score, test score, baseline:  -0.27796666666666675 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.045]
 [-0.05 ]
 [-0.037]
 [-0.052]
 [-0.052]
 [-0.037]
 [-0.044]] [[45.875]
 [52.952]
 [45.994]
 [38.872]
 [39.062]
 [46.519]
 [43.49 ]] [[0.132]
 [0.175]
 [0.141]
 [0.077]
 [0.078]
 [0.144]
 [0.116]]
maxi score, test score, baseline:  -0.27796666666666675 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.453054697243545
printing an ep nov before normalisation:  35.05841902427301
printing an ep nov before normalisation:  39.76500263130212
printing an ep nov before normalisation:  38.279093983134246
printing an ep nov before normalisation:  40.423885542689874
actor:  1 policy actor:  1  step number:  70 total reward:  0.059999999999999276  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  39 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.2750066666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  76.40682704059166
maxi score, test score, baseline:  -0.2750066666666668 0.6956666666666668 0.6956666666666668
printing an ep nov before normalisation:  52.14723020470381
printing an ep nov before normalisation:  70.57177593201808
actions average: 
K:  2  action  0 :  tensor([0.3870, 0.0569, 0.0837, 0.1057, 0.1383, 0.1156, 0.1127],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0124, 0.9158, 0.0136, 0.0123, 0.0078, 0.0095, 0.0286],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1362, 0.0061, 0.2176, 0.1799, 0.1503, 0.1812, 0.1287],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1136, 0.0459, 0.1201, 0.3112, 0.1159, 0.1509, 0.1425],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1621, 0.0083, 0.1028, 0.1155, 0.3708, 0.1339, 0.1066],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1385, 0.0189, 0.1240, 0.1392, 0.1413, 0.3033, 0.1349],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0910, 0.2804, 0.0667, 0.1251, 0.0930, 0.1358, 0.2080],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  26.618938446044922
Printing some Q and Qe and total Qs values:  [[-0.089]
 [-0.068]
 [-0.085]
 [-0.085]
 [-0.085]
 [-0.085]
 [-0.077]] [[85.109]
 [78.712]
 [68.215]
 [68.215]
 [68.215]
 [68.215]
 [88.625]] [[1.162]
 [1.05 ]
 [0.816]
 [0.816]
 [0.816]
 [0.816]
 [1.248]]
Printing some Q and Qe and total Qs values:  [[0.328]
 [0.382]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]] [[39.15 ]
 [46.397]
 [39.15 ]
 [39.15 ]
 [39.15 ]
 [39.15 ]
 [39.15 ]] [[0.531]
 [0.655]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]]
Printing some Q and Qe and total Qs values:  [[-0.055]
 [-0.103]
 [-0.028]
 [-0.043]
 [-0.072]
 [-0.029]
 [-0.066]] [[44.532]
 [41.971]
 [44.995]
 [44.672]
 [49.358]
 [44.9  ]
 [45.377]] [[0.833]
 [0.686]
 [0.878]
 [0.851]
 [1.004]
 [0.873]
 [0.855]]
maxi score, test score, baseline:  -0.2750066666666668 0.6956666666666668 0.6956666666666668
printing an ep nov before normalisation:  39.745800495147705
printing an ep nov before normalisation:  48.74313851680334
Printing some Q and Qe and total Qs values:  [[-0.123]
 [-0.123]
 [-0.123]
 [-0.123]
 [-0.123]
 [-0.123]
 [-0.123]] [[16.083]
 [16.083]
 [16.083]
 [16.083]
 [16.083]
 [16.083]
 [16.083]] [[21.316]
 [21.316]
 [21.316]
 [21.316]
 [21.316]
 [21.316]
 [21.316]]
actor:  1 policy actor:  1  step number:  61 total reward:  0.25333333333333286  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.2750066666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.34223808094782
printing an ep nov before normalisation:  21.89072708068667
maxi score, test score, baseline:  -0.2750066666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.45868102402869
maxi score, test score, baseline:  -0.2750066666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2750066666666668 0.6956666666666668 0.6956666666666668
printing an ep nov before normalisation:  51.66616290294036
actor:  1 policy actor:  1  step number:  35 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2750066666666668 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  44 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  1.0
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.27208666666666675 0.6956666666666668 0.6956666666666668
printing an ep nov before normalisation:  45.0551701475374
siam score:  -0.8530112
maxi score, test score, baseline:  -0.27208666666666675 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.762]
 [0.809]
 [0.703]
 [0.705]
 [0.738]
 [0.713]
 [0.768]] [[28.846]
 [31.135]
 [29.135]
 [28.987]
 [28.511]
 [28.955]
 [29.113]] [[0.762]
 [0.809]
 [0.703]
 [0.705]
 [0.738]
 [0.713]
 [0.768]]
siam score:  -0.8528497
actor:  0 policy actor:  0  step number:  45 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  43.292952623718655
printing an ep nov before normalisation:  37.692772451130764
printing an ep nov before normalisation:  38.503644797388176
maxi score, test score, baseline:  -0.26939333333333343 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  42 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  26.167779782676536
actor:  0 policy actor:  0  step number:  56 total reward:  0.15333333333333277  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.26432666666666677 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.26432666666666677 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.26432666666666677 0.6956666666666668 0.6956666666666668
siam score:  -0.8560344
printing an ep nov before normalisation:  53.88354532316445
actor:  1 policy actor:  1  step number:  52 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.26432666666666677 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.26432666666666677 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  38 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.26432666666666677 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.35333333333333283  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  41.21004411269046
using explorer policy with actor:  1
printing an ep nov before normalisation:  59.485616037225974
printing an ep nov before normalisation:  54.7331610178416
maxi score, test score, baseline:  -0.26432666666666677 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.303]
 [0.283]
 [0.22 ]
 [0.22 ]
 [0.22 ]
 [0.22 ]
 [0.22 ]] [[48.928]
 [49.418]
 [34.473]
 [34.473]
 [34.473]
 [34.473]
 [34.473]] [[0.813]
 [0.801]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.85823899478177
maxi score, test score, baseline:  -0.26432666666666677 0.6956666666666668 0.6956666666666668
actor:  1 policy actor:  1  step number:  44 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.047]
 [-0.039]
 [-0.047]
 [-0.047]
 [-0.055]
 [-0.047]
 [-0.047]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.047]
 [-0.039]
 [-0.047]
 [-0.047]
 [-0.055]
 [-0.047]
 [-0.047]]
printing an ep nov before normalisation:  59.38464559584432
siam score:  -0.84875214
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.678]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]] [[28.936]
 [38.742]
 [28.936]
 [28.936]
 [28.936]
 [28.936]
 [28.936]] [[0.842]
 [1.198]
 [0.842]
 [0.842]
 [0.842]
 [0.842]
 [0.842]]
maxi score, test score, baseline:  -0.26432666666666677 0.6956666666666668 0.6956666666666668
maxi score, test score, baseline:  -0.26432666666666677 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.26432666666666677 0.6956666666666668 0.6956666666666668
printing an ep nov before normalisation:  45.02703665979397
printing an ep nov before normalisation:  64.64332057668928
printing an ep nov before normalisation:  32.483633603779126
maxi score, test score, baseline:  -0.26432666666666677 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.491630257951954
printing an ep nov before normalisation:  39.62428569793701
maxi score, test score, baseline:  -0.26432666666666677 0.6956666666666668 0.6956666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 47.636377512489254
printing an ep nov before normalisation:  46.109813758108984
using explorer policy with actor:  1
printing an ep nov before normalisation:  23.158279563060375
printing an ep nov before normalisation:  43.3376113231372
printing an ep nov before normalisation:  28.279845714569092
printing an ep nov before normalisation:  37.83253953519118
printing an ep nov before normalisation:  35.23528162900588
printing an ep nov before normalisation:  49.09607299628084
printing an ep nov before normalisation:  26.633849143981934
printing an ep nov before normalisation:  32.002821506833534
actor:  1 policy actor:  1  step number:  68 total reward:  0.01999999999999935  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.26432666666666677 0.6956666666666668 0.6956666666666668
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333333  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333333  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333333  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333333  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333333  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333333  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333333  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333333  reward:  1.0 rdn_beta:  2
actor:  1 policy actor:  1  step number:  52 total reward:  0.28666666666666607  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.19696666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.19696666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.545442619266744
actor:  1 policy actor:  1  step number:  33 total reward:  0.56  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.065]
 [-0.058]
 [-0.07 ]
 [-0.068]
 [-0.065]
 [-0.066]
 [-0.065]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.065]
 [-0.058]
 [-0.07 ]
 [-0.068]
 [-0.065]
 [-0.066]
 [-0.065]]
printing an ep nov before normalisation:  55.933767668592566
printing an ep nov before normalisation:  22.33673095703125
actor:  1 policy actor:  1  step number:  45 total reward:  0.4399999999999995  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.19696666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]]
siam score:  -0.8419429
maxi score, test score, baseline:  -0.19696666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.19696666666666676 0.6840000000000002 0.6840000000000002
maxi score, test score, baseline:  -0.19696666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.401946844220674
maxi score, test score, baseline:  -0.19696666666666676 0.6840000000000002 0.6840000000000002
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  68 total reward:  0.16666666666666585  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.19696666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.19696666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.77241230010986
printing an ep nov before normalisation:  46.94935316370154
printing an ep nov before normalisation:  46.75283520007362
maxi score, test score, baseline:  -0.19696666666666676 0.6840000000000002 0.6840000000000002
maxi score, test score, baseline:  -0.19696666666666676 0.6840000000000002 0.6840000000000002
maxi score, test score, baseline:  -0.19696666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.42666666666666664  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.643]
 [0.768]
 [0.643]
 [0.643]
 [0.643]
 [0.643]
 [0.643]] [[37.075]
 [45.303]
 [37.075]
 [37.075]
 [37.075]
 [37.075]
 [37.075]] [[0.643]
 [0.768]
 [0.643]
 [0.643]
 [0.643]
 [0.643]
 [0.643]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.4933333333333335  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.19696666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.19696666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  47 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.1940333333333334 0.6840000000000002 0.6840000000000002
printing an ep nov before normalisation:  34.88243938368249
Printing some Q and Qe and total Qs values:  [[0.4  ]
 [0.511]
 [0.4  ]
 [0.4  ]
 [0.34 ]
 [0.4  ]
 [0.4  ]] [[39.355]
 [45.97 ]
 [39.355]
 [39.355]
 [33.665]
 [39.355]
 [39.355]] [[0.933]
 [1.241]
 [0.933]
 [0.933]
 [0.704]
 [0.933]
 [0.933]]
maxi score, test score, baseline:  -0.1940333333333334 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.87675460229337
actor:  1 policy actor:  1  step number:  50 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  1.333
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.808]
 [0.898]
 [0.808]
 [0.808]
 [0.808]
 [0.808]
 [0.808]] [[41.93 ]
 [45.474]
 [41.93 ]
 [41.93 ]
 [41.93 ]
 [41.93 ]
 [41.93 ]] [[0.808]
 [0.898]
 [0.808]
 [0.808]
 [0.808]
 [0.808]
 [0.808]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.1940333333333334 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
printing an ep nov before normalisation:  24.38061237335205
maxi score, test score, baseline:  -0.1940333333333334 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.1940333333333334 0.6840000000000002 0.6840000000000002
actor:  0 policy actor:  0  step number:  43 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  47.72340471523842
maxi score, test score, baseline:  -0.19123333333333342 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.556]
 [0.459]
 [0.547]
 [0.509]
 [0.504]
 [0.445]] [[46.568]
 [42.488]
 [45.108]
 [44.695]
 [45.366]
 [46.458]
 [50.01 ]] [[1.802]
 [1.676]
 [1.702]
 [1.771]
 [1.764]
 [1.81 ]
 [1.918]]
Printing some Q and Qe and total Qs values:  [[0.252]
 [0.365]
 [0.252]
 [0.231]
 [0.252]
 [0.252]
 [0.252]] [[38.121]
 [48.199]
 [38.121]
 [48.686]
 [38.121]
 [38.121]
 [46.003]] [[0.436]
 [0.628]
 [0.436]
 [0.498]
 [0.436]
 [0.436]
 [0.498]]
printing an ep nov before normalisation:  40.42007553558769
printing an ep nov before normalisation:  52.54648050028203
maxi score, test score, baseline:  -0.19123333333333342 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.37474677723829
printing an ep nov before normalisation:  30.122857554063653
maxi score, test score, baseline:  -0.19123333333333342 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.35333333333333317  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  47 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.19123333333333342 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.27707498814647
printing an ep nov before normalisation:  0.02036147869361571
actor:  0 policy actor:  0  step number:  56 total reward:  0.13999999999999935  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.18895333333333345 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.90393157113872
maxi score, test score, baseline:  -0.18895333333333345 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.25989591136533
maxi score, test score, baseline:  -0.18895333333333345 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.32312581615686
maxi score, test score, baseline:  -0.18895333333333345 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.83937454
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.003]
 [1.016]
 [1.003]
 [1.003]
 [1.003]
 [1.003]
 [1.003]] [[43.274]
 [45.807]
 [43.274]
 [43.274]
 [43.274]
 [43.274]
 [43.274]] [[1.003]
 [1.016]
 [1.003]
 [1.003]
 [1.003]
 [1.003]
 [1.003]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.18895333333333345 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  41 total reward:  0.4933333333333333  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.468867505368856
printing an ep nov before normalisation:  28.083419799804688
maxi score, test score, baseline:  -0.18596666666666675 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.18596666666666675 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.92984247978268
printing an ep nov before normalisation:  55.96937367892928
Printing some Q and Qe and total Qs values:  [[0.528]
 [0.635]
 [0.531]
 [0.563]
 [0.521]
 [0.476]
 [0.581]] [[33.287]
 [31.335]
 [29.238]
 [30.375]
 [29.564]
 [30.362]
 [30.14 ]] [[1.736]
 [1.717]
 [1.477]
 [1.582]
 [1.488]
 [1.494]
 [1.585]]
printing an ep nov before normalisation:  29.015798568725586
printing an ep nov before normalisation:  0.032864622312445135
actor:  1 policy actor:  1  step number:  50 total reward:  0.4466666666666663  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.18596666666666675 0.6840000000000002 0.6840000000000002
maxi score, test score, baseline:  -0.18596666666666675 0.6840000000000002 0.6840000000000002
printing an ep nov before normalisation:  41.53255504247144
maxi score, test score, baseline:  -0.18596666666666675 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.36666666666666614  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.18596666666666675 0.6840000000000002 0.6840000000000002
maxi score, test score, baseline:  -0.18596666666666675 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.714]
 [0.78 ]
 [0.713]
 [0.793]
 [0.786]
 [0.782]
 [0.714]] [[36.115]
 [34.106]
 [36.95 ]
 [34.874]
 [33.766]
 [35.285]
 [37.518]] [[0.714]
 [0.78 ]
 [0.713]
 [0.793]
 [0.786]
 [0.782]
 [0.714]]
maxi score, test score, baseline:  -0.18596666666666675 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.18596666666666675 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.54155670811898
actor:  0 policy actor:  0  step number:  44 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]] [[41.429]
 [41.429]
 [41.429]
 [41.429]
 [41.429]
 [41.429]
 [41.429]] [[1.319]
 [1.319]
 [1.319]
 [1.319]
 [1.319]
 [1.319]
 [1.319]]
actor:  0 policy actor:  0  step number:  56 total reward:  0.2733333333333331  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  39.79989632215504
printing an ep nov before normalisation:  38.12346396667829
Printing some Q and Qe and total Qs values:  [[-0.042]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]] [[46.74]
 [46.74]
 [46.74]
 [46.74]
 [46.74]
 [46.74]
 [46.74]] [[0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.1805800000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.50270400033848
maxi score, test score, baseline:  -0.1805800000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8334642
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
printing an ep nov before normalisation:  46.27805745944853
printing an ep nov before normalisation:  54.42401669215471
printing an ep nov before normalisation:  55.79880867411136
printing an ep nov before normalisation:  53.245002539285814
printing an ep nov before normalisation:  39.59505558013916
maxi score, test score, baseline:  -0.1805800000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 27.776780378837017
printing an ep nov before normalisation:  43.980444310500616
printing an ep nov before normalisation:  42.13897345727633
Printing some Q and Qe and total Qs values:  [[-0.061]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]] [[32.535]
 [32.535]
 [32.535]
 [32.535]
 [32.535]
 [32.535]
 [32.535]] [[43.308]
 [43.308]
 [43.308]
 [43.308]
 [43.308]
 [43.308]
 [43.308]]
printing an ep nov before normalisation:  52.90364062062853
using explorer policy with actor:  1
printing an ep nov before normalisation:  21.345911026000977
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [ 0.045]
 [ 0.023]
 [ 0.023]
 [ 0.023]
 [ 0.011]
 [ 0.014]] [[27.501]
 [40.757]
 [31.006]
 [31.006]
 [31.006]
 [38.638]
 [40.237]] [[0.396]
 [0.83 ]
 [0.529]
 [0.529]
 [0.529]
 [0.736]
 [0.785]]
actor:  1 policy actor:  1  step number:  41 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.1805800000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.21591435882614
actor:  1 policy actor:  1  step number:  52 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.1805800000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.968679590181175
maxi score, test score, baseline:  -0.1805800000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.3133333333333328  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  35 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  39.25450683936353
Printing some Q and Qe and total Qs values:  [[0.793]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.688]
 [0.55 ]
 [0.55 ]] [[40.333]
 [33.795]
 [33.795]
 [33.795]
 [38.685]
 [33.795]
 [33.795]] [[0.793]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.688]
 [0.55 ]
 [0.55 ]]
actions average: 
K:  1  action  0 :  tensor([0.2751, 0.0168, 0.1123, 0.1366, 0.1614, 0.1740, 0.1238],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0050, 0.9419, 0.0042, 0.0077, 0.0038, 0.0055, 0.0318],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1368, 0.0630, 0.1974, 0.1579, 0.1339, 0.1871, 0.1239],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0947, 0.2140, 0.0795, 0.2580, 0.0911, 0.1687, 0.0939],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1314, 0.0050, 0.0490, 0.0513, 0.6236, 0.0716, 0.0681],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1032, 0.1174, 0.1058, 0.1116, 0.1093, 0.3621, 0.0907],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1510, 0.1609, 0.0944, 0.1143, 0.1128, 0.1331, 0.2335],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.18035333333333345 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.426928224362996
actor:  0 policy actor:  0  step number:  53 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.17782000000000014 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.17782000000000014 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.17782000000000014 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.17782000000000014 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.17782000000000014 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.943994730768104
maxi score, test score, baseline:  -0.1778200000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.1778200000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  46 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8450073
maxi score, test score, baseline:  -0.17506000000000013 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.3799999999999992  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.17506000000000013 0.6840000000000002 0.6840000000000002
maxi score, test score, baseline:  -0.17506000000000013 0.6840000000000002 0.6840000000000002
siam score:  -0.84621596
maxi score, test score, baseline:  -0.17506000000000013 0.6840000000000002 0.6840000000000002
Printing some Q and Qe and total Qs values:  [[-0.025]
 [-0.02 ]
 [-0.02 ]
 [-0.027]
 [-0.023]
 [-0.018]
 [-0.035]] [[53.575]
 [40.107]
 [50.478]
 [57.041]
 [54.356]
 [52.755]
 [53.177]] [[1.352]
 [0.823]
 [1.234]
 [1.488]
 [1.384]
 [1.327]
 [1.326]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.17506000000000013 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.929382665076517
maxi score, test score, baseline:  -0.17506000000000013 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.26924228668213
maxi score, test score, baseline:  -0.17506000000000013 0.6840000000000002 0.6840000000000002
printing an ep nov before normalisation:  22.169708018503034
actor:  1 policy actor:  1  step number:  58 total reward:  0.16666666666666619  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.901]
 [0.901]
 [0.901]
 [0.888]
 [0.901]
 [0.901]
 [0.901]] [[34.795]
 [34.795]
 [34.795]
 [31.743]
 [34.795]
 [34.795]
 [34.795]] [[2.74 ]
 [2.74 ]
 [2.74 ]
 [2.412]
 [2.74 ]
 [2.74 ]
 [2.74 ]]
maxi score, test score, baseline:  -0.17506000000000013 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  41.087589437612294
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85311174
printing an ep nov before normalisation:  38.82529699441534
maxi score, test score, baseline:  -0.17506000000000013 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  49 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
siam score:  -0.85269594
maxi score, test score, baseline:  -0.17250000000000013 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.17250000000000013 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  43 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.16956666666666678 0.6840000000000002 0.6840000000000002
printing an ep nov before normalisation:  52.21259357521032
maxi score, test score, baseline:  -0.16956666666666678 0.6840000000000002 0.6840000000000002
printing an ep nov before normalisation:  69.99317695960994
actions average: 
K:  4  action  0 :  tensor([0.3431, 0.0923, 0.0772, 0.0983, 0.1575, 0.1301, 0.1016],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0192, 0.8975, 0.0124, 0.0175, 0.0075, 0.0101, 0.0358],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1197, 0.0847, 0.2991, 0.1100, 0.1172, 0.1619, 0.1074],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1290, 0.0687, 0.0989, 0.2453, 0.1615, 0.1761, 0.1205],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2035, 0.0061, 0.0620, 0.0993, 0.3758, 0.1409, 0.1124],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1426, 0.0021, 0.1627, 0.1892, 0.1334, 0.2447, 0.1252],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1006, 0.2220, 0.0894, 0.1718, 0.0967, 0.1239, 0.1957],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.02  0.245 0.02  0.02  0.367 0.02  0.306]
printing an ep nov before normalisation:  25.11441946029663
maxi score, test score, baseline:  -0.16956666666666678 0.6840000000000002 0.6840000000000002
Printing some Q and Qe and total Qs values:  [[-0.004]
 [ 0.317]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]] [[47.551]
 [47.345]
 [47.551]
 [47.551]
 [47.551]
 [47.551]
 [47.551]] [[1.363]
 [1.675]
 [1.363]
 [1.363]
 [1.363]
 [1.363]
 [1.363]]
maxi score, test score, baseline:  -0.16956666666666678 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.292055130004883
maxi score, test score, baseline:  -0.16956666666666678 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.063]
 [ 0.288]
 [ 0.067]
 [ 0.049]
 [ 0.215]
 [ 0.159]
 [ 0.303]] [[35.299]
 [47.527]
 [33.154]
 [30.711]
 [36.694]
 [47.452]
 [46.882]] [[0.055]
 [0.505]
 [0.167]
 [0.129]
 [0.344]
 [0.376]
 [0.515]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.16956666666666678 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.4242, 0.0021, 0.1049, 0.1193, 0.1552, 0.1028, 0.0916],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0055, 0.9575, 0.0066, 0.0106, 0.0054, 0.0054, 0.0090],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0301, 0.0350, 0.5592, 0.0918, 0.0405, 0.1933, 0.0499],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1538, 0.0579, 0.1158, 0.1697, 0.1344, 0.1284, 0.2399],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1356, 0.0040, 0.1204, 0.1841, 0.2858, 0.1271, 0.1430],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1127, 0.0081, 0.1926, 0.1324, 0.1295, 0.2956, 0.1291],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1208, 0.0816, 0.1121, 0.1481, 0.1466, 0.1295, 0.2613],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  48 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.16956666666666678 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.16956666666666678 0.6840000000000002 0.6840000000000002
maxi score, test score, baseline:  -0.16956666666666678 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.16956666666666678 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  78 total reward:  0.16666666666666607  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.014]
 [-0.005]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]] [[33.849]
 [32.382]
 [34.418]
 [34.418]
 [34.418]
 [34.418]
 [34.418]] [[1.93 ]
 [1.794]
 [1.97 ]
 [1.97 ]
 [1.97 ]
 [1.97 ]
 [1.97 ]]
printing an ep nov before normalisation:  43.51252445049075
maxi score, test score, baseline:  -0.16956666666666678 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.23424142883117
printing an ep nov before normalisation:  28.859948968726165
maxi score, test score, baseline:  -0.16956666666666678 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.16956666666666678 0.6840000000000002 0.6840000000000002
maxi score, test score, baseline:  -0.16956666666666678 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.16956666666666678 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.4199999999999996  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.589]
 [0.58 ]
 [0.582]
 [0.58 ]
 [0.58 ]
 [0.58 ]
 [0.58 ]] [[13.592]
 [13.218]
 [13.047]
 [13.218]
 [13.218]
 [13.218]
 [13.218]] [[0.589]
 [0.58 ]
 [0.582]
 [0.58 ]
 [0.58 ]
 [0.58 ]
 [0.58 ]]
printing an ep nov before normalisation:  41.81276628947311
actor:  0 policy actor:  0  step number:  58 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  42.58701271767176
Printing some Q and Qe and total Qs values:  [[-0.089]
 [-0.108]
 [-0.109]
 [-0.097]
 [-0.096]
 [-0.109]
 [-0.114]] [[32.885]
 [39.783]
 [30.734]
 [25.246]
 [24.374]
 [24.553]
 [30.953]] [[0.341]
 [0.464]
 [0.276]
 [0.176]
 [0.159]
 [0.15 ]
 [0.276]]
Printing some Q and Qe and total Qs values:  [[-0.119]
 [-0.117]
 [-0.099]
 [-0.099]
 [-0.099]
 [-0.099]
 [-0.099]] [[44.268]
 [39.427]
 [42.3  ]
 [42.3  ]
 [42.3  ]
 [42.3  ]
 [42.3  ]] [[0.419]
 [0.311]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]]
printing an ep nov before normalisation:  46.09082024263216
maxi score, test score, baseline:  -0.16667333333333342 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.16667333333333342 0.6840000000000002 0.6840000000000002
line 256 mcts: sample exp_bonus 63.33459761128695
actor:  0 policy actor:  0  step number:  55 total reward:  0.3999999999999996  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  34 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  39.46099837147238
maxi score, test score, baseline:  -0.16387333333333348 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.16387333333333348 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  67 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.16387333333333348 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 42.53842474555101
actions average: 
K:  1  action  0 :  tensor([0.5292, 0.0151, 0.0952, 0.0893, 0.1058, 0.0869, 0.0786],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0104, 0.9232, 0.0139, 0.0178, 0.0079, 0.0105, 0.0163],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1143, 0.0118, 0.3320, 0.1600, 0.1331, 0.1332, 0.1156],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0842, 0.0128, 0.0921, 0.5187, 0.0926, 0.1059, 0.0937],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1599, 0.0043, 0.0970, 0.1128, 0.4048, 0.1146, 0.1065],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1530, 0.0043, 0.1418, 0.1568, 0.1637, 0.2408, 0.1396],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1343, 0.0228, 0.1196, 0.1780, 0.1460, 0.1329, 0.2664],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  51.696275426628965
printing an ep nov before normalisation:  49.4846362260141
maxi score, test score, baseline:  -0.16387333333333348 0.6840000000000002 0.6840000000000002
maxi score, test score, baseline:  -0.16387333333333348 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8484859
maxi score, test score, baseline:  -0.16387333333333348 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.16387333333333348 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.16387333333333348 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.16387333333333348 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.16387333333333348 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  47 total reward:  0.26666666666666616  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8496445
Printing some Q and Qe and total Qs values:  [[-0.232]
 [-0.259]
 [-0.264]
 [-0.26 ]
 [-0.266]
 [-0.268]
 [-0.263]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.232]
 [-0.259]
 [-0.264]
 [-0.26 ]
 [-0.266]
 [-0.268]
 [-0.263]]
siam score:  -0.8492402
maxi score, test score, baseline:  -0.16387333333333348 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.16387333333333348 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.657013215716695
maxi score, test score, baseline:  -0.16387333333333348 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  11.370361567546343
maxi score, test score, baseline:  -0.16387333333333343 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.215]
 [ 0.328]
 [-0.046]
 [ 0.2  ]
 [ 0.148]
 [ 0.2  ]
 [ 0.136]] [[37.679]
 [37.632]
 [37.798]
 [38.718]
 [37.523]
 [37.721]
 [35.16 ]] [[0.456]
 [0.568]
 [0.196]
 [0.454]
 [0.387]
 [0.441]
 [0.346]]
printing an ep nov before normalisation:  30.885005953218318
maxi score, test score, baseline:  -0.16387333333333343 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.182133197784424
maxi score, test score, baseline:  -0.16387333333333343 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.16387333333333343 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.16387333333333343 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  13.857707902066393
printing an ep nov before normalisation:  17.244446277618408
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.16387333333333343 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  0.1266666666666657  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  39.9466826027488
maxi score, test score, baseline:  -0.16387333333333343 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.16387333333333343 0.6840000000000002 0.6840000000000002
maxi score, test score, baseline:  -0.16387333333333343 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.061]
 [-0.047]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]] [[11.975]
 [43.027]
 [11.975]
 [11.975]
 [11.975]
 [11.975]
 [11.975]] [[0.015]
 [0.429]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]]
actor:  0 policy actor:  0  step number:  40 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.16095333333333345 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.012997601104146
printing an ep nov before normalisation:  31.6422936933337
printing an ep nov before normalisation:  34.673099312336696
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  62 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  34.88348960876465
maxi score, test score, baseline:  -0.16095333333333345 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.384]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]] [[43.972]
 [40.493]
 [43.972]
 [43.972]
 [43.972]
 [43.972]
 [43.972]] [[0.897]
 [0.889]
 [0.897]
 [0.897]
 [0.897]
 [0.897]
 [0.897]]
maxi score, test score, baseline:  -0.16095333333333345 0.6840000000000002 0.6840000000000002
actor:  1 policy actor:  1  step number:  61 total reward:  0.3199999999999992  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  50 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.16095333333333345 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.067]
 [-0.075]
 [-0.065]
 [-0.064]
 [-0.059]
 [-0.063]
 [-0.059]] [[21.961]
 [36.049]
 [23.898]
 [27.177]
 [32.421]
 [34.774]
 [24.34 ]] [[0.084]
 [0.373]
 [0.127]
 [0.197]
 [0.312]
 [0.358]
 [0.142]]
printing an ep nov before normalisation:  31.452175983281162
maxi score, test score, baseline:  -0.16095333333333345 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.16095333333333345 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.431]
 [0.534]
 [0.415]
 [0.392]
 [0.462]
 [0.349]
 [0.423]] [[36.131]
 [35.301]
 [37.945]
 [39.367]
 [35.253]
 [39.424]
 [34.919]] [[0.431]
 [0.534]
 [0.415]
 [0.392]
 [0.462]
 [0.349]
 [0.423]]
actor:  0 policy actor:  0  step number:  42 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.8457373
maxi score, test score, baseline:  -0.15803333333333341 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.416]
 [0.522]
 [0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.416]] [[32.994]
 [33.819]
 [32.994]
 [32.994]
 [32.994]
 [32.994]
 [32.994]] [[0.894]
 [1.026]
 [0.894]
 [0.894]
 [0.894]
 [0.894]
 [0.894]]
actor:  0 policy actor:  0  step number:  35 total reward:  0.4933333333333332  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.66543941122586
maxi score, test score, baseline:  -0.15504666666666678 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.15504666666666678 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.15504666666666678 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.15504666666666678 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.3602, 0.1623, 0.0979, 0.0791, 0.1211, 0.1047, 0.0747],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0164, 0.9174, 0.0100, 0.0226, 0.0047, 0.0040, 0.0248],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1463, 0.0221, 0.3955, 0.0925, 0.1110, 0.1272, 0.1055],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1235, 0.0677, 0.1065, 0.2690, 0.1815, 0.1585, 0.0934],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2003, 0.0441, 0.0996, 0.1112, 0.3034, 0.1166, 0.1248],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1282, 0.0602, 0.1336, 0.1018, 0.1162, 0.3542, 0.1057],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1586, 0.0397, 0.1223, 0.1429, 0.1007, 0.0953, 0.3405],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  0.019033727735404682
actor:  1 policy actor:  1  step number:  54 total reward:  0.28666666666666607  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  33.79328811864179
actor:  1 policy actor:  1  step number:  58 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.15504666666666678 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.15504666666666678 0.6840000000000002 0.6840000000000002
Printing some Q and Qe and total Qs values:  [[0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]] [[42.036]
 [42.036]
 [42.036]
 [42.036]
 [42.036]
 [42.036]
 [42.036]] [[0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]
 [0.757]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
maxi score, test score, baseline:  -0.15504666666666678 0.6840000000000002 0.6840000000000002
line 256 mcts: sample exp_bonus 36.23898392389154
Printing some Q and Qe and total Qs values:  [[0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]] [[36.451]
 [36.451]
 [36.451]
 [36.451]
 [36.451]
 [36.451]
 [36.451]] [[0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]]
Printing some Q and Qe and total Qs values:  [[0.303]
 [0.464]
 [0.303]
 [0.303]
 [0.303]
 [0.303]
 [0.303]] [[31.674]
 [39.343]
 [31.674]
 [31.674]
 [31.674]
 [31.674]
 [31.674]] [[0.885]
 [1.328]
 [0.885]
 [0.885]
 [0.885]
 [0.885]
 [0.885]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.15504666666666678 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  44 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.1521800000000001 0.6840000000000002 0.6840000000000002
maxi score, test score, baseline:  -0.1521800000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  33.31367554300937
Printing some Q and Qe and total Qs values:  [[0.73]
 [0.73]
 [0.73]
 [0.73]
 [0.73]
 [0.73]
 [0.73]] [[38.015]
 [38.015]
 [38.015]
 [38.015]
 [38.015]
 [38.015]
 [38.015]] [[2.077]
 [2.077]
 [2.077]
 [2.077]
 [2.077]
 [2.077]
 [2.077]]
printing an ep nov before normalisation:  35.98314613804389
maxi score, test score, baseline:  -0.1521800000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  42 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.1521800000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.992068115373897
actor:  1 policy actor:  1  step number:  48 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
maxi score, test score, baseline:  -0.1521800000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.00035270200004
actor:  1 policy actor:  1  step number:  50 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.1521800000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 38.58338451666392
printing an ep nov before normalisation:  40.322622159862725
maxi score, test score, baseline:  -0.1521800000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.1521800000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.361451148986816
maxi score, test score, baseline:  -0.1521800000000001 0.6840000000000002 0.6840000000000002
siam score:  -0.8328308
maxi score, test score, baseline:  -0.1521800000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.91425096067295
actor:  1 policy actor:  1  step number:  64 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.1521800000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.29635429382324
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  46 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.1521800000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.94344388757566
maxi score, test score, baseline:  -0.1521800000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 34.52902319587187
printing an ep nov before normalisation:  29.251487202619106
actor:  1 policy actor:  1  step number:  63 total reward:  0.2799999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.1521800000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
printing an ep nov before normalisation:  40.149127071543454
actor:  1 policy actor:  1  step number:  46 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.1521800000000001 0.6840000000000002 0.6840000000000002
printing an ep nov before normalisation:  45.22835488741241
printing an ep nov before normalisation:  67.1736610180513
maxi score, test score, baseline:  -0.1521800000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.1521800000000001 0.6840000000000002 0.6840000000000002
maxi score, test score, baseline:  -0.1521800000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.018]
 [ 0.01 ]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]] [[33.433]
 [36.22 ]
 [33.433]
 [33.433]
 [33.433]
 [33.433]
 [33.433]] [[1.154]
 [1.372]
 [1.154]
 [1.154]
 [1.154]
 [1.154]
 [1.154]]
maxi score, test score, baseline:  -0.1521800000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.715]
 [0.715]
 [0.715]
 [0.715]
 [0.715]
 [0.715]
 [0.715]] [[47.798]
 [47.798]
 [47.798]
 [47.798]
 [47.798]
 [47.798]
 [47.798]] [[0.995]
 [0.995]
 [0.995]
 [0.995]
 [0.995]
 [0.995]
 [0.995]]
maxi score, test score, baseline:  -0.1521800000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.05588265326147
maxi score, test score, baseline:  -0.1521800000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.26666666666666594  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.1521800000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8381691
actions average: 
K:  0  action  0 :  tensor([0.3647, 0.0060, 0.1207, 0.1272, 0.1646, 0.1167, 0.1001],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0050, 0.9730, 0.0036, 0.0059, 0.0020, 0.0017, 0.0089],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1184, 0.0048, 0.3264, 0.1298, 0.1209, 0.1387, 0.1610],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1391, 0.0707, 0.1271, 0.2844, 0.1169, 0.1126, 0.1493],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1298, 0.0083, 0.0954, 0.0844, 0.4798, 0.1167, 0.0855],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0910, 0.0043, 0.1328, 0.0957, 0.0960, 0.4944, 0.0858],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1614, 0.0949, 0.1587, 0.1401, 0.1441, 0.1499, 0.1508],
       grad_fn=<DivBackward0>)
actions average: 
K:  2  action  0 :  tensor([0.3342, 0.0073, 0.1288, 0.1150, 0.1904, 0.1193, 0.1050],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0123, 0.9511, 0.0056, 0.0093, 0.0017, 0.0017, 0.0184],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1093, 0.0135, 0.3933, 0.1045, 0.0907, 0.1889, 0.0999],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0037, 0.0423, 0.0421, 0.4559, 0.0223, 0.3918, 0.0420],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1683, 0.0665, 0.1043, 0.1008, 0.3573, 0.1027, 0.1001],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1309, 0.1103, 0.1571, 0.1603, 0.1426, 0.1941, 0.1047],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0913, 0.0829, 0.1018, 0.2103, 0.0962, 0.1000, 0.3174],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.49784576807023
printing an ep nov before normalisation:  62.86802622355057
maxi score, test score, baseline:  -0.1521800000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.59317503175821
maxi score, test score, baseline:  -0.1521800000000001 0.6840000000000002 0.6840000000000002
maxi score, test score, baseline:  -0.1521800000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3266666666666659  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.769]
 [0.927]
 [0.851]
 [0.851]
 [0.851]
 [0.851]
 [0.851]] [[37.248]
 [43.007]
 [40.969]
 [40.969]
 [40.969]
 [40.969]
 [40.969]] [[0.769]
 [0.927]
 [0.851]
 [0.851]
 [0.851]
 [0.851]
 [0.851]]
actor:  1 policy actor:  1  step number:  61 total reward:  0.37333333333333285  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.1521800000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.758]
 [0.864]
 [0.844]
 [0.824]
 [0.691]
 [0.797]
 [0.845]] [[49.198]
 [43.175]
 [44.014]
 [44.871]
 [45.589]
 [42.985]
 [44.275]] [[0.758]
 [0.864]
 [0.844]
 [0.824]
 [0.691]
 [0.797]
 [0.845]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.44 ]
 [0.533]
 [0.61 ]
 [0.536]
 [0.46 ]
 [0.61 ]
 [0.591]] [[47.853]
 [47.622]
 [45.08 ]
 [48.856]
 [48.94 ]
 [45.08 ]
 [48.047]] [[1.805]
 [1.888]
 [1.857]
 [1.943]
 [1.871]
 [1.857]
 [1.964]]
actor:  0 policy actor:  0  step number:  60 total reward:  0.04666666666666608  reward:  1.0 rdn_beta:  1.333
siam score:  -0.8406242
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.59 ]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]] [[38.993]
 [41.46 ]
 [38.993]
 [38.993]
 [38.993]
 [38.993]
 [38.993]] [[1.724]
 [1.985]
 [1.724]
 [1.724]
 [1.724]
 [1.724]
 [1.724]]
maxi score, test score, baseline:  -0.15008666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.15008666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.64280743452422
maxi score, test score, baseline:  -0.15008666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.575719833374023
Printing some Q and Qe and total Qs values:  [[0.54 ]
 [0.54 ]
 [0.54 ]
 [0.508]
 [0.54 ]
 [0.54 ]
 [0.54 ]] [[23.539]
 [23.539]
 [23.539]
 [34.555]
 [23.539]
 [23.539]
 [23.539]] [[1.11 ]
 [1.11 ]
 [1.11 ]
 [1.595]
 [1.11 ]
 [1.11 ]
 [1.11 ]]
maxi score, test score, baseline:  -0.15008666666666676 0.6840000000000002 0.6840000000000002
actor:  1 policy actor:  1  step number:  66 total reward:  0.2466666666666658  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  35.74347496032715
actor:  1 policy actor:  1  step number:  58 total reward:  0.206666666666666  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.15008666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.051]
 [-0.063]
 [-0.051]
 [-0.051]
 [-0.051]
 [-0.051]
 [-0.051]] [[12.283]
 [48.961]
 [12.283]
 [12.283]
 [12.283]
 [12.283]
 [12.283]] [[0.092]
 [0.919]
 [0.092]
 [0.092]
 [0.092]
 [0.092]
 [0.092]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.15008666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.88737114532407
maxi score, test score, baseline:  -0.15008666666666676 0.6840000000000002 0.6840000000000002
maxi score, test score, baseline:  -0.15008666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.830530254850636
maxi score, test score, baseline:  -0.15008666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.15008666666666676 0.6840000000000002 0.6840000000000002
maxi score, test score, baseline:  -0.15008666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.49 ]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]] [[39.368]
 [43.398]
 [39.368]
 [39.368]
 [39.368]
 [39.368]
 [39.368]] [[0.644]
 [0.724]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]]
actor:  1 policy actor:  1  step number:  44 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.15008666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.21071198929844
Printing some Q and Qe and total Qs values:  [[0.376]
 [0.558]
 [0.413]
 [0.383]
 [0.345]
 [0.372]
 [0.338]] [[39.466]
 [42.667]
 [39.827]
 [41.223]
 [43.389]
 [42.969]
 [39.533]] [[0.602]
 [0.823]
 [0.643]
 [0.63 ]
 [0.618]
 [0.639]
 [0.565]]
maxi score, test score, baseline:  -0.15008666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.15008666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.478052616119385
Printing some Q and Qe and total Qs values:  [[0.118]
 [0.234]
 [0.164]
 [0.164]
 [0.164]
 [0.164]
 [0.164]] [[46.422]
 [36.838]
 [43.018]
 [43.018]
 [43.018]
 [43.018]
 [43.018]] [[0.38 ]
 [0.427]
 [0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.401]]
Printing some Q and Qe and total Qs values:  [[0.85]
 [0.85]
 [0.85]
 [0.85]
 [0.85]
 [0.85]
 [0.85]] [[32.362]
 [32.362]
 [32.362]
 [32.362]
 [32.362]
 [32.362]
 [32.362]] [[0.85]
 [0.85]
 [0.85]
 [0.85]
 [0.85]
 [0.85]
 [0.85]]
printing an ep nov before normalisation:  50.3783150140531
printing an ep nov before normalisation:  37.372879575340356
maxi score, test score, baseline:  -0.15008666666666676 0.6840000000000002 0.6840000000000002
actor:  1 policy actor:  1  step number:  43 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.15008666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.2000557155868
actor:  0 policy actor:  0  step number:  48 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  3  action  0 :  tensor([0.5564, 0.0192, 0.0796, 0.0705, 0.1282, 0.0733, 0.0728],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0113, 0.9096, 0.0094, 0.0337, 0.0041, 0.0058, 0.0261],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1525, 0.1062, 0.2254, 0.1312, 0.1136, 0.1362, 0.1350],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0902, 0.0789, 0.1019, 0.4100, 0.0983, 0.1054, 0.1154],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1828, 0.0139, 0.0395, 0.0974, 0.5294, 0.0504, 0.0866],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1222, 0.0096, 0.1730, 0.1229, 0.1086, 0.3327, 0.1311],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1437, 0.0167, 0.1347, 0.1674, 0.1302, 0.1394, 0.2679],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.14743333333333342 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  34 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  1.0
siam score:  -0.843295
printing an ep nov before normalisation:  31.687250478409293
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.55243194196686
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  18.858206907562014
actor:  1 policy actor:  1  step number:  52 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  30.050320541844464
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.805]
 [0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]] [[19.942]
 [31.127]
 [19.942]
 [19.942]
 [19.942]
 [19.942]
 [19.942]] [[0.695]
 [0.805]
 [0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]]
Printing some Q and Qe and total Qs values:  [[0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]] [[40.575]
 [40.575]
 [40.575]
 [40.575]
 [40.575]
 [40.575]
 [40.575]] [[0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]]
Printing some Q and Qe and total Qs values:  [[-0.062]
 [-0.134]
 [-0.062]
 [-0.062]
 [-0.062]
 [-0.062]
 [-0.062]] [[10.893]
 [21.15 ]
 [10.893]
 [10.893]
 [10.893]
 [10.893]
 [10.893]] [[0.213]
 [0.411]
 [0.213]
 [0.213]
 [0.213]
 [0.213]
 [0.213]]
maxi score, test score, baseline:  -0.1443800000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.4607, 0.0402, 0.0973, 0.0923, 0.1050, 0.0930, 0.1116],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0068, 0.9497, 0.0067, 0.0059, 0.0022, 0.0024, 0.0263],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1488, 0.1135, 0.2234, 0.1152, 0.1352, 0.1395, 0.1244],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0571, 0.1283, 0.0568, 0.4428, 0.1026, 0.1156, 0.0968],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1775, 0.0377, 0.0927, 0.1241, 0.3462, 0.1019, 0.1200],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1229, 0.0166, 0.1753, 0.1039, 0.1211, 0.3291, 0.1311],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1028, 0.1845, 0.0927, 0.1405, 0.1063, 0.0884, 0.2849],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  54 total reward:  0.15333333333333266  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  59 total reward:  0.25333333333333297  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.14207333333333344 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.71130842841659
Printing some Q and Qe and total Qs values:  [[0.189]
 [0.382]
 [0.189]
 [0.189]
 [0.189]
 [0.189]
 [0.182]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.189]
 [0.382]
 [0.189]
 [0.189]
 [0.189]
 [0.189]
 [0.182]]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  48.899423134623724
maxi score, test score, baseline:  -0.14207333333333344 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.66836500167847
Printing some Q and Qe and total Qs values:  [[0.758]
 [0.865]
 [0.748]
 [0.747]
 [0.758]
 [0.758]
 [0.751]] [[40.   ]
 [47.232]
 [43.176]
 [43.105]
 [40.   ]
 [40.   ]
 [41.574]] [[0.758]
 [0.865]
 [0.748]
 [0.747]
 [0.758]
 [0.758]
 [0.751]]
actor:  1 policy actor:  1  step number:  40 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  41 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  74.9452029639996
actions average: 
K:  4  action  0 :  tensor([0.4017, 0.0153, 0.0938, 0.1366, 0.1206, 0.1412, 0.0907],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0146, 0.8478, 0.0128, 0.0449, 0.0089, 0.0141, 0.0570],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1440, 0.0612, 0.2414, 0.1342, 0.1193, 0.2016, 0.0983],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0955, 0.0994, 0.0912, 0.3504, 0.1013, 0.1611, 0.1011],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0939, 0.0006, 0.0795, 0.0865, 0.5412, 0.1290, 0.0693],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1565, 0.0435, 0.1402, 0.2280, 0.1189, 0.1774, 0.1354],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1323, 0.1074, 0.1062, 0.1813, 0.1313, 0.1979, 0.1436],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  43 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.13927333333333344 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.988551082658034
Printing some Q and Qe and total Qs values:  [[0.039]
 [0.122]
 [0.039]
 [0.039]
 [0.042]
 [0.039]
 [0.039]] [[38.774]
 [34.143]
 [38.774]
 [38.774]
 [38.296]
 [38.774]
 [38.774]] [[1.571]
 [1.358]
 [1.571]
 [1.571]
 [1.544]
 [1.571]
 [1.571]]
printing an ep nov before normalisation:  30.278861978675472
maxi score, test score, baseline:  -0.13927333333333344 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.13927333333333344 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.13927333333333344 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.13927333333333344 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.13927333333333344 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.13927333333333344 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.13927333333333344 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.13927333333333344 0.6840000000000002 0.6840000000000002
printing an ep nov before normalisation:  40.00839310089188
actions average: 
K:  1  action  0 :  tensor([0.5404, 0.0206, 0.0933, 0.0699, 0.0919, 0.0948, 0.0892],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0018,     0.9764,     0.0021,     0.0027,     0.0005,     0.0006,
            0.0159], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1432, 0.0057, 0.1606, 0.1656, 0.1532, 0.1726, 0.1992],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1168, 0.2111, 0.1076, 0.1987, 0.1045, 0.1501, 0.1113],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2083, 0.0108, 0.1133, 0.1349, 0.2427, 0.1635, 0.1265],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0941, 0.0887, 0.1365, 0.1077, 0.0975, 0.3653, 0.1102],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1423, 0.0243, 0.1447, 0.1247, 0.1368, 0.1555, 0.2719],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  46 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.13927333333333344 0.6840000000000002 0.6840000000000002
maxi score, test score, baseline:  -0.13927333333333344 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.13927333333333344 0.6840000000000002 0.6840000000000002
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.13927333333333344 0.6840000000000002 0.6840000000000002
maxi score, test score, baseline:  -0.13927333333333344 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.3668, 0.0016, 0.1118, 0.1121, 0.1309, 0.1495, 0.1272],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0169, 0.9304, 0.0137, 0.0090, 0.0095, 0.0055, 0.0148],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0900, 0.0054, 0.3187, 0.1606, 0.1256, 0.1821, 0.1177],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1111, 0.0071, 0.1135, 0.3068, 0.1352, 0.1855, 0.1409],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1328, 0.0110, 0.1096, 0.1487, 0.3248, 0.1552, 0.1179],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1245, 0.0286, 0.1409, 0.1069, 0.1476, 0.3372, 0.1141],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1870, 0.0166, 0.1209, 0.1465, 0.1619, 0.1544, 0.2127],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  40.538477857149836
maxi score, test score, baseline:  -0.13927333333333344 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  38 total reward:  0.54  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8432795
maxi score, test score, baseline:  -0.13927333333333344 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  73 total reward:  0.17333333333333245  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.13927333333333344 0.6840000000000002 0.6840000000000002
printing an ep nov before normalisation:  39.79307026195861
line 256 mcts: sample exp_bonus 46.657671832001654
printing an ep nov before normalisation:  51.410189602738676
printing an ep nov before normalisation:  56.26231667216828
actor:  1 policy actor:  1  step number:  66 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  47.82003646606092
actor:  1 policy actor:  1  step number:  44 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.019]] [[31.144]
 [31.144]
 [31.144]
 [31.144]
 [31.144]
 [31.144]
 [31.144]] [[0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]]
printing an ep nov before normalisation:  35.431969165802
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.518]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]] [[50.921]
 [51.513]
 [50.921]
 [50.921]
 [50.921]
 [50.921]
 [50.921]] [[1.766]
 [1.776]
 [1.766]
 [1.766]
 [1.766]
 [1.766]
 [1.766]]
printing an ep nov before normalisation:  43.10688753337022
printing an ep nov before normalisation:  42.583835101797874
maxi score, test score, baseline:  -0.13927333333333347 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.13927333333333347 0.6840000000000002 0.6840000000000002
maxi score, test score, baseline:  -0.13927333333333347 0.6840000000000002 0.6840000000000002
maxi score, test score, baseline:  -0.13927333333333347 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0025141301131270666
actor:  1 policy actor:  1  step number:  71 total reward:  0.19999999999999907  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  63 total reward:  0.21333333333333238  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.13927333333333347 0.6840000000000002 0.6840000000000002
printing an ep nov before normalisation:  61.626476431026965
maxi score, test score, baseline:  -0.13927333333333347 0.6840000000000002 0.6840000000000002
printing an ep nov before normalisation:  57.93227047803792
maxi score, test score, baseline:  -0.13927333333333347 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.52 ]
 [0.513]
 [0.513]
 [0.423]
 [0.376]
 [0.513]] [[44.063]
 [45.049]
 [41.358]
 [41.358]
 [46.778]
 [44.122]
 [41.358]] [[1.074]
 [1.211]
 [1.084]
 [1.084]
 [1.171]
 [1.037]
 [1.084]]
printing an ep nov before normalisation:  49.797994683866676
printing an ep nov before normalisation:  44.699155990977076
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.13927333333333344 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.13927333333333344 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.13927333333333344 0.6840000000000002 0.6840000000000002
maxi score, test score, baseline:  -0.13927333333333344 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  67 total reward:  0.13333333333333264  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  39.90116802825736
actor:  1 policy actor:  1  step number:  58 total reward:  0.17999999999999927  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  39.152102537255324
maxi score, test score, baseline:  -0.13927333333333344 0.6840000000000002 0.6840000000000002
actor:  0 policy actor:  0  step number:  36 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
maxi score, test score, baseline:  -0.13630000000000012 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  55 total reward:  0.09333333333333271  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  64.81215965967844
maxi score, test score, baseline:  -0.13411333333333345 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.5850366122896
printing an ep nov before normalisation:  36.30483627319336
maxi score, test score, baseline:  -0.13411333333333345 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.569]
 [0.464]
 [0.464]
 [0.464]
 [0.464]
 [0.464]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.464]
 [0.569]
 [0.464]
 [0.464]
 [0.464]
 [0.464]
 [0.464]]
maxi score, test score, baseline:  -0.13411333333333345 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.13411333333333345 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.26666666666666605  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  53 total reward:  0.4133333333333328  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  34.96659755706787
line 256 mcts: sample exp_bonus 48.84202160353507
maxi score, test score, baseline:  -0.13411333333333345 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.503667126076007
maxi score, test score, baseline:  -0.13411333333333345 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.50071114136876
actor:  0 policy actor:  0  step number:  41 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.709]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]] [[45.658]
 [46.597]
 [45.658]
 [45.658]
 [45.658]
 [45.658]
 [45.658]] [[0.827]
 [0.981]
 [0.827]
 [0.827]
 [0.827]
 [0.827]
 [0.827]]
siam score:  -0.84210324
printing an ep nov before normalisation:  34.624304679641625
maxi score, test score, baseline:  -0.13131333333333345 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.13131333333333345 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.85866539752026
actor:  1 policy actor:  1  step number:  61 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  59 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.13131333333333345 0.6840000000000002 0.6840000000000002
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.231144810379796
Printing some Q and Qe and total Qs values:  [[0.11 ]
 [0.194]
 [0.113]
 [0.103]
 [0.1  ]
 [0.108]
 [0.103]] [[58.296]
 [50.854]
 [58.68 ]
 [59.442]
 [58.803]
 [60.019]
 [61.443]] [[0.98 ]
 [0.86 ]
 [0.994]
 [1.004]
 [0.983]
 [1.024]
 [1.059]]
printing an ep nov before normalisation:  38.21052365955315
printing an ep nov before normalisation:  44.98859405517578
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  59 total reward:  0.2933333333333332  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  53 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  66.47039757678488
siam score:  -0.84862745
printing an ep nov before normalisation:  63.85977832947873
printing an ep nov before normalisation:  47.26291034085483
maxi score, test score, baseline:  -0.13131333333333345 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.73437985442134
Printing some Q and Qe and total Qs values:  [[1.011]
 [0.847]
 [1.011]
 [0.777]
 [1.011]
 [1.011]
 [0.851]] [[48.831]
 [49.033]
 [48.831]
 [55.424]
 [48.831]
 [48.831]
 [50.953]] [[1.011]
 [0.847]
 [1.011]
 [0.777]
 [1.011]
 [1.011]
 [0.851]]
maxi score, test score, baseline:  -0.13131333333333345 0.6840000000000002 0.6840000000000002
actor:  0 policy actor:  0  step number:  43 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.199]
 [0.322]
 [0.199]
 [0.199]
 [0.199]
 [0.199]
 [0.199]] [[57.689]
 [61.999]
 [57.689]
 [57.689]
 [57.689]
 [57.689]
 [57.689]] [[1.585]
 [1.867]
 [1.585]
 [1.585]
 [1.585]
 [1.585]
 [1.585]]
maxi score, test score, baseline:  -0.12856666666666675 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[-0.066]
 [-0.078]
 [-0.066]
 [-0.066]
 [-0.066]
 [-0.066]
 [-0.066]] [[53.973]
 [74.754]
 [53.973]
 [53.973]
 [53.973]
 [53.973]
 [53.973]] [[0.83 ]
 [1.374]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]]
siam score:  -0.8467912
printing an ep nov before normalisation:  83.96088204342908
actor:  0 policy actor:  0  step number:  37 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  1.0
siam score:  -0.84832793
maxi score, test score, baseline:  -0.1255000000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.1255000000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.57589006774443
maxi score, test score, baseline:  -0.1255000000000001 0.6840000000000002 0.6840000000000002
printing an ep nov before normalisation:  54.614633061177756
maxi score, test score, baseline:  -0.1282600000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.11296130626678
maxi score, test score, baseline:  -0.1282600000000001 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.3133333333333327  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  40 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  60.93881476932698
maxi score, test score, baseline:  -0.12531333333333342 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.12531333333333344 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.12531333333333344 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.12531333333333344 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.12531333333333344 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.12531333333333344 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.12531333333333344 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.12531333333333344 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  0.4066666666666663  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  25.066518783569336
actor:  0 policy actor:  0  step number:  45 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  58 total reward:  0.37999999999999934  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.627]
 [0.717]
 [0.619]
 [0.628]
 [0.617]
 [0.481]
 [0.624]] [[40.157]
 [39.773]
 [35.188]
 [31.684]
 [35.7  ]
 [37.829]
 [34.901]] [[0.627]
 [0.717]
 [0.619]
 [0.628]
 [0.617]
 [0.481]
 [0.624]]
Printing some Q and Qe and total Qs values:  [[-0.034]
 [-0.043]
 [-0.028]
 [-0.043]
 [-0.036]
 [-0.038]
 [-0.049]] [[54.281]
 [49.421]
 [52.111]
 [49.421]
 [53.545]
 [53.732]
 [52.811]] [[0.299]
 [0.238]
 [0.282]
 [0.238]
 [0.289]
 [0.289]
 [0.269]]
maxi score, test score, baseline:  -0.12251333333333342 0.6840000000000002 0.6840000000000002
maxi score, test score, baseline:  -0.12251333333333342 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.2796562933663
maxi score, test score, baseline:  -0.12251333333333342 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  33 total reward:  0.5866666666666667  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  49.65276162942911
printing an ep nov before normalisation:  56.86479933199442
printing an ep nov before normalisation:  48.4121414545296
siam score:  -0.84234697
maxi score, test score, baseline:  -0.11934000000000011 0.6840000000000002 0.6840000000000002
maxi score, test score, baseline:  -0.11934000000000011 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.11934000000000011 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.11934000000000011 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.11934000000000011 0.6840000000000002 0.6840000000000002
maxi score, test score, baseline:  -0.11934000000000011 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  10.719146728515625
actor:  1 policy actor:  1  step number:  61 total reward:  0.1599999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.11934000000000011 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.588]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]] [[46.853]
 [54.329]
 [46.853]
 [46.853]
 [46.853]
 [46.853]
 [46.853]] [[1.625]
 [2.057]
 [1.625]
 [1.625]
 [1.625]
 [1.625]
 [1.625]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.11934000000000011 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  2.0
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.11934000000000011 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.66093076978411
Printing some Q and Qe and total Qs values:  [[0.612]
 [0.674]
 [0.56 ]
 [0.535]
 [0.527]
 [0.282]
 [0.537]] [[45.797]
 [45.23 ]
 [38.838]
 [33.487]
 [38.119]
 [35.309]
 [30.95 ]] [[0.612]
 [0.674]
 [0.56 ]
 [0.535]
 [0.527]
 [0.282]
 [0.537]]
printing an ep nov before normalisation:  46.027359340245866
printing an ep nov before normalisation:  52.36811051149804
printing an ep nov before normalisation:  50.0530946410671
printing an ep nov before normalisation:  48.16019838446651
printing an ep nov before normalisation:  28.223547693198295
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.11934000000000011 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  39 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.11656666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.11656666666666676 0.6840000000000002 0.6840000000000002
maxi score, test score, baseline:  -0.11656666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.11656666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.15333333333333254  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  48.14549483892822
printing an ep nov before normalisation:  34.159195498705245
Printing some Q and Qe and total Qs values:  [[ 0.001]
 [ 0.029]
 [-0.002]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[26.839]
 [46.11 ]
 [42.064]
 [27.114]
 [27.371]
 [27.771]
 [33.077]] [[0.099]
 [0.266]
 [0.206]
 [0.1  ]
 [0.102]
 [0.104]
 [0.142]]
Printing some Q and Qe and total Qs values:  [[-0.049]
 [-0.055]
 [-0.05 ]
 [-0.051]
 [-0.075]
 [-0.063]
 [-0.048]] [[50.268]
 [55.002]
 [47.825]
 [48.43 ]
 [42.556]
 [48.751]
 [48.791]] [[0.892]
 [1.003]
 [0.831]
 [0.845]
 [0.676]
 [0.841]
 [0.857]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.11656666666666676 0.6840000000000002 0.6840000000000002
printing an ep nov before normalisation:  53.66706618217128
maxi score, test score, baseline:  -0.11656666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.11656666666666676 0.6840000000000002 0.6840000000000002
maxi score, test score, baseline:  -0.11656666666666676 0.6840000000000002 0.6840000000000002
maxi score, test score, baseline:  -0.11656666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.17845287211541
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
maxi score, test score, baseline:  -0.11656666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0021978159304580913
actor:  1 policy actor:  1  step number:  49 total reward:  0.23999999999999944  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  70 total reward:  0.25999999999999934  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.11656666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.92023095088778
maxi score, test score, baseline:  -0.11656666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.72687025222322
printing an ep nov before normalisation:  63.75619316564636
siam score:  -0.8392067
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.487]
 [0.395]
 [0.376]
 [0.434]
 [0.434]
 [0.491]] [[43.364]
 [41.216]
 [45.438]
 [41.427]
 [41.656]
 [41.656]
 [45.83 ]] [[1.497]
 [1.428]
 [1.533]
 [1.327]
 [1.395]
 [1.395]
 [1.647]]
printing an ep nov before normalisation:  52.41105686309968
printing an ep nov before normalisation:  50.03437519073486
Printing some Q and Qe and total Qs values:  [[0.551]
 [0.534]
 [0.551]
 [0.441]
 [0.416]
 [0.425]
 [0.483]] [[50.703]
 [47.905]
 [50.703]
 [53.701]
 [53.13 ]
 [52.508]
 [51.5  ]] [[1.269]
 [1.18 ]
 [1.269]
 [1.237]
 [1.198]
 [1.19 ]
 [1.222]]
printing an ep nov before normalisation:  39.823431968688965
actor:  1 policy actor:  1  step number:  52 total reward:  0.3933333333333333  reward:  1.0 rdn_beta:  1.333
siam score:  -0.83988845
maxi score, test score, baseline:  -0.11656666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  42 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  58 total reward:  0.3533333333333327  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.11656666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.02169096792324
printing an ep nov before normalisation:  34.10465203551151
maxi score, test score, baseline:  -0.11656666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.11656666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.11656666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.32361958996334
printing an ep nov before normalisation:  27.566939147947174
printing an ep nov before normalisation:  41.26242587535884
actor:  1 policy actor:  1  step number:  44 total reward:  0.5400000000000003  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.198]
 [0.214]
 [0.198]
 [0.192]
 [0.195]
 [0.19 ]
 [0.198]] [[66.03 ]
 [61.89 ]
 [66.03 ]
 [67.433]
 [66.712]
 [67.556]
 [66.03 ]] [[1.36 ]
 [1.26 ]
 [1.36 ]
 [1.393]
 [1.376]
 [1.395]
 [1.36 ]]
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.434]
 [0.391]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.388]] [[37.889]
 [37.922]
 [38.978]
 [38.918]
 [38.989]
 [39.227]
 [39.394]] [[1.328]
 [1.355]
 [1.363]
 [1.359]
 [1.363]
 [1.375]
 [1.381]]
maxi score, test score, baseline:  -0.11656666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.11656666666666676 0.6840000000000002 0.6840000000000002
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.11656666666666676 0.6840000000000002 0.6840000000000002
printing an ep nov before normalisation:  28.927721977233887
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.11656666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.449042320251465
printing an ep nov before normalisation:  25.632588752439354
printing an ep nov before normalisation:  0.00016964708038358367
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.539]
 [0.336]
 [0.465]
 [0.525]
 [0.469]
 [0.451]] [[49.841]
 [42.639]
 [49.061]
 [48.022]
 [55.86 ]
 [44.655]
 [40.262]] [[0.538]
 [0.539]
 [0.336]
 [0.465]
 [0.525]
 [0.469]
 [0.451]]
maxi score, test score, baseline:  -0.11656666666666676 0.6840000000000002 0.6840000000000002
actor:  1 policy actor:  1  step number:  52 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.11656666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.913]
 [0.976]
 [0.913]
 [0.913]
 [0.913]
 [0.913]
 [0.913]] [[57.425]
 [50.552]
 [57.425]
 [57.425]
 [57.425]
 [57.425]
 [57.425]] [[0.913]
 [0.976]
 [0.913]
 [0.913]
 [0.913]
 [0.913]
 [0.913]]
maxi score, test score, baseline:  -0.11656666666666676 0.6840000000000002 0.6840000000000002
printing an ep nov before normalisation:  30.694748038087575
actor:  1 policy actor:  1  step number:  51 total reward:  0.4133333333333329  reward:  1.0 rdn_beta:  1.333
siam score:  -0.8420617
actor:  0 policy actor:  0  step number:  46 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.11402000000000011 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.11402000000000011 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.11402000000000011 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.75543212890625
printing an ep nov before normalisation:  36.27051832801873
actor:  1 policy actor:  1  step number:  56 total reward:  0.3399999999999992  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.11402000000000011 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.00090503692627
printing an ep nov before normalisation:  64.53667135039794
maxi score, test score, baseline:  -0.11402000000000011 0.6840000000000002 0.6840000000000002
printing an ep nov before normalisation:  32.30151542269192
actor:  0 policy actor:  0  step number:  49 total reward:  0.3333333333333327  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.11135333333333343 0.6840000000000002 0.6840000000000002
printing an ep nov before normalisation:  50.01996622663171
maxi score, test score, baseline:  -0.11135333333333343 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.11135333333333343 0.6840000000000002 0.6840000000000002
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.478]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]] [[34.938]
 [47.234]
 [34.938]
 [34.938]
 [34.938]
 [34.938]
 [34.938]] [[1.001]
 [1.536]
 [1.001]
 [1.001]
 [1.001]
 [1.001]
 [1.001]]
maxi score, test score, baseline:  -0.11135333333333346 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.548]
 [0.639]
 [0.626]
 [0.547]
 [0.538]
 [0.498]
 [0.595]] [[27.352]
 [24.416]
 [26.727]
 [28.442]
 [28.357]
 [28.665]
 [26.775]] [[1.435]
 [1.431]
 [1.492]
 [1.468]
 [1.456]
 [1.426]
 [1.462]]
maxi score, test score, baseline:  -0.11135333333333346 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.11135333333333346 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.11135333333333346 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.453]
 [0.282]
 [0.282]
 [0.271]
 [0.282]
 [0.282]] [[38.819]
 [42.728]
 [38.819]
 [38.819]
 [37.111]
 [38.819]
 [38.819]] [[1.304]
 [1.658]
 [1.304]
 [1.304]
 [1.213]
 [1.304]
 [1.304]]
actor:  1 policy actor:  1  step number:  62 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.11135333333333346 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.448084547800605
actor:  1 policy actor:  1  step number:  58 total reward:  0.33999999999999897  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.11135333333333346 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.886571645469992
actor:  0 policy actor:  0  step number:  44 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8352849
actor:  1 policy actor:  1  step number:  73 total reward:  0.1466666666666654  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  66.93444005013947
actor:  1 policy actor:  1  step number:  43 total reward:  0.5466666666666666  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.10862000000000009 0.6840000000000002 0.6840000000000002
printing an ep nov before normalisation:  37.07636599884898
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.38 ]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]] [[40.132]
 [49.087]
 [40.132]
 [40.132]
 [40.132]
 [40.132]
 [40.132]] [[1.042]
 [1.395]
 [1.042]
 [1.042]
 [1.042]
 [1.042]
 [1.042]]
maxi score, test score, baseline:  -0.10862000000000009 0.6840000000000002 0.6840000000000002
maxi score, test score, baseline:  -0.10862000000000009 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.377713696559596
maxi score, test score, baseline:  -0.10862000000000009 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  33 total reward:  0.5866666666666667  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  52.363198768797254
maxi score, test score, baseline:  -0.10544666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
actor:  1 policy actor:  1  step number:  46 total reward:  0.5000000000000002  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  33.048060205247666
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.10544666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.10544666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.843305892288384
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.529]
 [0.562]
 [0.562]
 [0.562]
 [0.484]
 [0.562]] [[45.606]
 [52.213]
 [45.606]
 [45.606]
 [45.606]
 [52.533]
 [45.606]] [[1.565]
 [1.773]
 [1.565]
 [1.565]
 [1.565]
 [1.74 ]
 [1.565]]
Printing some Q and Qe and total Qs values:  [[0.31 ]
 [0.256]
 [0.31 ]
 [0.31 ]
 [0.31 ]
 [0.31 ]
 [0.31 ]] [[43.424]
 [50.252]
 [43.424]
 [43.424]
 [43.424]
 [43.424]
 [43.424]] [[1.087]
 [1.256]
 [1.087]
 [1.087]
 [1.087]
 [1.087]
 [1.087]]
maxi score, test score, baseline:  -0.10544666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.10544666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.346]
 [0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]] [[51.407]
 [55.132]
 [51.407]
 [51.407]
 [51.407]
 [51.407]
 [51.407]] [[1.13 ]
 [1.298]
 [1.13 ]
 [1.13 ]
 [1.13 ]
 [1.13 ]
 [1.13 ]]
maxi score, test score, baseline:  -0.10544666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.10544666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.053]
 [-0.053]
 [-0.053]
 [-0.046]
 [-0.053]
 [-0.146]
 [-0.053]] [[48.247]
 [48.247]
 [48.247]
 [55.489]
 [48.247]
 [55.307]
 [48.247]] [[0.359]
 [0.359]
 [0.359]
 [0.494]
 [0.359]
 [0.39 ]
 [0.359]]
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  33.064395067632894
printing an ep nov before normalisation:  33.10478687286377
printing an ep nov before normalisation:  53.826041279729026
printing an ep nov before normalisation:  35.178796926021995
printing an ep nov before normalisation:  52.14148256707293
printing an ep nov before normalisation:  6.36809545540018e-06
maxi score, test score, baseline:  -0.10544666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.47140092875739
maxi score, test score, baseline:  -0.10544666666666676 0.6840000000000002 0.6840000000000002
actor:  1 policy actor:  1  step number:  63 total reward:  0.39999999999999936  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.037]
 [-0.004]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]] [[57.885]
 [53.733]
 [57.885]
 [57.885]
 [57.885]
 [57.885]
 [57.885]] [[0.383]
 [0.355]
 [0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.383]]
maxi score, test score, baseline:  -0.10544666666666676 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  36 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.433]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]] [[30.642]
 [41.501]
 [30.642]
 [30.642]
 [30.642]
 [30.642]
 [30.642]] [[0.656]
 [1.093]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]]
actor:  0 policy actor:  0  step number:  54 total reward:  0.17999999999999938  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  57.52578024534892
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.522]
 [0.386]
 [0.512]
 [0.388]
 [0.388]
 [0.528]] [[43.457]
 [44.289]
 [48.342]
 [44.122]
 [43.457]
 [43.457]
 [43.895]] [[0.894]
 [1.049]
 [1.011]
 [1.035]
 [0.894]
 [0.894]
 [1.046]]
maxi score, test score, baseline:  -0.10308666666666677 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.10308666666666677 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.9730882816169
actor:  1 policy actor:  1  step number:  60 total reward:  0.35333333333333317  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.196]
 [-0.034]
 [-0.041]
 [-0.033]
 [-0.152]
 [-0.032]
 [-0.038]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.196]
 [-0.034]
 [-0.041]
 [-0.033]
 [-0.152]
 [-0.032]
 [-0.038]]
maxi score, test score, baseline:  -0.10308666666666677 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.42777756385332
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.772]
 [0.67 ]
 [0.673]
 [0.666]
 [0.666]
 [0.675]] [[43.745]
 [46.245]
 [45.88 ]
 [46.862]
 [43.745]
 [43.745]
 [47.868]] [[1.509]
 [1.693]
 [1.58 ]
 [1.614]
 [1.509]
 [1.509]
 [1.647]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.39333333333333287  reward:  1.0 rdn_beta:  1.0
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  55.583544100788096
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
printing an ep nov before normalisation:  28.962520910163605
maxi score, test score, baseline:  -0.10308666666666677 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.493984784428804
siam score:  -0.8413392
maxi score, test score, baseline:  -0.10308666666666677 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.84 ]
 [0.761]
 [0.697]
 [0.637]
 [0.663]
 [0.604]
 [0.68 ]] [[50.672]
 [43.602]
 [42.728]
 [46.769]
 [58.118]
 [64.91 ]
 [35.953]] [[0.84 ]
 [0.761]
 [0.697]
 [0.637]
 [0.663]
 [0.604]
 [0.68 ]]
printing an ep nov before normalisation:  55.46467354361457
actor:  1 policy actor:  1  step number:  49 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  44.58427218506215
maxi score, test score, baseline:  -0.10308666666666677 0.6840000000000002 0.6840000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.09080179868031
printing an ep nov before normalisation:  45.669023501120975
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  1 policy actor:  1  step number:  49 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333333  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  87.69099937886446
printing an ep nov before normalisation:  62.95394933521886
actor:  1 policy actor:  1  step number:  47 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.681]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]] [[49.591]
 [51.968]
 [49.591]
 [49.591]
 [49.591]
 [49.591]
 [49.591]] [[1.46 ]
 [1.726]
 [1.46 ]
 [1.46 ]
 [1.46 ]
 [1.46 ]
 [1.46 ]]
maxi score, test score, baseline:  -0.05866000000000011 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  42 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  52.8503760500647
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.661387819465055
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.651463149158126
printing an ep nov before normalisation:  38.80350351333618
Printing some Q and Qe and total Qs values:  [[0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]] [[47.04]
 [47.04]
 [47.04]
 [47.04]
 [47.04]
 [47.04]
 [47.04]] [[2.091]
 [2.091]
 [2.091]
 [2.091]
 [2.091]
 [2.091]
 [2.091]]
maxi score, test score, baseline:  -0.05866000000000011 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.667
siam score:  -0.836388
maxi score, test score, baseline:  -0.05866000000000011 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8365936
maxi score, test score, baseline:  -0.05866000000000011 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.625018625361975
printing an ep nov before normalisation:  29.896341358045973
printing an ep nov before normalisation:  35.71096420288086
Printing some Q and Qe and total Qs values:  [[0.172]
 [0.198]
 [0.172]
 [0.172]
 [0.172]
 [0.172]
 [0.172]] [[39.589]
 [49.992]
 [39.589]
 [39.589]
 [39.589]
 [39.589]
 [39.589]] [[0.992]
 [1.439]
 [0.992]
 [0.992]
 [0.992]
 [0.992]
 [0.992]]
maxi score, test score, baseline:  -0.061980000000000084 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.83952934
printing an ep nov before normalisation:  50.036007673361304
printing an ep nov before normalisation:  47.257917403077904
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.061980000000000084 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.880576760438714
actor:  1 policy actor:  1  step number:  54 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  45 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.061980000000000084 0.6970000000000002 0.6970000000000002
printing an ep nov before normalisation:  72.05951997645921
actions average: 
K:  1  action  0 :  tensor([0.3049, 0.0533, 0.0947, 0.1055, 0.2397, 0.0995, 0.1025],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0104, 0.9057, 0.0079, 0.0121, 0.0057, 0.0061, 0.0521],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1398, 0.0075, 0.4529, 0.0707, 0.0992, 0.1109, 0.1189],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0993, 0.0348, 0.0795, 0.5063, 0.0878, 0.0911, 0.1013],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1191, 0.0022, 0.0725, 0.0810, 0.5606, 0.0818, 0.0828],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1175, 0.0394, 0.1390, 0.1271, 0.1199, 0.3373, 0.1197],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1502, 0.0125, 0.1672, 0.1300, 0.1308, 0.1265, 0.2828],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  44.46791185891588
printing an ep nov before normalisation:  60.88636683677829
siam score:  -0.8430015
maxi score, test score, baseline:  -0.061980000000000084 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.98499986383489
maxi score, test score, baseline:  -0.061980000000000084 0.6970000000000002 0.6970000000000002
printing an ep nov before normalisation:  58.782272724953174
Printing some Q and Qe and total Qs values:  [[0.217]
 [0.378]
 [0.256]
 [0.239]
 [0.236]
 [0.237]
 [0.246]] [[38.542]
 [47.021]
 [42.735]
 [41.38 ]
 [40.759]
 [41.097]
 [40.097]] [[0.699]
 [1.081]
 [0.847]
 [0.795]
 [0.775]
 [0.785]
 [0.768]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.961859296019206
Printing some Q and Qe and total Qs values:  [[0.793]
 [0.954]
 [0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]] [[54.065]
 [48.504]
 [54.065]
 [54.065]
 [54.065]
 [54.065]
 [54.065]] [[0.793]
 [0.954]
 [0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]]
maxi score, test score, baseline:  -0.061980000000000084 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.061980000000000084 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.12313069198318
actor:  0 policy actor:  0  step number:  58 total reward:  0.09999999999999942  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.06307333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.4259, 0.0439, 0.1028, 0.1072, 0.1135, 0.1074, 0.0993],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0006,     0.9853,     0.0006,     0.0013,     0.0002,     0.0003,
            0.0117], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0816, 0.0109, 0.4751, 0.0661, 0.0837, 0.2189, 0.0637],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0935, 0.0156, 0.1034, 0.4184, 0.1138, 0.1275, 0.1279],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1509, 0.0037, 0.1162, 0.1124, 0.3909, 0.1163, 0.1095],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1150, 0.0117, 0.1712, 0.1273, 0.1256, 0.3513, 0.0978],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1300, 0.0122, 0.1269, 0.1616, 0.1308, 0.1445, 0.2940],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  43.873219163289704
maxi score, test score, baseline:  -0.06307333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.882194213183794
printing an ep nov before normalisation:  34.42643968133415
printing an ep nov before normalisation:  49.77623237606622
printing an ep nov before normalisation:  43.80535095570236
printing an ep nov before normalisation:  0.21822855604568758
actor:  0 policy actor:  0  step number:  50 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.541]
 [0.384]
 [0.387]
 [0.379]
 [0.378]
 [0.374]] [[28.704]
 [37.448]
 [28.233]
 [29.562]
 [27.309]
 [27.788]
 [27.273]] [[0.519]
 [0.765]
 [0.513]
 [0.53 ]
 [0.498]
 [0.503]
 [0.493]]
printing an ep nov before normalisation:  37.16218586979407
maxi score, test score, baseline:  -0.0639800000000001 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.21999999999999975  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  53 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.0639800000000001 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0639800000000001 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0639800000000001 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0639800000000001 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.840876
maxi score, test score, baseline:  -0.0639800000000001 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.37795060011501
printing an ep nov before normalisation:  55.78163716725943
maxi score, test score, baseline:  -0.0639800000000001 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.23 ]
 [0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.019]] [[52.507]
 [53.283]
 [52.507]
 [52.507]
 [52.507]
 [52.507]
 [52.507]] [[1.805]
 [2.065]
 [1.805]
 [1.805]
 [1.805]
 [1.805]
 [1.805]]
printing an ep nov before normalisation:  46.93777779278452
printing an ep nov before normalisation:  44.36546802520752
actor:  1 policy actor:  1  step number:  65 total reward:  0.21333333333333304  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  53.02696346232045
maxi score, test score, baseline:  -0.0639800000000001 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0639800000000001 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.50945711135864
using explorer policy with actor:  1
printing an ep nov before normalisation:  18.712430340903147
printing an ep nov before normalisation:  66.42519747927649
actor:  0 policy actor:  0  step number:  43 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  63 total reward:  0.3199999999999994  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.06112666666666678 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.2852, 0.0063, 0.0994, 0.1218, 0.2220, 0.1445, 0.1208],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0196, 0.8797, 0.0051, 0.0067, 0.0105, 0.0031, 0.0753],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1281, 0.0916, 0.2318, 0.1259, 0.1391, 0.1632, 0.1204],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1156, 0.0149, 0.1180, 0.2702, 0.1745, 0.1715, 0.1353],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1343, 0.0118, 0.0688, 0.1597, 0.3625, 0.1378, 0.1251],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1373, 0.0014, 0.2491, 0.1086, 0.1137, 0.2588, 0.1312],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1292, 0.0734, 0.0855, 0.1671, 0.1152, 0.0934, 0.3363],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.06112666666666678 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.84181486827884
printing an ep nov before normalisation:  47.365583093185435
printing an ep nov before normalisation:  40.22909733216624
printing an ep nov before normalisation:  10.449075005680537
maxi score, test score, baseline:  -0.06112666666666679 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.58680534362793
actor:  1 policy actor:  1  step number:  55 total reward:  0.31999999999999984  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.06112666666666679 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.03999999999999948  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.06112666666666679 0.6970000000000002 0.6970000000000002
printing an ep nov before normalisation:  50.013669403451416
maxi score, test score, baseline:  -0.06112666666666679 0.6970000000000002 0.6970000000000002
printing an ep nov before normalisation:  36.86447806975002
maxi score, test score, baseline:  -0.06112666666666679 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.06112666666666679 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.06112666666666679 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.25333333333333286  reward:  1.0 rdn_beta:  0.667
siam score:  -0.83063096
siam score:  -0.83043975
actor:  1 policy actor:  1  step number:  49 total reward:  0.5466666666666669  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  36.12983477102388
maxi score, test score, baseline:  -0.06112666666666679 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.52  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  54.42827015425529
printing an ep nov before normalisation:  40.844361182253
Printing some Q and Qe and total Qs values:  [[0.814]
 [0.857]
 [0.39 ]
 [0.725]
 [0.766]
 [0.498]
 [0.764]] [[46.773]
 [44.676]
 [39.355]
 [37.606]
 [42.517]
 [39.365]
 [40.811]] [[0.814]
 [0.857]
 [0.39 ]
 [0.725]
 [0.766]
 [0.498]
 [0.764]]
actions average: 
K:  1  action  0 :  tensor([0.4544, 0.0077, 0.1017, 0.0990, 0.1140, 0.1255, 0.0977],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0048, 0.9606, 0.0041, 0.0065, 0.0035, 0.0040, 0.0167],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1163, 0.0135, 0.2821, 0.1624, 0.1235, 0.1769, 0.1252],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1412, 0.0506, 0.1384, 0.1950, 0.1389, 0.1564, 0.1795],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.1697,     0.0003,     0.0796,     0.0796,     0.4617,     0.0996,
            0.1096], grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0881, 0.0153, 0.1410, 0.1143, 0.1024, 0.4426, 0.0964],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0735, 0.1979, 0.0805, 0.0900, 0.0558, 0.0604, 0.4420],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  49.57767405914162
maxi score, test score, baseline:  -0.06112666666666679 0.6970000000000002 0.6970000000000002
printing an ep nov before normalisation:  37.07921188837196
maxi score, test score, baseline:  -0.06112666666666679 0.6970000000000002 0.6970000000000002
printing an ep nov before normalisation:  54.42928398565346
actor:  0 policy actor:  0  step number:  37 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.05824666666666678 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.62430075867867
maxi score, test score, baseline:  -0.05824666666666678 0.6970000000000002 0.6970000000000002
actor:  0 policy actor:  0  step number:  55 total reward:  0.10666666666666591  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.056033333333333435 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.87858772277832
maxi score, test score, baseline:  -0.056033333333333435 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.84842080916927
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.512]
 [0.355]
 [0.355]
 [0.355]
 [0.355]
 [0.355]] [[42.221]
 [38.516]
 [42.221]
 [42.221]
 [42.221]
 [42.221]
 [42.221]] [[0.853]
 [0.919]
 [0.853]
 [0.853]
 [0.853]
 [0.853]
 [0.853]]
printing an ep nov before normalisation:  52.980743428355765
maxi score, test score, baseline:  -0.056033333333333435 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.51447373843173
printing an ep nov before normalisation:  45.48977106824508
Printing some Q and Qe and total Qs values:  [[0.683]
 [0.845]
 [0.687]
 [0.688]
 [0.77 ]
 [0.683]
 [0.683]] [[35.663]
 [40.08 ]
 [35.344]
 [34.526]
 [38.451]
 [34.04 ]
 [34.179]] [[0.683]
 [0.845]
 [0.687]
 [0.688]
 [0.77 ]
 [0.683]
 [0.683]]
actor:  1 policy actor:  1  step number:  33 total reward:  0.5866666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.056033333333333435 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.321]
 [0.405]
 [0.311]
 [0.294]
 [0.295]
 [0.302]
 [0.235]] [[71.365]
 [65.078]
 [77.015]
 [78.42 ]
 [78.759]
 [78.453]
 [78.729]] [[1.654]
 [1.576]
 [1.79 ]
 [1.809]
 [1.818]
 [1.818]
 [1.757]]
actor:  1 policy actor:  1  step number:  74 total reward:  0.11333333333333229  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.056033333333333435 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.484]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]] [[67.276]
 [63.343]
 [67.276]
 [67.276]
 [67.276]
 [67.276]
 [67.276]] [[1.87 ]
 [1.852]
 [1.87 ]
 [1.87 ]
 [1.87 ]
 [1.87 ]
 [1.87 ]]
printing an ep nov before normalisation:  60.445131115357334
UNIT TEST: sample policy line 217 mcts : [0.041 0.388 0.061 0.061 0.143 0.061 0.245]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.056033333333333435 0.6970000000000002 0.6970000000000002
maxi score, test score, baseline:  -0.056033333333333435 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.1744633324872
actor:  0 policy actor:  0  step number:  41 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.667
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.691]
 [0.884]
 [0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.691]] [[32.832]
 [42.149]
 [32.832]
 [32.832]
 [32.832]
 [32.832]
 [32.832]] [[0.691]
 [0.884]
 [0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.691]]
actor:  1 policy actor:  1  step number:  57 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  27.972092628479004
maxi score, test score, baseline:  -0.05323333333333345 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.05323333333333345 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  58 total reward:  0.046666666666665746  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.05114000000000011 0.6970000000000002 0.6970000000000002
maxi score, test score, baseline:  -0.05114000000000011 0.6970000000000002 0.6970000000000002
maxi score, test score, baseline:  -0.05114000000000011 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  0.3066666666666661  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  61.71173461318277
printing an ep nov before normalisation:  52.92350308513007
printing an ep nov before normalisation:  49.966813416522875
maxi score, test score, baseline:  -0.05114000000000011 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.54977430217253
printing an ep nov before normalisation:  50.063216778372485
siam score:  -0.8367679
printing an ep nov before normalisation:  0.007493260982300853
actor:  1 policy actor:  1  step number:  43 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  0.0005959547695510992
actor:  1 policy actor:  1  step number:  47 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.05114000000000011 0.6970000000000002 0.6970000000000002
printing an ep nov before normalisation:  57.02186209820365
printing an ep nov before normalisation:  39.59069008774868
maxi score, test score, baseline:  -0.05114000000000011 0.6970000000000002 0.6970000000000002
printing an ep nov before normalisation:  43.24660064432779
printing an ep nov before normalisation:  43.85835117816157
maxi score, test score, baseline:  -0.05114000000000011 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.450414180438585
actor:  0 policy actor:  0  step number:  47 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  53 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  48 total reward:  0.4066666666666662  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  37.47231960296631
maxi score, test score, baseline:  -0.04847333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.69971808188766
printing an ep nov before normalisation:  44.54711366057465
printing an ep nov before normalisation:  53.412065505981445
line 256 mcts: sample exp_bonus 54.70746906253831
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.04847333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  17.055976390838623
actor:  1 policy actor:  1  step number:  62 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.04847333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 43.72169402165613
Printing some Q and Qe and total Qs values:  [[-0.023]
 [-0.021]
 [-0.023]
 [-0.023]
 [-0.023]
 [-0.023]
 [-0.023]] [[ 0.  ]
 [32.99]
 [ 0.  ]
 [ 0.  ]
 [ 0.  ]
 [ 0.  ]
 [ 0.  ]] [[-0.804]
 [ 1.193]
 [-0.804]
 [-0.804]
 [-0.804]
 [-0.804]
 [-0.804]]
maxi score, test score, baseline:  -0.04847333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.04847333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.38382839098296
actor:  1 policy actor:  1  step number:  57 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.04847333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.04847333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.025]
 [-0.025]
 [-0.025]
 [-0.025]
 [-0.055]
 [-0.025]
 [-0.025]] [[47.932]
 [47.932]
 [47.932]
 [47.932]
 [38.657]
 [47.932]
 [47.932]] [[0.886]
 [0.886]
 [0.886]
 [0.886]
 [0.564]
 [0.886]
 [0.886]]
Printing some Q and Qe and total Qs values:  [[0.736]
 [0.877]
 [0.709]
 [0.794]
 [0.73 ]
 [0.69 ]
 [0.73 ]] [[37.865]
 [39.448]
 [38.421]
 [35.702]
 [40.065]
 [38.572]
 [34.51 ]] [[0.736]
 [0.877]
 [0.709]
 [0.794]
 [0.73 ]
 [0.69 ]
 [0.73 ]]
maxi score, test score, baseline:  -0.04847333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  57.65308380126953
line 256 mcts: sample exp_bonus 45.001461640244024
Printing some Q and Qe and total Qs values:  [[-0.158]
 [-0.159]
 [-0.16 ]
 [-0.043]
 [-0.16 ]
 [-0.161]
 [-0.043]] [[41.541]
 [42.483]
 [41.632]
 [41.608]
 [41.212]
 [41.933]
 [41.608]] [[0.669]
 [0.704]
 [0.669]
 [0.786]
 [0.654]
 [0.68 ]
 [0.786]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.04847333333333344 0.6970000000000002 0.6970000000000002
maxi score, test score, baseline:  -0.04847333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.65812371503576
printing an ep nov before normalisation:  25.287890505982062
maxi score, test score, baseline:  -0.048473333333333424 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.20754623413086
printing an ep nov before normalisation:  22.388144646586333
line 256 mcts: sample exp_bonus 51.35558901722303
printing an ep nov before normalisation:  38.7135051869517
printing an ep nov before normalisation:  43.9713366646677
printing an ep nov before normalisation:  35.99575427402875
maxi score, test score, baseline:  -0.04847333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.537930488586426
maxi score, test score, baseline:  -0.04847333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.4140, 0.0459, 0.1197, 0.1055, 0.1042, 0.1121, 0.0988],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0105, 0.9544, 0.0055, 0.0066, 0.0034, 0.0038, 0.0157],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1860, 0.0326, 0.2098, 0.1120, 0.0971, 0.2076, 0.1549],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0823, 0.0738, 0.0909, 0.3663, 0.1045, 0.0749, 0.2074],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2511, 0.0101, 0.1264, 0.1202, 0.2466, 0.1291, 0.1164],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2026, 0.0159, 0.1725, 0.1412, 0.1504, 0.1769, 0.1405],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1417, 0.0905, 0.1081, 0.1678, 0.1130, 0.1199, 0.2590],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.617]
 [0.489]
 [0.617]
 [0.617]
 [0.617]
 [0.617]] [[22.645]
 [22.645]
 [31.763]
 [22.645]
 [22.645]
 [22.645]
 [22.645]] [[1.718]
 [1.718]
 [2.034]
 [1.718]
 [1.718]
 [1.718]
 [1.718]]
actor:  1 policy actor:  1  step number:  58 total reward:  0.20666666666666644  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  47.03741936237714
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.312]
 [0.359]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]] [[37.163]
 [52.511]
 [37.163]
 [37.163]
 [37.163]
 [37.163]
 [37.163]] [[0.922]
 [1.499]
 [0.922]
 [0.922]
 [0.922]
 [0.922]
 [0.922]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.44299459580882
printing an ep nov before normalisation:  24.479269981384277
actor:  0 policy actor:  0  step number:  55 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.04588666666666677 0.6970000000000002 0.6970000000000002
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.77262323061643
actions average: 
K:  4  action  0 :  tensor([0.4736, 0.0051, 0.0845, 0.1004, 0.1297, 0.0806, 0.1261],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0164, 0.9075, 0.0147, 0.0172, 0.0098, 0.0098, 0.0247],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0774, 0.0510, 0.3164, 0.1290, 0.0967, 0.2372, 0.0924],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1316, 0.0775, 0.1245, 0.2277, 0.1461, 0.1490, 0.1437],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1711, 0.0043, 0.0952, 0.1279, 0.3814, 0.1082, 0.1118],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1241, 0.1439, 0.1109, 0.1501, 0.1277, 0.2251, 0.1182],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1336, 0.0812, 0.1054, 0.1903, 0.1153, 0.1151, 0.2590],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.04588666666666677 0.6970000000000002 0.6970000000000002
printing an ep nov before normalisation:  32.11117560874107
printing an ep nov before normalisation:  31.824283599853516
actor:  1 policy actor:  1  step number:  54 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.04588666666666677 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.04588666666666677 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  0.14666666666666595  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.04588666666666677 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  42 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.114]
 [0.172]
 [0.114]
 [0.114]
 [0.104]
 [0.049]
 [0.114]] [[52.062]
 [54.831]
 [52.062]
 [52.062]
 [57.825]
 [62.317]
 [52.062]] [[1.096]
 [1.235]
 [1.096]
 [1.096]
 [1.255]
 [1.331]
 [1.096]]
Printing some Q and Qe and total Qs values:  [[0.178]
 [0.305]
 [0.178]
 [0.178]
 [0.095]
 [0.074]
 [0.178]] [[42.155]
 [38.446]
 [42.155]
 [42.155]
 [44.359]
 [45.63 ]
 [42.155]] [[1.907]
 [1.743]
 [1.907]
 [1.907]
 [1.996]
 [2.074]
 [1.907]]
siam score:  -0.82080305
maxi score, test score, baseline:  -0.04294000000000012 0.6970000000000002 0.6970000000000002
maxi score, test score, baseline:  -0.04294000000000012 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.883673667907715
using explorer policy with actor:  1
printing an ep nov before normalisation:  53.29104641440674
maxi score, test score, baseline:  -0.04294000000000012 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.79820340246977
maxi score, test score, baseline:  -0.04294000000000012 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.00027298319082547096
actor:  1 policy actor:  1  step number:  50 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.04294000000000012 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 53.500562738647666
actions average: 
K:  4  action  0 :  tensor([0.5724, 0.0094, 0.0534, 0.0627, 0.1494, 0.0774, 0.0753],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0235, 0.8308, 0.0149, 0.0386, 0.0194, 0.0234, 0.0494],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1065, 0.0828, 0.3444, 0.1199, 0.1129, 0.1280, 0.1054],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0912, 0.3130, 0.0655, 0.2583, 0.0765, 0.1162, 0.0793],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0901, 0.1045, 0.0892, 0.1237, 0.3274, 0.1669, 0.0982],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1284, 0.1802, 0.1604, 0.1585, 0.0849, 0.1418, 0.1458],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0807, 0.2473, 0.1530, 0.0971, 0.0629, 0.1303, 0.2287],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  51.20744723414591
Printing some Q and Qe and total Qs values:  [[0.842]
 [0.898]
 [0.842]
 [0.842]
 [0.842]
 [0.842]
 [0.842]] [[62.165]
 [52.56 ]
 [62.165]
 [62.165]
 [62.165]
 [62.165]
 [62.165]] [[0.842]
 [0.898]
 [0.842]
 [0.842]
 [0.842]
 [0.842]
 [0.842]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.04294000000000012 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 47.997159284181905
actor:  0 policy actor:  0  step number:  54 total reward:  0.12666666666666604  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  45 total reward:  0.5600000000000002  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.778]
 [0.886]
 [0.778]
 [0.77 ]
 [0.778]
 [0.778]
 [0.778]] [[36.834]
 [44.478]
 [36.834]
 [39.87 ]
 [36.834]
 [36.834]
 [36.834]] [[0.778]
 [0.886]
 [0.778]
 [0.77 ]
 [0.778]
 [0.778]
 [0.778]]
maxi score, test score, baseline:  -0.040686666666666774 0.6970000000000002 0.6970000000000002
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.464]
 [0.218]
 [0.299]
 [0.333]
 [0.333]
 [0.358]] [[60.28 ]
 [55.658]
 [62.663]
 [62.588]
 [60.28 ]
 [60.28 ]
 [60.359]] [[1.254]
 [1.279]
 [1.194]
 [1.273]
 [1.254]
 [1.254]
 [1.281]]
actor:  0 policy actor:  0  step number:  51 total reward:  0.3599999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.03796666666666678 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.03796666666666678 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.147]
 [0.315]
 [0.147]
 [0.147]
 [0.147]
 [0.147]
 [0.147]] [[47.388]
 [55.756]
 [47.388]
 [47.388]
 [47.388]
 [47.388]
 [47.388]] [[0.724]
 [1.05 ]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.03796666666666678 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.03796666666666678 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.03796666666666678 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  51 total reward:  0.3866666666666666  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  44 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.053]
 [-0.025]
 [-0.053]
 [-0.053]
 [-0.053]
 [-0.053]
 [-0.053]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.053]
 [-0.025]
 [-0.053]
 [-0.053]
 [-0.053]
 [-0.053]
 [-0.053]]
maxi score, test score, baseline:  -0.03519333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.03519333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.85566168606558
printing an ep nov before normalisation:  12.279552039675126
printing an ep nov before normalisation:  61.83387653372311
actor:  1 policy actor:  1  step number:  58 total reward:  0.3399999999999994  reward:  1.0 rdn_beta:  1.0
siam score:  -0.83002746
maxi score, test score, baseline:  -0.03519333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  37.2970004189703
line 256 mcts: sample exp_bonus 53.0425980345428
maxi score, test score, baseline:  -0.03519333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.193627778476106
printing an ep nov before normalisation:  38.0866706089607
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.035193333333333444 0.6970000000000002 0.6970000000000002
printing an ep nov before normalisation:  17.237028538687582
actor:  1 policy actor:  1  step number:  61 total reward:  0.173333333333333  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.03519333333333344 0.6970000000000002 0.6970000000000002
maxi score, test score, baseline:  -0.03519333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.3466666666666661  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.03519333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.043]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.043]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.024]
 [-0.03 ]
 [-0.163]
 [-0.043]
 [-0.027]
 [-0.222]
 [-0.038]] [[39.363]
 [37.583]
 [40.399]
 [43.036]
 [44.011]
 [44.46 ]
 [39.425]] [[0.784]
 [0.692]
 [0.696]
 [0.944]
 [1.007]
 [0.834]
 [0.773]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  38.11826743866205
siam score:  -0.83128035
Printing some Q and Qe and total Qs values:  [[-0.033]
 [-0.027]
 [-0.039]
 [-0.035]
 [-0.031]
 [-0.033]
 [-0.037]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.033]
 [-0.027]
 [-0.039]
 [-0.035]
 [-0.031]
 [-0.033]
 [-0.037]]
printing an ep nov before normalisation:  54.9721864981407
actor:  1 policy actor:  1  step number:  56 total reward:  0.326666666666666  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  43.41363885280931
printing an ep nov before normalisation:  45.910564466405404
Printing some Q and Qe and total Qs values:  [[0.808]
 [0.813]
 [0.392]
 [0.725]
 [0.772]
 [0.455]
 [0.733]] [[41.273]
 [40.766]
 [30.48 ]
 [32.377]
 [37.699]
 [33.256]
 [34.159]] [[0.808]
 [0.813]
 [0.392]
 [0.725]
 [0.772]
 [0.455]
 [0.733]]
Printing some Q and Qe and total Qs values:  [[-0.027]
 [-0.022]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]] [[ 0.   ]
 [40.815]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.552]
 [ 1.095]
 [-0.552]
 [-0.552]
 [-0.552]
 [-0.552]
 [-0.552]]
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  40 total reward:  0.44666666666666666  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.03519333333333344 0.6970000000000002 0.6970000000000002
printing an ep nov before normalisation:  37.63832118575814
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
maxi score, test score, baseline:  -0.03519333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  47 total reward:  0.2933333333333328  reward:  1.0 rdn_beta:  2.0
siam score:  -0.83422107
maxi score, test score, baseline:  -0.03495333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.03495333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.0597675067919
Printing some Q and Qe and total Qs values:  [[0.678]
 [0.707]
 [0.675]
 [0.651]
 [0.673]
 [0.677]
 [0.677]] [[36.741]
 [37.088]
 [36.29 ]
 [35.583]
 [36.763]
 [36.204]
 [35.875]] [[2.416]
 [2.462]
 [2.392]
 [2.334]
 [2.412]
 [2.39 ]
 [2.374]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
printing an ep nov before normalisation:  21.900850785051517
Printing some Q and Qe and total Qs values:  [[0.69 ]
 [0.824]
 [0.651]
 [0.777]
 [0.693]
 [0.59 ]
 [0.725]] [[40.706]
 [42.275]
 [37.135]
 [40.283]
 [41.878]
 [40.223]
 [43.259]] [[2.086]
 [2.315]
 [1.829]
 [2.147]
 [2.16 ]
 [1.957]
 [2.276]]
maxi score, test score, baseline:  -0.03495333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.03495333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  72 total reward:  0.20666666666666544  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  49 total reward:  0.34666666666666657  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.031]
 [-0.012]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]] [[35.242]
 [52.657]
 [35.242]
 [35.242]
 [35.242]
 [35.242]
 [35.242]] [[0.332]
 [0.925]
 [0.332]
 [0.332]
 [0.332]
 [0.332]
 [0.332]]
maxi score, test score, baseline:  -0.03495333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.031]
 [-0.004]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]] [[56.037]
 [63.759]
 [56.037]
 [56.037]
 [56.037]
 [56.037]
 [56.037]] [[1.018]
 [1.332]
 [1.018]
 [1.018]
 [1.018]
 [1.018]
 [1.018]]
maxi score, test score, baseline:  -0.03495333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.03495333333333344 0.6970000000000002 0.6970000000000002
printing an ep nov before normalisation:  39.83180832507561
maxi score, test score, baseline:  -0.03495333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  70 total reward:  0.04666666666666586  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
maxi score, test score, baseline:  -0.03495333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.59187147134295
printing an ep nov before normalisation:  47.7559852037424
printing an ep nov before normalisation:  49.341171325033265
maxi score, test score, baseline:  -0.03495333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6266666666666669  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.03495333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.03495333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  52 total reward:  0.21999999999999942  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.187628114178736
maxi score, test score, baseline:  -0.03251333333333345 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.03251333333333345 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.279]
 [0.338]
 [0.329]
 [0.286]
 [0.283]
 [0.272]
 [0.282]] [[30.026]
 [39.939]
 [38.049]
 [33.412]
 [32.224]
 [30.164]
 [32.164]] [[0.674]
 [0.992]
 [0.933]
 [0.77 ]
 [0.735]
 [0.67 ]
 [0.733]]
maxi score, test score, baseline:  -0.03251333333333345 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.03251333333333345 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  50 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  67 total reward:  0.013333333333332642  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.03048666666666678 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.03048666666666678 0.6970000000000002 0.6970000000000002
maxi score, test score, baseline:  -0.03048666666666678 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.2355842590332
siam score:  -0.8331433
printing an ep nov before normalisation:  34.14923158679372
printing an ep nov before normalisation:  36.099958419799805
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.03048666666666678 0.6970000000000002 0.6970000000000002
maxi score, test score, baseline:  -0.03048666666666678 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  46.538712151772955
maxi score, test score, baseline:  -0.03048666666666678 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.082797050476074
actor:  1 policy actor:  1  step number:  49 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.03 ]
 [-0.009]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.03 ]
 [-0.009]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]
 [-0.03 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.03048666666666678 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.302]
 [0.28 ]
 [0.248]
 [0.248]
 [0.225]
 [0.17 ]
 [0.248]] [[44.29 ]
 [40.604]
 [50.94 ]
 [50.94 ]
 [56.245]
 [53.018]
 [50.94 ]] [[1.262]
 [1.066]
 [1.521]
 [1.521]
 [1.749]
 [1.542]
 [1.521]]
maxi score, test score, baseline:  -0.03048666666666678 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.35333333333333283  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8379945
printing an ep nov before normalisation:  46.580543518066406
printing an ep nov before normalisation:  59.97697054311987
Printing some Q and Qe and total Qs values:  [[-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]] [[45.147]
 [45.147]
 [45.147]
 [45.147]
 [45.147]
 [45.147]
 [45.147]] [[1.638]
 [1.638]
 [1.638]
 [1.638]
 [1.638]
 [1.638]
 [1.638]]
maxi score, test score, baseline:  -0.03048666666666678 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.03048666666666678 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.43516325494597
siam score:  -0.83538747
maxi score, test score, baseline:  -0.03048666666666678 0.6970000000000002 0.6970000000000002
maxi score, test score, baseline:  -0.03048666666666678 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.03048666666666678 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.524]
 [0.433]
 [0.483]
 [0.493]
 [0.485]
 [0.477]] [[12.668]
 [12.466]
 [18.184]
 [17.226]
 [12.557]
 [12.948]
 [12.712]] [[1.05 ]
 [1.086]
 [1.253]
 [1.259]
 [1.059]
 [1.069]
 [1.05 ]]
Printing some Q and Qe and total Qs values:  [[0.049]
 [0.098]
 [0.049]
 [0.049]
 [0.049]
 [0.049]
 [0.049]] [[37.507]
 [38.13 ]
 [37.507]
 [37.507]
 [37.507]
 [37.507]
 [37.507]] [[0.945]
 [1.022]
 [0.945]
 [0.945]
 [0.945]
 [0.945]
 [0.945]]
Printing some Q and Qe and total Qs values:  [[0.118]
 [0.067]
 [0.056]
 [0.06 ]
 [0.118]
 [0.071]
 [0.118]] [[36.342]
 [47.876]
 [43.409]
 [44.882]
 [36.342]
 [45.226]
 [36.342]] [[0.656]
 [1.072]
 [0.88 ]
 [0.944]
 [0.656]
 [0.969]
 [0.656]]
maxi score, test score, baseline:  -0.03048666666666678 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.03048666666666678 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  51.02796449206747
printing an ep nov before normalisation:  21.88968298599203
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]] [[12.346]
 [12.346]
 [12.346]
 [12.346]
 [12.346]
 [12.346]
 [12.346]] [[1.604]
 [1.604]
 [1.604]
 [1.604]
 [1.604]
 [1.604]
 [1.604]]
actor:  1 policy actor:  1  step number:  78 total reward:  0.24666666666666548  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  49.66774940490723
actions average: 
K:  0  action  0 :  tensor([0.5084, 0.0211, 0.0835, 0.0751, 0.1158, 0.1097, 0.0864],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0056, 0.9345, 0.0080, 0.0118, 0.0035, 0.0057, 0.0309],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1474, 0.0018, 0.2468, 0.1253, 0.1554, 0.1716, 0.1516],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1183, 0.0076, 0.1502, 0.2640, 0.1385, 0.1826, 0.1388],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1215, 0.0018, 0.1012, 0.0953, 0.4720, 0.1078, 0.1005],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1254, 0.0053, 0.1889, 0.1084, 0.1350, 0.3210, 0.1161],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0554, 0.3483, 0.0868, 0.1183, 0.0546, 0.0696, 0.2670],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  48.09971800189463
maxi score, test score, baseline:  -0.03048666666666678 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.12042498754672
maxi score, test score, baseline:  -0.03048666666666678 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.128590382721235
actor:  1 policy actor:  1  step number:  49 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.03048666666666678 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  45 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.02763333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.455410684939444
maxi score, test score, baseline:  -0.02763333333333344 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.02763333333333344 0.6970000000000002 0.6970000000000002
printing an ep nov before normalisation:  34.48120502609424
Printing some Q and Qe and total Qs values:  [[0.673]
 [0.673]
 [0.673]
 [0.683]
 [0.673]
 [0.673]
 [0.673]] [[33.999]
 [33.999]
 [33.999]
 [40.042]
 [33.999]
 [33.999]
 [33.999]] [[1.894]
 [1.894]
 [1.894]
 [2.295]
 [1.894]
 [1.894]
 [1.894]]
printing an ep nov before normalisation:  50.20765842425732
actor:  1 policy actor:  1  step number:  49 total reward:  0.5200000000000001  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  61 total reward:  0.05333333333333257  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  16.207332611083984
actor:  1 policy actor:  1  step number:  45 total reward:  0.3866666666666666  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  42.954079716258505
printing an ep nov before normalisation:  32.40480310093581
actor:  0 policy actor:  0  step number:  39 total reward:  0.48  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.02467333333333346 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.518]
 [0.637]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]] [[36.593]
 [40.654]
 [36.593]
 [36.593]
 [36.593]
 [36.593]
 [36.593]] [[0.925]
 [1.121]
 [0.925]
 [0.925]
 [0.925]
 [0.925]
 [0.925]]
maxi score, test score, baseline:  -0.02467333333333346 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.24603435951911
maxi score, test score, baseline:  -0.02467333333333346 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.398]
 [0.524]
 [0.398]
 [0.405]
 [0.398]
 [0.398]
 [0.415]] [[42.944]
 [43.279]
 [42.944]
 [48.627]
 [42.944]
 [42.944]
 [46.649]] [[0.914]
 [1.048]
 [0.914]
 [1.053]
 [0.914]
 [0.914]
 [1.017]]
maxi score, test score, baseline:  -0.02467333333333346 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 31.56838846206665
printing an ep nov before normalisation:  43.93268759294009
actor:  1 policy actor:  1  step number:  52 total reward:  0.3133333333333328  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  63 total reward:  0.22666666666666602  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.02467333333333346 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.026]
 [-0.012]
 [-0.03 ]
 [-0.018]
 [-0.02 ]
 [-0.024]
 [-0.028]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.026]
 [-0.012]
 [-0.03 ]
 [-0.018]
 [-0.02 ]
 [-0.024]
 [-0.028]]
printing an ep nov before normalisation:  14.207353591918945
printing an ep nov before normalisation:  42.98047533151407
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.56 ]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]] [[52.072]
 [45.739]
 [52.072]
 [52.072]
 [52.072]
 [52.072]
 [52.072]] [[2.295]
 [1.989]
 [2.295]
 [2.295]
 [2.295]
 [2.295]
 [2.295]]
UNIT TEST: sample policy line 217 mcts : [0.143 0.367 0.184 0.041 0.122 0.102 0.041]
printing an ep nov before normalisation:  31.56283554161435
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  40.48655750484741
actions average: 
K:  1  action  0 :  tensor([0.4631, 0.0030, 0.0798, 0.0881, 0.1962, 0.0777, 0.0921],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0071, 0.9576, 0.0065, 0.0062, 0.0040, 0.0047, 0.0139],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0892, 0.0108, 0.4270, 0.0962, 0.1083, 0.1297, 0.1387],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0553, 0.1707, 0.0781, 0.4118, 0.0950, 0.0775, 0.1116],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1514, 0.0026, 0.0816, 0.1112, 0.4730, 0.0756, 0.1045],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1290, 0.0297, 0.1561, 0.1508, 0.1264, 0.2889, 0.1192],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1007, 0.1264, 0.1217, 0.1384, 0.1059, 0.0961, 0.3107],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.775]
 [0.851]
 [0.775]
 [0.744]
 [0.775]
 [0.775]
 [0.797]] [[37.088]
 [39.168]
 [37.088]
 [34.487]
 [37.088]
 [37.088]
 [36.982]] [[0.775]
 [0.851]
 [0.775]
 [0.744]
 [0.775]
 [0.775]
 [0.797]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  41.63107940745668
actor:  0 policy actor:  0  step number:  50 total reward:  0.27333333333333265  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  44 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.019313333333333443 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.02993909793406
maxi score, test score, baseline:  -0.019313333333333443 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.75611788379818
maxi score, test score, baseline:  -0.019313333333333443 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.060029064784295
printing an ep nov before normalisation:  40.57388539073969
printing an ep nov before normalisation:  31.783569488566435
maxi score, test score, baseline:  -0.019313333333333443 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  0.5666666666666668  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  53 total reward:  0.13333333333333286  reward:  1.0 rdn_beta:  1.0
siam score:  -0.845187
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.017046666666666783 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]] [[33.867]
 [33.867]
 [33.867]
 [33.867]
 [33.867]
 [33.867]
 [33.867]] [[1.879]
 [1.879]
 [1.879]
 [1.879]
 [1.879]
 [1.879]
 [1.879]]
printing an ep nov before normalisation:  45.61977160314302
actions average: 
K:  4  action  0 :  tensor([0.3162, 0.0315, 0.0928, 0.0954, 0.2328, 0.1267, 0.1047],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0192, 0.8833, 0.0126, 0.0320, 0.0060, 0.0081, 0.0389],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1307, 0.1476, 0.2289, 0.0903, 0.0831, 0.1740, 0.1455],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1580, 0.1010, 0.1375, 0.1532, 0.1285, 0.1566, 0.1652],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1559, 0.0427, 0.1347, 0.1700, 0.2089, 0.1620, 0.1258],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0391, 0.0123, 0.0955, 0.2379, 0.0897, 0.4233, 0.1022],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1910, 0.2599, 0.0743, 0.1111, 0.1013, 0.0658, 0.1966],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.017046666666666783 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.017046666666666783 0.6970000000000002 0.6970000000000002
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
maxi score, test score, baseline:  -0.017046666666666783 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.017046666666666783 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.23333333333333273  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  53 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.439074518425016
actor:  1 policy actor:  1  step number:  61 total reward:  0.2133333333333327  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  34.18062558043082
printing an ep nov before normalisation:  26.42961025238037
maxi score, test score, baseline:  -0.017046666666666783 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]] [[34.568]
 [34.568]
 [34.568]
 [34.568]
 [34.568]
 [34.568]
 [34.568]] [[0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]]
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.689]
 [0.597]
 [0.64 ]
 [0.582]
 [0.579]
 [0.518]] [[45.812]
 [44.694]
 [45.832]
 [44.124]
 [46.103]
 [46.386]
 [46.249]] [[0.562]
 [0.689]
 [0.597]
 [0.64 ]
 [0.582]
 [0.579]
 [0.518]]
maxi score, test score, baseline:  -0.017046666666666783 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 36.06415225561704
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.17155233099021
maxi score, test score, baseline:  -0.019473333333333454 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  56 total reward:  0.2599999999999991  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.006]
 [ 0.055]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.22 ]
 [-0.006]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.006]
 [ 0.055]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.22 ]
 [-0.006]]
maxi score, test score, baseline:  -0.01948666666666678 0.6970000000000002 0.6970000000000002
maxi score, test score, baseline:  -0.01948666666666678 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.542704662775435
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 39.988221450273706
printing an ep nov before normalisation:  29.11924306991131
maxi score, test score, baseline:  -0.01948666666666678 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.741]
 [0.659]
 [0.69 ]
 [0.623]
 [0.596]
 [0.694]] [[27.205]
 [24.644]
 [26.105]
 [25.895]
 [30.772]
 [25.851]
 [29.514]] [[1.491]
 [1.564]
 [1.53 ]
 [1.554]
 [1.65 ]
 [1.459]
 [1.679]]
actor:  1 policy actor:  1  step number:  36 total reward:  0.5933333333333334  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.01948666666666678 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.01948666666666678 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  10.381810737935666
actor:  1 policy actor:  1  step number:  56 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.01948666666666678 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8394253
maxi score, test score, baseline:  -0.01948666666666678 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.01948666666666678 0.6970000000000002 0.6970000000000002
maxi score, test score, baseline:  -0.019486666666666777 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.019486666666666777 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.019486666666666777 0.6970000000000002 0.6970000000000002
actor:  0 policy actor:  0  step number:  36 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.016513333333333446 0.6970000000000002 0.6970000000000002
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.016513333333333446 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.11374530949071
maxi score, test score, baseline:  -0.016513333333333446 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.16235618884969
maxi score, test score, baseline:  -0.016513333333333446 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.3133333333333327  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  3  action  0 :  tensor([0.5538, 0.0170, 0.0865, 0.0699, 0.1064, 0.0629, 0.1036],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0056, 0.9510, 0.0060, 0.0130, 0.0019, 0.0020, 0.0206],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0758, 0.0509, 0.4617, 0.0829, 0.0865, 0.1670, 0.0752],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1616, 0.0317, 0.1462, 0.2565, 0.1472, 0.1273, 0.1294],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0800, 0.0038, 0.1093, 0.1726, 0.4838, 0.0890, 0.0614],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1010, 0.0166, 0.1446, 0.0952, 0.1042, 0.4142, 0.1243],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0967, 0.1671, 0.1138, 0.1230, 0.1133, 0.0927, 0.2934],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.016513333333333446 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.516568660736084
maxi score, test score, baseline:  -0.016513333333333446 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.83814566732027
siam score:  -0.8457413
maxi score, test score, baseline:  -0.016513333333333446 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.26478491451145
printing an ep nov before normalisation:  35.08472822095167
actor:  1 policy actor:  1  step number:  64 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  70.99843908228873
maxi score, test score, baseline:  -0.016513333333333446 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.581441438549064
printing an ep nov before normalisation:  37.49367140298968
maxi score, test score, baseline:  -0.016513333333333446 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.2533333333333331  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.624]
 [0.624]
 [0.607]
 [0.599]
 [0.624]
 [0.624]
 [0.624]] [[23.057]
 [23.057]
 [28.842]
 [30.241]
 [23.057]
 [23.057]
 [23.057]] [[1.603]
 [1.603]
 [2.037]
 [2.138]
 [1.603]
 [1.603]
 [1.603]]
printing an ep nov before normalisation:  21.95804718411508
actor:  1 policy actor:  1  step number:  53 total reward:  0.4666666666666668  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.016513333333333446 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.016513333333333446 0.6970000000000002 0.6970000000000002
printing an ep nov before normalisation:  46.16360329734417
printing an ep nov before normalisation:  36.86880942126959
actor:  1 policy actor:  1  step number:  55 total reward:  0.3599999999999991  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  52.49502002465595
printing an ep nov before normalisation:  37.39205360412598
Printing some Q and Qe and total Qs values:  [[ 0.082]
 [ 0.38 ]
 [ 0.215]
 [ 0.268]
 [-0.005]
 [ 0.217]
 [ 0.385]] [[42.354]
 [41.655]
 [39.43 ]
 [42.892]
 [37.6  ]
 [36.544]
 [43.893]] [[0.538]
 [0.818]
 [0.601]
 [0.736]
 [0.337]
 [0.534]
 [0.877]]
printing an ep nov before normalisation:  33.26406393732688
maxi score, test score, baseline:  -0.019060000000000115 0.6970000000000002 0.6970000000000002
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.019060000000000115 0.6970000000000002 0.6970000000000002
printing an ep nov before normalisation:  20.308244228363037
actor:  1 policy actor:  1  step number:  47 total reward:  0.4266666666666662  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.019060000000000115 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.019060000000000115 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.3516, 0.0122, 0.1191, 0.1240, 0.1303, 0.1432, 0.1195],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0038, 0.9527, 0.0030, 0.0031, 0.0014, 0.0019, 0.0341],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1124, 0.0155, 0.3677, 0.1170, 0.1130, 0.1721, 0.1024],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0821, 0.0661, 0.0765, 0.4111, 0.1174, 0.1292, 0.1177],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1563, 0.0129, 0.0891, 0.1468, 0.3711, 0.1266, 0.0973],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1424, 0.0075, 0.2356, 0.1576, 0.0883, 0.2594, 0.1091],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1105, 0.1014, 0.0966, 0.1521, 0.0762, 0.0998, 0.3633],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  42.53682312770918
printing an ep nov before normalisation:  27.76996309206849
maxi score, test score, baseline:  -0.019060000000000115 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  39 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.002696090778897542
Printing some Q and Qe and total Qs values:  [[-0.001]
 [ 0.142]
 [-0.001]
 [ 0.131]
 [-0.001]
 [-0.001]
 [-0.001]] [[31.773]
 [33.047]
 [30.581]
 [31.362]
 [31.174]
 [31.222]
 [30.457]] [[1.38 ]
 [1.626]
 [1.283]
 [1.479]
 [1.331]
 [1.335]
 [1.273]]
actor:  1 policy actor:  1  step number:  68 total reward:  0.3533333333333325  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.01851333333333345 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.01851333333333345 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.4733333333333334  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  49.837353490956886
printing an ep nov before normalisation:  54.8736535121768
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.29287242889404
actor:  1 policy actor:  1  step number:  54 total reward:  0.39333333333333287  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  43.83736610412598
maxi score, test score, baseline:  -0.01851333333333345 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 52.44713944054871
maxi score, test score, baseline:  -0.01851333333333345 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.01851333333333345 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  46.00487245792181
maxi score, test score, baseline:  -0.01851333333333345 0.6970000000000002 0.6970000000000002
printing an ep nov before normalisation:  40.79855971039117
Printing some Q and Qe and total Qs values:  [[-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]] [[42.189]
 [42.189]
 [42.189]
 [42.189]
 [42.189]
 [42.189]
 [42.189]] [[1.355]
 [1.355]
 [1.355]
 [1.355]
 [1.355]
 [1.355]
 [1.355]]
printing an ep nov before normalisation:  41.921608437172274
printing an ep nov before normalisation:  45.16186618484623
maxi score, test score, baseline:  -0.01851333333333345 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8391039
actor:  1 policy actor:  1  step number:  50 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  50 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 38.37587337039504
printing an ep nov before normalisation:  14.170129052181961
actor:  1 policy actor:  1  step number:  81 total reward:  0.23999999999999877  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.01851333333333345 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.1859797130229
maxi score, test score, baseline:  -0.01851333333333345 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.04591601359519
Printing some Q and Qe and total Qs values:  [[0.722]
 [0.887]
 [0.698]
 [0.778]
 [0.738]
 [0.81 ]
 [0.839]] [[37.335]
 [34.378]
 [39.68 ]
 [43.646]
 [44.572]
 [37.548]
 [39.869]] [[0.722]
 [0.887]
 [0.698]
 [0.778]
 [0.738]
 [0.81 ]
 [0.839]]
printing an ep nov before normalisation:  35.27375118835672
printing an ep nov before normalisation:  39.342003304840226
maxi score, test score, baseline:  -0.01851333333333345 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  41 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.015606666666666779 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  47.52234123405491
maxi score, test score, baseline:  -0.015606666666666779 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.015606666666666779 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.14641761779785
printing an ep nov before normalisation:  55.9047103138818
printing an ep nov before normalisation:  34.916062355041504
printing an ep nov before normalisation:  35.92274907888909
maxi score, test score, baseline:  -0.015606666666666779 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.545]
 [0.6  ]
 [0.572]
 [0.575]
 [0.591]
 [0.558]
 [0.546]] [[30.3  ]
 [31.8  ]
 [31.096]
 [31.72 ]
 [32.37 ]
 [31.905]
 [31.387]] [[0.967]
 [1.067]
 [1.017]
 [1.04 ]
 [1.075]
 [1.028]
 [1.001]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.19999999999999962  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  54 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.015606666666666779 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  49 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.012806666666666773 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.80284377072602
printing an ep nov before normalisation:  53.56859487355236
maxi score, test score, baseline:  -0.012806666666666773 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.012806666666666773 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.610188161517634
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.642]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]] [[35.322]
 [36.826]
 [35.322]
 [35.322]
 [35.322]
 [35.322]
 [35.322]] [[1.395]
 [1.582]
 [1.395]
 [1.395]
 [1.395]
 [1.395]
 [1.395]]
printing an ep nov before normalisation:  52.98189491506684
actor:  0 policy actor:  0  step number:  36 total reward:  0.5  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  49 total reward:  0.52  reward:  1.0 rdn_beta:  1.667
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  57 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  43.32104741644382
Printing some Q and Qe and total Qs values:  [[0.733]
 [0.841]
 [0.733]
 [0.733]
 [0.733]
 [0.733]
 [0.733]] [[51.003]
 [42.219]
 [51.003]
 [51.003]
 [51.003]
 [51.003]
 [51.003]] [[0.733]
 [0.841]
 [0.733]
 [0.733]
 [0.733]
 [0.733]
 [0.733]]
printing an ep nov before normalisation:  48.09450289124307
printing an ep nov before normalisation:  38.8375539534461
printing an ep nov before normalisation:  53.59058856964111
maxi score, test score, baseline:  -0.009806666666666788 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.013]
 [ 0.026]
 [-0.001]
 [ 0.013]
 [ 0.007]
 [-0.002]
 [ 0.014]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.013]
 [ 0.026]
 [-0.001]
 [ 0.013]
 [ 0.007]
 [-0.002]
 [ 0.014]]
printing an ep nov before normalisation:  44.25552425347294
Printing some Q and Qe and total Qs values:  [[0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.822]] [[37.464]
 [37.464]
 [37.464]
 [37.464]
 [37.464]
 [37.464]
 [37.464]] [[0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.822]]
maxi score, test score, baseline:  -0.009806666666666781 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.585601442170145
maxi score, test score, baseline:  -0.009806666666666781 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8390713
printing an ep nov before normalisation:  48.50680826282225
printing an ep nov before normalisation:  49.851385339408644
maxi score, test score, baseline:  -0.009806666666666781 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  56 total reward:  0.08666666666666611  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  10.73310996855753
actor:  1 policy actor:  1  step number:  67 total reward:  0.27999999999999936  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  58 total reward:  0.0733333333333328  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.007633333333333458 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.57444278418481
printing an ep nov before normalisation:  48.91793714804434
printing an ep nov before normalisation:  39.01075581454359
printing an ep nov before normalisation:  43.39960397598633
maxi score, test score, baseline:  -0.007633333333333458 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.007633333333333458 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.39 ]
 [0.363]
 [0.363]
 [0.364]
 [0.366]
 [0.364]] [[50.377]
 [46.677]
 [51.621]
 [51.498]
 [51.547]
 [52.27 ]
 [51.127]] [[1.382]
 [1.279]
 [1.432]
 [1.428]
 [1.431]
 [1.459]
 [1.415]]
maxi score, test score, baseline:  -0.007633333333333458 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.66424552637546
maxi score, test score, baseline:  -0.007633333333333458 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.379]
 [0.391]
 [0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]] [[35.756]
 [36.679]
 [35.756]
 [35.756]
 [35.756]
 [35.756]
 [35.756]] [[1.584]
 [1.653]
 [1.584]
 [1.584]
 [1.584]
 [1.584]
 [1.584]]
line 256 mcts: sample exp_bonus 41.99945000606782
maxi score, test score, baseline:  -0.007633333333333458 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.762475235387825
actor:  1 policy actor:  1  step number:  68 total reward:  0.21999999999999886  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.007633333333333458 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.69721648127222
actor:  1 policy actor:  1  step number:  70 total reward:  0.09999999999999976  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  54.0823652776259
actor:  1 policy actor:  1  step number:  62 total reward:  0.16666666666666596  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.007633333333333458 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.07629357239921
printing an ep nov before normalisation:  47.60559655032624
maxi score, test score, baseline:  -0.007633333333333458 0.6970000000000002 0.6970000000000002
maxi score, test score, baseline:  -0.007633333333333458 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.279]
 [0.239]
 [0.239]
 [0.16 ]
 [0.239]
 [0.239]] [[45.406]
 [40.005]
 [42.504]
 [42.504]
 [40.11 ]
 [42.504]
 [42.504]] [[1.61 ]
 [1.232]
 [1.317]
 [1.317]
 [1.118]
 [1.317]
 [1.317]]
printing an ep nov before normalisation:  36.65671785050565
maxi score, test score, baseline:  -0.007633333333333458 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.93251090040513
printing an ep nov before normalisation:  46.735069837502984
maxi score, test score, baseline:  -0.007633333333333458 0.6970000000000002 0.6970000000000002
printing an ep nov before normalisation:  51.201697019141626
actor:  1 policy actor:  1  step number:  61 total reward:  0.2266666666666659  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.007633333333333451 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8340335
printing an ep nov before normalisation:  40.096169091031086
maxi score, test score, baseline:  -0.007633333333333451 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.54753757272381
printing an ep nov before normalisation:  33.45961936459153
printing an ep nov before normalisation:  34.9488859533289
maxi score, test score, baseline:  -0.010553333333333451 0.6970000000000002 0.6970000000000002
printing an ep nov before normalisation:  0.029774771430766123
siam score:  -0.8348456
printing an ep nov before normalisation:  21.473244258335658
printing an ep nov before normalisation:  46.279519126314206
actor:  0 policy actor:  0  step number:  47 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.010806666666666787 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.339]
 [0.482]
 [0.339]
 [0.35 ]
 [0.339]
 [0.339]
 [0.362]] [[47.775]
 [42.849]
 [47.775]
 [45.716]
 [47.775]
 [47.775]
 [49.18 ]] [[1.146]
 [1.126]
 [1.146]
 [1.088]
 [1.146]
 [1.146]
 [1.215]]
maxi score, test score, baseline:  -0.010806666666666787 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.010806666666666787 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.58 ]
 [0.47 ]
 [0.436]
 [0.439]
 [0.528]
 [0.504]] [[47.981]
 [44.302]
 [49.541]
 [51.511]
 [51.129]
 [51.505]
 [50.182]] [[1.305]
 [1.311]
 [1.358]
 [1.382]
 [1.373]
 [1.474]
 [1.41 ]]
printing an ep nov before normalisation:  43.17312717437744
Printing some Q and Qe and total Qs values:  [[0.886]
 [0.761]
 [0.716]
 [0.792]
 [0.801]
 [0.792]
 [0.761]] [[35.429]
 [34.46 ]
 [35.099]
 [30.532]
 [37.394]
 [30.532]
 [36.867]] [[0.886]
 [0.761]
 [0.716]
 [0.792]
 [0.801]
 [0.792]
 [0.761]]
actions average: 
K:  4  action  0 :  tensor([0.4949, 0.0058, 0.1160, 0.0832, 0.0982, 0.1268, 0.0750],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0119, 0.9297, 0.0093, 0.0105, 0.0068, 0.0064, 0.0253],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1066, 0.0423, 0.3055, 0.1946, 0.1133, 0.1247, 0.1130],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1241, 0.1216, 0.1040, 0.2883, 0.1177, 0.1121, 0.1322],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1253, 0.0101, 0.0954, 0.1334, 0.4163, 0.1051, 0.1144],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1679, 0.0456, 0.1747, 0.1334, 0.1405, 0.1953, 0.1426],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1735, 0.0166, 0.1301, 0.1173, 0.1679, 0.1495, 0.2451],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.010806666666666787 0.6970000000000002 0.6970000000000002
maxi score, test score, baseline:  -0.010806666666666787 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
printing an ep nov before normalisation:  32.83247278253935
maxi score, test score, baseline:  -0.010806666666666787 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.25278240549732
actor:  0 policy actor:  0  step number:  47 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8318764
maxi score, test score, baseline:  -0.008033333333333455 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.008033333333333455 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  1.1502449665357517
maxi score, test score, baseline:  -0.008033333333333455 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.008033333333333455 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  29.10656803029886
printing an ep nov before normalisation:  41.68323040008545
maxi score, test score, baseline:  -0.008033333333333455 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.750397507216704
maxi score, test score, baseline:  -0.008033333333333455 0.6970000000000002 0.6970000000000002
printing an ep nov before normalisation:  48.69064165803968
printing an ep nov before normalisation:  46.034927018595795
actor:  0 policy actor:  0  step number:  47 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  45.53180952971404
maxi score, test score, baseline:  -0.005233333333333456 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.29041942409039
maxi score, test score, baseline:  -0.005233333333333456 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.24021899259572
printing an ep nov before normalisation:  36.130388386212076
maxi score, test score, baseline:  -0.005233333333333456 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  51 total reward:  0.2799999999999997  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  52 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  1.667
siam score:  -0.83873236
maxi score, test score, baseline:  -0.009473333333333452 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.097]
 [ 0.303]
 [ 0.238]
 [ 0.21 ]
 [-0.019]
 [ 0.21 ]
 [ 0.263]] [[38.644]
 [50.099]
 [47.741]
 [46.221]
 [42.57 ]
 [46.221]
 [49.32 ]] [[0.037]
 [0.539]
 [0.453]
 [0.411]
 [0.15 ]
 [0.411]
 [0.493]]
printing an ep nov before normalisation:  47.11160080696789
maxi score, test score, baseline:  -0.009473333333333452 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.42414192013562
printing an ep nov before normalisation:  44.121203733714616
actor:  1 policy actor:  1  step number:  53 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.009473333333333452 0.6970000000000002 0.6970000000000002
printing an ep nov before normalisation:  41.39425821448019
printing an ep nov before normalisation:  39.04098033905029
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.645]
 [0.497]
 [0.494]
 [0.565]
 [0.506]
 [0.565]] [[41.548]
 [43.279]
 [38.315]
 [39.041]
 [43.088]
 [39.419]
 [43.088]] [[1.447]
 [1.668]
 [1.291]
 [1.322]
 [1.58 ]
 [1.351]
 [1.58 ]]
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]] [[44.305]
 [44.305]
 [44.305]
 [44.305]
 [44.305]
 [44.305]
 [44.305]] [[1.802]
 [1.802]
 [1.802]
 [1.802]
 [1.802]
 [1.802]
 [1.802]]
maxi score, test score, baseline:  -0.009473333333333452 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.871]
 [0.912]
 [0.871]
 [0.87 ]
 [0.871]
 [0.871]
 [0.871]] [[38.764]
 [35.155]
 [38.764]
 [39.315]
 [38.764]
 [38.764]
 [38.764]] [[0.871]
 [0.912]
 [0.871]
 [0.87 ]
 [0.871]
 [0.871]
 [0.871]]
maxi score, test score, baseline:  -0.009473333333333452 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  0.0009137188277463792
actor:  1 policy actor:  1  step number:  46 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  41 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  69 total reward:  0.14666666666666572  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  45.57610318153649
maxi score, test score, baseline:  -0.010086666666666792 0.6970000000000002 0.6970000000000002
maxi score, test score, baseline:  -0.010086666666666792 0.6970000000000002 0.6970000000000002
maxi score, test score, baseline:  -0.010086666666666792 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  54.68981932959535
printing an ep nov before normalisation:  60.56703930539915
printing an ep nov before normalisation:  37.70467413492887
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.525]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]] [[44.791]
 [46.387]
 [44.791]
 [44.791]
 [44.791]
 [44.791]
 [44.791]] [[1.348]
 [1.525]
 [1.348]
 [1.348]
 [1.348]
 [1.348]
 [1.348]]
maxi score, test score, baseline:  -0.010086666666666792 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  66 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.010086666666666792 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.010086666666666792 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  0.5466666666666667  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [-0.004]
 [-0.017]
 [-0.015]
 [-0.015]
 [-0.008]
 [-0.014]] [[38.827]
 [42.087]
 [38.276]
 [38.256]
 [38.664]
 [37.954]
 [37.777]] [[0.223]
 [0.265]
 [0.209]
 [0.211]
 [0.216]
 [0.215]
 [0.207]]
maxi score, test score, baseline:  -0.010086666666666792 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.010086666666666792 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.010086666666666792 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.11 ]
 [0.207]
 [0.076]
 [0.082]
 [0.113]
 [0.081]
 [0.081]] [[39.433]
 [42.292]
 [32.644]
 [32.569]
 [38.954]
 [32.55 ]
 [32.727]] [[0.625]
 [0.797]
 [0.415]
 [0.419]
 [0.616]
 [0.418]
 [0.423]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.010086666666666792 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.93514442443848
maxi score, test score, baseline:  -0.010086666666666792 0.6970000000000002 0.6970000000000002
maxi score, test score, baseline:  -0.010086666666666792 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.98572793820682
printing an ep nov before normalisation:  38.39141368865967
siam score:  -0.82579225
actor:  1 policy actor:  1  step number:  51 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.010086666666666792 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.010086666666666792 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.4266666666666663  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  47 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  38.20503683769502
printing an ep nov before normalisation:  45.76796095532177
printing an ep nov before normalisation:  43.25966401007027
printing an ep nov before normalisation:  40.57283634866519
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.552]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]] [[35.106]
 [37.49 ]
 [35.106]
 [35.106]
 [35.106]
 [35.106]
 [35.106]] [[1.343]
 [1.509]
 [1.343]
 [1.343]
 [1.343]
 [1.343]
 [1.343]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  29.48362339037923
printing an ep nov before normalisation:  27.643008892199735
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.010086666666666792 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.010086666666666792 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.2112248760841
Printing some Q and Qe and total Qs values:  [[1.013]
 [1.003]
 [1.013]
 [1.013]
 [1.013]
 [1.013]
 [1.013]] [[42.447]
 [34.76 ]
 [42.447]
 [42.447]
 [42.447]
 [42.447]
 [42.447]] [[1.013]
 [1.003]
 [1.013]
 [1.013]
 [1.013]
 [1.013]
 [1.013]]
printing an ep nov before normalisation:  55.24140640540871
maxi score, test score, baseline:  -0.010086666666666792 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.818]
 [0.781]
 [0.76 ]
 [0.657]
 [0.791]
 [0.306]
 [0.364]] [[43.822]
 [37.637]
 [40.216]
 [38.426]
 [48.356]
 [57.127]
 [40.276]] [[0.818]
 [0.781]
 [0.76 ]
 [0.657]
 [0.791]
 [0.306]
 [0.364]]
printing an ep nov before normalisation:  49.89799068129163
Printing some Q and Qe and total Qs values:  [[0.773]
 [0.808]
 [0.773]
 [0.773]
 [0.773]
 [0.773]
 [0.773]] [[40.709]
 [38.706]
 [40.709]
 [40.709]
 [40.709]
 [40.709]
 [40.709]] [[0.773]
 [0.808]
 [0.773]
 [0.773]
 [0.773]
 [0.773]
 [0.773]]
maxi score, test score, baseline:  -0.010086666666666792 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.1432524794072
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
Printing some Q and Qe and total Qs values:  [[0.795]
 [0.894]
 [0.795]
 [0.805]
 [0.795]
 [0.795]
 [0.802]] [[46.819]
 [38.677]
 [46.819]
 [42.413]
 [46.819]
 [46.819]
 [41.913]] [[0.795]
 [0.894]
 [0.795]
 [0.805]
 [0.795]
 [0.795]
 [0.802]]
Printing some Q and Qe and total Qs values:  [[0.807]
 [0.906]
 [0.807]
 [0.81 ]
 [0.805]
 [0.809]
 [0.86 ]] [[41.93 ]
 [41.832]
 [40.96 ]
 [41.615]
 [41.902]
 [41.191]
 [37.938]] [[0.807]
 [0.906]
 [0.807]
 [0.81 ]
 [0.805]
 [0.809]
 [0.86 ]]
Printing some Q and Qe and total Qs values:  [[0.814]
 [0.865]
 [0.814]
 [0.814]
 [0.814]
 [0.814]
 [0.814]] [[50.609]
 [40.203]
 [50.609]
 [50.609]
 [50.609]
 [50.609]
 [50.609]] [[0.814]
 [0.865]
 [0.814]
 [0.814]
 [0.814]
 [0.814]
 [0.814]]
maxi score, test score, baseline:  -0.010086666666666792 0.6970000000000002 0.6970000000000002
printing an ep nov before normalisation:  35.65990209579468
printing an ep nov before normalisation:  35.54495257513483
printing an ep nov before normalisation:  41.29710532902286
maxi score, test score, baseline:  -0.010086666666666792 0.6970000000000002 0.6970000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  2.2738881721056714
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.00328666666666677 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  -0.00328666666666677 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.955890742872835
maxi score, test score, baseline:  -0.00328666666666677 0.6986666666666668 0.6986666666666668
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.00328666666666677 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  71 total reward:  0.03999999999999948  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  32.215981068549894
printing an ep nov before normalisation:  37.19012260437012
actor:  1 policy actor:  1  step number:  57 total reward:  0.10666666666666613  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  33.35305188711857
maxi score, test score, baseline:  -0.00328666666666677 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  -0.00328666666666677 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  41 total reward:  0.5466666666666666  reward:  1.0 rdn_beta:  0.667
siam score:  -0.82614595
printing an ep nov before normalisation:  42.111268043518066
maxi score, test score, baseline:  -0.00328666666666677 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.00328666666666677 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.00328666666666677 0.6986666666666668 0.6986666666666668
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.00328666666666677 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.477]
 [0.427]
 [0.428]
 [0.419]
 [0.42 ]
 [0.41 ]] [[29.053]
 [30.766]
 [29.799]
 [29.887]
 [31.002]
 [30.97 ]
 [28.274]] [[0.661]
 [0.738]
 [0.673]
 [0.675]
 [0.684]
 [0.685]
 [0.632]]
printing an ep nov before normalisation:  43.289117580002156
actor:  1 policy actor:  1  step number:  43 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  51 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.00328666666666677 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.00328666666666677 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.3333333333333328  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.18 ]
 [0.246]
 [0.17 ]
 [0.163]
 [0.168]
 [0.221]
 [0.182]] [[43.843]
 [43.798]
 [43.612]
 [43.526]
 [43.879]
 [43.538]
 [42.796]] [[1.47 ]
 [1.533]
 [1.449]
 [1.438]
 [1.459]
 [1.497]
 [1.425]]
printing an ep nov before normalisation:  44.9696853073761
printing an ep nov before normalisation:  49.20485628304135
maxi score, test score, baseline:  -0.00328666666666677 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  50.66159024567235
maxi score, test score, baseline:  -0.00328666666666677 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.66487026492975
maxi score, test score, baseline:  -0.00328666666666677 0.6986666666666668 0.6986666666666668
actor:  1 policy actor:  1  step number:  41 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  54 total reward:  0.31333333333333313  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.667165636219686
maxi score, test score, baseline:  -0.00328666666666677 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.5133333333333336  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.00328666666666677 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  38.504958152770996
printing an ep nov before normalisation:  48.866656083512666
printing an ep nov before normalisation:  51.51780448945508
printing an ep nov before normalisation:  32.496492862701416
actor:  1 policy actor:  1  step number:  41 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  45.685951088650995
printing an ep nov before normalisation:  49.22713779251034
printing an ep nov before normalisation:  38.86570125886519
maxi score, test score, baseline:  -0.00328666666666677 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.00328666666666677 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  65 total reward:  0.21333333333333315  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  36.10104541855552
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.538]
 [0.584]
 [0.49 ]
 [0.449]
 [0.474]
 [0.569]] [[40.163]
 [38.615]
 [39.289]
 [36.293]
 [36.142]
 [35.55 ]
 [41.014]] [[1.759]
 [1.66 ]
 [1.747]
 [1.474]
 [1.424]
 [1.413]
 [1.834]]
printing an ep nov before normalisation:  46.14114766281878
actor:  0 policy actor:  0  step number:  43 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  41.78555487896282
maxi score, test score, baseline:  -0.0026600000000001046 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.07372479857019
maxi score, test score, baseline:  -0.0026600000000001046 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  -0.0026600000000001046 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.625431060791016
printing an ep nov before normalisation:  11.436011587259658
actor:  1 policy actor:  1  step number:  58 total reward:  0.19333333333333302  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  1  action  0 :  tensor([0.4839, 0.0408, 0.0839, 0.0741, 0.1445, 0.0946, 0.0782],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0011,     0.9644,     0.0011,     0.0087,     0.0004,     0.0006,
            0.0236], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1256, 0.0281, 0.3049, 0.1433, 0.1147, 0.1577, 0.1257],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1119, 0.0623, 0.1348, 0.3280, 0.1038, 0.1347, 0.1245],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1173, 0.0246, 0.0995, 0.1423, 0.3535, 0.1268, 0.1360],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1210, 0.0092, 0.1639, 0.1163, 0.1099, 0.3629, 0.1167],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1083, 0.0252, 0.1052, 0.1210, 0.0826, 0.1041, 0.4536],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  80 total reward:  0.15333333333333143  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.0026600000000001046 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.353386137220596
siam score:  -0.8357413
printing an ep nov before normalisation:  36.29617190078782
actor:  0 policy actor:  0  step number:  47 total reward:  0.25333333333333286  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.002740000000000103 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  -0.002740000000000103 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  -0.002740000000000103 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.002740000000000103 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  38 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  43.365615024005045
maxi score, test score, baseline:  0.00012666666666657136 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  0.00012666666666657136 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.00012666666666657136 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  13.12814474105835
printing an ep nov before normalisation:  43.90385685958438
actor:  1 policy actor:  1  step number:  58 total reward:  0.23333333333333306  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.00012666666666657136 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.93728994093657
actor:  1 policy actor:  1  step number:  47 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.00012666666666657136 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.00012666666666657136 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.00012666666666657136 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.67521826273011
maxi score, test score, baseline:  0.00012666666666657136 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.51825342417304
siam score:  -0.8330095
maxi score, test score, baseline:  0.00012666666666657136 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.00012666666666657136 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.83227586746216
maxi score, test score, baseline:  0.00012666666666657136 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  67 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]] [[35.672]
 [35.672]
 [35.672]
 [35.672]
 [35.672]
 [35.672]
 [35.672]] [[0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]]
Printing some Q and Qe and total Qs values:  [[0.066]
 [0.414]
 [0.21 ]
 [0.241]
 [0.188]
 [0.216]
 [0.301]] [[36.515]
 [40.518]
 [38.211]
 [36.627]
 [37.234]
 [38.669]
 [40.668]] [[1.143]
 [1.718]
 [1.383]
 [1.325]
 [1.306]
 [1.415]
 [1.613]]
maxi score, test score, baseline:  0.00012666666666657136 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.80028485715245
maxi score, test score, baseline:  0.00012666666666657136 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  46 total reward:  0.5666666666666669  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.00012666666666657136 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.00012666666666657136 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  41 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  34.21850919723511
maxi score, test score, baseline:  0.00012666666666657136 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.11501356052528
actor:  1 policy actor:  1  step number:  59 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.00012666666666657136 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  49 total reward:  0.22666666666666613  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.0001933333333334218 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.79842558492372
maxi score, test score, baseline:  -0.0001933333333334218 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.23317791727749
maxi score, test score, baseline:  -0.0001933333333334218 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.5658509374
actor:  1 policy actor:  1  step number:  71 total reward:  0.18666666666666643  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.0001933333333334218 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.899926594325475
printing an ep nov before normalisation:  39.35271396918405
printing an ep nov before normalisation:  51.84771395693445
maxi score, test score, baseline:  -0.0001933333333334218 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0001933333333334218 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.71477969638145
UNIT TEST: sample policy line 217 mcts : [0.02  0.571 0.02  0.02  0.122 0.061 0.184]
printing an ep nov before normalisation:  35.06606101989746
maxi score, test score, baseline:  -0.0001933333333334218 0.6986666666666668 0.6986666666666668
actor:  1 policy actor:  1  step number:  71 total reward:  0.26666666666666605  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.0001933333333334218 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.8673333611515
actor:  1 policy actor:  1  step number:  57 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.365]
 [0.476]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]] [[44.767]
 [49.744]
 [44.767]
 [44.767]
 [44.767]
 [44.767]
 [44.767]] [[0.77 ]
 [0.977]
 [0.77 ]
 [0.77 ]
 [0.77 ]
 [0.77 ]
 [0.77 ]]
maxi score, test score, baseline:  -0.0001933333333334218 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0001933333333334218 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.77424733197132
maxi score, test score, baseline:  -0.0001933333333334218 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0001933333333334218 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.445]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]] [[32.706]
 [41.464]
 [32.706]
 [32.706]
 [32.706]
 [32.706]
 [32.706]] [[1.442]
 [2.06 ]
 [1.442]
 [1.442]
 [1.442]
 [1.442]
 [1.442]]
maxi score, test score, baseline:  -0.0001933333333334218 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.59 ]
 [0.734]
 [0.663]
 [0.631]
 [0.663]
 [0.663]
 [0.687]] [[26.737]
 [30.545]
 [26.089]
 [28.171]
 [26.089]
 [26.089]
 [30.181]] [[1.88 ]
 [2.378]
 [1.893]
 [2.054]
 [1.893]
 [1.893]
 [2.297]]
Printing some Q and Qe and total Qs values:  [[0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]] [[27.559]
 [27.559]
 [27.559]
 [27.559]
 [27.559]
 [27.559]
 [27.559]] [[2.509]
 [2.509]
 [2.509]
 [2.509]
 [2.509]
 [2.509]
 [2.509]]
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  43 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  0.015082391821010788
maxi score, test score, baseline:  -0.0001933333333334218 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  -0.0001933333333334218 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  59 total reward:  0.09333333333333271  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  32.56766561351846
Printing some Q and Qe and total Qs values:  [[0.86 ]
 [0.927]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]] [[32.599]
 [32.223]
 [32.599]
 [32.599]
 [32.599]
 [32.599]
 [32.599]] [[0.86 ]
 [0.927]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]]
printing an ep nov before normalisation:  55.93331221212027
actions average: 
K:  4  action  0 :  tensor([0.7111, 0.0122, 0.0317, 0.0957, 0.0716, 0.0344, 0.0433],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0232, 0.8387, 0.0220, 0.0357, 0.0113, 0.0290, 0.0402],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0972, 0.0038, 0.3957, 0.1126, 0.0949, 0.1695, 0.1264],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1425, 0.2628, 0.1066, 0.1726, 0.1106, 0.0919, 0.1130],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1098, 0.0256, 0.0891, 0.1729, 0.3554, 0.1489, 0.0982],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1377, 0.0994, 0.1223, 0.1400, 0.1168, 0.2475, 0.1362],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0852, 0.2537, 0.0967, 0.2011, 0.0722, 0.0760, 0.2151],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0019933333333332173 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.4930424758542
Printing some Q and Qe and total Qs values:  [[0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]] [[50.509]
 [50.509]
 [50.509]
 [50.509]
 [50.509]
 [50.509]
 [50.509]] [[1.963]
 [1.963]
 [1.963]
 [1.963]
 [1.963]
 [1.963]
 [1.963]]
actor:  1 policy actor:  1  step number:  44 total reward:  0.5666666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.00047333333333345143 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.410690491553225
printing an ep nov before normalisation:  42.22726808251142
actions average: 
K:  3  action  0 :  tensor([0.5355, 0.0065, 0.0842, 0.0963, 0.0927, 0.1076, 0.0772],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0095, 0.9243, 0.0082, 0.0126, 0.0050, 0.0063, 0.0341],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1266, 0.0332, 0.2723, 0.1541, 0.1203, 0.1925, 0.1009],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1299, 0.0116, 0.1693, 0.1905, 0.1608, 0.2063, 0.1317],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1286, 0.0445, 0.0847, 0.1080, 0.4213, 0.1192, 0.0937],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0953, 0.0127, 0.1386, 0.1409, 0.1315, 0.3783, 0.1027],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1315, 0.0054, 0.1150, 0.1856, 0.1516, 0.1806, 0.2302],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.00047333333333345143 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.00047333333333345143 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.472]
 [0.563]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]] [[39.482]
 [37.877]
 [39.482]
 [39.482]
 [39.482]
 [39.482]
 [39.482]] [[0.72 ]
 [0.793]
 [0.72 ]
 [0.72 ]
 [0.72 ]
 [0.72 ]
 [0.72 ]]
printing an ep nov before normalisation:  35.037940569473
printing an ep nov before normalisation:  37.43158258598335
printing an ep nov before normalisation:  39.96547422741478
maxi score, test score, baseline:  -0.00047333333333345143 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.00047333333333345143 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  57 total reward:  0.3333333333333326  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  41.52868182722715
printing an ep nov before normalisation:  36.53763187088943
maxi score, test score, baseline:  -0.00047333333333345143 0.6986666666666668 0.6986666666666668
actor:  1 policy actor:  1  step number:  45 total reward:  0.4399999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.00047333333333345143 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.00047333333333345143 0.6986666666666668 0.6986666666666668
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.613]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]] [[42.591]
 [45.126]
 [42.591]
 [42.591]
 [42.591]
 [42.591]
 [42.591]] [[1.137]
 [1.198]
 [1.137]
 [1.137]
 [1.137]
 [1.137]
 [1.137]]
printing an ep nov before normalisation:  44.99467570721953
maxi score, test score, baseline:  -0.00047333333333345143 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  36.27025657161974
printing an ep nov before normalisation:  43.66957610719769
maxi score, test score, baseline:  -0.00047333333333345143 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  30.606105794381367
printing an ep nov before normalisation:  41.499325103231904
maxi score, test score, baseline:  -0.00047333333333345143 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.19999999999999962  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  57.75384421889887
maxi score, test score, baseline:  -0.00047333333333345143 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.285]
 [ 0.388]
 [ 0.253]
 [ 0.215]
 [ 0.247]
 [-0.043]
 [ 0.226]] [[38.446]
 [36.924]
 [33.203]
 [31.788]
 [35.523]
 [32.053]
 [29.779]] [[0.527]
 [0.614]
 [0.44 ]
 [0.387]
 [0.458]
 [0.132]
 [0.377]]
Printing some Q and Qe and total Qs values:  [[0.376]
 [0.528]
 [0.374]
 [0.374]
 [0.367]
 [0.368]
 [0.369]] [[33.109]
 [32.751]
 [33.099]
 [33.259]
 [32.757]
 [32.696]
 [32.697]] [[0.619]
 [0.766]
 [0.617]
 [0.619]
 [0.605]
 [0.605]
 [0.606]]
Printing some Q and Qe and total Qs values:  [[0.078]
 [0.107]
 [0.078]
 [0.078]
 [0.078]
 [0.078]
 [0.078]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.078]
 [0.107]
 [0.078]
 [0.078]
 [0.078]
 [0.078]
 [0.078]]
maxi score, test score, baseline:  -0.00047333333333345143 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  42 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.00047333333333345143 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.4763, 0.0726, 0.0728, 0.1003, 0.1070, 0.0874, 0.0836],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0162, 0.9266, 0.0051, 0.0151, 0.0064, 0.0064, 0.0241],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1140, 0.0945, 0.2783, 0.1135, 0.1034, 0.1772, 0.1191],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1264, 0.0123, 0.1168, 0.2937, 0.1714, 0.1567, 0.1226],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1421, 0.0468, 0.0986, 0.1014, 0.4350, 0.0900, 0.0861],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0950, 0.0613, 0.1652, 0.0861, 0.0910, 0.4217, 0.0796],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1386, 0.0702, 0.1510, 0.1254, 0.1146, 0.1609, 0.2392],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  60 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  35.10432759283144
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  33 total reward:  0.5333333333333333  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  34.34245911074286
line 256 mcts: sample exp_bonus 37.66005981291141
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0025933333333332115 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.765]
 [0.89 ]
 [0.786]
 [0.762]
 [0.771]
 [0.786]
 [0.756]] [[40.22 ]
 [39.927]
 [39.732]
 [42.66 ]
 [47.173]
 [39.732]
 [45.201]] [[0.765]
 [0.89 ]
 [0.786]
 [0.762]
 [0.771]
 [0.786]
 [0.756]]
maxi score, test score, baseline:  0.0025933333333332115 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.11856398568349
siam score:  -0.8362938
maxi score, test score, baseline:  -0.0001666666666667652 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  37 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.002846666666666556 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.81080583822781
Printing some Q and Qe and total Qs values:  [[ 0.038]
 [-0.001]
 [ 0.009]
 [ 0.029]
 [ 0.017]
 [ 0.029]
 [-0.001]] [[37.641]
 [40.383]
 [37.571]
 [39.236]
 [38.535]
 [39.162]
 [37.669]] [[0.282]
 [0.278]
 [0.252]
 [0.293]
 [0.273]
 [0.292]
 [0.243]]
printing an ep nov before normalisation:  49.259684228922644
Printing some Q and Qe and total Qs values:  [[ 0.356]
 [ 0.16 ]
 [ 0.349]
 [-0.028]
 [-0.083]
 [ 0.415]
 [-0.029]] [[33.355]
 [37.44 ]
 [36.791]
 [34.437]
 [37.205]
 [39.622]
 [37.997]] [[1.26 ]
 [1.299]
 [1.451]
 [0.939]
 [1.042]
 [1.678]
 [1.141]]
maxi score, test score, baseline:  0.002846666666666556 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.002846666666666556 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.380558137931274
maxi score, test score, baseline:  0.002846666666666556 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.04194500701232
maxi score, test score, baseline:  0.002846666666666556 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.002846666666666556 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.2866666666666665  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.002846666666666556 0.6986666666666668 0.6986666666666668
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  44 total reward:  0.6066666666666668  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  29.21236316965344
maxi score, test score, baseline:  0.002846666666666556 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  58 total reward:  0.19333333333333302  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  42.528587973777675
maxi score, test score, baseline:  0.002846666666666556 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.818995057425695
printing an ep nov before normalisation:  38.044831166352594
printing an ep nov before normalisation:  45.6478350989297
actor:  0 policy actor:  0  step number:  38 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0057933333333332285 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.09567503667284
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  39.36496375080195
line 256 mcts: sample exp_bonus 58.245775643042926
printing an ep nov before normalisation:  36.35725199532024
siam score:  -0.8237405
actor:  1 policy actor:  1  step number:  63 total reward:  0.14666666666666628  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  30.115273327268532
maxi score, test score, baseline:  0.0057933333333332285 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0057933333333332285 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0057933333333332285 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.68719448761724
maxi score, test score, baseline:  0.0057933333333332285 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  45.71762491127842
maxi score, test score, baseline:  0.0057933333333332285 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0057933333333332285 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  71 total reward:  0.2266666666666658  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  45 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.019]
 [ 0.127]
 [ 0.019]
 [ 0.031]
 [-0.002]
 [ 0.017]
 [ 0.054]] [[37.167]
 [32.182]
 [48.626]
 [48.038]
 [45.889]
 [43.54 ]
 [42.092]] [[0.964]
 [0.884]
 [1.524]
 [1.509]
 [1.379]
 [1.291]
 [1.261]]
Printing some Q and Qe and total Qs values:  [[1.002]
 [1.018]
 [1.002]
 [1.002]
 [1.002]
 [1.002]
 [1.002]] [[35.551]
 [38.939]
 [35.551]
 [35.551]
 [35.551]
 [35.551]
 [35.551]] [[1.002]
 [1.018]
 [1.002]
 [1.002]
 [1.002]
 [1.002]
 [1.002]]
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.622]
 [0.512]
 [0.512]
 [0.516]
 [0.512]
 [0.512]] [[52.697]
 [49.232]
 [52.697]
 [52.697]
 [52.232]
 [52.697]
 [52.697]] [[2.31 ]
 [2.23 ]
 [2.31 ]
 [2.31 ]
 [2.288]
 [2.31 ]
 [2.31 ]]
maxi score, test score, baseline:  0.008246666666666532 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  43 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.011073333333333215 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.011073333333333215 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  28.53756630430519
printing an ep nov before normalisation:  37.44340208052898
maxi score, test score, baseline:  0.011073333333333215 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.4777, 0.0098, 0.0800, 0.0860, 0.1507, 0.1025, 0.0934],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0094, 0.9098, 0.0109, 0.0212, 0.0055, 0.0068, 0.0365],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1155, 0.0049, 0.3403, 0.1088, 0.1158, 0.2114, 0.1033],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0794, 0.0249, 0.1063, 0.3871, 0.1364, 0.1031, 0.1629],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1391, 0.1236, 0.1101, 0.1228, 0.2850, 0.1170, 0.1024],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0931, 0.0606, 0.1482, 0.1051, 0.1056, 0.4056, 0.0819],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0920, 0.1021, 0.1045, 0.0937, 0.0879, 0.1058, 0.4140],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  37.457919221442616
printing an ep nov before normalisation:  41.103754819983294
maxi score, test score, baseline:  0.011073333333333215 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  0.011073333333333215 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.011073333333333215 0.6986666666666668 0.6986666666666668
actor:  0 policy actor:  0  step number:  41 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.013846666666666556 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.013846666666666556 0.6986666666666668 0.6986666666666668
using explorer policy with actor:  1
maxi score, test score, baseline:  0.013846666666666556 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  39 total reward:  0.5333333333333333  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.036]
 [-0.003]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]] [[34.402]
 [44.84 ]
 [34.402]
 [34.402]
 [34.402]
 [34.402]
 [34.402]] [[0.548]
 [0.873]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]]
printing an ep nov before normalisation:  55.23873344470921
maxi score, test score, baseline:  0.01384666666666657 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.19999999999999973  reward:  1.0 rdn_beta:  1.333
siam score:  -0.8080133
printing an ep nov before normalisation:  54.67904001390984
printing an ep nov before normalisation:  43.08520114885797
maxi score, test score, baseline:  0.01384666666666657 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.01384666666666657 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.22666666666666613  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.01384666666666657 0.6986666666666668 0.6986666666666668
actor:  1 policy actor:  1  step number:  70 total reward:  0.2733333333333331  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.01384666666666657 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.01384666666666657 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
printing an ep nov before normalisation:  51.623162019888966
actor:  1 policy actor:  1  step number:  37 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  1  action  0 :  tensor([0.3743, 0.0208, 0.1229, 0.1642, 0.1130, 0.1017, 0.1030],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0101, 0.9462, 0.0070, 0.0076, 0.0061, 0.0068, 0.0163],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1474, 0.0077, 0.3239, 0.1283, 0.1262, 0.1394, 0.1271],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1190, 0.0817, 0.1549, 0.2280, 0.1485, 0.1298, 0.1381],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1310, 0.0035, 0.0617, 0.1070, 0.5259, 0.0844, 0.0866],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1136, 0.0552, 0.1208, 0.1648, 0.1451, 0.2677, 0.1328],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0602, 0.3145, 0.0435, 0.1421, 0.0491, 0.0431, 0.3474],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  44.237694694446
printing an ep nov before normalisation:  39.934575898650934
maxi score, test score, baseline:  0.01384666666666657 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  41.56530842024876
maxi score, test score, baseline:  0.01384666666666657 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.01384666666666657 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.01384666666666657 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.08759439286372
maxi score, test score, baseline:  0.01384666666666657 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.01384666666666657 0.6986666666666668 0.6986666666666668
siam score:  -0.813054
actor:  1 policy actor:  1  step number:  48 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.01384666666666657 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 31.742738003546464
printing an ep nov before normalisation:  33.128258347001676
using explorer policy with actor:  1
siam score:  -0.8135507
maxi score, test score, baseline:  0.01384666666666657 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.28348007721828
line 256 mcts: sample exp_bonus 43.91407844592638
maxi score, test score, baseline:  0.01384666666666657 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.69246714047084
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.668]
 [0.624]
 [0.633]
 [0.514]
 [0.518]
 [0.506]] [[24.849]
 [24.31 ]
 [25.991]
 [29.993]
 [24.849]
 [24.941]
 [24.269]] [[0.931]
 [1.059]
 [1.042]
 [1.118]
 [0.914]
 [0.919]
 [0.897]]
maxi score, test score, baseline:  0.01384666666666657 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.245393932994567
maxi score, test score, baseline:  0.01384666666666657 0.6986666666666668 0.6986666666666668
actor:  1 policy actor:  1  step number:  56 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  22.10388635481871
printing an ep nov before normalisation:  40.65247136456732
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  60 total reward:  0.23333333333333262  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  67 total reward:  0.09333333333333282  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.01384666666666657 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.091]
 [0.166]
 [0.107]
 [0.091]
 [0.091]
 [0.091]
 [0.142]] [[33.942]
 [45.163]
 [35.349]
 [33.942]
 [33.942]
 [33.942]
 [34.211]] [[0.609]
 [1.068]
 [0.673]
 [0.609]
 [0.609]
 [0.609]
 [0.669]]
maxi score, test score, baseline:  0.01384666666666657 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  50.26065487980957
printing an ep nov before normalisation:  36.80902671427042
maxi score, test score, baseline:  0.01384666666666657 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.01384666666666657 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.01384666666666657 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.013846666666666563 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 39.32998890091596
maxi score, test score, baseline:  0.013846666666666563 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.88040758171663
maxi score, test score, baseline:  0.013846666666666563 0.6986666666666668 0.6986666666666668
actions average: 
K:  1  action  0 :  tensor([0.4635, 0.0197, 0.0781, 0.0692, 0.2082, 0.0712, 0.0900],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0097, 0.9252, 0.0068, 0.0133, 0.0091, 0.0087, 0.0273],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1175, 0.0316, 0.3823, 0.1025, 0.1141, 0.1251, 0.1268],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0864, 0.1588, 0.1201, 0.2908, 0.1070, 0.1086, 0.1282],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1217, 0.0583, 0.1495, 0.1209, 0.2788, 0.1364, 0.1344],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0765, 0.0080, 0.2024, 0.0489, 0.0680, 0.5277, 0.0684],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1649, 0.0032, 0.1723, 0.1622, 0.1801, 0.1824, 0.1349],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.013846666666666563 0.6986666666666668 0.6986666666666668
actor:  1 policy actor:  1  step number:  60 total reward:  0.23333333333333273  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.013846666666666563 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  0.013846666666666563 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  0.013846666666666563 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.013846666666666563 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.013846666666666563 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  82 total reward:  0.08666666666666523  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  43.38064592634862
maxi score, test score, baseline:  0.013846666666666563 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.3266667424512
maxi score, test score, baseline:  0.013846666666666563 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  39 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  39.68628302219721
actor:  1 policy actor:  1  step number:  57 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  44.847285425659145
printing an ep nov before normalisation:  56.984588934427144
actor:  1 policy actor:  1  step number:  59 total reward:  0.25333333333333286  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.492]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]] [[56.834]
 [52.886]
 [56.834]
 [56.834]
 [56.834]
 [56.834]
 [56.834]] [[1.465]
 [1.383]
 [1.465]
 [1.465]
 [1.465]
 [1.465]
 [1.465]]
siam score:  -0.8168525
using explorer policy with actor:  1
maxi score, test score, baseline:  0.013846666666666563 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.013846666666666563 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.33893141291819
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]] [[37.843]
 [37.843]
 [37.843]
 [37.843]
 [37.843]
 [37.843]
 [37.843]] [[1.653]
 [1.653]
 [1.653]
 [1.653]
 [1.653]
 [1.653]
 [1.653]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
actor:  1 policy actor:  1  step number:  49 total reward:  0.3866666666666666  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.013846666666666563 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.507766734651256
maxi score, test score, baseline:  0.013846666666666563 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.92973354741302
Printing some Q and Qe and total Qs values:  [[0.891]
 [0.879]
 [0.804]
 [0.804]
 [0.769]
 [0.804]
 [0.804]] [[39.202]
 [39.613]
 [41.245]
 [41.245]
 [42.021]
 [41.245]
 [41.245]] [[0.891]
 [0.879]
 [0.804]
 [0.804]
 [0.769]
 [0.804]
 [0.804]]
actor:  1 policy actor:  1  step number:  63 total reward:  0.1599999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.013846666666666563 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.30666666666666575  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.219]
 [0.223]
 [0.273]
 [0.158]
 [0.165]
 [0.181]
 [0.273]] [[38.231]
 [48.159]
 [42.923]
 [45.968]
 [43.309]
 [42.197]
 [42.923]] [[1.034]
 [1.391]
 [1.255]
 [1.249]
 [1.161]
 [1.137]
 [1.255]]
maxi score, test score, baseline:  0.013846666666666563 0.6986666666666668 0.6986666666666668
Printing some Q and Qe and total Qs values:  [[0.252]
 [0.264]
 [0.256]
 [0.253]
 [0.256]
 [0.251]
 [0.341]] [[35.4  ]
 [43.785]
 [41.337]
 [42.198]
 [41.337]
 [39.652]
 [41.689]] [[0.864]
 [1.301]
 [1.168]
 [1.209]
 [1.168]
 [1.079]
 [1.271]]
maxi score, test score, baseline:  0.013846666666666563 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  0.013846666666666563 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  0.013846666666666563 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  59 total reward:  0.11999999999999922  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.01608666666666655 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  36.64005795465267
actions average: 
K:  4  action  0 :  tensor([0.3564, 0.0108, 0.0781, 0.1218, 0.2314, 0.0845, 0.1170],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0070, 0.9472, 0.0085, 0.0097, 0.0053, 0.0081, 0.0143],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1126, 0.0217, 0.3974, 0.0888, 0.1106, 0.1459, 0.1229],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1315, 0.0618, 0.1605, 0.1664, 0.1498, 0.1761, 0.1538],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1381, 0.0169, 0.0993, 0.0760, 0.5301, 0.0710, 0.0687],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1073, 0.0491, 0.1499, 0.1223, 0.1255, 0.3489, 0.0970],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0776, 0.2081, 0.0754, 0.1544, 0.1177, 0.1436, 0.2232],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.01608666666666655 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  41 total reward:  0.5466666666666667  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.01608666666666655 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  0.01608666666666655 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]] [[44.745]
 [44.745]
 [44.745]
 [44.745]
 [44.745]
 [44.745]
 [44.745]] [[0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]]
maxi score, test score, baseline:  0.01608666666666655 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.01608666666666655 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.592842502852506
printing an ep nov before normalisation:  37.78214749546037
printing an ep nov before normalisation:  41.17378001918687
actor:  1 policy actor:  1  step number:  61 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  69 total reward:  0.15999999999999936  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.006]
 [-0.005]
 [-0.006]
 [-0.014]
 [-0.014]
 [-0.006]
 [-0.006]] [[41.46 ]
 [54.081]
 [41.46 ]
 [51.348]
 [51.464]
 [41.46 ]
 [41.46 ]] [[0.187]
 [0.31 ]
 [0.187]
 [0.275]
 [0.276]
 [0.187]
 [0.187]]
maxi score, test score, baseline:  0.01608666666666655 0.6986666666666668 0.6986666666666668
Printing some Q and Qe and total Qs values:  [[-0.011]
 [-0.013]
 [-0.013]
 [-0.012]
 [-0.012]
 [-0.012]
 [-0.013]] [[36.997]
 [41.088]
 [37.655]
 [37.256]
 [37.522]
 [37.932]
 [37.801]] [[0.867]
 [1.1  ]
 [0.902]
 [0.881]
 [0.896]
 [0.92 ]
 [0.912]]
Printing some Q and Qe and total Qs values:  [[-0.016]
 [-0.009]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]] [[39.607]
 [41.195]
 [39.607]
 [39.607]
 [39.607]
 [39.607]
 [39.607]] [[1.217]
 [1.324]
 [1.217]
 [1.217]
 [1.217]
 [1.217]
 [1.217]]
actor:  1 policy actor:  1  step number:  65 total reward:  0.10666666666666569  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.01608666666666655 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  0.01608666666666655 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.0007012743463
maxi score, test score, baseline:  0.01608666666666655 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.01608666666666655 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.35077575242432
printing an ep nov before normalisation:  32.12853185293989
Printing some Q and Qe and total Qs values:  [[0.37 ]
 [0.411]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.306]
 [0.37 ]] [[26.478]
 [29.722]
 [26.478]
 [26.478]
 [26.478]
 [13.817]
 [26.478]] [[0.37 ]
 [0.411]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.306]
 [0.37 ]]
actor:  1 policy actor:  1  step number:  65 total reward:  0.14666666666666595  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  6.31482731705546e-05
actor:  1 policy actor:  1  step number:  51 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.01608666666666655 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.98367964799679
maxi score, test score, baseline:  0.01608666666666655 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.758]
 [0.814]
 [0.741]
 [0.741]
 [0.733]
 [0.741]
 [0.741]] [[39.102]
 [39.814]
 [34.581]
 [34.581]
 [39.437]
 [34.581]
 [34.581]] [[0.758]
 [0.814]
 [0.741]
 [0.741]
 [0.733]
 [0.741]
 [0.741]]
maxi score, test score, baseline:  0.01608666666666655 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.06475120764921
printing an ep nov before normalisation:  30.153057252702276
maxi score, test score, baseline:  0.01608666666666655 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.699]
 [0.841]
 [0.701]
 [0.743]
 [0.757]
 [0.757]
 [0.703]] [[36.262]
 [36.928]
 [36.303]
 [35.447]
 [33.748]
 [33.748]
 [36.978]] [[0.699]
 [0.841]
 [0.701]
 [0.743]
 [0.757]
 [0.757]
 [0.703]]
maxi score, test score, baseline:  0.01608666666666655 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.01608666666666655 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.88 ]
 [0.951]
 [1.006]
 [0.88 ]
 [0.944]
 [0.949]
 [0.88 ]] [[25.119]
 [18.185]
 [31.52 ]
 [25.119]
 [30.908]
 [17.052]
 [25.119]] [[0.88 ]
 [0.951]
 [1.006]
 [0.88 ]
 [0.944]
 [0.949]
 [0.88 ]]
actor:  0 policy actor:  0  step number:  58 total reward:  0.08666666666666578  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.01825999999999989 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  0.01825999999999989 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  55 total reward:  0.27999999999999936  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.018259999999999884 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.6684385825234
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.36408666647586
maxi score, test score, baseline:  0.018259999999999884 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  0.018259999999999884 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.018259999999999884 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  59.29366501390679
actor:  0 policy actor:  0  step number:  41 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.006]
 [ 0.027]
 [-0.015]
 [-0.01 ]
 [-0.017]
 [-0.019]
 [-0.007]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.006]
 [ 0.027]
 [-0.015]
 [-0.01 ]
 [-0.017]
 [-0.019]
 [-0.007]]
maxi score, test score, baseline:  0.018299999999999875 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.13579974114446
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.10221531871876
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.018299999999999875 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  39.70825384744736
maxi score, test score, baseline:  0.018299999999999875 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.4  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.018299999999999875 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.018299999999999875 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.8231977
maxi score, test score, baseline:  0.018299999999999875 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  69 total reward:  0.13333333333333286  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.018299999999999875 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.018299999999999875 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.018299999999999875 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  39.92919950940676
printing an ep nov before normalisation:  41.18875207449672
printing an ep nov before normalisation:  14.744431972503662
printing an ep nov before normalisation:  15.16187933321206
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.595]
 [0.515]
 [0.378]
 [0.503]
 [0.399]
 [0.399]] [[32.835]
 [34.465]
 [33.535]
 [30.132]
 [33.303]
 [29.96 ]
 [29.763]] [[0.769]
 [0.853]
 [0.76 ]
 [0.579]
 [0.746]
 [0.598]
 [0.596]]
printing an ep nov before normalisation:  48.39721197015921
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
maxi score, test score, baseline:  0.018299999999999882 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  42.28828119225347
maxi score, test score, baseline:  0.018299999999999882 0.6986666666666668 0.6986666666666668
Printing some Q and Qe and total Qs values:  [[0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]] [[32.504]
 [32.504]
 [32.504]
 [32.504]
 [32.504]
 [32.504]
 [32.504]] [[10.966]
 [10.966]
 [10.966]
 [10.966]
 [10.966]
 [10.966]
 [10.966]]
actor:  1 policy actor:  1  step number:  64 total reward:  0.23333333333333262  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.34666897661992
Printing some Q and Qe and total Qs values:  [[0.908]
 [0.945]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]] [[31.807]
 [39.457]
 [31.807]
 [31.807]
 [31.807]
 [31.807]
 [31.807]] [[0.908]
 [0.945]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]]
printing an ep nov before normalisation:  42.72998014715822
maxi score, test score, baseline:  0.018299999999999882 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  0.018299999999999882 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.49224901199341
Printing some Q and Qe and total Qs values:  [[0.187]
 [0.339]
 [0.185]
 [0.175]
 [0.185]
 [0.184]
 [0.187]] [[45.324]
 [46.228]
 [46.825]
 [44.283]
 [43.865]
 [44.38 ]
 [44.367]] [[0.402]
 [0.562]
 [0.414]
 [0.381]
 [0.387]
 [0.391]
 [0.394]]
maxi score, test score, baseline:  0.018299999999999882 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.00412282541327
maxi score, test score, baseline:  0.018299999999999882 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.018299999999999882 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.018299999999999882 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.34942687453003
maxi score, test score, baseline:  0.018299999999999882 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.21333333333333304  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  57.38506331460406
printing an ep nov before normalisation:  65.91123367655793
maxi score, test score, baseline:  0.018299999999999882 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.5045, 0.0896, 0.0914, 0.0812, 0.0649, 0.0873, 0.0811],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0189, 0.8861, 0.0134, 0.0230, 0.0115, 0.0146, 0.0326],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2221, 0.0400, 0.1523, 0.1407, 0.1328, 0.1678, 0.1442],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0965, 0.0169, 0.1298, 0.4146, 0.0710, 0.1589, 0.1123],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1459, 0.0527, 0.1072, 0.1257, 0.3449, 0.1317, 0.0919],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1691, 0.0064, 0.1530, 0.1335, 0.1007, 0.3090, 0.1283],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1378, 0.2288, 0.1082, 0.1281, 0.0893, 0.1407, 0.1670],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.018299999999999882 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  0.018299999999999882 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.48  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  38 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  0.667
siam score:  -0.81563354
actor:  1 policy actor:  1  step number:  49 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  57.77557389525629
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.416]
 [0.372]
 [0.371]
 [0.372]
 [0.373]
 [0.372]] [[41.469]
 [38.592]
 [41.469]
 [40.896]
 [41.469]
 [43.495]
 [41.469]] [[1.579]
 [1.478]
 [1.579]
 [1.549]
 [1.579]
 [1.681]
 [1.579]]
maxi score, test score, baseline:  0.018406666666666543 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.48454679006711
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.599]
 [0.478]
 [0.473]
 [0.471]
 [0.443]
 [0.462]] [[37.694]
 [36.97 ]
 [39.095]
 [38.996]
 [38.879]
 [40.888]
 [39.1  ]] [[1.46 ]
 [1.525]
 [1.506]
 [1.496]
 [1.489]
 [1.558]
 [1.49 ]]
maxi score, test score, baseline:  0.018406666666666543 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  0.018406666666666543 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.3108, 0.1536, 0.0922, 0.1013, 0.1504, 0.0740, 0.1177],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0035,     0.9714,     0.0031,     0.0090,     0.0004,     0.0006,
            0.0120], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0968, 0.0052, 0.3271, 0.1911, 0.1405, 0.1318, 0.1075],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1346, 0.0074, 0.1448, 0.2637, 0.1698, 0.1508, 0.1291],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1240, 0.0782, 0.0701, 0.2006, 0.2956, 0.1097, 0.1218],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0969, 0.0071, 0.1576, 0.1112, 0.1134, 0.4425, 0.0714],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0737, 0.2612, 0.0867, 0.1506, 0.0880, 0.1112, 0.2286],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  52 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[ 0.036]
 [ 0.188]
 [ 0.012]
 [-0.009]
 [-0.009]
 [ 0.012]
 [-0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.036]
 [ 0.188]
 [ 0.012]
 [-0.009]
 [-0.009]
 [ 0.012]
 [-0.001]]
printing an ep nov before normalisation:  33.9499306678772
printing an ep nov before normalisation:  33.51953060942212
printing an ep nov before normalisation:  29.44636612643929
maxi score, test score, baseline:  0.018406666666666543 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.018406666666666543 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  40.04626006274594
actor:  1 policy actor:  1  step number:  67 total reward:  0.10666666666666613  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.018406666666666543 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  0.018406666666666543 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.018406666666666543 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.489162801478074
maxi score, test score, baseline:  0.018406666666666543 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  0.0020063852844032226
maxi score, test score, baseline:  0.018406666666666543 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.783254761471476
actor:  0 policy actor:  0  step number:  45 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.021073333333333208 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.5478293262019
maxi score, test score, baseline:  0.021073333333333208 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  58 total reward:  0.4733333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.021073333333333208 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.021073333333333208 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  39.23699704641548
actor:  1 policy actor:  1  step number:  56 total reward:  0.28666666666666607  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.021073333333333208 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  0.021073333333333208 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.21489119529724
printing an ep nov before normalisation:  36.55622690912219
siam score:  -0.8050771
actor:  1 policy actor:  1  step number:  65 total reward:  0.14666666666666572  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.021073333333333208 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]] [[14.249]
 [14.249]
 [14.249]
 [14.249]
 [14.249]
 [14.249]
 [14.249]] [[0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]]
maxi score, test score, baseline:  0.021073333333333208 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.961574207004865
maxi score, test score, baseline:  0.01831333333333321 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.01831333333333321 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.1891, 0.0269, 0.1240, 0.1910, 0.1511, 0.1088, 0.2091],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0065, 0.9259, 0.0047, 0.0210, 0.0034, 0.0024, 0.0361],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1324, 0.0539, 0.3741, 0.0870, 0.1017, 0.1422, 0.1087],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1134, 0.1154, 0.0903, 0.2788, 0.1303, 0.1274, 0.1445],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2534, 0.0560, 0.0735, 0.0908, 0.3623, 0.0746, 0.0895],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1174, 0.1859, 0.1380, 0.1440, 0.1432, 0.1085, 0.1631],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0936, 0.3541, 0.0661, 0.1042, 0.0669, 0.0611, 0.2539],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.01831333333333321 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.844181060791016
printing an ep nov before normalisation:  24.422730590751215
using explorer policy with actor:  1
siam score:  -0.80727655
actor:  1 policy actor:  1  step number:  61 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 51.25302484377437
maxi score, test score, baseline:  0.01831333333333321 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  40 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  52.98241557816136
printing an ep nov before normalisation:  41.340853080909035
maxi score, test score, baseline:  0.021286666666666548 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.44199489459856
maxi score, test score, baseline:  0.021286666666666548 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.021286666666666548 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.68089582673593
actor:  1 policy actor:  1  step number:  44 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  46.065748509065855
printing an ep nov before normalisation:  52.43033170392279
printing an ep nov before normalisation:  0.0005670607407637362
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.021286666666666548 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.334220611840024
actor:  0 policy actor:  0  step number:  47 total reward:  0.37333333333333274  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.02179333333333322 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.02179333333333322 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  43 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.021873333333333217 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.021873333333333217 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.021873333333333217 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.5133333333333336  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  48.604648809434494
maxi score, test score, baseline:  0.021873333333333217 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  34.653781206002456
actor:  1 policy actor:  1  step number:  43 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.021873333333333217 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.24174240742223
printing an ep nov before normalisation:  43.80012559067249
printing an ep nov before normalisation:  42.918850529703086
maxi score, test score, baseline:  0.021873333333333217 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.801]
 [0.856]
 [0.801]
 [0.801]
 [0.801]
 [0.801]
 [0.801]] [[31.404]
 [34.101]
 [31.404]
 [31.404]
 [31.404]
 [31.404]
 [31.404]] [[0.801]
 [0.856]
 [0.801]
 [0.801]
 [0.801]
 [0.801]
 [0.801]]
printing an ep nov before normalisation:  32.458740758374475
siam score:  -0.82255614
maxi score, test score, baseline:  0.021873333333333217 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.021873333333333217 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.021873333333333217 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  0.021873333333333217 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  0.021873333333333217 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  45 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.024459999999999885 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  54.977538528437734
actor:  1 policy actor:  1  step number:  74 total reward:  0.20666666666666567  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.024459999999999885 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  36.36332363475537
printing an ep nov before normalisation:  39.61165622754211
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.024459999999999885 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.446]
 [0.536]
 [0.446]
 [0.446]
 [0.446]
 [0.446]
 [0.446]] [[26.964]
 [31.551]
 [26.964]
 [26.964]
 [26.964]
 [26.964]
 [26.964]] [[0.786]
 [0.995]
 [0.786]
 [0.786]
 [0.786]
 [0.786]
 [0.786]]
maxi score, test score, baseline:  0.024459999999999885 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 37.54146204180277
printing an ep nov before normalisation:  38.62508956858256
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.549]
 [0.497]
 [0.519]
 [0.526]
 [0.52 ]
 [0.518]] [[37.269]
 [41.933]
 [35.685]
 [40.582]
 [42.204]
 [42.01 ]
 [41.954]] [[0.813]
 [0.974]
 [0.79 ]
 [0.916]
 [0.958]
 [0.947]
 [0.944]]
maxi score, test score, baseline:  0.024459999999999885 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.3214, 0.0227, 0.0986, 0.1236, 0.2198, 0.1221, 0.0918],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0130, 0.9351, 0.0107, 0.0100, 0.0056, 0.0059, 0.0196],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0920, 0.1031, 0.4149, 0.1025, 0.0995, 0.1082, 0.0798],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1215, 0.0152, 0.1164, 0.3623, 0.1691, 0.1330, 0.0826],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1291, 0.0207, 0.1293, 0.1565, 0.3282, 0.1329, 0.1033],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1103, 0.0958, 0.1942, 0.1429, 0.1501, 0.2217, 0.0851],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1145, 0.2062, 0.1696, 0.1216, 0.0935, 0.1420, 0.1526],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  45 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.027019999999999874 0.6986666666666668 0.6986666666666668
actor:  1 policy actor:  1  step number:  72 total reward:  0.1666666666666654  reward:  1.0 rdn_beta:  1.0
siam score:  -0.81700814
maxi score, test score, baseline:  0.027019999999999874 0.6986666666666668 0.6986666666666668
actor:  1 policy actor:  1  step number:  71 total reward:  0.05333333333333257  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.027019999999999874 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.027019999999999874 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  0.027019999999999874 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  0.5666666666666667  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.027019999999999874 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.45479520161946
maxi score, test score, baseline:  0.027019999999999874 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  0.027019999999999874 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.027019999999999874 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.4160, 0.0008, 0.1058, 0.0811, 0.1914, 0.1007, 0.1042],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0104, 0.9092, 0.0119, 0.0122, 0.0074, 0.0104, 0.0385],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0823, 0.0025, 0.5419, 0.0348, 0.0502, 0.2380, 0.0502],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1440, 0.2397, 0.1234, 0.1159, 0.1120, 0.1318, 0.1333],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1485, 0.0017, 0.0869, 0.0855, 0.5042, 0.0966, 0.0766],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1228, 0.0143, 0.1507, 0.1325, 0.1225, 0.3324, 0.1249],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1400, 0.1246, 0.1204, 0.1212, 0.1140, 0.1496, 0.2302],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  60 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.027019999999999874 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8119537
maxi score, test score, baseline:  0.027019999999999874 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.027019999999999874 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  0.027019999999999874 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.005]
 [ 0.029]
 [-0.005]
 [-0.005]
 [-0.006]
 [-0.007]
 [-0.005]] [[36.141]
 [46.81 ]
 [36.141]
 [36.141]
 [50.324]
 [50.829]
 [36.141]] [[0.629]
 [1.003]
 [0.629]
 [0.629]
 [1.079]
 [1.094]
 [0.629]]
printing an ep nov before normalisation:  41.925128015916584
maxi score, test score, baseline:  0.027019999999999874 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.1722, 0.1636, 0.1340, 0.1375, 0.1364, 0.1446, 0.1116],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0201, 0.8414, 0.0168, 0.0259, 0.0095, 0.0130, 0.0732],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0998, 0.0994, 0.3916, 0.1149, 0.0809, 0.1420, 0.0714],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1386, 0.0356, 0.1208, 0.2786, 0.1285, 0.1145, 0.1834],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0952, 0.1811, 0.1012, 0.1116, 0.3547, 0.0908, 0.0654],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1673, 0.0167, 0.1295, 0.2508, 0.1246, 0.1600, 0.1511],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0960, 0.1024, 0.0880, 0.0825, 0.1012, 0.1229, 0.4069],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  37.25490093231201
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
printing an ep nov before normalisation:  47.109775510640326
maxi score, test score, baseline:  0.027019999999999874 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.027019999999999874 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  33 total reward:  0.5333333333333333  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.030086666666666546 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.18846888434429
Printing some Q and Qe and total Qs values:  [[-0.015]
 [ 0.004]
 [-0.012]
 [-0.011]
 [-0.022]
 [-0.012]
 [-0.007]] [[28.419]
 [30.054]
 [20.954]
 [22.772]
 [22.901]
 [20.954]
 [29.562]] [[0.187]
 [0.222]
 [0.122]
 [0.14 ]
 [0.129]
 [0.122]
 [0.206]]
maxi score, test score, baseline:  0.030086666666666546 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.030086666666666546 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  30.02226158671575
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  67 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.030086666666666546 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  43.82383517196405
maxi score, test score, baseline:  0.030086666666666546 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.030086666666666546 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  0.030086666666666546 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.030086666666666546 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.030086666666666546 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.030086666666666546 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8098266
printing an ep nov before normalisation:  41.60977629923349
printing an ep nov before normalisation:  34.70447977200683
maxi score, test score, baseline:  0.030086666666666546 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  8.00291161340283e-06
actor:  1 policy actor:  1  step number:  84 total reward:  0.12666666666666515  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.030086666666666546 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.4933333333333332  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.030086666666666546 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  0.030086666666666546 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  0.030086666666666546 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.030086666666666546 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.030086666666666546 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  38 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.030966666666666552 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  43 total reward:  0.3999999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.03376666666666656 0.6986666666666668 0.6986666666666668
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[ 0.008]
 [ 0.044]
 [ 0.005]
 [ 0.014]
 [ 0.004]
 [-0.   ]
 [ 0.005]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.008]
 [ 0.044]
 [ 0.005]
 [ 0.014]
 [ 0.004]
 [-0.   ]
 [ 0.005]]
maxi score, test score, baseline:  0.03376666666666656 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.377578514663455
maxi score, test score, baseline:  0.03376666666666656 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.71421683162598
printing an ep nov before normalisation:  16.548490524291992
maxi score, test score, baseline:  0.03376666666666656 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03376666666666656 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03376666666666656 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  49.792945696376115
using explorer policy with actor:  1
siam score:  -0.7950937
printing an ep nov before normalisation:  47.136278783852845
maxi score, test score, baseline:  0.03376666666666656 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  0.03376666666666656 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.820359326718794
actor:  0 policy actor:  0  step number:  64 total reward:  0.15333333333333277  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.771]
 [0.   ]
 [0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]] [[15.829]
 [ 0.   ]
 [15.829]
 [15.829]
 [15.829]
 [15.829]
 [15.829]] [[1.499]
 [0.   ]
 [1.499]
 [1.499]
 [1.499]
 [1.499]
 [1.499]]
printing an ep nov before normalisation:  35.51111409327597
actor:  1 policy actor:  1  step number:  66 total reward:  0.2733333333333322  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  51 total reward:  0.3599999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.03607333333333322 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.856252249313606
printing an ep nov before normalisation:  32.00479742212711
maxi score, test score, baseline:  0.03607333333333322 0.6986666666666668 0.6986666666666668
Printing some Q and Qe and total Qs values:  [[0.185]
 [0.237]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]] [[45.693]
 [44.134]
 [45.693]
 [45.693]
 [45.693]
 [45.693]
 [45.693]] [[0.518]
 [0.548]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]]
printing an ep nov before normalisation:  45.06370544433594
maxi score, test score, baseline:  0.03607333333333322 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]] [[37.812]
 [37.812]
 [37.812]
 [37.812]
 [37.812]
 [37.812]
 [37.812]] [[12.789]
 [12.789]
 [12.789]
 [12.789]
 [12.789]
 [12.789]
 [12.789]]
Printing some Q and Qe and total Qs values:  [[-0.004]
 [-0.006]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]] [[23.305]
 [37.299]
 [23.305]
 [23.305]
 [23.305]
 [23.305]
 [23.305]] [[0.755]
 [1.369]
 [0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]]
printing an ep nov before normalisation:  54.19339958458387
printing an ep nov before normalisation:  36.80318014766864
maxi score, test score, baseline:  0.03607333333333322 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03607333333333322 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03607333333333322 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.2889090913174
printing an ep nov before normalisation:  48.50033174950461
actions average: 
K:  4  action  0 :  tensor([0.3230, 0.0258, 0.1357, 0.1363, 0.0995, 0.1312, 0.1484],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0016, 0.9740, 0.0013, 0.0032, 0.0010, 0.0013, 0.0176],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0977, 0.1148, 0.3644, 0.1032, 0.0954, 0.1173, 0.1073],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0961, 0.0989, 0.1071, 0.3449, 0.0907, 0.1346, 0.1276],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0630, 0.3284, 0.0783, 0.1038, 0.2168, 0.1037, 0.1061],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1301, 0.1857, 0.1473, 0.1307, 0.1079, 0.1639, 0.1343],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0922, 0.1641, 0.1503, 0.0999, 0.0793, 0.1510, 0.2632],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.03607333333333322 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.16796255015736
printing an ep nov before normalisation:  38.549279980652834
siam score:  -0.8040622
maxi score, test score, baseline:  0.03607333333333322 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  0.03607333333333322 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03607333333333322 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  0.03607333333333322 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.877729310572605
maxi score, test score, baseline:  0.03607333333333322 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.42335040542182
siam score:  -0.8040401
maxi score, test score, baseline:  0.03607333333333322 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.638]
 [0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]] [[44.933]
 [45.617]
 [44.933]
 [44.933]
 [44.933]
 [44.933]
 [44.933]] [[2.087]
 [2.216]
 [2.087]
 [2.087]
 [2.087]
 [2.087]
 [2.087]]
maxi score, test score, baseline:  0.03607333333333322 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.44011087616978
actor:  1 policy actor:  1  step number:  54 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.741]
 [0.815]
 [0.766]
 [0.766]
 [0.743]
 [0.766]
 [0.766]] [[35.881]
 [36.322]
 [35.992]
 [35.992]
 [36.146]
 [35.992]
 [35.992]] [[0.741]
 [0.815]
 [0.766]
 [0.766]
 [0.743]
 [0.766]
 [0.766]]
printing an ep nov before normalisation:  41.71764474773306
maxi score, test score, baseline:  0.03607333333333322 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  36.97016477584839
maxi score, test score, baseline:  0.033459999999999886 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.033459999999999886 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.033459999999999886 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  0.033459999999999886 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.587302207946777
maxi score, test score, baseline:  0.033459999999999886 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.031059999999999883 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.031059999999999883 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.586666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.031059999999999883 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  0.031059999999999883 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  0.031059999999999883 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.031059999999999883 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  57 total reward:  0.26666666666666594  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.031059999999999883 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.031059999999999883 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  0.031059999999999883 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  52 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  60 total reward:  0.23333333333333306  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  44.972138972211546
maxi score, test score, baseline:  0.03347333333333321 0.6986666666666668 0.6986666666666668
actor:  0 policy actor:  0  step number:  36 total reward:  0.5  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.036473333333333226 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.036473333333333226 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.611008495253984
maxi score, test score, baseline:  0.036473333333333226 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.69094490676409
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
maxi score, test score, baseline:  0.036473333333333226 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  46.636286085153806
maxi score, test score, baseline:  0.036473333333333226 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.135054996756764
printing an ep nov before normalisation:  34.31429723163415
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.648]
 [0.528]
 [0.515]
 [0.394]
 [0.416]
 [0.592]] [[35.443]
 [35.448]
 [35.938]
 [36.823]
 [36.676]
 [38.684]
 [36.025]] [[1.778]
 [1.944]
 [1.86 ]
 [1.913]
 [1.781]
 [1.95 ]
 [1.931]]
Starting evaluation
maxi score, test score, baseline:  0.036473333333333226 0.6986666666666668 0.6986666666666668
Printing some Q and Qe and total Qs values:  [[0.806]
 [0.85 ]
 [0.806]
 [0.806]
 [0.806]
 [0.806]
 [0.806]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.806]
 [0.85 ]
 [0.806]
 [0.806]
 [0.806]
 [0.806]
 [0.806]]
printing an ep nov before normalisation:  29.999847386232812
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.343]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.386]
 [0.343]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]]
Printing some Q and Qe and total Qs values:  [[0.767]
 [0.813]
 [0.767]
 [0.767]
 [0.767]
 [0.767]
 [0.767]] [[35.969]
 [37.625]
 [35.969]
 [35.969]
 [35.969]
 [35.969]
 [35.969]] [[0.767]
 [0.813]
 [0.767]
 [0.767]
 [0.767]
 [0.767]
 [0.767]]
Printing some Q and Qe and total Qs values:  [[0.776]
 [0.776]
 [0.776]
 [0.776]
 [0.776]
 [0.776]
 [0.776]] [[45.207]
 [45.207]
 [45.207]
 [45.207]
 [45.207]
 [45.207]
 [45.207]] [[0.776]
 [0.776]
 [0.776]
 [0.776]
 [0.776]
 [0.776]
 [0.776]]
maxi score, test score, baseline:  0.036473333333333226 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.827]
 [0.765]
 [0.765]
 [0.765]
 [0.78 ]
 [0.765]
 [0.765]] [[39.515]
 [34.917]
 [34.917]
 [34.917]
 [40.791]
 [34.917]
 [34.917]] [[0.827]
 [0.765]
 [0.765]
 [0.765]
 [0.78 ]
 [0.765]
 [0.765]]
printing an ep nov before normalisation:  38.166091297206435
printing an ep nov before normalisation:  31.52291533665874
printing an ep nov before normalisation:  43.005977408129645
maxi score, test score, baseline:  0.036473333333333226 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.859]
 [0.875]
 [0.859]
 [0.859]
 [0.859]
 [0.859]
 [0.838]] [[37.488]
 [38.414]
 [37.488]
 [37.488]
 [37.488]
 [37.488]
 [39.768]] [[0.859]
 [0.875]
 [0.859]
 [0.859]
 [0.859]
 [0.859]
 [0.838]]
printing an ep nov before normalisation:  37.468092892189226
Printing some Q and Qe and total Qs values:  [[0.75 ]
 [0.8  ]
 [0.75 ]
 [0.75 ]
 [0.75 ]
 [0.75 ]
 [0.718]] [[51.09 ]
 [36.437]
 [51.09 ]
 [51.09 ]
 [51.09 ]
 [51.09 ]
 [45.28 ]] [[0.75 ]
 [0.8  ]
 [0.75 ]
 [0.75 ]
 [0.75 ]
 [0.75 ]
 [0.718]]
Printing some Q and Qe and total Qs values:  [[0.705]
 [0.684]
 [0.684]
 [0.684]
 [0.676]
 [0.684]
 [0.684]] [[40.442]
 [43.581]
 [43.581]
 [43.581]
 [39.339]
 [43.581]
 [43.581]] [[0.705]
 [0.684]
 [0.684]
 [0.684]
 [0.676]
 [0.684]
 [0.684]]
actor:  1 policy actor:  1  step number:  38 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.036473333333333226 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.66160929437933
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [-0.009]
 [-0.013]
 [-0.011]
 [-0.014]
 [-0.014]
 [-0.011]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.01 ]
 [-0.009]
 [-0.013]
 [-0.011]
 [-0.014]
 [-0.014]
 [-0.011]]
printing an ep nov before normalisation:  34.05949428218385
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  1 policy actor:  1  step number:  59 total reward:  0.25333333333333286  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.05055333333333321 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.05055333333333321 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.311127026875816
maxi score, test score, baseline:  0.05055333333333321 0.7 0.7
maxi score, test score, baseline:  0.05055333333333321 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.05055333333333321 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
actor:  1 policy actor:  1  step number:  42 total reward:  0.646666666666667  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.05055333333333321 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 28.774002226628404
printing an ep nov before normalisation:  43.36692918510738
maxi score, test score, baseline:  0.05055333333333321 0.7 0.7
Printing some Q and Qe and total Qs values:  [[0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]] [[39.29]
 [39.29]
 [39.29]
 [39.29]
 [39.29]
 [39.29]
 [39.29]] [[1.451]
 [1.451]
 [1.451]
 [1.451]
 [1.451]
 [1.451]
 [1.451]]
printing an ep nov before normalisation:  49.37434738587002
siam score:  -0.8009671
maxi score, test score, baseline:  0.05055333333333321 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.05055333333333321 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.05055333333333321 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.42402629754781
maxi score, test score, baseline:  0.04724666666666655 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.061 0.143 0.204 0.224 0.041 0.143 0.184]
actor:  1 policy actor:  1  step number:  57 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  1.333
siam score:  -0.80409104
maxi score, test score, baseline:  0.04724666666666655 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.319746118875837
printing an ep nov before normalisation:  29.16051580278656
actor:  0 policy actor:  0  step number:  44 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.046726666666666555 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.043686666666666554 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.043686666666666554 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.076]
 [0.076]
 [0.076]
 [0.076]
 [0.076]
 [0.076]
 [0.076]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.076]
 [0.076]
 [0.076]
 [0.076]
 [0.076]
 [0.076]
 [0.076]]
line 256 mcts: sample exp_bonus 31.235648279846192
maxi score, test score, baseline:  0.043686666666666554 0.7 0.7
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.648]
 [0.612]
 [0.607]
 [0.602]
 [0.6  ]
 [0.586]] [[31.209]
 [36.384]
 [32.171]
 [31.95 ]
 [31.282]
 [31.926]
 [30.8  ]] [[1.781]
 [2.232]
 [1.884]
 [1.863]
 [1.808]
 [1.854]
 [1.756]]
printing an ep nov before normalisation:  43.43445966269967
maxi score, test score, baseline:  0.043686666666666554 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.043686666666666554 0.7 0.7
maxi score, test score, baseline:  0.043686666666666554 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.42562402673068
actor:  1 policy actor:  1  step number:  72 total reward:  0.04666666666666586  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  30.27587073535266
printing an ep nov before normalisation:  28.64044189453125
printing an ep nov before normalisation:  27.73597759032103
actor:  1 policy actor:  1  step number:  42 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.593]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]] [[48.013]
 [44.337]
 [48.013]
 [48.013]
 [48.013]
 [48.013]
 [48.013]] [[1.952]
 [1.913]
 [1.952]
 [1.952]
 [1.952]
 [1.952]
 [1.952]]
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.389]
 [0.268]
 [0.268]
 [0.268]
 [0.268]
 [0.268]] [[43.822]
 [44.645]
 [43.822]
 [43.822]
 [43.822]
 [43.822]
 [43.822]] [[1.418]
 [1.578]
 [1.418]
 [1.418]
 [1.418]
 [1.418]
 [1.418]]
maxi score, test score, baseline:  0.043686666666666554 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7927151
maxi score, test score, baseline:  0.041513333333333215 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.041513333333333215 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.704]
 [0.806]
 [0.698]
 [0.698]
 [0.722]
 [0.722]
 [0.745]] [[26.963]
 [36.826]
 [26.012]
 [25.924]
 [33.679]
 [34.02 ]
 [34.79 ]] [[0.704]
 [0.806]
 [0.698]
 [0.698]
 [0.722]
 [0.722]
 [0.745]]
printing an ep nov before normalisation:  33.27134370803833
maxi score, test score, baseline:  0.041513333333333215 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  0.19999999999999907  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.61 ]
 [0.666]
 [0.594]
 [0.593]
 [0.597]
 [0.542]
 [0.601]] [[11.46 ]
 [10.296]
 [11.483]
 [11.318]
 [11.295]
 [10.625]
 [11.562]] [[1.901]
 [1.825]
 [1.888]
 [1.867]
 [1.87 ]
 [1.739]
 [1.903]]
actor:  1 policy actor:  1  step number:  57 total reward:  0.4266666666666662  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.06113074204579
printing an ep nov before normalisation:  32.18654028100152
maxi score, test score, baseline:  0.041513333333333215 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.041513333333333215 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 9.19674570099005e-06
actor:  0 policy actor:  0  step number:  52 total reward:  0.17999999999999938  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.70307416174648
maxi score, test score, baseline:  0.04387333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.04387333333333322 0.7 0.7
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  33.547234089494005
printing an ep nov before normalisation:  27.900625039021516
maxi score, test score, baseline:  0.04387333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.046750541630477
line 256 mcts: sample exp_bonus 0.007763478869833307
actor:  1 policy actor:  1  step number:  56 total reward:  0.15333333333333288  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.04387333333333322 0.7 0.7
maxi score, test score, baseline:  0.04387333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  51 total reward:  0.26666666666666605  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  38.378496170043945
siam score:  -0.78946054
Printing some Q and Qe and total Qs values:  [[0.781]
 [0.865]
 [0.746]
 [0.746]
 [0.688]
 [0.746]
 [0.746]] [[30.138]
 [30.883]
 [28.997]
 [28.997]
 [29.872]
 [28.997]
 [28.997]] [[1.418]
 [1.532]
 [1.335]
 [1.335]
 [1.314]
 [1.335]
 [1.335]]
printing an ep nov before normalisation:  47.073060109485525
maxi score, test score, baseline:  0.04640666666666654 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.839]
 [0.929]
 [0.839]
 [0.839]
 [0.839]
 [0.839]
 [0.839]] [[31.969]
 [32.668]
 [31.969]
 [31.969]
 [31.969]
 [31.969]
 [31.969]] [[0.839]
 [0.929]
 [0.839]
 [0.839]
 [0.839]
 [0.839]
 [0.839]]
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.451]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]] [[28.643]
 [32.079]
 [28.643]
 [28.643]
 [28.643]
 [28.643]
 [28.643]] [[0.75 ]
 [0.913]
 [0.75 ]
 [0.75 ]
 [0.75 ]
 [0.75 ]
 [0.75 ]]
maxi score, test score, baseline:  0.04640666666666654 0.7 0.7
actor:  0 policy actor:  0  step number:  53 total reward:  0.27999999999999925  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.04896666666666655 0.7 0.7
actor:  1 policy actor:  1  step number:  42 total reward:  0.54  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.04896666666666655 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.04896666666666655 0.7 0.7
actor:  0 policy actor:  0  step number:  38 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.05188666666666655 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.74763780314245
Printing some Q and Qe and total Qs values:  [[-0.015]
 [-0.056]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.012]
 [-0.015]] [[42.263]
 [35.549]
 [42.263]
 [42.263]
 [42.263]
 [38.64 ]
 [42.263]] [[0.913]
 [0.625]
 [0.913]
 [0.913]
 [0.913]
 [0.783]
 [0.913]]
actions average: 
K:  4  action  0 :  tensor([0.3397, 0.0325, 0.1156, 0.1529, 0.1309, 0.1441, 0.0843],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0032, 0.9328, 0.0029, 0.0128, 0.0011, 0.0018, 0.0453],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1757, 0.0529, 0.1589, 0.1504, 0.1532, 0.1750, 0.1340],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1353, 0.1106, 0.0878, 0.2868, 0.1226, 0.1535, 0.1033],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1161, 0.0074, 0.0783, 0.1502, 0.4538, 0.1161, 0.0781],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1626, 0.1097, 0.1702, 0.1410, 0.0913, 0.1546, 0.1705],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1060, 0.1370, 0.0793, 0.1387, 0.0934, 0.1025, 0.3430],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  62 total reward:  0.16666666666666607  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  67 total reward:  0.31999999999999995  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.05188666666666655 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.05188666666666656 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.05188666666666656 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.05188666666666656 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.6354, 0.0898, 0.0569, 0.0587, 0.0695, 0.0456, 0.0440],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0064, 0.9189, 0.0074, 0.0167, 0.0021, 0.0021, 0.0464],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1579, 0.1281, 0.3432, 0.1115, 0.0886, 0.0796, 0.0911],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1289, 0.2161, 0.0996, 0.2527, 0.0849, 0.0826, 0.1351],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1859, 0.0234, 0.1477, 0.1377, 0.2826, 0.1074, 0.1153],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1649, 0.0098, 0.1333, 0.1629, 0.1090, 0.3448, 0.0753],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1475, 0.0657, 0.1477, 0.2649, 0.1426, 0.1278, 0.1038],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.05188666666666656 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.05188666666666656 0.7 0.7
Printing some Q and Qe and total Qs values:  [[0.214]
 [0.284]
 [0.176]
 [0.214]
 [0.214]
 [0.167]
 [0.214]] [[33.315]
 [35.317]
 [30.855]
 [33.315]
 [33.315]
 [37.576]
 [33.315]] [[0.383]
 [0.474]
 [0.32 ]
 [0.383]
 [0.383]
 [0.379]
 [0.383]]
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.716]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]] [[36.52 ]
 [34.704]
 [36.52 ]
 [36.52 ]
 [36.52 ]
 [36.52 ]
 [36.52 ]] [[1.327]
 [1.398]
 [1.327]
 [1.327]
 [1.327]
 [1.327]
 [1.327]]
printing an ep nov before normalisation:  44.21922218760757
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.427]
 [0.428]
 [0.41 ]
 [0.412]
 [0.413]
 [0.419]] [[37.436]
 [35.871]
 [37.111]
 [38.603]
 [38.165]
 [38.104]
 [37.351]] [[1.26 ]
 [1.207]
 [1.262]
 [1.308]
 [1.291]
 [1.289]
 [1.263]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.27333333333333254  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.05188666666666656 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  39 total reward:  0.52  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.05188666666666656 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.05188666666666656 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  76 total reward:  0.11333333333333206  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  57.42362816063371
actor:  1 policy actor:  1  step number:  50 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.05188666666666656 0.7 0.7
printing an ep nov before normalisation:  32.58753279366687
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.97619362479634
maxi score, test score, baseline:  0.05188666666666656 0.7 0.7
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.05188666666666656 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  38 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  35.94050217469329
maxi score, test score, baseline:  0.05255333333333324 0.7 0.7
maxi score, test score, baseline:  0.05255333333333324 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.05255333333333324 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.05255333333333324 0.7 0.7
printing an ep nov before normalisation:  44.25640759554248
actor:  1 policy actor:  1  step number:  52 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  59.11971158110606
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
maxi score, test score, baseline:  0.05255333333333324 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  68 total reward:  0.21999999999999886  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  52 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
siam score:  -0.7945932
printing an ep nov before normalisation:  39.33301398901165
maxi score, test score, baseline:  0.05255333333333324 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.05255333333333324 0.7 0.7
maxi score, test score, baseline:  0.05255333333333324 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.1189882425401
printing an ep nov before normalisation:  17.90116792529498
actor:  1 policy actor:  1  step number:  59 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.711662390080114
actor:  0 policy actor:  0  step number:  45 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.055273333333333216 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.055273333333333216 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0
printing an ep nov before normalisation:  34.67599808807729
Printing some Q and Qe and total Qs values:  [[0.928]
 [0.928]
 [0.928]
 [0.88 ]
 [0.928]
 [0.928]
 [0.928]] [[43.264]
 [43.264]
 [43.264]
 [37.403]
 [43.264]
 [43.264]
 [43.264]] [[1.245]
 [1.245]
 [1.245]
 [1.122]
 [1.245]
 [1.245]
 [1.245]]
actor:  1 policy actor:  1  step number:  57 total reward:  0.11999999999999922  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
printing an ep nov before normalisation:  27.385902808753265
actor:  1 policy actor:  1  step number:  65 total reward:  0.22666666666666602  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.055273333333333216 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.055273333333333216 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.803653
maxi score, test score, baseline:  0.055273333333333216 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.26366861314762
printing an ep nov before normalisation:  40.68651110741234
maxi score, test score, baseline:  0.055273333333333216 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.055273333333333216 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.055273333333333216 0.7 0.7
maxi score, test score, baseline:  0.055273333333333216 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.056]
 [ 0.037]
 [ 0.05 ]
 [ 0.052]
 [ 0.056]
 [ 0.062]
 [-0.002]] [[29.265]
 [30.641]
 [33.848]
 [37.062]
 [39.611]
 [36.883]
 [34.404]] [[1.221]
 [1.268]
 [1.433]
 [1.587]
 [1.712]
 [1.588]
 [1.407]]
maxi score, test score, baseline:  0.055273333333333216 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.192]
 [0.303]
 [0.192]
 [0.192]
 [0.192]
 [0.192]
 [0.192]] [[34.195]
 [36.224]
 [34.195]
 [34.195]
 [34.195]
 [34.195]
 [34.195]] [[0.4  ]
 [0.534]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.4  ]]
maxi score, test score, baseline:  0.055273333333333216 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.055273333333333216 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.99613666534424
printing an ep nov before normalisation:  38.45400109422356
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
maxi score, test score, baseline:  0.055273333333333216 0.7 0.7
Printing some Q and Qe and total Qs values:  [[0.367]
 [0.42 ]
 [0.367]
 [0.361]
 [0.367]
 [0.36 ]
 [0.367]] [[37.123]
 [34.574]
 [37.123]
 [37.757]
 [37.123]
 [37.68 ]
 [37.123]] [[2.304]
 [2.104]
 [2.304]
 [2.361]
 [2.304]
 [2.352]
 [2.304]]
actor:  1 policy actor:  1  step number:  70 total reward:  0.09999999999999876  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.461]
 [0.473]
 [0.461]
 [0.462]
 [0.485]
 [0.485]
 [0.455]] [[41.783]
 [41.798]
 [40.973]
 [42.859]
 [40.84 ]
 [40.84 ]
 [41.49 ]] [[1.943]
 [1.956]
 [1.886]
 [2.019]
 [1.9  ]
 [1.9  ]
 [1.916]]
maxi score, test score, baseline:  0.055273333333333216 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.4916, 0.0069, 0.0934, 0.0972, 0.1463, 0.0908, 0.0736],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0258, 0.8242, 0.0302, 0.0331, 0.0156, 0.0240, 0.0470],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1690, 0.0139, 0.3060, 0.0946, 0.1348, 0.1463, 0.1354],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1241, 0.0054, 0.0979, 0.3278, 0.1550, 0.1815, 0.1083],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1373, 0.0087, 0.1393, 0.1479, 0.3047, 0.1406, 0.1214],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1125, 0.0197, 0.1720, 0.1128, 0.1047, 0.3703, 0.1080],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0740, 0.4646, 0.0664, 0.0590, 0.0562, 0.0788, 0.2011],
       grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.311]
 [0.405]
 [0.305]
 [0.305]
 [0.292]
 [0.305]
 [0.313]] [[40.306]
 [38.69 ]
 [37.103]
 [37.103]
 [39.987]
 [37.103]
 [38.894]] [[1.088]
 [1.126]
 [0.972]
 [0.972]
 [1.058]
 [0.972]
 [1.041]]
maxi score, test score, baseline:  0.055273333333333216 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.27079155071981
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.05527333333333323 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.29682434696574
maxi score, test score, baseline:  0.05527333333333323 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.70340171051144
actor:  0 policy actor:  0  step number:  58 total reward:  0.16666666666666619  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.05760666666666655 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.05760666666666655 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.05760666666666655 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.05760666666666655 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.6925482654827
maxi score, test score, baseline:  0.05760666666666655 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.02162174933971528
actor:  1 policy actor:  1  step number:  74 total reward:  0.05999999999999872  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.05760666666666655 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.55141482435291
printing an ep nov before normalisation:  39.364836897813234
maxi score, test score, baseline:  0.05483333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.84050661020054
printing an ep nov before normalisation:  31.674329734097775
maxi score, test score, baseline:  0.05483333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  39 total reward:  0.5466666666666666  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.05483333333333322 0.7 0.7
actor:  1 policy actor:  1  step number:  43 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.05483333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.213485552824537
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.244]
 [0.294]
 [0.196]
 [0.201]
 [0.195]
 [0.153]
 [0.244]] [[38.992]
 [32.796]
 [39.826]
 [40.377]
 [40.8  ]
 [43.403]
 [38.992]] [[1.264]
 [1.074]
 [1.248]
 [1.275]
 [1.285]
 [1.344]
 [1.264]]
printing an ep nov before normalisation:  31.296670705660357
line 256 mcts: sample exp_bonus 40.730369115341766
printing an ep nov before normalisation:  36.59880608668555
Printing some Q and Qe and total Qs values:  [[0.446]
 [0.595]
 [0.446]
 [0.446]
 [0.446]
 [0.446]
 [0.446]] [[32.598]
 [31.892]
 [32.598]
 [32.598]
 [32.598]
 [32.598]
 [32.598]] [[1.429]
 [1.535]
 [1.429]
 [1.429]
 [1.429]
 [1.429]
 [1.429]]
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.581]
 [0.452]
 [0.453]
 [0.387]
 [0.389]
 [0.408]] [[33.13 ]
 [36.019]
 [38.231]
 [37.625]
 [32.839]
 [34.852]
 [35.69 ]] [[0.396]
 [0.581]
 [0.452]
 [0.453]
 [0.387]
 [0.389]
 [0.408]]
printing an ep nov before normalisation:  36.98115592918292
printing an ep nov before normalisation:  39.63840892529797
siam score:  -0.7917617
maxi score, test score, baseline:  0.05483333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
printing an ep nov before normalisation:  42.194618094625795
maxi score, test score, baseline:  0.05483333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.33042799670089
printing an ep nov before normalisation:  42.50945041408537
maxi score, test score, baseline:  0.05483333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.05483333333333322 0.7 0.7
maxi score, test score, baseline:  0.05483333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.76202238308829
actor:  1 policy actor:  1  step number:  52 total reward:  0.36666666666666603  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.05483333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.122417547284662
maxi score, test score, baseline:  0.05483333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  55 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  58 total reward:  0.2199999999999993  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.057313333333333216 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [ 0.018]
 [-0.007]
 [-0.011]
 [-0.008]
 [-0.008]
 [-0.009]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.01 ]
 [ 0.018]
 [-0.007]
 [-0.011]
 [-0.008]
 [-0.008]
 [-0.009]]
maxi score, test score, baseline:  0.057313333333333216 0.7 0.7
maxi score, test score, baseline:  0.057313333333333216 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.42393953089029
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.479]
 [0.346]
 [0.344]
 [0.346]
 [0.417]
 [0.348]] [[34.221]
 [35.392]
 [36.137]
 [34.507]
 [34.567]
 [34.221]
 [36.011]] [[1.083]
 [1.19 ]
 [1.087]
 [1.022]
 [1.025]
 [1.083]
 [1.083]]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  33.00387739119004
printing an ep nov before normalisation:  34.59653040562231
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  3.6663681157733663e-06
Printing some Q and Qe and total Qs values:  [[0.709]
 [0.709]
 [0.709]
 [0.646]
 [0.709]
 [0.709]
 [0.709]] [[28.404]
 [28.404]
 [28.404]
 [29.802]
 [28.404]
 [28.404]
 [28.404]] [[1.459]
 [1.459]
 [1.459]
 [1.433]
 [1.459]
 [1.459]
 [1.459]]
actor:  1 policy actor:  1  step number:  60 total reward:  0.28666666666666596  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.05731333333333322 0.7 0.7
maxi score, test score, baseline:  0.05731333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.37851285289879
maxi score, test score, baseline:  0.05731333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.16666666666666607  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.05731333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.692]
 [ 0.731]
 [ 0.692]
 [ 0.692]
 [ 0.692]
 [-0.007]
 [ 0.692]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.692]
 [ 0.731]
 [ 0.692]
 [ 0.692]
 [ 0.692]
 [-0.007]
 [ 0.692]]
Printing some Q and Qe and total Qs values:  [[0.767]
 [0.765]
 [0.693]
 [0.677]
 [0.693]
 [0.693]
 [0.757]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.767]
 [0.765]
 [0.693]
 [0.677]
 [0.693]
 [0.693]
 [0.757]]
siam score:  -0.7958903
maxi score, test score, baseline:  0.05731333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.09006481981752
printing an ep nov before normalisation:  28.223449667780965
Printing some Q and Qe and total Qs values:  [[0.898]
 [0.961]
 [0.898]
 [0.898]
 [0.898]
 [0.898]
 [0.898]] [[39.958]
 [32.42 ]
 [39.958]
 [39.958]
 [39.958]
 [39.958]
 [39.958]] [[0.898]
 [0.961]
 [0.898]
 [0.898]
 [0.898]
 [0.898]
 [0.898]]
actions average: 
K:  4  action  0 :  tensor([0.4572, 0.0101, 0.1243, 0.0851, 0.1083, 0.1178, 0.0971],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0062, 0.9187, 0.0064, 0.0269, 0.0043, 0.0057, 0.0317],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0976, 0.1150, 0.2628, 0.1290, 0.0851, 0.2195, 0.0911],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0858, 0.0251, 0.1104, 0.4426, 0.1109, 0.1278, 0.0973],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1112, 0.0026, 0.1024, 0.1195, 0.4337, 0.1231, 0.1074],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1412, 0.0209, 0.1868, 0.1437, 0.1387, 0.2640, 0.1047],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0931, 0.2491, 0.0840, 0.1351, 0.1193, 0.1141, 0.2052],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  35.98943669422463
maxi score, test score, baseline:  0.05731333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.60888953116543
actor:  0 policy actor:  0  step number:  54 total reward:  0.16666666666666607  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  46 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  21.960751881889266
siam score:  -0.790701
maxi score, test score, baseline:  0.062459999999999884 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.18046188354492
siam score:  -0.7905317
printing an ep nov before normalisation:  19.70406223442322
using explorer policy with actor:  1
maxi score, test score, baseline:  0.062459999999999884 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.57288848835524
siam score:  -0.7885969
printing an ep nov before normalisation:  41.71832037484257
actor:  1 policy actor:  1  step number:  42 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  1.333
siam score:  -0.7914112
printing an ep nov before normalisation:  35.68043878636265
printing an ep nov before normalisation:  35.21546618064092
Printing some Q and Qe and total Qs values:  [[ 0.015]
 [ 0.001]
 [-0.02 ]
 [ 0.275]
 [ 0.48 ]
 [ 0.105]
 [ 0.118]] [[34.523]
 [40.317]
 [44.679]
 [42.001]
 [39.096]
 [46.029]
 [41.599]] [[0.927]
 [1.203]
 [1.401]
 [1.561]
 [1.621]
 [1.594]
 [1.385]]
maxi score, test score, baseline:  0.062459999999999884 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.41469181535906
maxi score, test score, baseline:  0.062459999999999884 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  44.21013061176025
maxi score, test score, baseline:  0.062459999999999884 0.7 0.7
printing an ep nov before normalisation:  35.26445680205168
printing an ep nov before normalisation:  40.61885404487717
printing an ep nov before normalisation:  45.715084075927734
actor:  1 policy actor:  1  step number:  45 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  40 total reward:  0.5  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  42.94412825679132
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
printing an ep nov before normalisation:  41.25513874805635
printing an ep nov before normalisation:  38.590666866157235
actor:  1 policy actor:  1  step number:  45 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  31.169138858596387
Printing some Q and Qe and total Qs values:  [[0.489]
 [0.573]
 [0.507]
 [0.507]
 [0.436]
 [0.542]
 [0.507]] [[38.535]
 [40.439]
 [37.611]
 [37.611]
 [38.899]
 [36.213]
 [37.611]] [[1.864]
 [2.085]
 [1.816]
 [1.816]
 [1.838]
 [1.751]
 [1.816]]
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]] [[33.675]
 [33.675]
 [33.675]
 [33.675]
 [33.675]
 [33.675]
 [33.675]] [[2.544]
 [2.544]
 [2.544]
 [2.544]
 [2.544]
 [2.544]
 [2.544]]
maxi score, test score, baseline:  0.062459999999999884 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  0.24666666666666648  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  29.67156879320962
printing an ep nov before normalisation:  13.389467834886482
actor:  1 policy actor:  1  step number:  60 total reward:  0.36666666666666603  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.   ]
 [ 0.184]
 [-0.   ]
 [-0.018]
 [-0.   ]
 [-0.   ]
 [-0.   ]] [[44.557]
 [46.623]
 [44.557]
 [45.209]
 [44.557]
 [44.557]
 [44.557]] [[1.082]
 [1.35 ]
 [1.082]
 [1.091]
 [1.082]
 [1.082]
 [1.082]]
actor:  0 policy actor:  0  step number:  58 total reward:  0.12666666666666604  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.281]
 [0.347]
 [0.281]
 [0.287]
 [0.281]
 [0.263]
 [0.291]] [[48.558]
 [44.688]
 [48.558]
 [43.167]
 [48.558]
 [39.882]
 [48.323]] [[1.506]
 [1.472]
 [1.506]
 [1.373]
 [1.506]
 [1.264]
 [1.51 ]]
printing an ep nov before normalisation:  36.48770268064597
maxi score, test score, baseline:  0.06471333333333323 0.7 0.7
maxi score, test score, baseline:  0.06471333333333323 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.211457961333856
using explorer policy with actor:  1
maxi score, test score, baseline:  0.06471333333333323 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.06239333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.12 ]
 [0.314]
 [0.115]
 [0.115]
 [0.115]
 [0.115]
 [0.115]] [[32.683]
 [37.75 ]
 [34.858]
 [34.858]
 [34.858]
 [34.858]
 [34.858]] [[0.405]
 [0.691]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]]
printing an ep nov before normalisation:  47.73439218973251
actor:  1 policy actor:  1  step number:  48 total reward:  0.4066666666666662  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.06239333333333322 0.7 0.7
maxi score, test score, baseline:  0.06239333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.5382, 0.0691, 0.0680, 0.0708, 0.0782, 0.1072, 0.0686],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0198, 0.8087, 0.0197, 0.0541, 0.0198, 0.0287, 0.0491],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0600, 0.0038, 0.5561, 0.2200, 0.0521, 0.0492, 0.0588],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1421, 0.0071, 0.1418, 0.2055, 0.2103, 0.1528, 0.1404],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1390, 0.0035, 0.0656, 0.1061, 0.4616, 0.1029, 0.1212],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1274, 0.1567, 0.1189, 0.1479, 0.1344, 0.1767, 0.1381],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1222, 0.1233, 0.1561, 0.1778, 0.1383, 0.1487, 0.1336],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.06239333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.74601267285789
maxi score, test score, baseline:  0.06239333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.06239333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.06239333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.437227723610064
actor:  1 policy actor:  1  step number:  72 total reward:  0.05999999999999939  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.06239333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  0.23333333333333262  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.061]
 [0.212]
 [0.061]
 [0.061]
 [0.061]
 [0.061]
 [0.061]] [[31.26 ]
 [34.776]
 [31.26 ]
 [31.26 ]
 [31.26 ]
 [31.26 ]
 [31.26 ]] [[0.404]
 [0.631]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]]
printing an ep nov before normalisation:  35.57241258557672
Printing some Q and Qe and total Qs values:  [[-0.021]
 [ 0.234]
 [-0.008]
 [-0.025]
 [-0.021]
 [ 0.008]
 [-0.018]] [[50.856]
 [49.408]
 [48.915]
 [50.617]
 [50.856]
 [51.677]
 [50.037]] [[0.455]
 [0.688]
 [0.439]
 [0.447]
 [0.455]
 [0.496]
 [0.446]]
printing an ep nov before normalisation:  24.644980430603027
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.53 ]
 [0.352]
 [0.348]
 [0.445]
 [0.445]
 [0.445]] [[40.   ]
 [39.049]
 [40.386]
 [41.105]
 [40.966]
 [40.966]
 [40.966]] [[0.986]
 [1.021]
 [0.878]
 [0.892]
 [0.986]
 [0.986]
 [0.986]]
printing an ep nov before normalisation:  33.57716928479893
maxi score, test score, baseline:  0.06239333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  68 total reward:  0.2866666666666656  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  40.318580284511256
maxi score, test score, baseline:  0.06239333333333322 0.7 0.7
siam score:  -0.7944783
maxi score, test score, baseline:  0.06239333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  50 total reward:  0.4600000000000002  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.83729368238243
actor:  1 policy actor:  1  step number:  42 total reward:  0.5  reward:  1.0 rdn_beta:  2.0
siam score:  -0.7907355
maxi score, test score, baseline:  0.06239333333333322 0.7 0.7
Printing some Q and Qe and total Qs values:  [[-0.01]
 [-0.01]
 [-0.01]
 [-0.01]
 [-0.01]
 [-0.01]
 [-0.01]] [[35.21]
 [35.21]
 [35.21]
 [35.21]
 [35.21]
 [35.21]
 [35.21]] [[0.853]
 [0.853]
 [0.853]
 [0.853]
 [0.853]
 [0.853]
 [0.853]]
siam score:  -0.78997344
maxi score, test score, baseline:  0.06239333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  25.007858362059167
siam score:  -0.7888451
printing an ep nov before normalisation:  43.78636455138153
printing an ep nov before normalisation:  47.62673214221559
Printing some Q and Qe and total Qs values:  [[0.431]
 [0.458]
 [0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]] [[35.898]
 [38.397]
 [35.898]
 [35.898]
 [35.898]
 [35.898]
 [35.898]] [[0.645]
 [0.701]
 [0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]]
printing an ep nov before normalisation:  37.49742185812209
siam score:  -0.79020095
maxi score, test score, baseline:  0.06239333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7916557
actor:  1 policy actor:  1  step number:  42 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.06239333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.06239333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.06239333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.146858371451195
maxi score, test score, baseline:  0.06239333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.647]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]] [[41.183]
 [45.741]
 [41.183]
 [41.183]
 [41.183]
 [41.183]
 [41.183]] [[1.432]
 [1.787]
 [1.432]
 [1.432]
 [1.432]
 [1.432]
 [1.432]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.44000000000000006  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  57 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  51 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  31.297649540342817
maxi score, test score, baseline:  0.06476666666666654 0.7 0.7
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.22663252258677
printing an ep nov before normalisation:  37.23304260095463
line 256 mcts: sample exp_bonus 44.181244476379604
maxi score, test score, baseline:  0.06476666666666654 0.7 0.7
maxi score, test score, baseline:  0.06476666666666654 0.7 0.7
line 256 mcts: sample exp_bonus 43.048118376323266
actor:  1 policy actor:  1  step number:  36 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  50 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.06476666666666654 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]]
Printing some Q and Qe and total Qs values:  [[0.322]
 [0.327]
 [0.32 ]
 [0.321]
 [0.32 ]
 [0.316]
 [0.315]] [[55.39 ]
 [48.143]
 [57.881]
 [57.531]
 [58.591]
 [58.83 ]
 [58.005]] [[1.33 ]
 [1.138]
 [1.396]
 [1.387]
 [1.415]
 [1.417]
 [1.394]]
printing an ep nov before normalisation:  35.891672657150316
maxi score, test score, baseline:  0.06476666666666654 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.15323155166259
maxi score, test score, baseline:  0.06476666666666654 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  57 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.179]
 [0.171]
 [0.171]
 [0.162]
 [0.142]
 [0.171]] [[44.076]
 [33.791]
 [42.496]
 [42.496]
 [47.764]
 [47.278]
 [42.496]] [[0.383]
 [0.366]
 [0.449]
 [0.449]
 [0.495]
 [0.47 ]
 [0.449]]
maxi score, test score, baseline:  0.06476666666666654 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  67 total reward:  0.10666666666666547  reward:  1.0 rdn_beta:  1.0
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.06420666666666655 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.06420666666666655 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.06420666666666655 0.7 0.7
maxi score, test score, baseline:  0.06420666666666655 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.97216378494975
printing an ep nov before normalisation:  0.0003082902708229085
actor:  1 policy actor:  1  step number:  62 total reward:  0.15333333333333288  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  25.284844232076722
Printing some Q and Qe and total Qs values:  [[0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.229]] [[38.547]
 [38.547]
 [38.547]
 [38.547]
 [38.547]
 [38.547]
 [38.547]] [[0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.229]]
maxi score, test score, baseline:  0.06420666666666655 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.18654715758538
maxi score, test score, baseline:  0.06420666666666656 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.06420666666666656 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.11863354928796
maxi score, test score, baseline:  0.06420666666666656 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.43996883827367
actor:  1 policy actor:  1  step number:  43 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.06420666666666656 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.54301590511261
maxi score, test score, baseline:  0.06420666666666656 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.335]
 [0.449]
 [0.448]
 [0.437]
 [0.39 ]
 [0.482]
 [0.404]] [[57.678]
 [54.243]
 [55.31 ]
 [54.803]
 [53.654]
 [47.643]
 [55.671]] [[1.862]
 [1.805]
 [1.857]
 [1.821]
 [1.717]
 [1.509]
 [1.832]]
printing an ep nov before normalisation:  49.69947040037236
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.288]
 [0.255]
 [0.2  ]
 [0.2  ]
 [0.167]
 [0.09 ]
 [0.173]] [[49.035]
 [47.161]
 [42.039]
 [42.039]
 [45.734]
 [43.168]
 [45.881]] [[0.509]
 [0.462]
 [0.369]
 [0.369]
 [0.364]
 [0.267]
 [0.371]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.376505092608376
actor:  1 policy actor:  1  step number:  45 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  0.1095715638300021
maxi score, test score, baseline:  0.06420666666666656 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.06420666666666656 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.29292800389919
maxi score, test score, baseline:  0.06420666666666656 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.57982842500463
maxi score, test score, baseline:  0.06420666666666656 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.06420666666666656 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.061646666666666544 0.7 0.7
maxi score, test score, baseline:  0.061646666666666544 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.061646666666666544 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.061646666666666544 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.061646666666666544 0.7 0.7
printing an ep nov before normalisation:  63.76027307466497
Printing some Q and Qe and total Qs values:  [[0.31 ]
 [0.381]
 [0.31 ]
 [0.31 ]
 [0.31 ]
 [0.31 ]
 [0.31 ]] [[43.859]
 [43.27 ]
 [43.859]
 [43.859]
 [43.859]
 [43.859]
 [43.859]] [[0.834]
 [0.893]
 [0.834]
 [0.834]
 [0.834]
 [0.834]
 [0.834]]
siam score:  -0.7888962
maxi score, test score, baseline:  0.061646666666666544 0.7 0.7
maxi score, test score, baseline:  0.061646666666666544 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.14572651054848
printing an ep nov before normalisation:  35.69555044174194
printing an ep nov before normalisation:  43.48493174590126
maxi score, test score, baseline:  0.061646666666666544 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.395448380645234
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.564]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]] [[32.993]
 [34.781]
 [32.993]
 [32.993]
 [32.993]
 [32.993]
 [32.993]] [[1.261]
 [1.418]
 [1.261]
 [1.261]
 [1.261]
 [1.261]
 [1.261]]
actor:  1 policy actor:  1  step number:  60 total reward:  0.3133333333333328  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  38.434680371713895
maxi score, test score, baseline:  0.061646666666666544 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.607260704040527
printing an ep nov before normalisation:  24.219199393830483
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.208]
 [0.315]
 [0.208]
 [0.208]
 [0.178]
 [0.208]
 [0.208]] [[38.498]
 [45.733]
 [38.498]
 [38.498]
 [42.349]
 [38.498]
 [38.498]] [[1.32 ]
 [1.834]
 [1.32 ]
 [1.32 ]
 [1.506]
 [1.32 ]
 [1.32 ]]
printing an ep nov before normalisation:  32.104054537801446
actor:  1 policy actor:  1  step number:  51 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  56 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.06181999999999989 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.06181999999999989 0.7 0.7
maxi score, test score, baseline:  0.06181999999999989 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.685391534695505
actor:  1 policy actor:  1  step number:  38 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.06181999999999989 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.789055688112235
printing an ep nov before normalisation:  57.832275728984406
printing an ep nov before normalisation:  43.01376610512991
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.323598353946224
printing an ep nov before normalisation:  52.25928723267751
printing an ep nov before normalisation:  36.83777057499891
maxi score, test score, baseline:  0.06181999999999989 0.7 0.7
maxi score, test score, baseline:  0.06181999999999989 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.06181999999999989 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.8104841675281
actor:  1 policy actor:  1  step number:  56 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.06181999999999989 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  16.80262012193131
maxi score, test score, baseline:  0.06181999999999989 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  15.486884117126465
printing an ep nov before normalisation:  43.212210330588135
actor:  1 policy actor:  1  step number:  63 total reward:  0.11999999999999944  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  51 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.06181999999999989 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.06181999999999989 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.06181999999999989 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.44965367705328
printing an ep nov before normalisation:  55.931434839901755
siam score:  -0.77360964
Printing some Q and Qe and total Qs values:  [[0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.707]
 [0.73 ]
 [0.707]] [[36.024]
 [21.002]
 [40.628]
 [38.524]
 [39.343]
 [38.699]
 [39.343]] [[1.848]
 [1.382]
 [1.991]
 [1.926]
 [1.929]
 [1.931]
 [1.929]]
actions average: 
K:  1  action  0 :  tensor([0.4440, 0.0508, 0.1062, 0.0636, 0.1636, 0.0749, 0.0969],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0084, 0.9476, 0.0079, 0.0075, 0.0021, 0.0028, 0.0236],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1130, 0.0032, 0.4598, 0.0945, 0.0927, 0.1464, 0.0903],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0913, 0.0065, 0.1044, 0.3766, 0.1511, 0.1724, 0.0977],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1342, 0.0064, 0.1445, 0.1227, 0.3458, 0.1362, 0.1102],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1076, 0.0875, 0.1574, 0.0911, 0.0785, 0.3872, 0.0906],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1024, 0.1964, 0.1029, 0.1096, 0.0986, 0.1122, 0.2779],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  50.35456204993517
maxi score, test score, baseline:  0.06181999999999989 0.7 0.7
actor:  1 policy actor:  1  step number:  54 total reward:  0.4066666666666662  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  43.76149128139096
maxi score, test score, baseline:  0.06181999999999989 0.7 0.7
actor:  1 policy actor:  1  step number:  43 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  45.58788631068416
printing an ep nov before normalisation:  56.254511525229816
maxi score, test score, baseline:  0.06181999999999989 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.29333333333333267  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 53.00835074121089
maxi score, test score, baseline:  0.06181999999999989 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.06181999999999989 0.7 0.7
maxi score, test score, baseline:  0.06181999999999989 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  40 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.06473999999999988 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  43 total reward:  0.52  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  56.51067088795342
maxi score, test score, baseline:  0.06473999999999988 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.52 ]
 [0.512]
 [0.511]
 [0.487]
 [0.498]
 [0.484]
 [0.487]] [[42.4  ]
 [44.473]
 [43.342]
 [42.96 ]
 [43.006]
 [42.6  ]
 [45.06 ]] [[1.159]
 [1.216]
 [1.179]
 [1.144]
 [1.157]
 [1.13 ]
 [1.208]]
Printing some Q and Qe and total Qs values:  [[0.278]
 [0.258]
 [0.278]
 [0.278]
 [0.142]
 [0.278]
 [0.278]] [[36.598]
 [33.673]
 [36.598]
 [36.598]
 [46.764]
 [36.598]
 [36.598]] [[1.159]
 [1.023]
 [1.159]
 [1.159]
 [1.428]
 [1.159]
 [1.159]]
actor:  1 policy actor:  1  step number:  42 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  32.38007223720729
actions average: 
K:  4  action  0 :  tensor([0.4542, 0.0358, 0.0917, 0.1269, 0.0984, 0.1136, 0.0794],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0087, 0.9146, 0.0171, 0.0090, 0.0035, 0.0050, 0.0421],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1320, 0.0723, 0.3694, 0.0911, 0.0816, 0.1573, 0.0963],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2707, 0.0194, 0.0318, 0.3454, 0.0745, 0.0567, 0.2015],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1277, 0.0862, 0.0552, 0.0837, 0.5055, 0.0781, 0.0635],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1561, 0.0550, 0.1729, 0.1681, 0.1301, 0.1822, 0.1355],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1213, 0.1098, 0.0944, 0.1477, 0.1302, 0.1420, 0.2547],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  72 total reward:  0.13999999999999924  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  62.18404128371043
actor:  0 policy actor:  0  step number:  31 total reward:  0.56  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.06572666666666656 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.276661018774696
actions average: 
K:  0  action  0 :  tensor([0.5273, 0.0236, 0.0789, 0.0530, 0.1918, 0.0612, 0.0641],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0030, 0.9616, 0.0041, 0.0053, 0.0018, 0.0026, 0.0216],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1024, 0.0028, 0.5426, 0.0771, 0.0765, 0.1106, 0.0881],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1610, 0.0698, 0.1589, 0.2029, 0.1155, 0.1424, 0.1495],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1064, 0.0080, 0.0994, 0.1302, 0.3979, 0.1693, 0.0889],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1521, 0.0179, 0.1486, 0.0987, 0.0999, 0.3552, 0.1277],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1736, 0.0079, 0.1533, 0.1168, 0.1187, 0.1382, 0.2915],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  53.98580511707077
maxi score, test score, baseline:  0.06345999999999989 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.5371, 0.0147, 0.0988, 0.0959, 0.0986, 0.0967, 0.0581],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0216, 0.9030, 0.0168, 0.0135, 0.0072, 0.0092, 0.0288],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1067, 0.0270, 0.2204, 0.1148, 0.0755, 0.3246, 0.1311],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1559, 0.0933, 0.1266, 0.2435, 0.1089, 0.1142, 0.1576],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1670, 0.0030, 0.1458, 0.1312, 0.3267, 0.1348, 0.0915],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1183, 0.0355, 0.2773, 0.1501, 0.0875, 0.2248, 0.1065],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0953, 0.4503, 0.0957, 0.0843, 0.0717, 0.0859, 0.1169],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  56 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  23.166478849944244
maxi score, test score, baseline:  0.06345999999999989 0.7 0.7
printing an ep nov before normalisation:  30.673428605934365
actor:  1 policy actor:  1  step number:  51 total reward:  0.3199999999999994  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.06345999999999989 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.06345999999999989 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  0.11999999999999933  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]] [[55.699]
 [55.699]
 [55.699]
 [55.699]
 [55.699]
 [55.699]
 [55.699]] [[2.369]
 [2.369]
 [2.369]
 [2.369]
 [2.369]
 [2.369]
 [2.369]]
printing an ep nov before normalisation:  42.59790897369385
maxi score, test score, baseline:  0.06345999999999989 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.703]
 [0.   ]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]] [[34.502]
 [ 0.013]
 [34.502]
 [34.502]
 [34.502]
 [34.502]
 [34.502]] [[2.465]
 [0.001]
 [2.465]
 [2.465]
 [2.465]
 [2.465]
 [2.465]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  44 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  2.0
siam score:  -0.7746181
printing an ep nov before normalisation:  35.52360232915687
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0972],
        [-0.0000],
        [ 0.0079],
        [ 0.0507],
        [-0.0000],
        [ 0.7020],
        [-0.0278],
        [ 0.0179],
        [ 0.6052],
        [-0.0000]], dtype=torch.float64)
-0.032346567066 -0.12953923855155766
0.957165 0.957165
-0.070771701198 -0.062847593841237
-0.083839701198 -0.03315822852379107
-0.93815238 -0.93815238
-0.084359833866 0.617631979167531
-0.032346567066 -0.06010052676122937
-0.045026434398 -0.02710651599032369
-0.084359833866 0.5208843926290673
-0.7854000000000001 -0.7854000000000001
printing an ep nov before normalisation:  47.369725686635206
actions average: 
K:  1  action  0 :  tensor([0.4990, 0.0329, 0.0738, 0.0605, 0.1896, 0.0732, 0.0710],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0077, 0.9412, 0.0063, 0.0111, 0.0092, 0.0080, 0.0165],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0654, 0.0067, 0.5601, 0.0608, 0.0611, 0.1625, 0.0835],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0891, 0.1613, 0.0843, 0.1935, 0.0923, 0.0785, 0.3011],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2050, 0.0086, 0.1166, 0.1266, 0.2947, 0.1299, 0.1185],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1302, 0.0119, 0.1295, 0.1370, 0.1027, 0.3821, 0.1066],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0899, 0.2245, 0.0489, 0.0495, 0.0716, 0.0797, 0.4358],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  46 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.492]
 [0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]] [[36.652]
 [43.405]
 [36.652]
 [36.652]
 [36.652]
 [36.652]
 [36.652]] [[0.647]
 [0.782]
 [0.647]
 [0.647]
 [0.647]
 [0.647]
 [0.647]]
maxi score, test score, baseline:  0.06317999999999989 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.7012, 0.0040, 0.0625, 0.0445, 0.0902, 0.0513, 0.0464],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0249, 0.8210, 0.0191, 0.0586, 0.0173, 0.0168, 0.0423],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1525, 0.0591, 0.3223, 0.1014, 0.1003, 0.1423, 0.1221],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1038, 0.0865, 0.1393, 0.1781, 0.1412, 0.1511, 0.2001],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1801, 0.0012, 0.1828, 0.1765, 0.1697, 0.1501, 0.1395],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1182, 0.0211, 0.1786, 0.1479, 0.1256, 0.2786, 0.1300],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0719, 0.0887, 0.0720, 0.1484, 0.1292, 0.1082, 0.3816],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  0.324089933124867
actor:  1 policy actor:  1  step number:  59 total reward:  0.2933333333333328  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  61 total reward:  0.13333333333333275  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  44.62259656015192
Printing some Q and Qe and total Qs values:  [[0.29 ]
 [0.369]
 [0.37 ]
 [0.382]
 [0.311]
 [0.343]
 [0.365]] [[42.336]
 [44.531]
 [39.644]
 [44.267]
 [40.47 ]
 [39.649]
 [40.322]] [[1.169]
 [1.329]
 [1.15 ]
 [1.333]
 [1.122]
 [1.123]
 [1.17 ]]
printing an ep nov before normalisation:  43.526764935363154
maxi score, test score, baseline:  0.06317999999999989 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.06317999999999989 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.06317999999999989 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.5600000000000002  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  3  action  0 :  tensor([0.5461, 0.0621, 0.0989, 0.0707, 0.0974, 0.0728, 0.0519],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0136, 0.8952, 0.0155, 0.0313, 0.0067, 0.0083, 0.0293],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1962, 0.0070, 0.2849, 0.1049, 0.1202, 0.1979, 0.0889],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1762, 0.0508, 0.1622, 0.2035, 0.1528, 0.1444, 0.1102],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1189, 0.0067, 0.0560, 0.0663, 0.6145, 0.0758, 0.0618],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1114, 0.0318, 0.1313, 0.1352, 0.1286, 0.3747, 0.0869],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1443, 0.1803, 0.1119, 0.1047, 0.0557, 0.0934, 0.3098],
       grad_fn=<DivBackward0>)
siam score:  -0.77661604
maxi score, test score, baseline:  0.06317999999999989 0.7 0.7
printing an ep nov before normalisation:  35.01416787348974
printing an ep nov before normalisation:  47.968469854240254
maxi score, test score, baseline:  0.06317999999999989 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  45 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  35.53373148037896
actor:  1 policy actor:  1  step number:  68 total reward:  0.15333333333333277  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.321]
 [0.35 ]
 [0.329]
 [0.321]
 [0.321]
 [0.263]
 [0.321]] [[43.076]
 [41.129]
 [37.391]
 [43.076]
 [43.076]
 [32.051]
 [43.076]] [[1.404]
 [1.358]
 [1.191]
 [1.404]
 [1.404]
 [0.917]
 [1.404]]
maxi score, test score, baseline:  0.06333999999999987 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.5444812453714
printing an ep nov before normalisation:  39.055258390151344
maxi score, test score, baseline:  0.06333999999999987 0.7 0.7
actor:  0 policy actor:  0  step number:  42 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.06615333333333323 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.06615333333333323 0.7 0.7
printing an ep nov before normalisation:  44.74339044484729
maxi score, test score, baseline:  0.06615333333333323 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  1.0
siam score:  -0.77930063
maxi score, test score, baseline:  0.06615333333333323 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
actor:  1 policy actor:  1  step number:  51 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  62 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  1.667
siam score:  -0.7769356
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.06615333333333323 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [-0.022]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]] [[36.922]
 [44.11 ]
 [36.922]
 [36.922]
 [36.922]
 [36.922]
 [36.922]] [[0.3  ]
 [0.434]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]]
maxi score, test score, baseline:  0.06615333333333323 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.801]
 [0.882]
 [0.801]
 [0.798]
 [0.801]
 [0.801]
 [0.834]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.801]
 [0.882]
 [0.801]
 [0.798]
 [0.801]
 [0.801]
 [0.834]]
maxi score, test score, baseline:  0.06615333333333323 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.25 ]
 [0.301]
 [0.25 ]
 [0.25 ]
 [0.25 ]
 [0.25 ]
 [0.25 ]] [[44.519]
 [51.361]
 [44.519]
 [44.519]
 [44.519]
 [44.519]
 [44.519]] [[0.741]
 [0.908]
 [0.741]
 [0.741]
 [0.741]
 [0.741]
 [0.741]]
maxi score, test score, baseline:  0.06615333333333323 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.390958309173584
actor:  0 policy actor:  0  step number:  47 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  28.202440809571566
printing an ep nov before normalisation:  41.47969722747803
actor:  1 policy actor:  1  step number:  60 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  44 total reward:  0.5400000000000003  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]] [[48.805]
 [48.805]
 [48.805]
 [48.805]
 [48.805]
 [48.805]
 [48.805]] [[1.373]
 [1.373]
 [1.373]
 [1.373]
 [1.373]
 [1.373]
 [1.373]]
maxi score, test score, baseline:  0.06871333333333324 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.06871333333333324 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.09569454193115
printing an ep nov before normalisation:  40.32918385496523
maxi score, test score, baseline:  0.06871333333333324 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.06871333333333324 0.7 0.7
maxi score, test score, baseline:  0.06871333333333324 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.06647333333333322 0.7 0.7
Printing some Q and Qe and total Qs values:  [[0.577]
 [0.575]
 [0.572]
 [0.599]
 [0.556]
 [0.55 ]
 [0.61 ]] [[40.405]
 [48.259]
 [43.738]
 [42.142]
 [42.481]
 [42.179]
 [44.543]] [[0.927]
 [1.071]
 [0.984]
 [0.981]
 [0.945]
 [0.933]
 [1.037]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.06647333333333322 0.7 0.7
actor:  1 policy actor:  1  step number:  60 total reward:  0.1933333333333328  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  34.01239556264904
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  50.12010711984229
printing an ep nov before normalisation:  52.051839230028904
maxi score, test score, baseline:  0.06647333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.06647333333333322 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.888583310490134
printing an ep nov before normalisation:  37.924219656829635
actor:  0 policy actor:  0  step number:  67 total reward:  0.07999999999999896  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  34.67197345380979
actor:  1 policy actor:  1  step number:  51 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  41.781477579514444
printing an ep nov before normalisation:  46.84634511246257
printing an ep nov before normalisation:  43.70622213180467
maxi score, test score, baseline:  0.06863333333333321 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.29003757733089
Printing some Q and Qe and total Qs values:  [[0.136]
 [0.354]
 [0.122]
 [0.211]
 [0.208]
 [0.144]
 [0.125]] [[40.761]
 [35.979]
 [41.016]
 [35.597]
 [36.173]
 [38.644]
 [40.317]] [[0.657]
 [0.761]
 [0.649]
 [0.609]
 [0.619]
 [0.615]
 [0.635]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.5266666666666667  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  53 total reward:  0.346666666666666  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  42.368500398357085
printing an ep nov before normalisation:  36.192877710091615
printing an ep nov before normalisation:  37.58593326670897
siam score:  -0.77236646
maxi score, test score, baseline:  0.06863333333333321 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.06863333333333321 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0
actor:  0 policy actor:  0  step number:  32 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.06925999999999989 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.184 0.429 0.    0.122 0.02  0.    0.245]
maxi score, test score, baseline:  0.06925999999999989 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.594]
 [0.561]
 [0.58 ]
 [0.567]
 [0.594]
 [0.594]] [[33.865]
 [34.291]
 [34.17 ]
 [33.368]
 [33.743]
 [34.291]
 [34.291]] [[2.436]
 [2.46 ]
 [2.42 ]
 [2.395]
 [2.403]
 [2.46 ]
 [2.46 ]]
printing an ep nov before normalisation:  53.289415445541984
printing an ep nov before normalisation:  49.06146361914528
actor:  1 policy actor:  1  step number:  73 total reward:  0.27999999999999914  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.06925999999999989 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.830021167815076
actor:  0 policy actor:  0  step number:  52 total reward:  0.23333333333333262  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.07172666666666654 0.7 0.7
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.482]
 [0.365]
 [0.414]
 [0.409]
 [0.332]
 [0.377]] [[39.499]
 [37.073]
 [33.857]
 [36.46 ]
 [38.284]
 [42.11 ]
 [36.164]] [[1.163]
 [1.153]
 [0.933]
 [1.065]
 [1.119]
 [1.164]
 [1.019]]
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.622]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]] [[37.093]
 [36.859]
 [37.093]
 [37.093]
 [37.093]
 [37.093]
 [37.093]] [[1.24 ]
 [1.399]
 [1.24 ]
 [1.24 ]
 [1.24 ]
 [1.24 ]
 [1.24 ]]
actor:  1 policy actor:  1  step number:  62 total reward:  0.24666666666666637  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.3381, 0.0211, 0.1120, 0.1147, 0.1099, 0.1395, 0.1647],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0074, 0.9538, 0.0046, 0.0090, 0.0043, 0.0057, 0.0153],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0915, 0.0028, 0.4166, 0.1034, 0.0826, 0.2239, 0.0793],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1063, 0.1364, 0.1072, 0.2496, 0.1207, 0.1408, 0.1391],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1769, 0.0011, 0.0567, 0.0831, 0.5224, 0.0798, 0.0800],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0825, 0.0031, 0.1076, 0.1282, 0.0814, 0.5246, 0.0727],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1060, 0.1227, 0.1016, 0.1596, 0.1078, 0.1467, 0.2555],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  44.10347961357256
actor:  1 policy actor:  1  step number:  39 total reward:  0.5466666666666666  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.362077713012695
maxi score, test score, baseline:  0.07172666666666654 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.011]
 [0.254]
 [0.011]
 [0.011]
 [0.151]
 [0.011]] [[49.203]
 [49.203]
 [47.597]
 [49.203]
 [49.203]
 [45.844]
 [49.203]] [[1.571]
 [1.571]
 [1.727]
 [1.571]
 [1.571]
 [1.529]
 [1.571]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.07172666666666655 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.07172666666666655 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.07172666666666655 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.961]
 [0.981]
 [0.961]
 [0.961]
 [0.961]
 [0.961]
 [0.961]] [[35.73 ]
 [44.139]
 [35.73 ]
 [35.73 ]
 [35.73 ]
 [35.73 ]
 [35.73 ]] [[0.961]
 [0.981]
 [0.961]
 [0.961]
 [0.961]
 [0.961]
 [0.961]]
siam score:  -0.7730151
maxi score, test score, baseline:  0.07172666666666655 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.2813226549206
maxi score, test score, baseline:  0.07172666666666654 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  47 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  43.12052742907192
actor:  1 policy actor:  1  step number:  50 total reward:  0.43333333333333346  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.07147333333333321 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.4133333333333328  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.612]
 [0.56 ]
 [0.513]
 [0.487]
 [0.487]
 [0.516]] [[52.496]
 [53.282]
 [52.496]
 [55.419]
 [54.829]
 [54.953]
 [53.692]] [[1.146]
 [1.213]
 [1.146]
 [1.156]
 [1.119]
 [1.121]
 [1.126]]
maxi score, test score, baseline:  0.07147333333333321 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.138]
 [0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.016]
 [0.138]
 [0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]]
line 256 mcts: sample exp_bonus 0.0
actor:  1 policy actor:  1  step number:  37 total reward:  0.56  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.561]
 [0.485]
 [0.581]
 [0.569]
 [0.498]
 [0.511]] [[38.03 ]
 [36.237]
 [38.03 ]
 [38.662]
 [38.883]
 [40.421]
 [40.377]] [[1.15 ]
 [1.157]
 [1.15 ]
 [1.27 ]
 [1.267]
 [1.254]
 [1.266]]
maxi score, test score, baseline:  0.07147333333333321 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.99733018875122
maxi score, test score, baseline:  0.07147333333333321 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.18494937671791
maxi score, test score, baseline:  0.07147333333333321 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.18987919329849
printing an ep nov before normalisation:  30.10041692712378
printing an ep nov before normalisation:  39.27200304120607
Printing some Q and Qe and total Qs values:  [[0.763]
 [0.763]
 [0.763]
 [0.763]
 [0.763]
 [0.763]
 [0.763]] [[5.343]
 [3.363]
 [4.658]
 [3.039]
 [4.14 ]
 [4.541]
 [4.193]] [[1.066]
 [0.954]
 [1.027]
 [0.936]
 [0.998]
 [1.021]
 [1.001]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.4133333333333329  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  35.281714135420216
maxi score, test score, baseline:  0.07147333333333321 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.77236321614938
printing an ep nov before normalisation:  34.45062218381827
maxi score, test score, baseline:  0.07147333333333321 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.07147333333333321 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.07147333333333321 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.013497725567503949
actor:  1 policy actor:  1  step number:  46 total reward:  0.5000000000000001  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  62.99816448525374
printing an ep nov before normalisation:  40.26111298319645
actor:  0 policy actor:  0  step number:  66 total reward:  0.07333333333333214  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.958]
 [1.002]
 [0.958]
 [0.958]
 [0.958]
 [0.958]
 [0.968]] [[37.232]
 [34.214]
 [37.232]
 [37.232]
 [37.232]
 [37.232]
 [35.167]] [[0.958]
 [1.002]
 [0.958]
 [0.958]
 [0.958]
 [0.958]
 [0.968]]
maxi score, test score, baseline:  0.07361999999999987 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  41 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  38.40127580023639
maxi score, test score, baseline:  0.07649999999999986 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.4194, 0.0065, 0.1247, 0.0985, 0.1325, 0.1203, 0.0982],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0041, 0.9459, 0.0049, 0.0108, 0.0020, 0.0034, 0.0289],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1435, 0.0091, 0.3474, 0.1509, 0.1003, 0.1484, 0.1004],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1255, 0.0402, 0.1211, 0.2928, 0.1251, 0.1433, 0.1520],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1396, 0.0049, 0.0968, 0.1112, 0.4332, 0.1178, 0.0965],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1481, 0.0130, 0.1317, 0.1528, 0.1181, 0.3143, 0.1220],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1333, 0.1711, 0.0887, 0.1209, 0.0923, 0.1044, 0.2893],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.07649999999999986 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.07649999999999986 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  15.24453702959207
actor:  1 policy actor:  1  step number:  47 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.07649999999999986 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.07649999999999986 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  36 total reward:  0.5  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.07693999999999988 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.07693999999999988 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.27004961897112
actor:  1 policy actor:  1  step number:  67 total reward:  0.26666666666666616  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  22.405800106640964
printing an ep nov before normalisation:  0.006257216377463237
actor:  0 policy actor:  0  step number:  49 total reward:  0.22666666666666613  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  43.32877574942242
printing an ep nov before normalisation:  49.81146260102198
actor:  1 policy actor:  1  step number:  93 total reward:  0.13333333333333197  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  38.4282949816196
maxi score, test score, baseline:  0.0793933333333332 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.670353837638075
printing an ep nov before normalisation:  39.808725654823085
printing an ep nov before normalisation:  45.37833635306022
printing an ep nov before normalisation:  38.93906540389214
maxi score, test score, baseline:  0.07651333333333321 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.07651333333333321 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.814659468732536
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.426468365129892
maxi score, test score, baseline:  0.07651333333333321 0.7 0.7
printing an ep nov before normalisation:  33.98527592006039
printing an ep nov before normalisation:  47.007906244386724
printing an ep nov before normalisation:  41.47730917731426
actor:  0 policy actor:  0  step number:  45 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  47 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  43 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.08192666666666654 0.7 0.7
printing an ep nov before normalisation:  41.9048552409642
actor:  0 policy actor:  0  step number:  37 total reward:  0.52  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  37.983319175426935
maxi score, test score, baseline:  0.08197999999999987 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08197999999999987 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.4399999999999996  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  42.442732643379166
maxi score, test score, baseline:  0.08197999999999987 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.41333333333333333  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.08197999999999987 0.7 0.7
maxi score, test score, baseline:  0.08197999999999987 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.08197999999999987 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.526]
 [0.431]
 [0.526]
 [0.526]
 [0.526]
 [0.485]
 [0.526]] [[26.316]
 [22.912]
 [26.316]
 [26.316]
 [26.316]
 [25.476]
 [26.316]] [[0.526]
 [0.431]
 [0.526]
 [0.526]
 [0.526]
 [0.485]
 [0.526]]
printing an ep nov before normalisation:  34.75529028246553
actor:  1 policy actor:  1  step number:  57 total reward:  0.14666666666666606  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.08197999999999987 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  9.025877714157104
printing an ep nov before normalisation:  35.43095300301808
printing an ep nov before normalisation:  45.11920784359926
maxi score, test score, baseline:  0.07944666666666654 0.7 0.7
printing an ep nov before normalisation:  44.42979246658239
printing an ep nov before normalisation:  30.786948204040527
Starting evaluation
maxi score, test score, baseline:  0.07944666666666654 0.7 0.7
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.25235716904496
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.613]
 [0.531]
 [0.525]
 [0.531]
 [0.531]
 [0.528]] [[43.064]
 [47.809]
 [43.064]
 [44.679]
 [43.064]
 [43.064]
 [42.523]] [[0.531]
 [0.613]
 [0.531]
 [0.525]
 [0.531]
 [0.531]
 [0.528]]
printing an ep nov before normalisation:  52.67501074362802
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.949080070792725
printing an ep nov before normalisation:  35.49036993940448
printing an ep nov before normalisation:  46.07654862037211
printing an ep nov before normalisation:  39.07348474701971
printing an ep nov before normalisation:  37.29112607379581
printing an ep nov before normalisation:  41.10038394893267
printing an ep nov before normalisation:  44.89971367054587
printing an ep nov before normalisation:  35.532265669736006
Printing some Q and Qe and total Qs values:  [[0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]] [[35.737]
 [35.737]
 [35.737]
 [35.737]
 [35.737]
 [35.737]
 [35.737]] [[0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]]
actor:  0 policy actor:  0  step number:  44 total reward:  0.4466666666666663  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  33.54468518220191
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actions average: 
K:  0  action  0 :  tensor([0.4733, 0.0251, 0.0982, 0.0918, 0.0927, 0.1080, 0.1109],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0036, 0.9681, 0.0029, 0.0045, 0.0014, 0.0017, 0.0179],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0992, 0.0133, 0.4416, 0.1278, 0.0905, 0.1436, 0.0840],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2280, 0.0111, 0.1706, 0.1563, 0.1469, 0.1661, 0.1210],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1643, 0.0041, 0.1159, 0.1158, 0.3699, 0.1186, 0.1114],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1153, 0.0097, 0.1708, 0.1151, 0.1033, 0.3594, 0.1264],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0658, 0.1521, 0.1344, 0.1099, 0.0524, 0.1093, 0.3762],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  48 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  2.0
siam score:  -0.774198
maxi score, test score, baseline:  0.10760666666666655 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.3066666666666661  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.614]
 [0.555]
 [0.545]
 [0.545]
 [0.556]
 [0.548]] [[37.984]
 [38.136]
 [39.477]
 [40.056]
 [40.274]
 [39.29 ]
 [40.372]] [[0.549]
 [0.614]
 [0.555]
 [0.545]
 [0.545]
 [0.556]
 [0.548]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.099]
 [0.007]
 [0.022]
 [0.04 ]
 [0.011]
 [0.021]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.099]
 [0.007]
 [0.022]
 [0.04 ]
 [0.011]
 [0.021]]
maxi score, test score, baseline:  0.10424666666666657 0.6946666666666668 0.6946666666666668
maxi score, test score, baseline:  0.10088666666666656 0.6946666666666668 0.6946666666666668
maxi score, test score, baseline:  0.10088666666666656 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.22666666666666568  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.09752666666666655 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.3533, 0.0300, 0.1323, 0.1128, 0.1443, 0.1121, 0.1151],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0047, 0.9504, 0.0059, 0.0091, 0.0035, 0.0050, 0.0213],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0767, 0.0070, 0.5268, 0.0821, 0.0741, 0.1180, 0.1152],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0580, 0.1746, 0.0850, 0.4363, 0.0600, 0.0773, 0.1088],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1176, 0.0102, 0.1492, 0.2253, 0.2531, 0.1288, 0.1158],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0664, 0.0327, 0.1335, 0.0914, 0.1071, 0.4866, 0.0824],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0869, 0.3173, 0.1057, 0.1050, 0.0792, 0.0711, 0.2348],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.361]
 [0.108]
 [0.099]
 [0.116]
 [0.219]
 [0.191]] [[35.015]
 [34.111]
 [35.854]
 [36.222]
 [36.163]
 [36.443]
 [35.901]] [[1.822]
 [1.823]
 [1.718]
 [1.74 ]
 [1.753]
 [1.879]
 [1.805]]
printing an ep nov before normalisation:  34.41337744338321
maxi score, test score, baseline:  0.09752666666666655 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09752666666666655 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.78102064
printing an ep nov before normalisation:  18.439306020736694
maxi score, test score, baseline:  0.09752666666666655 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.105051756589546
printing an ep nov before normalisation:  32.06530413422307
maxi score, test score, baseline:  0.09752666666666655 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  52 total reward:  0.21999999999999964  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  57 total reward:  0.3599999999999992  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  65 total reward:  0.3199999999999992  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.498]
 [0.464]
 [0.464]
 [0.464]
 [0.464]
 [0.464]] [[47.787]
 [37.306]
 [47.787]
 [47.787]
 [47.787]
 [47.787]
 [47.787]] [[0.464]
 [0.498]
 [0.464]
 [0.464]
 [0.464]
 [0.464]
 [0.464]]
printing an ep nov before normalisation:  42.97026860430298
maxi score, test score, baseline:  0.09657999999999989 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09657999999999989 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.671552164067776
maxi score, test score, baseline:  0.09657999999999989 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.525]
 [0.526]
 [0.489]
 [0.485]
 [0.486]
 [0.526]] [[27.447]
 [38.362]
 [36.278]
 [27.029]
 [26.986]
 [26.872]
 [36.508]] [[0.833]
 [1.181]
 [1.121]
 [0.817]
 [0.812]
 [0.81 ]
 [1.128]]
printing an ep nov before normalisation:  35.22306458877696
printing an ep nov before normalisation:  40.15489149744266
printing an ep nov before normalisation:  26.258535385131836
actor:  1 policy actor:  1  step number:  47 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  54 total reward:  0.35333333333333283  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  33.16406682698187
actor:  0 policy actor:  0  step number:  56 total reward:  0.23333333333333262  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.11990928649902
printing an ep nov before normalisation:  44.09028789858923
Printing some Q and Qe and total Qs values:  [[0.175]
 [0.194]
 [0.172]
 [0.175]
 [0.175]
 [0.198]
 [0.171]] [[36.9  ]
 [46.218]
 [41.833]
 [36.9  ]
 [36.9  ]
 [42.529]
 [38.479]] [[0.897]
 [1.409]
 [1.155]
 [0.897]
 [0.897]
 [1.218]
 [0.977]]
maxi score, test score, baseline:  0.09565999999999988 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.51261437525143
printing an ep nov before normalisation:  38.00809679952587
maxi score, test score, baseline:  0.09565999999999988 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.30892160958318
maxi score, test score, baseline:  0.09565999999999988 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
actor:  0 policy actor:  0  step number:  57 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09472666666666654 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.2025984460997
Printing some Q and Qe and total Qs values:  [[0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]]
printing an ep nov before normalisation:  43.245744705200195
maxi score, test score, baseline:  0.09472666666666654 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  46 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.796]
 [0.839]
 [0.796]
 [0.796]
 [0.695]
 [0.796]
 [0.796]] [[36.759]
 [39.431]
 [36.759]
 [36.759]
 [35.635]
 [36.759]
 [36.759]] [[1.405]
 [1.533]
 [1.405]
 [1.405]
 [1.267]
 [1.405]
 [1.405]]
printing an ep nov before normalisation:  32.44146811869116
maxi score, test score, baseline:  0.09472666666666654 0.6946666666666668 0.6946666666666668
maxi score, test score, baseline:  0.09472666666666654 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.76637974446285
actor:  1 policy actor:  1  step number:  67 total reward:  0.18666666666666587  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  41.603741781447916
actor:  0 policy actor:  0  step number:  39 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.227]] [[28.771]
 [28.771]
 [28.771]
 [28.771]
 [28.771]
 [28.771]
 [28.771]] [[0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]]
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]] [[43.971]
 [50.508]
 [50.508]
 [50.508]
 [50.508]
 [50.508]
 [50.508]] [[0.661]
 [0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.753]]
printing an ep nov before normalisation:  31.815060176777887
maxi score, test score, baseline:  0.09421999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.019031658469196
maxi score, test score, baseline:  0.09421999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.48867801756467
printing an ep nov before normalisation:  42.41599095143684
actor:  1 policy actor:  1  step number:  67 total reward:  0.38666666666666605  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 30.4878449487261
maxi score, test score, baseline:  0.09421999999999986 0.6946666666666668 0.6946666666666668
UNIT TEST: sample policy line 217 mcts : [0. 1. 0. 0. 0. 0. 0.]
Printing some Q and Qe and total Qs values:  [[0.822]
 [0.789]
 [0.611]
 [0.621]
 [0.626]
 [0.626]
 [0.807]] [[33.555]
 [33.918]
 [22.162]
 [22.071]
 [22.401]
 [22.044]
 [33.523]] [[1.005]
 [0.976]
 [0.673]
 [0.682]
 [0.691]
 [0.687]
 [0.99 ]]
maxi score, test score, baseline:  0.09421999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09421999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09421999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.5525540593259
printing an ep nov before normalisation:  36.8651556968689
siam score:  -0.77964705
line 256 mcts: sample exp_bonus 39.22681403160095
maxi score, test score, baseline:  0.09421999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.09333333333333238  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  35.992799049939016
actor:  1 policy actor:  1  step number:  43 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  2.0
siam score:  -0.78155345
maxi score, test score, baseline:  0.09421999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.08808625239685
printing an ep nov before normalisation:  46.86646517995745
maxi score, test score, baseline:  0.09421999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.953294745326595
siam score:  -0.78024584
actor:  1 policy actor:  1  step number:  63 total reward:  0.13333333333333275  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09421999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09421999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09421999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.291]
 [0.333]
 [0.326]
 [0.279]
 [0.278]
 [0.285]
 [0.326]] [[24.448]
 [32.112]
 [32.023]
 [24.815]
 [24.75 ]
 [25.589]
 [32.023]] [[0.643]
 [0.902]
 [0.893]
 [0.641]
 [0.638]
 [0.669]
 [0.893]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  65 total reward:  0.39999999999999947  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  40.14914953588699
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09421999999999986 0.6946666666666668 0.6946666666666668
printing an ep nov before normalisation:  46.065375274580745
printing an ep nov before normalisation:  26.4444604067863
maxi score, test score, baseline:  0.09421999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.813]
 [0.835]
 [0.777]
 [0.777]
 [0.778]
 [0.777]
 [0.791]] [[27.257]
 [31.904]
 [27.599]
 [27.663]
 [27.652]
 [27.7  ]
 [27.951]] [[0.813]
 [0.835]
 [0.777]
 [0.777]
 [0.778]
 [0.777]
 [0.791]]
actor:  0 policy actor:  0  step number:  38 total reward:  0.5  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.09385999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  50 total reward:  0.24666666666666626  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  40.677603333245024
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09297999999999987 0.6946666666666668 0.6946666666666668
printing an ep nov before normalisation:  36.302373475840234
printing an ep nov before normalisation:  56.8222157580322
siam score:  -0.7801675
maxi score, test score, baseline:  0.09297999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.281153152090354
maxi score, test score, baseline:  0.09297999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.65703150377169
actor:  1 policy actor:  1  step number:  46 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  1.333
siam score:  -0.78193146
printing an ep nov before normalisation:  33.91480081092686
maxi score, test score, baseline:  0.09297999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.647]
 [0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]] [[43.787]
 [43.074]
 [43.787]
 [43.787]
 [43.787]
 [43.787]
 [43.787]] [[0.723]
 [0.929]
 [0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]]
printing an ep nov before normalisation:  29.570210206484663
maxi score, test score, baseline:  0.09297999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09297999999999987 0.6946666666666668 0.6946666666666668
actor:  0 policy actor:  0  step number:  57 total reward:  0.23999999999999921  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  48 total reward:  0.35333333333333283  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.09213999999999985 0.6946666666666668 0.6946666666666668
maxi score, test score, baseline:  0.09213999999999985 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.58660047528803
maxi score, test score, baseline:  0.09213999999999985 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.491426773762406
maxi score, test score, baseline:  0.09213999999999985 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.517]
 [0.422]
 [0.422]
 [0.422]
 [0.422]
 [0.422]] [[30.694]
 [38.232]
 [30.694]
 [30.694]
 [30.694]
 [30.694]
 [30.694]] [[1.637]
 [2.295]
 [1.637]
 [1.637]
 [1.637]
 [1.637]
 [1.637]]
maxi score, test score, baseline:  0.09213999999999985 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09213999999999985 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.452]
 [0.549]
 [0.602]
 [0.455]
 [0.453]
 [0.452]
 [0.528]] [[35.445]
 [39.142]
 [38.412]
 [35.504]
 [35.738]
 [35.342]
 [37.219]] [[0.666]
 [0.806]
 [0.851]
 [0.669]
 [0.67 ]
 [0.665]
 [0.763]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  52.004303497412415
printing an ep nov before normalisation:  39.30641054679395
maxi score, test score, baseline:  0.09213999999999985 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.659]
 [0.597]
 [0.605]
 [0.606]
 [0.586]
 [0.596]] [[22.278]
 [22.76 ]
 [22.017]
 [21.522]
 [21.154]
 [22.278]
 [21.31 ]] [[2.306]
 [2.416]
 [2.296]
 [2.266]
 [2.239]
 [2.306]
 [2.24 ]]
actor:  1 policy actor:  1  step number:  62 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09213999999999985 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.693438851035125
printing an ep nov before normalisation:  36.55018125261579
maxi score, test score, baseline:  0.09213999999999985 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.13999999999999946  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.2  ]
 [0.211]
 [0.211]] [[35.712]
 [35.712]
 [35.712]
 [35.712]
 [37.818]
 [35.712]
 [35.712]] [[1.137]
 [1.137]
 [1.137]
 [1.137]
 [1.233]
 [1.137]
 [1.137]]
printing an ep nov before normalisation:  44.31112217593939
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09213999999999985 0.6946666666666668 0.6946666666666668
actor:  1 policy actor:  1  step number:  41 total reward:  0.45333333333333303  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.09213999999999985 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.09213999999999985 0.6946666666666668 0.6946666666666668
printing an ep nov before normalisation:  39.151294005636956
printing an ep nov before normalisation:  39.20404013523608
printing an ep nov before normalisation:  50.61366346987719
printing an ep nov before normalisation:  48.47018399644325
actor:  0 policy actor:  0  step number:  56 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  1.0
siam score:  -0.78134257
maxi score, test score, baseline:  0.09123333333333321 0.6946666666666668 0.6946666666666668
maxi score, test score, baseline:  0.09123333333333321 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  41 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09071333333333319 0.6946666666666668 0.6946666666666668
actions average: 
K:  4  action  0 :  tensor([0.5438, 0.0827, 0.0712, 0.0749, 0.1089, 0.0671, 0.0513],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0113, 0.9279, 0.0102, 0.0295, 0.0024, 0.0030, 0.0157],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1364, 0.0299, 0.4263, 0.0927, 0.0817, 0.1369, 0.0961],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1411, 0.0298, 0.1486, 0.3127, 0.1190, 0.1301, 0.1189],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1531, 0.0489, 0.1223, 0.1354, 0.3041, 0.1366, 0.0996],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2047, 0.0054, 0.1517, 0.1255, 0.1039, 0.2983, 0.1105],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2027, 0.0077, 0.1349, 0.1735, 0.1383, 0.1804, 0.1625],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  35.942044258117676
maxi score, test score, baseline:  0.09071333333333319 0.6946666666666668 0.6946666666666668
siam score:  -0.7840401
maxi score, test score, baseline:  0.09071333333333319 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  54 total reward:  0.27333333333333254  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.09001999999999985 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09001999999999985 0.6946666666666668 0.6946666666666668
maxi score, test score, baseline:  0.09001999999999985 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.064735172970472
printing an ep nov before normalisation:  42.24833011627197
maxi score, test score, baseline:  0.09001999999999985 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.50049528527306
printing an ep nov before normalisation:  45.81943223024351
maxi score, test score, baseline:  0.09001999999999985 0.6946666666666668 0.6946666666666668
printing an ep nov before normalisation:  42.77205670000587
printing an ep nov before normalisation:  47.4615075234972
line 256 mcts: sample exp_bonus 44.84610448594544
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]] [[39.327]
 [39.327]
 [39.327]
 [39.327]
 [39.327]
 [39.327]
 [39.327]] [[1.748]
 [1.748]
 [1.748]
 [1.748]
 [1.748]
 [1.748]
 [1.748]]
maxi score, test score, baseline:  0.08768666666666654 0.6946666666666668 0.6946666666666668
maxi score, test score, baseline:  0.08768666666666654 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.75225436770468
actor:  1 policy actor:  1  step number:  51 total reward:  0.3199999999999994  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.08768666666666654 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.617219818129286
Printing some Q and Qe and total Qs values:  [[0.039]
 [0.077]
 [0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.039]
 [0.077]
 [0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]]
maxi score, test score, baseline:  0.08768666666666654 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  0.56  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.08768666666666654 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08768666666666654 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.715543700042105
actor:  1 policy actor:  1  step number:  46 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  0.667
siam score:  -0.77831894
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08492666666666654 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.959]
 [0.991]
 [0.959]
 [0.959]
 [0.959]
 [0.959]
 [0.959]] [[42.291]
 [44.415]
 [42.291]
 [42.291]
 [42.291]
 [42.291]
 [42.291]] [[0.959]
 [0.991]
 [0.959]
 [0.959]
 [0.959]
 [0.959]
 [0.959]]
actor:  0 policy actor:  0  step number:  46 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.08501999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08501999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  36 total reward:  0.5133333333333333  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0859533333333332 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0859533333333332 0.6946666666666668 0.6946666666666668
maxi score, test score, baseline:  0.0859533333333332 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.951]
 [0.955]
 [0.951]
 [0.951]
 [0.951]
 [0.951]
 [0.951]] [[48.162]
 [37.64 ]
 [48.162]
 [48.162]
 [48.162]
 [48.162]
 [48.162]] [[0.951]
 [0.955]
 [0.951]
 [0.951]
 [0.951]
 [0.951]
 [0.951]]
maxi score, test score, baseline:  0.0859533333333332 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 23.268471479415894
printing an ep nov before normalisation:  29.38539262761485
actor:  0 policy actor:  0  step number:  67 total reward:  0.06666666666666599  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.531]
 [0.335]
 [0.332]
 [0.331]
 [0.321]
 [0.328]] [[45.496]
 [46.267]
 [39.853]
 [40.531]
 [40.767]
 [40.645]
 [40.324]] [[1.435]
 [1.47 ]
 [1.048]
 [1.069]
 [1.076]
 [1.062]
 [1.058]]
maxi score, test score, baseline:  0.08808666666666654 0.6946666666666668 0.6946666666666668
maxi score, test score, baseline:  0.08808666666666654 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.165975093841553
Printing some Q and Qe and total Qs values:  [[0.627]
 [0.627]
 [0.627]
 [1.045]
 [0.627]
 [0.627]
 [0.627]] [[44.717]
 [44.717]
 [44.717]
 [40.284]
 [44.717]
 [44.717]
 [44.717]] [[1.96]
 [1.96]
 [1.96]
 [2.17]
 [1.96]
 [1.96]
 [1.96]]
printing an ep nov before normalisation:  40.89098194609654
actor:  0 policy actor:  0  step number:  52 total reward:  0.206666666666666  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.09049999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.3333333333333326  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.09049999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09049999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09049999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.72645213062845
printing an ep nov before normalisation:  28.374677941988047
actor:  1 policy actor:  1  step number:  44 total reward:  0.35333333333333317  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.08785999999999988 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08785999999999988 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7674897
printing an ep nov before normalisation:  37.76156273938452
printing an ep nov before normalisation:  36.1285067368752
printing an ep nov before normalisation:  39.28911702659833
maxi score, test score, baseline:  0.08785999999999988 0.6946666666666668 0.6946666666666668
maxi score, test score, baseline:  0.08785999999999988 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.85163248013415
actor:  0 policy actor:  0  step number:  40 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  52 total reward:  0.5000000000000001  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.09061999999999985 0.6946666666666668 0.6946666666666668
maxi score, test score, baseline:  0.09061999999999985 0.6946666666666668 0.6946666666666668
actor:  1 policy actor:  1  step number:  45 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  37.809511566087565
maxi score, test score, baseline:  0.09061999999999985 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09061999999999985 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09061999999999985 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.125633989037798
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([0.5209, 0.0349, 0.1085, 0.0796, 0.0952, 0.0758, 0.0851],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0129, 0.9406, 0.0071, 0.0038, 0.0034, 0.0044, 0.0280],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0730, 0.0329, 0.4737, 0.1054, 0.0583, 0.1129, 0.1438],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1266, 0.0930, 0.1305, 0.2493, 0.1114, 0.1593, 0.1299],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1719, 0.0261, 0.1634, 0.1482, 0.2282, 0.1519, 0.1103],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0929, 0.1551, 0.1999, 0.0899, 0.0647, 0.3085, 0.0890],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0493, 0.3271, 0.1290, 0.1400, 0.0332, 0.0641, 0.2574],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  30.487752954928172
printing an ep nov before normalisation:  39.90500840290443
printing an ep nov before normalisation:  25.25047779083252
maxi score, test score, baseline:  0.09061999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  58 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.09061999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09061999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
Printing some Q and Qe and total Qs values:  [[0.991]
 [0.991]
 [0.991]
 [0.991]
 [0.991]
 [0.991]
 [0.991]] [[34.312]
 [34.312]
 [34.312]
 [34.312]
 [34.312]
 [34.312]
 [34.312]] [[0.991]
 [0.991]
 [0.991]
 [0.991]
 [0.991]
 [0.991]
 [0.991]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.22886153121611
actor:  1 policy actor:  1  step number:  59 total reward:  0.3066666666666664  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  53 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  0.333
siam score:  -0.76019293
maxi score, test score, baseline:  0.09061999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.18582201004028
printing an ep nov before normalisation:  47.90827482663397
maxi score, test score, baseline:  0.09061999999999987 0.6946666666666668 0.6946666666666668
maxi score, test score, baseline:  0.09061999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.689]
 [0.716]
 [0.651]
 [0.645]
 [0.702]
 [0.652]
 [0.702]] [[30.978]
 [38.657]
 [28.723]
 [31.055]
 [34.72 ]
 [30.51 ]
 [34.72 ]] [[1.795]
 [2.322]
 [1.61 ]
 [1.756]
 [2.051]
 [1.728]
 [2.051]]
printing an ep nov before normalisation:  48.79614764045178
printing an ep nov before normalisation:  35.541868643390416
maxi score, test score, baseline:  0.09061999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09061999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09061999999999987 0.6946666666666668 0.6946666666666668
maxi score, test score, baseline:  0.09061999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.26666666666666616  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09061999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  48.93210673764416
printing an ep nov before normalisation:  48.90722700155413
maxi score, test score, baseline:  0.09061999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09061999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.26087398832047
maxi score, test score, baseline:  0.09061999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.738160949586145
printing an ep nov before normalisation:  36.33247375488281
actor:  1 policy actor:  1  step number:  58 total reward:  0.3666666666666659  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.09061999999999987 0.6946666666666668 0.6946666666666668
maxi score, test score, baseline:  0.09061999999999987 0.6946666666666668 0.6946666666666668
printing an ep nov before normalisation:  40.922746658325195
maxi score, test score, baseline:  0.09061999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.48063087463379
actor:  1 policy actor:  1  step number:  69 total reward:  0.17333333333333278  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.09061999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09061999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.60395823656701
actor:  1 policy actor:  1  step number:  73 total reward:  0.18666666666666587  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.09061999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.399]
 [0.349]
 [0.391]
 [0.349]
 [0.349]
 [0.387]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.349]
 [0.399]
 [0.349]
 [0.391]
 [0.349]
 [0.349]
 [0.387]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.37333333333333285  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.09061999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.205471228166
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  26.37621622566035
maxi score, test score, baseline:  0.09061999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.39333333333333287  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.09061999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09061999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09061999999999987 0.6946666666666668 0.6946666666666668
maxi score, test score, baseline:  0.09061999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.12394345957936
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.550091610028044
printing an ep nov before normalisation:  39.4463948958048
printing an ep nov before normalisation:  28.42568110288198
printing an ep nov before normalisation:  0.10650796397158047
printing an ep nov before normalisation:  38.411771753275495
printing an ep nov before normalisation:  33.75354980677614
maxi score, test score, baseline:  0.09061999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  42 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.09329999999999988 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.077]
 [0.095]
 [0.078]
 [0.08 ]
 [0.081]
 [0.076]
 [0.084]] [[38.483]
 [38.69 ]
 [38.252]
 [38.123]
 [37.505]
 [46.837]
 [36.725]] [[1.396]
 [1.428]
 [1.383]
 [1.377]
 [1.338]
 [1.931]
 [1.291]]
maxi score, test score, baseline:  0.09329999999999988 0.6946666666666668 0.6946666666666668
printing an ep nov before normalisation:  43.10619342560928
maxi score, test score, baseline:  0.09329999999999988 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  52 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  3  action  0 :  tensor([0.4413, 0.0103, 0.0790, 0.0801, 0.2136, 0.0899, 0.0859],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0079,     0.9592,     0.0054,     0.0037,     0.0008,     0.0012,
            0.0217], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0891, 0.1371, 0.5004, 0.0594, 0.0555, 0.0827, 0.0759],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1042, 0.0268, 0.1273, 0.4158, 0.1140, 0.1155, 0.0964],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0967, 0.0439, 0.1847, 0.0738, 0.2851, 0.2205, 0.0954],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1161, 0.0139, 0.2016, 0.1224, 0.1226, 0.3105, 0.1128],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1211, 0.2378, 0.0966, 0.2176, 0.1193, 0.0942, 0.1135],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  42.97453186757499
siam score:  -0.77017623
printing an ep nov before normalisation:  50.09365847801188
printing an ep nov before normalisation:  45.811137710738286
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.68318627856026
printing an ep nov before normalisation:  37.116632776831665
actor:  1 policy actor:  1  step number:  67 total reward:  0.0666666666666661  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  44 total reward:  0.5800000000000002  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09329999999999988 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  33.4943356897301
maxi score, test score, baseline:  0.09329999999999988 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09329999999999988 0.6946666666666668 0.6946666666666668
maxi score, test score, baseline:  0.09329999999999988 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.543]
 [0.515]
 [0.52 ]
 [0.523]
 [0.524]
 [0.571]] [[27.988]
 [29.258]
 [27.966]
 [27.972]
 [27.974]
 [28.308]
 [29.503]] [[1.188]
 [1.284]
 [1.182]
 [1.188]
 [1.192]
 [1.212]
 [1.326]]
maxi score, test score, baseline:  0.09329999999999988 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.045465028870115
actor:  1 policy actor:  1  step number:  51 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09329999999999988 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  43 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09601999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09601999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
printing an ep nov before normalisation:  35.65408945083618
maxi score, test score, baseline:  0.09601999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.99842828579417
maxi score, test score, baseline:  0.09601999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.33648565118003
maxi score, test score, baseline:  0.09601999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09601999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  54 total reward:  0.27333333333333276  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  47 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.647]
 [0.7  ]
 [0.639]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]] [[40.005]
 [42.232]
 [39.106]
 [36.217]
 [36.217]
 [36.217]
 [36.217]] [[1.218]
 [1.303]
 [1.197]
 [1.147]
 [1.147]
 [1.147]
 [1.147]]
maxi score, test score, baseline:  0.09601999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
siam score:  -0.77837145
Printing some Q and Qe and total Qs values:  [[-0.018]
 [ 0.041]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]] [[16.259]
 [26.223]
 [16.259]
 [16.259]
 [16.259]
 [16.259]
 [16.259]] [[0.056]
 [0.266]
 [0.056]
 [0.056]
 [0.056]
 [0.056]
 [0.056]]
maxi score, test score, baseline:  0.09601999999999987 0.6946666666666668 0.6946666666666668
maxi score, test score, baseline:  0.09601999999999988 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09601999999999988 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.446815533007054
maxi score, test score, baseline:  0.09601999999999988 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.464]
 [0.351]
 [0.337]
 [0.299]
 [0.318]
 [0.35 ]] [[41.089]
 [40.464]
 [38.788]
 [41.355]
 [44.79 ]
 [44.956]
 [36.395]] [[1.403]
 [1.425]
 [1.244]
 [1.334]
 [1.433]
 [1.459]
 [1.147]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09601999999999988 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09601999999999988 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.22666666666666635  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]] [[29.753]
 [29.753]
 [29.753]
 [29.753]
 [29.753]
 [29.753]
 [29.753]] [[1.503]
 [1.503]
 [1.503]
 [1.503]
 [1.503]
 [1.503]
 [1.503]]
actor:  1 policy actor:  1  step number:  42 total reward:  0.5000000000000001  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  37.600088119506836
maxi score, test score, baseline:  0.09601999999999988 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.87479870946866
actor:  0 policy actor:  0  step number:  42 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.04 ]
 [0.13 ]
 [0.036]
 [0.031]
 [0.072]
 [0.036]
 [0.039]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.04 ]
 [0.13 ]
 [0.036]
 [0.031]
 [0.072]
 [0.036]
 [0.039]]
maxi score, test score, baseline:  0.09621999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.08410431147934
printing an ep nov before normalisation:  30.49666909056564
actor:  1 policy actor:  1  step number:  37 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.65 ]
 [0.765]
 [0.661]
 [0.661]
 [0.658]
 [0.661]
 [0.645]] [[40.281]
 [43.35 ]
 [38.76 ]
 [38.76 ]
 [45.115]
 [38.76 ]
 [39.591]] [[0.65 ]
 [0.765]
 [0.661]
 [0.661]
 [0.658]
 [0.661]
 [0.645]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.09621999999999986 0.6946666666666668 0.6946666666666668
maxi score, test score, baseline:  0.09621999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09621999999999986 0.6946666666666668 0.6946666666666668
printing an ep nov before normalisation:  34.778151512145996
maxi score, test score, baseline:  0.09621999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.17483627650217
actor:  0 policy actor:  0  step number:  51 total reward:  0.23999999999999944  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  34.423854585480484
maxi score, test score, baseline:  0.09869999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  48 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  51 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.09861999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  47 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  38.88512397599397
maxi score, test score, baseline:  0.09844666666666653 0.6946666666666668 0.6946666666666668
siam score:  -0.7658113
maxi score, test score, baseline:  0.09844666666666653 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.698]
 [0.33 ]
 [0.633]
 [0.535]
 [0.193]
 [0.671]] [[42.453]
 [41.911]
 [44.358]
 [45.29 ]
 [48.806]
 [47.771]
 [41.844]] [[1.419]
 [1.511]
 [1.251]
 [1.594]
 [1.65 ]
 [1.263]
 [1.481]]
printing an ep nov before normalisation:  35.62476262806002
maxi score, test score, baseline:  0.09844666666666653 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.387]
 [0.655]
 [0.504]
 [0.387]
 [0.387]
 [0.551]
 [0.387]] [[46.182]
 [44.891]
 [48.502]
 [46.182]
 [46.182]
 [49.251]
 [46.182]] [[1.125]
 [1.353]
 [1.315]
 [1.125]
 [1.125]
 [1.385]
 [1.125]]
maxi score, test score, baseline:  0.09844666666666653 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7709082
actor:  1 policy actor:  1  step number:  37 total reward:  0.52  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09844666666666653 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09844666666666653 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09844666666666653 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09844666666666653 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.607]
 [0.598]
 [0.591]
 [0.607]
 [0.607]
 [0.607]] [[34.449]
 [34.449]
 [35.28 ]
 [35.565]
 [34.449]
 [34.449]
 [34.449]] [[2.359]
 [2.359]
 [2.433]
 [2.454]
 [2.359]
 [2.359]
 [2.359]]
printing an ep nov before normalisation:  52.91877534916919
actor:  1 policy actor:  1  step number:  57 total reward:  0.25333333333333263  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  42.230393621236956
printing an ep nov before normalisation:  44.97791730788482
printing an ep nov before normalisation:  36.39024257659912
actor:  0 policy actor:  0  step number:  49 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  39.630063893136324
maxi score, test score, baseline:  0.10105999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.   ]
 [ 0.011]
 [-0.001]
 [-0.001]
 [-0.002]
 [-0.002]
 [-0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.   ]
 [ 0.011]
 [-0.001]
 [-0.001]
 [-0.002]
 [-0.002]
 [-0.001]]
maxi score, test score, baseline:  0.10105999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.10105999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.10105999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.63958141050723
printing an ep nov before normalisation:  32.49947422251308
printing an ep nov before normalisation:  33.71195839877537
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10105999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
maxi score, test score, baseline:  0.10105999999999987 0.6946666666666668 0.6946666666666668
actions average: 
K:  3  action  0 :  tensor([0.3027, 0.0252, 0.1568, 0.1254, 0.1266, 0.1374, 0.1260],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0307, 0.8898, 0.0123, 0.0140, 0.0063, 0.0149, 0.0320],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0916, 0.0014, 0.4524, 0.1209, 0.0899, 0.1533, 0.0905],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1193, 0.1479, 0.1265, 0.2077, 0.1190, 0.1334, 0.1462],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1746, 0.0226, 0.1329, 0.1063, 0.3189, 0.1305, 0.1142],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0939, 0.0567, 0.1144, 0.1644, 0.1441, 0.2772, 0.1493],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1034, 0.1227, 0.0998, 0.1060, 0.1352, 0.1452, 0.2876],
       grad_fn=<DivBackward0>)
siam score:  -0.77265626
Printing some Q and Qe and total Qs values:  [[0.369]
 [0.433]
 [0.369]
 [0.369]
 [0.369]
 [0.369]
 [0.369]] [[32.392]
 [36.827]
 [32.392]
 [32.392]
 [32.392]
 [32.392]
 [32.392]] [[0.559]
 [0.68 ]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]]
actor:  1 policy actor:  1  step number:  73 total reward:  0.2533333333333324  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  49 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.10105999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.931088350786254
printing an ep nov before normalisation:  31.280157791356284
printing an ep nov before normalisation:  39.91383596408695
actor:  1 policy actor:  1  step number:  63 total reward:  0.2933333333333331  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.10105999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.347907144428426
maxi score, test score, baseline:  0.10105999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]] [[43.068]
 [43.068]
 [43.068]
 [43.068]
 [43.068]
 [43.068]
 [43.068]] [[0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]]
printing an ep nov before normalisation:  26.542909145355225
printing an ep nov before normalisation:  45.27515371416751
actor:  0 policy actor:  0  step number:  39 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  32.63961490442669
actor:  1 policy actor:  1  step number:  63 total reward:  0.17333333333333278  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  4  action  0 :  tensor([0.4800, 0.1651, 0.0657, 0.0763, 0.1040, 0.0609, 0.0480],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0030, 0.9113, 0.0031, 0.0339, 0.0017, 0.0016, 0.0455],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1388, 0.1865, 0.3127, 0.1078, 0.0849, 0.0969, 0.0723],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0851, 0.2509, 0.0910, 0.2531, 0.1100, 0.1140, 0.0960],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.3626, 0.0031, 0.0159, 0.0055, 0.5850, 0.0198, 0.0081],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1501, 0.0221, 0.1371, 0.1498, 0.1521, 0.2940, 0.0949],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0740, 0.1619, 0.0815, 0.2007, 0.1132, 0.0798, 0.2889],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  27.341787815093994
maxi score, test score, baseline:  0.10093999999999986 0.6946666666666668 0.6946666666666668
actor:  1 policy actor:  1  step number:  46 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  28.623266220092773
Printing some Q and Qe and total Qs values:  [[0.443]
 [0.518]
 [0.37 ]
 [0.437]
 [0.443]
 [0.443]
 [0.517]] [[40.488]
 [44.349]
 [43.373]
 [40.66 ]
 [40.488]
 [40.488]
 [43.604]] [[0.825]
 [0.972]
 [0.806]
 [0.823]
 [0.825]
 [0.825]
 [0.958]]
maxi score, test score, baseline:  0.10093999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.34221418982296
printing an ep nov before normalisation:  34.999792450419015
maxi score, test score, baseline:  0.10093999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.26164483410952
maxi score, test score, baseline:  0.10093999999999986 0.6946666666666668 0.6946666666666668
printing an ep nov before normalisation:  22.717385292053223
actor:  1 policy actor:  1  step number:  52 total reward:  0.3266666666666661  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.10093999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.09592270341882
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [ 0.045]
 [-0.012]
 [-0.004]
 [-0.004]
 [-0.011]
 [-0.01 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.01 ]
 [ 0.045]
 [-0.012]
 [-0.004]
 [-0.004]
 [-0.011]
 [-0.01 ]]
maxi score, test score, baseline:  0.10093999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.10093999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  9.17406006326928
actor:  1 policy actor:  1  step number:  66 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.10093999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 29.107754230499268
maxi score, test score, baseline:  0.10093999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.10093999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.83657890528211
actor:  1 policy actor:  1  step number:  49 total reward:  0.35999999999999943  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10093999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.10093999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  54 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.10093999999999986 0.6946666666666668 0.6946666666666668
printing an ep nov before normalisation:  51.85280204438485
printing an ep nov before normalisation:  30.153704246453426
Printing some Q and Qe and total Qs values:  [[0.82 ]
 [0.887]
 [0.82 ]
 [0.82 ]
 [0.82 ]
 [0.82 ]
 [0.82 ]] [[46.908]
 [60.612]
 [46.908]
 [46.908]
 [46.908]
 [46.908]
 [46.908]] [[0.82 ]
 [0.887]
 [0.82 ]
 [0.82 ]
 [0.82 ]
 [0.82 ]
 [0.82 ]]
maxi score, test score, baseline:  0.10093999999999986 0.6946666666666668 0.6946666666666668
maxi score, test score, baseline:  0.10093999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.4517, 0.0588, 0.1267, 0.0813, 0.1062, 0.0814, 0.0940],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0035, 0.9652, 0.0025, 0.0059, 0.0022, 0.0026, 0.0182],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1059, 0.0205, 0.4905, 0.0526, 0.0592, 0.1395, 0.1319],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1555, 0.0138, 0.0951, 0.3826, 0.1180, 0.1162, 0.1187],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1341, 0.1095, 0.0940, 0.0963, 0.3565, 0.1064, 0.1032],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0923, 0.0805, 0.1245, 0.1089, 0.0752, 0.4008, 0.1178],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1199, 0.2916, 0.0833, 0.1642, 0.0835, 0.0940, 0.1635],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  34.56050952737072
actor:  1 policy actor:  1  step number:  44 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  50 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  1  action  0 :  tensor([0.5108, 0.0064, 0.0638, 0.1091, 0.1523, 0.0867, 0.0708],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0040, 0.9732, 0.0031, 0.0038, 0.0028, 0.0023, 0.0107],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1011, 0.0111, 0.5092, 0.0657, 0.0737, 0.1442, 0.0951],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1030, 0.0409, 0.0938, 0.3562, 0.1364, 0.1202, 0.1494],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2158, 0.0037, 0.0680, 0.0974, 0.4525, 0.0822, 0.0803],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0753, 0.0118, 0.2719, 0.0920, 0.0936, 0.3126, 0.1427],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1300, 0.0384, 0.1030, 0.1320, 0.1442, 0.1304, 0.3220],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  51 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.10119333333333319 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.10119333333333319 0.6946666666666668 0.6946666666666668
maxi score, test score, baseline:  0.10119333333333319 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  4  action  0 :  tensor([0.5321, 0.0199, 0.0967, 0.0851, 0.1030, 0.0852, 0.0779],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0075, 0.9059, 0.0095, 0.0267, 0.0070, 0.0081, 0.0353],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0875, 0.2247, 0.1901, 0.1063, 0.0888, 0.2213, 0.0813],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1839, 0.0089, 0.1782, 0.1528, 0.1426, 0.1946, 0.1392],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1138, 0.0562, 0.0816, 0.0655, 0.5412, 0.0803, 0.0615],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2004, 0.0211, 0.1122, 0.1328, 0.1041, 0.3330, 0.0965],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0848, 0.2207, 0.1301, 0.1366, 0.0912, 0.0942, 0.2423],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  54.71819177055287
printing an ep nov before normalisation:  43.02785354215936
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.85261825846152
printing an ep nov before normalisation:  33.084869159516906
printing an ep nov before normalisation:  0.003097130982894214
actions average: 
K:  2  action  0 :  tensor([0.4785, 0.0104, 0.1063, 0.1008, 0.0962, 0.1040, 0.1039],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0053, 0.9546, 0.0066, 0.0085, 0.0048, 0.0069, 0.0133],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0707, 0.0895, 0.4901, 0.0772, 0.0725, 0.1003, 0.0997],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0993, 0.0046, 0.2729, 0.1801, 0.1110, 0.1685, 0.1636],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2413, 0.0020, 0.1113, 0.1067, 0.2974, 0.1226, 0.1187],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0014, 0.0064, 0.1552, 0.0030, 0.0047, 0.8263, 0.0031],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0690, 0.3047, 0.1263, 0.0816, 0.0504, 0.0875, 0.2804],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  55 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  51 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09896666666666652 0.6946666666666668 0.6946666666666668
siam score:  -0.7804942
Printing some Q and Qe and total Qs values:  [[0.235]
 [0.393]
 [0.266]
 [0.243]
 [0.235]
 [0.235]
 [0.321]] [[44.458]
 [47.234]
 [45.375]
 [50.496]
 [44.458]
 [44.458]
 [48.435]] [[0.427]
 [0.608]
 [0.466]
 [0.486]
 [0.427]
 [0.427]
 [0.546]]
maxi score, test score, baseline:  0.09896666666666652 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09896666666666652 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  39 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  1.667
siam score:  -0.7798933
maxi score, test score, baseline:  0.10197999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  0.5866666666666669  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  47 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.10197999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.776699
maxi score, test score, baseline:  0.10197999999999986 0.6946666666666668 0.6946666666666668
maxi score, test score, baseline:  0.10197999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.280589488725276
actions average: 
K:  3  action  0 :  tensor([0.3339, 0.0806, 0.1097, 0.0905, 0.2067, 0.1161, 0.0625],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0203, 0.9328, 0.0048, 0.0066, 0.0053, 0.0019, 0.0283],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1003, 0.0504, 0.3673, 0.1073, 0.1328, 0.1452, 0.0966],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0816, 0.1529, 0.1431, 0.3110, 0.1015, 0.1266, 0.0832],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2096, 0.2276, 0.0942, 0.0929, 0.1852, 0.0786, 0.1119],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1067, 0.0882, 0.1021, 0.0673, 0.0577, 0.4785, 0.0995],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1051, 0.3087, 0.0761, 0.1194, 0.0657, 0.0840, 0.2409],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.204]
 [0.239]
 [0.217]
 [0.161]
 [0.19 ]
 [0.169]
 [0.217]] [[48.473]
 [47.912]
 [58.814]
 [51.582]
 [55.617]
 [54.936]
 [58.814]] [[1.196]
 [1.212]
 [1.55 ]
 [1.256]
 [1.418]
 [1.374]
 [1.55 ]]
printing an ep nov before normalisation:  40.659860042338074
printing an ep nov before normalisation:  37.50478506088257
printing an ep nov before normalisation:  42.3636864728847
maxi score, test score, baseline:  0.10197999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.38305349472506
maxi score, test score, baseline:  0.10197999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.79307532814386
printing an ep nov before normalisation:  29.565751552581787
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  68 total reward:  0.05999999999999939  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  33.465261358873704
maxi score, test score, baseline:  0.10197999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.17]
 [0.17]
 [0.17]
 [0.17]
 [0.17]
 [0.17]
 [0.17]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.17]
 [0.17]
 [0.17]
 [0.17]
 [0.17]
 [0.17]
 [0.17]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10197999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.4199999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.10197999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 49.692721546797145
printing an ep nov before normalisation:  52.80128179199317
printing an ep nov before normalisation:  47.553019523620605
printing an ep nov before normalisation:  30.417799671916274
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.018]
 [0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.018]
 [0.017]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  62 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.09905999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.849572310722255
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.75580922350107
printing an ep nov before normalisation:  59.71146731833183
siam score:  -0.77793
maxi score, test score, baseline:  0.09905999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09905999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.70664930293348
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  45 total reward:  0.45333333333333303  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  32.81705892450907
actions average: 
K:  4  action  0 :  tensor([0.2947, 0.0147, 0.1140, 0.1582, 0.1770, 0.0985, 0.1429],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0043, 0.9546, 0.0027, 0.0058, 0.0035, 0.0030, 0.0262],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1533, 0.0033, 0.3498, 0.1041, 0.1194, 0.1557, 0.1143],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0777, 0.1895, 0.0686, 0.4196, 0.0852, 0.0763, 0.0831],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2089, 0.0514, 0.0949, 0.0978, 0.3410, 0.0959, 0.1101],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0552, 0.1103, 0.1733, 0.1118, 0.0609, 0.3679, 0.1205],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1400, 0.1546, 0.1133, 0.1733, 0.0928, 0.1327, 0.1933],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.189]
 [0.177]
 [0.155]
 [0.16 ]
 [0.197]
 [0.192]
 [0.146]] [[39.244]
 [38.163]
 [35.215]
 [39.993]
 [38.684]
 [39.451]
 [38.309]] [[1.297]
 [1.231]
 [1.061]
 [1.306]
 [1.277]
 [1.31 ]
 [1.207]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09905999999999987 0.6946666666666668 0.6946666666666668
printing an ep nov before normalisation:  39.787893295288086
printing an ep nov before normalisation:  38.25412035151167
printing an ep nov before normalisation:  31.511588055723287
Printing some Q and Qe and total Qs values:  [[0.17 ]
 [0.296]
 [0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.224]] [[33.245]
 [36.956]
 [31.723]
 [31.723]
 [31.723]
 [31.723]
 [31.723]] [[0.17 ]
 [0.296]
 [0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.224]]
printing an ep nov before normalisation:  37.035219958811666
Printing some Q and Qe and total Qs values:  [[0.526]
 [0.637]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]] [[33.5  ]
 [45.026]
 [33.5  ]
 [33.5  ]
 [33.5  ]
 [33.5  ]
 [33.5  ]] [[1.4  ]
 [1.811]
 [1.4  ]
 [1.4  ]
 [1.4  ]
 [1.4  ]
 [1.4  ]]
maxi score, test score, baseline:  0.09905999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09905999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.6006, 0.0192, 0.0902, 0.0656, 0.1062, 0.0727, 0.0456],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0087, 0.9441, 0.0080, 0.0085, 0.0054, 0.0079, 0.0175],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1564, 0.0076, 0.2749, 0.1294, 0.1083, 0.2478, 0.0757],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1333, 0.0116, 0.1755, 0.3043, 0.1213, 0.1337, 0.1202],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1409, 0.0141, 0.1322, 0.1657, 0.2728, 0.1388, 0.1354],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0935, 0.0324, 0.1540, 0.1233, 0.0928, 0.3624, 0.1416],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1346, 0.1597, 0.1541, 0.1239, 0.1109, 0.1703, 0.1464],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  47.0081546893178
maxi score, test score, baseline:  0.09905999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.756]
 [0.794]
 [0.71 ]
 [0.729]
 [0.715]
 [0.705]
 [0.735]] [[35.095]
 [39.499]
 [30.183]
 [31.955]
 [29.505]
 [29.429]
 [29.794]] [[0.756]
 [0.794]
 [0.71 ]
 [0.729]
 [0.715]
 [0.705]
 [0.735]]
maxi score, test score, baseline:  0.09905999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09905999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.745]
 [0.759]
 [0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]] [[38.954]
 [41.235]
 [38.954]
 [38.954]
 [38.954]
 [38.954]
 [38.954]] [[1.002]
 [1.04 ]
 [1.002]
 [1.002]
 [1.002]
 [1.002]
 [1.002]]
maxi score, test score, baseline:  0.09905999999999987 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  46 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.09883333333333319 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.009]
 [ 0.084]
 [-0.175]
 [ 0.006]
 [-0.004]
 [ 0.031]
 [ 0.005]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.009]
 [ 0.084]
 [-0.175]
 [ 0.006]
 [-0.004]
 [ 0.031]
 [ 0.005]]
maxi score, test score, baseline:  0.09883333333333319 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.697590044657
actor:  0 policy actor:  0  step number:  58 total reward:  0.16666666666666619  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  60 total reward:  0.11333333333333262  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.795]
 [0.795]
 [0.795]
 [0.795]
 [0.795]
 [0.795]
 [0.795]] [[34.597]
 [34.597]
 [34.597]
 [34.597]
 [34.597]
 [34.597]
 [34.597]] [[0.795]
 [0.795]
 [0.795]
 [0.795]
 [0.795]
 [0.795]
 [0.795]]
maxi score, test score, baseline:  0.09851333333333319 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.11 ]
 [0.159]
 [0.053]
 [0.061]
 [0.14 ]
 [0.054]
 [0.097]] [[32.093]
 [34.059]
 [28.53 ]
 [28.802]
 [33.415]
 [28.84 ]
 [32.491]] [[0.277]
 [0.344]
 [0.187]
 [0.198]
 [0.319]
 [0.19 ]
 [0.267]]
printing an ep nov before normalisation:  39.094391091989394
printing an ep nov before normalisation:  47.570312136227194
maxi score, test score, baseline:  0.09851333333333319 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  47 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.09883333333333319 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.90983131388655
maxi score, test score, baseline:  0.09883333333333319 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.43333333333333335  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  41 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0992333333333332 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.29088072494033
printing an ep nov before normalisation:  26.202309360479425
printing an ep nov before normalisation:  42.5315883424547
printing an ep nov before normalisation:  47.337172047685385
maxi score, test score, baseline:  0.0992333333333332 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.03205802598624
maxi score, test score, baseline:  0.0992333333333332 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.663]
 [0.663]
 [0.663]
 [0.663]
 [0.663]
 [0.663]
 [0.663]] [[36.943]
 [36.943]
 [36.943]
 [36.943]
 [36.943]
 [36.943]
 [36.943]] [[1.975]
 [1.975]
 [1.975]
 [1.975]
 [1.975]
 [1.975]
 [1.975]]
printing an ep nov before normalisation:  40.39865055225988
maxi score, test score, baseline:  0.0992333333333332 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.807]
 [0.82 ]
 [0.82 ]
 [0.822]
 [0.776]
 [0.758]
 [0.816]] [[33.21 ]
 [23.47 ]
 [33.398]
 [33.31 ]
 [37.289]
 [36.819]
 [32.459]] [[1.081]
 [1.014]
 [1.096]
 [1.097]
 [1.085]
 [1.063]
 [1.084]]
maxi score, test score, baseline:  0.0992333333333332 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.071221351623535
printing an ep nov before normalisation:  33.82997369816027
actor:  1 policy actor:  1  step number:  62 total reward:  0.16666666666666607  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 34.3780254293137
printing an ep nov before normalisation:  36.711496094436235
actor:  1 policy actor:  1  step number:  54 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  37 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0992333333333332 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76488984
maxi score, test score, baseline:  0.0992333333333332 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0992333333333332 0.6946666666666668 0.6946666666666668
maxi score, test score, baseline:  0.0992333333333332 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]] [[38.743]
 [38.743]
 [38.743]
 [38.743]
 [38.743]
 [38.743]
 [38.743]] [[0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]]
printing an ep nov before normalisation:  30.534123651768624
maxi score, test score, baseline:  0.0992333333333332 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.894906997680664
actor:  0 policy actor:  0  step number:  44 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  53 total reward:  0.26666666666666605  reward:  1.0 rdn_beta:  1.333
siam score:  -0.7647531
maxi score, test score, baseline:  0.09892666666666654 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.566]
 [0.488]
 [0.488]
 [0.465]
 [0.488]
 [0.442]] [[43.23 ]
 [45.188]
 [43.23 ]
 [43.23 ]
 [47.107]
 [43.23 ]
 [41.27 ]] [[0.488]
 [0.566]
 [0.488]
 [0.488]
 [0.465]
 [0.488]
 [0.442]]
printing an ep nov before normalisation:  51.825293099533155
Printing some Q and Qe and total Qs values:  [[0.942]
 [0.965]
 [0.942]
 [0.942]
 [0.942]
 [0.942]
 [0.884]] [[50.003]
 [35.18 ]
 [50.003]
 [50.003]
 [50.003]
 [50.003]
 [36.549]] [[0.942]
 [0.965]
 [0.942]
 [0.942]
 [0.942]
 [0.942]
 [0.884]]
printing an ep nov before normalisation:  43.37478202097473
siam score:  -0.76337934
printing an ep nov before normalisation:  47.96462151659712
siam score:  -0.7628399
printing an ep nov before normalisation:  34.954689745583934
actor:  0 policy actor:  0  step number:  39 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  40 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0967133333333332 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0967133333333332 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.213]
 [0.415]
 [0.317]
 [0.185]
 [0.224]
 [0.317]
 [0.235]] [[36.43 ]
 [40.108]
 [40.444]
 [33.321]
 [33.901]
 [40.444]
 [34.892]] [[1.346]
 [1.796]
 [1.721]
 [1.107]
 [1.185]
 [1.721]
 [1.263]]
maxi score, test score, baseline:  0.0967133333333332 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  12.898888951430378
siam score:  -0.765555
maxi score, test score, baseline:  0.0967133333333332 0.6946666666666668 0.6946666666666668
actor:  1 policy actor:  1  step number:  83 total reward:  0.11999999999999833  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0967133333333332 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0967133333333332 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0967133333333332 0.6946666666666668 0.6946666666666668
printing an ep nov before normalisation:  33.50357532501221
maxi score, test score, baseline:  0.0967133333333332 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0967133333333332 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.4066666666666662  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.357]] [[39.706]
 [39.706]
 [39.706]
 [39.706]
 [39.706]
 [39.706]
 [39.706]] [[0.519]
 [0.519]
 [0.519]
 [0.519]
 [0.519]
 [0.519]
 [0.519]]
Printing some Q and Qe and total Qs values:  [[0.239]
 [0.263]
 [0.231]
 [0.285]
 [0.239]
 [0.079]
 [0.258]] [[42.219]
 [45.535]
 [40.895]
 [47.194]
 [42.219]
 [43.366]
 [46.606]] [[1.165]
 [1.356]
 [1.091]
 [1.461]
 [1.165]
 [1.063]
 [1.404]]
Printing some Q and Qe and total Qs values:  [[0.27]
 [0.36]
 [0.27]
 [0.27]
 [0.27]
 [0.27]
 [0.27]] [[49.092]
 [48.74 ]
 [49.092]
 [49.092]
 [49.092]
 [49.092]
 [49.092]] [[1.541]
 [1.614]
 [1.541]
 [1.541]
 [1.541]
 [1.541]
 [1.541]]
maxi score, test score, baseline:  0.0967133333333332 0.6946666666666668 0.6946666666666668
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]] [[34.075]
 [34.075]
 [34.075]
 [34.075]
 [34.075]
 [34.075]
 [34.075]] [[0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]]
maxi score, test score, baseline:  0.0967133333333332 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.797518730163574
printing an ep nov before normalisation:  39.53505817087498
actor:  1 policy actor:  1  step number:  65 total reward:  0.14666666666666606  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  55 total reward:  0.18666666666666598  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  4  action  0 :  tensor([0.2844, 0.0483, 0.1023, 0.1645, 0.1888, 0.1154, 0.0963],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0054, 0.9470, 0.0054, 0.0110, 0.0058, 0.0096, 0.0159],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0718, 0.0083, 0.5174, 0.1103, 0.0858, 0.1300, 0.0764],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1030, 0.1571, 0.1038, 0.2787, 0.1273, 0.1137, 0.1163],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1081, 0.0047, 0.0713, 0.1247, 0.5518, 0.0753, 0.0641],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0527, 0.0188, 0.1050, 0.1868, 0.0947, 0.4598, 0.0823],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0828, 0.3462, 0.0815, 0.1068, 0.0860, 0.0824, 0.2142],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  48 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0967133333333332 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09388666666666654 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.40580312756122
printing an ep nov before normalisation:  45.566548268167736
maxi score, test score, baseline:  0.09388666666666654 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.4199999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.09388666666666654 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09388666666666654 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  54 total reward:  0.16666666666666619  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.45848130369122
maxi score, test score, baseline:  0.09621999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09621999999999986 0.6946666666666668 0.6946666666666668
actions average: 
K:  0  action  0 :  tensor([0.4657, 0.0119, 0.0923, 0.0794, 0.1557, 0.0805, 0.1145],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0026, 0.9674, 0.0043, 0.0041, 0.0017, 0.0021, 0.0178],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1175, 0.0048, 0.2838, 0.1410, 0.1235, 0.1799, 0.1495],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1759, 0.0032, 0.1301, 0.2412, 0.1443, 0.1508, 0.1544],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0981, 0.0089, 0.0752, 0.0983, 0.5520, 0.0894, 0.0781],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1403, 0.0515, 0.1248, 0.1109, 0.1019, 0.3478, 0.1228],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2043, 0.0770, 0.0992, 0.1014, 0.1117, 0.1302, 0.2763],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  53 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.09621999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.196]
 [0.289]
 [0.261]
 [0.261]
 [0.261]
 [0.261]
 [0.261]] [[36.695]
 [33.379]
 [33.986]
 [33.986]
 [33.986]
 [33.986]
 [33.986]] [[0.407]
 [0.464]
 [0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.442]]
maxi score, test score, baseline:  0.09621999999999986 0.6946666666666668 0.6946666666666668
printing an ep nov before normalisation:  38.984719796471715
maxi score, test score, baseline:  0.09621999999999986 0.6946666666666668 0.6946666666666668
actor:  1 policy actor:  1  step number:  64 total reward:  0.16666666666666607  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  61 total reward:  0.306666666666666  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  33.41713264428545
maxi score, test score, baseline:  0.09621999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09621999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.481398105621338
maxi score, test score, baseline:  0.09621999999999986 0.6946666666666668 0.6946666666666668
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.474]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]] [[29.478]
 [33.028]
 [29.478]
 [29.478]
 [29.478]
 [29.478]
 [29.478]] [[1.405]
 [1.687]
 [1.405]
 [1.405]
 [1.405]
 [1.405]
 [1.405]]
maxi score, test score, baseline:  0.09621999999999986 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  63 total reward:  0.18666666666666587  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  73 total reward:  0.18666666666666532  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.09624666666666654 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.09624666666666654 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.223]
 [0.24 ]
 [0.199]
 [0.193]
 [0.243]
 [0.195]
 [0.199]] [[38.901]
 [41.091]
 [38.851]
 [37.808]
 [40.503]
 [37.393]
 [37.451]] [[1.272]
 [1.407]
 [1.245]
 [1.184]
 [1.377]
 [1.163]
 [1.17 ]]
Printing some Q and Qe and total Qs values:  [[0.827]
 [0.895]
 [0.827]
 [0.806]
 [0.827]
 [0.827]
 [0.816]] [[20.835]
 [33.753]
 [20.835]
 [25.486]
 [20.835]
 [20.835]
 [24.409]] [[0.827]
 [0.895]
 [0.827]
 [0.806]
 [0.827]
 [0.827]
 [0.816]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.49046431306771
maxi score, test score, baseline:  0.09624666666666654 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09624666666666654 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  55 total reward:  0.17333333333333267  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  51 total reward:  0.45333333333333337  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  33.80668735738337
maxi score, test score, baseline:  0.09576666666666653 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.045762274617346
actor:  1 policy actor:  1  step number:  66 total reward:  0.206666666666666  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.09331333333333319 0.6946666666666668 0.6946666666666668
maxi score, test score, baseline:  0.09331333333333319 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7632083
maxi score, test score, baseline:  0.09331333333333319 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.29130172729492
printing an ep nov before normalisation:  42.28081786600786
maxi score, test score, baseline:  0.09331333333333319 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.871]
 [0.999]
 [0.786]
 [0.903]
 [0.937]
 [0.937]
 [0.975]] [[36.563]
 [41.049]
 [36.88 ]
 [36.218]
 [39.152]
 [39.152]
 [41.079]] [[0.871]
 [0.999]
 [0.786]
 [0.903]
 [0.937]
 [0.937]
 [0.975]]
maxi score, test score, baseline:  0.09331333333333319 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  45 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.09376666666666654 0.6946666666666668 0.6946666666666668
maxi score, test score, baseline:  0.09376666666666654 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.87228292576774
maxi score, test score, baseline:  0.09376666666666654 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09376666666666654 0.6946666666666668 0.6946666666666668
printing an ep nov before normalisation:  42.27795853499666
maxi score, test score, baseline:  0.09376666666666654 0.6946666666666668 0.6946666666666668
actor:  1 policy actor:  1  step number:  66 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  34.96118927804944
maxi score, test score, baseline:  0.09376666666666654 0.6946666666666668 0.6946666666666668
printing an ep nov before normalisation:  32.88709362213302
printing an ep nov before normalisation:  45.62021111981531
actor:  1 policy actor:  1  step number:  45 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  1  action  0 :  tensor([0.4250, 0.0124, 0.1055, 0.1058, 0.1257, 0.1241, 0.1016],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0085, 0.9378, 0.0069, 0.0117, 0.0042, 0.0044, 0.0264],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0522, 0.0183, 0.6258, 0.0730, 0.0542, 0.1279, 0.0486],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1110, 0.1032, 0.1082, 0.3523, 0.0989, 0.1052, 0.1212],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1868, 0.0742, 0.0412, 0.0465, 0.5275, 0.0528, 0.0709],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1006, 0.0020, 0.1681, 0.1001, 0.0879, 0.4513, 0.0899],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1217, 0.1842, 0.1034, 0.1005, 0.0968, 0.0888, 0.3047],
       grad_fn=<DivBackward0>)
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.116]
 [0.061]
 [0.026]
 [0.061]
 [0.061]
 [0.086]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.015]
 [0.116]
 [0.061]
 [0.026]
 [0.061]
 [0.061]
 [0.086]]
printing an ep nov before normalisation:  38.98900388592555
Printing some Q and Qe and total Qs values:  [[0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]] [[28.195]
 [28.195]
 [28.195]
 [28.195]
 [28.195]
 [28.195]
 [28.195]] [[0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]]
printing an ep nov before normalisation:  30.336922732807214
maxi score, test score, baseline:  0.09376666666666654 0.6946666666666668 0.6946666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.09185956569926
printing an ep nov before normalisation:  45.788397724535855
using explorer policy with actor:  1
siam score:  -0.77214414
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  41.349828113153265
Printing some Q and Qe and total Qs values:  [[-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]] [[40.649]
 [40.649]
 [40.649]
 [40.649]
 [40.649]
 [40.649]
 [40.649]] [[1.982]
 [1.982]
 [1.982]
 [1.982]
 [1.982]
 [1.982]
 [1.982]]
printing an ep nov before normalisation:  51.604688230808435
printing an ep nov before normalisation:  39.315950145126976
printing an ep nov before normalisation:  34.06338738526794
printing an ep nov before normalisation:  32.799676012617205
maxi score, test score, baseline:  0.09376666666666654 0.6946666666666668 0.6946666666666668
printing an ep nov before normalisation:  36.887197494506836
printing an ep nov before normalisation:  24.962973594665527
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  0.011808677247699961
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333334  reward:  1.0 rdn_beta:  2
actions average: 
K:  2  action  0 :  tensor([0.3140, 0.0062, 0.1221, 0.1114, 0.1742, 0.1462, 0.1259],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0095, 0.9246, 0.0076, 0.0179, 0.0056, 0.0061, 0.0286],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1420, 0.0269, 0.2880, 0.1127, 0.1003, 0.2033, 0.1268],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1042, 0.0060, 0.1292, 0.4118, 0.1199, 0.0994, 0.1296],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2053, 0.0057, 0.0868, 0.0873, 0.4593, 0.0808, 0.0749],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0643, 0.0036, 0.0903, 0.0785, 0.0624, 0.6425, 0.0584],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0714, 0.4039, 0.0802, 0.0827, 0.0682, 0.0744, 0.2192],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  32.94172512537419
maxi score, test score, baseline:  0.09848666666666654 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.34641418990467
maxi score, test score, baseline:  0.09848666666666654 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.34949346788633
actions average: 
K:  4  action  0 :  tensor([0.2974, 0.0256, 0.1064, 0.0986, 0.2804, 0.0902, 0.1013],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0073, 0.9518, 0.0045, 0.0060, 0.0031, 0.0032, 0.0240],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1010, 0.0275, 0.3132, 0.1458, 0.0927, 0.1435, 0.1763],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1149, 0.1528, 0.1256, 0.1854, 0.1153, 0.1248, 0.1814],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1097, 0.1817, 0.1167, 0.0878, 0.3387, 0.0982, 0.0672],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0506, 0.1659, 0.1828, 0.0672, 0.0531, 0.3873, 0.0931],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0616, 0.4317, 0.0603, 0.0835, 0.0772, 0.0553, 0.2305],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  55 total reward:  0.1999999999999994  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  65 total reward:  0.14666666666666617  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  50 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  44.04958997453962
printing an ep nov before normalisation:  31.61557418230164
siam score:  -0.76308775
maxi score, test score, baseline:  0.09848666666666654 0.6993333333333334 0.6993333333333334
maxi score, test score, baseline:  0.09848666666666654 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.196407133327526
printing an ep nov before normalisation:  49.173410888867515
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11619
maxi score, test score, baseline:  0.09848666666666654 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.355]
 [0.332]
 [0.32 ]
 [0.319]
 [0.324]
 [0.319]] [[44.273]
 [32.403]
 [31.584]
 [31.618]
 [31.655]
 [31.892]
 [31.735]] [[3.785]
 [2.355]
 [2.233]
 [2.225]
 [2.229]
 [2.262]
 [2.239]]
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.561]
 [0.464]
 [0.464]
 [0.464]
 [0.464]
 [0.464]] [[51.181]
 [46.077]
 [51.181]
 [51.181]
 [51.181]
 [51.181]
 [51.181]] [[2.455]
 [2.24 ]
 [2.455]
 [2.455]
 [2.455]
 [2.455]
 [2.455]]
printing an ep nov before normalisation:  34.3405389544171
printing an ep nov before normalisation:  32.19724193560943
maxi score, test score, baseline:  0.09848666666666654 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.686]
 [0.686]
 [0.676]
 [0.686]
 [0.686]
 [0.686]
 [0.686]] [[28.707]
 [28.707]
 [28.717]
 [28.707]
 [28.707]
 [28.707]
 [28.707]] [[2.515]
 [2.515]
 [2.505]
 [2.515]
 [2.515]
 [2.515]
 [2.515]]
siam score:  -0.75951815
actor:  1 policy actor:  1  step number:  45 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  59 total reward:  0.31999999999999984  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09513999999999988 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.146]
 [ 0.163]
 [ 0.097]
 [ 0.073]
 [ 0.097]
 [-0.013]
 [ 0.099]] [[24.852]
 [38.357]
 [38.014]
 [32.259]
 [38.014]
 [36.413]
 [34.026]] [[0.432]
 [1.055]
 [0.981]
 [0.823]
 [0.981]
 [0.833]
 [0.89 ]]
Printing some Q and Qe and total Qs values:  [[0.786]
 [0.786]
 [0.786]
 [0.786]
 [0.786]
 [0.786]
 [0.786]] [[35.7]
 [35.7]
 [35.7]
 [35.7]
 [35.7]
 [35.7]
 [35.7]] [[0.786]
 [0.786]
 [0.786]
 [0.786]
 [0.786]
 [0.786]
 [0.786]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  65.60187035147142
printing an ep nov before normalisation:  66.21021136314643
actor:  0 policy actor:  0  step number:  46 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0943133333333332 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.42506780032881
actor:  1 policy actor:  1  step number:  57 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  40.75948043017448
printing an ep nov before normalisation:  26.736206346249684
maxi score, test score, baseline:  0.0943133333333332 0.6993333333333334 0.6993333333333334
maxi score, test score, baseline:  0.0943133333333332 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.56  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.03 ]
 [0.032]
 [0.03 ]
 [0.03 ]
 [0.03 ]
 [0.03 ]
 [0.03 ]] [[48.464]
 [50.455]
 [48.464]
 [48.464]
 [48.464]
 [48.464]
 [48.464]] [[0.553]
 [0.59 ]
 [0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.553]]
maxi score, test score, baseline:  0.0943133333333332 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.99]
 [0.99]
 [0.99]
 [0.99]
 [0.99]
 [0.99]
 [0.99]] [[29.834]
 [29.834]
 [29.834]
 [29.834]
 [29.834]
 [29.834]
 [29.834]] [[0.99]
 [0.99]
 [0.99]
 [0.99]
 [0.99]
 [0.99]
 [0.99]]
maxi score, test score, baseline:  0.09203333333333322 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  67 total reward:  0.07999999999999896  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.09148666666666655 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.4545, 0.0009, 0.1019, 0.0824, 0.1979, 0.0807, 0.0817],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0049, 0.9587, 0.0043, 0.0055, 0.0026, 0.0027, 0.0212],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1514, 0.0020, 0.3941, 0.1220, 0.0920, 0.1284, 0.1101],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1513, 0.0119, 0.1354, 0.3315, 0.1270, 0.1290, 0.1139],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1615, 0.0017, 0.0605, 0.0733, 0.5812, 0.0655, 0.0564],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0946, 0.0044, 0.1235, 0.1007, 0.0877, 0.5122, 0.0769],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0600, 0.2159, 0.0922, 0.2441, 0.0489, 0.1110, 0.2277],
       grad_fn=<DivBackward0>)
actions average: 
K:  1  action  0 :  tensor([0.4877, 0.0035, 0.1091, 0.0873, 0.1472, 0.0980, 0.0673],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0084, 0.9298, 0.0080, 0.0135, 0.0044, 0.0071, 0.0289],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1272, 0.0843, 0.4071, 0.0884, 0.0840, 0.1061, 0.1027],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1543, 0.0173, 0.1533, 0.3035, 0.1168, 0.1271, 0.1277],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2031, 0.0079, 0.0993, 0.0898, 0.4083, 0.1004, 0.0912],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1092, 0.0026, 0.1846, 0.0955, 0.0889, 0.4261, 0.0931],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1334, 0.1523, 0.0957, 0.0923, 0.0740, 0.0617, 0.3907],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  36.29455666269842
maxi score, test score, baseline:  0.09148666666666655 0.6993333333333334 0.6993333333333334
maxi score, test score, baseline:  0.09148666666666655 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7659945
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.912946495640114
siam score:  -0.76631284
maxi score, test score, baseline:  0.09148666666666655 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09148666666666655 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09148666666666655 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.45 ]
 [0.733]
 [0.679]
 [0.637]
 [0.679]
 [0.679]
 [0.604]] [[31.689]
 [36.647]
 [32.222]
 [31.23 ]
 [32.222]
 [32.222]
 [36.573]] [[1.537]
 [2.115]
 [1.798]
 [1.697]
 [1.798]
 [1.798]
 [1.982]]
maxi score, test score, baseline:  0.09148666666666655 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.55 ]
 [0.552]
 [0.483]
 [0.517]
 [0.431]
 [0.483]
 [0.514]] [[32.385]
 [32.773]
 [28.925]
 [29.869]
 [30.909]
 [29.577]
 [28.893]] [[1.521]
 [1.554]
 [1.18 ]
 [1.289]
 [1.285]
 [1.232]
 [1.209]]
printing an ep nov before normalisation:  36.79983474438841
printing an ep nov before normalisation:  41.639860060330875
actor:  1 policy actor:  1  step number:  61 total reward:  0.17333333333333278  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  45.66502230568207
actor:  1 policy actor:  1  step number:  65 total reward:  0.15999999999999936  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.09148666666666655 0.6993333333333334 0.6993333333333334
printing an ep nov before normalisation:  61.858427058017185
maxi score, test score, baseline:  0.09148666666666655 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  73 total reward:  0.119999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.09148666666666655 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09148666666666655 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.1730041292886
actor:  0 policy actor:  0  step number:  43 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.09167333333333322 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09167333333333322 0.6993333333333334 0.6993333333333334
maxi score, test score, baseline:  0.09167333333333322 0.6993333333333334 0.6993333333333334
printing an ep nov before normalisation:  50.70758890393419
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09167333333333322 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.39333333333333287  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.326]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.305]] [[37.88 ]
 [31.74 ]
 [37.88 ]
 [37.88 ]
 [37.88 ]
 [37.88 ]
 [35.812]] [[0.531]
 [0.474]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.5  ]]
maxi score, test score, baseline:  0.09167333333333322 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09167333333333322 0.6993333333333334 0.6993333333333334
printing an ep nov before normalisation:  39.14048952789476
printing an ep nov before normalisation:  36.3277524160761
maxi score, test score, baseline:  0.09167333333333322 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09167333333333322 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09167333333333322 0.6993333333333334 0.6993333333333334
maxi score, test score, baseline:  0.09167333333333322 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0916733333333332 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.53336900613237
maxi score, test score, baseline:  0.0916733333333332 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76412237
maxi score, test score, baseline:  0.0916733333333332 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0916733333333332 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.37333333333333285  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0916733333333332 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.415919389169705
printing an ep nov before normalisation:  35.59520266496698
maxi score, test score, baseline:  0.0916733333333332 0.6993333333333334 0.6993333333333334
actor:  0 policy actor:  0  step number:  48 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.683]
 [0.599]
 [0.606]
 [0.596]
 [0.604]
 [0.596]] [[24.884]
 [41.948]
 [35.48 ]
 [37.684]
 [34.777]
 [37.596]
 [33.882]] [[0.581]
 [0.683]
 [0.599]
 [0.606]
 [0.596]
 [0.604]
 [0.596]]
printing an ep nov before normalisation:  44.42476452407396
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.655]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]] [[40.917]
 [41.03 ]
 [40.917]
 [40.917]
 [40.917]
 [40.917]
 [40.917]] [[1.405]
 [1.518]
 [1.405]
 [1.405]
 [1.405]
 [1.405]
 [1.405]]
maxi score, test score, baseline:  0.09445999999999986 0.6993333333333334 0.6993333333333334
printing an ep nov before normalisation:  54.722341745127835
maxi score, test score, baseline:  0.09445999999999986 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.31999999999999995  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.672]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]] [[40.151]
 [44.501]
 [40.151]
 [40.151]
 [40.151]
 [40.151]
 [40.151]] [[0.994]
 [1.205]
 [0.994]
 [0.994]
 [0.994]
 [0.994]
 [0.994]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.018]
 [-0.019]
 [-0.022]
 [-0.02 ]
 [-0.018]
 [-0.018]
 [-0.011]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.018]
 [-0.019]
 [-0.022]
 [-0.02 ]
 [-0.018]
 [-0.018]
 [-0.011]]
printing an ep nov before normalisation:  51.266025628975754
maxi score, test score, baseline:  0.09445999999999988 0.6993333333333334 0.6993333333333334
maxi score, test score, baseline:  0.09445999999999988 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.186171582249465
maxi score, test score, baseline:  0.09445999999999988 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.18065719462915
actor:  1 policy actor:  1  step number:  45 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.09445999999999988 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.302243686960026
printing an ep nov before normalisation:  45.217295549215635
maxi score, test score, baseline:  0.09445999999999988 0.6993333333333334 0.6993333333333334
actor:  1 policy actor:  1  step number:  49 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  41.18712018184147
printing an ep nov before normalisation:  43.61558830804811
printing an ep nov before normalisation:  28.562929727625175
maxi score, test score, baseline:  0.09445999999999988 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09445999999999988 0.6993333333333334 0.6993333333333334
actor:  1 policy actor:  1  step number:  69 total reward:  0.2133333333333327  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  53.35276685771379
actor:  1 policy actor:  1  step number:  42 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.09445999999999988 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.613]
 [0.776]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.613]
 [0.776]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]]
Printing some Q and Qe and total Qs values:  [[0.827]
 [0.827]
 [0.827]
 [0.827]
 [0.827]
 [0.827]
 [0.827]] [[41.558]
 [41.558]
 [41.558]
 [41.558]
 [41.558]
 [41.558]
 [41.558]] [[0.827]
 [0.827]
 [0.827]
 [0.827]
 [0.827]
 [0.827]
 [0.827]]
maxi score, test score, baseline:  0.09224666666666653 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  41 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09268666666666654 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09268666666666654 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.26579761505127
maxi score, test score, baseline:  0.09268666666666654 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09268666666666654 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.832666801091584
printing an ep nov before normalisation:  54.26608085632324
actor:  1 policy actor:  1  step number:  48 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  35.104780713698545
maxi score, test score, baseline:  0.09268666666666654 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  65 total reward:  0.3199999999999994  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09268666666666654 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  29 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.017]
 [-0.007]
 [-0.024]
 [-0.022]
 [-0.022]
 [-0.023]
 [-0.023]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.017]
 [-0.007]
 [-0.024]
 [-0.022]
 [-0.022]
 [-0.023]
 [-0.023]]
maxi score, test score, baseline:  0.09593999999999987 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.3847263172935
printing an ep nov before normalisation:  43.341057180672735
printing an ep nov before normalisation:  47.48314815504394
printing an ep nov before normalisation:  46.17378160254615
maxi score, test score, baseline:  0.09593999999999987 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.8655256528946
siam score:  -0.7557152
actor:  1 policy actor:  1  step number:  47 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09593999999999987 0.6993333333333334 0.6993333333333334
printing an ep nov before normalisation:  35.98289461639714
printing an ep nov before normalisation:  34.0442156791687
line 256 mcts: sample exp_bonus 39.22627597407618
printing an ep nov before normalisation:  38.20655391282237
actor:  1 policy actor:  1  step number:  47 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  56 total reward:  0.25999999999999923  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09593999999999987 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.629597325647175
actions average: 
K:  4  action  0 :  tensor([0.4402, 0.0341, 0.1111, 0.0911, 0.1268, 0.1009, 0.0958],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0135, 0.8923, 0.0162, 0.0165, 0.0110, 0.0159, 0.0346],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1267, 0.0458, 0.2764, 0.1428, 0.1407, 0.1664, 0.1013],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1098, 0.0168, 0.1462, 0.4563, 0.1005, 0.0814, 0.0890],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1589, 0.0565, 0.0975, 0.1059, 0.3825, 0.1080, 0.0906],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0922, 0.0294, 0.1779, 0.1449, 0.1117, 0.3146, 0.1292],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1625, 0.1550, 0.1488, 0.1535, 0.0956, 0.0996, 0.1849],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.518]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]] [[37.811]
 [41.511]
 [37.811]
 [37.811]
 [37.811]
 [37.811]
 [37.811]] [[0.631]
 [0.879]
 [0.631]
 [0.631]
 [0.631]
 [0.631]
 [0.631]]
printing an ep nov before normalisation:  50.76063148101873
printing an ep nov before normalisation:  22.628803253173828
actions average: 
K:  4  action  0 :  tensor([0.4906, 0.0097, 0.0731, 0.0627, 0.2157, 0.0848, 0.0634],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0204, 0.8827, 0.0133, 0.0276, 0.0090, 0.0098, 0.0373],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1284, 0.0190, 0.3027, 0.1406, 0.1545, 0.1397, 0.1152],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0458, 0.2512, 0.0504, 0.3894, 0.0422, 0.0530, 0.1681],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1335, 0.0304, 0.1421, 0.1223, 0.3244, 0.1277, 0.1197],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1146, 0.0583, 0.2354, 0.1100, 0.0812, 0.2326, 0.1679],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1904, 0.0015, 0.2354, 0.1312, 0.1367, 0.1683, 0.1366],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.09593999999999987 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.678]
 [0.75 ]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]] [[32.11 ]
 [30.392]
 [32.11 ]
 [32.11 ]
 [32.11 ]
 [32.11 ]
 [32.11 ]] [[1.193]
 [1.212]
 [1.193]
 [1.193]
 [1.193]
 [1.193]
 [1.193]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  35.882973816223476
maxi score, test score, baseline:  0.09593999999999987 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09593999999999987 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.401861084624336
siam score:  -0.7515559
actor:  1 policy actor:  1  step number:  57 total reward:  0.18666666666666631  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.284]
 [0.468]
 [0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]] [[34.043]
 [37.798]
 [34.043]
 [34.043]
 [34.043]
 [34.043]
 [34.043]] [[0.922]
 [1.233]
 [0.922]
 [0.922]
 [0.922]
 [0.922]
 [0.922]]
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]] [[43.727]
 [43.727]
 [43.727]
 [43.727]
 [43.727]
 [43.727]
 [43.727]] [[2.006]
 [2.006]
 [2.006]
 [2.006]
 [2.006]
 [2.006]
 [2.006]]
maxi score, test score, baseline:  0.09593999999999987 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.506]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]] [[49.244]
 [53.084]
 [49.244]
 [49.244]
 [49.244]
 [49.244]
 [49.244]] [[2.151]
 [2.309]
 [2.151]
 [2.151]
 [2.151]
 [2.151]
 [2.151]]
maxi score, test score, baseline:  0.09593999999999987 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.21796764473629
maxi score, test score, baseline:  0.09593999999999987 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.004996839371785
actor:  1 policy actor:  1  step number:  48 total reward:  0.33999999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09593999999999987 0.6993333333333334 0.6993333333333334
maxi score, test score, baseline:  0.09593999999999987 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  69 total reward:  0.31999999999999895  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.09593999999999987 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09593999999999987 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09593999999999987 0.6993333333333334 0.6993333333333334
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.46253705145655
actor:  1 policy actor:  1  step number:  57 total reward:  0.23999999999999921  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  