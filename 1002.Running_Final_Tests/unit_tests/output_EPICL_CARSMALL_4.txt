append_mcts_svs:False
dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 50}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:1
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.Car_Driving_Env.RaceWorld'>
same_env_each_time:True
env_size:[6, 30]
observable_size:[6, 9]
game_modes:1
env_map:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 3. 3. 3. 3.
  3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3.
  3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 0. 2. 1. 1. 2. 2. 1. 3. 3. 3. 3.
  3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 3. 3. 3. 3.
  3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]]
max_steps:100
actions_size:7
optimal_score:0.86
total_frames:205000
exp_gamma:0.95
atari_env:False
memory_size:30
reward_clipping:False
image_size:[48, 48]
deque_length:3
PRESET_CONFIG:7
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:episodic
rdn_beta:[0.3333333333333333, 2, 6]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
contrast_vector:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 180)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:False
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 3. 3. 3. 3.
  3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3.
  3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 0. 2. 1. 1. 2. 2. 1. 3. 3. 3. 3.
  3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 3. 3. 3. 3.
  3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0.]]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
printing an ep nov before normalisation:  8.044362319509588
using explorer policy with actor:  1
printing an ep nov before normalisation:  7.02942687631662
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.012]] [[11.784]
 [11.784]
 [11.784]
 [11.784]
 [11.784]
 [11.784]
 [11.784]] [[0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]] [[11.789]
 [11.789]
 [11.789]
 [11.789]
 [11.789]
 [11.789]
 [11.789]] [[0.34]
 [0.34]
 [0.34]
 [0.34]
 [0.34]
 [0.34]
 [0.34]]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  9.899182617664337
using explorer policy with actor:  1
Starting evaluation
printing an ep nov before normalisation:  11.743626594543457
siam score:  0.004466055913574316
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.1977102784743119, 0.34556748110453983, 0.04985307584408401, 0.09209799088129195, 0.26491809785168824, 0.04985307584408401]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.19771027865740032, 0.34556748215965766, 0.049853075155142985, 0.09209799044150223, 0.26491809843115366, 0.049853075155142985]
printing an ep nov before normalisation:  7.726569681285582
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.22396382431847986, 0.3614770026828314, 0.08645064595412824, 0.05207235136304028, 0.22396382431847986, 0.05207235136304028]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
actions average: 
K:  4  action  0 :  tensor([0.1692, 0.1052, 0.1948, 0.0954, 0.1220, 0.1551, 0.1583],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.1956, 0.2089, 0.2002, 0.0747, 0.1222, 0.1113, 0.0870],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1209, 0.2375, 0.1620, 0.1275, 0.1272, 0.0787, 0.1462],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0977, 0.2411, 0.1724, 0.0875, 0.1101, 0.1206, 0.1705],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1953, 0.2065, 0.1711, 0.0875, 0.1277, 0.1278, 0.0841],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1947, 0.1561, 0.1662, 0.1164, 0.1010, 0.0839, 0.1818],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1482, 0.1472, 0.1824, 0.1184, 0.1328, 0.1065, 0.1644],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
siam score:  -0.12417540068701835
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
actions average: 
K:  4  action  0 :  tensor([0.2189, 0.1454, 0.1344, 0.1073, 0.1287, 0.1325, 0.1329],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0827, 0.4827, 0.1274, 0.0568, 0.0760, 0.0857, 0.0887],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.3214, 0.0835, 0.1309, 0.0939, 0.1290, 0.1252, 0.1160],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2468, 0.1331, 0.1301, 0.0890, 0.1562, 0.1329, 0.1120],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1514, 0.1025, 0.1889, 0.1278, 0.1451, 0.1334, 0.1508],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1029, 0.1022, 0.2534, 0.0578, 0.0798, 0.3406, 0.0633],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.3222, 0.0744, 0.1221, 0.0984, 0.1511, 0.1065, 0.1253],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
deleting a thread, now have 2 threads
Frames:  2255 train batches done:  62 episodes:  95
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
siam score:  -0.3796121
siam score:  -0.3795141
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
siam score:  -0.38341027
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
deleting a thread, now have 1 threads
Frames:  2255 train batches done:  171 episodes:  95
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
siam score:  -0.43406945
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.248789565681452, 0.32233246032155816, 0.0869951974732183, 0.04654660542115977, 0.248789565681452, 0.04654660542115977]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
printing an ep nov before normalisation:  17.47206687927246
UNIT TEST: sample policy line 217 mcts : [0.  0.2 0.2 0.6 0.  0.  0. ]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.16405389517303223, 0.3208201847911034, 0.1013473793258037, 0.046479177959478674, 0.3208201847911034, 0.046479177959478674]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.14117958536561367, 0.14117958536561367, 0.14117958536561367, 0.06471834146245449, 0.44702456097825005, 0.06471834146245449]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.06552220342438535, 0.17663865600041273, 0.17663865600041273, 0.06552220342438535, 0.45015607772601857, 0.06552220342438535]
using explorer policy with actor:  1
printing an ep nov before normalisation:  20.957489013671875
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
probs:  [0.21290623456529592, 0.21290623456529592, 0.21290623456529592, 0.031504852809134815, 0.1168702089296813, 0.21290623456529592]
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
probs:  [0.21290623456529592, 0.21290623456529592, 0.21290623456529592, 0.031504852809134815, 0.1168702089296813, 0.21290623456529592]
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
probs:  [0.21290623456529592, 0.21290623456529592, 0.21290623456529592, 0.031504852809134815, 0.1168702089296813, 0.21290623456529592]
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
probs:  [0.21290623456529592, 0.21290623456529592, 0.21290623456529592, 0.031504852809134815, 0.1168702089296813, 0.21290623456529592]
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
probs:  [0.21290623456529592, 0.21290623456529592, 0.21290623456529592, 0.031504852809134815, 0.1168702089296813, 0.21290623456529592]
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
probs:  [0.21290623456529592, 0.21290623456529592, 0.21290623456529592, 0.031504852809134815, 0.1168702089296813, 0.21290623456529592]
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
probs:  [0.21290623456529592, 0.21290623456529592, 0.21290623456529592, 0.031504852809134815, 0.1168702089296813, 0.21290623456529592]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.23553098613078013, 0.12928317895757638, 0.23553098613078013, 0.034840683692506945, 0.12928317895757638, 0.23553098613078013]
UNIT TEST: sample policy line 217 mcts : [0.6 0.  0.2 0.  0.2 0.  0. ]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.14465005917990165, 0.14465005917990165, 0.26353973960843247, 0.038970343243430246, 0.14465005917990165, 0.26353973960843247]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
maxi score, test score, baseline:  -0.9776777777777778 -1.0 -0.9776777777777778
probs:  [0.14465005881022763, 0.14465005881022763, 0.26353974123499796, 0.038970341099321036, 0.14465005881022763, 0.26353974123499796]
using another actor
from probs:  [0.1940313663588972, 0.1940313663588972, 0.1940313663588972, 0.1940313663588972, 0.1940313663588972, 0.029843168205513866]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.1940313678493995, 0.1940313678493995, 0.1940313678493995, 0.1940313678493995, 0.1940313678493995, 0.029843160753002418]
from probs:  [0.1940313678493995, 0.1940313678493995, 0.1940313678493995, 0.1940313678493995, 0.1940313678493995, 0.029843160753002418]
maxi score, test score, baseline:  -0.9794918367346939 -1.0 -0.9794918367346939
probs:  [0.1631057021577011, 0.03491097983494082, 0.1631057021577011, 0.23788595684597802, 0.23788595684597802, 0.1631057021577011]
using another actor
from probs:  [0.1631057021577011, 0.03491097983494082, 0.1631057021577011, 0.23788595684597802, 0.23788595684597802, 0.1631057021577011]
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
probs:  [0.19012127107653856, 0.040676004040187064, 0.11166250588245404, 0.27729767684774326, 0.19012127107653856, 0.19012127107653856]
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
siam score:  -0.48999625
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
probs:  [0.19012127107653856, 0.040676004040187064, 0.11166250588245404, 0.27729767684774326, 0.19012127107653856, 0.19012127107653856]
from probs:  [0.1913288858056655, 0.03707099003277708, 0.13476765735560647, 0.25417469519461994, 0.1913288858056655, 0.1913288858056655]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.16214083392260342, 0.03467208581333907, 0.09537148967489358, 0.235938530196388, 0.235938530196388, 0.235938530196388]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.17506038240220131, 0.0374264408209021, 0.10296641300247325, 0.25474319068611107, 0.25474319068611107, 0.17506038240220131]
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.2063904901334275, 0.04410579153372529, 0.12138421943834535, 0.3003447893227291, 0.2063904901334275, 0.12138421943834535]
maxi score, test score, baseline:  -0.9817181818181818 -1.0 -0.9817181818181818
probs:  [0.20639049061819018, 0.044105790038075625, 0.12138421888574916, 0.3003447909540457, 0.20639049061819018, 0.12138421888574916]
siam score:  -0.48319808
maxi score, test score, baseline:  -0.9817181818181818 -1.0 -0.9817181818181818
probs:  [0.22367962532612468, 0.04779170904939482, 0.04779170904939482, 0.32550947159159976, 0.22367962532612468, 0.1315478596573613]
maxi score, test score, baseline:  -0.9817181818181818 -1.0 -0.9817181818181818
probs:  [0.2595819294597915, 0.03868904055538159, 0.03868904055538159, 0.2595819294597915, 0.2595819294597915, 0.14387613050986217]
printing an ep nov before normalisation:  51.61910602156782
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.1752264970603063, 0.03743410535781555, 0.10319865594309534, 0.2544571222892382, 0.2544571222892382, 0.1752264970603063]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.22380787892404522, 0.047783607792797174, 0.047783607792797174, 0.22380787892404522, 0.32502183482451286, 0.13179519174180226]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.22380787892404522, 0.047783607792797174, 0.047783607792797174, 0.22380787892404522, 0.32502183482451286, 0.13179519174180226]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.22380787892404522, 0.047783607792797174, 0.047783607792797174, 0.22380787892404522, 0.32502183482451286, 0.13179519174180226]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.22380787892404522, 0.047783607792797174, 0.047783607792797174, 0.22380787892404522, 0.32502183482451286, 0.13179519174180226]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.22380787892404522, 0.047783607792797174, 0.047783607792797174, 0.22380787892404522, 0.32502183482451286, 0.13179519174180226]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.22380787892404522, 0.047783607792797174, 0.047783607792797174, 0.22380787892404522, 0.32502183482451286, 0.13179519174180226]
siam score:  -0.526052
siam score:  -0.527587
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.22380787892404522, 0.047783607792797174, 0.047783607792797174, 0.22380787892404522, 0.32502183482451286, 0.13179519174180226]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.22380787892404522, 0.047783607792797174, 0.047783607792797174, 0.22380787892404522, 0.32502183482451286, 0.13179519174180226]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.22380787892404522, 0.047783607792797174, 0.047783607792797174, 0.22380787892404522, 0.32502183482451286, 0.13179519174180226]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.22380787892404522, 0.047783607792797174, 0.047783607792797174, 0.22380787892404522, 0.32502183482451286, 0.13179519174180226]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.22380787892404522, 0.047783607792797174, 0.047783607792797174, 0.22380787892404522, 0.32502183482451286, 0.13179519174180226]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.22380787892404522, 0.047783607792797174, 0.047783607792797174, 0.22380787892404522, 0.32502183482451286, 0.13179519174180226]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.22380787892404522, 0.047783607792797174, 0.047783607792797174, 0.22380787892404522, 0.32502183482451286, 0.13179519174180226]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.22380787892404522, 0.047783607792797174, 0.047783607792797174, 0.22380787892404522, 0.32502183482451286, 0.13179519174180226]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.22380787892404522, 0.047783607792797174, 0.047783607792797174, 0.22380787892404522, 0.32502183482451286, 0.13179519174180226]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.22380787892404522, 0.047783607792797174, 0.047783607792797174, 0.22380787892404522, 0.32502183482451286, 0.13179519174180226]
siam score:  -0.5274772
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
using another actor
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.05836876059244752, 0.2172056895013022, 0.05836876059244752, 0.2172056895013022, 0.39048233922005293, 0.05836876059244752]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.06937583818083898, 0.06937583818083898, 0.06937583818083898, 0.25823450524156333, 0.4642621420350807, 0.06937583818083898]
using another actor
from probs:  [0.06937583818083898, 0.06937583818083898, 0.06937583818083898, 0.25823450524156333, 0.4642621420350807, 0.06937583818083898]
from probs:  [0.14678049625138728, 0.14678049625138728, 0.053031407150785075, 0.14678049625138728, 0.3598466078436657, 0.14678049625138728]
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
probs:  [0.176741375119361, 0.176741375119361, 0.03771039847217845, 0.176741375119361, 0.2553241010503774, 0.176741375119361]
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
probs:  [0.2591667049494598, 0.03892851856185718, 0.03892851856185718, 0.14464284802790614, 0.2591667049494598, 0.2591667049494598]
siam score:  -0.54034126
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.18762080472562856, 0.050466446521515, 0.050466446521515, 0.18762080472562856, 0.33620469278008425, 0.18762080472562856]
siam score:  -0.5417599
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.18762080472562856, 0.050466446521515, 0.050466446521515, 0.18762080472562856, 0.33620469278008425, 0.18762080472562856]
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.21745068746739632, 0.0584746223520686, 0.0584746223520686, 0.0584746223520686, 0.3896747580090015, 0.21745068746739632]
printing an ep nov before normalisation:  74.99494348154802
printing an ep nov before normalisation:  68.40331554412842
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.09680427612627893, 0.23499943855101213, 0.23499943855101213, 0.1633426876641135, 0.03485472055657125, 0.23499943855101213]
printing an ep nov before normalisation:  52.7552604675293
siam score:  -0.54556316
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.09680427535163023, 0.23499943930870024, 0.23499943930870024, 0.16334268762725643, 0.034854719095012615, 0.23499943930870024]
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.09680427459993017, 0.23499944004394194, 0.23499944004394194, 0.16334268759149143, 0.03485471767675246, 0.23499944004394194]
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.03512735708574193, 0.23243632145712906, 0.23243632145712906, 0.23243632145712906, 0.03512735708574193, 0.23243632145712906]
siam score:  -0.5488958
actions average: 
K:  0  action  0 :  tensor([0.2362, 0.0456, 0.1500, 0.1238, 0.1401, 0.1326, 0.1717],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0215, 0.8822, 0.0141, 0.0313, 0.0147, 0.0212, 0.0150],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1261, 0.0212, 0.2489, 0.1299, 0.1099, 0.2026, 0.1614],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1096, 0.1156, 0.1337, 0.2311, 0.1140, 0.1438, 0.1523],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1677, 0.0056, 0.1133, 0.1488, 0.2219, 0.1704, 0.1723],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0967, 0.0123, 0.1441, 0.1036, 0.0880, 0.4169, 0.1384],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1039, 0.0870, 0.1607, 0.1167, 0.0848, 0.1283, 0.3185],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.03512735503649141, 0.23243632248175428, 0.23243632248175428, 0.23243632248175428, 0.03512735503649141, 0.23243632248175428]
siam score:  -0.549117
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.03512735503649141, 0.23243632248175428, 0.23243632248175428, 0.23243632248175428, 0.03512735503649141, 0.23243632248175428]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
from probs:  [0.03703633693719091, 0.23148183153140456, 0.23148183153140456, 0.03703633693719091, 0.23148183153140456, 0.23148183153140456]
printing an ep nov before normalisation:  42.330970764160156
printing an ep nov before normalisation:  27.2680926322937
printing an ep nov before normalisation:  44.49225425720215
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
probs:  [0.03219537459799409, 0.2121569550616451, 0.2121569550616451, 0.11917680515542539, 0.2121569550616451, 0.2121569550616451]
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
siam score:  -0.56168187
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
probs:  [0.03510911491864803, 0.16443687765398846, 0.23579012605969332, 0.16443687765398846, 0.23579012605969332, 0.16443687765398846]
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
probs:  [0.03510911491864803, 0.16443687765398846, 0.23579012605969332, 0.16443687765398846, 0.23579012605969332, 0.16443687765398846]
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
siam score:  -0.5647705
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
probs:  [0.03510911491864803, 0.16443687765398846, 0.23579012605969332, 0.16443687765398846, 0.23579012605969332, 0.16443687765398846]
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
probs:  [0.03510911491864803, 0.16443687765398846, 0.23579012605969332, 0.16443687765398846, 0.23579012605969332, 0.16443687765398846]
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
probs:  [0.03510911491864803, 0.16443687765398846, 0.23579012605969332, 0.16443687765398846, 0.23579012605969332, 0.16443687765398846]
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
probs:  [0.03510911491864803, 0.16443687765398846, 0.23579012605969332, 0.16443687765398846, 0.23579012605969332, 0.16443687765398846]
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
probs:  [0.03510911491864803, 0.16443687765398846, 0.23579012605969332, 0.16443687765398846, 0.23579012605969332, 0.16443687765398846]
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
probs:  [0.03510911491864803, 0.16443687765398846, 0.23579012605969332, 0.16443687765398846, 0.23579012605969332, 0.16443687765398846]
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
probs:  [0.03510911491864803, 0.16443687765398846, 0.23579012605969332, 0.16443687765398846, 0.23579012605969332, 0.16443687765398846]
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
probs:  [0.03510911491864803, 0.16443687765398846, 0.23579012605969332, 0.16443687765398846, 0.23579012605969332, 0.16443687765398846]
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
probs:  [0.03510911491864803, 0.16443687765398846, 0.23579012605969332, 0.16443687765398846, 0.23579012605969332, 0.16443687765398846]
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
probs:  [0.03761287589911156, 0.17619861099769657, 0.25265970622450173, 0.10467048965649185, 0.25265970622450173, 0.17619861099769657]
printing an ep nov before normalisation:  32.65341005165196
from probs:  [0.04412898196299099, 0.2068088419904646, 0.2965632475228641, 0.12284504326660786, 0.2068088419904646, 0.12284504326660786]
from probs:  [0.05268492218246775, 0.14670905401045795, 0.35421058632050273, 0.05268492218246775, 0.24700146129364595, 0.14670905401045795]
from probs:  [0.04430632478951932, 0.16400665923455537, 0.29168701597592533, 0.04430632478951932, 0.29168701597592533, 0.16400665923455537]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9872417721518988 -1.0 -0.9872417721518988
from probs:  [0.12226276060484786, 0.12226276060484786, 0.2946543959036738, 0.12226276060484786, 0.2946543959036738, 0.04390292637810875]
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
probs:  [0.1631795807038091, 0.1631795807038091, 0.39332725425239745, 0.058567001818087626, 0.1631795807038091, 0.058567001818087626]
Printing some Q and Qe and total Qs values:  [[0.967]
 [0.967]
 [0.967]
 [0.967]
 [0.967]
 [0.967]
 [0.967]] [[44.87]
 [44.87]
 [44.87]
 [44.87]
 [44.87]
 [44.87]
 [44.87]] [[1.967]
 [1.967]
 [1.967]
 [1.967]
 [1.967]
 [1.967]
 [1.967]]
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
probs:  [0.058907225874095345, 0.2179806860917005, 0.387316950194313, 0.058907225874095345, 0.2179806860917005, 0.058907225874095345]
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
probs:  [0.07003575931745322, 0.07003575931745322, 0.4606280584974322, 0.07003575931745322, 0.259228904232755, 0.07003575931745322]
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
probs:  [0.07003575931745322, 0.07003575931745322, 0.4606280584974322, 0.07003575931745322, 0.259228904232755, 0.07003575931745322]
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
probs:  [0.07003575931745322, 0.07003575931745322, 0.4606280584974322, 0.07003575931745322, 0.259228904232755, 0.07003575931745322]
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
probs:  [0.07003575931745322, 0.07003575931745322, 0.4606280584974322, 0.07003575931745322, 0.259228904232755, 0.07003575931745322]
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
probs:  [0.07003575931745322, 0.07003575931745322, 0.4606280584974322, 0.07003575931745322, 0.259228904232755, 0.07003575931745322]
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
probs:  [0.07003575931745322, 0.07003575931745322, 0.4606280584974322, 0.07003575931745322, 0.259228904232755, 0.07003575931745322]
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
probs:  [0.07003575931745322, 0.07003575931745322, 0.4606280584974322, 0.07003575931745322, 0.259228904232755, 0.07003575931745322]
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
probs:  [0.07003575931745322, 0.07003575931745322, 0.4606280584974322, 0.07003575931745322, 0.259228904232755, 0.07003575931745322]
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
probs:  [0.07003575931745322, 0.07003575931745322, 0.4606280584974322, 0.07003575931745322, 0.259228904232755, 0.07003575931745322]
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
probs:  [0.07003575931745322, 0.07003575931745322, 0.4606280584974322, 0.07003575931745322, 0.259228904232755, 0.07003575931745322]
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
probs:  [0.07003575931745322, 0.07003575931745322, 0.4606280584974322, 0.07003575931745322, 0.259228904232755, 0.07003575931745322]
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
probs:  [0.07003575931745322, 0.07003575931745322, 0.4606280584974322, 0.07003575931745322, 0.259228904232755, 0.07003575931745322]
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
probs:  [0.07003575931745322, 0.07003575931745322, 0.4606280584974322, 0.07003575931745322, 0.259228904232755, 0.07003575931745322]
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
probs:  [0.048200044195306595, 0.1342944687312691, 0.3231467548101561, 0.1342944687312691, 0.22576979480073003, 0.1342944687312691]
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
probs:  [0.05304394105898762, 0.14781446555687222, 0.3556981967135233, 0.14781446555687222, 0.14781446555687222, 0.14781446555687222]
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
probs:  [0.04476526055167952, 0.16540995113970794, 0.2935949348894887, 0.16540995113970794, 0.16540995113970794, 0.16540995113970794]
siam score:  -0.5684495
maxi score, test score, baseline:  -0.9875543209876543 -1.0 -0.9875543209876543
probs:  [0.03145008480948273, 0.19370998303810344, 0.19370998303810344, 0.19370998303810344, 0.19370998303810344, 0.19370998303810344]
maxi score, test score, baseline:  -0.9875543209876543 -1.0 -0.9875543209876543
probs:  [0.11956938993143755, 0.21200329754263492, 0.21200329754263492, 0.21200329754263492, 0.032417419898022964, 0.21200329754263492]
maxi score, test score, baseline:  -0.9875543209876543 -1.0 -0.9875543209876543
printing an ep nov before normalisation:  59.91727736335715
printing an ep nov before normalisation:  47.53717169483686
maxi score, test score, baseline:  -0.9875543209876543 -1.0 -0.9875543209876543
probs:  [0.13174446306178397, 0.2336008902426918, 0.13174446306178397, 0.2336008902426918, 0.03570840314835679, 0.2336008902426918]
printing an ep nov before normalisation:  49.657364914240816
printing an ep nov before normalisation:  30.07573212726565
printing an ep nov before normalisation:  16.0090303968688
maxi score, test score, baseline:  -0.9875543209876543 -1.0 -0.9875543209876543
probs:  [0.1466834910551912, 0.2601015147959978, 0.1466834910551912, 0.2601015147959978, 0.03974649724243079, 0.1466834910551912]
printing an ep nov before normalisation:  1.62244189820413
maxi score, test score, baseline:  -0.9875543209876543 -1.0 -0.9875543209876543
probs:  [0.1647048325020797, 0.23533086242721, 0.1647048325020797, 0.23533086242721, 0.035223777639340995, 0.1647048325020797]
maxi score, test score, baseline:  -0.9875543209876543 -1.0 -0.9875543209876543
probs:  [0.1647048325020797, 0.23533086242721, 0.1647048325020797, 0.23533086242721, 0.035223777639340995, 0.1647048325020797]
actions average: 
K:  2  action  0 :  tensor([0.3110, 0.0418, 0.0922, 0.1243, 0.1556, 0.1196, 0.1555],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0590, 0.6479, 0.0627, 0.0907, 0.0337, 0.0435, 0.0625],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0969, 0.0158, 0.3121, 0.1497, 0.1102, 0.1813, 0.1340],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1421, 0.0402, 0.1409, 0.1800, 0.1612, 0.1989, 0.1366],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1668, 0.0403, 0.0953, 0.1816, 0.2046, 0.1860, 0.1255],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0926, 0.0382, 0.1656, 0.0980, 0.0692, 0.4383, 0.0981],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1101, 0.0777, 0.0981, 0.1739, 0.1225, 0.1372, 0.2804],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9875543209876543 -1.0 -0.9875543209876543
probs:  [0.1647048325020797, 0.23533086242721, 0.1647048325020797, 0.23533086242721, 0.035223777639340995, 0.1647048325020797]
maxi score, test score, baseline:  -0.9875543209876543 -1.0 -0.9875543209876543
probs:  [0.1647048325020797, 0.23533086242721, 0.1647048325020797, 0.23533086242721, 0.035223777639340995, 0.1647048325020797]
from probs:  [0.1647048325020797, 0.23533086242721, 0.1647048325020797, 0.23533086242721, 0.035223777639340995, 0.1647048325020797]
maxi score, test score, baseline:  -0.9875543209876543 -1.0 -0.9875543209876543
maxi score, test score, baseline:  -0.9875543209876543 -1.0 -0.9875543209876543
probs:  [0.17722190870988508, 0.2532196514210578, 0.17722190870988508, 0.17722190870988508, 0.037892713739401986, 0.17722190870988508]
maxi score, test score, baseline:  -0.9875543209876543 -1.0 -0.9875543209876543
probs:  [0.17722190870988508, 0.2532196514210578, 0.17722190870988508, 0.17722190870988508, 0.037892713739401986, 0.17722190870988508]
printing an ep nov before normalisation:  31.409874407843432
maxi score, test score, baseline:  -0.9875543209876543 -1.0 -0.9875543209876543
probs:  [0.17722190870988508, 0.2532196514210578, 0.17722190870988508, 0.17722190870988508, 0.037892713739401986, 0.17722190870988508]
maxi score, test score, baseline:  -0.9875543209876543 -1.0 -0.9875543209876543
maxi score, test score, baseline:  -0.9875543209876543 -1.0 -0.9875543209876543
probs:  [0.17722190870988508, 0.2532196514210578, 0.17722190870988508, 0.17722190870988508, 0.037892713739401986, 0.17722190870988508]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  69.06651891515408
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9877048780487805 -1.0 -0.9877048780487805
probs:  [0.1772219088449218, 0.2532196525283587, 0.1772219088449218, 0.1772219088449218, 0.03789271209195418, 0.1772219088449218]
maxi score, test score, baseline:  -0.9877048780487805 -1.0 -0.9877048780487805
probs:  [0.19090241353052936, 0.2727711562878144, 0.19090241353052936, 0.19090241353052936, 0.04080971847550763, 0.1137118846450899]
line 256 mcts: sample exp_bonus 19.264285976631424
printing an ep nov before normalisation:  13.95925760269165
maxi score, test score, baseline:  -0.9877048780487805 -1.0 -0.9877048780487805
maxi score, test score, baseline:  -0.9877048780487805 -1.0 -0.9877048780487805
probs:  [0.19090241353052936, 0.2727711562878144, 0.19090241353052936, 0.19090241353052936, 0.04080971847550763, 0.1137118846450899]
printing an ep nov before normalisation:  39.509526991285284
maxi score, test score, baseline:  -0.9877048780487805 -1.0 -0.9877048780487805
probs:  [0.19090241353052936, 0.2727711562878144, 0.19090241353052936, 0.19090241353052936, 0.04080971847550763, 0.1137118846450899]
maxi score, test score, baseline:  -0.9877048780487805 -1.0 -0.9877048780487805
probs:  [0.19090241353052936, 0.2727711562878144, 0.19090241353052936, 0.19090241353052936, 0.04080971847550763, 0.1137118846450899]
maxi score, test score, baseline:  -0.9878518072289156 -1.0 -0.9878518072289156
probs:  [0.19090241385634243, 0.2727711577142277, 0.19090241385634243, 0.19090241385634243, 0.0408097167835519, 0.11371188393319318]
printing an ep nov before normalisation:  51.8106746673584
printing an ep nov before normalisation:  13.904171808316143
maxi score, test score, baseline:  -0.9878518072289156 -1.0 -0.9878518072289156
maxi score, test score, baseline:  -0.9878518072289156 -1.0 -0.9878518072289156
probs:  [0.19090241385634243, 0.2727711577142277, 0.19090241385634243, 0.19090241385634243, 0.0408097167835519, 0.11371188393319318]
maxi score, test score, baseline:  -0.9878518072289156 -1.0 -0.9878518072289156
probs:  [0.19090241385634243, 0.2727711577142277, 0.19090241385634243, 0.19090241385634243, 0.0408097167835519, 0.11371188393319318]
printing an ep nov before normalisation:  18.91492509483846
printing an ep nov before normalisation:  23.534859790487918
maxi score, test score, baseline:  -0.9878518072289156 -1.0 -0.9878518072289156
probs:  [0.19090241385634243, 0.2727711577142277, 0.19090241385634243, 0.19090241385634243, 0.0408097167835519, 0.11371188393319318]
maxi score, test score, baseline:  -0.9878518072289156 -1.0 -0.9878518072289156
probs:  [0.19090241385634243, 0.2727711577142277, 0.19090241385634243, 0.19090241385634243, 0.0408097167835519, 0.11371188393319318]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  37.4598585620774
maxi score, test score, baseline:  -0.9878518072289156 -1.0 -0.9878518072289156
probs:  [0.20687357783039156, 0.29559635903230663, 0.20687357783039156, 0.12322066984001463, 0.04421514562688098, 0.12322066984001463]
maxi score, test score, baseline:  -0.9878518072289156 -1.0 -0.9878518072289156
printing an ep nov before normalisation:  29.245438056833883
printing an ep nov before normalisation:  2.944021528249152
maxi score, test score, baseline:  -0.9879952380952381 -1.0 -0.9879952380952381
probs:  [0.20687357840196594, 0.2955963608651478, 0.20687357840196594, 0.12322066922239411, 0.04421514388613229, 0.12322066922239411]
maxi score, test score, baseline:  -0.9879952380952381 -1.0 -0.9879952380952381
probs:  [0.20687357840196594, 0.2955963608651478, 0.20687357840196594, 0.12322066922239411, 0.04421514388613229, 0.12322066922239411]
maxi score, test score, baseline:  -0.9879952380952381 -1.0 -0.9879952380952381
probs:  [0.13446707648797052, 0.32259260574202275, 0.22576328921420194, 0.13446707648797052, 0.048242875579863606, 0.13446707648797052]
maxi score, test score, baseline:  -0.9879952380952381 -1.0 -0.9879952380952381
probs:  [0.13446707648797052, 0.32259260574202275, 0.22576328921420194, 0.13446707648797052, 0.048242875579863606, 0.13446707648797052]
maxi score, test score, baseline:  -0.9879952380952381 -1.0 -0.9879952380952381
probs:  [0.13446707648797052, 0.32259260574202275, 0.22576328921420194, 0.13446707648797052, 0.048242875579863606, 0.13446707648797052]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
printing an ep nov before normalisation:  55.777705183297556
printing an ep nov before normalisation:  57.07078463596224
printing an ep nov before normalisation:  81.96621247877488
printing an ep nov before normalisation:  42.66454653052282
printing an ep nov before normalisation:  43.52973364854101
printing an ep nov before normalisation:  55.285314441984
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9881352941176471 -1.0 -0.9881352941176471
printing an ep nov before normalisation:  57.139581434243006
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.20595417330608679, 0.2940534306187256, 0.20595417330608679, 0.20595417330608679, 0.04404202473150705, 0.04404202473150705]
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.20595417330608679, 0.2940534306187256, 0.20595417330608679, 0.20595417330608679, 0.04404202473150705, 0.04404202473150705]
printing an ep nov before normalisation:  21.437148694011793
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
printing an ep nov before normalisation:  41.42654469306451
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.20595417330608679, 0.2940534306187256, 0.20595417330608679, 0.20595417330608679, 0.04404202473150705, 0.04404202473150705]
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.20595417330608679, 0.2940534306187256, 0.20595417330608679, 0.20595417330608679, 0.04404202473150705, 0.04404202473150705]
siam score:  -0.61163807
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.20595417330608679, 0.2940534306187256, 0.20595417330608679, 0.20595417330608679, 0.04404202473150705, 0.04404202473150705]
printing an ep nov before normalisation:  35.21229558828012
siam score:  -0.612818
printing an ep nov before normalisation:  69.2823213507809
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.16263781407362768, 0.29549006073664186, 0.22716604816709204, 0.22716604816709204, 0.04377001442777317, 0.04377001442777317]
siam score:  -0.6143832
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.10819924331377778, 0.31470707970624756, 0.2419376516441393, 0.2419376516441393, 0.04660918684584802, 0.04660918684584802]
maxi score, test score, baseline:  -0.9882720930232558 -1.0 -0.9882720930232558
probs:  [0.10819924331377778, 0.31470707970624756, 0.2419376516441393, 0.2419376516441393, 0.04660918684584802, 0.04660918684584802]
maxi score, test score, baseline:  -0.9884057471264368 -1.0 -0.9884057471264368
probs:  [0.04966211277577232, 0.33537090716051454, 0.2578213772560843, 0.2578213772560843, 0.04966211277577232, 0.04966211277577232]
siam score:  -0.5995096
maxi score, test score, baseline:  -0.9884057471264368 -1.0 -0.9884057471264368
probs:  [0.04966211277577232, 0.33537090716051454, 0.2578213772560843, 0.2578213772560843, 0.04966211277577232, 0.04966211277577232]
printing an ep nov before normalisation:  48.2428730986704
printing an ep nov before normalisation:  33.06519313934569
printing an ep nov before normalisation:  32.95571354216718
printing an ep nov before normalisation:  30.83343926120131
Printing some Q and Qe and total Qs values:  [[0.277]
 [0.268]
 [0.268]
 [0.268]
 [0.268]
 [0.268]
 [0.268]] [[30.908]
 [27.93 ]
 [27.93 ]
 [27.93 ]
 [27.93 ]
 [27.93 ]
 [27.93 ]] [[1.277]
 [1.099]
 [1.099]
 [1.099]
 [1.099]
 [1.099]
 [1.099]]
maxi score, test score, baseline:  -0.9884057471264368 -1.0 -0.9884057471264368
probs:  [0.04966211277577232, 0.33537090716051454, 0.2578213772560843, 0.2578213772560843, 0.04966211277577232, 0.04966211277577232]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9884057471264368 -1.0 -0.9884057471264368
probs:  [0.053579731739322164, 0.3618874230666659, 0.19916947486612346, 0.2782039068492441, 0.053579731739322164, 0.053579731739322164]
maxi score, test score, baseline:  -0.9884057471264368 -1.0 -0.9884057471264368
probs:  [0.053579731739322164, 0.3618874230666659, 0.19916947486612346, 0.2782039068492441, 0.053579731739322164, 0.053579731739322164]
maxi score, test score, baseline:  -0.9884057471264368 -1.0 -0.9884057471264368
probs:  [0.053579731739322164, 0.3618874230666659, 0.19916947486612346, 0.2782039068492441, 0.053579731739322164, 0.053579731739322164]
printing an ep nov before normalisation:  45.26424880292383
maxi score, test score, baseline:  -0.9884057471264368 -1.0 -0.9884057471264368
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9884057471264368 -1.0 -0.9884057471264368
probs:  [0.053579731739322164, 0.3618874230666659, 0.19916947486612346, 0.2782039068492441, 0.053579731739322164, 0.053579731739322164]
printing an ep nov before normalisation:  29.16523616646387
Printing some Q and Qe and total Qs values:  [[0.171]
 [0.119]
 [0.119]
 [0.119]
 [0.153]
 [0.119]
 [0.158]] [[18.862]
 [17.246]
 [17.246]
 [17.246]
 [16.891]
 [17.246]
 [17.306]] [[0.767]
 [0.621]
 [0.621]
 [0.621]
 [0.634]
 [0.621]
 [0.663]]
maxi score, test score, baseline:  -0.9884057471264368 -1.0 -0.9884057471264368
probs:  [0.053579731739322164, 0.3618874230666659, 0.19916947486612346, 0.2782039068492441, 0.053579731739322164, 0.053579731739322164]
maxi score, test score, baseline:  -0.9884057471264368 -1.0 -0.9884057471264368
maxi score, test score, baseline:  -0.9884057471264368 -1.0 -0.9884057471264368
probs:  [0.05835266157171051, 0.3893412654662638, 0.16271843757449816, 0.2728823122441064, 0.05835266157171051, 0.05835266157171051]
printing an ep nov before normalisation:  24.21584540863436
actions average: 
K:  3  action  0 :  tensor([0.1615, 0.0518, 0.1209, 0.1523, 0.2668, 0.1128, 0.1339],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0794, 0.5662, 0.0961, 0.0523, 0.0353, 0.0482, 0.1224],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1350, 0.0356, 0.2044, 0.1557, 0.1325, 0.2236, 0.1132],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1781, 0.0825, 0.1617, 0.2094, 0.1305, 0.1027, 0.1351],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1248, 0.0439, 0.1274, 0.2112, 0.2562, 0.1174, 0.1190],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1038, 0.0157, 0.1232, 0.1344, 0.1037, 0.4479, 0.0712],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1410, 0.1329, 0.1595, 0.1513, 0.1300, 0.1132, 0.1722],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9884057471264368 -1.0 -0.9884057471264368
maxi score, test score, baseline:  -0.9884057471264368 -1.0 -0.9884057471264368
probs:  [0.0507534161637989, 0.3304571293337626, 0.18682549284107813, 0.3304571293337626, 0.0507534161637989, 0.0507534161637989]
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.659761232847956
maxi score, test score, baseline:  -0.9884057471264368 -1.0 -0.9884057471264368
probs:  [0.0507534161637989, 0.3304571293337626, 0.18682549284107813, 0.3304571293337626, 0.0507534161637989, 0.0507534161637989]
maxi score, test score, baseline:  -0.9884057471264368 -1.0 -0.9884057471264368
maxi score, test score, baseline:  -0.9884057471264368 -1.0 -0.9884057471264368
probs:  [0.0507534161637989, 0.3304571293337626, 0.18682549284107813, 0.3304571293337626, 0.0507534161637989, 0.0507534161637989]
maxi score, test score, baseline:  -0.9884057471264368 -1.0 -0.9884057471264368
probs:  [0.0507534161637989, 0.3304571293337626, 0.18682549284107813, 0.3304571293337626, 0.0507534161637989, 0.0507534161637989]
printing an ep nov before normalisation:  35.51189248894171
maxi score, test score, baseline:  -0.9884057471264368 -1.0 -0.9884057471264368
probs:  [0.0507534161637989, 0.3304571293337626, 0.18682549284107813, 0.3304571293337626, 0.0507534161637989, 0.0507534161637989]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  62.421248706094936
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.473]
 [0.334]
 [0.652]
 [0.649]
 [0.333]
 [0.271]] [[32.056]
 [35.833]
 [34.544]
 [31.308]
 [30.738]
 [31.905]
 [42.802]] [[0.776]
 [0.77 ]
 [0.613]
 [0.885]
 [0.873]
 [0.574]
 [0.668]]
printing an ep nov before normalisation:  50.31206649164466
printing an ep nov before normalisation:  16.102684319151624
using explorer policy with actor:  1
printing an ep nov before normalisation:  54.290406079148816
printing an ep nov before normalisation:  50.63684724075126
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
probs:  [0.1140283964225462, 0.2720991980775929, 0.19098391828092473, 0.040920650657086645, 0.19098391828092473, 0.19098391828092473]
from probs:  [0.1140283964225462, 0.2720991980775929, 0.19098391828092473, 0.040920650657086645, 0.19098391828092473, 0.19098391828092473]
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
probs:  [0.12353222794368719, 0.2947930381977458, 0.2069092013568474, 0.04432410320118495, 0.12353222794368719, 0.2069092013568474]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.231]
 [0.413]
 [0.387]
 [0.381]
 [0.468]
 [0.344]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.429]
 [0.231]
 [0.413]
 [0.387]
 [0.381]
 [0.468]
 [0.344]]
Printing some Q and Qe and total Qs values:  [[0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.749]] [[6.741]
 [6.741]
 [6.741]
 [6.741]
 [6.741]
 [6.741]
 [6.741]] [[0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.749]]
using another actor
printing an ep nov before normalisation:  16.35445237159729
maxi score, test score, baseline:  -0.9887888888888889 -1.0 -0.9887888888888889
probs:  [0.1544780395746751, 0.27961728840052624, 0.21540109492410278, 0.041547497951345595, 0.1544780395746751, 0.1544780395746751]
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.511801267815443
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.088]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.719]
 [0.61 ]] [[46.376]
 [34.978]
 [21.486]
 [21.486]
 [21.486]
 [31.02 ]
 [21.486]] [[1.277]
 [0.54 ]
 [0.772]
 [0.772]
 [0.772]
 [1.086]
 [0.772]]
maxi score, test score, baseline:  -0.9887888888888889 -1.0 -0.9887888888888889
probs:  [0.16496167471023815, 0.23486634492380806, 0.23486634492380806, 0.03538228602166935, 0.16496167471023815, 0.16496167471023815]
siam score:  -0.621135
printing an ep nov before normalisation:  51.127275021452114
Printing some Q and Qe and total Qs values:  [[0.244]
 [0.201]
 [0.269]
 [0.244]
 [0.244]
 [0.244]
 [0.244]] [[30.825]
 [47.77 ]
 [48.504]
 [30.825]
 [30.825]
 [30.825]
 [30.825]] [[0.55 ]
 [1.239]
 [1.338]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]]
printing an ep nov before normalisation:  23.47920944215204
printing an ep nov before normalisation:  43.60317654079861
printing an ep nov before normalisation:  40.33364573014642
maxi score, test score, baseline:  -0.9887888888888889 -1.0 -0.9887888888888889
probs:  [0.17278162276491282, 0.22414725399017962, 0.22414725399017962, 0.033360623724902325, 0.17278162276491282, 0.17278162276491282]
maxi score, test score, baseline:  -0.9887888888888889 -1.0 -0.9887888888888889
maxi score, test score, baseline:  -0.9887888888888889 -1.0 -0.9887888888888889
probs:  [0.17278162276491282, 0.22414725399017962, 0.22414725399017962, 0.033360623724902325, 0.17278162276491282, 0.17278162276491282]
printing an ep nov before normalisation:  30.259957313537598
from probs:  [0.1821378861510315, 0.1821378861510315, 0.2362871543463086, 0.035161301049565594, 0.1821378861510315, 0.1821378861510315]
Printing some Q and Qe and total Qs values:  [[0.87 ]
 [0.862]
 [0.914]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [0.87 ]] [[42.78 ]
 [42.471]
 [39.258]
 [42.78 ]
 [42.78 ]
 [42.78 ]
 [42.78 ]] [[2.46 ]
 [2.431]
 [2.26 ]
 [2.46 ]
 [2.46 ]
 [2.46 ]
 [2.46 ]]
printing an ep nov before normalisation:  48.90790057011686
maxi score, test score, baseline:  -0.9889109890109891 -1.0 -0.9889109890109891
probs:  [0.20602718599672207, 0.20602718599672207, 0.20602718599672207, 0.03109154452980929, 0.14479971148330242, 0.20602718599672207]
maxi score, test score, baseline:  -0.9889109890109891 -1.0 -0.9889109890109891
maxi score, test score, baseline:  -0.9889109890109891 -1.0 -0.9889109890109891
probs:  [0.20602718599672207, 0.20602718599672207, 0.20602718599672207, 0.03109154452980929, 0.14479971148330242, 0.20602718599672207]
printing an ep nov before normalisation:  76.1144455967553
printing an ep nov before normalisation:  30.933642387390137
maxi score, test score, baseline:  -0.9889109890109891 -1.0 -0.9889109890109891
probs:  [0.20602718599672207, 0.20602718599672207, 0.20602718599672207, 0.03109154452980929, 0.14479971148330242, 0.20602718599672207]
printing an ep nov before normalisation:  30.419345880653864
printing an ep nov before normalisation:  39.615728165902404
siam score:  -0.6356034
maxi score, test score, baseline:  -0.9889109890109891 -1.0 -0.9889109890109891
probs:  [0.2194671943177102, 0.2194671943177102, 0.2194671943177102, 0.03311239084343904, 0.1542430131017152, 0.1542430131017152]
maxi score, test score, baseline:  -0.9889109890109891 -1.0 -0.9889109890109891
probs:  [0.2194671943177102, 0.2194671943177102, 0.2194671943177102, 0.03311239084343904, 0.1542430131017152, 0.1542430131017152]
printing an ep nov before normalisation:  25.0130208383127
printing an ep nov before normalisation:  69.7741469236169
printing an ep nov before normalisation:  45.87266534777399
maxi score, test score, baseline:  -0.9889109890109891 -1.0 -0.9889109890109891
maxi score, test score, baseline:  -0.9889109890109891 -1.0 -0.9889109890109891
probs:  [0.23398772222624678, 0.23398772222624678, 0.23398772222624678, 0.035295704617888154, 0.16444551606332108, 0.09829561264005052]
printing an ep nov before normalisation:  97.63424856063692
printing an ep nov before normalisation:  34.68465559901076
printing an ep nov before normalisation:  38.316850198613636
maxi score, test score, baseline:  -0.9889109890109891 -1.0 -0.9889109890109891
probs:  [0.23398772222624678, 0.23398772222624678, 0.23398772222624678, 0.035295704617888154, 0.16444551606332108, 0.09829561264005052]
siam score:  -0.6251314
siam score:  -0.62762177
maxi score, test score, baseline:  -0.9889109890109891 -1.0 -0.9889109890109891
probs:  [0.17673673904241094, 0.25148098314168665, 0.25148098314168665, 0.037926000000899567, 0.17673673904241094, 0.10563855563090521]
printing an ep nov before normalisation:  49.07135071618658
maxi score, test score, baseline:  -0.9890304347826087 -1.0 -0.9890304347826087
probs:  [0.17673673918160715, 0.25148098431405264, 0.25148098431405264, 0.037925998221351016, 0.17673673918160715, 0.10563855478732936]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9890304347826087 -1.0 -0.9890304347826087
probs:  [0.17673673918160715, 0.25148098431405264, 0.25148098431405264, 0.037925998221351016, 0.17673673918160715, 0.10563855478732936]
maxi score, test score, baseline:  -0.9890304347826087 -1.0 -0.9890304347826087
probs:  [0.18957455809135343, 0.2697521780777566, 0.2697521780777566, 0.040673263830889934, 0.18957455809135343, 0.040673263830889934]
maxi score, test score, baseline:  -0.9890304347826087 -1.0 -0.9890304347826087
probs:  [0.18957455809135343, 0.2697521780777566, 0.2697521780777566, 0.040673263830889934, 0.18957455809135343, 0.040673263830889934]
maxi score, test score, baseline:  -0.9890304347826087 -1.0 -0.9890304347826087
probs:  [0.18957455809135343, 0.2697521780777566, 0.2697521780777566, 0.040673263830889934, 0.18957455809135343, 0.040673263830889934]
printing an ep nov before normalisation:  79.48433833322441
printing an ep nov before normalisation:  51.773258692634606
maxi score, test score, baseline:  -0.9890304347826087 -1.0 -0.9890304347826087
maxi score, test score, baseline:  -0.9890304347826087 -1.0 -0.9890304347826087
probs:  [0.22474128229570967, 0.31980258278355395, 0.22474128229570967, 0.04819886710399909, 0.13431711841702862, 0.04819886710399909]
printing an ep nov before normalisation:  65.49436516574495
printing an ep nov before normalisation:  40.39590793561494
printing an ep nov before normalisation:  39.5275646595195
printing an ep nov before normalisation:  31.865227213313574
maxi score, test score, baseline:  -0.9890304347826087 -1.0 -0.9890304347826087
maxi score, test score, baseline:  -0.9890304347826087 -1.0 -0.9890304347826087
probs:  [0.22474128229570967, 0.31980258278355395, 0.22474128229570967, 0.04819886710399909, 0.13431711841702862, 0.04819886710399909]
actions average: 
K:  3  action  0 :  tensor([0.1790, 0.0318, 0.1640, 0.1533, 0.1359, 0.1566, 0.1794],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0715, 0.4299, 0.1332, 0.0918, 0.0453, 0.1074, 0.1209],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1254, 0.0931, 0.2414, 0.1140, 0.1045, 0.1952, 0.1265],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2341, 0.0705, 0.1346, 0.1451, 0.1543, 0.1273, 0.1341],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2007, 0.0769, 0.1120, 0.1115, 0.2507, 0.0974, 0.1509],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0799, 0.0478, 0.0998, 0.0928, 0.0709, 0.5133, 0.0955],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1396, 0.0615, 0.1142, 0.1832, 0.1746, 0.1657, 0.1612],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9890304347826087 -1.0 -0.9890304347826087
probs:  [0.22474128229570967, 0.31980258278355395, 0.22474128229570967, 0.04819886710399909, 0.13431711841702862, 0.04819886710399909]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9890304347826087 -1.0 -0.9890304347826087
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9890304347826087 -1.0 -0.9890304347826087
probs:  [0.27294216016465955, 0.38840361114481364, 0.05851375120151672, 0.05851375120151672, 0.16311297508597664, 0.05851375120151672]
printing an ep nov before normalisation:  16.24668371603689
maxi score, test score, baseline:  -0.9890304347826087 -1.0 -0.9890304347826087
maxi score, test score, baseline:  -0.9890304347826087 -1.0 -0.9890304347826087
probs:  [0.27294216016465955, 0.38840361114481364, 0.05851375120151672, 0.05851375120151672, 0.16311297508597664, 0.05851375120151672]
siam score:  -0.60657746
maxi score, test score, baseline:  -0.9890304347826087 -1.0 -0.9890304347826087
printing an ep nov before normalisation:  12.244956493377686
printing an ep nov before normalisation:  18.924766183229888
from probs:  [0.27294216016465955, 0.38840361114481364, 0.05851375120151672, 0.05851375120151672, 0.16311297508597664, 0.05851375120151672]
printing an ep nov before normalisation:  23.525605216625213
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9890304347826087 -1.0 -0.9890304347826087
probs:  [0.32989366448279284, 0.32989366448279284, 0.051047543213577226, 0.051047543213577226, 0.18707004139368255, 0.051047543213577226]
maxi score, test score, baseline:  -0.9890304347826087 -1.0 -0.9890304347826087
probs:  [0.21824672251028948, 0.3848899798512248, 0.0595388583760654, 0.0595388583760654, 0.21824672251028948, 0.0595388583760654]
maxi score, test score, baseline:  -0.9890304347826087 -1.0 -0.9890304347826087
probs:  [0.21824672251028948, 0.3848899798512248, 0.0595388583760654, 0.0595388583760654, 0.21824672251028948, 0.0595388583760654]
printing an ep nov before normalisation:  72.54764011928013
maxi score, test score, baseline:  -0.9890304347826087 -1.0 -0.9890304347826087
probs:  [0.2857130103080031, 0.2857130103080031, 0.04762032302533021, 0.04762032302533021, 0.2857130103080031, 0.04762032302533021]
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]] [[22.652]
 [22.652]
 [22.652]
 [22.652]
 [22.652]
 [22.652]
 [22.652]] [[0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]]
maxi score, test score, baseline:  -0.9890304347826087 -1.0 -0.9890304347826087
probs:  [0.2857130103080031, 0.2857130103080031, 0.04762032302533021, 0.04762032302533021, 0.2857130103080031, 0.04762032302533021]
maxi score, test score, baseline:  -0.9890304347826087 -1.0 -0.9890304347826087
probs:  [0.2857130103080031, 0.2857130103080031, 0.04762032302533021, 0.04762032302533021, 0.2857130103080031, 0.04762032302533021]
maxi score, test score, baseline:  -0.9890304347826087 -1.0 -0.9890304347826087
probs:  [0.2857130103080031, 0.2857130103080031, 0.04762032302533021, 0.04762032302533021, 0.2857130103080031, 0.04762032302533021]
maxi score, test score, baseline:  -0.9890304347826087 -1.0 -0.9890304347826087
maxi score, test score, baseline:  -0.9890304347826087 -1.0 -0.9890304347826087
probs:  [0.3750491654531663, 0.3750491654531663, 0.062475417273416836, 0.062475417273416836, 0.062475417273416836, 0.062475417273416836]
maxi score, test score, baseline:  -0.9890304347826087 -1.0 -0.9890304347826087
probs:  [0.3750491654531663, 0.3750491654531663, 0.062475417273416836, 0.062475417273416836, 0.062475417273416836, 0.062475417273416836]
Printing some Q and Qe and total Qs values:  [[1.169]
 [1.169]
 [1.175]
 [1.169]
 [1.169]
 [1.169]
 [1.234]] [[ 0.   ]
 [ 0.   ]
 [32.434]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [33.788]] [[0.8  ]
 [0.8  ]
 [1.539]
 [0.8  ]
 [0.8  ]
 [0.8  ]
 [1.628]]
maxi score, test score, baseline:  -0.9890304347826087 -1.0 -0.9890304347826087
probs:  [0.3750491654531663, 0.3750491654531663, 0.062475417273416836, 0.062475417273416836, 0.062475417273416836, 0.062475417273416836]
printing an ep nov before normalisation:  25.364796143631644
using explorer policy with actor:  1
using another actor
printing an ep nov before normalisation:  15.869934545257758
maxi score, test score, baseline:  -0.989147311827957 -1.0 -0.989147311827957
maxi score, test score, baseline:  -0.989147311827957 -1.0 -0.989147311827957
probs:  [0.3750491784590968, 0.3750491784590968, 0.062475410770451635, 0.062475410770451635, 0.062475410770451635, 0.062475410770451635]
printing an ep nov before normalisation:  57.16877417301758
maxi score, test score, baseline:  -0.989147311827957 -1.0 -0.989147311827957
probs:  [0.5457225039648564, 0.0908554992070287, 0.0908554992070287, 0.0908554992070287, 0.0908554992070287, 0.0908554992070287]
printing an ep nov before normalisation:  38.74176946380789
siam score:  -0.63711554
maxi score, test score, baseline:  -0.989147311827957 -1.0 -0.989147311827957
probs:  [0.2919724692577643, 0.04527667040654079, 0.16568771508392371, 0.16568771508392371, 0.16568771508392371, 0.16568771508392371]
printing an ep nov before normalisation:  16.4497742522363
maxi score, test score, baseline:  -0.9892617021276596 -1.0 -0.9892617021276596
probs:  [0.29197247234918117, 0.04527666741173175, 0.16568771505977178, 0.16568771505977178, 0.16568771505977178, 0.16568771505977178]
maxi score, test score, baseline:  -0.9892617021276596 -1.0 -0.9892617021276596
maxi score, test score, baseline:  -0.9892617021276596 -1.0 -0.9892617021276596
probs:  [0.29197247234918117, 0.04527666741173175, 0.16568771505977178, 0.16568771505977178, 0.16568771505977178, 0.16568771505977178]
maxi score, test score, baseline:  -0.9892617021276596 -1.0 -0.9892617021276596
maxi score, test score, baseline:  -0.9892617021276596 -1.0 -0.9892617021276596
probs:  [0.19352029559409648, 0.03239852202951749, 0.19352029559409648, 0.19352029559409648, 0.19352029559409648, 0.19352029559409648]
maxi score, test score, baseline:  -0.9892617021276596 -1.0 -0.9892617021276596
actions average: 
K:  3  action  0 :  tensor([0.3266, 0.0122, 0.1232, 0.1349, 0.1723, 0.1024, 0.1284],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0266, 0.6977, 0.0659, 0.0815, 0.0282, 0.0631, 0.0371],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0564, 0.0059, 0.6433, 0.0510, 0.0685, 0.1009, 0.0739],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1422, 0.1270, 0.1568, 0.1495, 0.1415, 0.1307, 0.1523],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1621, 0.0222, 0.1251, 0.2108, 0.1725, 0.1504, 0.1568],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0835, 0.0337, 0.1436, 0.1002, 0.0814, 0.4412, 0.1165],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1491, 0.1460, 0.1506, 0.1389, 0.1471, 0.1025, 0.1659],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9892617021276596 -1.0 -0.9892617021276596
probs:  [0.19352029559409648, 0.03239852202951749, 0.19352029559409648, 0.19352029559409648, 0.19352029559409648, 0.19352029559409648]
printing an ep nov before normalisation:  23.45636717838049
maxi score, test score, baseline:  -0.9892617021276596 -1.0 -0.9892617021276596
probs:  [0.23069921607905336, 0.03860156784189327, 0.03860156784189327, 0.23069921607905336, 0.23069921607905336, 0.23069921607905336]
printing an ep nov before normalisation:  94.31541893909045
printing an ep nov before normalisation:  27.7691058688122
maxi score, test score, baseline:  -0.9893736842105263 -1.0 -0.9893736842105263
probs:  [0.21171016033702145, 0.12023721934491635, 0.032922139306997826, 0.21171016033702145, 0.21171016033702145, 0.21171016033702145]
printing an ep nov before normalisation:  26.108828682912765
maxi score, test score, baseline:  -0.9893736842105263 -1.0 -0.9893736842105263
probs:  [0.13234026020791254, 0.13234026020791254, 0.03622632212340119, 0.2330310524869246, 0.2330310524869246, 0.2330310524869246]
Printing some Q and Qe and total Qs values:  [[1.275]
 [1.081]
 [1.281]
 [1.278]
 [1.281]
 [1.281]
 [1.281]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.275]
 [1.081]
 [1.281]
 [1.278]
 [1.281]
 [1.281]
 [1.281]]
printing an ep nov before normalisation:  67.48967506623708
maxi score, test score, baseline:  -0.9893736842105263 -1.0 -0.9893736842105263
probs:  [0.16571131310837792, 0.16571131310837792, 0.045336764763987854, 0.16571131310837792, 0.2918179828025004, 0.16571131310837792]
maxi score, test score, baseline:  -0.9893736842105263 -1.0 -0.9893736842105263
probs:  [0.16571131310837792, 0.16571131310837792, 0.045336764763987854, 0.16571131310837792, 0.2918179828025004, 0.16571131310837792]
printing an ep nov before normalisation:  25.026895844185336
using explorer policy with actor:  1
printing an ep nov before normalisation:  20.099813678810097
printing an ep nov before normalisation:  25.505615443030837
printing an ep nov before normalisation:  42.4800798629439
Printing some Q and Qe and total Qs values:  [[0.178]
 [0.114]
 [0.122]
 [0.218]
 [0.226]
 [0.178]
 [0.084]] [[60.291]
 [67.965]
 [64.904]
 [65.262]
 [66.806]
 [60.291]
 [68.133]] [[1.378]
 [1.609]
 [1.5  ]
 [1.609]
 [1.677]
 [1.378]
 [1.586]]
maxi score, test score, baseline:  -0.9893736842105263 -1.0 -0.9893736842105263
probs:  [0.16571131310837792, 0.16571131310837792, 0.045336764763987854, 0.16571131310837792, 0.2918179828025004, 0.16571131310837792]
Printing some Q and Qe and total Qs values:  [[0.718]
 [0.739]
 [0.639]
 [0.743]
 [0.729]
 [0.707]
 [0.719]] [[35.822]
 [29.168]
 [39.79 ]
 [35.213]
 [34.553]
 [34.435]
 [35.951]] [[1.288]
 [1.147]
 [1.306]
 [1.299]
 [1.269]
 [1.243]
 [1.292]]
maxi score, test score, baseline:  -0.9893736842105263 -1.0 -0.9893736842105263
probs:  [0.16571131310837792, 0.16571131310837792, 0.045336764763987854, 0.16571131310837792, 0.2918179828025004, 0.16571131310837792]
actions average: 
K:  4  action  0 :  tensor([0.1696, 0.0247, 0.2290, 0.1312, 0.1331, 0.1722, 0.1401],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0224, 0.6846, 0.0285, 0.0980, 0.0551, 0.0739, 0.0376],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1291, 0.1316, 0.2659, 0.1229, 0.1061, 0.1322, 0.1123],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1513, 0.1840, 0.1266, 0.1314, 0.1557, 0.1065, 0.1445],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1772, 0.0452, 0.1639, 0.1347, 0.1847, 0.1450, 0.1493],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1808, 0.0846, 0.1836, 0.1189, 0.1194, 0.1332, 0.1796],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1774, 0.1222, 0.1773, 0.0961, 0.0969, 0.1031, 0.2270],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9894833333333334 -1.0 -0.9894833333333334
probs:  [0.1883908717821409, 0.1883908717821409, 0.05152837955465391, 0.1883908717821409, 0.3317706255442696, 0.05152837955465391]
maxi score, test score, baseline:  -0.9894833333333334 -1.0 -0.9894833333333334
actions average: 
K:  3  action  0 :  tensor([0.2006, 0.0322, 0.1471, 0.1665, 0.1570, 0.1316, 0.1650],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0343, 0.7728, 0.0271, 0.0531, 0.0300, 0.0342, 0.0484],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1304, 0.0221, 0.2814, 0.1409, 0.1151, 0.1391, 0.1710],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1413, 0.0224, 0.1582, 0.2097, 0.1675, 0.1392, 0.1618],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1936, 0.0210, 0.1156, 0.1903, 0.2257, 0.1119, 0.1420],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0691, 0.0053, 0.1866, 0.0835, 0.0687, 0.4948, 0.0920],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1449, 0.0702, 0.0926, 0.1502, 0.1657, 0.1283, 0.2480],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  45.595181722402955
maxi score, test score, baseline:  -0.9894833333333334 -1.0 -0.9894833333333334
probs:  [0.21826941833786467, 0.05968535222637861, 0.05968535222637861, 0.21826941833786467, 0.3844051066451349, 0.05968535222637861]
maxi score, test score, baseline:  -0.9894833333333334 -1.0 -0.9894833333333334
probs:  [0.21826941833786467, 0.05968535222637861, 0.05968535222637861, 0.21826941833786467, 0.3844051066451349, 0.05968535222637861]
printing an ep nov before normalisation:  51.91240310668945
from probs:  [0.20692593582898913, 0.12385822980629324, 0.12385822980629324, 0.20692593582898913, 0.2939492469003846, 0.04448242182905061]
printing an ep nov before normalisation:  40.40764892627196
printing an ep nov before normalisation:  28.127339574927824
maxi score, test score, baseline:  -0.9894833333333334 -1.0 -0.9894833333333334
maxi score, test score, baseline:  -0.9894833333333334 -1.0 -0.9894833333333334
probs:  [0.14719855247256852, 0.14719855247256852, 0.14719855247256852, 0.2590366552897275, 0.2590366552897275, 0.04033103200283935]
maxi score, test score, baseline:  -0.9894833333333334 -1.0 -0.9894833333333334
probs:  [0.14719855247256852, 0.14719855247256852, 0.14719855247256852, 0.2590366552897275, 0.2590366552897275, 0.04033103200283935]
maxi score, test score, baseline:  -0.9894833333333334 -1.0 -0.9894833333333334
probs:  [0.14719855247256852, 0.14719855247256852, 0.14719855247256852, 0.2590366552897275, 0.2590366552897275, 0.04033103200283935]
maxi score, test score, baseline:  -0.9894833333333334 -1.0 -0.9894833333333334
maxi score, test score, baseline:  -0.9894833333333334 -1.0 -0.9894833333333334
probs:  [0.14719855247256852, 0.14719855247256852, 0.14719855247256852, 0.2590366552897275, 0.2590366552897275, 0.04033103200283935]
printing an ep nov before normalisation:  28.001411068486686
printing an ep nov before normalisation:  52.72400296124215
maxi score, test score, baseline:  -0.9894833333333334 -1.0 -0.9894833333333334
probs:  [0.1657338256434226, 0.1657338256434226, 0.1657338256434226, 0.1657338256434226, 0.29166736378137076, 0.0453973336449388]
siam score:  -0.57335335
maxi score, test score, baseline:  -0.9894833333333334 -1.0 -0.9894833333333334
probs:  [0.1657338256434226, 0.1657338256434226, 0.1657338256434226, 0.1657338256434226, 0.29166736378137076, 0.0453973336449388]
maxi score, test score, baseline:  -0.9894833333333334 -1.0 -0.9894833333333334
probs:  [0.1657338256434226, 0.1657338256434226, 0.1657338256434226, 0.1657338256434226, 0.29166736378137076, 0.0453973336449388]
maxi score, test score, baseline:  -0.9894833333333334 -1.0 -0.9894833333333334
probs:  [0.1657338256434226, 0.1657338256434226, 0.1657338256434226, 0.1657338256434226, 0.29166736378137076, 0.0453973336449388]
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.685424474367842
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
probs:  [0.1657338256197743, 0.1657338256197743, 0.1657338256197743, 0.1657338256197743, 0.29166736695025997, 0.04539733057064292]
Printing some Q and Qe and total Qs values:  [[0.986]
 [1.098]
 [0.518]
 [1.174]
 [1.098]
 [0.986]
 [1.098]] [[70.62 ]
 [46.503]
 [56.659]
 [88.903]
 [46.503]
 [59.542]
 [46.503]] [[1.858]
 [1.573]
 [1.161]
 [2.349]
 [1.573]
 [1.675]
 [1.573]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  49.9108567148747
printing an ep nov before normalisation:  35.50052319283365
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
probs:  [0.23058031065075424, 0.23058031065075424, 0.23058031065075424, 0.038839378698491474, 0.23058031065075424, 0.038839378698491474]
siam score:  -0.6039676
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
probs:  [0.23058031065075424, 0.23058031065075424, 0.23058031065075424, 0.038839378698491474, 0.23058031065075424, 0.038839378698491474]
Printing some Q and Qe and total Qs values:  [[0.859]
 [0.883]
 [0.872]
 [0.883]
 [0.89 ]
 [0.893]
 [0.886]] [[43.534]
 [41.99 ]
 [41.287]
 [41.99 ]
 [41.308]
 [41.597]
 [40.479]] [[1.165]
 [1.169]
 [1.149]
 [1.169]
 [1.168]
 [1.174]
 [1.153]]
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
probs:  [0.23058031065075424, 0.23058031065075424, 0.23058031065075424, 0.038839378698491474, 0.23058031065075424, 0.038839378698491474]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9896959183673469 -1.0 -0.9896959183673469
probs:  [0.23058031317895156, 0.23058031317895156, 0.23058031317895156, 0.03883937364209697, 0.23058031317895156, 0.03883937364209697]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9896959183673469 -1.0 -0.9896959183673469
probs:  [0.2853027233100064, 0.048030610023326946, 0.2853027233100064, 0.048030610023326946, 0.2853027233100064, 0.048030610023326946]
printing an ep nov before normalisation:  15.847605936054576
maxi score, test score, baseline:  -0.9896959183673469 -1.0 -0.9896959183673469
probs:  [0.06294633106393417, 0.06294633106393417, 0.3741073378721317, 0.06294633106393417, 0.3741073378721317, 0.06294633106393417]
maxi score, test score, baseline:  -0.9896959183673469 -1.0 -0.9896959183673469
probs:  [0.06294633106393417, 0.06294633106393417, 0.3741073378721317, 0.06294633106393417, 0.3741073378721317, 0.06294633106393417]
printing an ep nov before normalisation:  32.4138943328228
using explorer policy with actor:  1
printing an ep nov before normalisation:  57.5696460760895
maxi score, test score, baseline:  -0.9896959183673469 -1.0 -0.9896959183673469
maxi score, test score, baseline:  -0.9896959183673469 -1.0 -0.9896959183673469
probs:  [0.16575532582308347, 0.16575532582308347, 0.2915203622375679, 0.16575532582308347, 0.04545833447009819, 0.16575532582308347]
Starting evaluation
maxi score, test score, baseline:  -0.9896959183673469 -1.0 -0.9896959183673469
probs:  [0.18842429558113769, 0.18842429558113769, 0.3314029998762343, 0.05166205669017631, 0.05166205669017631, 0.18842429558113769]
printing an ep nov before normalisation:  70.71236177611189
maxi score, test score, baseline:  -0.9896959183673469 -1.0 -0.9896959183673469
probs:  [0.18842429558113769, 0.18842429558113769, 0.3314029998762343, 0.05166205669017631, 0.05166205669017631, 0.18842429558113769]
printing an ep nov before normalisation:  63.01706177847726
printing an ep nov before normalisation:  45.60190982585455
printing an ep nov before normalisation:  45.09551299896331
printing an ep nov before normalisation:  46.44544064303712
printing an ep nov before normalisation:  44.44637863892365
printing an ep nov before normalisation:  30.6194767688303
printing an ep nov before normalisation:  41.18062496185303
printing an ep nov before normalisation:  26.689867973327637
printing an ep nov before normalisation:  30.730023093074887
printing an ep nov before normalisation:  21.477413177490234
printing an ep nov before normalisation:  38.47719994927288
printing an ep nov before normalisation:  40.24404023131467
printing an ep nov before normalisation:  50.3363749685564
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.1884242991688578, 0.1884242991688578, 0.33140302704040075, 0.051662037726512844, 0.051662037726512844, 0.1884242991688578]
printing an ep nov before normalisation:  16.484259366989136
printing an ep nov before normalisation:  25.165530736787165
using explorer policy with actor:  0
printing an ep nov before normalisation:  48.7273295853763
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.448]
 [0.455]
 [0.484]
 [0.455]
 [0.455]
 [0.455]] [[22.145]
 [21.304]
 [21.655]
 [24.672]
 [21.655]
 [21.655]
 [21.655]] [[0.451]
 [0.448]
 [0.455]
 [0.484]
 [0.455]
 [0.455]
 [0.455]]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.18842430027321858, 0.18842430027321858, 0.33140303540198696, 0.05166203188917857, 0.05166203188917857, 0.18842430027321858]
maxi score, test score, baseline:  -0.9907256880733946 -1.0 -0.9907256880733946
printing an ep nov before normalisation:  27.16216496985088
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
probs:  [0.28516659632022306, 0.048166737013110285, 0.28516659632022306, 0.048166737013110285, 0.048166737013110285, 0.28516659632022306]
actions average: 
K:  2  action  0 :  tensor([0.1648, 0.1193, 0.1282, 0.1412, 0.1543, 0.1610, 0.1313],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0322, 0.7634, 0.0317, 0.0687, 0.0273, 0.0412, 0.0357],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1531, 0.0173, 0.1893, 0.1018, 0.1166, 0.3215, 0.1004],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2349, 0.0455, 0.1451, 0.1403, 0.1801, 0.1305, 0.1236],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1721, 0.0618, 0.1183, 0.1219, 0.2880, 0.1202, 0.1178],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0854, 0.1411, 0.0986, 0.1112, 0.0884, 0.3870, 0.0883],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1294, 0.1240, 0.1277, 0.1765, 0.1071, 0.1212, 0.2141],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.2851666008510343, 0.04816673248229906, 0.2851666008510343, 0.04816673248229906, 0.04816673248229906, 0.2851666008510343]
printing an ep nov before normalisation:  52.958011627197266
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.2851666139686517, 0.04816671936468162, 0.2851666139686517, 0.04816671936468162, 0.04816671936468162, 0.2851666139686517]
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.2851666139686517, 0.04816671936468162, 0.2851666139686517, 0.04816671936468162, 0.04816671936468162, 0.2851666139686517]
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.28516662233842666, 0.04816671099490666, 0.28516662233842666, 0.04816671099490666, 0.04816671099490666, 0.28516662233842666]
Printing some Q and Qe and total Qs values:  [[-0.041]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]] [[19.111]
 [19.111]
 [19.111]
 [19.111]
 [19.111]
 [19.111]
 [19.111]] [[0.903]
 [0.903]
 [0.903]
 [0.903]
 [0.903]
 [0.903]
 [0.903]]
printing an ep nov before normalisation:  22.77601745904236
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
probs:  [0.28516662641647206, 0.04816670691686128, 0.28516662641647206, 0.04816670691686128, 0.04816670691686128, 0.28516662641647206]
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
probs:  [0.3737953707926112, 0.06310231460369442, 0.06310231460369442, 0.06310231460369442, 0.06310231460369442, 0.3737953707926112]
actions average: 
K:  1  action  0 :  tensor([0.2600, 0.0475, 0.1332, 0.1427, 0.1469, 0.1344, 0.1353],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0647, 0.6013, 0.0492, 0.0911, 0.0535, 0.0682, 0.0719],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1067, 0.0184, 0.3894, 0.1031, 0.1128, 0.1795, 0.0901],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1236, 0.0775, 0.1212, 0.2500, 0.1519, 0.1512, 0.1246],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2043, 0.0412, 0.1145, 0.1558, 0.1996, 0.1392, 0.1454],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1427, 0.0231, 0.1197, 0.1536, 0.1353, 0.2902, 0.1355],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1346, 0.0081, 0.1608, 0.1798, 0.1756, 0.1793, 0.1619],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  56.11953146294637
Printing some Q and Qe and total Qs values:  [[0.564]
 [0.519]
 [0.631]
 [0.639]
 [0.622]
 [0.623]
 [0.631]] [[48.154]
 [44.588]
 [44.22 ]
 [45.342]
 [44.923]
 [44.483]
 [44.22 ]] [[2.263]
 [2.015]
 [2.106]
 [2.178]
 [2.137]
 [2.113]
 [2.106]]
printing an ep nov before normalisation:  36.895010955014875
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.031345450090585085, 0.20586031171156832, 0.20586031171156832, 0.1452133030631415, 0.20586031171156832, 0.20586031171156832]
Printing some Q and Qe and total Qs values:  [[0.207]
 [0.207]
 [0.236]
 [0.269]
 [0.207]
 [0.207]
 [0.207]] [[32.662]
 [32.662]
 [35.855]
 [43.766]
 [32.662]
 [32.662]
 [32.662]] [[0.389]
 [0.389]
 [0.452]
 [0.569]
 [0.389]
 [0.389]
 [0.389]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.0356578175586207, 0.2342841371740454, 0.1652579693644295, 0.1652579693644295, 0.2342841371740454, 0.1652579693644295]
siam score:  -0.7056248
printing an ep nov before normalisation:  35.589026711735926
printing an ep nov before normalisation:  63.23932512656026
printing an ep nov before normalisation:  51.79079078801935
siam score:  -0.7179173
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.03524642718430654, 0.23501109440870055, 0.130966996895995, 0.1818821935511487, 0.23501109440870055, 0.1818821935511487]
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.03524642718430654, 0.23501109440870055, 0.130966996895995, 0.1818821935511487, 0.23501109440870055, 0.1818821935511487]
printing an ep nov before normalisation:  56.441170723682404
printing an ep nov before normalisation:  57.58032496370525
printing an ep nov before normalisation:  43.85748668084445
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.03932763544661413, 0.20300688696051322, 0.14617381351818748, 0.20300688696051322, 0.2623109635959844, 0.14617381351818748]
Printing some Q and Qe and total Qs values:  [[-0.003]
 [-0.003]
 [ 0.   ]
 [-0.   ]
 [-0.001]
 [ 0.005]
 [-0.003]] [[45.051]
 [45.038]
 [45.999]
 [45.139]
 [45.646]
 [45.217]
 [45.038]] [[1.573]
 [1.572]
 [1.639]
 [1.581]
 [1.615]
 [1.592]
 [1.572]]
printing an ep nov before normalisation:  67.93691881403012
printing an ep nov before normalisation:  93.05601086992685
printing an ep nov before normalisation:  31.70720328062188
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.03821344239355856, 0.25080905235549367, 0.10618618843581001, 0.17699113222982213, 0.25080905235549367, 0.17699113222982213]
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.03821344239355856, 0.25080905235549367, 0.10618618843581001, 0.17699113222982213, 0.25080905235549367, 0.17699113222982213]
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
printing an ep nov before normalisation:  48.69387535004636
printing an ep nov before normalisation:  24.858456230928237
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.03821344099054078, 0.2508090532745309, 0.10618618777521766, 0.17699113234258987, 0.2508090532745309, 0.17699113234258987]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.040076492182698494, 0.1721238977008671, 0.1721238977008671, 0.2198076830268724, 0.26952056560079274, 0.126347463787902]
STARTED EXPV TRAINING ON FRAME NO.  10901
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.03529502159074944, 0.18192728356056193, 0.18192728356056193, 0.23487782260521647, 0.23487782260521647, 0.1310947660776937]
printing an ep nov before normalisation:  53.942888391860166
printing an ep nov before normalisation:  54.93683017599378
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.03529502159074944, 0.18192728356056193, 0.18192728356056193, 0.23487782260521647, 0.23487782260521647, 0.1310947660776937]
printing an ep nov before normalisation:  15.923156633818962
printing an ep nov before normalisation:  32.16102792559454
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.039277843064354605, 0.1459289511965227, 0.1459289511965227, 0.26146765167303865, 0.26146765167303865, 0.1459289511965227]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  10901
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.04173198136120999, 0.15506948965579753, 0.15506948965579753, 0.27785179030826723, 0.2152077593631303, 0.15506948965579753]
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.47 ]
 [0.546]
 [0.514]
 [0.514]
 [0.514]
 [0.514]] [[40.155]
 [48.907]
 [41.497]
 [40.155]
 [40.155]
 [40.155]
 [40.155]] [[1.666]
 [2.022]
 [1.759]
 [1.666]
 [1.666]
 [1.666]
 [1.666]]
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.0401630422896032, 0.17248644599636195, 0.17248644599636195, 0.26989117372494886, 0.17248644599636195, 0.17248644599636195]
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.0401630422896032, 0.17248644599636195, 0.17248644599636195, 0.26989117372494886, 0.17248644599636195, 0.17248644599636195]
printing an ep nov before normalisation:  0.10984287090991529
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  10901
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.03350842238739911, 0.21900048346231216, 0.21900048346231216, 0.15474506361283213, 0.21900048346231216, 0.15474506361283213]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
printing an ep nov before normalisation:  65.23464776497748
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.04448410855844294, 0.2909766533075839, 0.20559034695657397, 0.04448410855844294, 0.2909766533075839, 0.12348812931137237]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
using another actor
printing an ep nov before normalisation:  65.51594644103886
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.09913892989526571, 0.2333816802344476, 0.2333816802344476, 0.03574652001287338, 0.2333816802344476, 0.16496950938851798]
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.0991389283772008, 0.2333816817342415, 0.2333816817342415, 0.03574651706970933, 0.2333816817342415, 0.1649695093503653]
printing an ep nov before normalisation:  19.72570341835535
maxi score, test score, baseline:  -0.9920875 -1.0 -0.9920875
probs:  [0.10641576377568286, 0.2505252487142542, 0.17708483812055903, 0.03836406255469061, 0.2505252487142542, 0.17708483812055903]
Printing some Q and Qe and total Qs values:  [[0.151]
 [0.142]
 [0.151]
 [0.151]
 [0.159]
 [0.151]
 [0.151]] [[42.786]
 [44.273]
 [42.786]
 [42.786]
 [47.779]
 [42.786]
 [42.786]] [[0.151]
 [0.142]
 [0.151]
 [0.151]
 [0.159]
 [0.151]
 [0.151]]
maxi score, test score, baseline:  -0.9920875 -1.0 -0.9920875
probs:  [0.10641576377568286, 0.2505252487142542, 0.17708483812055903, 0.03836406255469061, 0.2505252487142542, 0.17708483812055903]
printing an ep nov before normalisation:  49.44089889526367
printing an ep nov before normalisation:  68.9201286202049
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.12078089892234098, 0.21143326934405746, 0.21143326934405746, 0.03348602370142897, 0.21143326934405746, 0.21143326934405746]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.12078089892234098, 0.21143326934405746, 0.21143326934405746, 0.03348602370142897, 0.21143326934405746, 0.21143326934405746]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.12078089892234098, 0.21143326934405746, 0.21143326934405746, 0.03348602370142897, 0.21143326934405746, 0.21143326934405746]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.16542103681218923, 0.16542103681218923, 0.23393067880844143, 0.035875531946549494, 0.16542103681218923, 0.23393067880844143]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
using explorer policy with actor:  1
printing an ep nov before normalisation:  24.164588451385498
printing an ep nov before normalisation:  35.81500887156656
maxi score, test score, baseline:  -0.9922664122137405 -1.0 -0.9922664122137405
probs:  [0.20256128104753424, 0.03938088999074654, 0.2611388573243298, 0.09179640954232093, 0.20256128104753424, 0.20256128104753424]
maxi score, test score, baseline:  -0.9922664122137405 -1.0 -0.9922664122137405
probs:  [0.15488973923308297, 0.041729109111207245, 0.2767550332104873, 0.0972806911710373, 0.21467271363709256, 0.21467271363709256]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
probs:  [0.1548897391185892, 0.041729107896579076, 0.2767550342807531, 0.09728069049647527, 0.21467271410380165, 0.21467271410380165]
printing an ep nov before normalisation:  48.04291836068547
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  76.71249448793084
printing an ep nov before normalisation:  31.386303255845455
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.20689650426053896, 0.04488410280605393, 0.20689650426053896, 0.12441746352007434, 0.2924879616327194, 0.12441746352007434]
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.20689650426053896, 0.04488410280605393, 0.20689650426053896, 0.12441746352007434, 0.2924879616327194, 0.12441746352007434]
printing an ep nov before normalisation:  87.39570654899158
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
probs:  [0.20689650480365285, 0.044884101161956645, 0.20689650480365285, 0.12441746294969803, 0.29248796333134147, 0.12441746294969803]
siam score:  -0.8472874
Printing some Q and Qe and total Qs values:  [[0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]]
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
probs:  [0.16467990151832976, 0.059381348656474715, 0.16467990151832976, 0.059381348656474715, 0.3871975981320614, 0.16467990151832976]
actions average: 
K:  1  action  0 :  tensor([0.2370, 0.0026, 0.1641, 0.1607, 0.1383, 0.1302, 0.1670],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0098, 0.9410, 0.0078, 0.0184, 0.0070, 0.0062, 0.0100],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1743, 0.0013, 0.2373, 0.1649, 0.1277, 0.1279, 0.1667],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1760, 0.0116, 0.1451, 0.2140, 0.1420, 0.1352, 0.1761],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1808, 0.0062, 0.1601, 0.1697, 0.1897, 0.1363, 0.1571],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1623, 0.0060, 0.1543, 0.1617, 0.1261, 0.2344, 0.1551],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1693, 0.0771, 0.1699, 0.1501, 0.1233, 0.1035, 0.2067],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
probs:  [0.1885215776063674, 0.05234867098207979, 0.1885215776063674, 0.05234867098207979, 0.32973792521673817, 0.1885215776063674]
printing an ep nov before normalisation:  46.96582860870869
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
probs:  [0.19113400157128196, 0.11498591579129665, 0.19113400157128196, 0.04150969267025866, 0.27010238682459864, 0.19113400157128196]
Printing some Q and Qe and total Qs values:  [[-0.06 ]
 [-0.058]
 [-0.061]
 [-0.064]
 [-0.064]
 [-0.06 ]
 [-0.064]] [[32.463]
 [33.666]
 [31.165]
 [36.613]
 [36.613]
 [31.203]
 [36.613]] [[0.239]
 [0.263]
 [0.216]
 [0.308]
 [0.308]
 [0.218]
 [0.308]]
Printing some Q and Qe and total Qs values:  [[-0.128]
 [-0.128]
 [-0.109]
 [-0.104]
 [-0.109]
 [-0.114]
 [-0.117]] [[21.587]
 [39.8  ]
 [20.427]
 [21.561]
 [21.664]
 [22.56 ]
 [21.703]] [[-0.056]
 [ 0.061]
 [-0.045]
 [-0.033]
 [-0.037]
 [-0.036]
 [-0.044]]
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
probs:  [0.13293944344427922, 0.13293944344427922, 0.23237660156545578, 0.03699130841507431, 0.23237660156545578, 0.23237660156545578]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.040906561447491614, 0.14704680370408404, 0.25704669113364426, 0.040906561447491614, 0.25704669113364426, 0.25704669113364426]
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
probs:  [0.04090655916889815, 0.14704680334860062, 0.257046692771201, 0.04090655916889815, 0.257046692771201, 0.257046692771201]
from probs:  [0.04090655916889815, 0.14704680334860062, 0.257046692771201, 0.04090655916889815, 0.257046692771201, 0.257046692771201]
printing an ep nov before normalisation:  31.20515428391367
printing an ep nov before normalisation:  33.291286947429136
siam score:  -0.8608255
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
probs:  [0.040906556923935675, 0.14704680299836367, 0.2570466943845883, 0.040906556923935675, 0.2570466943845883, 0.2570466943845883]
Printing some Q and Qe and total Qs values:  [[0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]] [[40.873]
 [40.873]
 [40.873]
 [40.873]
 [40.873]
 [40.873]
 [40.873]] [[1.951]
 [1.951]
 [1.951]
 [1.951]
 [1.951]
 [1.951]
 [1.951]]
line 256 mcts: sample exp_bonus 49.444850853979084
maxi score, test score, baseline:  -0.9927057553956835 -1.0 -0.9927057553956835
probs:  [0.04595047719447237, 0.16522096379873613, 0.28882855900679144, 0.04595047719447237, 0.28882855900679144, 0.16522096379873613]
maxi score, test score, baseline:  -0.9927057553956835 -1.0 -0.9927057553956835
probs:  [0.07199646685530706, 0.07199646685530706, 0.2590693167819459, 0.07199646685530706, 0.4529448157968259, 0.07199646685530706]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.51 ]
 [0.492]
 [0.488]
 [0.474]
 [0.489]
 [0.488]
 [0.469]] [[64.455]
 [61.311]
 [66.284]
 [63.77 ]
 [65.869]
 [65.872]
 [63.652]] [[1.434]
 [1.33 ]
 [1.462]
 [1.379]
 [1.452]
 [1.451]
 [1.371]]
from probs:  [0.1356785218387711, 0.04899052359179663, 0.2254625200231371, 0.1356785218387711, 0.31851139086875285, 0.1356785218387711]
printing an ep nov before normalisation:  39.31504480727945
printing an ep nov before normalisation:  39.41911051702052
maxi score, test score, baseline:  -0.9927057553956835 -1.0 -0.9927057553956835
probs:  [0.15533175381273662, 0.041890546371289784, 0.21509096130421268, 0.15533175381273662, 0.27702323088628783, 0.15533175381273662]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9927057553956835 -1.0 -0.9927057553956835
probs:  [0.1776273811845579, 0.03865379333506958, 0.1776273811845579, 0.1776273811845579, 0.25083668192669883, 0.1776273811845579]
maxi score, test score, baseline:  -0.9927057553956835 -1.0 -0.9927057553956835
probs:  [0.18239907744062572, 0.035563243550341586, 0.18239907744062572, 0.18239907744062572, 0.2348404466871555, 0.18239907744062572]
maxi score, test score, baseline:  -0.9927057553956835 -1.0 -0.9927057553956835
probs:  [0.13879238333708305, 0.0374544421206481, 0.1921281418720492, 0.1921281418720492, 0.24736874892612123, 0.1921281418720492]
printing an ep nov before normalisation:  61.33595943450928
using explorer policy with actor:  0
printing an ep nov before normalisation:  21.88665196971897
printing an ep nov before normalisation:  69.86218958247031
actor:  1 policy actor:  1  step number:  37 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  37.3843745676086
printing an ep nov before normalisation:  1.676671559051215
printing an ep nov before normalisation:  45.17209053039551
siam score:  -0.8641202
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.465]
 [0.471]
 [0.45 ]
 [0.454]
 [0.459]
 [0.465]] [[16.677]
 [16.677]
 [19.467]
 [20.562]
 [20.168]
 [19.332]
 [16.677]] [[0.56 ]
 [0.56 ]
 [0.596]
 [0.587]
 [0.587]
 [0.582]
 [0.56 ]]
printing an ep nov before normalisation:  19.128882904038285
printing an ep nov before normalisation:  19.978153648237804
printing an ep nov before normalisation:  18.74353879183048
printing an ep nov before normalisation:  20.10502925609984
maxi score, test score, baseline:  -0.9927571428571429 -1.0 -0.9927571428571429
probs:  [0.08082402244530451, 0.08082402244530451, 0.5640842631109529, 0.08975075858993642, 0.09930393235875301, 0.0852130010497485]
Printing some Q and Qe and total Qs values:  [[0.18 ]
 [0.273]
 [0.381]
 [0.493]
 [0.612]
 [0.412]
 [0.628]] [[33.919]
 [29.098]
 [26.131]
 [24.079]
 [25.1  ]
 [31.167]
 [24.032]] [[0.856]
 [0.785]
 [0.791]
 [0.834]
 [0.987]
 [0.994]
 [0.967]]
maxi score, test score, baseline:  -0.9928078014184397 -1.0 -0.9928078014184397
probs:  [0.08113534198264391, 0.08113534198264391, 0.5662258192027525, 0.08565951102329243, 0.1001844747853747, 0.08565951102329243]
maxi score, test score, baseline:  -0.9928078014184397 -1.0 -0.9928078014184397
probs:  [0.08154341994473854, 0.08154341994473854, 0.5690777189442128, 0.08609038115170302, 0.09565467886290414, 0.08609038115170302]
maxi score, test score, baseline:  -0.9928577464788733 -1.0 -0.9928577464788733
probs:  [0.08139849844601217, 0.08139849844601217, 0.5679578407199148, 0.08630687915176015, 0.0966314040845405, 0.08630687915176015]
maxi score, test score, baseline:  -0.992906993006993 -1.0 -0.992906993006993
probs:  [0.0849641408950135, 0.08032188765603009, 0.5604709587089893, 0.08976113590862977, 0.09472074092270762, 0.08976113590862977]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.992906993006993 -1.0 -0.992906993006993
probs:  [0.08500382978638828, 0.08024624176197638, 0.5599070919489048, 0.08992000407828059, 0.09500282834616929, 0.08992000407828059]
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.408]
 [0.408]
 [0.708]
 [0.408]
 [0.408]
 [0.619]] [[55.261]
 [55.261]
 [55.261]
 [61.846]
 [55.261]
 [55.261]
 [63.675]] [[1.118]
 [1.118]
 [1.118]
 [1.529]
 [1.118]
 [1.118]
 [1.471]]
maxi score, test score, baseline:  -0.992906993006993 -1.0 -0.992906993006993
maxi score, test score, baseline:  -0.992906993006993 -1.0 -0.992906993006993
probs:  [0.0912564767771556, 0.07777249104822323, 0.542656164883583, 0.0912564767771556, 0.10100760769977334, 0.09605078281410925]
Printing some Q and Qe and total Qs values:  [[0.637]
 [0.67 ]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]] [[59.221]
 [69.504]
 [59.221]
 [59.221]
 [59.221]
 [59.221]
 [59.221]] [[1.336]
 [1.579]
 [1.336]
 [1.336]
 [1.336]
 [1.336]
 [1.336]]
maxi score, test score, baseline:  -0.992906993006993 -1.0 -0.992906993006993
printing an ep nov before normalisation:  52.34516216664309
maxi score, test score, baseline:  -0.992906993006993 -1.0 -0.992906993006993
probs:  [0.09144014337773106, 0.07764226496915277, 0.5417131224381272, 0.09144014337773106, 0.10141827013647693, 0.09634605570078107]
printing an ep nov before normalisation:  50.706469168061616
maxi score, test score, baseline:  -0.9929555555555556 -1.0 -0.9929555555555556
probs:  [0.09829041316037317, 0.07542903330385224, 0.526217568435626, 0.09342315164253323, 0.10331991672880775, 0.10331991672880775]
maxi score, test score, baseline:  -0.9929555555555556 -1.0 -0.9929555555555556
probs:  [0.09829041316037317, 0.07542903330385224, 0.526217568435626, 0.09342315164253323, 0.10331991672880775, 0.10331991672880775]
printing an ep nov before normalisation:  64.48658176670929
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]] [[36.516]
 [36.516]
 [36.516]
 [36.516]
 [36.516]
 [36.516]
 [36.516]] [[0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]]
printing an ep nov before normalisation:  41.131216149597094
maxi score, test score, baseline:  -0.9931432432432432 -1.0 -0.9931432432432432
probs:  [0.096405784502143, 0.07564201008840829, 0.5275019903349338, 0.096405784502143, 0.10202221528618596, 0.10202221528618596]
printing an ep nov before normalisation:  27.20630005804189
maxi score, test score, baseline:  -0.9931432432432432 -1.0 -0.9931432432432432
using explorer policy with actor:  0
printing an ep nov before normalisation:  21.117542318630456
maxi score, test score, baseline:  -0.9931885906040269 -1.0 -0.9931885906040269
probs:  [0.08784967721247663, 0.07689668659081562, 0.5361522743447061, 0.08784967721247663, 0.10562584231976248, 0.10562584231976248]
maxi score, test score, baseline:  -0.9931885906040269 -1.0 -0.9931885906040269
probs:  [0.08784967721247663, 0.07689668659081562, 0.5361522743447061, 0.08784967721247663, 0.10562584231976248, 0.10562584231976248]
printing an ep nov before normalisation:  23.897782793928904
maxi score, test score, baseline:  -0.9931885906040269 -1.0 -0.9931885906040269
probs:  [0.08784967721247663, 0.07689668659081562, 0.5361522743447061, 0.08784967721247663, 0.10562584231976248, 0.10562584231976248]
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.612]
 [0.555]
 [0.595]
 [0.589]
 [0.608]
 [0.607]] [[22.931]
 [31.122]
 [22.185]
 [19.099]
 [18.757]
 [19.392]
 [19.533]] [[0.621]
 [0.612]
 [0.555]
 [0.595]
 [0.589]
 [0.608]
 [0.607]]
Printing some Q and Qe and total Qs values:  [[0.437]
 [0.467]
 [0.453]
 [0.411]
 [0.412]
 [0.418]
 [0.397]] [[52.699]
 [65.482]
 [62.79 ]
 [54.7  ]
 [55.429]
 [58.021]
 [56.303]] [[0.437]
 [0.467]
 [0.453]
 [0.411]
 [0.412]
 [0.418]
 [0.397]]
maxi score, test score, baseline:  -0.9932774834437086 -1.0 -0.9932774834437086
probs:  [0.08849737679656453, 0.07724120854266113, 0.53851870411966, 0.08849737679656453, 0.10676558429060445, 0.10047974945394554]
printing an ep nov before normalisation:  61.111143886279834
printing an ep nov before normalisation:  46.171342781897046
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.0884974846543437, 0.07724107958848418, 0.538517765218415, 0.0884974846543437, 0.10676607648254188, 0.10048010940187153]
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
printing an ep nov before normalisation:  50.183672060003474
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.508]
 [0.514]
 [0.502]
 [0.513]
 [0.502]
 [0.504]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.546]
 [0.508]
 [0.514]
 [0.502]
 [0.513]
 [0.502]
 [0.504]]
printing an ep nov before normalisation:  38.63147497177124
printing an ep nov before normalisation:  83.68146956251886
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.363]
 [0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]] [[35.006]
 [37.896]
 [35.006]
 [35.006]
 [35.006]
 [35.006]
 [35.006]] [[0.982]
 [1.117]
 [0.982]
 [0.982]
 [0.982]
 [0.982]
 [0.982]]
from probs:  [0.08937355129143924, 0.07731405369439646, 0.5389047540944665, 0.08325103712678678, 0.10894552280139402, 0.10221108099151704]
printing an ep nov before normalisation:  33.70563873100835
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.08990696286354483, 0.07777538265580745, 0.5421251252539, 0.07777538265580745, 0.1095959209056105, 0.10282122566532981]
printing an ep nov before normalisation:  43.72495115489772
printing an ep nov before normalisation:  15.583228834757392
printing an ep nov before normalisation:  10.109407262393972
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.09018731724122979, 0.07753570940994693, 0.5403658515618744, 0.07753570940994693, 0.11072025454118067, 0.10365515783582123]
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
probs:  [0.09033141114144941, 0.0774125242368221, 0.5394616365317719, 0.0774125242368221, 0.1112981292325658, 0.1040837746205688]
Printing some Q and Qe and total Qs values:  [[0.624]
 [0.638]
 [0.667]
 [0.638]
 [0.638]
 [0.638]
 [0.638]] [[21.361]
 [42.129]
 [22.278]
 [42.129]
 [42.129]
 [42.129]
 [42.129]] [[0.624]
 [0.638]
 [0.667]
 [0.638]
 [0.638]
 [0.638]
 [0.638]]
printing an ep nov before normalisation:  65.97879100650485
maxi score, test score, baseline:  -0.9934064935064936 -1.0 -0.9934064935064936
probs:  [0.09098755718166186, 0.07797442969663765, 0.5433831008678165, 0.07797442969663765, 0.10484024127862317, 0.10484024127862317]
line 256 mcts: sample exp_bonus 50.15523295606549
using another actor
maxi score, test score, baseline:  -0.9934064935064936 -1.0 -0.9934064935064936
probs:  [0.08500949887041696, 0.07828253921816929, 0.5454406826574092, 0.07828253921816929, 0.1064923700179176, 0.1064923700179176]
printing an ep nov before normalisation:  65.79567612080965
printing an ep nov before normalisation:  37.557135720787365
using explorer policy with actor:  1
printing an ep nov before normalisation:  66.66960702483058
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.632]
 [0.648]
 [0.625]
 [0.613]
 [0.619]
 [0.616]
 [0.623]] [[55.602]
 [58.593]
 [58.831]
 [58.608]
 [58.015]
 [57.315]
 [56.611]] [[1.13 ]
 [1.2  ]
 [1.18 ]
 [1.164]
 [1.16 ]
 [1.145]
 [1.14 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9934483870967742 -1.0 -0.9934483870967742
probs:  [0.0831939285219329, 0.0831939285219329, 0.5347088808899342, 0.076742596152682, 0.11108033295675904, 0.11108033295675904]
printing an ep nov before normalisation:  67.02419451107602
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.08776724767028359, 0.0815104184999239, 0.5256589274285434, 0.07543761371692771, 0.11481289634216076, 0.11481289634216076]
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.08776724767028359, 0.0815104184999239, 0.5256589274285434, 0.07543761371692771, 0.11481289634216076, 0.11481289634216076]
printing an ep nov before normalisation:  60.35746280732603
Printing some Q and Qe and total Qs values:  [[0.207]
 [0.286]
 [0.225]
 [0.219]
 [0.261]
 [0.286]
 [0.286]] [[44.044]
 [42.921]
 [44.238]
 [44.443]
 [43.692]
 [42.921]
 [42.921]] [[1.846]
 [1.846]
 [1.877]
 [1.886]
 [1.875]
 [1.846]
 [1.846]]
printing an ep nov before normalisation:  41.874991224500846
Printing some Q and Qe and total Qs values:  [[0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.224]] [[17.507]
 [17.507]
 [17.507]
 [17.507]
 [17.507]
 [17.507]
 [17.507]] [[0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.224]]
Printing some Q and Qe and total Qs values:  [[0.159]
 [0.159]
 [0.159]
 [0.259]
 [0.159]
 [0.159]
 [0.159]] [[46.682]
 [46.682]
 [46.682]
 [64.788]
 [46.682]
 [46.682]
 [46.682]] [[0.159]
 [0.159]
 [0.159]
 [0.259]
 [0.159]
 [0.159]
 [0.159]]
Printing some Q and Qe and total Qs values:  [[0.703]
 [0.658]
 [0.521]
 [0.45 ]
 [0.45 ]
 [0.43 ]
 [0.43 ]] [[11.791]
 [13.975]
 [18.516]
 [19.228]
 [35.894]
 [35.812]
 [35.269]] [[0.703]
 [0.658]
 [0.521]
 [0.45 ]
 [0.45 ]
 [0.43 ]
 [0.43 ]]
printing an ep nov before normalisation:  18.516529213628132
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.274]
 [0.219]
 [0.219]
 [0.281]
 [0.219]
 [0.219]
 [0.219]] [[56.388]
 [45.662]
 [45.662]
 [58.569]
 [45.662]
 [45.662]
 [45.662]] [[0.274]
 [0.219]
 [0.219]
 [0.281]
 [0.219]
 [0.219]
 [0.219]]
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.494]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]] [[70.379]
 [70.172]
 [70.379]
 [70.379]
 [70.379]
 [70.379]
 [70.379]] [[1.825]
 [1.804]
 [1.825]
 [1.825]
 [1.825]
 [1.825]
 [1.825]]
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.092588160861207, 0.0747573870643373, 0.52096445323044, 0.08052616682214812, 0.11211710359111195, 0.11904672843075566]
printing an ep nov before normalisation:  43.72268815219048
maxi score, test score, baseline:  -0.9935708860759493 -1.0 -0.9935708860759493
probs:  [0.0927675650691073, 0.07458989381987646, 0.5197543092036463, 0.08047090510639236, 0.11267644310397926, 0.11974088369699822]
Printing some Q and Qe and total Qs values:  [[0.094]
 [0.013]
 [0.031]
 [0.044]
 [0.064]
 [0.081]
 [0.088]] [[37.895]
 [47.617]
 [36.876]
 [37.257]
 [37.539]
 [37.542]
 [37.916]] [[0.752]
 [1.01 ]
 [0.654]
 [0.681]
 [0.71 ]
 [0.727]
 [0.747]]
from probs:  [0.09331601249095918, 0.07503071919667446, 0.5228307584792355, 0.07503071919667446, 0.11334276228946165, 0.12044902834699474]
maxi score, test score, baseline:  -0.9935708860759493 -1.0 -0.9935708860759493
probs:  [0.09398347083481985, 0.07556720193723654, 0.5265747850835037, 0.07556720193723654, 0.1141536701036017, 0.1141536701036017]
printing an ep nov before normalisation:  24.693802934380564
maxi score, test score, baseline:  -0.9935708860759493 -1.0 -0.9935708860759493
maxi score, test score, baseline:  -0.9935708860759493 -1.0 -0.9935708860759493
from probs:  [0.09865595477994665, 0.08061950810247816, 0.5223253861926404, 0.07495091057527385, 0.10503808206582013, 0.11841015828384077]
printing an ep nov before normalisation:  0.23814381858642264
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
probs:  [0.09865655044548012, 0.0806194008663985, 0.5223230160164789, 0.07495058242725865, 0.10503892645038589, 0.11841152379399804]
printing an ep nov before normalisation:  46.863967838956384
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.563]] [[16.234]
 [16.234]
 [16.234]
 [16.234]
 [16.234]
 [16.234]
 [16.234]] [[0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.563]]
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
probs:  [0.09921869734627033, 0.07537746961540334, 0.5253024868732725, 0.07537746961540334, 0.10563748942765766, 0.11908638712199285]
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
probs:  [0.09952534517555493, 0.07522292374341812, 0.5241831395100404, 0.07522292374341812, 0.1060683047918995, 0.11977736303566898]
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
probs:  [0.09952534517555493, 0.07522292374341812, 0.5241831395100404, 0.07522292374341812, 0.1060683047918995, 0.11977736303566898]
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
probs:  [0.09952534517555493, 0.07522292374341812, 0.5241831395100404, 0.07522292374341812, 0.1060683047918995, 0.11977736303566898]
printing an ep nov before normalisation:  37.972209453582764
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
probs:  [0.09952534517555493, 0.07522292374341812, 0.5241831395100404, 0.07522292374341812, 0.1060683047918995, 0.11977736303566898]
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
probs:  [0.10263611647916251, 0.07366746385673821, 0.5133771992098298, 0.07913012406553822, 0.10896465289821523, 0.12222444349051609]
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  32.58430796628602
actions average: 
K:  2  action  0 :  tensor([0.1986, 0.0382, 0.1523, 0.1662, 0.1466, 0.1327, 0.1654],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0130, 0.8775, 0.0121, 0.0530, 0.0129, 0.0154, 0.0161],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1624, 0.0159, 0.1792, 0.1975, 0.1413, 0.1417, 0.1619],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1558, 0.0038, 0.1406, 0.2741, 0.1422, 0.1336, 0.1499],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1930, 0.0087, 0.1480, 0.1764, 0.1918, 0.1324, 0.1497],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1441, 0.0485, 0.1454, 0.1513, 0.1315, 0.2465, 0.1327],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1604, 0.0299, 0.1564, 0.1832, 0.1342, 0.1313, 0.2046],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
probs:  [0.10042023055629648, 0.0724881155957824, 0.5051550581818863, 0.08318212532352202, 0.1127963061080319, 0.12595816423448075]
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
printing an ep nov before normalisation:  69.65035259721746
siam score:  -0.86101013
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.1007285851717407, 0.07229141494617333, 0.5037444884724218, 0.08317878868967629, 0.11332843905629983, 0.12672828366368802]
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.1007285851717407, 0.07229141494617333, 0.5037444884724218, 0.08317878868967629, 0.11332843905629983, 0.12672828366368802]
using another actor
from probs:  [0.1007285851717407, 0.07229141494617333, 0.5037444884724218, 0.08317878868967629, 0.11332843905629983, 0.12672828366368802]
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.10104121675665827, 0.07209198599919318, 0.5023143537453512, 0.08317540577490846, 0.11386795284611975, 0.12750908487776924]
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.10104121675665827, 0.07209198599919318, 0.5023143537453512, 0.08317540577490846, 0.11386795284611975, 0.12750908487776924]
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
probs:  [0.10135821485960861, 0.07188977163145252, 0.5008642443603483, 0.0831719756102323, 0.11441500201300707, 0.12830079152535132]
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
probs:  [0.10135821485960861, 0.07188977163145252, 0.5008642443603483, 0.0831719756102323, 0.11441500201300707, 0.12830079152535132]
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
probs:  [0.10135821485960861, 0.07188977163145252, 0.5008642443603483, 0.0831719756102323, 0.11441500201300707, 0.12830079152535132]
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
probs:  [0.10135821485960861, 0.07188977163145252, 0.5008642443603483, 0.0831719756102323, 0.11441500201300707, 0.12830079152535132]
maxi score, test score, baseline:  -0.9938393939393939 -1.0 -0.9938393939393939
probs:  [0.10207797527612848, 0.07239956033811765, 0.5044216094919923, 0.08376215348581319, 0.11522779604866251, 0.12211090535928582]
maxi score, test score, baseline:  -0.9938393939393939 -1.0 -0.9938393939393939
Printing some Q and Qe and total Qs values:  [[0.472]
 [0.644]
 [0.472]
 [0.507]
 [0.488]
 [0.472]
 [0.472]] [[60.762]
 [75.456]
 [60.762]
 [67.891]
 [67.283]
 [60.762]
 [68.784]] [[0.954]
 [1.378]
 [0.954]
 [1.112]
 [1.083]
 [0.954]
 [1.092]]
UNIT TEST: sample policy line 217 mcts : [0.041 0.51  0.102 0.061 0.041 0.041 0.204]
maxi score, test score, baseline:  -0.9938393939393939 -1.0 -0.9938393939393939
probs:  [0.09757378179662993, 0.07299740218959819, 0.5085121516021889, 0.08493450085587073, 0.11799108177785626, 0.11799108177785626]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.532]
 [0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.534]] [[63.03 ]
 [67.178]
 [63.03 ]
 [63.03 ]
 [63.03 ]
 [63.03 ]
 [63.03 ]] [[1.624]
 [1.751]
 [1.624]
 [1.624]
 [1.624]
 [1.624]
 [1.624]]
maxi score, test score, baseline:  -0.9938393939393939 -1.0 -0.9938393939393939
probs:  [0.09783102293413042, 0.0728096515143997, 0.5071604196477613, 0.084962889061126, 0.11861800842129136, 0.11861800842129136]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.35 ]
 [0.535]
 [0.417]
 [0.415]
 [0.522]
 [0.359]] [[33.026]
 [33.641]
 [30.441]
 [33.96 ]
 [34.043]
 [33.009]
 [34.336]] [[1.52 ]
 [1.489]
 [1.46 ]
 [1.577]
 [1.581]
 [1.619]
 [1.545]]
maxi score, test score, baseline:  -0.9939119760479042 -1.0 -0.9939119760479042
probs:  [0.09372244950999294, 0.07473711241607905, 0.5206072673309736, 0.07473711241607905, 0.11443372633971723, 0.12176233198715804]
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.29267771400615
actions average: 
K:  2  action  0 :  tensor([0.1864, 0.0119, 0.1569, 0.1615, 0.1555, 0.1691, 0.1588],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0338, 0.8009, 0.0350, 0.0394, 0.0283, 0.0302, 0.0324],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1541, 0.0144, 0.2399, 0.1496, 0.1390, 0.1608, 0.1421],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1354, 0.1122, 0.1320, 0.1994, 0.1373, 0.1413, 0.1425],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1981, 0.0118, 0.1213, 0.1730, 0.2116, 0.1499, 0.1343],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1464, 0.0047, 0.1712, 0.1487, 0.1414, 0.2320, 0.1555],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1755, 0.0064, 0.1587, 0.1683, 0.1581, 0.1621, 0.1708],
       grad_fn=<DivBackward0>)
deleting a thread, now have 2 threads
Frames:  15017 train batches done:  1759 episodes:  563
maxi score, test score, baseline:  -0.9939119760479042 -1.0 -0.9939119760479042
probs:  [0.08862836639257171, 0.07549724026407705, 0.5258209608243362, 0.07549724026407705, 0.117278096127469, 0.117278096127469]
line 256 mcts: sample exp_bonus 50.91675878476204
maxi score, test score, baseline:  -0.9939119760479042 -1.0 -0.9939119760479042
printing an ep nov before normalisation:  40.5326806302395
maxi score, test score, baseline:  -0.9939119760479042 -1.0 -0.9939119760479042
probs:  [0.08862836639257171, 0.07549724026407705, 0.5258209608243362, 0.07549724026407705, 0.117278096127469, 0.117278096127469]
maxi score, test score, baseline:  -0.9939119760479042 -1.0 -0.9939119760479042
probs:  [0.08862836639257171, 0.07549724026407705, 0.5258209608243362, 0.07549724026407705, 0.117278096127469, 0.117278096127469]
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
actions average: 
K:  0  action  0 :  tensor([0.1851, 0.0051, 0.1466, 0.1506, 0.1982, 0.1445, 0.1698],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0192, 0.9129, 0.0156, 0.0145, 0.0111, 0.0086, 0.0182],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1641, 0.0086, 0.1610, 0.1643, 0.1694, 0.1613, 0.1712],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1638, 0.0153, 0.1467, 0.2047, 0.1579, 0.1411, 0.1706],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1597, 0.0033, 0.1400, 0.1594, 0.2191, 0.1424, 0.1760],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1492, 0.0028, 0.1663, 0.1336, 0.1405, 0.2369, 0.1707],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1671, 0.0391, 0.1551, 0.1545, 0.1580, 0.1395, 0.1868],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
probs:  [0.08862846648341116, 0.07549709032858934, 0.5258198693988195, 0.07549709032858934, 0.11727874173029527, 0.11727874173029527]
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
probs:  [0.08862846648341116, 0.07549709032858934, 0.5258198693988195, 0.07549709032858934, 0.11727874173029527, 0.11727874173029527]
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
probs:  [0.08862846648341116, 0.07549709032858934, 0.5258198693988195, 0.07549709032858934, 0.11727874173029527, 0.11727874173029527]
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
probs:  [0.08862846648341116, 0.07549709032858934, 0.5258198693988195, 0.07549709032858934, 0.11727874173029527, 0.11727874173029527]
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.445]
 [0.427]
 [0.426]
 [0.428]
 [0.429]
 [0.441]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.429]
 [0.445]
 [0.427]
 [0.426]
 [0.428]
 [0.429]
 [0.441]]
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.43 ]
 [0.503]
 [0.501]
 [0.473]
 [0.461]
 [0.538]] [[15.078]
 [13.577]
 [14.631]
 [14.399]
 [14.479]
 [15.281]
 [14.447]] [[1.486]
 [1.367]
 [1.522]
 [1.502]
 [1.48 ]
 [1.531]
 [1.543]]
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
printing an ep nov before normalisation:  47.08061746513545
printing an ep nov before normalisation:  41.2746583757313
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
probs:  [0.09430091619031256, 0.07427084273638777, 0.517192454335933, 0.07427084273638777, 0.11612472756548432, 0.12384021643549459]
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
probs:  [0.09941244264550173, 0.07982645149774915, 0.5129281465011282, 0.07365068852323255, 0.12075240314976954, 0.11342986768261881]
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
probs:  [0.09941244264550173, 0.07982645149774915, 0.5129281465011282, 0.07365068852323255, 0.12075240314976954, 0.11342986768261881]
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
probs:  [0.09941244264550173, 0.07982645149774915, 0.5129281465011282, 0.07365068852323255, 0.12075240314976954, 0.11342986768261881]
printing an ep nov before normalisation:  25.81554412841797
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
using explorer policy with actor:  0
printing an ep nov before normalisation:  27.03259501051713
from probs:  [0.09969758200438637, 0.07976354085721045, 0.5116799819373468, 0.07347803238738031, 0.12141676116474215, 0.11396410164893388]
maxi score, test score, baseline:  -0.9940176470588236 -1.0 -0.9940176470588236
probs:  [0.09998781753010422, 0.0796995056027022, 0.5104095096153772, 0.073302290130098, 0.12209299321219898, 0.11450788390951938]
from probs:  [0.09998781753010422, 0.0796995056027022, 0.5104095096153772, 0.073302290130098, 0.12209299321219898, 0.11450788390951938]
maxi score, test score, baseline:  -0.9940176470588236 -1.0 -0.9940176470588236
probs:  [0.10138237022922912, 0.07432415716726948, 0.5175359437835115, 0.07432415716726948, 0.12379626313876294, 0.10863710851395753]
line 256 mcts: sample exp_bonus 0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.9940860465116279 -1.0 -0.9940860465116279
probs:  [0.10138303310345052, 0.07432382416261879, 0.5175335292147081, 0.07432382416261879, 0.12379775095742299, 0.10863803839918072]
siam score:  -0.8521562
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
probs:  [0.09564389637048837, 0.07438112296162445, 0.5177913526410429, 0.07438112296162445, 0.12695623930095448, 0.11084626576426534]
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
probs:  [0.09564389637048837, 0.07438112296162445, 0.5177913526410429, 0.07438112296162445, 0.12695623930095448, 0.11084626576426534]
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
probs:  [0.09564389637048837, 0.07438112296162445, 0.5177913526410429, 0.07438112296162445, 0.12695623930095448, 0.11084626576426534]
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
probs:  [0.09643161538002704, 0.07499347841359869, 0.5220607096441251, 0.07499347841359869, 0.11976135266702259, 0.11175936548162795]
printing an ep nov before normalisation:  41.29657845936183
printing an ep nov before normalisation:  35.88378667831421
printing an ep nov before normalisation:  44.67159597045644
actor:  1 policy actor:  1  step number:  46 total reward:  0.20999999999999963  reward:  1.0 rdn_beta:  1.333
using another actor
from probs:  [0.09643161538002704, 0.07499347841359869, 0.5220607096441251, 0.07499347841359869, 0.11976135266702259, 0.11175936548162795]
printing an ep nov before normalisation:  40.37974598288088
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
probs:  [0.07133212948966484, 0.060660368255475654, 0.2832074466137071, 0.4228923579259467, 0.08294551671510605, 0.07896218100009963]
printing an ep nov before normalisation:  43.692844445494735
printing an ep nov before normalisation:  1.1788660275593088
from probs:  [0.07193770014435506, 0.061175064699298884, 0.2811344825370897, 0.4264873035781953, 0.07963272452053051, 0.07963272452053051]
actions average: 
K:  0  action  0 :  tensor([0.2143, 0.0099, 0.1440, 0.1384, 0.1958, 0.1441, 0.1535],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0291, 0.8200, 0.0278, 0.0337, 0.0274, 0.0282, 0.0339],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1484, 0.0101, 0.2215, 0.1541, 0.1498, 0.1720, 0.1441],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1612, 0.0141, 0.1579, 0.1665, 0.1737, 0.1707, 0.1559],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1588, 0.0026, 0.1526, 0.1643, 0.1970, 0.1707, 0.1539],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1469, 0.0169, 0.1526, 0.1383, 0.1400, 0.2517, 0.1536],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1722, 0.0543, 0.1393, 0.1536, 0.1787, 0.1422, 0.1596],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9942181818181818 -1.0 -0.9942181818181818
probs:  [0.07452782963478358, 0.06037068519974534, 0.28369039835781895, 0.42086923504226575, 0.07831992189416885, 0.08222192987121743]
printing an ep nov before normalisation:  54.94501571526086
printing an ep nov before normalisation:  39.793387269276096
printing an ep nov before normalisation:  44.45132255554199
printing an ep nov before normalisation:  13.638913894542384
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.38 ]
 [0.403]
 [0.412]
 [0.409]
 [0.397]
 [0.394]] [[54.056]
 [42.775]
 [47.737]
 [51.465]
 [50.018]
 [48.652]
 [47.407]] [[1.448]
 [1.063]
 [1.236]
 [1.358]
 [1.312]
 [1.257]
 [1.217]]
maxi score, test score, baseline:  -0.9942181818181818 -1.0 -0.9942181818181818
probs:  [0.0712229820623255, 0.06049529360648721, 0.2848003685600833, 0.4217122261048807, 0.07888561667363864, 0.08288351299258456]
Printing some Q and Qe and total Qs values:  [[0.393]
 [0.393]
 [0.393]
 [0.45 ]
 [0.393]
 [0.393]
 [0.393]] [[36.267]
 [36.267]
 [36.267]
 [78.861]
 [36.267]
 [36.267]
 [36.267]] [[0.735]
 [0.735]
 [0.735]
 [1.469]
 [0.735]
 [0.735]
 [0.735]]
maxi score, test score, baseline:  -0.9942181818181818 -1.0 -0.9942181818181818
probs:  [0.07102715455581313, 0.06012647257658775, 0.2880486664826558, 0.4191086284908612, 0.07881335596954561, 0.08287572192453639]
printing an ep nov before normalisation:  51.91521874340571
siam score:  -0.85463595
maxi score, test score, baseline:  -0.9942181818181818 -1.0 -0.9942181818181818
printing an ep nov before normalisation:  60.64865050288798
siam score:  -0.8531355
maxi score, test score, baseline:  -0.9942181818181818 -1.0 -0.9942181818181818
probs:  [0.07063183605141528, 0.05938193070641327, 0.2946060306622833, 0.41385272581835836, 0.07866748272641677, 0.08285999403511313]
printing an ep nov before normalisation:  71.26828193664551
maxi score, test score, baseline:  -0.9942502824858758 -1.0 -0.9942502824858758
printing an ep nov before normalisation:  22.763619422912598
using another actor
maxi score, test score, baseline:  -0.9942502824858758 -1.0 -0.9942502824858758
maxi score, test score, baseline:  -0.9942502824858758 -1.0 -0.9942502824858758
printing an ep nov before normalisation:  30.481469408094338
maxi score, test score, baseline:  -0.9942502824858758 -1.0 -0.9942502824858758
probs:  [0.07070927149997015, 0.05881258749224752, 0.2978412713984064, 0.4097895169060844, 0.07920690293405772, 0.08364044976923388]
maxi score, test score, baseline:  -0.9942502824858758 -1.0 -0.9942502824858758
probs:  [0.07070927149997015, 0.05881258749224752, 0.2978412713984064, 0.4097895169060844, 0.07920690293405772, 0.08364044976923388]
maxi score, test score, baseline:  -0.9942820224719101 -1.0 -0.9942820224719101
probs:  [0.07070950953452101, 0.05881266708866621, 0.2978396524825807, 0.40979005673638336, 0.07920725413870296, 0.08364086001914581]
siam score:  -0.852527
maxi score, test score, baseline:  -0.9942820224719101 -1.0 -0.9942820224719101
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.392]
 [0.413]
 [0.449]
 [0.449]
 [0.443]
 [0.449]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.449]
 [0.392]
 [0.413]
 [0.449]
 [0.449]
 [0.443]
 [0.449]]
printing an ep nov before normalisation:  95.14512341991346
printing an ep nov before normalisation:  88.71626232197296
printing an ep nov before normalisation:  44.149191922523414
printing an ep nov before normalisation:  49.24957775377329
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.07114578419767507, 0.05895735495703547, 0.30383784977344463, 0.41076972135347845, 0.07543748463452003, 0.0798518050838463]
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.07114578419767507, 0.05895735495703547, 0.30383784977344463, 0.41076972135347845, 0.07543748463452003, 0.0798518050838463]
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.0707680654042354, 0.05820292001242952, 0.3106521044180872, 0.4054413285365415, 0.07519241237318113, 0.07974316925552534]
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.0707680654042354, 0.05820292001242952, 0.3106521044180872, 0.4054413285365415, 0.07519241237318113, 0.07974316925552534]
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.0707680654042354, 0.05820292001242952, 0.3106521044180872, 0.4054413285365415, 0.07519241237318113, 0.07974316925552534]
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.0707680654042354, 0.05820292001242952, 0.3106521044180872, 0.4054413285365415, 0.07519241237318113, 0.07974316925552534]
printing an ep nov before normalisation:  42.12877077431829
printing an ep nov before normalisation:  61.88305351832929
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.0707680654042354, 0.05820292001242952, 0.3106521044180872, 0.4054413285365415, 0.07519241237318113, 0.07974316925552534]
printing an ep nov before normalisation:  43.96508575333988
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.07364375003217741, 0.057495074641892005, 0.30795024717007174, 0.40053552779531315, 0.07796522654507067, 0.08241017381547515]
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.07335525329326312, 0.056727416479581357, 0.31461406273951903, 0.39511651810724485, 0.07780495610255829, 0.08238179327783325]
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.578]
 [0.486]
 [0.501]
 [0.491]
 [0.523]
 [0.535]] [[63.721]
 [61.481]
 [61.835]
 [62.361]
 [61.788]
 [62.265]
 [60.888]] [[2.176]
 [2.144]
 [2.07 ]
 [2.114]
 [2.073]
 [2.131]
 [2.07 ]]
printing an ep nov before normalisation:  53.413882161811
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.07369219002486355, 0.05698783895373459, 0.31606117025666497, 0.39693406380370144, 0.07816236848051776, 0.07816236848051776]
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.07369219002486355, 0.05698783895373459, 0.31606117025666497, 0.39693406380370144, 0.07816236848051776, 0.07816236848051776]
printing an ep nov before normalisation:  43.328562121644964
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.06964694934695327, 0.05723630506747513, 0.31744183904156564, 0.3986681634324405, 0.0785033715557827, 0.0785033715557827]
printing an ep nov before normalisation:  61.85439623112387
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.06515369549765661, 0.05671442447270193, 0.32576086702017126, 0.39496286435655603, 0.07870407432645711, 0.07870407432645711]
printing an ep nov before normalisation:  66.01278774315169
printing an ep nov before normalisation:  15.71511514856214
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.06611257790473948, 0.057548849633158636, 0.32524886254681606, 0.4007855721991361, 0.07515206885807484, 0.07515206885807484]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.06611257790473948, 0.057548849633158636, 0.32524886254681606, 0.4007855721991361, 0.07515206885807484, 0.07515206885807484]
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
maxi score, test score, baseline:  -0.994313407821229 -1.0 -0.994313407821229
probs:  [0.06611257790473948, 0.057548849633158636, 0.32524886254681606, 0.4007855721991361, 0.07515206885807484, 0.07515206885807484]
maxi score, test score, baseline:  -0.9943444444444445 -1.0 -0.9943444444444445
probs:  [0.06585867983673171, 0.05716871102103455, 0.3288095744268601, 0.3981001853198826, 0.07503142469774551, 0.07503142469774551]
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.516]
 [0.477]
 [0.466]
 [0.477]
 [0.477]
 [0.477]] [[65.758]
 [74.989]
 [65.758]
 [67.469]
 [65.758]
 [65.758]
 [65.758]] [[2.055]
 [2.516]
 [2.055]
 [2.123]
 [2.055]
 [2.055]
 [2.055]]
printing an ep nov before normalisation:  68.76213085868821
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.6  ]
 [0.665]
 [0.638]
 [0.608]
 [0.549]
 [0.549]] [[64.885]
 [69.652]
 [67.582]
 [66.847]
 [65.817]
 [64.885]
 [64.885]] [[2.   ]
 [2.231]
 [2.218]
 [2.164]
 [2.094]
 [2.   ]
 [2.   ]]
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.06585888230955339, 0.057168793337403136, 0.32880807519015604, 0.3981007411581301, 0.07503175400237869, 0.07503175400237869]
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.06616613230615934, 0.057435419474352635, 0.33034454287861537, 0.39996113367020425, 0.07538188473973306, 0.07071088693093541]
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.533]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]] [[45.543]
 [71.965]
 [45.543]
 [45.543]
 [45.543]
 [45.543]
 [45.543]] [[0.846]
 [1.619]
 [0.846]
 [0.846]
 [0.846]
 [0.846]
 [0.846]]
Printing some Q and Qe and total Qs values:  [[0.737]
 [0.954]
 [0.624]
 [0.624]
 [0.672]
 [0.624]
 [0.624]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.737]
 [0.954]
 [0.624]
 [0.624]
 [0.672]
 [0.624]
 [0.624]]
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.59 ]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]] [[60.782]
 [62.775]
 [60.782]
 [60.782]
 [60.782]
 [60.782]
 [60.782]] [[1.699]
 [1.894]
 [1.699]
 [1.699]
 [1.699]
 [1.699]
 [1.699]]
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.06532460343039481, 0.0568356880684104, 0.3334996549275792, 0.3957827238943934, 0.07427866483961115, 0.07427866483961115]
siam score:  -0.8510231
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.0651609313262828, 0.056379621692735214, 0.3370776004380949, 0.39253502932091994, 0.07442340861098362, 0.07442340861098362]
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
probs:  [0.06490214809750144, 0.05599601750610604, 0.34068394672982094, 0.389825315977392, 0.07429628584458976, 0.07429628584458976]
maxi score, test score, baseline:  -0.994375138121547 -1.0 -0.994375138121547
printing an ep nov before normalisation:  75.11350248282857
Printing some Q and Qe and total Qs values:  [[0.648]
 [0.707]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]] [[56.286]
 [65.371]
 [56.286]
 [56.286]
 [56.286]
 [56.286]
 [56.286]] [[0.87 ]
 [0.987]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [0.87 ]]
printing an ep nov before normalisation:  22.977014238624733
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.907]
 [0.516]] [[46.473]
 [46.473]
 [46.473]
 [46.473]
 [46.473]
 [45.409]
 [46.473]] [[1.115]
 [1.115]
 [1.115]
 [1.115]
 [1.115]
 [1.486]
 [1.115]]
from probs:  [0.05598034212799478, 0.05598034212799478, 0.34750207984751524, 0.38961112509225093, 0.07546305540212211, 0.07546305540212211]
maxi score, test score, baseline:  -0.9944054945054945 -1.0 -0.9944054945054945
probs:  [0.05553278826411859, 0.059994357119503156, 0.3398128221868512, 0.38653407208421997, 0.07906298017265354, 0.07906298017265354]
using another actor
maxi score, test score, baseline:  -0.9944355191256831 -1.0 -0.9944355191256831
probs:  [0.05553285934122265, 0.05999449057222291, 0.3398113421377736, 0.3865345475146887, 0.07906338021704602, 0.07906338021704602]
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
probs:  [0.05553292964259406, 0.0599946225688594, 0.33980987823727554, 0.3865350177561605, 0.0790637758975552, 0.0790637758975552]
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
probs:  [0.055834643842705076, 0.060320635746079494, 0.3362180222865841, 0.3886394136502033, 0.07949364223721404, 0.07949364223721404]
printing an ep nov before normalisation:  23.223672857368456
printing an ep nov before normalisation:  29.171122298539558
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
printing an ep nov before normalisation:  28.238295749140168
using explorer policy with actor:  1
siam score:  -0.85241926
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
probs:  [0.05603844671599367, 0.06063505693766574, 0.33784956358167756, 0.3900258058192527, 0.07517028385484491, 0.08028084309056546]
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
probs:  [0.05603844671599367, 0.06063505693766574, 0.33784956358167756, 0.3900258058192527, 0.07517028385484491, 0.08028084309056546]
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
probs:  [0.05565705090758338, 0.06031781198612092, 0.34140115833572987, 0.3873303073113093, 0.0750558943155504, 0.08023777714370599]
printing an ep nov before normalisation:  29.396924624747456
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
probs:  [0.05565705090758338, 0.06031781198612092, 0.34140115833572987, 0.3873303073113093, 0.0750558943155504, 0.08023777714370599]
printing an ep nov before normalisation:  27.86865404826866
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
probs:  [0.05596228369531683, 0.06064866651584703, 0.33778428508125513, 0.3894588574625035, 0.07546776894833437, 0.0806781382967432]
Printing some Q and Qe and total Qs values:  [[0.246]
 [0.263]
 [0.227]
 [0.169]
 [0.205]
 [0.222]
 [0.215]] [[51.86 ]
 [50.852]
 [52.883]
 [60.902]
 [54.262]
 [52.112]
 [51.909]] [[0.246]
 [0.263]
 [0.227]
 [0.169]
 [0.205]
 [0.222]
 [0.215]]
maxi score, test score, baseline:  -0.9945236559139785 -1.0 -0.9945236559139785
probs:  [0.05596235877576161, 0.060648806720540814, 0.33778280265194716, 0.3894593599245936, 0.07546811508646405, 0.08067855684069283]
printing an ep nov before normalisation:  41.332130432128906
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]] [[66.274]
 [66.274]
 [66.274]
 [66.274]
 [66.274]
 [66.274]
 [66.274]] [[1.372]
 [1.372]
 [1.372]
 [1.372]
 [1.372]
 [1.372]
 [1.372]]
printing an ep nov before normalisation:  52.40156139954516
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.05685274602211228, 0.06161405308026416, 0.3376770910518891, 0.39566845998252664, 0.07151757176122009, 0.07667007810198766]
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
Printing some Q and Qe and total Qs values:  [[0.452]
 [0.455]
 [0.449]
 [0.448]
 [0.452]
 [0.457]
 [0.457]] [[75.423]
 [79.141]
 [73.903]
 [74.702]
 [72.439]
 [74.028]
 [72.108]] [[1.395]
 [1.488]
 [1.356]
 [1.374]
 [1.323]
 [1.367]
 [1.32 ]]
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.05746679607829922, 0.06227964942781959, 0.330513941836086, 0.3999505560812652, 0.07229038439482191, 0.07749867218170824]
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.05777112026461616, 0.06260952003827626, 0.3269638736704664, 0.40207276973119394, 0.07267339156748921, 0.0779093247279581]
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.057707367456321136, 0.06264160263198548, 0.3269112697164849, 0.40159049399416036, 0.07290481179736731, 0.0782444544036809]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9946089947089947 -1.0 -0.9946089947089947
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9946089947089947 -1.0 -0.9946089947089947
maxi score, test score, baseline:  -0.9946089947089947 -1.0 -0.9946089947089947
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.056833120569901474, 0.06211379237836425, 0.3338058449884207, 0.3953375193614649, 0.07309758973996686, 0.07881213296188168]
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
siam score:  -0.86260194
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.05617703670730735, 0.0665420911714488, 0.32590258870430433, 0.3907781497321629, 0.07745994854034448, 0.0831401851444321]
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.43 ]
 [0.376]
 [0.401]
 [0.37 ]
 [0.372]
 [0.374]] [[37.22 ]
 [62.691]
 [38.587]
 [38.647]
 [37.982]
 [35.62 ]
 [34.585]] [[0.876]
 [1.575]
 [0.897]
 [0.923]
 [0.875]
 [0.816]
 [0.791]]
actions average: 
K:  0  action  0 :  tensor([0.2073, 0.0021, 0.1646, 0.1778, 0.1555, 0.1483, 0.1443],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0100, 0.9476, 0.0136, 0.0103, 0.0043, 0.0051, 0.0091],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1911, 0.0017, 0.2198, 0.1621, 0.1343, 0.1489, 0.1421],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1934, 0.0119, 0.1793, 0.1828, 0.1404, 0.1435, 0.1487],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1666, 0.0062, 0.1411, 0.1612, 0.2663, 0.1361, 0.1225],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1758, 0.0017, 0.1613, 0.1481, 0.1264, 0.2486, 0.1381],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1834, 0.0567, 0.1560, 0.1634, 0.1535, 0.1399, 0.1470],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  61.2644277253079
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.056121551329318425, 0.06669170328138181, 0.3311842709537804, 0.3903512810937425, 0.07782559667088848, 0.07782559667088848]
printing an ep nov before normalisation:  82.5680223383031
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.056044536500052825, 0.06682010912084459, 0.33102105372863283, 0.38977354275431253, 0.0781703789480785, 0.0781703789480785]
printing an ep nov before normalisation:  28.160926273890905
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]] [[63.928]
 [63.928]
 [63.928]
 [63.928]
 [63.928]
 [63.928]
 [63.928]] [[2.106]
 [2.106]
 [2.106]
 [2.106]
 [2.106]
 [2.106]
 [2.106]]
Printing some Q and Qe and total Qs values:  [[0.39 ]
 [0.515]
 [0.382]
 [0.387]
 [0.382]
 [0.384]
 [0.388]] [[35.771]
 [49.786]
 [35.694]
 [35.964]
 [35.622]
 [35.404]
 [35.041]] [[0.39 ]
 [0.515]
 [0.382]
 [0.387]
 [0.382]
 [0.384]
 [0.388]]
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.0562963228562499, 0.06734646177064385, 0.33279553705678816, 0.39148611081434415, 0.07308962607483536, 0.07898594142713868]
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.0562963228562499, 0.06734646177064385, 0.33279553705678816, 0.39148611081434415, 0.07308962607483536, 0.07898594142713868]
maxi score, test score, baseline:  -0.9946916666666666 -1.0 -0.9946916666666666
probs:  [0.05694988499217423, 0.062467661143219506, 0.33666459507222196, 0.3960402040884414, 0.07393882735197146, 0.07393882735197146]
printing an ep nov before normalisation:  58.30031896456199
siam score:  -0.86280805
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
probs:  [0.05760149727984575, 0.05760149727984575, 0.34052207793260764, 0.4005807102068148, 0.07478549846317721, 0.06890871883770888]
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
line 256 mcts: sample exp_bonus 48.4186113585927
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.713]
 [0.717]
 [0.709]
 [0.708]
 [0.7  ]
 [0.663]] [[56.173]
 [57.211]
 [56.746]
 [57.728]
 [56.184]
 [54.543]
 [53.241]] [[1.611]
 [1.689]
 [1.678]
 [1.703]
 [1.651]
 [1.589]
 [1.51 ]]
printing an ep nov before normalisation:  45.61142921447754
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
probs:  [0.05787428286280573, 0.05787428286280573, 0.33683995033239444, 0.40243722152997957, 0.0755012696371115, 0.06947299277490307]
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
probs:  [0.058224754830865984, 0.058224754830865984, 0.33888366542907994, 0.40487909361035207, 0.06989386564941798, 0.06989386564941798]
from probs:  [0.058224754830865984, 0.058224754830865984, 0.33888366542907994, 0.40487909361035207, 0.06989386564941798, 0.06989386564941798]
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
probs:  [0.05818474532282876, 0.05818474532282876, 0.33889466185938355, 0.4045547444682816, 0.0700905515133387, 0.0700905515133387]
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
probs:  [0.057482109941268233, 0.06305139890167279, 0.33049686099113806, 0.3997217353842706, 0.07462394739082523, 0.07462394739082523]
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.344]
 [0.334]
 [0.335]
 [0.334]
 [0.334]
 [0.326]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.333]
 [0.344]
 [0.334]
 [0.335]
 [0.334]
 [0.334]
 [0.326]]
UNIT TEST: sample policy line 217 mcts : [0.061 0.286 0.102 0.082 0.204 0.061 0.204]
printing an ep nov before normalisation:  80.99968067099397
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.05683661759897392, 0.06754588534283695, 0.3227124980959092, 0.39528204159471225, 0.07881147868378387, 0.07881147868378387]
siam score:  -0.8610955
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.05679875145963419, 0.06771891532478345, 0.3279104887192773, 0.3949764836580134, 0.0733890004086109, 0.0792063604296808]
printing an ep nov before normalisation:  61.54311765861955
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.05606118573566405, 0.06728451010643935, 0.33469940083609373, 0.3897520335543418, 0.07311200545280348, 0.07909086431465766]
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.055172083069496225, 0.07127934488529299, 0.3302934197986314, 0.38361657879008965, 0.07692376996604229, 0.08271480349044737]
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.721]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.371]
 [0.721]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]]
printing an ep nov before normalisation:  51.0453438431438
Printing some Q and Qe and total Qs values:  [[0.4  ]
 [0.679]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.4  ]] [[57.175]
 [74.373]
 [57.175]
 [57.175]
 [57.175]
 [57.175]
 [57.175]] [[0.927]
 [1.507]
 [0.927]
 [0.927]
 [0.927]
 [0.927]
 [0.927]]
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.0554929294355535, 0.07169408775949923, 0.33221807276642923, 0.3858520765488911, 0.07737141674481346, 0.07737141674481346]
UNIT TEST: sample policy line 217 mcts : [0.061 0.612 0.041 0.041 0.061 0.122 0.061]
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
actions average: 
K:  4  action  0 :  tensor([0.1979, 0.0430, 0.1452, 0.1599, 0.1545, 0.1412, 0.1583],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0754, 0.6500, 0.0466, 0.0705, 0.0440, 0.0387, 0.0749],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1562, 0.0428, 0.2298, 0.1449, 0.1429, 0.1540, 0.1295],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1564, 0.0548, 0.1328, 0.2392, 0.1444, 0.1189, 0.1535],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1823, 0.0142, 0.1161, 0.1425, 0.2830, 0.1257, 0.1362],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1341, 0.0029, 0.1539, 0.1562, 0.1376, 0.2827, 0.1325],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1614, 0.0307, 0.1448, 0.1752, 0.1545, 0.1359, 0.1975],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.055363460220173556, 0.07218726090169186, 0.3373134207405617, 0.3848658164372468, 0.07218726090169186, 0.07808278079863412]
Printing some Q and Qe and total Qs values:  [[0.132]
 [0.162]
 [0.034]
 [0.116]
 [0.12 ]
 [0.475]
 [0.104]] [[55.377]
 [63.961]
 [54.138]
 [66.395]
 [70.815]
 [19.215]
 [60.025]] [[0.132]
 [0.162]
 [0.034]
 [0.116]
 [0.12 ]
 [0.475]
 [0.104]]
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.05481174276636068, 0.07080495703049566, 0.33474577947286044, 0.38108099404320317, 0.07640595611877925, 0.08215057056830079]
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.05481174276636068, 0.07080495703049566, 0.33474577947286044, 0.38108099404320317, 0.07640595611877925, 0.08215057056830079]
printing an ep nov before normalisation:  73.8138539133829
printing an ep nov before normalisation:  64.59279764879795
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.05481174276636068, 0.07080495703049566, 0.33474577947286044, 0.38108099404320317, 0.07640595611877925, 0.08215057056830079]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.05481180446203346, 0.07080523672451623, 0.3347442427700994, 0.3810813989744161, 0.07640631215821274, 0.082151004910722]
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.05481186552909773, 0.07080551357019325, 0.3347427217190201, 0.3810817997796458, 0.07640666457192725, 0.08215143483011583]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.055097596706938846, 0.07117482188153589, 0.3312698040540297, 0.38307250194236236, 0.07680524251230202, 0.0825800329028313]
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
Printing some Q and Qe and total Qs values:  [[0.357]
 [0.421]
 [0.37 ]
 [0.368]
 [0.364]
 [0.36 ]
 [0.38 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.357]
 [0.421]
 [0.37 ]
 [0.368]
 [0.364]
 [0.36 ]
 [0.38 ]]
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.0535428110742319, 0.07461941808408186, 0.3334311948286423, 0.3722161552391003, 0.08022206045378885, 0.08596836032015504]
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.0535428110742319, 0.07461941808408186, 0.3334311948286423, 0.3722161552391003, 0.08022206045378885, 0.08596836032015504]
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.053851740709961966, 0.07505025820851166, 0.33535904394362814, 0.3743683428102536, 0.08068530716382237, 0.08068530716382237]
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.35125639851323
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.05502490536781452, 0.07107044188809104, 0.33223247445254056, 0.38254130552902843, 0.08244449309233773, 0.07668637967018781]
maxi score, test score, baseline:  -0.9948494949494949 -1.0 -0.9948494949494949
probs:  [0.05494174404687168, 0.07128005109520075, 0.3319975975305769, 0.38192051292691465, 0.0828616358383201, 0.07699845856211596]
maxi score, test score, baseline:  -0.9948494949494949 -1.0 -0.9948494949494949
probs:  [0.05494174404687168, 0.07128005109520075, 0.3319975975305769, 0.38192051292691465, 0.0828616358383201, 0.07699845856211596]
printing an ep nov before normalisation:  77.35068054650162
siam score:  -0.85600966
maxi score, test score, baseline:  -0.9948494949494949 -1.0 -0.9948494949494949
printing an ep nov before normalisation:  34.321898043478996
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.054573042844821916, 0.07112353392525125, 0.33522158658896584, 0.3793101032350416, 0.08285552760251771, 0.07691620580340158]
printing an ep nov before normalisation:  54.29105090004465
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.054573042844821916, 0.07112353392525125, 0.33522158658896584, 0.3793101032350416, 0.08285552760251771, 0.07691620580340158]
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.054573042844821916, 0.07112353392525125, 0.33522158658896584, 0.3793101032350416, 0.08285552760251771, 0.07691620580340158]
Printing some Q and Qe and total Qs values:  [[0.274]
 [0.225]
 [0.224]
 [0.238]
 [0.228]
 [0.229]
 [0.231]] [[41.   ]
 [47.473]
 [44.831]
 [46.681]
 [44.072]
 [45.809]
 [45.564]] [[0.819]
 [0.964]
 [0.884]
 [0.953]
 [0.865]
 [0.918]
 [0.913]]
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.054573042844821916, 0.07112353392525125, 0.33522158658896584, 0.3793101032350416, 0.08285552760251771, 0.07691620580340158]
printing an ep nov before normalisation:  52.48330797142654
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.05485776774157902, 0.07149522846023246, 0.3317466134757472, 0.38129317910937716, 0.08328887150130322, 0.07731833971176114]
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.05485776774157902, 0.07149522846023246, 0.3317466134757472, 0.38129317910937716, 0.08328887150130322, 0.07731833971176114]
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.05448736963400484, 0.07134099046681086, 0.33497365884308783, 0.3786703623672314, 0.08328786093057203, 0.0772397577582929]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.05477274545694425, 0.07171526603332762, 0.33148394064671405, 0.38065774559111243, 0.0837251540368398, 0.07764514823506177]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.05402498972571974, 0.071410572051997, 0.3379724249898802, 0.3753619582221675, 0.08373452914404159, 0.07749552586619401]
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.471]
 [0.435]
 [0.447]
 [0.606]
 [0.427]
 [0.606]] [[33.334]
 [41.039]
 [39.822]
 [41.079]
 [33.334]
 [41.29 ]
 [33.334]] [[1.099]
 [1.234]
 [1.155]
 [1.211]
 [1.099]
 [1.199]
 [1.099]]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.054312659945512605, 0.071791045216304, 0.3344421268391298, 0.3773649012638926, 0.08418078667408015, 0.07790848006108092]
printing an ep nov before normalisation:  81.59363979065962
printing an ep nov before normalisation:  60.70977634574435
printing an ep nov before normalisation:  58.34011408045576
Printing some Q and Qe and total Qs values:  [[1.24 ]
 [1.242]
 [0.589]
 [1.245]
 [0.589]
 [0.589]
 [0.589]] [[ 2.618]
 [ 2.247]
 [76.13 ]
 [ 2.878]
 [76.13 ]
 [76.13 ]
 [76.13 ]] [[1.269]
 [1.263]
 [2.256]
 [1.28 ]
 [2.256]
 [2.256]
 [2.256]]
printing an ep nov before normalisation:  2.587866120829858
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949980392156863 -1.0 -0.9949980392156863
probs:  [0.054654970965367596, 0.0722440026276493, 0.3365524379999469, 0.3797482609881403, 0.078400163709448, 0.078400163709448]
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.686511516571045
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.611]
 [0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.358]] [[60.044]
 [69.653]
 [60.044]
 [60.044]
 [60.044]
 [60.044]
 [60.044]] [[0.934]
 [1.337]
 [0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.934]]
maxi score, test score, baseline:  -0.9949980392156863 -1.0 -0.9949980392156863
probs:  [0.05457161039501534, 0.07248678588907495, 0.3363056990995539, 0.37912170999236405, 0.07875709731199587, 0.07875709731199587]
printing an ep nov before normalisation:  51.809120309841205
printing an ep nov before normalisation:  46.93415098504531
printing an ep nov before normalisation:  43.25216515942864
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.113999356238494
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([0.2270, 0.0072, 0.1344, 0.1962, 0.1840, 0.1175, 0.1337],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0371, 0.8410, 0.0117, 0.0363, 0.0301, 0.0136, 0.0300],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1756, 0.0144, 0.3000, 0.1539, 0.1280, 0.1125, 0.1157],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1647, 0.0125, 0.1386, 0.2377, 0.1657, 0.1309, 0.1498],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1821, 0.0101, 0.1495, 0.1802, 0.1991, 0.1322, 0.1468],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1548, 0.0076, 0.1378, 0.1498, 0.1320, 0.3017, 0.1164],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1725, 0.0102, 0.1448, 0.1863, 0.1614, 0.1336, 0.1911],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9949980392156863 -1.0 -0.9949980392156863
maxi score, test score, baseline:  -0.9949980392156863 -1.0 -0.9949980392156863
probs:  [0.05515296502455444, 0.07325945902433541, 0.32922513445587714, 0.3831689776467155, 0.07959673192425874, 0.07959673192425874]
siam score:  -0.85307086
printing an ep nov before normalisation:  11.228283755700659
maxi score, test score, baseline:  -0.9949980392156863 -1.0 -0.9949980392156863
probs:  [0.05477916967595919, 0.07312555160393773, 0.33248244543312877, 0.3805192627295136, 0.07954678527873028, 0.07954678527873028]
maxi score, test score, baseline:  -0.9949980392156863 -1.0 -0.9949980392156863
probs:  [0.05477916967595919, 0.07312555160393773, 0.33248244543312877, 0.3805192627295136, 0.07954678527873028, 0.07954678527873028]
printing an ep nov before normalisation:  15.90190192927659
maxi score, test score, baseline:  -0.9949980392156863 -1.0 -0.9949980392156863
probs:  [0.05477916967595919, 0.07312555160393773, 0.33248244543312877, 0.3805192627295136, 0.07954678527873028, 0.07954678527873028]
Printing some Q and Qe and total Qs values:  [[0.383]
 [0.35 ]
 [0.38 ]
 [0.464]
 [0.471]
 [0.38 ]
 [0.235]] [[39.603]
 [41.479]
 [37.116]
 [37.249]
 [37.125]
 [37.116]
 [39.583]] [[1.155]
 [1.159]
 [1.102]
 [1.189]
 [1.194]
 [1.102]
 [1.006]]
Printing some Q and Qe and total Qs values:  [[0.263]
 [0.238]
 [0.259]
 [0.264]
 [0.238]
 [0.238]
 [0.238]] [[31.549]
 [30.212]
 [34.445]
 [35.262]
 [30.212]
 [30.212]
 [30.212]] [[0.263]
 [0.238]
 [0.259]
 [0.264]
 [0.238]
 [0.238]
 [0.238]]
printing an ep nov before normalisation:  29.773813861766538
printing an ep nov before normalisation:  61.41649613698637
maxi score, test score, baseline:  -0.9949980392156863 -1.0 -0.9949980392156863
probs:  [0.055070288705020974, 0.07351440855475334, 0.32892988274016266, 0.38254571899574386, 0.0799698505021597, 0.0799698505021597]
maxi score, test score, baseline:  -0.9950219512195122 -1.0 -0.9950219512195122
probs:  [0.05542759295487905, 0.07399190262118376, 0.331066334774587, 0.385032856023776, 0.08048941100439033, 0.07399190262118376]
maxi score, test score, baseline:  -0.9950219512195122 -1.0 -0.9950219512195122
probs:  [0.05542759295487905, 0.07399190262118376, 0.331066334774587, 0.385032856023776, 0.08048941100439033, 0.07399190262118376]
printing an ep nov before normalisation:  17.90288530153159
line 256 mcts: sample exp_bonus 79.6561229649854
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.575]
 [0.566]
 [0.569]
 [0.567]
 [0.569]
 [0.555]] [[59.077]
 [59.759]
 [53.223]
 [51.475]
 [51.044]
 [50.352]
 [49.706]] [[1.427]
 [1.467]
 [1.265]
 [1.217]
 [1.201]
 [1.183]
 [1.15 ]]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.055409496399703086, 0.07434399184724283, 0.3365391809755884, 0.3848577111462327, 0.08097106525388172, 0.06787855437735124]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.055409496399703086, 0.07434399184724283, 0.3365391809755884, 0.3848577111462327, 0.08097106525388172, 0.06787855437735124]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.05465490143647152, 0.07410046011290958, 0.34337259091782735, 0.3795051286353442, 0.08090640564966294, 0.06746051324778443]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.05465490143647152, 0.07410046011290958, 0.34337259091782735, 0.3795051286353442, 0.08090640564966294, 0.06746051324778443]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.05462951240406953, 0.07446517593350885, 0.349139278816105, 0.37927629702891824, 0.08140765816881258, 0.06108207764858595]
actor:  1 policy actor:  1  step number:  42 total reward:  0.14999999999999947  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
probs:  [0.05690788703846265, 0.06640841462262204, 0.19796412205499045, 0.21239932366604167, 0.06973359927707783, 0.39658665334080534]
from probs:  [0.05690788703846265, 0.06640841462262204, 0.19796412205499045, 0.21239932366604167, 0.06973359927707783, 0.39658665334080534]
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
probs:  [0.05690804385195027, 0.0664086555744947, 0.19796282486873898, 0.21239886474875563, 0.06973386967738526, 0.3965877412786752]
printing an ep nov before normalisation:  48.16300443652228
maxi score, test score, baseline:  -0.9951153110047847 -1.0 -0.9951153110047847
probs:  [0.056713618626702204, 0.06638064125588229, 0.20023636423434002, 0.21170119456118744, 0.0697640991760953, 0.39520408214579283]
printing an ep nov before normalisation:  38.52909782741108
printing an ep nov before normalisation:  3.9104743682611343
maxi score, test score, baseline:  -0.9951153110047847 -1.0 -0.9951153110047847
probs:  [0.057024874354881155, 0.06674507647024133, 0.19584043011950816, 0.21286505609868894, 0.07014714721061736, 0.3973774157460631]
maxi score, test score, baseline:  -0.9951380952380953 -1.0 -0.9951380952380953
probs:  [0.05627385382753204, 0.06627133229254278, 0.19904647908684278, 0.21655745721813982, 0.06977044975529652, 0.39208042781964597]
maxi score, test score, baseline:  -0.9951380952380953 -1.0 -0.9951380952380953
probs:  [0.05627385382753204, 0.06627133229254278, 0.19904647908684278, 0.21655745721813982, 0.06977044975529652, 0.39208042781964597]
printing an ep nov before normalisation:  59.253061762779886
printing an ep nov before normalisation:  17.208207196847525
maxi score, test score, baseline:  -0.9951380952380953 -1.0 -0.9951380952380953
from probs:  [0.056458980540019, 0.06648942622356281, 0.1997024061470315, 0.2139762102538073, 0.07000008221280314, 0.3933728946227762]
maxi score, test score, baseline:  -0.995160663507109 -1.0 -0.995160663507109
probs:  [0.05592084358902176, 0.06888965607579735, 0.19960257391128472, 0.21360949222855893, 0.0723344968925971, 0.3896429373027403]
from probs:  [0.05592084358902176, 0.06888965607579735, 0.19960257391128472, 0.21360949222855893, 0.0723344968925971, 0.3896429373027403]
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
printing an ep nov before normalisation:  60.328595207206064
line 256 mcts: sample exp_bonus 44.40131897974748
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.27067508877996
printing an ep nov before normalisation:  47.57298220984296
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
using explorer policy with actor:  1
siam score:  -0.8235811
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.0551333258318989, 0.06520334386030799, 0.20286089033714544, 0.22040077080804868, 0.07233627329709771, 0.3840653958655013]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.05550038680396249, 0.06563761035868948, 0.20421385905001224, 0.21520230855135036, 0.0728181437099544, 0.38662769152603105]
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.05494797187286903, 0.06539970965171062, 0.20827538028066467, 0.219604753867427, 0.0690556673109762, 0.38271651701635256]
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.05494797187286903, 0.06539970965171062, 0.20827538028066467, 0.219604753867427, 0.0690556673109762, 0.38271651701635256]
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.05494797187286903, 0.06539970965171062, 0.20827538028066467, 0.219604753867427, 0.0690556673109762, 0.38271651701635256]
Printing some Q and Qe and total Qs values:  [[0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]] [[92.308]
 [92.308]
 [92.308]
 [92.308]
 [92.308]
 [92.308]
 [92.308]] [[1.606]
 [1.606]
 [1.606]
 [1.606]
 [1.606]
 [1.606]
 [1.606]]
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
probs:  [0.054948124532411324, 0.06539995552317462, 0.20827408155458266, 0.2196043180520064, 0.06905594578743339, 0.3827175745503916]
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
Printing some Q and Qe and total Qs values:  [[0.399]
 [0.456]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]] [[29.616]
 [30.974]
 [29.616]
 [29.616]
 [29.616]
 [29.616]
 [29.616]] [[1.368]
 [1.472]
 [1.368]
 [1.368]
 [1.368]
 [1.368]
 [1.368]]
printing an ep nov before normalisation:  63.432939082719564
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
probs:  [0.055133112892045484, 0.06562021653948501, 0.20897651167598855, 0.21697290039151376, 0.06928854497583221, 0.3840087135251349]
printing an ep nov before normalisation:  66.62586318097432
actions average: 
K:  4  action  0 :  tensor([0.2039, 0.0093, 0.1261, 0.1631, 0.1989, 0.1409, 0.1577],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0477, 0.6726, 0.0654, 0.0525, 0.0356, 0.0441, 0.0820],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1557, 0.0296, 0.1416, 0.1749, 0.1719, 0.1521, 0.1743],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1277, 0.0512, 0.1229, 0.2878, 0.1310, 0.1290, 0.1504],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1617, 0.0099, 0.1457, 0.1754, 0.1726, 0.1520, 0.1826],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1408, 0.0585, 0.1232, 0.1589, 0.1612, 0.2023, 0.1551],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1745, 0.0134, 0.1262, 0.1593, 0.1605, 0.1264, 0.2398],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.055122742468021776, 0.06582235493570775, 0.20908464432514295, 0.2202399524559352, 0.06582235493570775, 0.3839079508794846]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.05490850277576419, 0.06578410380995577, 0.2083977514394965, 0.2227415813223695, 0.06578410380995577, 0.38238395684245835]
printing an ep nov before normalisation:  80.15207279104024
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.05452906582577955, 0.06554967294097212, 0.2100648105272524, 0.22459988892607613, 0.06554967294097212, 0.3797068888389476]
actions average: 
K:  2  action  0 :  tensor([0.2139, 0.0252, 0.1608, 0.1533, 0.1493, 0.1254, 0.1721],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0167, 0.9154, 0.0178, 0.0168, 0.0079, 0.0115, 0.0138],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1646, 0.0069, 0.1681, 0.1748, 0.1719, 0.1450, 0.1687],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1361, 0.0918, 0.1367, 0.2246, 0.1332, 0.1225, 0.1551],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1547, 0.0035, 0.1335, 0.1711, 0.2632, 0.1238, 0.1501],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1375, 0.0014, 0.1536, 0.1409, 0.1390, 0.2918, 0.1356],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1489, 0.1516, 0.1442, 0.1411, 0.1277, 0.1228, 0.1637],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  45.51580026724211
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.526]
 [0.519]
 [0.519]
 [0.519]
 [0.519]
 [0.519]] [[42.781]
 [42.372]
 [52.307]
 [52.307]
 [52.307]
 [52.307]
 [52.307]] [[1.278]
 [1.248]
 [1.666]
 [1.666]
 [1.666]
 [1.666]
 [1.666]]
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.054909172465930835, 0.06600685799573618, 0.21152983973400782, 0.2191880109045654, 0.06600685799573618, 0.3823592609040236]
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.054909172465930835, 0.06600685799573618, 0.21152983973400782, 0.2191880109045654, 0.06600685799573618, 0.3823592609040236]
printing an ep nov before normalisation:  30.78920612502145
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
probs:  [0.054909328784138955, 0.06600711408722108, 0.21152853246750078, 0.21918756735905703, 0.06600711408722108, 0.382360343214861]
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
probs:  [0.05526462102741927, 0.06643438949267418, 0.20985402657815588, 0.21717300431628006, 0.06643438949267418, 0.3848395690927965]
from probs:  [0.05526462102741927, 0.06643438949267418, 0.20985402657815588, 0.21717300431628006, 0.06643438949267418, 0.3848395690927965]
printing an ep nov before normalisation:  62.59139889794268
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
printing an ep nov before normalisation:  53.98882865905762
printing an ep nov before normalisation:  28.675349789643953
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
probs:  [0.05544947698332925, 0.06688514492070023, 0.21371892670494955, 0.2109620437315556, 0.06688514492070023, 0.3860992627387651]
line 256 mcts: sample exp_bonus 59.53074021739493
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
siam score:  -0.85897046
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
probs:  [0.05544947698332925, 0.06688514492070023, 0.21371892670494955, 0.2109620437315556, 0.06688514492070023, 0.3860992627387651]
from probs:  [0.0550742619892355, 0.06666380056717544, 0.2154703270382705, 0.21267721358772385, 0.06666380056717544, 0.3834505962504194]
maxi score, test score, baseline:  -0.9953545454545455 -1.0 -0.9953545454545455
probs:  [0.05526020067757152, 0.06688910431996829, 0.2161952252169218, 0.21001851380501615, 0.06688910431996829, 0.3847478516605539]
Printing some Q and Qe and total Qs values:  [[0.291]
 [0.399]
 [0.295]
 [0.315]
 [0.308]
 [0.308]
 [0.285]] [[80.711]
 [84.31 ]
 [81.96 ]
 [77.894]
 [68.114]
 [68.114]
 [77.617]] [[1.219]
 [1.379]
 [1.241]
 [1.204]
 [1.056]
 [1.056]
 [1.17 ]]
maxi score, test score, baseline:  -0.995375113122172 -1.0 -0.995375113122172
probs:  [0.055260358953395275, 0.06688936612591016, 0.21619393494543096, 0.2100180266570398, 0.06688936612591016, 0.3847489471923136]
siam score:  -0.8605502
Starting evaluation
maxi score, test score, baseline:  -0.995375113122172 -1.0 -0.995375113122172
probs:  [0.055260358953395275, 0.06688936612591016, 0.21619393494543096, 0.2100180266570398, 0.06688936612591016, 0.3847489471923136]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.05526215197329047, 0.0668923319949221, 0.21617931822444147, 0.21001250794471063, 0.0668923319949221, 0.38476135786771326]
line 256 mcts: sample exp_bonus 34.100764389158826
printing an ep nov before normalisation:  47.06732346223799
Printing some Q and Qe and total Qs values:  [[0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.331]
 [0.832]
 [0.305]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.331]
 [0.832]
 [0.305]]
printing an ep nov before normalisation:  36.15404563174741
printing an ep nov before normalisation:  42.235217326760655
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
probs:  [0.05521661665846151, 0.0706679015780486, 0.21799211750576822, 0.20501014089991676, 0.06666546030369774, 0.3844477630541071]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
probs:  [0.07120845873498567, 0.07794612569505419, 0.14218812933682762, 0.1351241847460045, 0.07620082642226535, 0.4973322750648625]
printing an ep nov before normalisation:  50.936452872178904
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
probs:  [0.07120845873498567, 0.07794612569505419, 0.14218812933682762, 0.1351241847460045, 0.07620082642226535, 0.4973322750648625]
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
probs:  [0.07120845873498567, 0.07794612569505419, 0.14218812933682762, 0.1351241847460045, 0.07620082642226535, 0.4973322750648625]
printing an ep nov before normalisation:  33.33681953922458
using explorer policy with actor:  1
printing an ep nov before normalisation:  71.73174071781068
from probs:  [0.07130517361178228, 0.07805200878153168, 0.14102177726130982, 0.13530787227690366, 0.07630433461105444, 0.49800883345741814]
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
probs:  [0.07089378421538582, 0.07915163283849753, 0.1415265098350623, 0.1358665855701851, 0.07742046939220665, 0.4951410181486626]
printing an ep nov before normalisation:  50.91649610351343
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.07098807680420104, 0.07925700936978917, 0.14038337561995406, 0.13604737701798286, 0.07752352230182251, 0.49580063888625037]
from probs:  [0.07098807680420104, 0.07925700936978917, 0.14038337561995406, 0.13604737701798286, 0.07752352230182251, 0.49580063888625037]
printing an ep nov before normalisation:  59.35844409931531
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.0709462139254179, 0.07932056699029592, 0.1412262323820071, 0.1354429807010023, 0.07756497972127331, 0.4954990262800034]
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.0709462139254179, 0.07932056699029592, 0.1412262323820071, 0.1354429807010023, 0.07756497972127331, 0.4954990262800034]
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
probs:  [0.07090477130837243, 0.07938525754188289, 0.14207445367715144, 0.13482779567573197, 0.07760742066883372, 0.4952003011280276]
printing an ep nov before normalisation:  76.75257014678364
printing an ep nov before normalisation:  88.79476862909728
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.82751226425171
printing an ep nov before normalisation:  20.90051834934614
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  29.011170623891395
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.612]
 [0.593]
 [0.593]
 [0.553]
 [0.56 ]
 [0.535]] [[52.845]
 [67.674]
 [52.845]
 [52.845]
 [45.485]
 [43.029]
 [40.91 ]] [[1.618]
 [2.091]
 [1.618]
 [1.618]
 [1.352]
 [1.283]
 [1.193]]
siam score:  -0.8630241
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.633]
 [0.259]
 [0.445]
 [0.471]
 [0.205]
 [0.486]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.473]
 [0.633]
 [0.259]
 [0.445]
 [0.471]
 [0.205]
 [0.486]]
printing an ep nov before normalisation:  62.500072762373975
siam score:  -0.8567952
printing an ep nov before normalisation:  68.56114026326719
maxi score, test score, baseline:  -0.9958183673469387 -1.0 -0.9958183673469387
probs:  [0.07086892978641464, 0.07623230470050708, 0.1448227870161464, 0.13694815608328234, 0.07623230470050708, 0.49489551771314255]
maxi score, test score, baseline:  -0.9958183673469387 -1.0 -0.9958183673469387
maxi score, test score, baseline:  -0.9958183673469387 -1.0 -0.9958183673469387
probs:  [0.07073455448605294, 0.0761555963114049, 0.14548356226753156, 0.13752426344355784, 0.0761555963114049, 0.49394642718004794]
maxi score, test score, baseline:  -0.9958183673469387 -1.0 -0.9958183673469387
probs:  [0.07083600823835658, 0.07626484007350336, 0.14425658368194644, 0.13772169367118392, 0.07626484007350336, 0.49465603426150645]
maxi score, test score, baseline:  -0.9958183673469387 -1.0 -0.9958183673469387
probs:  [0.07083600823835658, 0.07626484007350336, 0.14425658368194644, 0.13772169367118392, 0.07626484007350336, 0.49465603426150645]
printing an ep nov before normalisation:  19.992817108898056
printing an ep nov before normalisation:  24.768259541317164
line 256 mcts: sample exp_bonus 22.497801468626633
printing an ep nov before normalisation:  60.58716996626398
Printing some Q and Qe and total Qs values:  [[0.58 ]
 [0.557]
 [0.593]
 [0.589]
 [0.573]
 [0.69 ]
 [0.597]] [[59.264]
 [54.883]
 [56.363]
 [54.986]
 [54.899]
 [42.186]
 [52.442]] [[0.58 ]
 [0.557]
 [0.593]
 [0.589]
 [0.573]
 [0.69 ]
 [0.597]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.07083606301500849, 0.07626500399334009, 0.14425593661594863, 0.13772158879401988, 0.07626500399334009, 0.49465640358834295]
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.07083606301500849, 0.07626500399334009, 0.14425593661594863, 0.13772158879401988, 0.07626500399334009, 0.49465640358834295]
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.07083609007162213, 0.07626508495995873, 0.1442556170038833, 0.13772153698927617, 0.07626508495995873, 0.494656586015301]
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
printing an ep nov before normalisation:  79.38430670060359
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.07107119338448152, 0.07465983647450933, 0.14473485716212472, 0.13671488190734687, 0.07651824093184513, 0.4963009901396923]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
from probs:  [0.07093940106828188, 0.07456651002999004, 0.14539264936778792, 0.13728670977181182, 0.07644483431373177, 0.4953698954483966]
printing an ep nov before normalisation:  61.80491772946902
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.07077848791802131, 0.07448831521109947, 0.14545912703260297, 0.13863888562019586, 0.07640947577358637, 0.49422570844449404]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.07077848791802131, 0.07448831521109947, 0.14545912703260297, 0.13863888562019586, 0.07640947577358637, 0.49422570844449404]
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.446]
 [0.343]
 [0.348]
 [0.337]
 [0.344]
 [0.368]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.333]
 [0.446]
 [0.343]
 [0.348]
 [0.337]
 [0.344]
 [0.368]]
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.07077848791802131, 0.07448831521109947, 0.14545912703260297, 0.13863888562019586, 0.07640947577358637, 0.49422570844449404]
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([0.2608, 0.0505, 0.1327, 0.1463, 0.1537, 0.1087, 0.1474],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0161, 0.8004, 0.0369, 0.0459, 0.0173, 0.0307, 0.0526],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1129, 0.1118, 0.3150, 0.1279, 0.0989, 0.1013, 0.1321],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1344, 0.0107, 0.1348, 0.2984, 0.1598, 0.1248, 0.1372],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1586, 0.0137, 0.1129, 0.1533, 0.3244, 0.1005, 0.1365],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1268, 0.0281, 0.1578, 0.1447, 0.1485, 0.2655, 0.1285],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1543, 0.0262, 0.1602, 0.1989, 0.1698, 0.1339, 0.1566],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.07064646591923776, 0.07439524720606655, 0.14611015664757743, 0.13921864521427577, 0.07633658037246005, 0.49329290464038256]
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.07078376214354706, 0.07453987843518464, 0.14639400336200936, 0.13948934964178847, 0.07453987843518464, 0.4942531279822858]
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.07078376214354706, 0.07453987843518464, 0.14639400336200936, 0.13948934964178847, 0.07453987843518464, 0.4942531279822858]
printing an ep nov before normalisation:  48.58816300710894
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
actions average: 
K:  0  action  0 :  tensor([0.1888, 0.0036, 0.1499, 0.1769, 0.1701, 0.1370, 0.1737],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0169, 0.9283, 0.0101, 0.0176, 0.0056, 0.0055, 0.0161],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1260, 0.0420, 0.2505, 0.1598, 0.1289, 0.1418, 0.1509],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1451, 0.0034, 0.1302, 0.2578, 0.1688, 0.1310, 0.1636],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1436, 0.0111, 0.1418, 0.1595, 0.2733, 0.1321, 0.1386],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1410, 0.0034, 0.1621, 0.1748, 0.1408, 0.2155, 0.1623],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1581, 0.0074, 0.1512, 0.1914, 0.1501, 0.1386, 0.2032],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.07052083411859905, 0.07435543700149146, 0.14771100248426008, 0.1406620713529532, 0.07435543700149146, 0.4923952180412048]
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.07065773257412436, 0.07449979346321786, 0.14799802985063112, 0.14093538909314038, 0.0725564254553624, 0.4933526295635239]
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.07065773257412436, 0.07449979346321786, 0.14799802985063112, 0.14093538909314038, 0.0725564254553624, 0.4933526295635239]
printing an ep nov before normalisation:  55.56134223937988
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.07039552940961215, 0.0743168574955065, 0.14933146734404137, 0.1421231142329419, 0.07233339503345526, 0.4914996364844427]
printing an ep nov before normalisation:  0.09659119112285453
printing an ep nov before normalisation:  43.58331996828337
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.06978154961136313, 0.07558909178295561, 0.15050554485822168, 0.14330662389218565, 0.07360822468566826, 0.4872089651696058]
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.42 ]
 [0.355]
 [0.354]
 [0.359]
 [0.363]
 [0.368]] [[63.715]
 [66.814]
 [62.651]
 [62.269]
 [62.751]
 [61.658]
 [60.664]] [[0.373]
 [0.42 ]
 [0.355]
 [0.354]
 [0.359]
 [0.363]
 [0.368]]
printing an ep nov before normalisation:  51.52089632496622
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
printing an ep nov before normalisation:  42.93983759294525
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.06978154961136313, 0.07558909178295561, 0.15050554485822168, 0.14330662389218565, 0.07360822468566826, 0.4872089651696058]
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.06964615686056336, 0.0755115785394295, 0.1511746688775113, 0.14390400155370378, 0.07351096959469994, 0.4862526245740921]
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.06964615686056336, 0.0755115785394295, 0.1511746688775113, 0.14390400155370378, 0.07351096959469994, 0.4862526245740921]
siam score:  -0.85450655
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.06964620815441575, 0.07551174625316869, 0.15117403991149853, 0.14390393972669044, 0.07351109759933046, 0.486252968354896]
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.06975408404207084, 0.0756287247590141, 0.1498577966387657, 0.1441270547904964, 0.07362497133618075, 0.4870073684334721]
from probs:  [0.06975408404207084, 0.0756287247590141, 0.1498577966387657, 0.1441270547904964, 0.07362497133618075, 0.4870073684334721]
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.06928814126745321, 0.07694705503121513, 0.1503525373888294, 0.14468537950868451, 0.0749655337376837, 0.4837613530661341]
printing an ep nov before normalisation:  47.962773305123996
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
probs:  [0.06901187328104874, 0.0768212551569662, 0.15166660185548433, 0.14588879903020252, 0.07480080461348756, 0.48181066606281064]
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
probs:  [0.06901055959256032, 0.07691115777974901, 0.15263072430131205, 0.14678543471851876, 0.07287004721273872, 0.4817920763951211]
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
probs:  [0.06912288897941342, 0.07703637194761116, 0.15287942659116388, 0.1453950463479789, 0.07298867088916519, 0.4825775952446674]
Printing some Q and Qe and total Qs values:  [[0.39]
 [0.39]
 [0.39]
 [0.39]
 [0.39]
 [0.39]
 [0.39]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.39]
 [0.39]
 [0.39]
 [0.39]
 [0.39]
 [0.39]
 [0.39]]
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
probs:  [0.06907129685905826, 0.07716359764112929, 0.15312038201779304, 0.14542249148990774, 0.07302443229857573, 0.48219779969353593]
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
probs:  [0.06907129685905826, 0.07716359764112929, 0.15312038201779304, 0.14542249148990774, 0.07302443229857573, 0.48219779969353593]
printing an ep nov before normalisation:  65.0835366858871
using another actor
Printing some Q and Qe and total Qs values:  [[0.37 ]
 [0.37 ]
 [0.37 ]
 [0.368]
 [0.364]
 [0.359]
 [0.369]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.37 ]
 [0.37 ]
 [0.37 ]
 [0.368]
 [0.364]
 [0.359]
 [0.369]]
printing an ep nov before normalisation:  60.993902861031515
printing an ep nov before normalisation:  78.4620486174565
siam score:  -0.8526747
maxi score, test score, baseline:  -0.995978431372549 -1.0 -0.995978431372549
probs:  [0.06891230049717621, 0.07717281993015285, 0.15470737634343187, 0.14519314155436744, 0.07294761171443491, 0.4810667499604366]
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
probs:  [0.06891232396820068, 0.07717292384417772, 0.15470706541496204, 0.14519310596632967, 0.07294767448232738, 0.4810669063240025]
printing an ep nov before normalisation:  78.72575769318375
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
probs:  [0.06892468696616766, 0.07511915399028321, 0.15571500719713463, 0.14609064592175713, 0.07300686446864993, 0.48114364145600746]
Printing some Q and Qe and total Qs values:  [[0.299]
 [0.273]
 [0.273]
 [0.273]
 [0.293]
 [0.299]
 [0.299]] [[85.723]
 [57.238]
 [57.238]
 [57.238]
 [86.387]
 [91.37 ]
 [84.294]] [[1.053]
 [0.515]
 [0.515]
 [0.515]
 [1.059]
 [1.154]
 [1.027]]
printing an ep nov before normalisation:  32.68256127330428
siam score:  -0.85730827
printing an ep nov before normalisation:  83.10689938590161
printing an ep nov before normalisation:  49.01666072098409
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
probs:  [0.06903722017439261, 0.07524182048024541, 0.1543349647262879, 0.14632941184694992, 0.07312607554836456, 0.48193050722375963]
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
printing an ep nov before normalisation:  44.477736332900754
printing an ep nov before normalisation:  43.34627337224401
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
probs:  [0.0691516659795107, 0.0753665717907801, 0.15459108547018852, 0.14491261759884536, 0.07324731272103689, 0.4827307464396384]
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
probs:  [0.0691516659795107, 0.0753665717907801, 0.15459108547018852, 0.14491261759884536, 0.07324731272103689, 0.4827307464396384]
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
probs:  [0.06915171777962378, 0.07536674408816002, 0.1545904612344455, 0.14491253994205763, 0.07324744392931046, 0.48273109302640255]
printing an ep nov before normalisation:  56.693115234375
printing an ep nov before normalisation:  54.498646745248585
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
probs:  [0.06916238115736796, 0.07544904598156854, 0.1555859482408403, 0.14579647280826302, 0.07121030985009996, 0.48279584196186026]
Printing some Q and Qe and total Qs values:  [[0.575]
 [0.647]
 [0.614]
 [0.69 ]
 [0.668]
 [0.609]
 [0.621]] [[57.319]
 [71.176]
 [65.383]
 [66.044]
 [65.784]
 [62.557]
 [64.944]] [[1.13 ]
 [1.479]
 [1.33 ]
 [1.42 ]
 [1.392]
 [1.269]
 [1.329]]
printing an ep nov before normalisation:  42.78679318689931
using explorer policy with actor:  1
printing an ep nov before normalisation:  69.6327112512751
printing an ep nov before normalisation:  5.677015360402038
Printing some Q and Qe and total Qs values:  [[0.287]
 [0.287]
 [0.287]
 [0.328]
 [0.324]
 [0.287]
 [0.287]] [[59.876]
 [59.876]
 [59.876]
 [91.314]
 [91.276]
 [59.876]
 [59.876]] [[0.624]
 [0.624]
 [0.624]
 [0.922]
 [0.918]
 [0.624]
 [0.624]]
printing an ep nov before normalisation:  45.10161724490132
printing an ep nov before normalisation:  56.99774553117803
printing an ep nov before normalisation:  0.3281851753331466
printing an ep nov before normalisation:  60.14116828654782
siam score:  -0.8560701
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
probs:  [0.06955125600911365, 0.07376469129393519, 0.15413840986102045, 0.14748940656898604, 0.06955125600911365, 0.4855049802578311]
printing an ep nov before normalisation:  68.75455531250108
printing an ep nov before normalisation:  28.718203768719142
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.546]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]] [[64.896]
 [64.715]
 [64.896]
 [64.896]
 [64.896]
 [64.896]
 [64.896]] [[0.508]
 [0.546]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]]
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.512]
 [0.432]
 [0.428]
 [0.419]
 [0.435]
 [0.428]] [[58.276]
 [61.759]
 [59.08 ]
 [59.074]
 [57.644]
 [56.518]
 [56.726]] [[0.439]
 [0.512]
 [0.432]
 [0.428]
 [0.419]
 [0.435]
 [0.428]]
printing an ep nov before normalisation:  39.45641479753746
Printing some Q and Qe and total Qs values:  [[0.741]
 [0.923]
 [0.741]
 [0.741]
 [0.83 ]
 [0.741]
 [0.741]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.741]
 [0.923]
 [0.741]
 [0.741]
 [0.83 ]
 [0.741]
 [0.741]]
actor:  1 policy actor:  1  step number:  33 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.569]
 [0.564]
 [0.549]
 [0.543]
 [0.557]
 [0.575]] [[40.905]
 [47.455]
 [45.446]
 [43.983]
 [41.952]
 [41.34 ]
 [43.189]] [[1.661]
 [2.092]
 [1.961]
 [1.855]
 [1.723]
 [1.699]
 [1.832]]
Printing some Q and Qe and total Qs values:  [[0.35 ]
 [0.517]
 [0.35 ]
 [0.445]
 [0.35 ]
 [0.35 ]
 [0.35 ]] [[36.425]
 [40.03 ]
 [36.425]
 [33.571]
 [36.425]
 [36.425]
 [36.425]] [[1.427]
 [1.798]
 [1.427]
 [1.361]
 [1.427]
 [1.427]
 [1.427]]
printing an ep nov before normalisation:  27.598695755004883
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.058136949579341415, 0.06169728471372651, 0.29234750554051714, 0.12399367311874, 0.058136949579341415, 0.40568763746833364]
printing an ep nov before normalisation:  48.15455671437064
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.557]
 [0.541]
 [0.561]
 [0.604]
 [0.512]
 [0.512]] [[48.775]
 [47.083]
 [38.573]
 [39.014]
 [37.869]
 [48.775]
 [48.775]] [[1.179]
 [1.18 ]
 [0.942]
 [0.974]
 [0.987]
 [1.179]
 [1.179]]
siam score:  -0.8537791
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.05824165149262386, 0.060004766729293606, 0.29287497847822236, 0.12421725231473765, 0.05824165149262386, 0.4064196994924987]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  36 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]] [[54.759]
 [54.759]
 [54.759]
 [54.759]
 [54.759]
 [54.759]
 [54.759]] [[2.232]
 [2.232]
 [2.232]
 [2.232]
 [2.232]
 [2.232]
 [2.232]]
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.0460610958385951, 0.2569050273713974, 0.23151094840861655, 0.09820698348793772, 0.0460610958385951, 0.3212548490548581]
printing an ep nov before normalisation:  78.12853498052598
line 256 mcts: sample exp_bonus 35.58024069665788
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.04583127241866073, 0.2579741223074529, 0.2324236013970266, 0.09829840834944406, 0.04583127241866073, 0.3196413231087549]
printing an ep nov before normalisation:  81.18196777828817
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.04591683481467904, 0.25845688879108175, 0.23098768266497444, 0.09848220719730531, 0.04591683481467904, 0.32023955171728036]
printing an ep nov before normalisation:  56.75715644047816
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.045859802833456496, 0.25673469831886236, 0.23276530719429459, 0.09894626972551569, 0.045859802833456496, 0.3198341190944144]
printing an ep nov before normalisation:  52.85726989733828
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.045859802833456496, 0.25673469831886236, 0.23276530719429459, 0.09894626972551569, 0.045859802833456496, 0.3198341190944144]
using explorer policy with actor:  1
printing an ep nov before normalisation:  66.60223768134941
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.04585809148016949, 0.25705038999414015, 0.23134444285034467, 0.10005718079549517, 0.04585809148016949, 0.31983180339968104]
printing an ep nov before normalisation:  16.715645790100098
from probs:  [0.04585809148016949, 0.25705038999414015, 0.23134444285034467, 0.10005718079549517, 0.04585809148016949, 0.31983180339968104]
using explorer policy with actor:  1
printing an ep nov before normalisation:  75.36036955220632
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.04599474879155402, 0.2578182349945768, 0.23019473069098573, 0.09921025433356276, 0.04599474879155402, 0.3207872823977667]
printing an ep nov before normalisation:  29.85352980520529
printing an ep nov before normalisation:  39.0074019902179
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.046078387430827206, 0.2582881806590904, 0.2287920351883476, 0.09939094322921257, 0.046078387430827206, 0.3213720660616948]
actions average: 
K:  3  action  0 :  tensor([0.1716, 0.0309, 0.1514, 0.1970, 0.1496, 0.1471, 0.1524],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0177, 0.8901, 0.0212, 0.0159, 0.0106, 0.0167, 0.0279],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1751, 0.0080, 0.1937, 0.1815, 0.1418, 0.1524, 0.1475],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1752, 0.0417, 0.1502, 0.1886, 0.1336, 0.1410, 0.1698],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2044, 0.0209, 0.1402, 0.1553, 0.2058, 0.1315, 0.1418],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1408, 0.0823, 0.1230, 0.1659, 0.1340, 0.2220, 0.1319],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1701, 0.0145, 0.1444, 0.1930, 0.1239, 0.1309, 0.2233],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  83.46444796685607
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
using another actor
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
probs:  [0.046332173541512214, 0.25600590175161547, 0.22824411337744008, 0.09993915831166743, 0.046332173541512214, 0.3231464794762526]
printing an ep nov before normalisation:  74.94674259992763
siam score:  -0.8528633
maxi score, test score, baseline:  -0.9960977186311787 -1.0 -0.9960977186311787
probs:  [0.046584014116224565, 0.2537537089181639, 0.22768785959349397, 0.10048311846358264, 0.046584014116224565, 0.32490728479231057]
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.385]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]] [[54.358]
 [56.645]
 [53.82 ]
 [53.82 ]
 [53.82 ]
 [53.82 ]
 [53.82 ]] [[1.039]
 [1.052]
 [1.061]
 [1.061]
 [1.061]
 [1.061]
 [1.061]]
maxi score, test score, baseline:  -0.9960977186311787 -1.0 -0.9960977186311787
probs:  [0.046584014116224565, 0.2537537089181639, 0.22768785959349397, 0.10048311846358264, 0.046584014116224565, 0.32490728479231057]
Printing some Q and Qe and total Qs values:  [[0.31 ]
 [0.437]
 [0.31 ]
 [0.31 ]
 [0.31 ]
 [0.31 ]
 [0.31 ]] [[56.608]
 [75.719]
 [56.608]
 [56.608]
 [56.608]
 [56.608]
 [56.608]] [[1.063]
 [1.776]
 [1.063]
 [1.063]
 [1.063]
 [1.063]
 [1.063]]
maxi score, test score, baseline:  -0.9960977186311787 -1.0 -0.9960977186311787
probs:  [0.04699870108193059, 0.24889190119533136, 0.22792502164088463, 0.10137898685385281, 0.04699870108193059, 0.32780668814607006]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  31.84908861052159
maxi score, test score, baseline:  -0.9960977186311787 -1.0 -0.9960977186311787
probs:  [0.04829726629615642, 0.2488816224138581, 0.22628858949122596, 0.10232501265378931, 0.04692269276624081, 0.32728481637872936]
printing an ep nov before normalisation:  44.52043056488037
printing an ep nov before normalisation:  27.334137588679795
maxi score, test score, baseline:  -0.9960977186311787 -1.0 -0.9960977186311787
probs:  [0.04829726629615642, 0.2488816224138581, 0.22628858949122596, 0.10232501265378931, 0.04692269276624081, 0.32728481637872936]
maxi score, test score, baseline:  -0.9960977186311787 -1.0 -0.9960977186311787
probs:  [0.04829726629615642, 0.2488816224138581, 0.22628858949122596, 0.10232501265378931, 0.04692269276624081, 0.32728481637872936]
actions average: 
K:  2  action  0 :  tensor([0.1874, 0.0140, 0.1704, 0.1685, 0.1622, 0.1407, 0.1568],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0241, 0.8398, 0.0301, 0.0314, 0.0224, 0.0231, 0.0290],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1551, 0.0046, 0.2526, 0.1493, 0.1304, 0.1558, 0.1523],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1579, 0.0376, 0.1439, 0.1786, 0.1518, 0.1577, 0.1725],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1794, 0.0244, 0.1347, 0.1502, 0.2398, 0.1428, 0.1287],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1376, 0.0302, 0.1445, 0.1596, 0.1295, 0.2674, 0.1312],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1808, 0.0116, 0.1671, 0.1812, 0.1437, 0.1419, 0.1738],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  55.22902488708496
Printing some Q and Qe and total Qs values:  [[0.299]
 [0.235]
 [0.434]
 [0.335]
 [0.339]
 [0.357]
 [0.308]] [[46.263]
 [53.593]
 [39.184]
 [43.098]
 [43.901]
 [40.964]
 [52.599]] [[0.753]
 [0.801]
 [0.781]
 [0.741]
 [0.758]
 [0.731]
 [0.859]]
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
probs:  [0.04829727986717948, 0.24888266420653338, 0.22628790030528972, 0.10232484024489216, 0.0469226704553029, 0.32728464492080234]
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
probs:  [0.04829727986717948, 0.24888266420653338, 0.22628790030528972, 0.10232484024489216, 0.0469226704553029, 0.32728464492080234]
UNIT TEST: sample policy line 217 mcts : [0.041 0.184 0.469 0.102 0.061 0.061 0.082]
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
probs:  [0.04829727986717948, 0.24888266420653338, 0.22628790030528972, 0.10232484024489216, 0.0469226704553029, 0.32728464492080234]
printing an ep nov before normalisation:  54.823353971656516
printing an ep nov before normalisation:  22.93147386775843
printing an ep nov before normalisation:  92.69525700754741
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
probs:  [0.04863179593860426, 0.24715834946100204, 0.22437079362498596, 0.10303459648148686, 0.04724763938709837, 0.3295568251068226]
maxi score, test score, baseline:  -0.9961546816479401 -1.0 -0.9961546816479401
probs:  [0.04841624920937969, 0.24817512142601436, 0.22524558284036242, 0.10315659409404197, 0.04702349109275396, 0.32798296133744764]
maxi score, test score, baseline:  -0.9961546816479401 -1.0 -0.9961546816479401
probs:  [0.04841624920937969, 0.24817512142601436, 0.22524558284036242, 0.10315659409404197, 0.04702349109275396, 0.32798296133744764]
maxi score, test score, baseline:  -0.9961546816479401 -1.0 -0.9961546816479401
probs:  [0.04858115437834642, 0.24560962420918991, 0.2260144284765274, 0.10350845868193197, 0.047183639454858525, 0.3291026947991457]
maxi score, test score, baseline:  -0.9961546816479401 -1.0 -0.9961546816479401
probs:  [0.04858115437834642, 0.24560962420918991, 0.2260144284765274, 0.10350845868193197, 0.047183639454858525, 0.3291026947991457]
siam score:  -0.85100174
maxi score, test score, baseline:  -0.9961546816479401 -1.0 -0.9961546816479401
maxi score, test score, baseline:  -0.9961546816479401 -1.0 -0.9961546816479401
probs:  [0.04858115437834642, 0.24560962420918991, 0.2260144284765274, 0.10350845868193197, 0.047183639454858525, 0.3291026947991457]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9961546816479401 -1.0 -0.9961546816479401
probs:  [0.04815242538223981, 0.2476092581976523, 0.22777255296114277, 0.10375670511342472, 0.04673768617886234, 0.32597137216667804]
printing an ep nov before normalisation:  104.26075900450988
maxi score, test score, baseline:  -0.9961546816479401 -1.0 -0.9961546816479401
probs:  [0.04815242538223981, 0.2476092581976523, 0.22777255296114277, 0.10375670511342472, 0.04673768617886234, 0.32597137216667804]
printing an ep nov before normalisation:  69.2367447222657
maxi score, test score, baseline:  -0.996168656716418 -1.0 -0.996168656716418
probs:  [0.04823677787151748, 0.2480442972079752, 0.22641704902322388, 0.10393867105641154, 0.046819541444625704, 0.3265436633962462]
maxi score, test score, baseline:  -0.996168656716418 -1.0 -0.996168656716418
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
probs:  [0.04816461917409068, 0.24977844034679098, 0.22620415102028363, 0.1031754274015887, 0.04673456079976426, 0.3259428012574817]
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
probs:  [0.04824798783247801, 0.25021175250953853, 0.22486222058964403, 0.10335427897328449, 0.04681544728884376, 0.32650831280621095]
Printing some Q and Qe and total Qs values:  [[0.344]
 [0.379]
 [0.323]
 [0.32 ]
 [0.321]
 [0.317]
 [0.326]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.344]
 [0.379]
 [0.323]
 [0.32 ]
 [0.321]
 [0.317]
 [0.326]]
printing an ep nov before normalisation:  50.11781231389534
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
probs:  [0.04803559362749929, 0.25122081511719246, 0.22571797174308672, 0.10347516215021875, 0.04659438922077457, 0.3249560681412282]
siam score:  -0.8504508
printing an ep nov before normalisation:  36.62041187286377
Printing some Q and Qe and total Qs values:  [[0.12 ]
 [0.002]
 [0.13 ]
 [0.156]
 [0.197]
 [0.171]
 [0.057]] [[41.37 ]
 [37.814]
 [37.606]
 [36.193]
 [35.384]
 [39.28 ]
 [42.241]] [[1.428]
 [1.197]
 [1.319]
 [1.299]
 [1.314]
 [1.413]
 [1.393]]
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
probs:  [0.046441119963891254, 0.25259347895067374, 0.22690045665694683, 0.10374606222160435, 0.046441119963891254, 0.3238777622429926]
printing an ep nov before normalisation:  32.82750786073925
Printing some Q and Qe and total Qs values:  [[0.865]
 [0.965]
 [0.865]
 [0.865]
 [0.865]
 [0.865]
 [0.865]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.865]
 [0.965]
 [0.865]
 [0.865]
 [0.865]
 [0.865]
 [0.865]]
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
probs:  [0.04578350800744988, 0.25560726301016434, 0.22945667011113333, 0.10410900194709485, 0.04578350800744988, 0.3192600489167079]
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.496]
 [0.421]
 [0.474]
 [0.448]
 [0.408]
 [0.386]] [[56.347]
 [49.639]
 [54.003]
 [57.026]
 [56.255]
 [54.063]
 [53.154]] [[1.971]
 [1.852]
 [1.906]
 [2.047]
 [1.999]
 [1.894]
 [1.846]]
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
probs:  [0.04556547186124562, 0.2566065055807815, 0.23030420173668698, 0.10422933731854789, 0.04556547186124562, 0.3177290116414923]
printing an ep nov before normalisation:  52.86215038978577
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
probs:  [0.045646523165641884, 0.2570640549213843, 0.2289322497832348, 0.10441504519020824, 0.045646523165641884, 0.318295603773889]
printing an ep nov before normalisation:  49.340945451621955
printing an ep nov before normalisation:  44.8846639005809
maxi score, test score, baseline:  -0.9961825278810409 -1.0 -0.9961825278810409
probs:  [0.04570252286247123, 0.257380183385313, 0.22921376478949, 0.10331393438052007, 0.04570252286247123, 0.31868707171973454]
printing an ep nov before normalisation:  70.7532686467337
printing an ep nov before normalisation:  55.251736966688284
actor:  1 policy actor:  1  step number:  41 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.0
using another actor
maxi score, test score, baseline:  -0.9961962962962964 -1.0 -0.9961962962962964
maxi score, test score, baseline:  -0.9962099630996311 -1.0 -0.9962099630996311
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
probs:  [0.04359363188421672, 0.23019836335680025, 0.3039916879832536, 0.09438063506150472, 0.04359363188421672, 0.28424204983000806]
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
probs:  [0.043703854149990215, 0.2307818438962681, 0.3047623197334506, 0.09461966097153801, 0.043703854149990215, 0.28242846709876274]
printing an ep nov before normalisation:  56.37078266190042
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.493]
 [0.477]
 [0.315]
 [0.357]
 [0.313]
 [0.316]] [[53.106]
 [62.222]
 [60.398]
 [54.31 ]
 [61.706]
 [53.888]
 [52.854]] [[0.99 ]
 [1.156]
 [1.106]
 [0.832]
 [1.011]
 [0.822]
 [0.806]]
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
probs:  [0.04364780561540301, 0.2319784722033791, 0.30436586451470343, 0.09490454539313788, 0.04364780561540301, 0.28145550665797364]
line 256 mcts: sample exp_bonus 54.61662843794354
printing an ep nov before normalisation:  51.18397655397107
maxi score, test score, baseline:  -0.9962235294117647 -1.0 -0.9962235294117647
probs:  [0.043318544032391625, 0.23321043550388895, 0.3020546591131017, 0.09500019229178258, 0.043318544032391625, 0.28309762502644353]
printing an ep nov before normalisation:  26.163275031784444
printing an ep nov before normalisation:  81.55915782311557
printing an ep nov before normalisation:  61.24099645103925
Printing some Q and Qe and total Qs values:  [[0.656]
 [0.666]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]] [[52.068]
 [57.465]
 [52.068]
 [52.068]
 [52.068]
 [52.068]
 [52.068]] [[2.085]
 [2.374]
 [2.085]
 [2.085]
 [2.085]
 [2.085]
 [2.085]]
Printing some Q and Qe and total Qs values:  [[0.285]
 [0.24 ]
 [0.261]
 [0.283]
 [0.27 ]
 [0.266]
 [0.275]] [[39.028]
 [64.395]
 [38.422]
 [61.161]
 [61.393]
 [35.402]
 [35.349]] [[0.595]
 [0.911]
 [0.563]
 [0.907]
 [0.898]
 [0.525]
 [0.532]]
printing an ep nov before normalisation:  50.413173600858556
printing an ep nov before normalisation:  36.640080042144795
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9962369963369964 -1.0 -0.9962369963369964
probs:  [0.043357183209814194, 0.23492503327880465, 0.30232016890489627, 0.09331153387642867, 0.043357183209814194, 0.28272889752024205]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.9962369963369964 -1.0 -0.9962369963369964
probs:  [0.043357183209814194, 0.23492503327880465, 0.30232016890489627, 0.09331153387642867, 0.043357183209814194, 0.28272889752024205]
Printing some Q and Qe and total Qs values:  [[0.613]
 [0.642]
 [0.612]
 [0.615]
 [0.598]
 [0.611]
 [0.598]] [[65.74 ]
 [70.558]
 [63.372]
 [64.738]
 [62.321]
 [62.551]
 [62.321]] [[0.613]
 [0.642]
 [0.612]
 [0.615]
 [0.598]
 [0.611]
 [0.598]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
probs:  [0.04357204281126288, 0.2360925112802119, 0.3038222885866623, 0.0937746556961247, 0.04357204281126288, 0.2791664588144753]
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
probs:  [0.04357204281126288, 0.2360925112802119, 0.3038222885866623, 0.0937746556961247, 0.04357204281126288, 0.2791664588144753]
printing an ep nov before normalisation:  37.32809543609619
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
probs:  [0.04357204281126288, 0.2360925112802119, 0.3038222885866623, 0.0937746556961247, 0.04357204281126288, 0.2791664588144753]
printing an ep nov before normalisation:  43.15138816833496
actor:  1 policy actor:  1  step number:  32 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  65.81866221213609
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
probs:  [0.05077634972889783, 0.20879953116268263, 0.3543929456884946, 0.0910996636936159, 0.05077634972889783, 0.24415515999741128]
printing an ep nov before normalisation:  36.10345779678489
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
probs:  [0.05068002870769912, 0.20952454162872902, 0.35371571420360404, 0.09033575406513086, 0.05068002870769912, 0.24506393268713794]
printing an ep nov before normalisation:  80.35376167466396
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.759]
 [0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.695]
 [0.759]
 [0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]]
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
probs:  [0.050641630879087116, 0.21049045428003813, 0.3534436612394786, 0.09054808356623871, 0.050641630879087116, 0.24423453915607024]
using explorer policy with actor:  1
siam score:  -0.83583426
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
printing an ep nov before normalisation:  43.633967029703946
printing an ep nov before normalisation:  43.84364915422766
printing an ep nov before normalisation:  67.31362342834473
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
probs:  [0.050742561294668614, 0.2109108089271882, 0.35414967778461315, 0.09072875862874981, 0.050742561294668614, 0.24272563207011172]
printing an ep nov before normalisation:  2.4345344658172507
printing an ep nov before normalisation:  42.84535770849088
printing an ep nov before normalisation:  20.15233568973258
maxi score, test score, baseline:  -0.9962503649635036 -1.0 -0.9962503649635036
probs:  [0.050802151800983665, 0.21229152064453835, 0.35456304070411643, 0.09111816864815521, 0.050802151800983665, 0.24042296640122263]
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.05094046516865231, 0.21014329720371147, 0.35553054066534684, 0.09136646778298055, 0.05094046516865231, 0.2410787640106565]
printing an ep nov before normalisation:  69.67878722699534
printing an ep nov before normalisation:  42.88175615238597
printing an ep nov before normalisation:  41.73175811767578
printing an ep nov before normalisation:  50.39452877145896
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.05094046516865231, 0.21014329720371147, 0.35553054066534684, 0.09136646778298055, 0.05094046516865231, 0.2410787640106565]
printing an ep nov before normalisation:  28.682169914245605
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.05107669253101621, 0.20802727293290701, 0.356483452891525, 0.09161108772637586, 0.05107669253101621, 0.24172480138715968]
actions average: 
K:  2  action  0 :  tensor([0.1808, 0.0054, 0.1662, 0.1631, 0.1879, 0.1396, 0.1570],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0125, 0.8526, 0.0092, 0.0764, 0.0116, 0.0067, 0.0311],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1469, 0.0092, 0.2311, 0.1539, 0.1542, 0.1682, 0.1365],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1746, 0.0408, 0.1465, 0.1760, 0.1786, 0.1362, 0.1472],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1577, 0.0208, 0.1348, 0.1789, 0.2470, 0.1321, 0.1288],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1378, 0.0293, 0.1534, 0.1836, 0.1721, 0.1958, 0.1281],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1599, 0.0171, 0.1509, 0.1959, 0.1898, 0.1416, 0.1449],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.051210878225457626, 0.2059429619596513, 0.357422083623212, 0.09185204149960557, 0.051210878225457626, 0.24236115646661585]
printing an ep nov before normalisation:  51.2291787101584
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.051210878225457626, 0.2059429619596513, 0.357422083623212, 0.09185204149960557, 0.051210878225457626, 0.24236115646661585]
siam score:  -0.83316284
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.051210878225457626, 0.2059429619596513, 0.357422083623212, 0.09185204149960557, 0.051210878225457626, 0.24236115646661585]
printing an ep nov before normalisation:  76.06683771078195
Printing some Q and Qe and total Qs values:  [[0.169]
 [0.234]
 [0.271]
 [0.374]
 [0.094]
 [0.436]
 [0.311]] [[51.035]
 [60.259]
 [46.955]
 [44.535]
 [46.677]
 [45.94 ]
 [51.02 ]] [[0.737]
 [0.976]
 [0.762]
 [0.819]
 [0.58 ]
 [0.907]
 [0.879]]
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.05103429693099443, 0.2074057107713633, 0.3561798709656801, 0.09210603853703984, 0.05103429693099443, 0.2422397858639279]
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.05103429693099443, 0.2074057107713633, 0.3561798709656801, 0.09210603853703984, 0.05103429693099443, 0.2422397858639279]
using explorer policy with actor:  1
siam score:  -0.82903457
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.050896476204932435, 0.20793444281736304, 0.3552123109839725, 0.09214329126301471, 0.050896476204932435, 0.24291700252578494]
printing an ep nov before normalisation:  62.09410707479126
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.47726917266846
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.05099251439194306, 0.20678253827561435, 0.35588053911769685, 0.09260520342674482, 0.05099251439194306, 0.24274669039605776]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.05099251439194306, 0.20678253827561435, 0.35588053911769685, 0.09260520342674482, 0.05099251439194306, 0.24274669039605776]
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.05099251439194306, 0.20678253827561435, 0.35588053911769685, 0.09260520342674482, 0.05099251439194306, 0.24274669039605776]
from probs:  [0.05122427103409903, 0.20512377615561786, 0.3575016157468756, 0.09302658162140949, 0.05122427103409903, 0.24189948440789905]
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.05132301272606969, 0.20551995348905278, 0.3581922878574573, 0.09320611302777468, 0.05132301272606969, 0.2404356201735758]
Printing some Q and Qe and total Qs values:  [[0.233]
 [0.377]
 [0.196]
 [0.192]
 [0.21 ]
 [0.232]
 [0.211]] [[44.727]
 [57.43 ]
 [44.21 ]
 [44.155]
 [43.739]
 [46.708]
 [43.438]] [[1.233]
 [1.908]
 [1.174]
 [1.168]
 [1.169]
 [1.315]
 [1.157]]
printing an ep nov before normalisation:  67.32532839606847
siam score:  -0.8363934
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
probs:  [0.05146795448609826, 0.20610149750400192, 0.3592061172674305, 0.09255204605481218, 0.05146795448609826, 0.23920443020155902]
maxi score, test score, baseline:  -0.9962636363636364 -1.0 -0.9962636363636364
Printing some Q and Qe and total Qs values:  [[0.906]
 [0.906]
 [0.904]
 [0.903]
 [0.906]
 [0.906]
 [0.906]] [[1.781]
 [1.781]
 [1.899]
 [1.904]
 [1.781]
 [1.781]
 [1.781]] [[0.906]
 [0.906]
 [0.904]
 [0.903]
 [0.906]
 [0.906]
 [0.906]]
maxi score, test score, baseline:  -0.9962768115942029 -1.0 -0.9962768115942029
probs:  [0.051599897803097046, 0.20406291385323044, 0.36012902192077073, 0.09278951436011042, 0.051599897803097046, 0.23981875425969434]
siam score:  -0.84163636
using another actor
maxi score, test score, baseline:  -0.9962768115942029 -1.0 -0.9962768115942029
maxi score, test score, baseline:  -0.9962768115942029 -1.0 -0.9962768115942029
probs:  [0.051722551093100595, 0.20057497429999507, 0.360983345366237, 0.09329693752604325, 0.051722551093100595, 0.24169964062152352]
printing an ep nov before normalisation:  29.975814533212947
printing an ep nov before normalisation:  51.91702153046633
printing an ep nov before normalisation:  52.67598963190302
maxi score, test score, baseline:  -0.9962898916967509 -1.0 -0.9962898916967509
probs:  [0.052121708424529734, 0.19724369573893832, 0.36377531365875043, 0.09309342879142467, 0.052121708424529734, 0.24164414496182707]
maxi score, test score, baseline:  -0.9962898916967509 -1.0 -0.9962898916967509
probs:  [0.052121708424529734, 0.19724369573893832, 0.36377531365875043, 0.09309342879142467, 0.052121708424529734, 0.24164414496182707]
printing an ep nov before normalisation:  74.32592613197151
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.788]
 [0.381]
 [0.371]
 [0.38 ]
 [0.374]
 [0.374]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.381]
 [0.788]
 [0.381]
 [0.371]
 [0.38 ]
 [0.374]
 [0.374]]
maxi score, test score, baseline:  -0.9963028776978418 -1.0 -0.9963028776978418
printing an ep nov before normalisation:  72.0893048279431
maxi score, test score, baseline:  -0.9963028776978418 -1.0 -0.9963028776978418
probs:  [0.052169258518649064, 0.19742414691572288, 0.3641079066102525, 0.09226454416764011, 0.052169258518649064, 0.24186488526908645]
maxi score, test score, baseline:  -0.9963028776978418 -1.0 -0.9963028776978418
maxi score, test score, baseline:  -0.9963028776978418 -1.0 -0.9963028776978418
probs:  [0.052169258518649064, 0.19742414691572288, 0.3641079066102525, 0.09226454416764011, 0.052169258518649064, 0.24186488526908645]
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.425]
 [0.425]
 [0.428]
 [0.425]
 [0.425]
 [0.425]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.425]
 [0.425]
 [0.425]
 [0.428]
 [0.425]
 [0.425]
 [0.425]]
maxi score, test score, baseline:  -0.9963028776978418 -1.0 -0.9963028776978418
probs:  [0.05213406173761624, 0.19829496075049155, 0.36385807112909396, 0.09247943710711483, 0.05213406173761624, 0.2410994075380672]
maxi score, test score, baseline:  -0.9963028776978418 -1.0 -0.9963028776978418
maxi score, test score, baseline:  -0.9963028776978418 -1.0 -0.9963028776978418
probs:  [0.05213406173761624, 0.19829496075049155, 0.36385807112909396, 0.09247943710711483, 0.05213406173761624, 0.2410994075380672]
printing an ep nov before normalisation:  61.25762930163987
printing an ep nov before normalisation:  39.19675778588325
maxi score, test score, baseline:  -0.9963028776978418 -1.0 -0.9963028776978418
probs:  [0.05213406173761624, 0.19829496075049155, 0.36385807112909396, 0.09247943710711483, 0.05213406173761624, 0.2410994075380672]
printing an ep nov before normalisation:  36.28835394249871
Printing some Q and Qe and total Qs values:  [[0.095]
 [0.055]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]] [[35.24]
 [46.37]
 [35.24]
 [35.24]
 [35.24]
 [35.24]
 [35.24]] [[0.691]
 [0.931]
 [0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.691]]
printing an ep nov before normalisation:  0.01472973262167443
printing an ep nov before normalisation:  65.42255878448486
printing an ep nov before normalisation:  79.13486872307885
siam score:  -0.8449733
maxi score, test score, baseline:  -0.9963028776978418 -1.0 -0.9963028776978418
probs:  [0.052038669199456225, 0.1975287467763567, 0.3631835217053414, 0.0919471566362298, 0.052038669199456225, 0.24326323648315964]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.66533946990967
maxi score, test score, baseline:  -0.9963028776978418 -1.0 -0.9963028776978418
probs:  [0.052038669199456225, 0.1975287467763567, 0.3631835217053414, 0.0919471566362298, 0.052038669199456225, 0.24326323648315964]
from probs:  [0.052038669199456225, 0.1975287467763567, 0.3631835217053414, 0.0919471566362298, 0.052038669199456225, 0.24326323648315964]
maxi score, test score, baseline:  -0.996315770609319 -1.0 -0.996315770609319
probs:  [0.052162263251747935, 0.19561977777650744, 0.36404799060181947, 0.09216569310660679, 0.052162263251747935, 0.24384201201157032]
printing an ep nov before normalisation:  53.55268344059656
maxi score, test score, baseline:  -0.996315770609319 -1.0 -0.996315770609319
probs:  [0.052162263251747935, 0.19561977777650744, 0.36404799060181947, 0.09216569310660679, 0.052162263251747935, 0.24384201201157032]
Printing some Q and Qe and total Qs values:  [[0.398]
 [0.501]
 [0.372]
 [0.402]
 [0.586]
 [0.372]
 [0.435]] [[83.338]
 [81.972]
 [62.029]
 [71.764]
 [71.24 ]
 [62.029]
 [78.653]] [[0.942]
 [1.031]
 [0.704]
 [0.83 ]
 [1.009]
 [0.704]
 [0.932]]
maxi score, test score, baseline:  -0.9963285714285715 -1.0 -0.9963285714285715
probs:  [0.05210733509139985, 0.19374208697565073, 0.3636716234212722, 0.09311715737712606, 0.053309194120631835, 0.24405260301391923]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9963285714285715 -1.0 -0.9963285714285715
probs:  [0.05222629896382784, 0.19189816274646504, 0.3645037288065454, 0.09332998889678386, 0.053430908935958095, 0.24461091165041987]
maxi score, test score, baseline:  -0.9963412811387901 -1.0 -0.9963412811387901
probs:  [0.05222630278637846, 0.19189831814224734, 0.36450375150276465, 0.09332992018977021, 0.053430922295572066, 0.24461078508326733]
siam score:  -0.844453
maxi score, test score, baseline:  -0.9963412811387901 -1.0 -0.9963412811387901
probs:  [0.05238926509667487, 0.19249821518627105, 0.3656436086015861, 0.09362146658385478, 0.05238926509667487, 0.24345817943493828]
printing an ep nov before normalisation:  78.90736215567397
printing an ep nov before normalisation:  60.763397216796875
maxi score, test score, baseline:  -0.9963412811387901 -1.0 -0.9963412811387901
probs:  [0.05238926509667487, 0.19249821518627105, 0.3656436086015861, 0.09362146658385478, 0.05238926509667487, 0.24345817943493828]
using another actor
printing an ep nov before normalisation:  21.264490753153908
printing an ep nov before normalisation:  76.89313727012838
maxi score, test score, baseline:  -0.9963664310954063 -1.0 -0.9963664310954063
maxi score, test score, baseline:  -0.9963664310954063 -1.0 -0.9963664310954063
probs:  [0.05250736337020516, 0.19067522720057017, 0.36646965141533294, 0.09383259904364914, 0.05250736337020516, 0.24400779560003757]
printing an ep nov before normalisation:  54.300994873046875
printing an ep nov before normalisation:  49.288514068307876
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9963664310954063 -1.0 -0.9963664310954063
probs:  [0.05246057940532602, 0.1901463453481839, 0.3661351227705175, 0.09431338920641535, 0.05246057940532602, 0.244483983864231]
printing an ep nov before normalisation:  60.083557294831486
maxi score, test score, baseline:  -0.9963664310954063 -1.0 -0.9963664310954063
probs:  [0.052608800386047, 0.19068457113947782, 0.36717184940052594, 0.09365300410647992, 0.052608800386047, 0.24327297458142214]
printing an ep nov before normalisation:  65.02143095174367
Printing some Q and Qe and total Qs values:  [[0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]] [[49.966]
 [49.966]
 [49.966]
 [49.966]
 [49.966]
 [49.966]
 [49.966]] [[1.585]
 [1.585]
 [1.585]
 [1.585]
 [1.585]
 [1.585]
 [1.585]]
maxi score, test score, baseline:  -0.9963788732394366 -1.0 -0.9963788732394366
probs:  [0.05260880609517264, 0.1906847190127737, 0.3671718852787718, 0.09365293515743278, 0.05260880609517264, 0.2432728483606764]
maxi score, test score, baseline:  -0.9963788732394366 -1.0 -0.9963788732394366
probs:  [0.05260880609517264, 0.1906847190127737, 0.3671718852787718, 0.09365293515743278, 0.05260880609517264, 0.2432728483606764]
actor:  1 policy actor:  1  step number:  38 total reward:  0.4099999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9963912280701754 -1.0 -0.9963912280701754
probs:  [0.04676276746625175, 0.17028826649962298, 0.32627869606197707, 0.19257312547902794, 0.04676276746625175, 0.21733437702686853]
from probs:  [0.04676276746625175, 0.17028826649962298, 0.32627869606197707, 0.19257312547902794, 0.04676276746625175, 0.21733437702686853]
maxi score, test score, baseline:  -0.9964034965034965 -1.0 -0.9964034965034965
probs:  [0.04676276971232345, 0.17028838851016437, 0.32627870818926624, 0.1925731087641132, 0.04676276971232345, 0.21733425511180937]
siam score:  -0.8424926
printing an ep nov before normalisation:  47.061209410942084
printing an ep nov before normalisation:  44.98065320503573
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9964156794425088 -1.0 -0.9964156794425088
probs:  [0.04676277194268727, 0.17028850966860448, 0.3262787202316976, 0.19257309216569546, 0.04676277194268727, 0.21733413404862795]
maxi score, test score, baseline:  -0.9964277777777778 -1.0 -0.9964277777777778
maxi score, test score, baseline:  -0.9964277777777778 -1.0 -0.9964277777777778
maxi score, test score, baseline:  -0.9964277777777778 -1.0 -0.9964277777777778
probs:  [0.04676277415750744, 0.17028862998383867, 0.326278732190159, 0.19257307568256088, 0.04676277415750744, 0.21733401382842657]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9964397923875432 -1.0 -0.9964397923875432
probs:  [0.04676277635694585, 0.17028874946463882, 0.32627874406552554, 0.19257305931351257, 0.04676277635694585, 0.21733389444243134]
maxi score, test score, baseline:  -0.9964397923875432 -1.0 -0.9964397923875432
maxi score, test score, baseline:  -0.9964397923875432 -1.0 -0.9964397923875432
probs:  [0.04676277635694585, 0.17028874946463882, 0.32627874406552554, 0.19257305931351257, 0.04676277635694585, 0.21733389444243134]
maxi score, test score, baseline:  -0.9964397923875432 -1.0 -0.9964397923875432
probs:  [0.04670327145519506, 0.1709053185945354, 0.3258592688298251, 0.19331159342634335, 0.04670327145519506, 0.21651727623890613]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.996451724137931 -1.0 -0.996451724137931
probs:  [0.046565495039892844, 0.1712344040999824, 0.32489233590485955, 0.19372474430478379, 0.046565495039892844, 0.2170175256105885]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.996451724137931 -1.0 -0.996451724137931
from probs:  [0.046565495039892844, 0.1712344040999824, 0.32489233590485955, 0.19372474430478379, 0.046565495039892844, 0.2170175256105885]
maxi score, test score, baseline:  -0.996451724137931 -1.0 -0.996451724137931
probs:  [0.046626759224302115, 0.16811152952604563, 0.3253279311456966, 0.19449480114924655, 0.04771079029421801, 0.21772818866049115]
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]] [[72.132]
 [72.132]
 [72.132]
 [72.132]
 [72.132]
 [72.132]
 [72.132]] [[0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]]
printing an ep nov before normalisation:  41.20568562819404
maxi score, test score, baseline:  -0.996451724137931 -1.0 -0.996451724137931
probs:  [0.04648988734619253, 0.1684272405710428, 0.32436737091676804, 0.19490880120744836, 0.04757795689722125, 0.21822874306132697]
printing an ep nov before normalisation:  51.94478660809107
maxi score, test score, baseline:  -0.996451724137931 -1.0 -0.996451724137931
probs:  [0.04648988734619253, 0.1684272405710428, 0.32436737091676804, 0.19490880120744836, 0.04757795689722125, 0.21822874306132697]
printing an ep nov before normalisation:  30.59410572052002
maxi score, test score, baseline:  -0.996451724137931 -1.0 -0.996451724137931
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.617]
 [0.604]
 [0.617]
 [0.631]
 [0.612]
 [0.622]] [[32.885]
 [43.498]
 [35.427]
 [31.361]
 [34.921]
 [32.583]
 [32.473]] [[0.635]
 [0.617]
 [0.604]
 [0.617]
 [0.631]
 [0.612]
 [0.622]]
Printing some Q and Qe and total Qs values:  [[0.4  ]
 [0.329]
 [0.387]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.4  ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.4  ]
 [0.329]
 [0.387]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.4  ]]
printing an ep nov before normalisation:  0.9354392910410071
printing an ep nov before normalisation:  81.13755417480502
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.146]
 [0.197]
 [0.079]
 [0.174]
 [0.169]
 [0.006]
 [0.173]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.146]
 [0.197]
 [0.079]
 [0.174]
 [0.169]
 [0.006]
 [0.173]]
maxi score, test score, baseline:  -0.9964986394557823 -1.0 -0.9964986394557823
probs:  [0.04630700976833043, 0.16743744045686915, 0.32308179586890345, 0.19611531463831539, 0.04740529751206746, 0.2196531417555141]
printing an ep nov before normalisation:  102.0823884913802
maxi score, test score, baseline:  -0.9964986394557823 -1.0 -0.9964986394557823
probs:  [0.04630700976833043, 0.16743744045686915, 0.32308179586890345, 0.19611531463831539, 0.04740529751206746, 0.2196531417555141]
maxi score, test score, baseline:  -0.9964986394557823 -1.0 -0.9964986394557823
probs:  [0.04630700976833043, 0.16743744045686915, 0.32308179586890345, 0.19611531463831539, 0.04740529751206746, 0.2196531417555141]
maxi score, test score, baseline:  -0.9964986394557823 -1.0 -0.9964986394557823
probs:  [0.046265866923362264, 0.16789305687523548, 0.32280104974511775, 0.1964352543578393, 0.048428793014033154, 0.21817597908441208]
maxi score, test score, baseline:  -0.9964986394557823 -1.0 -0.9964986394557823
probs:  [0.046265866923362264, 0.16789305687523548, 0.32280104974511775, 0.1964352543578393, 0.048428793014033154, 0.21817597908441208]
Printing some Q and Qe and total Qs values:  [[0.344]
 [0.484]
 [0.344]
 [0.344]
 [0.344]
 [0.429]
 [0.344]] [[37.061]
 [90.496]
 [37.061]
 [37.061]
 [37.061]
 [78.147]
 [37.061]] [[0.681]
 [1.749]
 [0.681]
 [0.681]
 [0.681]
 [1.48 ]
 [0.681]]
maxi score, test score, baseline:  -0.9964986394557823 -1.0 -0.9964986394557823
probs:  [0.04641181457997521, 0.1684237907328534, 0.3238218587093404, 0.19556548916520905, 0.048581583417846075, 0.21719546339477583]
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.635]
 [0.596]
 [0.536]
 [0.603]
 [0.812]
 [0.599]] [[17.411]
 [20.892]
 [17.834]
 [17.447]
 [17.19 ]
 [26.554]
 [17.567]] [[1.733]
 [1.994]
 [1.754]
 [1.669]
 [1.72 ]
 [2.544]
 [1.74 ]]
printing an ep nov before normalisation:  62.83038485146604
actor:  1 policy actor:  1  step number:  45 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9964986394557823 -1.0 -0.9964986394557823
printing an ep nov before normalisation:  32.786522971259224
maxi score, test score, baseline:  -0.9965101694915255 -1.0 -0.9965101694915255
maxi score, test score, baseline:  -0.9965101694915255 -1.0 -0.9965101694915255
probs:  [0.042146390493112085, 0.1518964990015728, 0.2939850893959493, 0.17710039276652093, 0.04412919172419777, 0.29074243661864707]
printing an ep nov before normalisation:  57.12064182346387
printing an ep nov before normalisation:  83.6622306592624
maxi score, test score, baseline:  -0.9965101694915255 -1.0 -0.9965101694915255
probs:  [0.041878897391447505, 0.15235016573179466, 0.2920680872806722, 0.17771967241537617, 0.043874727462521655, 0.2921084497181879]
maxi score, test score, baseline:  -0.9965101694915255 -1.0 -0.9965101694915255
probs:  [0.041878897391447505, 0.15235016573179466, 0.2920680872806722, 0.17771967241537617, 0.043874727462521655, 0.2921084497181879]
maxi score, test score, baseline:  -0.9965101694915255 -1.0 -0.9965101694915255
probs:  [0.041878897391447505, 0.15235016573179466, 0.2920680872806722, 0.17771967241537617, 0.043874727462521655, 0.2921084497181879]
Printing some Q and Qe and total Qs values:  [[-0.043]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]] [[59.321]
 [59.321]
 [59.321]
 [59.321]
 [59.321]
 [59.321]
 [59.321]] [[0.587]
 [0.587]
 [0.587]
 [0.587]
 [0.587]
 [0.587]
 [0.587]]
Printing some Q and Qe and total Qs values:  [[0.37 ]
 [0.39 ]
 [0.373]
 [0.37 ]
 [0.379]
 [0.373]
 [0.368]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.37 ]
 [0.39 ]
 [0.373]
 [0.37 ]
 [0.379]
 [0.373]
 [0.368]]
Printing some Q and Qe and total Qs values:  [[0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.761]] [[77.816]
 [77.816]
 [77.816]
 [77.816]
 [77.816]
 [77.816]
 [77.816]] [[2.433]
 [2.433]
 [2.433]
 [2.433]
 [2.433]
 [2.433]
 [2.433]]
printing an ep nov before normalisation:  73.58132309844976
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9965216216216216 -1.0 -0.9965216216216216
probs:  [0.041897642226116964, 0.15141809874274034, 0.29223656779241813, 0.17871026564023168, 0.043907765621454625, 0.2918296599770382]
maxi score, test score, baseline:  -0.9965216216216216 -1.0 -0.9965216216216216
probs:  [0.041897642226116964, 0.15141809874274034, 0.29223656779241813, 0.17871026564023168, 0.043907765621454625, 0.2918296599770382]
maxi score, test score, baseline:  -0.9965216216216216 -1.0 -0.9965216216216216
Printing some Q and Qe and total Qs values:  [[0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]] [[74.916]
 [74.916]
 [74.916]
 [74.916]
 [74.916]
 [74.916]
 [74.916]] [[0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]]
from probs:  [0.04196930819610378, 0.14996309406428887, 0.29273780167143154, 0.1790166879181621, 0.043982895199076485, 0.2923302129509372]
maxi score, test score, baseline:  -0.9965555183946488 -1.0 -0.9965555183946488
printing an ep nov before normalisation:  63.63106550218113
maxi score, test score, baseline:  -0.9965555183946488 -1.0 -0.9965555183946488
probs:  [0.04212698973018486, 0.14883573009825496, 0.29384063155545764, 0.17969088880062942, 0.044148194468083365, 0.2913575653473897]
maxi score, test score, baseline:  -0.9965666666666667 -1.0 -0.9965666666666667
probs:  [0.04221328306691519, 0.14914137389330148, 0.29444417032000036, 0.18005985510400496, 0.04423865531226568, 0.2899026623035123]
actions average: 
K:  4  action  0 :  tensor([0.2816, 0.0038, 0.1277, 0.1643, 0.1605, 0.1154, 0.1467],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0190, 0.7929, 0.0196, 0.0729, 0.0165, 0.0179, 0.0613],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1743, 0.0364, 0.1593, 0.1835, 0.1518, 0.1318, 0.1630],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1639, 0.0697, 0.1441, 0.1860, 0.1386, 0.1301, 0.1676],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1778, 0.0209, 0.1577, 0.1971, 0.1545, 0.1278, 0.1642],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1587, 0.0113, 0.1526, 0.1796, 0.1394, 0.2022, 0.1562],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1648, 0.0212, 0.1325, 0.2121, 0.1626, 0.1246, 0.1822],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  52.353483440986224
maxi score, test score, baseline:  -0.9965666666666667 -1.0 -0.9965666666666667
probs:  [0.04236952887378436, 0.14802099870249402, 0.2955369647427076, 0.18072793361164013, 0.044402421449390994, 0.28894215261998285]
printing an ep nov before normalisation:  34.3652284916646
line 256 mcts: sample exp_bonus 45.19830740868525
maxi score, test score, baseline:  -0.9965666666666667 -1.0 -0.9965666666666667
siam score:  -0.84672105
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.465]
 [0.488]
 [0.488]
 [0.423]
 [0.412]
 [0.488]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.457]
 [0.465]
 [0.488]
 [0.488]
 [0.423]
 [0.412]
 [0.488]]
maxi score, test score, baseline:  -0.9965666666666667 -1.0 -0.9965666666666667
probs:  [0.04252473502999531, 0.14691240699249963, 0.29662248777385797, 0.1813915667631143, 0.044565097896010875, 0.2879837055445218]
maxi score, test score, baseline:  -0.9965666666666667 -1.0 -0.9965666666666667
maxi score, test score, baseline:  -0.9965666666666667 -1.0 -0.9965666666666667
maxi score, test score, baseline:  -0.9965666666666667 -1.0 -0.9965666666666667
probs:  [0.04267891315323878, 0.14581537982339918, 0.2977008206684169, 0.18205080423354922, 0.044726696828991865, 0.28702738529240407]
Printing some Q and Qe and total Qs values:  [[0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]] [[72.427]
 [72.427]
 [72.427]
 [72.427]
 [72.427]
 [72.427]
 [72.427]] [[0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]]
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.605]
 [0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]] [[57.546]
 [64.929]
 [57.546]
 [57.546]
 [57.546]
 [57.546]
 [57.546]] [[1.176]
 [1.473]
 [1.176]
 [1.176]
 [1.176]
 [1.176]
 [1.176]]
from probs:  [0.04267891087263377, 0.14581542824490087, 0.29770080165266966, 0.18205078619975568, 0.044726708616663835, 0.2870273644133761]
printing an ep nov before normalisation:  65.1215318111374
maxi score, test score, baseline:  -0.9965777408637874 -1.0 -0.9965777408637874
probs:  [0.0426023177806727, 0.14622446193740413, 0.29716213320866347, 0.18125002318932706, 0.044659757748999315, 0.28810130613493323]
line 256 mcts: sample exp_bonus 60.48952813046895
maxi score, test score, baseline:  -0.9965887417218543 -1.0 -0.9965887417218543
maxi score, test score, baseline:  -0.9965887417218543 -1.0 -0.9965887417218543
probs:  [0.042646555657300904, 0.14637669512129625, 0.2974715299373434, 0.1814386728904124, 0.043665281197599104, 0.28840126519604786]
maxi score, test score, baseline:  -0.9965887417218543 -1.0 -0.9965887417218543
probs:  [0.042646555657300904, 0.14637669512129625, 0.2974715299373434, 0.1814386728904124, 0.043665281197599104, 0.28840126519604786]
maxi score, test score, baseline:  -0.9965887417218543 -1.0 -0.9965887417218543
probs:  [0.042558016537961894, 0.14666259011124383, 0.29685881976524353, 0.18151604404661492, 0.0445620309984698, 0.28784249854046595]
maxi score, test score, baseline:  -0.9965887417218543 -1.0 -0.9965887417218543
probs:  [0.04242374447280708, 0.14687085181299753, 0.29591678499429247, 0.18183898356180644, 0.04443435271299763, 0.2885152824450989]
printing an ep nov before normalisation:  67.48088106992014
Printing some Q and Qe and total Qs values:  [[ 0.129]
 [-0.011]
 [ 0.14 ]
 [ 0.158]
 [ 0.132]
 [ 0.129]
 [ 0.139]] [[47.919]
 [58.211]
 [46.946]
 [46.875]
 [41.165]
 [45.379]
 [48.297]] [[0.794]
 [0.889]
 [0.782]
 [0.798]
 [0.641]
 [0.735]
 [0.812]]
Printing some Q and Qe and total Qs values:  [[0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.37]
 [0.38]
 [0.38]] [[51.79 ]
 [51.79 ]
 [51.79 ]
 [51.79 ]
 [59.985]
 [51.79 ]
 [51.79 ]] [[1.106]
 [1.106]
 [1.106]
 [1.106]
 [1.318]
 [1.106]
 [1.106]]
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.585]
 [0.469]
 [0.491]
 [0.453]
 [0.463]
 [0.449]] [[46.208]
 [51.244]
 [46.201]
 [51.868]
 [45.602]
 [44.368]
 [44.013]] [[0.944]
 [1.148]
 [0.937]
 [1.066]
 [0.909]
 [0.895]
 [0.875]]
actions average: 
K:  3  action  0 :  tensor([0.2336, 0.0088, 0.1415, 0.1629, 0.1620, 0.1427, 0.1485],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0271, 0.8564, 0.0244, 0.0298, 0.0194, 0.0187, 0.0243],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1275, 0.0726, 0.2611, 0.1362, 0.1201, 0.1420, 0.1405],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1550, 0.0632, 0.1341, 0.2165, 0.1506, 0.1346, 0.1460],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1542, 0.0012, 0.1172, 0.1361, 0.3528, 0.1112, 0.1272],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1532, 0.0122, 0.1714, 0.1526, 0.1561, 0.2053, 0.1490],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1728, 0.0022, 0.1574, 0.1707, 0.1437, 0.1594, 0.1938],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.457]
 [0.434]
 [0.411]
 [0.426]
 [0.208]
 [0.429]] [[56.258]
 [61.588]
 [58.258]
 [56.691]
 [58.439]
 [60.359]
 [61.409]] [[1.076]
 [1.25 ]
 [1.146]
 [1.084]
 [1.142]
 [0.972]
 [1.219]]
maxi score, test score, baseline:  -0.9965887417218543 -1.0 -0.9965887417218543
probs:  [0.04229856040116355, 0.14778352508815631, 0.295035393694722, 0.1817235516496467, 0.044329147409428796, 0.28882982175688265]
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.45 ]
 [0.466]
 [0.422]
 [0.466]
 [0.466]
 [0.466]] [[60.56 ]
 [73.058]
 [60.56 ]
 [65.787]
 [60.56 ]
 [60.56 ]
 [60.56 ]] [[1.467]
 [1.91 ]
 [1.467]
 [1.615]
 [1.467]
 [1.467]
 [1.467]]
maxi score, test score, baseline:  -0.9965887417218543 -1.0 -0.9965887417218543
maxi score, test score, baseline:  -0.9965887417218543 -1.0 -0.9965887417218543
probs:  [0.04229856040116355, 0.14778352508815631, 0.295035393694722, 0.1817235516496467, 0.044329147409428796, 0.28882982175688265]
siam score:  -0.83975387
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.38 ]
 [0.404]
 [0.397]
 [0.397]
 [0.397]
 [0.395]] [[69.607]
 [82.198]
 [77.155]
 [66.933]
 [66.874]
 [67.465]
 [66.304]] [[1.137]
 [1.375]
 [1.298]
 [1.085]
 [1.084]
 [1.097]
 [1.07 ]]
siam score:  -0.84408104
printing an ep nov before normalisation:  53.970957650785074
maxi score, test score, baseline:  -0.9966105263157895 -1.0 -0.9966105263157895
probs:  [0.0425173973914905, 0.14758630535002437, 0.29656297018982214, 0.18083057243364767, 0.04457165159691636, 0.287931103038099]
using explorer policy with actor:  1
printing an ep nov before normalisation:  60.977532210498396
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9966105263157895 -1.0 -0.9966105263157895
probs:  [0.0425173973914905, 0.14758630535002437, 0.29656297018982214, 0.18083057243364767, 0.04457165159691636, 0.287931103038099]
printing an ep nov before normalisation:  45.059242630514014
printing an ep nov before normalisation:  76.70263051315631
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.351]
 [0.32 ]
 [0.373]
 [0.393]
 [0.399]
 [0.378]] [[71.447]
 [72.299]
 [70.295]
 [70.725]
 [71.658]
 [72.264]
 [72.349]] [[1.233]
 [1.218]
 [1.145]
 [1.207]
 [1.247]
 [1.265]
 [1.246]]
printing an ep nov before normalisation:  58.290462555165874
maxi score, test score, baseline:  -0.9966105263157895 -1.0 -0.9966105263157895
probs:  [0.04233565981145693, 0.1482953932174018, 0.2952860101391306, 0.18182152141173838, 0.04440733098596718, 0.28785408443430516]
printing an ep nov before normalisation:  80.9217629621873
printing an ep nov before normalisation:  73.21316308539019
printing an ep nov before normalisation:  32.75157928466797
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9966320261437909 -1.0 -0.9966320261437909
probs:  [0.042343922106902625, 0.14899884927877244, 0.29534081748427843, 0.1813805776470842, 0.04442921112212351, 0.28750662236083885]
printing an ep nov before normalisation:  57.35823602384477
maxi score, test score, baseline:  -0.996642671009772 -1.0 -0.996642671009772
probs:  [0.04241340361691577, 0.1475991436633909, 0.29582674503140555, 0.18167891178568799, 0.044502139190838105, 0.2879796567117617]
maxi score, test score, baseline:  -0.996642671009772 -1.0 -0.996642671009772
maxi score, test score, baseline:  -0.996642671009772 -1.0 -0.996642671009772
probs:  [0.04241340361691577, 0.1475991436633909, 0.29582674503140555, 0.18167891178568799, 0.044502139190838105, 0.2879796567117617]
printing an ep nov before normalisation:  27.33452320098877
printing an ep nov before normalisation:  8.338602201645244
siam score:  -0.8345117
maxi score, test score, baseline:  -0.996642671009772 -1.0 -0.996642671009772
printing an ep nov before normalisation:  18.07478565049809
printing an ep nov before normalisation:  71.75360844956853
Printing some Q and Qe and total Qs values:  [[0.585]
 [0.498]
 [0.585]
 [0.615]
 [0.615]
 [0.585]
 [0.585]] [[60.088]
 [70.395]
 [60.088]
 [62.196]
 [62.524]
 [60.088]
 [60.088]] [[0.585]
 [0.498]
 [0.585]
 [0.615]
 [0.615]
 [0.585]
 [0.585]]
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
probs:  [0.04259841779029395, 0.14824449965171413, 0.29712066966620454, 0.18111660397925455, 0.04363620365160482, 0.287283605260928]
printing an ep nov before normalisation:  5.254730999542971
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
printing an ep nov before normalisation:  97.70761259361191
printing an ep nov before normalisation:  44.7823890132379
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
probs:  [0.04385650908861922, 0.14487247930045533, 0.29880741456892324, 0.18260798065575917, 0.0428386323914418, 0.2870169839948012]
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]]
printing an ep nov before normalisation:  93.75264420419103
printing an ep nov before normalisation:  3.8760539606988687
using another actor
printing an ep nov before normalisation:  22.850735909189044
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
probs:  [0.04368543708406519, 0.14555006829725461, 0.2975452511028692, 0.1836025952505863, 0.04265900894442506, 0.28695763932079943]
from probs:  [0.04368543708406519, 0.14555006829725461, 0.2975452511028692, 0.1836025952505863, 0.04265900894442506, 0.28695763932079943]
printing an ep nov before normalisation:  90.67752839573062
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
probs:  [0.04374457134732167, 0.14574751218642332, 0.2979490708128103, 0.1824950127109774, 0.04271674954542284, 0.2873470833970447]
maxi score, test score, baseline:  -0.9966532467532467 -1.0 -0.9966532467532467
probs:  [0.04374457134732167, 0.14574751218642332, 0.2979490708128103, 0.1824950127109774, 0.04271674954542284, 0.2873470833970447]
maxi score, test score, baseline:  -0.9966637540453075 -1.0 -0.9966637540453075
probs:  [0.04374457663420922, 0.14574755565631992, 0.2979490573316869, 0.1824949947011663, 0.04271674805283049, 0.2873470676237871]
printing an ep nov before normalisation:  68.13848614692688
maxi score, test score, baseline:  -0.9966741935483872 -1.0 -0.9966741935483872
probs:  [0.043911204997087956, 0.1463039383463291, 0.29908688974777875, 0.18319171466124767, 0.04287944269550233, 0.28462680955205416]
maxi score, test score, baseline:  -0.9966741935483872 -1.0 -0.9966741935483872
probs:  [0.04292362753752056, 0.14645502844066674, 0.29939590471152855, 0.18338093399189084, 0.04292362753752056, 0.28492087778087277]
printing an ep nov before normalisation:  35.00920768704118
actor:  1 policy actor:  1  step number:  31 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9966741935483872 -1.0 -0.9966741935483872
siam score:  -0.8346369
maxi score, test score, baseline:  -0.9966741935483872 -1.0 -0.9966741935483872
probs:  [0.04891721055744923, 0.13710544575549696, 0.34145022646731726, 0.16855899912428257, 0.04891721055744923, 0.25505090753800486]
printing an ep nov before normalisation:  87.11322200286403
maxi score, test score, baseline:  -0.9966845659163988 -1.0 -0.9966845659163988
maxi score, test score, baseline:  -0.9966845659163988 -1.0 -0.9966845659163988
probs:  [0.048691469669194305, 0.1375832041092311, 0.33987380207585444, 0.16898083787236565, 0.04955157079325505, 0.25531911548009945]
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.278]
 [0.381]
 [0.209]
 [0.197]
 [0.194]
 [0.135]] [[ 7.989]
 [ 6.706]
 [ 8.621]
 [ 9.252]
 [10.256]
 [14.693]
 [15.742]] [[0.542]
 [0.487]
 [0.649]
 [0.496]
 [0.516]
 [0.651]
 [0.624]]
maxi score, test score, baseline:  -0.9966845659163988 -1.0 -0.9966845659163988
maxi score, test score, baseline:  -0.9966845659163988 -1.0 -0.9966845659163988
probs:  [0.0487333040911281, 0.1377016214983725, 0.34016649904597235, 0.16912630529008424, 0.0487333040911281, 0.25553896598331466]
maxi score, test score, baseline:  -0.9966845659163988 -1.0 -0.9966845659163988
maxi score, test score, baseline:  -0.9966845659163988 -1.0 -0.9966845659163988
probs:  [0.04867258482331396, 0.1380434525368745, 0.339739360373572, 0.16845804793341598, 0.04867258482331396, 0.25641396950950973]
printing an ep nov before normalisation:  58.18075089553126
maxi score, test score, baseline:  -0.9966845659163988 -1.0 -0.9966845659163988
probs:  [0.04867258482331396, 0.1380434525368745, 0.339739360373572, 0.16845804793341598, 0.04867258482331396, 0.25641396950950973]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.378]
 [0.265]
 [0.226]
 [0.217]
 [0.026]
 [0.279]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.465]
 [0.378]
 [0.265]
 [0.226]
 [0.217]
 [0.026]
 [0.279]]
maxi score, test score, baseline:  -0.9966845659163988 -1.0 -0.9966845659163988
probs:  [0.04865821236624309, 0.13642662671329478, 0.3396417351886862, 0.16959818793046258, 0.04950746054868567, 0.2561677772526276]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.721]
 [0.72 ]
 [0.704]
 [0.7  ]
 [0.705]
 [0.692]
 [0.708]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.721]
 [0.72 ]
 [0.704]
 [0.7  ]
 [0.705]
 [0.692]
 [0.708]]
printing an ep nov before normalisation:  44.61801052093506
siam score:  -0.81838197
printing an ep nov before normalisation:  36.618194580078125
printing an ep nov before normalisation:  0.15925207797806706
maxi score, test score, baseline:  -0.9966948717948718 -1.0 -0.9966948717948718
probs:  [0.04837913514430487, 0.13722507908571577, 0.337689751731845, 0.16934307072009366, 0.050065142609851045, 0.2572978207081896]
printing an ep nov before normalisation:  51.830030301982646
maxi score, test score, baseline:  -0.9966948717948718 -1.0 -0.9966948717948718
probs:  [0.048456378534091925, 0.13744456794475696, 0.33823019155571843, 0.16961398160909885, 0.050145085356665155, 0.25610979499966857]
Printing some Q and Qe and total Qs values:  [[-0.009]
 [-0.023]
 [-0.005]
 [-0.   ]
 [-0.003]
 [-0.009]
 [-0.01 ]] [[61.985]
 [64.973]
 [64.069]
 [61.938]
 [60.135]
 [61.985]
 [65.069]] [[0.9  ]
 [0.936]
 [0.939]
 [0.909]
 [0.876]
 [0.9  ]
 [0.951]]
maxi score, test score, baseline:  -0.9966948717948718 -1.0 -0.9966948717948718
printing an ep nov before normalisation:  53.865458314422376
printing an ep nov before normalisation:  24.395663247853943
printing an ep nov before normalisation:  44.17945861816406
printing an ep nov before normalisation:  41.733219321667114
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.633]
 [0.593]
 [0.606]
 [0.619]
 [0.625]
 [0.601]
 [0.604]] [[13.343]
 [13.288]
 [ 3.349]
 [10.378]
 [12.943]
 [17.213]
 [17.397]] [[0.633]
 [0.593]
 [0.606]
 [0.619]
 [0.625]
 [0.601]
 [0.604]]
actions average: 
K:  4  action  0 :  tensor([0.1797, 0.0708, 0.1622, 0.1523, 0.1735, 0.1152, 0.1463],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0368, 0.7868, 0.0468, 0.0354, 0.0268, 0.0224, 0.0450],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1414, 0.0647, 0.1820, 0.1566, 0.1559, 0.1427, 0.1569],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1182, 0.0440, 0.1324, 0.3372, 0.1206, 0.1088, 0.1389],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1440, 0.0585, 0.1561, 0.1544, 0.2001, 0.1393, 0.1476],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1148, 0.0075, 0.2089, 0.1530, 0.1289, 0.2402, 0.1468],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1552, 0.0426, 0.1589, 0.1537, 0.1418, 0.1129, 0.2349],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
probs:  [0.048509849131797074, 0.13860889066270038, 0.338599783684726, 0.16777215953270486, 0.049356024329566206, 0.2571532926585054]
from probs:  [0.04858669049713487, 0.1388288452307851, 0.3391374035324217, 0.1680384369998975, 0.04943420975850978, 0.2559744139812511]
printing an ep nov before normalisation:  15.448050498962402
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
maxi score, test score, baseline:  -0.9967051118210862 -1.0 -0.9967051118210862
probs:  [0.0486407072122964, 0.13898346537841624, 0.3395153309423592, 0.16711169523686273, 0.0494891713020282, 0.25625962992803714]
maxi score, test score, baseline:  -0.9967152866242038 -1.0 -0.9967152866242038
probs:  [0.04864070621710167, 0.13898350824714054, 0.3395153214863636, 0.16711167673572316, 0.04948917570806995, 0.2562596116056012]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9967152866242038 -1.0 -0.9967152866242038
maxi score, test score, baseline:  -0.9967152866242038 -1.0 -0.9967152866242038
probs:  [0.048364950904597206, 0.13836812747053368, 0.33757920484806714, 0.1681601905984494, 0.04922290459231083, 0.2583046215860417]
maxi score, test score, baseline:  -0.9967152866242038 -1.0 -0.9967152866242038
probs:  [0.048364950904597206, 0.13836812747053368, 0.33757920484806714, 0.1681601905984494, 0.04922290459231083, 0.2583046215860417]
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.485]
 [0.515]
 [0.518]
 [0.516]
 [0.511]
 [0.513]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.515]
 [0.485]
 [0.515]
 [0.518]
 [0.516]
 [0.511]
 [0.513]]
maxi score, test score, baseline:  -0.9967152866242038 -1.0 -0.9967152866242038
probs:  [0.04840179643841558, 0.1377272017310137, 0.33784219145431676, 0.16878219180426776, 0.05009694493493097, 0.2571496736370552]
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.527]
 [0.321]
 [0.438]
 [0.422]
 [0.389]
 [0.48 ]] [[61.997]
 [62.915]
 [62.137]
 [61.82 ]
 [60.684]
 [60.016]
 [61.964]] [[0.892]
 [1.012]
 [0.797]
 [0.911]
 [0.881]
 [0.84 ]
 [0.954]]
maxi score, test score, baseline:  -0.9967152866242038 -1.0 -0.9967152866242038
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.26 ]
 [0.361]
 [0.294]
 [0.346]
 [0.298]
 [0.297]
 [0.348]] [[42.685]
 [42.42 ]
 [42.616]
 [42.654]
 [42.308]
 [42.36 ]
 [41.695]] [[1.488]
 [1.575]
 [1.519]
 [1.573]
 [1.505]
 [1.507]
 [1.521]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9967354430379747 -1.0 -0.9967354430379747
probs:  [0.04842850660344637, 0.13698731008881754, 0.33802681846461496, 0.16954644497065163, 0.050134062738773744, 0.25687685713369574]
printing an ep nov before normalisation:  62.796216654525644
printing an ep nov before normalisation:  31.84159776050182
Printing some Q and Qe and total Qs values:  [[0.223]
 [0.304]
 [0.221]
 [0.196]
 [0.196]
 [0.196]
 [0.215]] [[57.047]
 [56.608]
 [57.155]
 [55.508]
 [55.508]
 [55.508]
 [57.51 ]] [[0.989]
 [1.06 ]
 [0.989]
 [0.926]
 [0.926]
 [0.926]
 [0.992]]
maxi score, test score, baseline:  -0.9967354430379747 -1.0 -0.9967354430379747
probs:  [0.04842850660344637, 0.13698731008881754, 0.33802681846461496, 0.16954644497065163, 0.050134062738773744, 0.25687685713369574]
maxi score, test score, baseline:  -0.9967354430379747 -1.0 -0.9967354430379747
probs:  [0.04842850660344637, 0.13698731008881754, 0.33802681846461496, 0.16954644497065163, 0.050134062738773744, 0.25687685713369574]
maxi score, test score, baseline:  -0.9967354430379747 -1.0 -0.9967354430379747
maxi score, test score, baseline:  -0.9967354430379747 -1.0 -0.9967354430379747
probs:  [0.04820411561832941, 0.13733168988898184, 0.3364524021854752, 0.17009993650207236, 0.0499206257239483, 0.25799123008119296]
siam score:  -0.830835
maxi score, test score, baseline:  -0.9967354430379747 -1.0 -0.9967354430379747
probs:  [0.04820411561832941, 0.13733168988898184, 0.3364524021854752, 0.17009993650207236, 0.0499206257239483, 0.25799123008119296]
maxi score, test score, baseline:  -0.9967354430379747 -1.0 -0.9967354430379747
probs:  [0.04820411561832941, 0.13733168988898184, 0.3364524021854752, 0.17009993650207236, 0.0499206257239483, 0.25799123008119296]
maxi score, test score, baseline:  -0.9967354430379747 -1.0 -0.9967354430379747
printing an ep nov before normalisation:  57.26480689267675
printing an ep nov before normalisation:  67.76993221539297
printing an ep nov before normalisation:  71.4946885640423
siam score:  -0.8332722
printing an ep nov before normalisation:  39.58656469409054
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.629]
 [0.173]
 [0.446]
 [0.404]
 [0.196]
 [0.447]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.454]
 [0.629]
 [0.173]
 [0.446]
 [0.404]
 [0.196]
 [0.447]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.674]
 [0.674]
 [0.674]
 [0.674]
 [0.674]
 [0.674]
 [0.674]] [[68.644]
 [68.644]
 [68.644]
 [68.644]
 [68.644]
 [68.644]
 [68.644]] [[2.232]
 [2.232]
 [2.232]
 [2.232]
 [2.232]
 [2.232]
 [2.232]]
Printing some Q and Qe and total Qs values:  [[0.584]
 [0.605]
 [0.584]
 [0.589]
 [0.584]
 [0.584]
 [0.584]] [[64.376]
 [63.197]
 [64.376]
 [69.422]
 [64.376]
 [64.376]
 [64.376]] [[2.351]
 [2.318]
 [2.351]
 [2.589]
 [2.351]
 [2.351]
 [2.351]]
printing an ep nov before normalisation:  0.2933234142403762
printing an ep nov before normalisation:  42.052380789309325
Printing some Q and Qe and total Qs values:  [[0.888]
 [1.154]
 [0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.888]
 [1.154]
 [0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]]
printing an ep nov before normalisation:  44.5457649230957
printing an ep nov before normalisation:  67.92644394768608
printing an ep nov before normalisation:  45.23159398115108
maxi score, test score, baseline:  -0.9967652037617555 -1.0 -0.9967652037617555
probs:  [0.04826846599987267, 0.13767102067207448, 0.3368958320501585, 0.17007418695014354, 0.05001583864802117, 0.25707465567972954]
printing an ep nov before normalisation:  26.98436918144868
Printing some Q and Qe and total Qs values:  [[0.191]
 [0.216]
 [0.269]
 [0.246]
 [0.265]
 [0.235]
 [0.192]] [[53.923]
 [70.113]
 [58.141]
 [47.382]
 [54.091]
 [52.761]
 [51.472]] [[0.191]
 [0.216]
 [0.269]
 [0.246]
 [0.265]
 [0.235]
 [0.192]]
Printing some Q and Qe and total Qs values:  [[0.52]
 [0.55]
 [0.52]
 [0.52]
 [0.52]
 [0.52]
 [0.52]] [[72.606]
 [82.448]
 [72.606]
 [72.606]
 [72.606]
 [72.606]
 [72.606]] [[1.941]
 [2.275]
 [1.941]
 [1.941]
 [1.941]
 [1.941]
 [1.941]]
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
probs:  [0.048343231708316996, 0.13788468860965047, 0.33741890360870447, 0.1703381373347607, 0.05009332929977213, 0.2559217094387951]
Printing some Q and Qe and total Qs values:  [[0.288]
 [0.283]
 [0.259]
 [0.282]
 [0.273]
 [0.218]
 [0.243]] [[49.777]
 [45.643]
 [44.978]
 [46.606]
 [45.189]
 [43.732]
 [49.207]] [[1.676]
 [1.557]
 [1.513]
 [1.582]
 [1.533]
 [1.438]
 [1.616]]
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
probs:  [0.048343231708316996, 0.13788468860965047, 0.33741890360870447, 0.1703381373347607, 0.05009332929977213, 0.2559217094387951]
Printing some Q and Qe and total Qs values:  [[0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]] [[63.047]
 [63.047]
 [63.047]
 [63.047]
 [63.047]
 [63.047]
 [63.047]] [[2.094]
 [2.094]
 [2.094]
 [2.094]
 [2.094]
 [2.094]
 [2.094]]
Printing some Q and Qe and total Qs values:  [[0.233]
 [0.258]
 [0.233]
 [0.233]
 [0.233]
 [0.233]
 [0.233]] [[42.734]
 [50.522]
 [42.734]
 [42.734]
 [42.734]
 [42.734]
 [42.734]] [[1.317]
 [1.717]
 [1.317]
 [1.317]
 [1.317]
 [1.317]
 [1.317]]
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
probs:  [0.04838572351047422, 0.13859260666383474, 0.33772146202040537, 0.17098012525073472, 0.05097959808955904, 0.25334048446499174]
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
probs:  [0.0484583162449233, 0.13880090977088894, 0.3382293415035875, 0.17123715326725858, 0.05105609313914542, 0.25221818607419616]
printing an ep nov before normalisation:  77.77146654759501
printing an ep nov before normalisation:  85.79334677169729
maxi score, test score, baseline:  -0.996775 -1.0 -0.996775
printing an ep nov before normalisation:  49.144253730773926
maxi score, test score, baseline:  -0.9967847352024922 -1.0 -0.9967847352024922
probs:  [0.04872041347337422, 0.13823845050806496, 0.3400630460383991, 0.1710366848291775, 0.0513322957828669, 0.2506091093681173]
maxi score, test score, baseline:  -0.9967847352024922 -1.0 -0.9967847352024922
probs:  [0.04872041347337422, 0.13823845050806496, 0.3400630460383991, 0.1710366848291775, 0.0513322957828669, 0.2506091093681173]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9967847352024922 -1.0 -0.9967847352024922
probs:  [0.04872041347337422, 0.13823845050806496, 0.3400630460383991, 0.1710366848291775, 0.0513322957828669, 0.2506091093681173]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  35.67545215814728
maxi score, test score, baseline:  -0.9967847352024922 -1.0 -0.9967847352024922
probs:  [0.04861035728603267, 0.13841275458298116, 0.33929081029063984, 0.171315174773468, 0.05123053642109388, 0.2511403666457845]
maxi score, test score, baseline:  -0.9967847352024922 -1.0 -0.9967847352024922
probs:  [0.04861035728603267, 0.13841275458298116, 0.33929081029063984, 0.171315174773468, 0.05123053642109388, 0.2511403666457845]
printing an ep nov before normalisation:  33.756830596486814
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.3440],
        [-0.6409],
        [-0.0000],
        [-0.6503],
        [-0.6461],
        [ 0.0329],
        [-0.5674],
        [-0.6558],
        [-0.6559],
        [-0.3683]], dtype=torch.float64)
-0.145559551797 -0.4895472814555023
-0.048519850599000006 -0.689454790533019
-0.8919900000000001 -0.8919900000000001
-0.048519850599000006 -0.6988463941225074
-0.048519850599000006 -0.6945960664136213
-0.048519850599000006 -0.01563655363973295
-0.048519850599000006 -0.6158919931671228
-0.048519850599000006 -0.7043678549218146
-0.048519850599000006 -0.70440972043062
-0.145559551797 -0.5138933965641204
maxi score, test score, baseline:  -0.9967944099378883 -1.0 -0.9967944099378883
probs:  [0.04861035630089108, 0.13841279004635287, 0.3392908009428124, 0.17131516332922944, 0.05123055139728259, 0.25114033798343166]
maxi score, test score, baseline:  -0.9967944099378883 -1.0 -0.9967944099378883
probs:  [0.04861035630089108, 0.13841279004635287, 0.3392908009428124, 0.17131516332922944, 0.05123055139728259, 0.25114033798343166]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9968135802469136 -1.0 -0.9968135802469136
probs:  [0.04867346993130952, 0.13729197024964243, 0.33973235410026487, 0.17153801372151462, 0.05129710810099027, 0.2514670838962783]
maxi score, test score, baseline:  -0.9968135802469136 -1.0 -0.9968135802469136
probs:  [0.04867346993130952, 0.13729197024964243, 0.33973235410026487, 0.17153801372151462, 0.05129710810099027, 0.2514670838962783]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9968135802469136 -1.0 -0.9968135802469136
probs:  [0.05370214582386584, 0.12974440005747223, 0.37501619810883546, 0.1591304349692423, 0.055953451815269346, 0.22645336922531487]
printing an ep nov before normalisation:  39.07650579491282
maxi score, test score, baseline:  -0.9968135802469136 -1.0 -0.9968135802469136
probs:  [0.05370214582386584, 0.12974440005747223, 0.37501619810883546, 0.1591304349692423, 0.055953451815269346, 0.22645336922531487]
maxi score, test score, baseline:  -0.9968135802469136 -1.0 -0.9968135802469136
probs:  [0.05366329287179306, 0.12893629904415818, 0.3747424676843095, 0.1595567250402374, 0.055924531484537396, 0.22717668387496445]
maxi score, test score, baseline:  -0.9968230769230769 -1.0 -0.9968230769230769
probs:  [0.05366328955696426, 0.1289363278576195, 0.3747424423526539, 0.15955672440518584, 0.05592454193674416, 0.22717667389083232]
Printing some Q and Qe and total Qs values:  [[0.416]
 [0.955]
 [0.728]
 [0.583]
 [0.556]
 [0.857]
 [0.487]] [[57.591]
 [38.632]
 [53.676]
 [53.828]
 [54.617]
 [45.259]
 [54.664]] [[1.545]
 [1.701]
 [1.778]
 [1.636]
 [1.624]
 [1.737]
 [1.557]]
maxi score, test score, baseline:  -0.9968230769230769 -1.0 -0.9968230769230769
probs:  [0.05366328955696426, 0.1289363278576195, 0.3747424423526539, 0.15955672440518584, 0.05592454193674416, 0.22717667389083232]
printing an ep nov before normalisation:  46.420970704827624
using explorer policy with actor:  1
siam score:  -0.83596754
printing an ep nov before normalisation:  41.29493336233501
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.147]
 [0.295]
 [0.283]
 [0.287]
 [0.281]
 [0.278]] [[10.999]
 [14.005]
 [10.765]
 [10.858]
 [10.966]
 [11.247]
 [10.872]] [[1.075]
 [1.157]
 [1.066]
 [1.061]
 [1.074]
 [1.088]
 [1.057]]
printing an ep nov before normalisation:  11.68441229369716
printing an ep nov before normalisation:  73.67204651735368
maxi score, test score, baseline:  -0.9968230769230769 -1.0 -0.9968230769230769
maxi score, test score, baseline:  -0.9968230769230769 -1.0 -0.9968230769230769
probs:  [0.05390120816310936, 0.12950880344752946, 0.3764074226974267, 0.1583341720505099, 0.05617251085828492, 0.2256758827831397]
printing an ep nov before normalisation:  72.81860391642583
maxi score, test score, baseline:  -0.9968230769230769 -1.0 -0.9968230769230769
probs:  [0.05390120816310936, 0.12950880344752946, 0.3764074226974267, 0.1583341720505099, 0.05617251085828492, 0.2256758827831397]
printing an ep nov before normalisation:  0.07748354247240741
using another actor
from probs:  [0.053960112584703374, 0.12855588424046518, 0.3768196422617697, 0.15850748588773908, 0.05623390355948351, 0.22592297146583912]
maxi score, test score, baseline:  -0.9968230769230769 -1.0 -0.9968230769230769
probs:  [0.0537657271893556, 0.128854597597735, 0.3754556307240909, 0.15900418764412788, 0.056054548553506195, 0.2268653082911843]
printing an ep nov before normalisation:  2.9027256774611487
maxi score, test score, baseline:  -0.9968325153374233 -1.0 -0.9968325153374233
probs:  [0.053765724336973034, 0.12885462503139225, 0.37545560861190186, 0.15900418472811695, 0.05605455959623446, 0.22686529769538136]
maxi score, test score, baseline:  -0.9968325153374233 -1.0 -0.9968325153374233
probs:  [0.05366878043726221, 0.1290035994559982, 0.37477534866330087, 0.15925189983384808, 0.05596511169828143, 0.2273352599113092]
printing an ep nov before normalisation:  78.59956814852052
printing an ep nov before normalisation:  70.3339921708578
printing an ep nov before normalisation:  59.939004986332534
maxi score, test score, baseline:  -0.9968325153374233 -1.0 -0.9968325153374233
probs:  [0.05357200191351474, 0.12915231974610925, 0.37409624916568923, 0.15949919236400328, 0.05587581638891145, 0.22780442042177193]
printing an ep nov before normalisation:  51.700617598246396
maxi score, test score, baseline:  -0.9968325153374233 -1.0 -0.9968325153374233
probs:  [0.05357200191351474, 0.12915231974610925, 0.37409624916568923, 0.15949919236400328, 0.05587581638891145, 0.22780442042177193]
printing an ep nov before normalisation:  41.0462272832345
maxi score, test score, baseline:  -0.9968325153374233 -1.0 -0.9968325153374233
printing an ep nov before normalisation:  34.623513976902764
printing an ep nov before normalisation:  33.3834050373036
maxi score, test score, baseline:  -0.9968325153374233 -1.0 -0.9968325153374233
maxi score, test score, baseline:  -0.9968325153374233 -1.0 -0.9968325153374233
probs:  [0.053475388342920614, 0.12930078655146082, 0.37341830715218904, 0.1597460633989645, 0.05578667327800751, 0.2282727812764575]
maxi score, test score, baseline:  -0.9968325153374233 -1.0 -0.9968325153374233
maxi score, test score, baseline:  -0.9968325153374233 -1.0 -0.9968325153374233
probs:  [0.053475388342920614, 0.12930078655146082, 0.37341830715218904, 0.1597460633989645, 0.05578667327800751, 0.2282727812764575]
maxi score, test score, baseline:  -0.9968325153374233 -1.0 -0.9968325153374233
probs:  [0.053475388342920614, 0.12930078655146082, 0.37341830715218904, 0.1597460633989645, 0.05578667327800751, 0.2282727812764575]
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.532]
 [0.532]
 [1.253]
 [0.532]
 [0.532]
 [0.532]] [[37.821]
 [37.821]
 [37.821]
 [ 0.035]
 [37.821]
 [37.821]
 [37.821]] [[1.516]
 [1.516]
 [1.516]
 [1.253]
 [1.516]
 [1.516]
 [1.516]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.03999999999999937  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9968325153374233 -1.0 -0.9968325153374233
probs:  [0.05517855791693594, 0.1266835129731114, 0.38536952894473575, 0.15539405329452546, 0.05735814851995356, 0.22001619835073788]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.412316277390666
maxi score, test score, baseline:  -0.9968325153374233 -1.0 -0.9968325153374233
probs:  [0.05517855791693594, 0.1266835129731114, 0.38536952894473575, 0.15539405329452546, 0.05735814851995356, 0.22001619835073788]
Printing some Q and Qe and total Qs values:  [[0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.208]] [[36.893]
 [36.893]
 [36.893]
 [36.893]
 [36.893]
 [36.893]
 [25.884]] [[3.911]
 [3.911]
 [3.911]
 [3.911]
 [3.911]
 [3.911]
 [2.208]]
printing an ep nov before normalisation:  51.85318134703539
printing an ep nov before normalisation:  45.24809068335405
printing an ep nov before normalisation:  60.35324925948104
Printing some Q and Qe and total Qs values:  [[0.287]
 [0.341]
 [0.287]
 [0.308]
 [0.287]
 [0.287]
 [0.287]] [[61.187]
 [33.893]
 [61.187]
 [60.528]
 [61.187]
 [61.187]
 [61.187]] [[0.954]
 [0.588]
 [0.954]
 [0.965]
 [0.954]
 [0.954]
 [0.954]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9968325153374233 -1.0 -0.9968325153374233
probs:  [0.054996682647788675, 0.12751604463855873, 0.38409738163489104, 0.1554366049919974, 0.05788692083099993, 0.22006636525576415]
printing an ep nov before normalisation:  46.94037437438965
maxi score, test score, baseline:  -0.9968325153374233 -1.0 -0.9968325153374233
probs:  [0.054996682647788675, 0.12751604463855873, 0.38409738163489104, 0.1554366049919974, 0.05788692083099993, 0.22006636525576415]
maxi score, test score, baseline:  -0.9968325153374233 -1.0 -0.9968325153374233
probs:  [0.055061517041701535, 0.12766657813408386, 0.3845511278100018, 0.15562013335776287, 0.05795517073780407, 0.2191454729186459]
maxi score, test score, baseline:  -0.9968325153374233 -1.0 -0.9968325153374233
probs:  [0.055061517041701535, 0.12766657813408386, 0.3845511278100018, 0.15562013335776287, 0.05795517073780407, 0.2191454729186459]
maxi score, test score, baseline:  -0.9968325153374233 -1.0 -0.9968325153374233
probs:  [0.055061517041701535, 0.12766657813408386, 0.3845511278100018, 0.15562013335776287, 0.05795517073780407, 0.2191454729186459]
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.503877639770508
printing an ep nov before normalisation:  26.65238380432129
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  50 total reward:  0.1899999999999995  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8331218
printing an ep nov before normalisation:  66.8464000833441
maxi score, test score, baseline:  -0.9968325153374233 -1.0 -0.9968325153374233
maxi score, test score, baseline:  -0.9968325153374233 -1.0 -0.9968325153374233
maxi score, test score, baseline:  -0.9968325153374233 -1.0 -0.9968325153374233
probs:  [0.05138484930645556, 0.186012304482798, 0.3588198172919603, 0.14521249660527985, 0.05408481404691593, 0.20448571826659034]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.042]
 [0.003]
 [0.042]
 [0.042]
 [0.042]
 [0.042]
 [0.042]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.042]
 [0.003]
 [0.042]
 [0.042]
 [0.042]
 [0.042]
 [0.042]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
Printing some Q and Qe and total Qs values:  [[0.461]
 [0.461]
 [0.461]
 [0.461]
 [0.461]
 [0.461]
 [0.461]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.461]
 [0.461]
 [0.461]
 [0.461]
 [0.461]
 [0.461]
 [0.461]]
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.555]
 [0.456]
 [0.464]
 [0.456]
 [0.456]
 [0.456]] [[49.906]
 [70.252]
 [49.906]
 [73.05 ]
 [49.906]
 [49.906]
 [49.906]] [[0.456]
 [0.555]
 [0.456]
 [0.464]
 [0.456]
 [0.456]
 [0.456]]
printing an ep nov before normalisation:  41.03162869066581
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]] [[50.402]
 [50.402]
 [50.402]
 [50.402]
 [50.402]
 [50.402]
 [50.402]] [[1.27]
 [1.27]
 [1.27]
 [1.27]
 [1.27]
 [1.27]
 [1.27]]
printing an ep nov before normalisation:  70.46383033822539
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.468]
 [0.554]
 [0.579]
 [0.582]
 [0.602]
 [0.579]] [[71.498]
 [76.829]
 [72.718]
 [74.331]
 [75.305]
 [71.643]
 [62.314]] [[0.524]
 [0.468]
 [0.554]
 [0.579]
 [0.582]
 [0.602]
 [0.579]]
printing an ep nov before normalisation:  47.183142209230766
printing an ep nov before normalisation:  81.02677900226132
printing an ep nov before normalisation:  71.1073145771209
Printing some Q and Qe and total Qs values:  [[0.62 ]
 [0.621]
 [0.631]
 [0.629]
 [0.626]
 [0.624]
 [0.603]] [[64.155]
 [80.502]
 [69.047]
 [71.289]
 [68.728]
 [68.653]
 [62.465]] [[0.62 ]
 [0.621]
 [0.631]
 [0.629]
 [0.626]
 [0.624]
 [0.603]]
maxi score, test score, baseline:  -0.9968325153374233 -1.0 -0.9968325153374233
probs:  [0.05145557579415086, 0.186268814884785, 0.3593147995279922, 0.14541270387058566, 0.05278038421373692, 0.2047677217087495]
printing an ep nov before normalisation:  66.38225362531188
printing an ep nov before normalisation:  76.82625422547243
printing an ep nov before normalisation:  0.39972209157213
maxi score, test score, baseline:  -0.9968418960244648 -1.0 -0.9968418960244648
probs:  [0.05145557231873998, 0.18626897663843833, 0.3593147734678073, 0.14541265503955864, 0.05278038806748303, 0.20476763446797278]
maxi score, test score, baseline:  -0.996896996996997 -1.0 -0.996896996996997
probs:  [0.051455551905898504, 0.1862699267182352, 0.35931462040291096, 0.14541236822070466, 0.05278041070365154, 0.20476712204859926]
printing an ep nov before normalisation:  11.53386116027832
Printing some Q and Qe and total Qs values:  [[0.742]
 [0.742]
 [0.742]
 [0.742]
 [0.742]
 [0.742]
 [0.742]] [[48.503]
 [48.503]
 [48.503]
 [48.503]
 [48.503]
 [48.503]
 [48.503]] [[2.015]
 [2.015]
 [2.015]
 [2.015]
 [2.015]
 [2.015]
 [2.015]]
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.668]
 [0.474]
 [0.564]
 [0.474]
 [0.474]
 [0.474]] [[38.059]
 [35.284]
 [35.073]
 [36.214]
 [35.073]
 [35.073]
 [35.073]] [[0.553]
 [0.668]
 [0.474]
 [0.564]
 [0.474]
 [0.474]
 [0.474]]
printing an ep nov before normalisation:  20.58829009141747
Printing some Q and Qe and total Qs values:  [[0.622]
 [0.461]
 [0.622]
 [0.487]
 [0.484]
 [0.482]
 [0.622]] [[29.67 ]
 [59.354]
 [29.67 ]
 [56.557]
 [54.993]
 [57.621]
 [29.67 ]] [[0.622]
 [0.461]
 [0.622]
 [0.487]
 [0.484]
 [0.482]
 [0.622]]
printing an ep nov before normalisation:  19.867529789311682
printing an ep nov before normalisation:  39.04576073791276
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9969501474926253 -1.0 -0.9969501474926253
probs:  [0.051489917796439855, 0.18639555397664884, 0.3595551216451818, 0.14550942737196254, 0.05214624848663223, 0.2049037307231348]
printing an ep nov before normalisation:  18.7187922000885
maxi score, test score, baseline:  -0.9969588235294118 -1.0 -0.9969588235294118
probs:  [0.05148991475669654, 0.18639570429511068, 0.3595550987636701, 0.14550938268183017, 0.05214624880670003, 0.2049036506959924]
printing an ep nov before normalisation:  41.74883635419842
printing an ep nov before normalisation:  15.831434296621824
line 256 mcts: sample exp_bonus 37.94285317191853
printing an ep nov before normalisation:  15.887000452241267
printing an ep nov before normalisation:  18.373302011440515
maxi score, test score, baseline:  -0.9969588235294118 -1.0 -0.9969588235294118
probs:  [0.05156388544710149, 0.18522480329989782, 0.36007278565221873, 0.14571877263745883, 0.052221164836158206, 0.205198588127165]
Printing some Q and Qe and total Qs values:  [[0.641]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]] [[14.534]
 [13.963]
 [13.963]
 [13.963]
 [13.963]
 [13.963]
 [13.963]] [[0.641]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]]
actor:  0 policy actor:  1  step number:  44 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  25.352462970719188
printing an ep nov before normalisation:  13.951226618142906
printing an ep nov before normalisation:  16.275718966912642
printing an ep nov before normalisation:  43.0900465513404
maxi score, test score, baseline:  -0.9928325513196482 -1.0 -0.9928325513196482
probs:  [0.05162162616167704, 0.18535717986343117, 0.3604776546897965, 0.14589944874561211, 0.05227801926413402, 0.20436607127534936]
printing an ep nov before normalisation:  15.988462861752994
maxi score, test score, baseline:  -0.9928325513196482 -1.0 -0.9928325513196482
probs:  [0.05162162616167704, 0.18535717986343117, 0.3604776546897965, 0.14589944874561211, 0.05227801926413402, 0.20436607127534936]
printing an ep nov before normalisation:  33.69485378265381
printing an ep nov before normalisation:  18.524320125579834
printing an ep nov before normalisation:  30.95164344462121
using explorer policy with actor:  0
printing an ep nov before normalisation:  32.76817981446703
printing an ep nov before normalisation:  25.775555158505085
maxi score, test score, baseline:  -0.9928532163742692 -1.0 -0.9928532163742692
probs:  [0.051621619439546025, 0.18535753447858558, 0.3604776037883262, 0.14589934331884502, 0.05227802060902544, 0.20436587836567183]
printing an ep nov before normalisation:  12.329406048678692
maxi score, test score, baseline:  -0.9928737609329447 -1.0 -0.9928737609329447
probs:  [0.05162161275689753, 0.18535788701632336, 0.3604775531857481, 0.14589923850876582, 0.052278021946178706, 0.2043656865860864]
siam score:  -0.8244429
maxi score, test score, baseline:  -0.9929747126436783 -0.9295 -0.9295
probs:  [0.051742428532958476, 0.1845753644732411, 0.36133536006369443, 0.14567295186152524, 0.05237441314685352, 0.20429948192172723]
Printing some Q and Qe and total Qs values:  [[0.111]
 [0.098]
 [0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]] [[21.143]
 [25.351]
 [21.143]
 [21.143]
 [21.143]
 [21.143]
 [21.143]] [[1.166]
 [1.411]
 [1.166]
 [1.166]
 [1.166]
 [1.166]
 [1.166]]
siam score:  -0.82747436
printing an ep nov before normalisation:  27.504007005822245
actions average: 
K:  1  action  0 :  tensor([0.1894, 0.0227, 0.1514, 0.1750, 0.1635, 0.1337, 0.1642],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0080, 0.9368, 0.0094, 0.0237, 0.0062, 0.0059, 0.0100],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1590, 0.0063, 0.2039, 0.1830, 0.1464, 0.1369, 0.1646],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1749, 0.0111, 0.1571, 0.1925, 0.1629, 0.1456, 0.1559],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1833, 0.0084, 0.1245, 0.1730, 0.2574, 0.1080, 0.1454],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1387, 0.0122, 0.1358, 0.2023, 0.1349, 0.2152, 0.1610],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1481, 0.0145, 0.1377, 0.2025, 0.1390, 0.1282, 0.2299],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9929945558739256 -0.9295 -0.9295
probs:  [0.0516807368367676, 0.1842344749997888, 0.36090059092503124, 0.14640849304993503, 0.05231808540370904, 0.20445761878476845]
maxi score, test score, baseline:  -0.9929945558739256 -0.9295 -0.9295
maxi score, test score, baseline:  -0.9929945558739256 -0.9295 -0.9295
probs:  [0.05163438087538014, 0.1844264943434555, 0.36057977537865704, 0.1458753780059421, 0.052893037981622745, 0.20459093341494253]
maxi score, test score, baseline:  -0.9929945558739256 -0.9295 -0.9295
probs:  [0.05163438087538014, 0.1844264943434555, 0.36057977537865704, 0.1458753780059421, 0.052893037981622745, 0.20459093341494253]
siam score:  -0.83242965
printing an ep nov before normalisation:  62.614635464767844
maxi score, test score, baseline:  -0.9930142857142858 -0.9295 -0.9295
actor:  1 policy actor:  1  step number:  35 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  3  action  0 :  tensor([0.2296, 0.0196, 0.1272, 0.1498, 0.1835, 0.1312, 0.1592],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0389, 0.8653, 0.0161, 0.0162, 0.0224, 0.0094, 0.0317],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1958, 0.0026, 0.1398, 0.1722, 0.1716, 0.1500, 0.1680],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1469, 0.0714, 0.1189, 0.2529, 0.1511, 0.1202, 0.1385],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1776, 0.0129, 0.1308, 0.1570, 0.2356, 0.1352, 0.1508],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1283, 0.0729, 0.1338, 0.1604, 0.1520, 0.2188, 0.1338],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1755, 0.0050, 0.1336, 0.1530, 0.1544, 0.1443, 0.2341],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  66.68897967147012
Printing some Q and Qe and total Qs values:  [[0.252]
 [0.342]
 [0.225]
 [0.255]
 [0.27 ]
 [0.225]
 [0.225]] [[82.538]
 [81.159]
 [68.373]
 [77.882]
 [75.875]
 [68.373]
 [68.373]] [[0.861]
 [0.939]
 [0.709]
 [0.823]
 [0.821]
 [0.709]
 [0.709]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9930142857142858 -0.9295 -0.9295
probs:  [0.047494032099306145, 0.24993960365668483, 0.3316022805662197, 0.13415868256800056, 0.04865150158173673, 0.18815389952805198]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.005]
 [ 0.043]
 [ 0.005]
 [ 0.031]
 [-0.004]
 [ 0.026]
 [-0.004]] [[51.911]
 [74.459]
 [59.548]
 [67.241]
 [50.246]
 [65.526]
 [51.091]] [[0.291]
 [0.576]
 [0.382]
 [0.488]
 [0.275]
 [0.465]
 [0.283]]
maxi score, test score, baseline:  -0.9930142857142858 -0.9295 -0.9295
probs:  [0.04748289135908504, 0.24902013901354678, 0.3315229454093572, 0.134543295865203, 0.04864564642471926, 0.18878508192808857]
actions average: 
K:  3  action  0 :  tensor([0.1654, 0.0586, 0.1500, 0.1483, 0.1566, 0.1423, 0.1789],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0243, 0.7539, 0.0207, 0.0862, 0.0198, 0.0290, 0.0660],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1421, 0.0736, 0.2082, 0.1436, 0.1288, 0.1428, 0.1607],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1368, 0.0082, 0.1450, 0.2509, 0.1487, 0.1429, 0.1675],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1324, 0.0013, 0.1503, 0.1386, 0.2492, 0.1484, 0.1799],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1320, 0.0475, 0.1530, 0.1455, 0.1381, 0.2289, 0.1550],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1640, 0.0057, 0.1485, 0.1702, 0.1624, 0.1580, 0.1912],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  50.298771950663415
maxi score, test score, baseline:  -0.9930142857142858 -0.9295 -0.9295
probs:  [0.04738535892975987, 0.24947038686919937, 0.3308389750393559, 0.1346823944990965, 0.048551274375465886, 0.18907161028712238]
maxi score, test score, baseline:  -0.9930142857142858 -0.9295 -0.9295
probs:  [0.04738535892975987, 0.24947038686919937, 0.3308389750393559, 0.1346823944990965, 0.048551274375465886, 0.18907161028712238]
maxi score, test score, baseline:  -0.9930142857142858 -0.9295 -0.9295
probs:  [0.047288069112995046, 0.24991951473034546, 0.3301567060501549, 0.13482114712422202, 0.04845713707730661, 0.18935742590497587]
printing an ep nov before normalisation:  64.1577717869433
maxi score, test score, baseline:  -0.9930142857142858 -0.9295 -0.9295
probs:  [0.047288069112995046, 0.24991951473034546, 0.3301567060501549, 0.13482114712422202, 0.04845713707730661, 0.18935742590497587]
maxi score, test score, baseline:  -0.9930142857142858 -0.9295 -0.9295
maxi score, test score, baseline:  -0.9930142857142858 -0.9295 -0.9295
maxi score, test score, baseline:  -0.9930142857142858 -0.9295 -0.9295
probs:  [0.047288069112995046, 0.24991951473034546, 0.3301567060501549, 0.13482114712422202, 0.04845713707730661, 0.18935742590497587]
maxi score, test score, baseline:  -0.9930142857142858 -0.9295 -0.9295
probs:  [0.047288069112995046, 0.24991951473034546, 0.3301567060501549, 0.13482114712422202, 0.04845713707730661, 0.18935742590497587]
printing an ep nov before normalisation:  21.479060396553233
maxi score, test score, baseline:  -0.9930142857142858 -0.9295 -0.9295
maxi score, test score, baseline:  -0.9930339031339033 -0.9295 -0.9295
probs:  [0.047324311932386565, 0.2501114983924339, 0.33041035934971597, 0.13415668984323528, 0.04849427843260591, 0.1895028620496224]
maxi score, test score, baseline:  -0.9930339031339033 -0.9295 -0.9295
probs:  [0.047438373671833206, 0.248892760011353, 0.3312086453840273, 0.13448062707138764, 0.048019021845628555, 0.18996057201577035]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9930339031339033 -0.9295 -0.9295
probs:  [0.047438373671833206, 0.248892760011353, 0.3312086453840273, 0.13448062707138764, 0.048019021845628555, 0.18996057201577035]
actor:  1 policy actor:  1  step number:  39 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9930339031339033 -0.9295 -0.9295
probs:  [0.050261558511251234, 0.23542583267125505, 0.3510053717504353, 0.13098574442040029, 0.05080005963339134, 0.18152143301326687]
maxi score, test score, baseline:  -0.9930339031339033 -0.9295 -0.9295
maxi score, test score, baseline:  -0.9930339031339033 -0.9295 -0.9295
probs:  [0.05016927675607198, 0.23584970362021573, 0.35035821281490276, 0.13111848451155458, 0.05070927897129086, 0.18179504332596425]
using explorer policy with actor:  1
printing an ep nov before normalisation:  25.979902552571392
printing an ep nov before normalisation:  41.82394981384277
maxi score, test score, baseline:  -0.9930534090909092 -0.9295 -0.9295
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.19508820644849
maxi score, test score, baseline:  -0.9930534090909092 -0.9295 -0.9295
probs:  [0.05007720324849719, 0.23627261804088373, 0.3497125142904532, 0.1312509250548086, 0.05061870316935343, 0.1820680361960039]
printing an ep nov before normalisation:  32.45209775183515
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.52 ]
 [0.482]
 [0.464]
 [0.464]
 [0.472]
 [0.449]] [[25.185]
 [40.493]
 [19.798]
 [18.696]
 [18.356]
 [19.743]
 [19.169]] [[0.474]
 [0.52 ]
 [0.482]
 [0.464]
 [0.464]
 [0.472]
 [0.449]]
printing an ep nov before normalisation:  64.13107666020453
maxi score, test score, baseline:  -0.993072804532578 -0.9295 -0.9295
probs:  [0.050241928993753675, 0.2337558814571508, 0.35086550035014297, 0.13168337881769113, 0.05078521489530821, 0.18266809548595314]
maxi score, test score, baseline:  -0.993072804532578 -0.9295 -0.9295
probs:  [0.05015021528005574, 0.23417226996705345, 0.3502223188839604, 0.13181715521628895, 0.050695005398433704, 0.18294303525420785]
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]]
maxi score, test score, baseline:  -0.993072804532578 -0.9295 -0.9295
probs:  [0.05015021528005574, 0.23417226996705345, 0.3502223188839604, 0.13181715521628895, 0.050695005398433704, 0.18294303525420785]
printing an ep nov before normalisation:  3.812874979573735
maxi score, test score, baseline:  -0.993072804532578 -0.9295 -0.9295
probs:  [0.05026730296467339, 0.2330975002135372, 0.35104186285119837, 0.1314089801631816, 0.05081336841400271, 0.18337098539340685]
printing an ep nov before normalisation:  52.2076307554185
printing an ep nov before normalisation:  55.86720341465612
maxi score, test score, baseline:  -0.993072804532578 -0.9295 -0.9295
probs:  [0.05026730296467339, 0.2330975002135372, 0.35104186285119837, 0.1314089801631816, 0.05081336841400271, 0.18337098539340685]
actor:  1 policy actor:  1  step number:  45 total reward:  0.05999999999999939  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.993072804532578 -0.9295 -0.9295
probs:  [0.051812831287905724, 0.22651094396053523, 0.36188001301169653, 0.12934541659916077, 0.05233460835115372, 0.17811618678954805]
maxi score, test score, baseline:  -0.993072804532578 -0.9295 -0.9295
probs:  [0.05189306934165294, 0.226862419317277, 0.3624416631701955, 0.12886749926739577, 0.05241565651899567, 0.1775196923844831]
maxi score, test score, baseline:  -0.993072804532578 -0.9295 -0.9295
probs:  [0.05189306934165294, 0.226862419317277, 0.3624416631701955, 0.12886749926739577, 0.05241565651899567, 0.1775196923844831]
printing an ep nov before normalisation:  31.859173519886635
printing an ep nov before normalisation:  43.85399008195746
actions average: 
K:  0  action  0 :  tensor([0.2516, 0.0032, 0.1385, 0.1702, 0.1756, 0.1258, 0.1351],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0074, 0.9268, 0.0092, 0.0362, 0.0062, 0.0051, 0.0090],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1523, 0.0048, 0.2181, 0.1863, 0.1629, 0.1391, 0.1365],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1467, 0.0106, 0.1551, 0.2442, 0.1604, 0.1354, 0.1476],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1580, 0.0031, 0.1529, 0.1769, 0.2379, 0.1359, 0.1352],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1445, 0.0069, 0.1761, 0.1574, 0.1468, 0.2129, 0.1553],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1767, 0.0084, 0.1497, 0.1803, 0.1567, 0.1311, 0.1972],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  28.66466769063947
printing an ep nov before normalisation:  0.6147592618793851
actor:  1 policy actor:  1  step number:  48 total reward:  0.20999999999999952  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.993072804532578 -0.9295 -0.9295
probs:  [0.049064823751755145, 0.26733560898697944, 0.3426423001427152, 0.12249192825657916, 0.04956332777862022, 0.1689020110833508]
using explorer policy with actor:  1
printing an ep nov before normalisation:  16.576840591410228
Printing some Q and Qe and total Qs values:  [[0.176]
 [0.205]
 [0.185]
 [0.193]
 [0.192]
 [0.167]
 [0.178]] [[36.582]
 [41.93 ]
 [36.782]
 [37.867]
 [37.926]
 [36.841]
 [37.507]] [[0.328]
 [0.401]
 [0.339]
 [0.355]
 [0.355]
 [0.321]
 [0.337]]
printing an ep nov before normalisation:  61.98422284709537
printing an ep nov before normalisation:  54.3214750289917
printing an ep nov before normalisation:  0.0016103040184134443
Printing some Q and Qe and total Qs values:  [[0.45 ]
 [0.416]
 [0.397]
 [0.388]
 [0.388]
 [0.407]
 [0.411]] [[86.184]
 [82.83 ]
 [85.981]
 [86.759]
 [87.138]
 [87.005]
 [87.114]] [[1.957]
 [1.812]
 [1.898]
 [1.913]
 [1.926]
 [1.941]
 [1.948]]
printing an ep nov before normalisation:  41.80971581366584
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  62.35729941420122
maxi score, test score, baseline:  -0.9930920903954803 -0.9295 -0.9295
probs:  [0.04906712000250745, 0.267475406429462, 0.34266112469865223, 0.12286440300684026, 0.04955525201512777, 0.16837669384741039]
printing an ep nov before normalisation:  53.299647099661
Printing some Q and Qe and total Qs values:  [[-0.073]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.055]
 [-0.074]
 [-0.047]] [[44.497]
 [44.497]
 [44.497]
 [44.497]
 [56.177]
 [49.938]
 [52.017]] [[0.265]
 [0.265]
 [0.265]
 [0.265]
 [0.487]
 [0.359]
 [0.422]]
maxi score, test score, baseline:  -0.9930920903954803 -0.9295 -0.9295
probs:  [0.049179002968961036, 0.2662913256583745, 0.3434442840748979, 0.12314501685469871, 0.049179002968961036, 0.1687613674741069]
maxi score, test score, baseline:  -0.9930920903954803 -0.9295 -0.9295
probs:  [0.049219147440098414, 0.266509178793622, 0.3437252876983467, 0.12324570326723473, 0.049219147440098414, 0.16808153536059975]
maxi score, test score, baseline:  -0.9930920903954803 -0.9295 -0.9295
probs:  [0.04925897258319442, 0.26672529902236924, 0.3440040560865526, 0.12334558877259731, 0.04925897258319442, 0.16740711095209207]
printing an ep nov before normalisation:  45.28780971879222
maxi score, test score, baseline:  -0.9930920903954803 -0.9295 -0.9295
probs:  [0.04941777005801087, 0.2658068699240871, 0.34511560805258146, 0.12309972477333297, 0.04941777005801087, 0.16714225713397665]
maxi score, test score, baseline:  -0.9930920903954803 -0.9295 -0.9295
maxi score, test score, baseline:  -0.9930920903954803 -0.9295 -0.9295
probs:  [0.04963020641111554, 0.26344643660016964, 0.34660262191235763, 0.12362976020921979, 0.04963020641111554, 0.1670607684560219]
printing an ep nov before normalisation:  17.393606820128014
maxi score, test score, baseline:  -0.993111267605634 -0.9295 -0.9295
printing an ep nov before normalisation:  44.702801214788494
printing an ep nov before normalisation:  38.99033895177652
printing an ep nov before normalisation:  40.572586353240396
line 256 mcts: sample exp_bonus 44.99783189655176
printing an ep nov before normalisation:  65.31343794532577
maxi score, test score, baseline:  -0.993111267605634 -0.9295 -0.9295
probs:  [0.049531461878521164, 0.2623429131393659, 0.3459081060367201, 0.12437875226865212, 0.049531461878521164, 0.1683073047982195]
maxi score, test score, baseline:  -0.993111267605634 -0.9295 -0.9295
probs:  [0.049531461878521164, 0.2623429131393659, 0.3459081060367201, 0.12437875226865212, 0.049531461878521164, 0.1683073047982195]
maxi score, test score, baseline:  -0.993111267605634 -0.9295 -0.9295
probs:  [0.049531461878521164, 0.2623429131393659, 0.3459081060367201, 0.12437875226865212, 0.049531461878521164, 0.1683073047982195]
maxi score, test score, baseline:  -0.993111267605634 -0.9295 -0.9295
probs:  [0.049531461878521164, 0.2623429131393659, 0.3459081060367201, 0.12437875226865212, 0.049531461878521164, 0.1683073047982195]
printing an ep nov before normalisation:  36.82256519312709
maxi score, test score, baseline:  -0.993111267605634 -0.9295 -0.9295
probs:  [0.049531461878521164, 0.2623429131393659, 0.3459081060367201, 0.12437875226865212, 0.049531461878521164, 0.1683073047982195]
printing an ep nov before normalisation:  0.00123169577364024
maxi score, test score, baseline:  -0.993111267605634 -0.9295 -0.9295
probs:  [0.04995611894818858, 0.2621439575022953, 0.34546427487334186, 0.12458408034509263, 0.04946766162873858, 0.168383906702343]
maxi score, test score, baseline:  -0.993111267605634 -0.9295 -0.9295
probs:  [0.049868407927923235, 0.26261329269954836, 0.3448402463886399, 0.12469228641366953, 0.049378668285678134, 0.16860709828454082]
maxi score, test score, baseline:  -0.993111267605634 -0.9295 -0.9295
probs:  [0.049868407927923235, 0.26261329269954836, 0.3448402463886399, 0.12469228641366953, 0.049378668285678134, 0.16860709828454082]
printing an ep nov before normalisation:  72.40996370988032
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.44437815804535
maxi score, test score, baseline:  -0.993111267605634 -0.9295 -0.9295
probs:  [0.049868407927923235, 0.26261329269954836, 0.3448402463886399, 0.12469228641366953, 0.049378668285678134, 0.16860709828454082]
maxi score, test score, baseline:  -0.993111267605634 -0.9295 -0.9295
probs:  [0.04990839182071147, 0.2628243098219905, 0.34511736870307835, 0.1247924239012499, 0.049418258459234095, 0.16793924729373572]
maxi score, test score, baseline:  -0.993111267605634 -0.9295 -0.9295
probs:  [0.04990839182071147, 0.2628243098219905, 0.34511736870307835, 0.1247924239012499, 0.049418258459234095, 0.16793924729373572]
printing an ep nov before normalisation:  57.317209243774414
printing an ep nov before normalisation:  45.055623054504395
maxi score, test score, baseline:  -0.993111267605634 -0.9295 -0.9295
probs:  [0.04990839182071147, 0.2628243098219905, 0.34511736870307835, 0.1247924239012499, 0.049418258459234095, 0.16793924729373572]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[29.921]
 [29.921]
 [29.921]
 [29.921]
 [29.921]
 [29.921]
 [29.921]] [[1.927]
 [1.927]
 [1.927]
 [1.927]
 [1.927]
 [1.927]
 [1.927]]
maxi score, test score, baseline:  -0.993111267605634 -0.9295 -0.9295
probs:  [0.049860623541310156, 0.263503781462005, 0.34477018878574534, 0.12500043103764188, 0.04936881607040193, 0.1674961591028957]
printing an ep nov before normalisation:  89.9701844590459
maxi score, test score, baseline:  -0.993111267605634 -0.9295 -0.9295
probs:  [0.04977338472421316, 0.2639728000257613, 0.34414948162736814, 0.12510883186996086, 0.04928029674641524, 0.1677152050062813]
maxi score, test score, baseline:  -0.993111267605634 -0.9295 -0.9295
probs:  [0.04977338472421316, 0.2639728000257613, 0.34414948162736814, 0.12510883186996086, 0.04928029674641524, 0.1677152050062813]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.993111267605634 -0.9295 -0.9295
probs:  [0.04977338472421316, 0.2639728000257613, 0.34414948162736814, 0.12510883186996086, 0.04928029674641524, 0.1677152050062813]
line 256 mcts: sample exp_bonus 34.207359597672294
Printing some Q and Qe and total Qs values:  [[0.392]
 [0.437]
 [0.379]
 [0.379]
 [0.402]
 [0.379]
 [0.379]] [[14.856]
 [21.659]
 [13.511]
 [13.511]
 [14.277]
 [13.511]
 [13.511]] [[0.392]
 [0.437]
 [0.379]
 [0.379]
 [0.402]
 [0.379]
 [0.379]]
siam score:  -0.8291315
Printing some Q and Qe and total Qs values:  [[0.348]
 [0.348]
 [0.348]
 [0.348]
 [0.348]
 [0.348]
 [0.348]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.348]
 [0.348]
 [0.348]
 [0.348]
 [0.348]
 [0.348]
 [0.348]]
maxi score, test score, baseline:  -0.9931303370786518 -0.9295 -0.9295
printing an ep nov before normalisation:  83.29524493754205
siam score:  -0.8353424
maxi score, test score, baseline:  -0.9931303370786518 -0.9295 -0.9295
probs:  [0.04942089156758937, 0.26472756480883497, 0.345133607514031, 0.1254663490699622, 0.04942089156758937, 0.16583069547199306]
printing an ep nov before normalisation:  54.13357793067355
Printing some Q and Qe and total Qs values:  [[-0.006]
 [-0.006]
 [ 0.032]
 [-0.006]
 [-0.01 ]
 [-0.024]
 [-0.006]] [[20.068]
 [20.068]
 [31.811]
 [20.068]
 [35.86 ]
 [37.092]
 [20.068]] [[0.353]
 [0.353]
 [0.821]
 [0.353]
 [0.928]
 [0.959]
 [0.353]]
siam score:  -0.83412284
maxi score, test score, baseline:  -0.9931303370786518 -0.9295 -0.9295
probs:  [0.04945907293745881, 0.264932536465019, 0.3454008668210451, 0.12556344010314494, 0.04945907293745881, 0.16518501073587338]
printing an ep nov before normalisation:  73.96223391737183
maxi score, test score, baseline:  -0.9931303370786518 -0.9295 -0.9295
probs:  [0.04949696240822196, 0.26513594109929, 0.3456660829129223, 0.1256597888689362, 0.04949696240822196, 0.1645442623024076]
maxi score, test score, baseline:  -0.9931303370786518 -0.9295 -0.9295
siam score:  -0.83123803
using another actor
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.294]
 [0.294]
 [0.294]
 [0.294]
 [0.294]
 [0.294]
 [0.294]] [[41.418]
 [41.418]
 [41.418]
 [41.418]
 [41.418]
 [41.418]
 [41.418]] [[2.294]
 [2.294]
 [2.294]
 [2.294]
 [2.294]
 [2.294]
 [2.294]]
maxi score, test score, baseline:  -0.9931303370786518 -0.9295 -0.9295
maxi score, test score, baseline:  -0.9931303370786518 -0.9295 -0.9295
probs:  [0.04931808843057746, 0.26528397396817327, 0.3444107209177215, 0.12620698264512828, 0.04931808843057746, 0.16546214560782196]
maxi score, test score, baseline:  -0.9931492997198881 -0.9295 -0.9295
probs:  [0.04931808843057746, 0.26528397396817327, 0.3444107209177215, 0.12620698264512828, 0.04931808843057746, 0.16546214560782196]
printing an ep nov before normalisation:  49.48220729827881
maxi score, test score, baseline:  -0.9931492997198881 -0.9295 -0.9295
probs:  [0.04935591051548513, 0.2654878691044465, 0.34467546279503347, 0.12630393061117473, 0.04935591051548513, 0.16482091645837504]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  -0.9931492997198881 -0.9295 -0.9295
probs:  [0.04939344618492101, 0.26569022020310795, 0.3449381998603454, 0.1264001444189245, 0.04939344618492101, 0.16418454314778]
maxi score, test score, baseline:  -0.9931492997198881 -0.9295 -0.9295
probs:  [0.04947786303655762, 0.26443298084172207, 0.3455290894380514, 0.12661652704527082, 0.04947786303655762, 0.16446567660184058]
actions average: 
K:  2  action  0 :  tensor([0.1829, 0.0014, 0.1475, 0.1758, 0.1918, 0.1528, 0.1479],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0099, 0.9040, 0.0021, 0.0340, 0.0036, 0.0017, 0.0447],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1356, 0.0126, 0.2458, 0.1586, 0.1259, 0.1754, 0.1461],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1594, 0.0363, 0.1532, 0.1908, 0.1609, 0.1460, 0.1535],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1500, 0.0014, 0.1533, 0.2300, 0.1682, 0.1590, 0.1381],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1507, 0.0032, 0.1457, 0.1740, 0.1454, 0.2397, 0.1413],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1467, 0.1068, 0.1552, 0.1524, 0.1345, 0.1488, 0.1557],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9931492997198881 -0.9295 -0.9295
probs:  [0.04947786303655762, 0.26443298084172207, 0.3455290894380514, 0.12661652704527082, 0.04947786303655762, 0.16446567660184058]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9931492997198881 -0.9295 -0.9295
probs:  [0.04959900285665944, 0.2633870321504579, 0.3463770274475315, 0.12692704031271165, 0.04959900285665944, 0.16411089437598006]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
using explorer policy with actor:  1
using another actor
from probs:  [0.04959900285665944, 0.2633870321504579, 0.3463770274475315, 0.12692704031271165, 0.04959900285665944, 0.16411089437598006]
using explorer policy with actor:  1
printing an ep nov before normalisation:  60.08345711128214
Printing some Q and Qe and total Qs values:  [[0.362]
 [0.456]
 [0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]] [[41.   ]
 [50.601]
 [41.   ]
 [41.   ]
 [41.   ]
 [41.   ]
 [41.   ]] [[0.884]
 [1.265]
 [0.884]
 [0.884]
 [0.884]
 [0.884]
 [0.884]]
printing an ep nov before normalisation:  42.36072695491877
printing an ep nov before normalisation:  31.709162344001037
printing an ep nov before normalisation:  71.49061027140417
printing an ep nov before normalisation:  60.71027135471591
from probs:  [0.04959900285665944, 0.2633870321504579, 0.3463770274475315, 0.12692704031271165, 0.04959900285665944, 0.16411089437598006]
printing an ep nov before normalisation:  48.31616148550053
using explorer policy with actor:  0
printing an ep nov before normalisation:  35.833432838830106
using explorer policy with actor:  1
printing an ep nov before normalisation:  52.08380727794414
Printing some Q and Qe and total Qs values:  [[0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]] [[0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]] [[0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]]
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.482]
 [0.396]
 [0.444]
 [0.396]
 [0.396]
 [0.396]] [[60.286]
 [60.765]
 [60.286]
 [63.666]
 [60.286]
 [60.286]
 [60.286]] [[1.91 ]
 [2.021]
 [1.91 ]
 [2.138]
 [1.91 ]
 [1.91 ]
 [1.91 ]]
actions average: 
K:  0  action  0 :  tensor([0.1894, 0.0102, 0.1627, 0.1617, 0.1770, 0.1428, 0.1563],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0024,     0.9857,     0.0021,     0.0035,     0.0009,     0.0014,
            0.0040], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1584, 0.0072, 0.2364, 0.1473, 0.1378, 0.1803, 0.1325],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1293, 0.0232, 0.1323, 0.3190, 0.1386, 0.1266, 0.1311],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1585, 0.0038, 0.1686, 0.1750, 0.1721, 0.1597, 0.1622],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1317, 0.0050, 0.1332, 0.1260, 0.1182, 0.3667, 0.1193],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1754, 0.0034, 0.1510, 0.1562, 0.1542, 0.1404, 0.2195],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9932055555555557 -0.9295 -0.9295
probs:  [0.04988318033022261, 0.26247617771168275, 0.34836506128023514, 0.12534902727426536, 0.04988318033022261, 0.1640433730733715]
printing an ep nov before normalisation:  41.99661542055596
printing an ep nov before normalisation:  2.767953999855308
maxi score, test score, baseline:  -0.9932055555555557 -0.9295 -0.9295
probs:  [0.0499152993278223, 0.26264554857549494, 0.3485898827454401, 0.12478477515087925, 0.0499152993278223, 0.16414919487254118]
printing an ep nov before normalisation:  83.2869718443916
maxi score, test score, baseline:  -0.9932055555555557 -0.9295 -0.9295
probs:  [0.0499152993278223, 0.26264554857549494, 0.3485898827454401, 0.12478477515087925, 0.0499152993278223, 0.16414919487254118]
printing an ep nov before normalisation:  75.862135887146
maxi score, test score, baseline:  -0.9932055555555557 -0.9295 -0.9295
probs:  [0.0499152993278223, 0.26264554857549494, 0.3485898827454401, 0.12478477515087925, 0.0499152993278223, 0.16414919487254118]
maxi score, test score, baseline:  -0.9932055555555557 -0.9295 -0.9295
probs:  [0.0499152993278223, 0.26264554857549494, 0.3485898827454401, 0.12478477515087925, 0.0499152993278223, 0.16414919487254118]
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.87145915404158
maxi score, test score, baseline:  -0.9932240997229917 -0.9295 -0.9295
probs:  [0.04995249172504061, 0.2628416726427097, 0.3488502161886353, 0.12487790285051176, 0.04995249172504061, 0.16352522486806217]
actor:  1 policy actor:  1  step number:  55 total reward:  0.03999999999999937  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9932240997229917 -0.9295 -0.9295
probs:  [0.04868351881393266, 0.2561500865357165, 0.3399678608693195, 0.12170046506897328, 0.04868351881393266, 0.18481454989812546]
maxi score, test score, baseline:  -0.9932240997229917 -0.9295 -0.9295
probs:  [0.048595831810847846, 0.25658692310332687, 0.33935299775011585, 0.12179738183020344, 0.048595831810847846, 0.1850710336946581]
using explorer policy with actor:  1
printing an ep nov before normalisation:  22.06411619526236
actions average: 
K:  3  action  0 :  tensor([0.2282, 0.0114, 0.1423, 0.1479, 0.1731, 0.1512, 0.1458],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0251, 0.8812, 0.0252, 0.0178, 0.0150, 0.0182, 0.0174],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1539, 0.0116, 0.2012, 0.1665, 0.1513, 0.1643, 0.1511],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1448, 0.0123, 0.1471, 0.2293, 0.1533, 0.1462, 0.1670],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1518, 0.0332, 0.1362, 0.1715, 0.1977, 0.1547, 0.1550],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1488, 0.0078, 0.1521, 0.1564, 0.1615, 0.2301, 0.1433],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1483, 0.0208, 0.1383, 0.1734, 0.1504, 0.1646, 0.2041],
       grad_fn=<DivBackward0>)
actions average: 
K:  0  action  0 :  tensor([0.3364, 0.0025, 0.1234, 0.1177, 0.1698, 0.1174, 0.1327],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0022, 0.9590, 0.0013, 0.0289, 0.0016, 0.0020, 0.0050],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1582, 0.0165, 0.1982, 0.1689, 0.1512, 0.1476, 0.1595],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1652, 0.0033, 0.1591, 0.1772, 0.1680, 0.1601, 0.1671],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1504, 0.0024, 0.1384, 0.1408, 0.2898, 0.1370, 0.1412],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1650, 0.0031, 0.1550, 0.1591, 0.1674, 0.1987, 0.1517],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1289, 0.1156, 0.1274, 0.1505, 0.1286, 0.1199, 0.2290],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9932425414364642 -0.9295 -0.9295
probs:  [0.0486741847845585, 0.2553860707835117, 0.339901438713346, 0.12199408500645216, 0.0486741847845585, 0.18537003592757303]
maxi score, test score, baseline:  -0.9932425414364642 -0.9295 -0.9295
probs:  [0.0486741847845585, 0.2553860707835117, 0.339901438713346, 0.12199408500645216, 0.0486741847845585, 0.18537003592757303]
using explorer policy with actor:  1
printing an ep nov before normalisation:  15.02028226852417
maxi score, test score, baseline:  -0.9932425414364642 -0.9295 -0.9295
probs:  [0.0486741847845585, 0.2553860707835117, 0.339901438713346, 0.12199408500645216, 0.0486741847845585, 0.18537003592757303]
printing an ep nov before normalisation:  40.7400913083936
maxi score, test score, baseline:  -0.9932425414364642 -0.9295 -0.9295
probs:  [0.04871411133162495, 0.2541453959127344, 0.34018367039601854, 0.12246010670283938, 0.04871411133162495, 0.1857826043251577]
printing an ep nov before normalisation:  46.3485050201416
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.312304003669524
maxi score, test score, baseline:  -0.9932425414364642 -0.9295 -0.9295
probs:  [0.04871411133162495, 0.2541453959127344, 0.34018367039601854, 0.12246010670283938, 0.04871411133162495, 0.1857826043251577]
maxi score, test score, baseline:  -0.9932425414364642 -0.9295 -0.9295
Printing some Q and Qe and total Qs values:  [[0.57 ]
 [0.531]
 [0.549]
 [0.559]
 [0.534]
 [0.53 ]
 [0.539]] [[41.425]
 [48.274]
 [43.583]
 [41.208]
 [44.385]
 [44.849]
 [45.312]] [[0.57 ]
 [0.531]
 [0.549]
 [0.559]
 [0.534]
 [0.53 ]
 [0.539]]
maxi score, test score, baseline:  -0.993279120879121 -0.9295 -0.9295
probs:  [0.04882128761184105, 0.2531276798112247, 0.3409338696143103, 0.12210370232623079, 0.04882128761184105, 0.18619217302455204]
using explorer policy with actor:  1
printing an ep nov before normalisation:  83.22350572568152
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.993279120879121 -0.9295 -0.9295
maxi score, test score, baseline:  -0.9933153005464482 -0.9295 -0.9295
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.596]
 [0.496]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]] [[20.8  ]
 [35.907]
 [20.8  ]
 [20.8  ]
 [20.8  ]
 [20.8  ]
 [20.8  ]] [[0.744]
 [1.179]
 [0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]]
maxi score, test score, baseline:  -0.9933153005464482 -0.9295 -0.9295
probs:  [0.04877486335561249, 0.25376857652452917, 0.3406078376720405, 0.1223038124039944, 0.04877486335561249, 0.18577004668821098]
printing an ep nov before normalisation:  35.13720841342917
using explorer policy with actor:  0
siam score:  -0.83611953
maxi score, test score, baseline:  -0.9933153005464482 -0.9295 -0.9295
probs:  [0.048764232127968106, 0.2530257135089458, 0.34053234441196695, 0.1225938578169016, 0.048764232127968106, 0.18631962000624955]
Printing some Q and Qe and total Qs values:  [[0.528]
 [0.616]
 [0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]] [[65.39 ]
 [77.835]
 [65.39 ]
 [65.39 ]
 [65.39 ]
 [65.39 ]
 [65.39 ]] [[0.917]
 [1.138]
 [0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]]
printing an ep nov before normalisation:  78.41701372158045
line 256 mcts: sample exp_bonus 72.72689925880549
printing an ep nov before normalisation:  66.08582224164691
maxi score, test score, baseline:  -0.9933332425068121 -0.9295 -0.9295
probs:  [0.04888060306397573, 0.25207569527654483, 0.34134689808265434, 0.12288689840183135, 0.04888060306397573, 0.1859293021110181]
printing an ep nov before normalisation:  53.36423554846169
printing an ep nov before normalisation:  66.69737545038957
printing an ep nov before normalisation:  76.00905101994297
maxi score, test score, baseline:  -0.9933332425068121 -0.9295 -0.9295
probs:  [0.04895567569127525, 0.2509248921275471, 0.3418723788103418, 0.12307594325590575, 0.04895567569127525, 0.186215434423655]
maxi score, test score, baseline:  -0.9933332425068121 -0.9295 -0.9295
probs:  [0.04895567569127525, 0.2509248921275471, 0.3418723788103418, 0.12307594325590575, 0.04895567569127525, 0.186215434423655]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 27.788261012432173
maxi score, test score, baseline:  -0.9933510869565219 -0.9295 -0.9295
probs:  [0.049418895709625314, 0.25096862633525324, 0.3417272170028516, 0.12338521703891799, 0.04893454578200999, 0.18556549813134185]
maxi score, test score, baseline:  -0.9933510869565219 -0.9295 -0.9295
probs:  [0.049418895709625314, 0.25096862633525324, 0.3417272170028516, 0.12338521703891799, 0.04893454578200999, 0.18556549813134185]
siam score:  -0.8352224
actions average: 
K:  3  action  0 :  tensor([0.3076, 0.0429, 0.1090, 0.1276, 0.1675, 0.1073, 0.1381],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0357, 0.8147, 0.0312, 0.0435, 0.0239, 0.0208, 0.0301],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1457, 0.0022, 0.2323, 0.1731, 0.1539, 0.1424, 0.1504],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1225, 0.0920, 0.1109, 0.2907, 0.1355, 0.1199, 0.1287],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1597, 0.0211, 0.1440, 0.1635, 0.2358, 0.1305, 0.1452],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1275, 0.0539, 0.1404, 0.1802, 0.1418, 0.2146, 0.1416],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1569, 0.0818, 0.1325, 0.1552, 0.1695, 0.1383, 0.1657],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  70.46712575501127
maxi score, test score, baseline:  -0.9933510869565219 -0.9295 -0.9295
probs:  [0.049524828877300676, 0.24998728690279068, 0.3424614290813608, 0.12302295643331024, 0.049039437902192624, 0.1859640608030451]
printing an ep nov before normalisation:  68.85501428189932
maxi score, test score, baseline:  -0.9933510869565219 -0.9295 -0.9295
probs:  [0.049524828877300676, 0.24998728690279068, 0.3424614290813608, 0.12302295643331024, 0.049039437902192624, 0.1859640608030451]
maxi score, test score, baseline:  -0.9933510869565219 -0.9295 -0.9295
printing an ep nov before normalisation:  66.17775436785013
maxi score, test score, baseline:  -0.9933510869565219 -0.9295 -0.9295
probs:  [0.04935476381827205, 0.25082460428545195, 0.3412518145755496, 0.12322224094689634, 0.04886693361164656, 0.18647964276218368]
maxi score, test score, baseline:  -0.9933510869565219 -0.9295 -0.9295
probs:  [0.04942925675306362, 0.2496917942399924, 0.3417680707965752, 0.12340852651322526, 0.048940688253691966, 0.18676166344345144]
printing an ep nov before normalisation:  55.652456355955394
maxi score, test score, baseline:  -0.9933510869565219 -0.9295 -0.9295
maxi score, test score, baseline:  -0.9933510869565219 -0.9295 -0.9295
probs:  [0.04942925675306362, 0.2496917942399924, 0.3417680707965752, 0.12340852651322526, 0.048940688253691966, 0.18676166344345144]
actor:  1 policy actor:  1  step number:  40 total reward:  0.1899999999999995  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.508]
 [0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]] [[77.971]
 [66.657]
 [77.971]
 [77.971]
 [77.971]
 [77.971]
 [77.971]] [[1.74 ]
 [1.569]
 [1.74 ]
 [1.74 ]
 [1.74 ]
 [1.74 ]
 [1.74 ]]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.9933510869565219 -0.9295 -0.9295
probs:  [0.04756974994399797, 0.24027813850556382, 0.32888118479986284, 0.1564495063531501, 0.04709961084860176, 0.1797218095488236]
printing an ep nov before normalisation:  29.856864634254222
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  2.4535284584840156
siam score:  -0.84085464
maxi score, test score, baseline:  -0.9933510869565219 -0.9295 -0.9295
probs:  [0.04712001679010239, 0.23914666954194208, 0.3290219521588728, 0.15665955101958948, 0.04712001679010239, 0.18093179369939086]
printing an ep nov before normalisation:  37.48268323508313
maxi score, test score, baseline:  -0.9933510869565219 -0.9295 -0.9295
maxi score, test score, baseline:  -0.9933510869565219 -0.9295 -0.9295
probs:  [0.04707131793701418, 0.23971186651025989, 0.3286800452901718, 0.15696104242071118, 0.04707131793701418, 0.18050440990482883]
printing an ep nov before normalisation:  33.744962215423584
maxi score, test score, baseline:  -0.9933510869565219 -0.9295 -0.9295
probs:  [0.04710890311139695, 0.2399037065430852, 0.32894312564039246, 0.15708662061639742, 0.04710890311139695, 0.17984874097733106]
printing an ep nov before normalisation:  47.383472238609414
printing an ep nov before normalisation:  68.06697621388437
using explorer policy with actor:  1
using explorer policy with actor:  0
printing an ep nov before normalisation:  62.23451384146429
line 256 mcts: sample exp_bonus 54.688292186955564
maxi score, test score, baseline:  -0.993368834688347 -0.9295 -0.9295
probs:  [0.047250482077029025, 0.23919938124920756, 0.3299341186176514, 0.15677071681659968, 0.047250482077029025, 0.17959481916248327]
maxi score, test score, baseline:  -0.993368834688347 -0.9295 -0.9295
probs:  [0.047250482077029025, 0.23919938124920756, 0.3299341186176514, 0.15677071681659968, 0.047250482077029025, 0.17959481916248327]
printing an ep nov before normalisation:  30.46018819431484
Printing some Q and Qe and total Qs values:  [[0.483]
 [0.483]
 [0.483]
 [0.462]
 [0.483]
 [0.483]
 [0.483]] [[65.244]
 [65.244]
 [65.244]
 [64.276]
 [65.244]
 [65.244]
 [65.244]] [[2.276]
 [2.276]
 [2.276]
 [2.219]
 [2.276]
 [2.276]
 [2.276]]
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]] [[68.645]
 [68.645]
 [68.645]
 [68.645]
 [68.645]
 [68.645]
 [68.645]] [[2.145]
 [2.145]
 [2.145]
 [2.145]
 [2.145]
 [2.145]
 [2.145]]
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  32 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9934040431266847 -0.9295 -0.9295
probs:  [0.04501604898455241, 0.22863476404816327, 0.31429304010855075, 0.14978333268908842, 0.04501604898455241, 0.2172567651850928]
Printing some Q and Qe and total Qs values:  [[0.124]
 [0.228]
 [0.416]
 [0.34 ]
 [0.334]
 [0.371]
 [0.309]] [[80.599]
 [81.468]
 [77.834]
 [76.198]
 [75.086]
 [76.244]
 [79.825]] [[0.882]
 [1.   ]
 [1.132]
 [1.03 ]
 [1.007]
 [1.062]
 [1.055]]
maxi score, test score, baseline:  -0.9934040431266847 -0.9295 -0.9295
maxi score, test score, baseline:  -0.9934040431266847 -0.9295 -0.9295
probs:  [0.04501604898455241, 0.22863476404816327, 0.31429304010855075, 0.14978333268908842, 0.04501604898455241, 0.2172567651850928]
maxi score, test score, baseline:  -0.9934040431266847 -0.9295 -0.9295
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.52 ]
 [0.242]
 [0.342]
 [0.327]
 [0.242]
 [0.308]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.242]
 [0.52 ]
 [0.242]
 [0.342]
 [0.327]
 [0.242]
 [0.308]]
maxi score, test score, baseline:  -0.9934215053763442 -0.9295 -0.9295
probs:  [0.044929094840743714, 0.2289641373240787, 0.3136834100341757, 0.14993392237560826, 0.044929094840743714, 0.21756034058464993]
Printing some Q and Qe and total Qs values:  [[0.086]
 [0.305]
 [0.011]
 [0.25 ]
 [0.229]
 [0.015]
 [0.249]] [[45.304]
 [84.271]
 [51.121]
 [48.138]
 [83.875]
 [41.636]
 [59.55 ]] [[0.347]
 [0.996]
 [0.336]
 [0.543]
 [0.915]
 [0.235]
 [0.667]]
maxi score, test score, baseline:  -0.9934388739946381 -0.9295 -0.9295
probs:  [0.04484236638710052, 0.22929265570906093, 0.31307536225946286, 0.15008412120541004, 0.04484236638710052, 0.2178631280518652]
printing an ep nov before normalisation:  37.50176464449075
printing an ep nov before normalisation:  24.63876301134885
printing an ep nov before normalisation:  38.681626935062994
maxi score, test score, baseline:  -0.9934388739946381 -0.9295 -0.9295
Printing some Q and Qe and total Qs values:  [[0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]] [[77.858]
 [77.858]
 [77.858]
 [77.858]
 [77.858]
 [77.858]
 [77.858]] [[1.071]
 [1.071]
 [1.071]
 [1.071]
 [1.071]
 [1.071]
 [1.071]]
Printing some Q and Qe and total Qs values:  [[0.215]
 [0.214]
 [0.208]
 [0.205]
 [0.204]
 [0.203]
 [0.207]] [[17.974]
 [37.493]
 [18.064]
 [20.75 ]
 [19.104]
 [19.276]
 [19.297]] [[0.215]
 [0.214]
 [0.208]
 [0.205]
 [0.204]
 [0.203]
 [0.207]]
Printing some Q and Qe and total Qs values:  [[0.25 ]
 [0.217]
 [0.224]
 [0.221]
 [0.222]
 [0.216]
 [0.205]] [[95.924]
 [86.608]
 [94.979]
 [94.947]
 [94.639]
 [94.227]
 [94.513]] [[1.181]
 [1.053]
 [1.145]
 [1.142]
 [1.139]
 [1.129]
 [1.121]]
printing an ep nov before normalisation:  24.061155319213867
from probs:  [0.04485032392106001, 0.22874492808696606, 0.313130070617037, 0.14979767342693912, 0.04485032392106001, 0.2186266800269378]
printing an ep nov before normalisation:  52.763974054951674
printing an ep nov before normalisation:  75.79480604848266
printing an ep nov before normalisation:  65.24144150309027
printing an ep nov before normalisation:  51.724491119384766
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  35.969438552856445
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]]
maxi score, test score, baseline:  -0.9934561497326204 -0.9295 -0.9295
probs:  [0.044798172963452856, 0.2273188075406239, 0.312763058067761, 0.15049923297391773, 0.044798172963452856, 0.2198225554907917]
printing an ep nov before normalisation:  30.56711256200458
maxi score, test score, baseline:  -0.993490425531915 -0.9295 -0.9295
probs:  [0.044798172963452856, 0.2273188075406239, 0.312763058067761, 0.15049923297391773, 0.044798172963452856, 0.2198225554907917]
printing an ep nov before normalisation:  49.22421020343393
printing an ep nov before normalisation:  33.636131286621094
printing an ep nov before normalisation:  25.483272075653076
maxi score, test score, baseline:  -0.993490425531915 -0.9295 -0.9295
probs:  [0.04493441945423764, 0.2266835432252784, 0.3137167078769358, 0.150203571274492, 0.04493441945423764, 0.21952733871481867]
printing an ep nov before normalisation:  4.673557029599351
actor:  1 policy actor:  1  step number:  35 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  62.23739430973992
printing an ep nov before normalisation:  58.19908618927002
maxi score, test score, baseline:  -0.993490425531915 -0.9295 -0.9295
probs:  [0.047167647105184514, 0.21685638408974323, 0.3293726755025872, 0.14688422925273295, 0.047167647105184514, 0.21255141694456756]
Printing some Q and Qe and total Qs values:  [[0.04 ]
 [0.07 ]
 [0.032]
 [0.015]
 [0.031]
 [0.029]
 [0.013]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.04 ]
 [0.07 ]
 [0.032]
 [0.015]
 [0.031]
 [0.029]
 [0.013]]
printing an ep nov before normalisation:  42.717551068794904
maxi score, test score, baseline:  -0.993490425531915 -0.9295 -0.9295
probs:  [0.047084437435337884, 0.217163233298588, 0.3287892860133095, 0.14703023537475676, 0.047084437435337884, 0.21284837044266985]
actor:  1 policy actor:  1  step number:  37 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.993490425531915 -0.9295 -0.9295
probs:  [0.049414985350493, 0.20856895831518243, 0.3451289393308368, 0.14294087576174824, 0.049414985350493, 0.20453125589124652]
Printing some Q and Qe and total Qs values:  [[ 0.131]
 [ 0.425]
 [ 0.015]
 [ 0.377]
 [ 0.432]
 [-0.008]
 [ 0.396]] [[92.585]
 [84.899]
 [91.12 ]
 [82.22 ]
 [84.603]
 [86.753]
 [85.651]] [[0.941]
 [1.129]
 [0.805]
 [1.045]
 [1.132]
 [0.722]
 [1.111]]
printing an ep nov before normalisation:  53.335259110729886
maxi score, test score, baseline:  -0.993507427055703 -0.9295 -0.9295
probs:  [0.04925570112193331, 0.20915634484856346, 0.3440121851732885, 0.14322036818798217, 0.04925570112193331, 0.2050996995462991]
printing an ep nov before normalisation:  87.0483126439403
maxi score, test score, baseline:  -0.993507427055703 -0.9295 -0.9295
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9935243386243388 -0.9295 -0.9295
probs:  [0.049339486921505575, 0.20951286534765118, 0.34459872267688396, 0.14346442491980374, 0.049339486921505575, 0.20374501321264996]
maxi score, test score, baseline:  -0.9935243386243388 -0.9295 -0.9295
probs:  [0.04961614134772336, 0.20976925929448045, 0.3436758367321039, 0.14372917339520122, 0.0492074524900389, 0.20400213674045228]
maxi score, test score, baseline:  -0.9935243386243388 -0.9295 -0.9295
maxi score, test score, baseline:  -0.9935243386243388 -0.9295 -0.9295
probs:  [0.04964923862265591, 0.2099094770132933, 0.34390561956333854, 0.14315689134625537, 0.049240276408246304, 0.20413849704621057]
maxi score, test score, baseline:  -0.9935243386243388 -0.9295 -0.9295
probs:  [0.04964923862265591, 0.2099094770132933, 0.34390561956333854, 0.14315689134625537, 0.049240276408246304, 0.20413849704621057]
printing an ep nov before normalisation:  50.37480057274479
maxi score, test score, baseline:  -0.9935243386243388 -0.9295 -0.9295
probs:  [0.04964923862265591, 0.2099094770132933, 0.34390561956333854, 0.14315689134625537, 0.049240276408246304, 0.20413849704621057]
maxi score, test score, baseline:  -0.9935243386243388 -0.9295 -0.9295
probs:  [0.04957113760800017, 0.21020259328598606, 0.3433514093456096, 0.14329538589852853, 0.04916122809783345, 0.20441824576404233]
printing an ep nov before normalisation:  71.52471036883696
maxi score, test score, baseline:  -0.9935243386243388 -0.9295 -0.9295
probs:  [0.04922281736767049, 0.21046648368626325, 0.3437825620885412, 0.14347522706292612, 0.04922281736767049, 0.20383009242692854]
maxi score, test score, baseline:  -0.9935243386243388 -0.9295 -0.9295
maxi score, test score, baseline:  -0.9935243386243388 -0.9295 -0.9295
probs:  [0.049120494182518355, 0.2103901195128256, 0.3430706404676512, 0.14371823015989876, 0.049915149770155605, 0.20378536590695046]
maxi score, test score, baseline:  -0.9935243386243388 -0.9295 -0.9295
probs:  [0.04896329003488978, 0.21097321017819595, 0.3419684993063135, 0.1439952690578658, 0.04976159342296338, 0.20433813799977152]
printing an ep nov before normalisation:  2.9684980097431435
maxi score, test score, baseline:  -0.9935243386243388 -0.9295 -0.9295
probs:  [0.04886323705342096, 0.21064000091125268, 0.3412664507161974, 0.14443826045648522, 0.049666102212941966, 0.20512594864970193]
maxi score, test score, baseline:  -0.9935243386243388 -0.9295 -0.9295
probs:  [0.04886323705342096, 0.21064000091125268, 0.3412664507161974, 0.14443826045648522, 0.049666102212941966, 0.20512594864970193]
maxi score, test score, baseline:  -0.9935243386243388 -0.9295 -0.9295
probs:  [0.04878527990693163, 0.21092756278393446, 0.3407199003221779, 0.14457624587169926, 0.04958995906296633, 0.20540105205229045]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  -0.9935243386243388 -0.9295 -0.9295
probs:  [0.04881609800828308, 0.2105326977289224, 0.34093482650342, 0.14436473693182525, 0.049216536403748894, 0.2061351044238003]
using explorer policy with actor:  0
printing an ep nov before normalisation:  63.506663584468015
maxi score, test score, baseline:  -0.9935411609498682 -0.9295 -0.9295
printing an ep nov before normalisation:  21.613388061523438
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9935411609498682 -0.9295 -0.9295
probs:  [0.04881372804393996, 0.2094795126643357, 0.3409166040446584, 0.1450924943816325, 0.04921722635793002, 0.20648043450750347]
using explorer policy with actor:  1
using another actor
printing an ep nov before normalisation:  30.043906093955002
maxi score, test score, baseline:  -0.9935578947368422 -0.9295 -0.9295
probs:  [0.04890997786925329, 0.20876676889138898, 0.34159039210483794, 0.14537910408811874, 0.04931427396965073, 0.20603948307675032]
printing an ep nov before normalisation:  57.44772089998622
printing an ep nov before normalisation:  64.19073977040257
Printing some Q and Qe and total Qs values:  [[0.28]
 [0.28]
 [0.28]
 [0.28]
 [0.28]
 [0.28]
 [0.28]] [[63.665]
 [63.665]
 [63.665]
 [63.665]
 [63.665]
 [63.665]
 [63.665]] [[0.947]
 [0.947]
 [0.947]
 [0.947]
 [0.947]
 [0.947]
 [0.947]]
maxi score, test score, baseline:  -0.9935578947368422 -0.9295 -0.9295
maxi score, test score, baseline:  -0.9935745406824148 -0.9295 -0.9295
probs:  [0.048991962313847795, 0.20911744220775952, 0.34216431667777514, 0.14562323479199463, 0.04939693795912029, 0.20470610604950257]
siam score:  -0.81994706
maxi score, test score, baseline:  -0.9935745406824148 -0.9295 -0.9295
actor:  1 policy actor:  1  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9935745406824148 -0.9295 -0.9295
probs:  [0.04611247708805619, 0.254780565806718, 0.3220059731652704, 0.13739631545032702, 0.0464950419699589, 0.19320962651966955]
siam score:  -0.81754947
line 256 mcts: sample exp_bonus 29.426553159098553
printing an ep nov before normalisation:  18.58896650692882
maxi score, test score, baseline:  -0.9935745406824148 -0.9295 -0.9295
probs:  [0.04603418483836677, 0.25514080409906165, 0.3214571256131322, 0.1375098625535625, 0.04641755370705084, 0.19344046918882604]
actor:  1 policy actor:  1  step number:  47 total reward:  0.01999999999999935  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9935745406824148 -0.9295 -0.9295
probs:  [0.04529274068052923, 0.24835308097977019, 0.3162667387995153, 0.1352907998523725, 0.04566991694191637, 0.2091267227458964]
printing an ep nov before normalisation:  89.49093127666822
printing an ep nov before normalisation:  53.63414249295083
maxi score, test score, baseline:  -0.9935910994764399 -0.9295 -0.9295
probs:  [0.045214375136778284, 0.24869266391756048, 0.3157173924955386, 0.13539767260831662, 0.04559232772034379, 0.2093855681214622]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9935910994764399 -0.9295 -0.9295
probs:  [0.045214375136778284, 0.24869266391756048, 0.3157173924955386, 0.13539767260831662, 0.04559232772034379, 0.2093855681214622]
Printing some Q and Qe and total Qs values:  [[0.153]
 [0.167]
 [0.153]
 [0.15 ]
 [0.151]
 [0.148]
 [0.152]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.153]
 [0.167]
 [0.153]
 [0.15 ]
 [0.151]
 [0.148]
 [0.152]]
maxi score, test score, baseline:  -0.9935910994764399 -0.9295 -0.9295
probs:  [0.0451362033856537, 0.24903140708998675, 0.31516940468737437, 0.13550428107524964, 0.04551493037160594, 0.20964377339012957]
actor:  1 policy actor:  1  step number:  35 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  69.24749716863796
maxi score, test score, baseline:  -0.9935910994764399 -0.9295 -0.9295
probs:  [0.04305515198296416, 0.2382566139827759, 0.3006006121652609, 0.1741214897545114, 0.04341773069994008, 0.20054840141454772]
printing an ep nov before normalisation:  43.132144206427455
maxi score, test score, baseline:  -0.9935910994764399 -0.9295 -0.9295
probs:  [0.04303123857632159, 0.23759315884705073, 0.30043248697773534, 0.17451820575050897, 0.04339498091220778, 0.20102992893617563]
using explorer policy with actor:  1
printing an ep nov before normalisation:  75.70155005127722
siam score:  -0.82435477
maxi score, test score, baseline:  -0.9936075718015667 -0.9295 -0.9295
probs:  [0.04303123857632159, 0.23759315884705073, 0.30043248697773534, 0.17451820575050897, 0.04339498091220778, 0.20102992893617563]
Printing some Q and Qe and total Qs values:  [[0.399]
 [0.333]
 [0.399]
 [0.399]
 [0.384]
 [0.399]
 [0.395]] [[44.381]
 [47.344]
 [44.381]
 [44.381]
 [45.287]
 [44.381]
 [45.105]] [[0.99 ]
 [1.   ]
 [0.99 ]
 [0.99 ]
 [0.999]
 [0.99 ]
 [1.005]]
printing an ep nov before normalisation:  52.28136552762186
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9936075718015667 -0.9295 -0.9295
maxi score, test score, baseline:  -0.9936075718015667 -0.9295 -0.9295
probs:  [0.04312018034267881, 0.23683043605061085, 0.3010551066634586, 0.17406314586328242, 0.04348467683672937, 0.20144645424324006]
Printing some Q and Qe and total Qs values:  [[0.446]
 [0.625]
 [0.601]
 [0.45 ]
 [0.589]
 [0.272]
 [0.565]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.446]
 [0.625]
 [0.601]
 [0.45 ]
 [0.589]
 [0.272]
 [0.565]]
printing an ep nov before normalisation:  55.3194522857666
printing an ep nov before normalisation:  56.71731242173622
maxi score, test score, baseline:  -0.9936075718015667 -0.9295 -0.9295
maxi score, test score, baseline:  -0.9936075718015667 -0.9295 -0.9295
probs:  [0.04324408505844478, 0.23626740406801994, 0.3019224777349974, 0.1745644779045306, 0.0436096321697343, 0.20039192306427303]
printing an ep nov before normalisation:  63.366873179756595
maxi score, test score, baseline:  -0.9936239583333334 -0.9295 -0.9295
probs:  [0.04324408505844478, 0.23626740406801994, 0.3019224777349974, 0.1745644779045306, 0.0436096321697343, 0.20039192306427303]
maxi score, test score, baseline:  -0.9936239583333334 -0.9295 -0.9295
probs:  [0.04316612672573632, 0.23656449092231802, 0.30137601886688437, 0.17474167569257193, 0.04353238409672615, 0.20061930369576322]
printing an ep nov before normalisation:  57.925112942538775
using explorer policy with actor:  1
printing an ep nov before normalisation:  67.6801018209055
printing an ep nov before normalisation:  29.761269627925913
actor:  1 policy actor:  1  step number:  38 total reward:  0.2699999999999996  reward:  1.0 rdn_beta:  2.0
from probs:  [0.043104153860063284, 0.23694777274851236, 0.3009414645465213, 0.1749826248849829, 0.043104153860063284, 0.20091983009985684]
siam score:  -0.8339714
maxi score, test score, baseline:  -0.9936239583333334 -0.9295 -0.9295
siam score:  -0.83500063
maxi score, test score, baseline:  -0.9936239583333334 -0.9295 -0.9295
maxi score, test score, baseline:  -0.9936239583333334 -0.9295 -0.9295
probs:  [0.04192047924881435, 0.2304244933004049, 0.2926554186570295, 0.17016623372727185, 0.04192047924881435, 0.22291289581766494]
maxi score, test score, baseline:  -0.9936239583333334 -0.9295 -0.9295
probs:  [0.04192047924881435, 0.2304244933004049, 0.2926554186570295, 0.17016623372727185, 0.04192047924881435, 0.22291289581766494]
maxi score, test score, baseline:  -0.9936402597402598 -0.9295 -0.9295
probs:  [0.041880271469297396, 0.2309067680318845, 0.2923732469179043, 0.17048148874339705, 0.041880271469297396, 0.22247795336821938]
using explorer policy with actor:  1
printing an ep nov before normalisation:  60.212231179580584
from probs:  [0.04198080632736513, 0.2290562487968866, 0.2930770167092274, 0.17089171989765828, 0.04198080632736513, 0.22301340194149724]
printing an ep nov before normalisation:  2.341206267649909
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.673]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.573]
 [0.673]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]]
printing an ep nov before normalisation:  0.027914085514011176
maxi score, test score, baseline:  -0.9936726098191215 -0.9295 -0.9295
probs:  [0.04190339245780832, 0.22932876696802643, 0.2925343923798445, 0.17105543900877618, 0.04190339245780832, 0.2232746167277362]
maxi score, test score, baseline:  -0.9936726098191215 -0.9295 -0.9295
probs:  [0.04190339245780832, 0.22932876696802643, 0.2925343923798445, 0.17105543900877618, 0.04190339245780832, 0.2232746167277362]
Printing some Q and Qe and total Qs values:  [[0.251]
 [0.449]
 [0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.251]] [[68.99 ]
 [75.235]
 [68.99 ]
 [68.99 ]
 [68.99 ]
 [68.99 ]
 [68.99 ]] [[1.49 ]
 [1.902]
 [1.49 ]
 [1.49 ]
 [1.49 ]
 [1.49 ]
 [1.49 ]]
printing an ep nov before normalisation:  66.26911059169859
maxi score, test score, baseline:  -0.9936726098191215 -0.9295 -0.9295
probs:  [0.0418635496877094, 0.22980618189799615, 0.2922547760253238, 0.17137203085022698, 0.0418635496877094, 0.22283991185103416]
actions average: 
K:  4  action  0 :  tensor([0.1882, 0.0091, 0.1447, 0.1940, 0.1774, 0.1341, 0.1526],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0363, 0.7913, 0.0300, 0.0465, 0.0281, 0.0199, 0.0479],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1451, 0.0466, 0.2713, 0.1509, 0.1268, 0.1143, 0.1451],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1760, 0.0722, 0.1411, 0.1735, 0.1586, 0.1249, 0.1537],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1739, 0.0349, 0.1408, 0.1779, 0.1931, 0.1266, 0.1530],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1405, 0.0006, 0.1380, 0.1468, 0.1488, 0.2745, 0.1509],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1613, 0.0019, 0.1309, 0.1560, 0.1791, 0.1439, 0.2270],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  54.54030841388923
printing an ep nov before normalisation:  84.27621157333787
printing an ep nov before normalisation:  32.06959780135184
maxi score, test score, baseline:  -0.9936726098191215 -0.9295 -0.9295
probs:  [0.04190067681656292, 0.23001052005787567, 0.2925146741913997, 0.1715243806212619, 0.04190067681656292, 0.22214907149633692]
from probs:  [0.04190067681656292, 0.23001052005787567, 0.2925146741913997, 0.1715243806212619, 0.04190067681656292, 0.22214907149633692]
using explorer policy with actor:  0
printing an ep nov before normalisation:  58.61382484436035
maxi score, test score, baseline:  -0.9936726098191215 -0.9295 -0.9295
probs:  [0.04193757106279581, 0.23021357649176033, 0.292772942127294, 0.1716757747672586, 0.04193757106279581, 0.22146256448809548]
printing an ep nov before normalisation:  90.28556486101976
printing an ep nov before normalisation:  63.98027500955534
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9936886597938145 -0.9295 -0.9295
probs:  [0.04197112824431903, 0.23039826658051601, 0.29300784986742573, 0.17101139709856006, 0.04197112824431903, 0.22164022996486019]
maxi score, test score, baseline:  -0.9936886597938145 -0.9295 -0.9295
probs:  [0.04197112824431903, 0.23039826658051601, 0.29300784986742573, 0.17101139709856006, 0.04197112824431903, 0.22164022996486019]
Printing some Q and Qe and total Qs values:  [[0.673]
 [0.559]
 [0.625]
 [0.622]
 [0.621]
 [0.621]
 [0.614]] [[30.919]
 [37.836]
 [34.349]
 [32.607]
 [30.304]
 [32.011]
 [34.892]] [[0.673]
 [0.559]
 [0.625]
 [0.622]
 [0.621]
 [0.621]
 [0.614]]
maxi score, test score, baseline:  -0.9936886597938145 -0.9295 -0.9295
probs:  [0.04197112824431903, 0.23039826658051601, 0.29300784986742573, 0.17101139709856006, 0.04197112824431903, 0.22164022996486019]
printing an ep nov before normalisation:  41.18871962384773
maxi score, test score, baseline:  -0.9937046272493574 -0.9295 -0.9295
printing an ep nov before normalisation:  35.92582702636719
printing an ep nov before normalisation:  68.92820648773305
Printing some Q and Qe and total Qs values:  [[0.084]
 [0.056]
 [0.146]
 [0.095]
 [0.123]
 [0.056]
 [0.056]] [[75.552]
 [67.619]
 [68.625]
 [67.026]
 [78.948]
 [67.619]
 [67.619]] [[0.808]
 [0.675]
 [0.778]
 [0.706]
 [0.893]
 [0.675]
 [0.675]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9937046272493574 -0.9295 -0.9295
probs:  [0.04186734840258, 0.23002271664980076, 0.29227995078122687, 0.1715423776211187, 0.04186734840258, 0.22242025814269367]
maxi score, test score, baseline:  -0.9937046272493574 -0.9295 -0.9295
probs:  [0.041840367530507044, 0.22937763979185619, 0.2920903704343215, 0.17190939472142136, 0.041840367530507044, 0.22294185999138674]
printing an ep nov before normalisation:  25.150301456451416
printing an ep nov before normalisation:  27.020117966397645
using another actor
from probs:  [0.041763988133271694, 0.22964619814996587, 0.2915549929809091, 0.17207225159833117, 0.041763988133271694, 0.22319858100425033]
actions average: 
K:  0  action  0 :  tensor([0.2640, 0.0031, 0.1411, 0.1661, 0.1499, 0.1371, 0.1387],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0040, 0.9080, 0.0171, 0.0209, 0.0015, 0.0075, 0.0410],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1306, 0.0018, 0.3122, 0.1336, 0.1080, 0.1920, 0.1218],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1210, 0.0140, 0.1251, 0.3349, 0.1258, 0.1248, 0.1544],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1721, 0.0028, 0.1418, 0.1809, 0.2298, 0.1398, 0.1327],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1463, 0.0051, 0.1703, 0.1779, 0.1474, 0.2152, 0.1378],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1333, 0.0596, 0.1411, 0.1746, 0.1277, 0.1393, 0.2244],
       grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 29.006216526031494
maxi score, test score, baseline:  -0.99373631713555 -0.9295 -0.9295
probs:  [0.041813193739250784, 0.22873646345220003, 0.29189943983047406, 0.17227547545560157, 0.041813193739250784, 0.2234622337832227]
from probs:  [0.041813193739250784, 0.22873646345220003, 0.29189943983047406, 0.17227547545560157, 0.041813193739250784, 0.2234622337832227]
maxi score, test score, baseline:  -0.9937520408163266 -0.9295 -0.9295
probs:  [0.041813193739250784, 0.22873646345220003, 0.29189943983047406, 0.17227547545560157, 0.041813193739250784, 0.2234622337832227]
printing an ep nov before normalisation:  19.81956958770752
Printing some Q and Qe and total Qs values:  [[-0.026]
 [-0.01 ]
 [-0.03 ]
 [-0.029]
 [-0.029]
 [-0.029]
 [-0.028]] [[18.305]
 [39.904]
 [18.178]
 [17.94 ]
 [17.372]
 [16.518]
 [16.73 ]] [[0.046]
 [0.217]
 [0.041]
 [0.04 ]
 [0.036]
 [0.03 ]
 [0.033]]
maxi score, test score, baseline:  -0.9937520408163266 -0.9295 -0.9295
Printing some Q and Qe and total Qs values:  [[-0.04]
 [-0.04]
 [-0.04]
 [-0.04]
 [-0.04]
 [-0.04]
 [-0.04]] [[42.746]
 [42.746]
 [42.746]
 [42.746]
 [42.746]
 [42.746]
 [42.746]] [[0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]]
printing an ep nov before normalisation:  37.047174241807724
printing an ep nov before normalisation:  24.20145034790039
maxi score, test score, baseline:  -0.9937520408163266 -0.9295 -0.9295
probs:  [0.04185018108215384, 0.22893932892728922, 0.2921583569494331, 0.17242823671817273, 0.04185018108215384, 0.2227737152407972]
maxi score, test score, baseline:  -0.9937520408163266 -0.9295 -0.9295
maxi score, test score, baseline:  -0.9937520408163266 -0.9295 -0.9295
printing an ep nov before normalisation:  26.22509933180661
maxi score, test score, baseline:  -0.9937520408163266 -0.9295 -0.9295
probs:  [0.041819885863585995, 0.22886517354729252, 0.291948281864537, 0.17247557910897246, 0.042178263802940534, 0.22271281581267152]
using explorer policy with actor:  0
using another actor
from probs:  [0.041819885863585995, 0.22886517354729252, 0.291948281864537, 0.17247557910897246, 0.042178263802940534, 0.22271281581267152]
maxi score, test score, baseline:  -0.9937832487309646 -0.9295 -0.9295
probs:  [0.041819885863585995, 0.22886517354729252, 0.291948281864537, 0.17247557910897246, 0.042178263802940534, 0.22271281581267152]
printing an ep nov before normalisation:  42.17296115734065
maxi score, test score, baseline:  -0.9937832487309646 -0.9295 -0.9295
probs:  [0.04189009344908871, 0.22925039793451973, 0.29243974917972576, 0.17196223122187468, 0.042249074959234384, 0.22220845325555674]
printing an ep nov before normalisation:  58.31875617217114
maxi score, test score, baseline:  -0.9937832487309646 -0.9295 -0.9295
probs:  [0.04189009344908871, 0.22925039793451973, 0.29243974917972576, 0.17196223122187468, 0.042249074959234384, 0.22220845325555674]
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.356]
 [0.396]
 [0.387]
 [0.39 ]
 [0.414]
 [0.428]] [[52.562]
 [57.077]
 [52.814]
 [56.429]
 [53.556]
 [53.083]
 [53.256]] [[1.227]
 [1.316]
 [1.229]
 [1.328]
 [1.245]
 [1.255]
 [1.274]]
printing an ep nov before normalisation:  53.334364891052246
maxi score, test score, baseline:  -0.9937987341772153 -0.9295 -0.9295
probs:  [0.041923387405601156, 0.2294330796761583, 0.2926728136171654, 0.1713025439077924, 0.042282655142078754, 0.2223855202512039]
Printing some Q and Qe and total Qs values:  [[0.256]
 [0.256]
 [0.293]
 [0.604]
 [0.256]
 [0.256]
 [0.256]] [[ 87.693]
 [ 87.693]
 [102.461]
 [ 93.997]
 [ 87.693]
 [ 87.693]
 [ 87.693]] [[0.707]
 [0.707]
 [0.87 ]
 [1.109]
 [0.707]
 [0.707]
 [0.707]]
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.519]
 [0.482]
 [0.428]
 [0.48 ]
 [0.428]
 [0.428]] [[66.616]
 [77.928]
 [76.306]
 [66.616]
 [79.377]
 [66.616]
 [66.616]] [[1.425]
 [1.871]
 [1.783]
 [1.425]
 [1.877]
 [1.425]
 [1.425]]
actions average: 
K:  0  action  0 :  tensor([0.2309, 0.0257, 0.1140, 0.1698, 0.1961, 0.1169, 0.1466],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0052, 0.9524, 0.0050, 0.0047, 0.0031, 0.0035, 0.0262],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1542, 0.0048, 0.2031, 0.1765, 0.1400, 0.1510, 0.1703],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1215, 0.0263, 0.1242, 0.3209, 0.1065, 0.1292, 0.1715],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1505, 0.0018, 0.1255, 0.1967, 0.1841, 0.1372, 0.2043],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1229, 0.0082, 0.1608, 0.1430, 0.1093, 0.2892, 0.1667],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1876, 0.0259, 0.1474, 0.1836, 0.1490, 0.1439, 0.1626],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9937987341772153 -0.9295 -0.9295
probs:  [0.04199302194401388, 0.22981515979182982, 0.29316026948668533, 0.17079720073469817, 0.04235288832482796, 0.22188145971794482]
maxi score, test score, baseline:  -0.9937987341772153 -0.9295 -0.9295
probs:  [0.041962905118677944, 0.2297403277227384, 0.2929514072728011, 0.17084724311346708, 0.042674702429512726, 0.22182341434280287]
actor:  1 policy actor:  1  step number:  36 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.71]
 [0.71]
 [0.71]
 [0.71]
 [0.71]
 [0.71]
 [0.71]] [[95.59]
 [95.59]
 [95.59]
 [95.59]
 [95.59]
 [95.59]
 [95.59]] [[2.043]
 [2.043]
 [2.043]
 [2.043]
 [2.043]
 [2.043]
 [2.043]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9937987341772153 -0.9295 -0.9295
probs:  [0.04021428263089291, 0.2608281869149526, 0.28070994672189026, 0.16416246742778373, 0.040898818750594645, 0.2131862975538858]
printing an ep nov before normalisation:  57.8002696539526
maxi score, test score, baseline:  -0.9937987341772153 -0.9295 -0.9295
maxi score, test score, baseline:  -0.9937987341772153 -0.9295 -0.9295
printing an ep nov before normalisation:  81.64098424980604
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9937987341772153 -0.9295 -0.9295
probs:  [0.04021616803294934, 0.26164516320163994, 0.2807224772428885, 0.16386790379799082, 0.0405566057645886, 0.21299168195994286]
from probs:  [0.04021616803294934, 0.26164516320163994, 0.2807224772428885, 0.16386790379799082, 0.0405566057645886, 0.21299168195994286]
using explorer policy with actor:  1
siam score:  -0.82510185
maxi score, test score, baseline:  -0.9937987341772153 -0.9295 -0.9295
probs:  [0.040282526970621, 0.2620781068364295, 0.2811870041460947, 0.1641389732076086, 0.04062352831075537, 0.21168986052849076]
maxi score, test score, baseline:  -0.9937987341772153 -0.9295 -0.9295
probs:  [0.04020706747367026, 0.2623911101979246, 0.2806581031244721, 0.1642804415194594, 0.04054866605904912, 0.21191461162542452]
maxi score, test score, baseline:  -0.9938141414141415 -0.9295 -0.9295
probs:  [0.04026036072297911, 0.26141106752378135, 0.28103116649710125, 0.16449873700305687, 0.04060241359220614, 0.21219625466087516]
printing an ep nov before normalisation:  46.39129638671875
maxi score, test score, baseline:  -0.9938294710327457 -0.9295 -0.9295
probs:  [0.04026036072297911, 0.26141106752378135, 0.28103116649710125, 0.16449873700305687, 0.04060241359220614, 0.21219625466087516]
printing an ep nov before normalisation:  47.228974234296246
maxi score, test score, baseline:  -0.9938294710327457 -0.9295 -0.9295
probs:  [0.04029054117754641, 0.26160758003844403, 0.2812424356975042, 0.16387084008653882, 0.04063285131187208, 0.21235575168809434]
maxi score, test score, baseline:  -0.9938294710327457 -0.9295 -0.9295
probs:  [0.040215279815504934, 0.26191899753278025, 0.2807149221237051, 0.16401149471948306, 0.04055818802439345, 0.21258111778413316]
maxi score, test score, baseline:  -0.9938294710327457 -0.9295 -0.9295
probs:  [0.04024522305350883, 0.26211456420268336, 0.280924530261061, 0.1633874683867808, 0.04058838743146374, 0.212739826664502]
maxi score, test score, baseline:  -0.9938294710327457 -0.9295 -0.9295
maxi score, test score, baseline:  -0.9938294710327457 -0.9295 -0.9295
probs:  [0.04019797100215679, 0.2613138734176287, 0.2805949667374798, 0.1638053594590018, 0.04087893150087865, 0.21320889788285421]
printing an ep nov before normalisation:  51.47399182896985
printing an ep nov before normalisation:  57.51123919624699
printing an ep nov before normalisation:  43.339842240199495
printing an ep nov before normalisation:  76.89834380476594
printing an ep nov before normalisation:  59.79159243038769
printing an ep nov before normalisation:  64.00878734245474
printing an ep nov before normalisation:  34.585703588250034
maxi score, test score, baseline:  -0.9938294710327457 -0.9295 -0.9295
probs:  [0.04009863746739193, 0.2614884378950804, 0.2799007938972906, 0.16400529636123776, 0.04111085475405705, 0.2133959796249422]
maxi score, test score, baseline:  -0.9938294710327457 -0.9295 -0.9295
probs:  [0.04018403485016372, 0.26073682169033807, 0.2804985962866803, 0.16435545530126874, 0.041198415024824485, 0.21302667684672477]
maxi score, test score, baseline:  -0.9938294710327457 -0.9295 -0.9295
probs:  [0.04018403485016372, 0.26073682169033807, 0.2804985962866803, 0.16435545530126874, 0.041198415024824485, 0.21302667684672477]
printing an ep nov before normalisation:  42.83744054443541
printing an ep nov before normalisation:  48.63861826158882
maxi score, test score, baseline:  -0.9938294710327457 -0.9295 -0.9295
probs:  [0.04018403485016372, 0.26073682169033807, 0.2804985962866803, 0.16435545530126874, 0.041198415024824485, 0.21302667684672477]
using explorer policy with actor:  1
from probs:  [0.04010958507876416, 0.26104261992465305, 0.27997677583206687, 0.1644950855044174, 0.0411257141138405, 0.21325021954625803]
maxi score, test score, baseline:  -0.9938294710327457 -0.9295 -0.9295
printing an ep nov before normalisation:  59.50003705588754
Printing some Q and Qe and total Qs values:  [[0.03 ]
 [0.134]
 [0.03 ]
 [0.03 ]
 [0.03 ]
 [0.03 ]
 [0.03 ]] [[44.273]
 [61.023]
 [44.273]
 [44.273]
 [44.273]
 [44.273]
 [44.273]] [[0.489]
 [0.897]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]]
maxi score, test score, baseline:  -0.9938447236180905 -0.9295 -0.9295
probs:  [0.040112700553612955, 0.26109039837605663, 0.28000039962475665, 0.1646660314608835, 0.04077150232321574, 0.21335896766147444]
printing an ep nov before normalisation:  74.94864775499572
maxi score, test score, baseline:  -0.9938447236180905 -0.9295 -0.9295
printing an ep nov before normalisation:  32.5036514730683
printing an ep nov before normalisation:  14.124140739440918
actor:  1 policy actor:  1  step number:  55 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9938447236180905 -0.9295 -0.9295
probs:  [0.03886051412234708, 0.25368236067908456, 0.27123412754941656, 0.18944143347720194, 0.039500963425853866, 0.207280600746096]
using explorer policy with actor:  1
printing an ep nov before normalisation:  76.28176642947481
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [-0.029]
 [-0.014]
 [-0.015]
 [-0.02 ]
 [-0.022]
 [-0.023]] [[49.633]
 [52.352]
 [49.146]
 [50.05 ]
 [49.964]
 [49.712]
 [48.934]] [[1.072]
 [1.166]
 [1.047]
 [1.084]
 [1.076]
 [1.063]
 [1.03 ]]
maxi score, test score, baseline:  -0.9938447236180905 -0.9295 -0.9295
probs:  [0.03892208592063685, 0.2540854737600857, 0.2716651458601085, 0.18974241109558243, 0.039563553462652126, 0.20602132990093433]
Printing some Q and Qe and total Qs values:  [[0.284]
 [0.214]
 [0.21 ]
 [0.195]
 [0.15 ]
 [0.188]
 [0.169]] [[38.532]
 [46.227]
 [45.415]
 [39.473]
 [44.092]
 [39.505]
 [39.072]] [[1.12 ]
 [1.376]
 [1.337]
 [1.07 ]
 [1.221]
 [1.065]
 [1.027]]
maxi score, test score, baseline:  -0.9938447236180905 -0.9295 -0.9295
maxi score, test score, baseline:  -0.9938447236180905 -0.9295 -0.9295
probs:  [0.038983787020789216, 0.25322407552748005, 0.27209706931685645, 0.19004402077227114, 0.03930221305074191, 0.20634883431186132]
printing an ep nov before normalisation:  76.33312357134967
maxi score, test score, baseline:  -0.9938447236180905 -0.9295 -0.9295
probs:  [0.038983787020789216, 0.25322407552748005, 0.27209706931685645, 0.19004402077227114, 0.03930221305074191, 0.20634883431186132]
Printing some Q and Qe and total Qs values:  [[0.244]
 [0.244]
 [0.244]
 [0.419]
 [0.244]
 [0.244]
 [0.244]] [[62.997]
 [62.997]
 [62.997]
 [62.86 ]
 [62.997]
 [62.997]
 [62.997]] [[0.792]
 [0.792]
 [0.792]
 [0.966]
 [0.792]
 [0.792]
 [0.792]]
actions average: 
K:  3  action  0 :  tensor([0.2386, 0.0152, 0.1434, 0.1442, 0.1971, 0.1254, 0.1361],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0190, 0.8414, 0.0269, 0.0448, 0.0160, 0.0157, 0.0363],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1047, 0.1614, 0.2756, 0.1170, 0.0989, 0.1263, 0.1162],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1416, 0.0340, 0.1491, 0.2791, 0.1450, 0.1254, 0.1258],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1581, 0.0352, 0.1669, 0.1819, 0.1744, 0.1445, 0.1390],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1472, 0.0277, 0.1514, 0.1414, 0.1443, 0.2579, 0.1302],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1816, 0.0348, 0.1504, 0.1541, 0.1609, 0.1286, 0.1896],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9938447236180905 -0.9295 -0.9295
probs:  [0.03899217701826428, 0.2527854442073074, 0.272155171677777, 0.18976507179130353, 0.03931180758052767, 0.20699032772482034]
line 256 mcts: sample exp_bonus 43.45042844653445
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.806468139705544
printing an ep nov before normalisation:  50.307213294278206
printing an ep nov before normalisation:  36.84646915956094
printing an ep nov before normalisation:  48.20409613825559
printing an ep nov before normalisation:  0.007180604268341995
printing an ep nov before normalisation:  59.116348661099636
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9938447236180905 -0.9295 -0.9295
probs:  [0.03896762287037589, 0.25289339394436033, 0.2719820262313637, 0.19071761237868273, 0.03896762287037589, 0.20647172170484143]
Printing some Q and Qe and total Qs values:  [[-0.   ]
 [-0.008]
 [-0.012]
 [-0.008]
 [-0.01 ]
 [-0.006]
 [-0.008]] [[24.666]
 [24.932]
 [26.403]
 [24.932]
 [24.114]
 [26.379]
 [24.932]] [[1.126]
 [1.131]
 [1.2  ]
 [1.131]
 [1.088]
 [1.206]
 [1.131]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.338]
 [0.445]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.338]
 [0.445]]
maxi score, test score, baseline:  -0.9938598997493735 -0.9295 -0.9295
probs:  [0.03895824257571674, 0.2535891135877301, 0.2719157312074632, 0.19034910475426367, 0.03895824257571674, 0.2062295652991096]
printing an ep nov before normalisation:  46.227288246154785
printing an ep nov before normalisation:  41.29755468046425
printing an ep nov before normalisation:  74.35756125429361
maxi score, test score, baseline:  -0.9938598997493735 -0.9295 -0.9295
probs:  [0.038864104918569266, 0.2537557766060959, 0.2712578848508814, 0.19053236029346818, 0.03918121910930175, 0.20640865422168359]
printing an ep nov before normalisation:  51.08455589839391
printing an ep nov before normalisation:  43.33108531104194
printing an ep nov before normalisation:  47.867045402526855
maxi score, test score, baseline:  -0.9938598997493735 -0.9295 -0.9295
probs:  [0.038912398596983144, 0.25282611246736114, 0.2715959512120925, 0.19076976969291354, 0.039229908198598944, 0.20666585983205077]
maxi score, test score, baseline:  -0.9938598997493735 -0.9295 -0.9295
probs:  [0.038960335283447924, 0.2519033205036975, 0.27193151855121245, 0.1910054241371138, 0.03927823737302822, 0.20692116415150003]
printing an ep nov before normalisation:  13.613236855167845
printing an ep nov before normalisation:  47.16217501538008
maxi score, test score, baseline:  -0.9938598997493735 -0.9295 -0.9295
probs:  [0.038960335283447924, 0.2519033205036975, 0.27193151855121245, 0.1910054241371138, 0.03927823737302822, 0.20692116415150003]
from probs:  [0.038960335283447924, 0.2519033205036975, 0.27193151855121245, 0.1910054241371138, 0.03927823737302822, 0.20692116415150003]
printing an ep nov before normalisation:  18.821538910441816
printing an ep nov before normalisation:  36.41658359103732
printing an ep nov before normalisation:  52.986721992492676
maxi score, test score, baseline:  -0.9939049751243783 -0.9295 -0.9295
actor:  0 policy actor:  1  step number:  26 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9902722084367246 -0.9295 -0.9295
probs:  [0.03908410207047191, 0.2514789262463243, 0.27279791325756303, 0.19075772434970922, 0.03908410207047191, 0.20679723200545974]
printing an ep nov before normalisation:  34.85005851048994
maxi score, test score, baseline:  -0.9902722084367246 -0.9295 -0.9295
probs:  [0.03908410207047191, 0.2514789262463243, 0.27279791325756303, 0.19075772434970922, 0.03908410207047191, 0.20679723200545974]
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.464]
 [0.467]
 [0.485]
 [0.486]
 [0.496]
 [0.477]] [[57.928]
 [58.488]
 [60.328]
 [57.269]
 [58.293]
 [61.253]
 [63.099]] [[1.081]
 [1.044]
 [1.082]
 [1.042]
 [1.063]
 [1.128]
 [1.144]]
using another actor
from probs:  [0.03908410207047191, 0.2514789262463243, 0.27279791325756303, 0.19075772434970922, 0.03908410207047191, 0.20679723200545974]
maxi score, test score, baseline:  -0.9902960396039604 -0.9295 -0.9295
printing an ep nov before normalisation:  33.90290975570679
printing an ep nov before normalisation:  33.92828251277233
printing an ep nov before normalisation:  20.470906725798145
maxi score, test score, baseline:  -0.9903197530864197 -0.9295 -0.9295
maxi score, test score, baseline:  -0.9903197530864197 -0.9295 -0.9295
maxi score, test score, baseline:  -0.9903433497536946 -0.9295 -0.9295
probs:  [0.03905879624992398, 0.25084335814599557, 0.27262014018300934, 0.19116673618025548, 0.03905879624992398, 0.20725217299089174]
maxi score, test score, baseline:  -0.9903433497536946 -0.9295 -0.9295
probs:  [0.03905879624992398, 0.25084335814599557, 0.27262014018300934, 0.19116673618025548, 0.03905879624992398, 0.20725217299089174]
printing an ep nov before normalisation:  59.70567881423036
printing an ep nov before normalisation:  39.403106089967615
maxi score, test score, baseline:  -0.9903433497536946 -0.9295 -0.9295
probs:  [0.03912240786541671, 0.2512530650991383, 0.2730654344455601, 0.19062668208776204, 0.03912240786541671, 0.20681000263670632]
printing an ep nov before normalisation:  67.9275801259521
maxi score, test score, baseline:  -0.9903433497536946 -0.9295 -0.9295
probs:  [0.03916962489715328, 0.2503471175023882, 0.27339596328316496, 0.19085737519978252, 0.03916962489715328, 0.20706029422035765]
Printing some Q and Qe and total Qs values:  [[0.74 ]
 [0.662]
 [0.713]
 [0.716]
 [0.72 ]
 [0.647]
 [0.647]] [[57.361]
 [50.552]
 [55.253]
 [55.299]
 [54.987]
 [57.302]
 [57.302]] [[1.332]
 [1.129]
 [1.266]
 [1.271]
 [1.269]
 [1.238]
 [1.238]]
maxi score, test score, baseline:  -0.9903433497536946 -0.9295 -0.9295
probs:  [0.03909707634245555, 0.25062121265685916, 0.2728874806442801, 0.19103381910991465, 0.03909707634245555, 0.20726333490403492]
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.822]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]] [[20.251]
 [22.205]
 [20.251]
 [20.251]
 [20.251]
 [20.251]
 [20.251]] [[0.998]
 [1.302]
 [0.998]
 [0.998]
 [0.998]
 [0.998]
 [0.998]]
siam score:  -0.820324
maxi score, test score, baseline:  -0.9903433497536946 -0.9295 -0.9295
maxi score, test score, baseline:  -0.9903433497536946 -0.9295 -0.9295
probs:  [0.03909707634245555, 0.25062121265685916, 0.2728874806442801, 0.19103381910991465, 0.03909707634245555, 0.20726333490403492]
actor:  1 policy actor:  1  step number:  36 total reward:  0.2899999999999996  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.116]
 [0.144]
 [0.086]
 [0.063]
 [0.071]
 [0.074]
 [0.117]] [[51.253]
 [66.117]
 [56.284]
 [57.399]
 [55.856]
 [53.68 ]
 [50.242]] [[0.776]
 [1.188]
 [0.875]
 [0.881]
 [0.85 ]
 [0.796]
 [0.75 ]]
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.553]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.496]
 [0.553]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]]
maxi score, test score, baseline:  -0.9903668304668305 -0.9295 -0.9295
probs:  [0.03932928558123122, 0.274521928824447, 0.2628833377140885, 0.18380366220718516, 0.03932928558123122, 0.20013250009181702]
printing an ep nov before normalisation:  68.97086519277167
maxi score, test score, baseline:  -0.9903668304668305 -0.9295 -0.9295
probs:  [0.03935845013598566, 0.2747260928288037, 0.26307884180880536, 0.18394032560222026, 0.03935845013598566, 0.19953783948819942]
maxi score, test score, baseline:  -0.9903668304668305 -0.9295 -0.9295
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.02  0.082 0.347 0.347 0.143 0.    0.061]
siam score:  -0.82482636
maxi score, test score, baseline:  -0.9903901960784314 -0.9295 -0.9295
using explorer policy with actor:  1
printing an ep nov before normalisation:  64.88578117920851
printing an ep nov before normalisation:  57.27785274933419
maxi score, test score, baseline:  -0.9903901960784314 -0.9295 -0.9295
probs:  [0.03939313389071442, 0.27496793707970685, 0.26221353212437487, 0.18490693336958008, 0.03939313389071442, 0.19912532964490925]
printing an ep nov before normalisation:  56.476360321573885
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.374]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]] [[93.969]
 [99.341]
 [93.969]
 [93.969]
 [93.969]
 [93.969]
 [93.969]] [[1.958]
 [2.111]
 [1.958]
 [1.958]
 [1.958]
 [1.958]
 [1.958]]
maxi score, test score, baseline:  -0.9903901960784314 -0.9295 -0.9295
probs:  [0.03942346468938911, 0.27518026453729094, 0.2616440593445732, 0.18504968292005616, 0.03942346468938911, 0.1992790638193015]
printing an ep nov before normalisation:  41.26544063096027
maxi score, test score, baseline:  -0.9903901960784314 -0.9295 -0.9295
probs:  [0.03970600666355593, 0.2750331253151869, 0.261521590684862, 0.18506681219902985, 0.039402205956631096, 0.19927025918073418]
line 256 mcts: sample exp_bonus 51.64062721113398
printing an ep nov before normalisation:  59.86584640651781
maxi score, test score, baseline:  -0.9903901960784314 -0.9295 -0.9295
probs:  [0.039765371479124186, 0.2754455143819743, 0.2611466083082296, 0.18534423896098487, 0.03946111502779684, 0.19883715184189021]
maxi score, test score, baseline:  -0.9903901960784314 -0.9295 -0.9295
using explorer policy with actor:  1
printing an ep nov before normalisation:  29.138271232292254
printing an ep nov before normalisation:  45.23081104653823
maxi score, test score, baseline:  -0.9903901960784314 -0.9295 -0.9295
probs:  [0.039794203458415445, 0.2756458012508839, 0.2613364928694507, 0.18547897807503697, 0.039489725663636614, 0.19825479868257645]
actor:  1 policy actor:  1  step number:  42 total reward:  0.16999999999999948  reward:  1.0 rdn_beta:  1.333
using another actor
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9904134474327628 -0.9295 -0.9295
probs:  [0.038882129072672945, 0.26930990175929653, 0.25458410577306134, 0.20564547923414975, 0.0385846532839138, 0.19299373087690572]
actions average: 
K:  0  action  0 :  tensor([0.3116, 0.0014, 0.1202, 0.1370, 0.1731, 0.1215, 0.1351],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0134, 0.9220, 0.0131, 0.0151, 0.0090, 0.0114, 0.0160],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1204, 0.0089, 0.3134, 0.1230, 0.1224, 0.1858, 0.1261],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1455, 0.0038, 0.1111, 0.3147, 0.1676, 0.1216, 0.1358],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1686, 0.0016, 0.1444, 0.1692, 0.2232, 0.1483, 0.1448],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1442, 0.0007, 0.1325, 0.1560, 0.1466, 0.2813, 0.1388],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1656, 0.0997, 0.1207, 0.1447, 0.1226, 0.1291, 0.2176],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.41 ]
 [0.322]
 [0.411]
 [0.459]
 [0.341]
 [0.759]] [[71.656]
 [49.968]
 [63.081]
 [62.794]
 [70.881]
 [60.327]
 [52.887]] [[1.073]
 [0.66 ]
 [0.724]
 [0.809]
 [0.95 ]
 [0.711]
 [1.042]]
printing an ep nov before normalisation:  51.948271572714155
maxi score, test score, baseline:  -0.9904365853658537 -0.9295 -0.9295
probs:  [0.038916901528107, 0.26955145528404173, 0.254812444676127, 0.2049332398567022, 0.03861915879078963, 0.19316679986423246]
maxi score, test score, baseline:  -0.9904365853658537 -0.9295 -0.9295
using another actor
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  38 total reward:  0.20999999999999952  reward:  1.0 rdn_beta:  1.0
from probs:  [0.0388036489628569, 0.2687524016720202, 0.2552492677151634, 0.20524297674411027, 0.0385051476132321, 0.1934465572926171]
maxi score, test score, baseline:  -0.9904365853658537 -0.9295 -0.9295
probs:  [0.03866841114580842, 0.26291831384067627, 0.26784893013728317, 0.2018780797852955, 0.03837570204267666, 0.1903105630482599]
printing an ep nov before normalisation:  54.68313694000244
maxi score, test score, baseline:  -0.9904365853658537 -0.9295 -0.9295
probs:  [0.03866841114580842, 0.26291831384067627, 0.26784893013728317, 0.2018780797852955, 0.03837570204267666, 0.1903105630482599]
printing an ep nov before normalisation:  67.96771954315697
Printing some Q and Qe and total Qs values:  [[0.26 ]
 [0.33 ]
 [0.115]
 [0.274]
 [0.271]
 [0.18 ]
 [0.306]] [[90.544]
 [74.169]
 [86.09 ]
 [83.265]
 [86.914]
 [80.238]
 [74.995]] [[0.26 ]
 [0.33 ]
 [0.115]
 [0.274]
 [0.271]
 [0.18 ]
 [0.306]]
maxi score, test score, baseline:  -0.9904596107055962 -0.9295 -0.9295
probs:  [0.03870213583387298, 0.2631482944474772, 0.26808322585245026, 0.20118017349351316, 0.03840917056164921, 0.19047699981103725]
printing an ep nov before normalisation:  70.84337745516576
printing an ep nov before normalisation:  53.20049285888672
maxi score, test score, baseline:  -0.9904825242718447 -0.9295 -0.9295
probs:  [0.03870213583387298, 0.2631482944474772, 0.26808322585245026, 0.20118017349351316, 0.03840917056164921, 0.19047699981103725]
actions average: 
K:  1  action  0 :  tensor([0.1834, 0.0293, 0.1405, 0.1530, 0.1600, 0.1410, 0.1927],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0028, 0.9806, 0.0030, 0.0045, 0.0025, 0.0028, 0.0039],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1125, 0.0602, 0.2687, 0.1337, 0.1254, 0.1718, 0.1277],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1585, 0.0448, 0.1404, 0.2015, 0.1518, 0.1336, 0.1694],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1807, 0.0024, 0.1277, 0.1399, 0.2428, 0.1394, 0.1671],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1616, 0.0127, 0.1410, 0.1490, 0.1535, 0.2268, 0.1554],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1539, 0.1503, 0.1111, 0.1356, 0.1320, 0.1044, 0.2127],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  55.179186854702074
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.319]
 [0.32 ]
 [0.329]
 [0.311]
 [0.321]
 [0.333]] [[40.672]
 [40.83 ]
 [40.281]
 [40.96 ]
 [40.913]
 [41.081]
 [40.39 ]] [[1.153]
 [1.164]
 [1.143]
 [1.18 ]
 [1.16 ]
 [1.177]
 [1.16 ]]
printing an ep nov before normalisation:  33.13561087026413
maxi score, test score, baseline:  -0.9904825242718447 -0.9295 -0.9295
probs:  [0.03873562169027253, 0.26337664637646374, 0.26831586233104693, 0.20048720963800698, 0.0384424020630963, 0.19064225790111344]
maxi score, test score, baseline:  -0.9904825242718447 -0.9295 -0.9295
probs:  [0.038666001416019266, 0.26365744528746254, 0.267824724143633, 0.20066990685447503, 0.038372324393391784, 0.1908095979050184]
maxi score, test score, baseline:  -0.9904825242718447 -0.9295 -0.9295
probs:  [0.038666001416019266, 0.26365744528746254, 0.267824724143633, 0.20066990685447503, 0.038372324393391784, 0.1908095979050184]
printing an ep nov before normalisation:  28.681494790199125
maxi score, test score, baseline:  -0.9904825242718447 -0.9295 -0.9295
printing an ep nov before normalisation:  81.1481442503768
printing an ep nov before normalisation:  65.8169399419486
maxi score, test score, baseline:  -0.9905053268765134 -0.9295 -0.9295
Printing some Q and Qe and total Qs values:  [[0.189]
 [0.189]
 [0.189]
 [0.189]
 [0.189]
 [0.189]
 [0.189]] [[76.127]
 [76.127]
 [76.127]
 [76.127]
 [76.127]
 [76.127]
 [76.127]] [[0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]]
maxi score, test score, baseline:  -0.9905053268765134 -0.9295 -0.9295
probs:  [0.03896214056871483, 0.26371968863958545, 0.2678826352909062, 0.20079763056653682, 0.038380369524961024, 0.19025753540929569]
siam score:  -0.82613325
actions average: 
K:  4  action  0 :  tensor([0.1886, 0.0068, 0.1607, 0.1758, 0.1856, 0.1525, 0.1299],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0315, 0.7350, 0.0372, 0.0696, 0.0198, 0.0275, 0.0793],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1401, 0.0184, 0.2612, 0.1635, 0.1326, 0.1450, 0.1391],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1625, 0.0133, 0.1571, 0.2222, 0.1541, 0.1452, 0.1455],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2114, 0.0037, 0.1393, 0.1539, 0.2268, 0.1448, 0.1202],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1667, 0.0013, 0.1575, 0.1863, 0.1754, 0.1603, 0.1525],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1330, 0.0075, 0.1198, 0.1748, 0.1253, 0.1162, 0.3234],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.292]
 [0.292]
 [0.292]
 [0.292]
 [0.292]
 [0.292]
 [0.292]] [[79.581]
 [79.581]
 [79.581]
 [79.581]
 [79.581]
 [79.581]
 [79.581]] [[1.455]
 [1.455]
 [1.455]
 [1.455]
 [1.455]
 [1.455]
 [1.455]]
maxi score, test score, baseline:  -0.9905053268765134 -0.9295 -0.9295
probs:  [0.03867522141325777, 0.2640394403606952, 0.2679127934319334, 0.2010541773329571, 0.038384836264063055, 0.18993353119709352]
Printing some Q and Qe and total Qs values:  [[0.227]
 [0.21 ]
 [0.236]
 [0.236]
 [0.224]
 [0.236]
 [0.236]] [[47.158]
 [47.318]
 [45.998]
 [45.998]
 [46.921]
 [45.998]
 [45.998]] [[1.502]
 [1.493]
 [1.452]
 [1.452]
 [1.488]
 [1.452]
 [1.452]]
maxi score, test score, baseline:  -0.9905053268765134 -0.9295 -0.9295
probs:  [0.03867522141325777, 0.2640394403606952, 0.2679127934319334, 0.2010541773329571, 0.038384836264063055, 0.18993353119709352]
maxi score, test score, baseline:  -0.9905053268765134 -0.9295 -0.9295
probs:  [0.038941573068953454, 0.2639204482090998, 0.2677871783451337, 0.20104288187417169, 0.038366667890803036, 0.18994125061183836]
printing an ep nov before normalisation:  52.892490072204104
maxi score, test score, baseline:  -0.9905053268765134 -0.9295 -0.9295
probs:  [0.038941573068953454, 0.2639204482090998, 0.2677871783451337, 0.20104288187417169, 0.038366667890803036, 0.18994125061183836]
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.597]
 [0.443]
 [0.435]
 [0.455]
 [0.443]
 [0.444]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.395]
 [0.597]
 [0.443]
 [0.435]
 [0.455]
 [0.443]
 [0.444]]
Printing some Q and Qe and total Qs values:  [[0.45 ]
 [0.444]
 [0.336]
 [0.45 ]
 [0.45 ]
 [0.337]
 [0.37 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.45 ]
 [0.444]
 [0.336]
 [0.45 ]
 [0.45 ]
 [0.337]
 [0.37 ]]
maxi score, test score, baseline:  -0.9905053268765134 -0.9295 -0.9295
probs:  [0.039022520315625794, 0.26324553673542034, 0.2683454564605024, 0.200602914409438, 0.03844641598843438, 0.19033715609057908]
maxi score, test score, baseline:  -0.9905053268765134 -0.9295 -0.9295
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9905053268765134 -0.9295 -0.9295
probs:  [0.039022520315625794, 0.26324553673542034, 0.2683454564605024, 0.200602914409438, 0.03844641598843438, 0.19033715609057908]
maxi score, test score, baseline:  -0.9905280193236715 -0.9295 -0.9295
probs:  [0.039022520315625794, 0.26324553673542034, 0.2683454564605024, 0.200602914409438, 0.03844641598843438, 0.19033715609057908]
maxi score, test score, baseline:  -0.9905280193236715 -0.9295 -0.9295
probs:  [0.039022520315625794, 0.26324553673542034, 0.2683454564605024, 0.200602914409438, 0.03844641598843438, 0.19033715609057908]
printing an ep nov before normalisation:  45.70742865638702
maxi score, test score, baseline:  -0.9905280193236715 -0.9295 -0.9295
probs:  [0.039022520315625794, 0.26324553673542034, 0.2683454564605024, 0.200602914409438, 0.03844641598843438, 0.19033715609057908]
maxi score, test score, baseline:  -0.9905280193236715 -0.9295 -0.9295
probs:  [0.038954029664873685, 0.26352336428419676, 0.2678592069241635, 0.2007839888214309, 0.038377035529773514, 0.1905023747755617]
maxi score, test score, baseline:  -0.9905506024096385 -0.9295 -0.9295
probs:  [0.038954029664873685, 0.26352336428419676, 0.2678592069241635, 0.2007839888214309, 0.038377035529773514, 0.1905023747755617]
using explorer policy with actor:  1
printing an ep nov before normalisation:  59.25985534073443
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.283]] [[37.568]
 [37.568]
 [37.568]
 [37.568]
 [37.568]
 [37.568]
 [37.568]] [[0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]]
printing an ep nov before normalisation:  13.902434185941388
maxi score, test score, baseline:  -0.9905506024096385 -0.9295 -0.9295
line 256 mcts: sample exp_bonus 15.559752548861976
printing an ep nov before normalisation:  34.31722048027747
maxi score, test score, baseline:  -0.9905506024096385 -0.9295 -0.9295
probs:  [0.038954029664873685, 0.26352336428419676, 0.2678592069241635, 0.2007839888214309, 0.038377035529773514, 0.1905023747755617]
printing an ep nov before normalisation:  2.6258085254801244
from probs:  [0.038954029664873685, 0.26352336428419676, 0.2678592069241635, 0.2007839888214309, 0.038377035529773514, 0.1905023747755617]
from probs:  [0.038674309895686654, 0.26360007916894423, 0.26793718466435235, 0.20084243026586338, 0.038388174403120034, 0.19055782160203338]
actor:  1 policy actor:  1  step number:  64 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.990573076923077 -0.9295 -0.9295
probs:  [0.04114207623372122, 0.28535956830747466, 0.2560088725092866, 0.1931271916481968, 0.04087390801568524, 0.18348838328563546]
maxi score, test score, baseline:  -0.990573076923077 -0.9295 -0.9295
probs:  [0.04114207623372122, 0.28535956830747466, 0.2560088725092866, 0.1931271916481968, 0.04087390801568524, 0.18348838328563546]
actions average: 
K:  2  action  0 :  tensor([0.2785, 0.0518, 0.1313, 0.1434, 0.1213, 0.1175, 0.1561],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0069, 0.9275, 0.0147, 0.0140, 0.0042, 0.0092, 0.0233],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1300, 0.0177, 0.3785, 0.1157, 0.0789, 0.1702, 0.1091],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1649, 0.0264, 0.1412, 0.2671, 0.1205, 0.1344, 0.1455],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1762, 0.0142, 0.1411, 0.1568, 0.2280, 0.1266, 0.1572],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1778, 0.0148, 0.1937, 0.1852, 0.1341, 0.1613, 0.1331],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2197, 0.0673, 0.1282, 0.1513, 0.1191, 0.1142, 0.2002],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  43.95240942626082
printing an ep nov before normalisation:  60.34475620478979
actions average: 
K:  4  action  0 :  tensor([0.2423, 0.0211, 0.1518, 0.1599, 0.1364, 0.1283, 0.1602],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0368, 0.7897, 0.0389, 0.0397, 0.0332, 0.0315, 0.0302],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1724, 0.0384, 0.2264, 0.1481, 0.1127, 0.1492, 0.1528],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1327, 0.0326, 0.1344, 0.3702, 0.1043, 0.1063, 0.1194],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1283, 0.1350, 0.1123, 0.1612, 0.2521, 0.1083, 0.1028],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1797, 0.0011, 0.1482, 0.1643, 0.1170, 0.2350, 0.1548],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2159, 0.0590, 0.1468, 0.1649, 0.1338, 0.1302, 0.1494],
       grad_fn=<DivBackward0>)
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  -0.9906398568019094 -0.9295 -0.9295
probs:  [0.04114207623372122, 0.28535956830747466, 0.2560088725092866, 0.1931271916481968, 0.04087390801568524, 0.18348838328563546]
maxi score, test score, baseline:  -0.9906398568019094 -0.9295 -0.9295
maxi score, test score, baseline:  -0.9906398568019094 -0.9295 -0.9295
probs:  [0.041174917482560615, 0.28558798703604044, 0.256213786225727, 0.19248155456374222, 0.040906534506508, 0.18363522018542175]
printing an ep nov before normalisation:  21.109593924248156
printing an ep nov before normalisation:  65.1478713385215
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9906398568019094 -0.9295 -0.9295
probs:  [0.041174917482560615, 0.28558798703604044, 0.256213786225727, 0.19248155456374222, 0.040906534506508, 0.18363522018542175]
maxi score, test score, baseline:  -0.9906398568019094 -0.9295 -0.9295
probs:  [0.041174917482560615, 0.28558798703604044, 0.256213786225727, 0.19248155456374222, 0.040906534506508, 0.18363522018542175]
maxi score, test score, baseline:  -0.9906398568019094 -0.9295 -0.9295
probs:  [0.040917489064845, 0.28566468012267765, 0.2562825873503406, 0.19253323275945602, 0.040917489064845, 0.18368452163783577]
actor:  1 policy actor:  1  step number:  44 total reward:  0.16999999999999948  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9906398568019094 -0.9295 -0.9295
probs:  [0.0392088582212999, 0.2737025206924445, 0.24555137209101183, 0.18447275634233978, 0.0810697362830647, 0.17599475636983927]
maxi score, test score, baseline:  -0.9906398568019094 -0.9295 -0.9295
probs:  [0.0392088582212999, 0.2737025206924445, 0.24555137209101183, 0.18447275634233978, 0.0810697362830647, 0.17599475636983927]
maxi score, test score, baseline:  -0.9906398568019094 -0.9295 -0.9295
probs:  [0.0392088582212999, 0.2737025206924445, 0.24555137209101183, 0.18447275634233978, 0.0810697362830647, 0.17599475636983927]
Starting evaluation
maxi score, test score, baseline:  -0.9906398568019094 -0.9295 -0.9295
probs:  [0.0392088582212999, 0.2737025206924445, 0.24555137209101183, 0.18447275634233978, 0.0810697362830647, 0.17599475636983927]
maxi score, test score, baseline:  -0.9906398568019094 -0.9295 -0.9295
probs:  [0.039232814981548673, 0.273870242460204, 0.245701834756553, 0.18458577250481004, 0.0811193574019385, 0.17548997789494566]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  50.71925322214763
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  64.34770992824009
maxi score, test score, baseline:  -0.9906398568019094 -0.9295 -0.9295
probs:  [0.039232814981548673, 0.273870242460204, 0.245701834756553, 0.18458577250481004, 0.0811193574019385, 0.17548997789494566]
line 256 mcts: sample exp_bonus 74.2631322145462
maxi score, test score, baseline:  -0.9906398568019094 -0.9295 -0.9295
probs:  [0.039232814981548673, 0.273870242460204, 0.245701834756553, 0.18458577250481004, 0.0811193574019385, 0.17548997789494566]
printing an ep nov before normalisation:  53.39376983385023
printing an ep nov before normalisation:  0.0030266719221572203
printing an ep nov before normalisation:  63.94637504263642
maxi score, test score, baseline:  -0.9906619047619047 -0.9295 -0.9295
probs:  [0.03912319349025375, 0.2731019987759695, 0.2461050497348803, 0.18483718482704606, 0.08111377547339715, 0.17571879769845317]
printing an ep nov before normalisation:  46.012272072437135
printing an ep nov before normalisation:  69.50724124908447
printing an ep nov before normalisation:  49.79138842500106
printing an ep nov before normalisation:  55.50236293094855
Printing some Q and Qe and total Qs values:  [[0.623]
 [0.57 ]
 [0.6  ]
 [0.596]
 [0.6  ]
 [0.604]
 [0.603]] [[47.2  ]
 [48.782]
 [47.781]
 [48.096]
 [47.821]
 [47.103]
 [45.495]] [[0.623]
 [0.57 ]
 [0.6  ]
 [0.596]
 [0.6  ]
 [0.604]
 [0.603]]
maxi score, test score, baseline:  -0.9907274231678487 -0.9295 -0.9295
probs:  [0.03912319349025375, 0.2731019987759695, 0.2461050497348803, 0.18483718482704606, 0.08111377547339715, 0.17571879769845317]
printing an ep nov before normalisation:  56.322785286385404
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.587]
 [0.523]
 [0.54 ]
 [0.58 ]
 [0.546]
 [0.58 ]
 [0.58 ]] [[23.434]
 [30.895]
 [19.488]
 [29.925]
 [19.549]
 [29.925]
 [29.925]] [[0.587]
 [0.523]
 [0.54 ]
 [0.58 ]
 [0.546]
 [0.58 ]
 [0.58 ]]
maxi score, test score, baseline:  -0.9907705882352942 -0.9295 -0.9295
probs:  [0.03912319349025375, 0.2731019987759695, 0.2461050497348803, 0.18483718482704606, 0.08111377547339715, 0.17571879769845317]
printing an ep nov before normalisation:  15.740142587033185
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 32.03222597295799
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  21.83709972065138
using explorer policy with actor:  1
using explorer policy with actor:  0
printing an ep nov before normalisation:  73.32306675985899
using explorer policy with actor:  0
printing an ep nov before normalisation:  17.3895263671875
actor:  0 policy actor:  1  step number:  47 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  2
actions average: 
K:  2  action  0 :  tensor([0.2327, 0.0097, 0.1356, 0.1659, 0.1725, 0.1372, 0.1463],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0127, 0.9144, 0.0142, 0.0222, 0.0059, 0.0088, 0.0218],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1050, 0.0285, 0.3517, 0.1353, 0.0905, 0.1747, 0.1142],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1372, 0.0626, 0.1312, 0.2937, 0.1278, 0.1214, 0.1261],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2068, 0.0096, 0.1439, 0.1801, 0.2057, 0.1310, 0.1230],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1471, 0.0022, 0.1411, 0.1709, 0.1382, 0.2899, 0.1106],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1541, 0.0945, 0.1304, 0.1342, 0.1351, 0.1108, 0.2410],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.319]
 [0.335]
 [0.319]
 [0.319]
 [0.319]
 [0.319]
 [0.319]] [[74.448]
 [80.362]
 [74.448]
 [74.448]
 [74.448]
 [74.448]
 [74.448]] [[1.207]
 [1.335]
 [1.207]
 [1.207]
 [1.207]
 [1.207]
 [1.207]]
printing an ep nov before normalisation:  32.80808999887147
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.6  ]
 [0.524]
 [0.595]
 [0.584]
 [0.583]
 [0.596]
 [0.597]] [[21.396]
 [26.952]
 [17.182]
 [19.166]
 [20.358]
 [17.239]
 [17.86 ]] [[0.6  ]
 [0.524]
 [0.595]
 [0.584]
 [0.583]
 [0.596]
 [0.597]]
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  49 total reward:  0.09999999999999942  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  17.864273993100444
printing an ep nov before normalisation:  17.42332110577212
maxi score, test score, baseline:  -0.9881837528604119 -0.9295 -0.9295
printing an ep nov before normalisation:  56.60606918975912
printing an ep nov before normalisation:  31.176578519947018
printing an ep nov before normalisation:  14.95093341349388
printing an ep nov before normalisation:  15.783789524066066
printing an ep nov before normalisation:  65.45383273547503
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.988290022675737 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.988290022675737 -0.9380000000000001 -0.9380000000000001
printing an ep nov before normalisation:  28.781194169540978
maxi score, test score, baseline:  -0.9883162895927602 -0.9380000000000001 -0.9380000000000001
probs:  [0.03842682561465382, 0.26822525675729025, 0.2393928579709715, 0.20064586759205683, 0.07997700489729861, 0.173332187167729]
maxi score, test score, baseline:  -0.9883162895927602 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.9883162895927602 -0.9380000000000001 -0.9380000000000001
probs:  [0.03842682561465382, 0.26822525675729025, 0.2393928579709715, 0.20064586759205683, 0.07997700489729861, 0.173332187167729]
using explorer policy with actor:  1
siam score:  -0.82080096
maxi score, test score, baseline:  -0.9883162895927602 -0.9380000000000001 -0.9380000000000001
probs:  [0.03842682561465382, 0.26822525675729025, 0.2393928579709715, 0.20064586759205683, 0.07997700489729861, 0.173332187167729]
maxi score, test score, baseline:  -0.9883162895927602 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.9883162895927602 -0.9380000000000001 -0.9380000000000001
Printing some Q and Qe and total Qs values:  [[0.207]
 [0.24 ]
 [0.214]
 [0.213]
 [0.208]
 [0.209]
 [0.208]] [[39.463]
 [43.761]
 [38.207]
 [38.373]
 [37.722]
 [38.443]
 [38.434]] [[0.207]
 [0.24 ]
 [0.214]
 [0.213]
 [0.208]
 [0.209]
 [0.208]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  65.19618148227782
maxi score, test score, baseline:  -0.9883162895927602 -0.9380000000000001 -0.9380000000000001
probs:  [0.03850046733371501, 0.26874081963313734, 0.23852563266310217, 0.20103146978154812, 0.07953634767016167, 0.17366526291833564]
actions average: 
K:  2  action  0 :  tensor([0.2763, 0.1021, 0.1422, 0.1190, 0.1089, 0.1117, 0.1398],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0189, 0.9231, 0.0139, 0.0114, 0.0068, 0.0059, 0.0199],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1447, 0.0166, 0.3142, 0.1236, 0.1176, 0.1602, 0.1231],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1846, 0.0350, 0.1380, 0.2336, 0.1226, 0.1194, 0.1668],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1972, 0.0310, 0.1375, 0.1315, 0.2406, 0.1249, 0.1374],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1427, 0.0037, 0.2033, 0.1184, 0.1107, 0.3044, 0.1169],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1752, 0.1004, 0.1292, 0.1298, 0.1229, 0.1184, 0.2240],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  76.94023179163624
maxi score, test score, baseline:  -0.9883162895927602 -0.9380000000000001 -0.9380000000000001
probs:  [0.038522922613409796, 0.26889802819270087, 0.238665157113406, 0.20114904994027244, 0.07899801574352676, 0.17376682639668417]
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.585]
 [0.469]
 [0.413]
 [0.388]
 [0.469]
 [0.485]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.542]
 [0.585]
 [0.469]
 [0.413]
 [0.388]
 [0.469]
 [0.485]]
maxi score, test score, baseline:  -0.9883424379232506 -0.9380000000000001 -0.9380000000000001
probs:  [0.03843968650306929, 0.26831452165726716, 0.23853853576820316, 0.2015671682166084, 0.079039558878843, 0.17410052897600894]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  53.83096218109131
maxi score, test score, baseline:  -0.9883424379232506 -0.9380000000000001 -0.9380000000000001
probs:  [0.03843968650306929, 0.26831452165726716, 0.23853853576820316, 0.2015671682166084, 0.079039558878843, 0.17410052897600894]
maxi score, test score, baseline:  -0.9883424379232506 -0.9380000000000001 -0.9380000000000001
probs:  [0.03840224856126066, 0.2680516469299346, 0.23869385657190834, 0.20222446414303372, 0.07859550274874273, 0.17403228104511997]
maxi score, test score, baseline:  -0.9883424379232506 -0.9380000000000001 -0.9380000000000001
probs:  [0.03845952770270745, 0.2684526537066961, 0.23839425719069274, 0.20168805589668037, 0.07871294112559622, 0.17429256437762713]
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.358]
 [0.355]
 [0.355]
 [0.354]
 [0.355]
 [0.359]] [[45.188]
 [73.879]
 [46.676]
 [46.922]
 [47.628]
 [47.475]
 [46.364]] [[0.937]
 [1.693]
 [0.997]
 [1.004]
 [1.02 ]
 [1.017]
 [0.993]]
maxi score, test score, baseline:  -0.9883424379232506 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.9883424379232506 -0.9380000000000001 -0.9380000000000001
probs:  [0.03845952770270745, 0.2684526537066961, 0.23839425719069274, 0.20168805589668037, 0.07871294112559622, 0.17429256437762713]
printing an ep nov before normalisation:  0.1256575485609801
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]] [[56.29]
 [56.29]
 [56.29]
 [56.29]
 [56.29]
 [56.29]
 [56.29]] [[1.268]
 [1.268]
 [1.268]
 [1.268]
 [1.268]
 [1.268]
 [1.268]]
printing an ep nov before normalisation:  49.063782691955566
maxi score, test score, baseline:  -0.9883424379232506 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.9883424379232506 -0.9380000000000001 -0.9380000000000001
probs:  [0.03850469167528413, 0.26876884313641275, 0.2386750256024069, 0.20192556947222431, 0.07823383431632185, 0.17389203579735005]
siam score:  -0.8156512
maxi score, test score, baseline:  -0.9883424379232506 -0.9380000000000001 -0.9380000000000001
probs:  [0.03850469167528413, 0.26876884313641275, 0.2386750256024069, 0.20192556947222431, 0.07823383431632185, 0.17389203579735005]
maxi score, test score, baseline:  -0.9883424379232506 -0.9380000000000001 -0.9380000000000001
probs:  [0.03839702896972661, 0.26801433026577, 0.23904774491591155, 0.20221009500801526, 0.07822151623742844, 0.17410928460314817]
maxi score, test score, baseline:  -0.9883684684684685 -0.9380000000000001 -0.9380000000000001
printing an ep nov before normalisation:  60.808414776051904
Printing some Q and Qe and total Qs values:  [[0.272]
 [0.362]
 [0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]] [[18.832]
 [34.776]
 [18.832]
 [18.832]
 [18.832]
 [18.832]
 [18.832]] [[0.531]
 [1.04 ]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]]
maxi score, test score, baseline:  -0.9883684684684685 -0.9380000000000001 -0.9380000000000001
probs:  [0.03841863570695285, 0.26816559690097136, 0.239182654750123, 0.20232420342220414, 0.07770138411911265, 0.17420752510063592]
actions average: 
K:  1  action  0 :  tensor([0.2580, 0.0039, 0.1417, 0.1516, 0.1719, 0.1252, 0.1477],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0189, 0.8218, 0.0144, 0.0469, 0.0128, 0.0080, 0.0772],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1399, 0.0017, 0.2803, 0.1469, 0.1429, 0.1469, 0.1415],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1418, 0.0144, 0.1325, 0.2981, 0.1396, 0.1341, 0.1394],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1646, 0.0033, 0.1359, 0.1555, 0.2663, 0.1342, 0.1403],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1525, 0.0071, 0.1669, 0.1478, 0.1400, 0.2350, 0.1507],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1845, 0.0094, 0.1346, 0.1604, 0.1548, 0.1357, 0.2205],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9883684684684685 -0.9380000000000001 -0.9380000000000001
probs:  [0.03841863570695285, 0.26816559690097136, 0.239182654750123, 0.20232420342220414, 0.07770138411911265, 0.17420752510063592]
printing an ep nov before normalisation:  62.261401424183354
printing an ep nov before normalisation:  69.91863991449047
Printing some Q and Qe and total Qs values:  [[0.201]
 [0.184]
 [0.305]
 [0.437]
 [0.259]
 [0.214]
 [0.264]] [[65.798]
 [63.286]
 [64.194]
 [61.752]
 [64.928]
 [63.254]
 [62.326]] [[0.799]
 [0.74 ]
 [0.877]
 [0.967]
 [0.843]
 [0.77 ]
 [0.803]]
printing an ep nov before normalisation:  85.81395683361824
Printing some Q and Qe and total Qs values:  [[0.325]
 [0.325]
 [0.325]
 [0.325]
 [0.325]
 [0.325]
 [0.325]] [[49.89]
 [49.89]
 [49.89]
 [49.89]
 [49.89]
 [49.89]
 [49.89]] [[0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]]
maxi score, test score, baseline:  -0.9883684684684685 -0.9380000000000001 -0.9380000000000001
probs:  [0.03822982269650214, 0.26684219016802346, 0.23942325177466095, 0.20302414797349333, 0.07772557692770701, 0.17475501045961309]
printing an ep nov before normalisation:  70.93662008985325
maxi score, test score, baseline:  -0.9883684684684685 -0.9380000000000001 -0.9380000000000001
probs:  [0.038123546884313846, 0.2660973960108159, 0.23979069099248304, 0.20330588457851875, 0.07771229487026829, 0.17497018666360004]
maxi score, test score, baseline:  -0.9883684684684685 -0.9380000000000001 -0.9380000000000001
probs:  [0.038123546884313846, 0.2660973960108159, 0.23979069099248304, 0.20330588457851875, 0.07771229487026829, 0.17497018666360004]
printing an ep nov before normalisation:  31.88616558189114
maxi score, test score, baseline:  -0.9883684684684685 -0.9380000000000001 -0.9380000000000001
using explorer policy with actor:  1
printing an ep nov before normalisation:  77.10259999821619
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.292]
 [0.329]
 [0.329]
 [0.311]
 [0.323]
 [0.327]] [[57.063]
 [60.683]
 [57.327]
 [57.54 ]
 [57.333]
 [56.356]
 [56.641]] [[0.835]
 [0.861]
 [0.841]
 [0.845]
 [0.823]
 [0.818]
 [0.827]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 59.18734603078792
printing an ep nov before normalisation:  60.89969364047198
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.191]
 [0.482]
 [0.378]
 [0.384]
 [0.482]
 [0.482]] [[54.586]
 [61.233]
 [54.7  ]
 [60.155]
 [75.951]
 [54.7  ]
 [54.7  ]] [[1.348]
 [1.239]
 [1.356]
 [1.397]
 [1.824]
 [1.356]
 [1.356]]
printing an ep nov before normalisation:  51.60635862177348
printing an ep nov before normalisation:  54.75034694210906
printing an ep nov before normalisation:  47.56640890830084
maxi score, test score, baseline:  -0.9883943820224719 -0.9380000000000001 -0.9380000000000001
probs:  [0.03783946527716891, 0.2641062817217871, 0.24108783222309, 0.20346871566150437, 0.07773861897006297, 0.17575908614638683]
actions average: 
K:  4  action  0 :  tensor([0.2322, 0.0326, 0.1288, 0.2073, 0.1235, 0.1158, 0.1598],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0065, 0.9467, 0.0142, 0.0099, 0.0038, 0.0087, 0.0102],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1551, 0.0213, 0.2505, 0.1929, 0.1271, 0.1189, 0.1342],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1414, 0.0042, 0.1588, 0.3176, 0.1126, 0.1426, 0.1228],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2058, 0.0127, 0.1376, 0.1330, 0.2709, 0.1082, 0.1318],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1689, 0.0616, 0.1782, 0.1854, 0.1247, 0.1385, 0.1427],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1840, 0.0040, 0.1655, 0.1720, 0.1339, 0.1339, 0.2067],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
printing an ep nov before normalisation:  54.365219280594346
maxi score, test score, baseline:  -0.9883943820224719 -0.9380000000000001 -0.9380000000000001
probs:  [0.03783946527716891, 0.2641062817217871, 0.24108783222309, 0.20346871566150437, 0.07773861897006297, 0.17575908614638683]
actor:  1 policy actor:  1  step number:  47 total reward:  0.19999999999999962  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.237]
 [0.237]
 [0.237]
 [0.237]
 [0.237]
 [0.237]
 [0.237]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.237]
 [0.237]
 [0.237]
 [0.237]
 [0.237]
 [0.237]
 [0.237]]
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]] [[97.613]
 [97.613]
 [97.613]
 [97.613]
 [97.613]
 [97.613]
 [97.613]] [[1.009]
 [1.009]
 [1.009]
 [1.009]
 [1.009]
 [1.009]
 [1.009]]
siam score:  -0.8256135
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.192051603311825
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.061]
 [0.026]
 [0.026]
 [0.026]
 [0.026]
 [0.026]] [[38.999]
 [44.37 ]
 [38.999]
 [38.999]
 [38.999]
 [38.999]
 [38.999]] [[0.442]
 [0.555]
 [0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.442]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9884201793721973 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.9884201793721973 -0.9380000000000001 -0.9380000000000001
probs:  [0.03959644736376306, 0.27642041566082093, 0.23491536910598385, 0.19932978392012013, 0.07826512595095682, 0.17147285799835524]
maxi score, test score, baseline:  -0.9884201793721973 -0.9380000000000001 -0.9380000000000001
probs:  [0.03959644736376306, 0.27642041566082093, 0.23491536910598385, 0.19932978392012013, 0.07826512595095682, 0.17147285799835524]
maxi score, test score, baseline:  -0.9884201793721973 -0.9380000000000001 -0.9380000000000001
siam score:  -0.8265406
printing an ep nov before normalisation:  72.22267233710784
Printing some Q and Qe and total Qs values:  [[0.323]
 [0.323]
 [0.323]
 [0.323]
 [0.323]
 [0.323]
 [0.323]] [[79.165]
 [79.165]
 [79.165]
 [79.165]
 [79.165]
 [79.165]
 [79.165]] [[1.179]
 [1.179]
 [1.179]
 [1.179]
 [1.179]
 [1.179]
 [1.179]]
siam score:  -0.82101333
actor:  1 policy actor:  1  step number:  48 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9884201793721973 -0.9380000000000001 -0.9380000000000001
probs:  [0.03866533409049653, 0.2699028666098775, 0.22941892075998882, 0.19470880661043105, 0.07662256254169138, 0.19068150938751485]
Printing some Q and Qe and total Qs values:  [[0.118]
 [0.157]
 [0.132]
 [0.13 ]
 [0.134]
 [0.132]
 [0.142]] [[19.347]
 [32.668]
 [17.449]
 [17.499]
 [17.474]
 [34.046]
 [17.167]] [[0.46 ]
 [0.911]
 [0.416]
 [0.415]
 [0.418]
 [0.929]
 [0.417]]
maxi score, test score, baseline:  -0.9884201793721973 -0.9380000000000001 -0.9380000000000001
probs:  [0.03866533409049653, 0.2699028666098775, 0.22941892075998882, 0.19470880661043105, 0.07662256254169138, 0.19068150938751485]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9884201793721973 -0.9380000000000001 -0.9380000000000001
probs:  [0.03869581762363941, 0.2701162868540727, 0.22960031333193068, 0.19407225137462983, 0.07668307480960698, 0.19083225600612053]
maxi score, test score, baseline:  -0.9884201793721973 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.9884201793721973 -0.9380000000000001 -0.9380000000000001
probs:  [0.038750027457712645, 0.2704958188450484, 0.22930311312436286, 0.19356002137709313, 0.0767906856953834, 0.19110033350039962]
printing an ep nov before normalisation:  31.09409127949358
maxi score, test score, baseline:  -0.9884201793721973 -0.9380000000000001 -0.9380000000000001
probs:  [0.038750027457712645, 0.2704958188450484, 0.22930311312436286, 0.19356002137709313, 0.0767906856953834, 0.19110033350039962]
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.414]
 [0.42 ]
 [0.417]
 [0.411]
 [0.413]
 [0.417]] [[35.763]
 [43.406]
 [34.117]
 [32.496]
 [31.638]
 [35.824]
 [35.348]] [[0.417]
 [0.414]
 [0.42 ]
 [0.417]
 [0.411]
 [0.413]
 [0.417]]
maxi score, test score, baseline:  -0.9884458612975392 -0.9380000000000001 -0.9380000000000001
probs:  [0.038648210875730934, 0.26978229371474105, 0.2296361448413183, 0.19381148620369526, 0.0767756791247286, 0.19134618523978592]
printing an ep nov before normalisation:  45.42444705963135
maxi score, test score, baseline:  -0.9884968819599109 -0.9380000000000001 -0.9380000000000001
probs:  [0.03854681996165995, 0.2690717516422575, 0.2299677842416942, 0.19406189972308122, 0.07676073529253401, 0.1915910091387731]
maxi score, test score, baseline:  -0.9884968819599109 -0.9380000000000001 -0.9380000000000001
probs:  [0.038571828245652855, 0.2692468378668231, 0.2301174127300708, 0.19418815251337734, 0.0768106218648416, 0.19106514677923428]
maxi score, test score, baseline:  -0.9884968819599109 -0.9380000000000001 -0.9380000000000001
probs:  [0.038571828245652855, 0.2692468378668231, 0.2301174127300708, 0.19418815251337734, 0.0768106218648416, 0.19106514677923428]
printing an ep nov before normalisation:  52.81844875638342
line 256 mcts: sample exp_bonus 0.6829517573245956
maxi score, test score, baseline:  -0.9884968819599109 -0.9380000000000001 -0.9380000000000001
probs:  [0.038571828245652855, 0.2692468378668231, 0.2301174127300708, 0.19418815251337734, 0.0768106218648416, 0.19106514677923428]
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.475]
 [0.474]
 [0.467]
 [0.417]
 [0.417]
 [0.473]] [[43.982]
 [53.886]
 [47.972]
 [52.393]
 [43.982]
 [43.982]
 [49.092]] [[0.417]
 [0.475]
 [0.474]
 [0.467]
 [0.417]
 [0.417]
 [0.473]]
maxi score, test score, baseline:  -0.9884968819599109 -0.9380000000000001 -0.9380000000000001
probs:  [0.038571828245652855, 0.2692468378668231, 0.2301174127300708, 0.19418815251337734, 0.0768106218648416, 0.19106514677923428]
maxi score, test score, baseline:  -0.9884968819599109 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.9884968819599109 -0.9380000000000001 -0.9380000000000001
probs:  [0.038571828245652855, 0.2692468378668231, 0.2301174127300708, 0.19418815251337734, 0.0768106218648416, 0.19106514677923428]
printing an ep nov before normalisation:  31.94380760192871
actor:  1 policy actor:  1  step number:  46 total reward:  0.12999999999999945  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  72.7721269003686
printing an ep nov before normalisation:  73.54172269533346
siam score:  -0.8187999
maxi score, test score, baseline:  -0.9885474501108648 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.9885474501108648 -0.9380000000000001 -0.9380000000000001
probs:  [0.03792821209781029, 0.2647407981310626, 0.22565742482001236, 0.20827864810299873, 0.07552673515327031, 0.18786818169484562]
UNIT TEST: sample policy line 217 mcts : [0.102 0.327 0.143 0.122 0.102 0.122 0.082]
maxi score, test score, baseline:  -0.9885725663716814 -0.9380000000000001 -0.9380000000000001
probs:  [0.03797614517102424, 0.26507638376433673, 0.2259434433962491, 0.20854262623738523, 0.07562235213856576, 0.18683904929243886]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  -0.9885725663716814 -0.9380000000000001 -0.9380000000000001
probs:  [0.038007857120879084, 0.2652984032189812, 0.2261326698768189, 0.2078799518666905, 0.0756856111962403, 0.18699550672038986]
maxi score, test score, baseline:  -0.9885725663716814 -0.9380000000000001 -0.9380000000000001
probs:  [0.038007857120879084, 0.2652984032189812, 0.2261326698768189, 0.2078799518666905, 0.0756856111962403, 0.18699550672038986]
printing an ep nov before normalisation:  35.14918202428522
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]]
maxi score, test score, baseline:  -0.9885975717439294 -0.9380000000000001 -0.9380000000000001
probs:  [0.03793089866123873, 0.2647589280491017, 0.2265922783591107, 0.2082875002036927, 0.07571611669622824, 0.1867142780306279]
printing an ep nov before normalisation:  38.342125415802
actions average: 
K:  1  action  0 :  tensor([0.2044, 0.0053, 0.1374, 0.1692, 0.1895, 0.1494, 0.1448],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0041, 0.8731, 0.0302, 0.0240, 0.0036, 0.0143, 0.0508],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1532, 0.0085, 0.2787, 0.1529, 0.1369, 0.1411, 0.1288],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1348, 0.0496, 0.1212, 0.2998, 0.1315, 0.1325, 0.1306],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1636, 0.0042, 0.1264, 0.1512, 0.2827, 0.1454, 0.1265],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1574, 0.0067, 0.1422, 0.1896, 0.1391, 0.2188, 0.1462],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1872, 0.0032, 0.1241, 0.1656, 0.1569, 0.1391, 0.2239],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  51.61247317085582
printing an ep nov before normalisation:  53.40258738620151
actor:  1 policy actor:  1  step number:  26 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9885975717439294 -0.9380000000000001 -0.9380000000000001
probs:  [0.03712022986054494, 0.2590833456900066, 0.2431657196492777, 0.20382309362269171, 0.07409504620037514, 0.182712564977104]
actions average: 
K:  1  action  0 :  tensor([0.1890, 0.0030, 0.1324, 0.1740, 0.2009, 0.1413, 0.1595],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0056, 0.9707, 0.0077, 0.0051, 0.0018, 0.0029, 0.0063],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1276, 0.0177, 0.3343, 0.1348, 0.1064, 0.1237, 0.1556],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1468, 0.0103, 0.1387, 0.2401, 0.1787, 0.1397, 0.1457],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1754, 0.0033, 0.1292, 0.1905, 0.1861, 0.1451, 0.1703],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1504, 0.0298, 0.1461, 0.1488, 0.1389, 0.2455, 0.1404],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1364, 0.0315, 0.1626, 0.1834, 0.1411, 0.1583, 0.1867],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  36 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  74.55736603225152
maxi score, test score, baseline:  -0.9885975717439294 -0.9380000000000001 -0.9380000000000001
probs:  [0.03678475274107022, 0.254359495879802, 0.2567360930904664, 0.2001917793004164, 0.07302854999394937, 0.17889932899429567]
printing an ep nov before normalisation:  51.21072533656253
maxi score, test score, baseline:  -0.9886224669603524 -0.9380000000000001 -0.9380000000000001
probs:  [0.03678475274107022, 0.254359495879802, 0.2567360930904664, 0.2001917793004164, 0.07302854999394937, 0.17889932899429567]
printing an ep nov before normalisation:  28.468082816796738
printing an ep nov before normalisation:  56.42065200341355
printing an ep nov before normalisation:  46.617774963378906
maxi score, test score, baseline:  -0.9886224669603524 -0.9380000000000001 -0.9380000000000001
probs:  [0.03680659403140087, 0.2545109937943865, 0.25688900726121705, 0.20031099771796973, 0.07307198960187038, 0.17841041759315543]
printing an ep nov before normalisation:  69.91190910339355
siam score:  -0.8181174
maxi score, test score, baseline:  -0.9886224669603524 -0.9380000000000001 -0.9380000000000001
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.464]
 [0.32 ]
 [0.322]
 [0.328]
 [0.317]
 [0.324]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.332]
 [0.464]
 [0.32 ]
 [0.322]
 [0.328]
 [0.317]
 [0.324]]
printing an ep nov before normalisation:  47.12791611985473
maxi score, test score, baseline:  -0.9886472527472527 -0.9380000000000001 -0.9380000000000001
probs:  [0.03679731726861584, 0.25475132906086706, 0.25682321835044686, 0.19968645050425796, 0.07328141823491437, 0.17866026658089784]
maxi score, test score, baseline:  -0.9886472527472527 -0.9380000000000001 -0.9380000000000001
probs:  [0.03679731726861584, 0.25475132906086706, 0.25682321835044686, 0.19968645050425796, 0.07328141823491437, 0.17866026658089784]
maxi score, test score, baseline:  -0.9886719298245614 -0.9380000000000001 -0.9380000000000001
probs:  [0.03673329850067759, 0.2549869312126077, 0.2563745946096937, 0.19984635508374712, 0.07326755408351786, 0.17879126650975616]
maxi score, test score, baseline:  -0.988696498905908 -0.9380000000000001 -0.9380000000000001
probs:  [0.03680989845618012, 0.25552031220036525, 0.25691087982548666, 0.19868402043579425, 0.07290971079460565, 0.1791651782875681]
maxi score, test score, baseline:  -0.988696498905908 -0.9380000000000001 -0.9380000000000001
probs:  [0.03680989845618012, 0.25552031220036525, 0.25691087982548666, 0.19868402043579425, 0.07290971079460565, 0.1791651782875681]
printing an ep nov before normalisation:  69.62213669493046
maxi score, test score, baseline:  -0.988696498905908 -0.9380000000000001 -0.9380000000000001
probs:  [0.036831668783213485, 0.25567190339204, 0.2570632964222637, 0.19880187506955402, 0.07295290903982056, 0.17867834729310825]
maxi score, test score, baseline:  -0.988696498905908 -0.9380000000000001 -0.9380000000000001
probs:  [0.036831668783213485, 0.25567190339204, 0.2570632964222637, 0.19880187506955402, 0.07295290903982056, 0.17867834729310825]
maxi score, test score, baseline:  -0.98872096069869 -0.9380000000000001 -0.9380000000000001
probs:  [0.036831668783213485, 0.25567190339204, 0.2570632964222637, 0.19880187506955402, 0.07295290903982056, 0.17867834729310825]
printing an ep nov before normalisation:  34.05265392170328
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.036806693729149194, 0.2551189892343946, 0.2568880218723494, 0.19917145927180502, 0.07301592528995723, 0.17899891060234466]
Printing some Q and Qe and total Qs values:  [[0.335]
 [0.335]
 [0.335]
 [0.367]
 [0.356]
 [0.335]
 [0.335]] [[62.068]
 [62.068]
 [62.068]
 [68.072]
 [64.633]
 [62.068]
 [62.068]] [[1.665]
 [1.665]
 [1.665]
 [1.924]
 [1.783]
 [1.665]
 [1.665]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.78284168243408
maxi score, test score, baseline:  -0.9887937093275488 -0.9380000000000001 -0.9380000000000001
probs:  [0.03682493296277613, 0.2548695785683257, 0.2570152948747673, 0.19977583929513223, 0.07316488073846342, 0.17834947356053515]
printing an ep nov before normalisation:  30.774904943500516
maxi score, test score, baseline:  -0.9887937093275488 -0.9380000000000001 -0.9380000000000001
probs:  [0.03686341205048588, 0.2540888488181885, 0.25728469056799447, 0.1999852071984537, 0.07324147002006255, 0.17853637134481487]
printing an ep nov before normalisation:  56.37924573945902
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  34 total reward:  0.24999999999999956  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9887937093275488 -0.9380000000000001 -0.9380000000000001
probs:  [0.03842898183855094, 0.2682565258386835, 0.25021718883294997, 0.1951619104761624, 0.07338224825245104, 0.17455314476120212]
printing an ep nov before normalisation:  46.887982324541355
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.981943562750978
actor:  1 policy actor:  1  step number:  35 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9887937093275488 -0.9380000000000001 -0.9380000000000001
probs:  [0.037196395880539704, 0.25962551900979786, 0.24410902405132404, 0.2186317391873074, 0.07134500273401262, 0.16909231913701833]
printing an ep nov before normalisation:  44.22070297071361
maxi score, test score, baseline:  -0.9887937093275488 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.9887937093275488 -0.9380000000000001 -0.9380000000000001
probs:  [0.037196395880539704, 0.25962551900979786, 0.24410902405132404, 0.2186317391873074, 0.07134500273401262, 0.16909231913701833]
printing an ep nov before normalisation:  57.821768500660866
maxi score, test score, baseline:  -0.9888177489177489 -0.9380000000000001 -0.9380000000000001
probs:  [0.03710007181927084, 0.25895052160963816, 0.2444443775628151, 0.21891393996187838, 0.07131992221089781, 0.16927116683549967]
maxi score, test score, baseline:  -0.9888177489177489 -0.9380000000000001 -0.9380000000000001
probs:  [0.03710007181927084, 0.25895052160963816, 0.2444443775628151, 0.21891393996187838, 0.07131992221089781, 0.16927116683549967]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9888416846652268 -0.9380000000000001 -0.9380000000000001
probs:  [0.03712383612913813, 0.2591169035224795, 0.2439590933117026, 0.21905458417028292, 0.07136568491221122, 0.1693798979541856]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.518]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]] [[46.292]
 [55.534]
 [46.292]
 [46.292]
 [46.292]
 [46.292]
 [46.292]] [[0.848]
 [1.04 ]
 [0.848]
 [0.848]
 [0.848]
 [0.848]
 [0.848]]
printing an ep nov before normalisation:  52.62499831742545
maxi score, test score, baseline:  -0.9888416846652268 -0.9380000000000001 -0.9380000000000001
probs:  [0.03712383612913813, 0.2591169035224795, 0.2439590933117026, 0.21905458417028292, 0.07136568491221122, 0.1693798979541856]
using explorer policy with actor:  1
printing an ep nov before normalisation:  61.473244680236405
maxi score, test score, baseline:  -0.9888416846652268 -0.9380000000000001 -0.9380000000000001
probs:  [0.03715546158745071, 0.25933832397516315, 0.24416755445652397, 0.21838747729308527, 0.07142658575093701, 0.16952459693683997]
maxi score, test score, baseline:  -0.9888416846652268 -0.9380000000000001 -0.9380000000000001
probs:  [0.0370595134815002, 0.25866596039069656, 0.24450142470121383, 0.2186678204703841, 0.07140179463807994, 0.16970348631812537]
printing an ep nov before normalisation:  0.17734817070831355
Printing some Q and Qe and total Qs values:  [[0.794]
 [0.885]
 [0.726]
 [0.824]
 [0.815]
 [0.844]
 [0.702]] [[20.412]
 [16.13 ]
 [23.47 ]
 [22.931]
 [25.8  ]
 [20.219]
 [21.344]] [[1.388]
 [1.355]
 [1.41 ]
 [1.492]
 [1.566]
 [1.433]
 [1.324]]
maxi score, test score, baseline:  -0.9888416846652268 -0.9380000000000001 -0.9380000000000001
probs:  [0.0371033731759943, 0.2589730361277497, 0.24415022049919471, 0.2189273878779352, 0.0714864448396399, 0.16935953747948626]
maxi score, test score, baseline:  -0.9888416846652268 -0.9380000000000001 -0.9380000000000001
probs:  [0.0371033731759943, 0.2589730361277497, 0.24415022049919471, 0.2189273878779352, 0.0714864448396399, 0.16935953747948626]
actor:  1 policy actor:  1  step number:  51 total reward:  0.09999999999999942  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9888416846652268 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.9888416846652268 -0.9380000000000001 -0.9380000000000001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  -0.9888416846652268 -0.9380000000000001 -0.9380000000000001
probs:  [0.036734593887357316, 0.2563910944800851, 0.2516832198136208, 0.2167449038224905, 0.07077469247851811, 0.1676714955179281]
maxi score, test score, baseline:  -0.9888416846652268 -0.9380000000000001 -0.9380000000000001
probs:  [0.036782660581606196, 0.2567276247668196, 0.2507013686468654, 0.21702936880474, 0.07086746228467887, 0.16789151491529006]
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.457]
 [0.496]
 [0.457]
 [0.457]
 [0.457]
 [0.369]] [[47.263]
 [47.263]
 [64.854]
 [47.263]
 [47.263]
 [47.263]
 [64.381]] [[0.829]
 [0.829]
 [1.297]
 [0.829]
 [0.829]
 [0.829]
 [1.159]]
printing an ep nov before normalisation:  31.537500699543287
siam score:  -0.821666
maxi score, test score, baseline:  -0.9888655172413793 -0.9380000000000001 -0.9380000000000001
probs:  [0.036782660581606196, 0.2567276247668196, 0.2507013686468654, 0.21702936880474, 0.07086746228467887, 0.16789151491529006]
maxi score, test score, baseline:  -0.9888655172413793 -0.9380000000000001 -0.9380000000000001
probs:  [0.036782660581606196, 0.2567276247668196, 0.2507013686468654, 0.21702936880474, 0.07086746228467887, 0.16789151491529006]
UNIT TEST: sample policy line 217 mcts : [0.082 0.286 0.245 0.102 0.102 0.102 0.082]
maxi score, test score, baseline:  -0.9888655172413793 -0.9380000000000001 -0.9380000000000001
probs:  [0.036782660581606196, 0.2567276247668196, 0.2507013686468654, 0.21702936880474, 0.07086746228467887, 0.16789151491529006]
actor:  1 policy actor:  1  step number:  37 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  36 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9888655172413793 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.9888655172413793 -0.9380000000000001 -0.9380000000000001
probs:  [0.039069998405362766, 0.24598564906376658, 0.2727575250454098, 0.20863907696639197, 0.0711356515021755, 0.1624120990168934]
printing an ep nov before normalisation:  64.04238918337236
maxi score, test score, baseline:  -0.9888892473118279 -0.9380000000000001 -0.9380000000000001
probs:  [0.03913845555260539, 0.24545530983141492, 0.2732368426913695, 0.208211495124261, 0.07126048525801366, 0.16269741154233566]
printing an ep nov before normalisation:  60.883344549055295
printing an ep nov before normalisation:  52.20425701722988
maxi score, test score, baseline:  -0.9889128755364807 -0.9380000000000001 -0.9380000000000001
probs:  [0.03913845555260539, 0.24545530983141492, 0.2732368426913695, 0.208211495124261, 0.07126048525801366, 0.16269741154233566]
siam score:  -0.82054716
maxi score, test score, baseline:  -0.9889128755364807 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.9889128755364807 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.9889128755364807 -0.9380000000000001 -0.9380000000000001
probs:  [0.039199801851755124, 0.2458411453298692, 0.273666372183631, 0.20696724067082456, 0.07137235215925176, 0.1629530878046683]
Printing some Q and Qe and total Qs values:  [[0.288]
 [0.432]
 [0.297]
 [0.319]
 [0.331]
 [0.292]
 [0.284]] [[80.609]
 [73.374]
 [77.257]
 [78.086]
 [81.679]
 [81.811]
 [75.912]] [[1.031]
 [1.044]
 [0.979]
 [1.016]
 [1.094]
 [1.056]
 [0.942]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  57.9156494140625
maxi score, test score, baseline:  -0.9889128755364807 -0.9380000000000001 -0.9380000000000001
probs:  [0.039199801851755124, 0.2458411453298692, 0.273666372183631, 0.20696724067082456, 0.07137235215925176, 0.1629530878046683]
maxi score, test score, baseline:  -0.9889364025695931 -0.9380000000000001 -0.9380000000000001
probs:  [0.039199801851755124, 0.2458411453298692, 0.273666372183631, 0.20696724067082456, 0.07137235215925176, 0.1629530878046683]
maxi score, test score, baseline:  -0.9889364025695931 -0.9380000000000001 -0.9380000000000001
probs:  [0.039199801851755124, 0.2458411453298692, 0.273666372183631, 0.20696724067082456, 0.07137235215925176, 0.1629530878046683]
maxi score, test score, baseline:  -0.9889364025695931 -0.9380000000000001 -0.9380000000000001
probs:  [0.039199801851755124, 0.2458411453298692, 0.273666372183631, 0.20696724067082456, 0.07137235215925176, 0.1629530878046683]
printing an ep nov before normalisation:  72.12778923498004
printing an ep nov before normalisation:  70.67837027394987
maxi score, test score, baseline:  -0.9889364025695931 -0.9380000000000001 -0.9380000000000001
probs:  [0.03907630642275305, 0.24629544207991705, 0.2728009747512486, 0.20731284165988348, 0.07133881475914593, 0.16317562032705196]
maxi score, test score, baseline:  -0.9889364025695931 -0.9380000000000001 -0.9380000000000001
probs:  [0.03905219627811577, 0.2457976292154724, 0.27263180404949716, 0.20768440767955662, 0.07139058293387361, 0.1634433798434844]
maxi score, test score, baseline:  -0.9889364025695931 -0.9380000000000001 -0.9380000000000001
probs:  [0.03905219627811577, 0.2457976292154724, 0.27263180404949716, 0.20768440767955662, 0.07139058293387361, 0.1634433798434844]
printing an ep nov before normalisation:  21.360661330922994
printing an ep nov before normalisation:  30.492146015167236
printing an ep nov before normalisation:  50.80324965973416
printing an ep nov before normalisation:  52.87308447812185
printing an ep nov before normalisation:  52.58947030484137
maxi score, test score, baseline:  -0.9889364025695931 -0.9380000000000001 -0.9380000000000001
probs:  [0.03899084053549566, 0.2460225236136101, 0.27220185178430245, 0.2078565322782633, 0.0713740014220579, 0.1635542503662706]
actor:  1 policy actor:  1  step number:  41 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  1  action  0 :  tensor([0.3190, 0.0128, 0.1272, 0.1470, 0.1535, 0.1160, 0.1245],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0188, 0.8458, 0.0218, 0.0335, 0.0158, 0.0166, 0.0477],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1612, 0.0634, 0.2233, 0.1541, 0.1425, 0.1354, 0.1201],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1401, 0.0559, 0.1295, 0.2876, 0.1367, 0.1216, 0.1286],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1948, 0.0247, 0.1518, 0.1741, 0.1732, 0.1375, 0.1439],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1620, 0.0130, 0.1488, 0.1551, 0.1357, 0.2567, 0.1287],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1574, 0.0026, 0.1349, 0.1630, 0.1434, 0.1205, 0.2782],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  73.81637172226243
maxi score, test score, baseline:  -0.9889364025695931 -0.9380000000000001 -0.9380000000000001
probs:  [0.04004207040617758, 0.24263802074110408, 0.27956810328153237, 0.204525157272034, 0.07128998660907387, 0.1619366616900781]
printing an ep nov before normalisation:  55.790458575241104
maxi score, test score, baseline:  -0.9889364025695931 -0.9380000000000001 -0.9380000000000001
probs:  [0.04004207040617758, 0.24263802074110408, 0.27956810328153237, 0.204525157272034, 0.07128998660907387, 0.1619366616900781]
Printing some Q and Qe and total Qs values:  [[0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]] [[69.533]
 [69.533]
 [69.533]
 [69.533]
 [69.533]
 [69.533]
 [69.533]] [[0.673]
 [0.673]
 [0.673]
 [0.673]
 [0.673]
 [0.673]
 [0.673]]
from probs:  [0.04004207040617758, 0.24263802074110408, 0.27956810328153237, 0.204525157272034, 0.07128998660907387, 0.1619366616900781]
using another actor
using explorer policy with actor:  0
printing an ep nov before normalisation:  39.99958038330078
maxi score, test score, baseline:  -0.9889364025695931 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.9889364025695931 -0.9380000000000001 -0.9380000000000001
probs:  [0.040096806231878036, 0.24203531817690985, 0.2799513541426414, 0.2048054851407253, 0.07095245777424158, 0.1621585785336038]
printing an ep nov before normalisation:  64.0819013794987
actions average: 
K:  1  action  0 :  tensor([0.2606, 0.0118, 0.1358, 0.1614, 0.1719, 0.1209, 0.1375],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0386, 0.7868, 0.0424, 0.0383, 0.0182, 0.0201, 0.0556],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1263, 0.0030, 0.3611, 0.1169, 0.1152, 0.1648, 0.1128],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1535, 0.0342, 0.1456, 0.2382, 0.1544, 0.1372, 0.1369],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1549, 0.0022, 0.1322, 0.1449, 0.2969, 0.1302, 0.1389],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1513, 0.0202, 0.1714, 0.1413, 0.1275, 0.2544, 0.1339],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1499, 0.0972, 0.1495, 0.1678, 0.1322, 0.1383, 0.1651],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]] [[35.84]
 [35.84]
 [35.84]
 [35.84]
 [35.84]
 [35.84]
 [35.84]] [[0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]]
printing an ep nov before normalisation:  43.493927415868434
printing an ep nov before normalisation:  63.1996787765409
Printing some Q and Qe and total Qs values:  [[0.114]
 [0.154]
 [0.123]
 [0.122]
 [0.114]
 [0.114]
 [0.114]] [[36.419]
 [38.395]
 [35.797]
 [35.787]
 [36.419]
 [36.419]
 [36.419]] [[0.591]
 [0.68 ]
 [0.584]
 [0.584]
 [0.591]
 [0.591]
 [0.591]]
printing an ep nov before normalisation:  65.40262494714767
siam score:  -0.8286855
maxi score, test score, baseline:  -0.988959829059829 -0.9380000000000001 -0.9380000000000001
probs:  [0.04013394204492666, 0.24133160500434517, 0.2802113727346099, 0.20499567507923117, 0.07101826593651621, 0.16230913920037088]
maxi score, test score, baseline:  -0.988959829059829 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.988959829059829 -0.9380000000000001 -0.9380000000000001
probs:  [0.04013394204492666, 0.24133160500434517, 0.2802113727346099, 0.20499567507923117, 0.07101826593651621, 0.16230913920037088]
actor:  0 policy actor:  0  step number:  42 total reward:  0.04999999999999938  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9867443496801706 -0.9380000000000001 -0.9380000000000001
probs:  [0.04013394204492666, 0.24133160500434517, 0.2802113727346099, 0.20499567507923117, 0.07101826593651621, 0.16230913920037088]
actor:  1 policy actor:  1  step number:  36 total reward:  0.14999999999999947  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  3  action  0 :  tensor([0.2574, 0.0076, 0.1336, 0.1434, 0.1978, 0.1187, 0.1415],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0200, 0.8154, 0.0421, 0.0324, 0.0119, 0.0364, 0.0419],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1337, 0.0303, 0.2562, 0.1430, 0.1301, 0.1667, 0.1400],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1217, 0.0615, 0.1075, 0.3353, 0.1240, 0.1254, 0.1246],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1430, 0.0129, 0.1268, 0.1493, 0.3279, 0.1199, 0.1203],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1641, 0.0010, 0.1552, 0.1782, 0.1651, 0.1775, 0.1588],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1488, 0.0045, 0.1533, 0.1945, 0.1595, 0.1643, 0.1752],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.328]
 [0.323]
 [0.321]
 [0.319]
 [0.268]
 [0.268]] [[57.08 ]
 [60.302]
 [57.848]
 [57.711]
 [58.043]
 [57.48 ]
 [57.48 ]] [[1.576]
 [1.735]
 [1.625]
 [1.618]
 [1.63 ]
 [1.554]
 [1.554]]
maxi score, test score, baseline:  -0.9867723404255319 -0.9380000000000001 -0.9380000000000001
probs:  [0.03951932640595171, 0.2527407888929253, 0.2759100059237688, 0.20197168574157393, 0.06984810381812306, 0.16001008921765708]
maxi score, test score, baseline:  -0.9867723404255319 -0.9380000000000001 -0.9380000000000001
probs:  [0.03951932640595171, 0.2527407888929253, 0.2759100059237688, 0.20197168574157393, 0.06984810381812306, 0.16001008921765708]
maxi score, test score, baseline:  -0.986800212314225 -0.9380000000000001 -0.9380000000000001
probs:  [0.03951932640595171, 0.2527407888929253, 0.2759100059237688, 0.20197168574157393, 0.06984810381812306, 0.16001008921765708]
printing an ep nov before normalisation:  65.85899583459742
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  -0.986800212314225 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.986800212314225 -0.9380000000000001 -0.9380000000000001
probs:  [0.03954880473432185, 0.25292985175345184, 0.2761164096683563, 0.20137490402434002, 0.06990028156754464, 0.16012974825198525]
actor:  1 policy actor:  1  step number:  32 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  1.0
from probs:  [0.03954880473432185, 0.25292985175345184, 0.2761164096683563, 0.20137490402434002, 0.06990028156754464, 0.16012974825198525]
Printing some Q and Qe and total Qs values:  [[0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]] [[60.917]
 [60.917]
 [60.917]
 [60.917]
 [60.917]
 [60.917]
 [60.917]] [[1.31]
 [1.31]
 [1.31]
 [1.31]
 [1.31]
 [1.31]
 [1.31]]
siam score:  -0.8302484
maxi score, test score, baseline:  -0.986800212314225 -0.9380000000000001 -0.9380000000000001
probs:  [0.040733541251247375, 0.24834524252485599, 0.28441836292564304, 0.19818422514943138, 0.07026438210866888, 0.1580542460401533]
Printing some Q and Qe and total Qs values:  [[0.099]
 [0.124]
 [0.096]
 [0.096]
 [0.1  ]
 [0.11 ]
 [0.114]] [[33.425]
 [38.198]
 [33.361]
 [33.36 ]
 [33.466]
 [34.636]
 [33.79 ]] [[0.637]
 [0.836]
 [0.631]
 [0.631]
 [0.639]
 [0.692]
 [0.665]]
maxi score, test score, baseline:  -0.986800212314225 -0.9380000000000001 -0.9380000000000001
probs:  [0.040733541251247375, 0.24834524252485599, 0.28441836292564304, 0.19818422514943138, 0.07026438210866888, 0.1580542460401533]
maxi score, test score, baseline:  -0.986800212314225 -0.9380000000000001 -0.9380000000000001
probs:  [0.040733541251247375, 0.24834524252485599, 0.28441836292564304, 0.19818422514943138, 0.07026438210866888, 0.1580542460401533]
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.482]
 [0.669]
 [0.594]
 [0.544]
 [0.539]
 [0.671]] [[47.864]
 [45.719]
 [43.772]
 [46.666]
 [50.917]
 [44.286]
 [42.719]] [[1.159]
 [1.035]
 [1.179]
 [1.168]
 [1.213]
 [1.06 ]
 [1.157]]
maxi score, test score, baseline:  -0.986800212314225 -0.9380000000000001 -0.9380000000000001
probs:  [0.040673115239031034, 0.24857907481524, 0.28399493212750565, 0.19834696175619518, 0.0702458116135152, 0.15816010444851286]
actor:  1 policy actor:  1  step number:  35 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  47.40888503953755
maxi score, test score, baseline:  -0.986800212314225 -0.9380000000000001 -0.9380000000000001
actor:  0 policy actor:  1  step number:  34 total reward:  0.3699999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9839254237288136 -0.9380000000000001 -0.9380000000000001
probs:  [0.04220940460808613, 0.2428217719910808, 0.29476026469617583, 0.19435186213392067, 0.07074465507265555, 0.15511204149808103]
maxi score, test score, baseline:  -0.9839254237288136 -0.9380000000000001 -0.9380000000000001
probs:  [0.04220940460808613, 0.2428217719910808, 0.29476026469617583, 0.19435186213392067, 0.07074465507265555, 0.15511204149808103]
using explorer policy with actor:  1
printing an ep nov before normalisation:  6.256334980909628
maxi score, test score, baseline:  -0.9839254237288136 -0.9380000000000001 -0.9380000000000001
probs:  [0.042285523092857304, 0.24145319344475488, 0.29529326376699466, 0.19470321601140791, 0.07087239564238308, 0.15539240804160215]
Printing some Q and Qe and total Qs values:  [[0.298]
 [0.394]
 [0.298]
 [0.298]
 [0.315]
 [0.298]
 [0.298]] [[46.094]
 [50.79 ]
 [46.094]
 [46.094]
 [63.343]
 [46.094]
 [46.094]] [[1.06 ]
 [1.277]
 [1.06 ]
 [1.06 ]
 [1.522]
 [1.06 ]
 [1.06 ]]
maxi score, test score, baseline:  -0.9839254237288136 -0.9380000000000001 -0.9380000000000001
probs:  [0.04238212948670227, 0.24111133867933512, 0.2959697238713873, 0.19375405181760835, 0.07103451862507482, 0.15574823751989214]
printing an ep nov before normalisation:  76.00006283023097
maxi score, test score, baseline:  -0.9839591966173362 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.9839591966173362 -0.9380000000000001 -0.9380000000000001
probs:  [0.04232234678258342, 0.24134143538325448, 0.2955507995500775, 0.19391507008893324, 0.07101653016769029, 0.15585381802746104]
maxi score, test score, baseline:  -0.9839591966173362 -0.9380000000000001 -0.9380000000000001
probs:  [0.04235151106295394, 0.24150817656721577, 0.29575501430765494, 0.1933583042789428, 0.07106553001724639, 0.15596146376598613]
actor:  1 policy actor:  1  step number:  31 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  65.6006338046488
maxi score, test score, baseline:  -0.9839591966173362 -0.9380000000000001 -0.9380000000000001
probs:  [0.04353867075825418, 0.23693086116335085, 0.3040739811530939, 0.1901746592256582, 0.0714215789543911, 0.15386024874525184]
siam score:  -0.82138497
actor:  1 policy actor:  1  step number:  27 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  54.40126640099557
maxi score, test score, baseline:  -0.9839591966173362 -0.9380000000000001 -0.9380000000000001
probs:  [0.0426307068246533, 0.2535304707896596, 0.29771608644045133, 0.18554782575220144, 0.0699303546582811, 0.1506445555347533]
printing an ep nov before normalisation:  35.479506811362505
maxi score, test score, baseline:  -0.9839928270042195 -0.9380000000000001 -0.9380000000000001
probs:  [0.04257150378579551, 0.25377777608041246, 0.29730123058991664, 0.1856963293820079, 0.06991082719467824, 0.1507423329671892]
printing an ep nov before normalisation:  17.892142749026842
maxi score, test score, baseline:  -0.9839928270042195 -0.9380000000000001 -0.9380000000000001
probs:  [0.04257150378579551, 0.25377777608041246, 0.29730123058991664, 0.1856963293820079, 0.06991082719467824, 0.1507423329671892]
maxi score, test score, baseline:  -0.9839928270042195 -0.9380000000000001 -0.9380000000000001
probs:  [0.04257150378579551, 0.25377777608041246, 0.29730123058991664, 0.1856963293820079, 0.06991082719467824, 0.1507423329671892]
maxi score, test score, baseline:  -0.9839928270042195 -0.9380000000000001 -0.9380000000000001
probs:  [0.04257150378579551, 0.25377777608041246, 0.29730123058991664, 0.1856963293820079, 0.06991082719467824, 0.1507423329671892]
printing an ep nov before normalisation:  63.05707811553211
maxi score, test score, baseline:  -0.9839928270042195 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.9839928270042195 -0.9380000000000001 -0.9380000000000001
actor:  1 policy actor:  1  step number:  41 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9839928270042195 -0.9380000000000001 -0.9380000000000001
probs:  [0.041966624634481677, 0.24927969503256087, 0.29306653074543726, 0.19836017892842273, 0.0690673784691891, 0.1482595921899083]
maxi score, test score, baseline:  -0.9840263157894737 -0.9380000000000001 -0.9380000000000001
probs:  [0.04195245122342195, 0.24922499453810412, 0.2929681480694288, 0.19835627624821248, 0.06919245956894471, 0.14830567035188794]
maxi score, test score, baseline:  -0.9840263157894737 -0.9380000000000001 -0.9380000000000001
probs:  [0.04195245122342195, 0.24922499453810412, 0.2929681480694288, 0.19835627624821248, 0.06919245956894471, 0.14830567035188794]
maxi score, test score, baseline:  -0.9840263157894737 -0.9380000000000001 -0.9380000000000001
probs:  [0.04195245122342195, 0.24922499453810412, 0.2929681480694288, 0.19835627624821248, 0.06919245956894471, 0.14830567035188794]
maxi score, test score, baseline:  -0.9840263157894737 -0.9380000000000001 -0.9380000000000001
probs:  [0.04195245122342195, 0.24922499453810412, 0.2929681480694288, 0.19835627624821248, 0.06919245956894471, 0.14830567035188794]
printing an ep nov before normalisation:  41.62415757728681
line 256 mcts: sample exp_bonus 36.49892748669839
maxi score, test score, baseline:  -0.9840263157894737 -0.9380000000000001 -0.9380000000000001
probs:  [0.041893445407464286, 0.24946130494023686, 0.29255468110792526, 0.19852011029981542, 0.06917226456756337, 0.14839819367699492]
actor:  0 policy actor:  1  step number:  32 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  61.30060331963942
maxi score, test score, baseline:  -0.9811394957983193 -0.9380000000000001 -0.9380000000000001
probs:  [0.04192240692302867, 0.24963421876228006, 0.29275748101593213, 0.1979647305435896, 0.06922014446936037, 0.1485010182858092]
printing an ep nov before normalisation:  72.89927051069007
Printing some Q and Qe and total Qs values:  [[0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]] [[70.782]
 [70.782]
 [70.782]
 [70.782]
 [70.782]
 [70.782]
 [70.782]] [[0.966]
 [0.966]
 [0.966]
 [0.966]
 [0.966]
 [0.966]
 [0.966]]
actions average: 
K:  2  action  0 :  tensor([0.2608, 0.0043, 0.1398, 0.1621, 0.1706, 0.1310, 0.1315],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0132, 0.9250, 0.0137, 0.0165, 0.0070, 0.0069, 0.0176],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1480, 0.0030, 0.2733, 0.1476, 0.1306, 0.1767, 0.1207],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1879, 0.0044, 0.1843, 0.1826, 0.1665, 0.1353, 0.1390],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1564, 0.0295, 0.1184, 0.1323, 0.3332, 0.1029, 0.1274],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1148, 0.0484, 0.1518, 0.1321, 0.1207, 0.2952, 0.1370],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1750, 0.0287, 0.1249, 0.1578, 0.1704, 0.1141, 0.2291],
       grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 68.94364837082216
actor:  1 policy actor:  1  step number:  37 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9811394957983193 -0.9380000000000001 -0.9380000000000001
actor:  1 policy actor:  1  step number:  37 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9811394957983193 -0.9380000000000001 -0.9380000000000001
probs:  [0.040594890959881086, 0.24083086304007317, 0.2834616927963706, 0.207422436782015, 0.06702546188073223, 0.16066465454092807]
actor:  1 policy actor:  1  step number:  29 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.367]
 [0.316]
 [0.326]
 [0.313]
 [0.304]
 [0.316]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.329]
 [0.367]
 [0.316]
 [0.326]
 [0.313]
 [0.304]
 [0.316]]
Printing some Q and Qe and total Qs values:  [[0.302]
 [0.519]
 [0.302]
 [0.348]
 [0.34 ]
 [0.302]
 [0.323]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.302]
 [0.519]
 [0.302]
 [0.348]
 [0.34 ]
 [0.302]
 [0.323]]
actor:  1 policy actor:  1  step number:  38 total reward:  0.24999999999999956  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  69.15557198979198
maxi score, test score, baseline:  -0.9811394957983193 -0.9380000000000001 -0.9380000000000001
probs:  [0.0394308307816881, 0.23390612232876157, 0.27531048618816495, 0.20145883841085124, 0.06510100848347279, 0.18479271380706136]
maxi score, test score, baseline:  -0.9811394957983193 -0.9380000000000001 -0.9380000000000001
probs:  [0.0394308307816881, 0.23390612232876157, 0.27531048618816495, 0.20145883841085124, 0.06510100848347279, 0.18479271380706136]
maxi score, test score, baseline:  -0.9811394957983193 -0.9380000000000001 -0.9380000000000001
probs:  [0.0394308307816881, 0.23390612232876157, 0.27531048618816495, 0.20145883841085124, 0.06510100848347279, 0.18479271380706136]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
from probs:  [0.0386428585938185, 0.2292186472386059, 0.2697927954148104, 0.19742197696319294, 0.06379831315697723, 0.2011254086325949]
actor:  1 policy actor:  1  step number:  32 total reward:  0.3299999999999996  reward:  1.0 rdn_beta:  0.667
from probs:  [0.038687946209601365, 0.228657364536371, 0.2701085166108468, 0.19765296539238203, 0.06353247415939825, 0.20136073309140046]
maxi score, test score, baseline:  -0.9811394957983193 -0.9380000000000001 -0.9380000000000001
printing an ep nov before normalisation:  57.49377910982222
maxi score, test score, baseline:  -0.9811788259958071 -0.9380000000000001 -0.9380000000000001
probs:  [0.03806828286620795, 0.24104391498825428, 0.2657693904778903, 0.19447836722764844, 0.0625135018994582, 0.19812654254054093]
maxi score, test score, baseline:  -0.9811788259958071 -0.9380000000000001 -0.9380000000000001
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.41 ]
 [0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.305]
 [0.41 ]
 [0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]]
printing an ep nov before normalisation:  50.61894976836392
printing an ep nov before normalisation:  50.31812954856849
maxi score, test score, baseline:  -0.981256993736952 -0.9380000000000001 -0.9380000000000001
probs:  [0.03806828286620795, 0.24104391498825428, 0.2657693904778903, 0.19447836722764844, 0.0625135018994582, 0.19812654254054093]
maxi score, test score, baseline:  -0.981256993736952 -0.9380000000000001 -0.9380000000000001
probs:  [0.038094076847441434, 0.2412077224636563, 0.26595001006631114, 0.19393110488606488, 0.06255591743303873, 0.19826116830348753]
using explorer policy with actor:  1
using another actor
from probs:  [0.038094076847441434, 0.2412077224636563, 0.26595001006631114, 0.19393110488606488, 0.06255591743303873, 0.19826116830348753]
maxi score, test score, baseline:  -0.981256993736952 -0.9380000000000001 -0.9380000000000001
probs:  [0.03812716313816006, 0.24054694743013438, 0.2661816932701514, 0.19410001714489117, 0.06261032441512995, 0.19843385460153312]
printing an ep nov before normalisation:  64.28320261829242
maxi score, test score, baseline:  -0.981256993736952 -0.9380000000000001 -0.9380000000000001
probs:  [0.03812716313816006, 0.24054694743013438, 0.2661816932701514, 0.19410001714489117, 0.06261032441512995, 0.19843385460153312]
maxi score, test score, baseline:  -0.9812958333333334 -0.9380000000000001 -0.9380000000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9812958333333334 -0.9380000000000001 -0.9380000000000001
probs:  [0.03806875123728882, 0.24074989400881358, 0.26577241149765096, 0.19424299281152935, 0.06258352445292528, 0.19858242599179216]
printing an ep nov before normalisation:  51.35448707802496
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9812958333333334 -0.9380000000000001 -0.9380000000000001
probs:  [0.03802305130080331, 0.24103214387796235, 0.26545214440540343, 0.1944499918260611, 0.062246222129705424, 0.19879644646006434]
maxi score, test score, baseline:  -0.9813345114345114 -0.9380000000000001 -0.9380000000000001
probs:  [0.03804576835019021, 0.24117657612759028, 0.26561121779852087, 0.19456649549430052, 0.062283462312270686, 0.19831647991712742]
actions average: 
K:  3  action  0 :  tensor([0.2911, 0.0097, 0.1246, 0.1387, 0.1703, 0.1252, 0.1404],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0263, 0.8708, 0.0221, 0.0295, 0.0131, 0.0120, 0.0261],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1381, 0.0021, 0.3340, 0.1399, 0.1197, 0.1404, 0.1257],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1476, 0.0081, 0.1372, 0.3513, 0.1314, 0.1064, 0.1179],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1997, 0.0532, 0.1359, 0.1413, 0.1976, 0.1235, 0.1490],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1388, 0.0401, 0.1260, 0.1670, 0.1154, 0.3107, 0.1020],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1532, 0.0703, 0.1769, 0.1492, 0.1375, 0.1521, 0.1609],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  44 total reward:  0.16999999999999948  reward:  1.0 rdn_beta:  0.667
from probs:  [0.03804576835019021, 0.24117657612759028, 0.26561121779852087, 0.19456649549430052, 0.062283462312270686, 0.19831647991712742]
maxi score, test score, baseline:  -0.9813345114345114 -0.9380000000000001 -0.9380000000000001
probs:  [0.037596686047805083, 0.2501571141517027, 0.2624665732448765, 0.19226339113078034, 0.06154727925245448, 0.19596895617238094]
actor:  1 policy actor:  1  step number:  43 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9813345114345114 -0.9380000000000001 -0.9380000000000001
probs:  [0.03711427809461814, 0.24693761669911019, 0.2590885698486903, 0.20265584349083526, 0.06075646527209799, 0.19344722659464816]
printing an ep nov before normalisation:  65.33111902966067
printing an ep nov before normalisation:  40.71593938277491
line 256 mcts: sample exp_bonus 0.1208923956568242
printing an ep nov before normalisation:  49.395228354753556
printing an ep nov before normalisation:  0.002802766832701309
maxi score, test score, baseline:  -0.9813730290456432 -0.9380000000000001 -0.9380000000000001
probs:  [0.037056429941062695, 0.2471421959916495, 0.25868324313271585, 0.20280503927490076, 0.06072818655939857, 0.19358490510027265]
printing an ep nov before normalisation:  57.290790917031124
Printing some Q and Qe and total Qs values:  [[0.277]
 [0.397]
 [0.118]
 [0.36 ]
 [0.385]
 [0.39 ]
 [0.404]] [[71.204]
 [80.066]
 [58.125]
 [66.185]
 [71.156]
 [44.698]
 [70.203]] [[0.679]
 [0.88 ]
 [0.401]
 [0.716]
 [0.787]
 [0.55 ]
 [0.797]]
printing an ep nov before normalisation:  39.32628513961867
maxi score, test score, baseline:  -0.981411387163561 -0.9380000000000001 -0.9380000000000001
probs:  [0.037095745228553065, 0.2471041872636859, 0.25895829102606055, 0.20207085753538, 0.060539298797457716, 0.19423162014886286]
maxi score, test score, baseline:  -0.981411387163561 -0.9380000000000001 -0.9380000000000001
probs:  [0.037095745228553065, 0.2471041872636859, 0.25895829102606055, 0.20207085753538, 0.060539298797457716, 0.19423162014886286]
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.53 ]
 [0.386]
 [0.431]
 [0.435]
 [0.386]
 [0.386]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.386]
 [0.53 ]
 [0.386]
 [0.431]
 [0.435]
 [0.386]
 [0.386]]
actor:  0 policy actor:  1  step number:  34 total reward:  0.20999999999999952  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9789927835051547 -0.9380000000000001 -0.9380000000000001
probs:  [0.037128535624098485, 0.24643686507539256, 0.2591879013606482, 0.2022500015726859, 0.06059288656899906, 0.19440380979817587]
Printing some Q and Qe and total Qs values:  [[0.452]
 [0.53 ]
 [0.458]
 [0.48 ]
 [0.486]
 [0.485]
 [0.467]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.452]
 [0.53 ]
 [0.458]
 [0.48 ]
 [0.486]
 [0.485]
 [0.467]]
maxi score, test score, baseline:  -0.9789927835051547 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.9789927835051547 -0.9380000000000001 -0.9380000000000001
probs:  [0.03715016982496022, 0.2465808990771867, 0.25933939194244116, 0.20236819587994545, 0.06062824230637199, 0.1939331009690945]
actions average: 
K:  0  action  0 :  tensor([0.2255, 0.0020, 0.1475, 0.1799, 0.1622, 0.1456, 0.1374],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0010,     0.9911,     0.0013,     0.0014,     0.0006,     0.0007,
            0.0040], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1506, 0.0088, 0.2800, 0.1645, 0.1321, 0.1358, 0.1283],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1507, 0.0262, 0.1327, 0.2469, 0.1432, 0.1412, 0.1591],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1473, 0.0014, 0.1367, 0.1642, 0.2841, 0.1378, 0.1284],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1618, 0.0062, 0.1418, 0.1742, 0.1360, 0.2519, 0.1280],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1470, 0.0120, 0.1350, 0.1896, 0.1299, 0.1278, 0.2588],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  42.955965802693555
maxi score, test score, baseline:  -0.9789927835051547 -0.9380000000000001 -0.9380000000000001
printing an ep nov before normalisation:  69.35134799241524
printing an ep nov before normalisation:  32.07883834838867
actions average: 
K:  0  action  0 :  tensor([0.2476, 0.0094, 0.1240, 0.1613, 0.1902, 0.1160, 0.1516],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0022, 0.9877, 0.0024, 0.0028, 0.0015, 0.0013, 0.0021],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1383, 0.0027, 0.3116, 0.1416, 0.1169, 0.1542, 0.1346],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1414, 0.0095, 0.1214, 0.3660, 0.1249, 0.1139, 0.1229],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1588, 0.0021, 0.1408, 0.1753, 0.2490, 0.1316, 0.1424],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1395, 0.0340, 0.1867, 0.1735, 0.1153, 0.2292, 0.1217],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1619, 0.0451, 0.1308, 0.1621, 0.1186, 0.1182, 0.2633],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.588]
 [0.499]
 [0.438]
 [0.397]
 [0.44 ]
 [0.664]] [[31.784]
 [60.867]
 [31.726]
 [41.414]
 [31.832]
 [32.296]
 [46.71 ]] [[0.907]
 [1.382]
 [0.908]
 [0.976]
 [0.807]
 [0.857]
 [1.271]]
actor:  1 policy actor:  1  step number:  35 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  29 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
using another actor
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]] [[91.108]
 [91.108]
 [91.108]
 [91.108]
 [91.108]
 [91.108]
 [91.108]] [[0.705]
 [0.705]
 [0.705]
 [0.705]
 [0.705]
 [0.705]
 [0.705]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9790786447638604 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.9790786447638604 -0.9380000000000001 -0.9380000000000001
probs:  [0.03791118019417787, 0.23946899046828585, 0.26467350507891096, 0.20919795613093967, 0.06050666508651653, 0.18824170304116902]
Printing some Q and Qe and total Qs values:  [[0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]] [[38.842]
 [38.842]
 [38.842]
 [38.842]
 [38.842]
 [38.842]
 [38.842]] [[1.584]
 [1.584]
 [1.584]
 [1.584]
 [1.584]
 [1.584]
 [1.584]]
actor:  1 policy actor:  1  step number:  36 total reward:  0.24999999999999956  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  62.5878651936849
printing an ep nov before normalisation:  33.90659095239246
maxi score, test score, baseline:  -0.9790786447638604 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.9790786447638604 -0.9380000000000001 -0.9380000000000001
probs:  [0.03871017132748824, 0.2361995362717032, 0.27027167020033355, 0.20724776451395974, 0.06094299473028263, 0.18662786295623257]
actor:  1 policy actor:  1  step number:  43 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  73.03301475644
maxi score, test score, baseline:  -0.9790786447638604 -0.9380000000000001 -0.9380000000000001
printing an ep nov before normalisation:  50.74000835418701
siam score:  -0.8181239
printing an ep nov before normalisation:  64.01142158065943
maxi score, test score, baseline:  -0.9790786447638604 -0.9380000000000001 -0.9380000000000001
probs:  [0.042948013555043074, 0.24240557757234793, 0.2809673683383494, 0.20819521559234394, 0.04025652324907933, 0.18522730169283652]
actor:  1 policy actor:  1  step number:  27 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.213]
 [0.834]
 [0.213]
 [0.246]
 [0.197]
 [0.213]
 [0.251]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.213]
 [0.834]
 [0.213]
 [0.246]
 [0.197]
 [0.213]
 [0.251]]
siam score:  -0.81635803
maxi score, test score, baseline:  -0.9790786447638604 -0.9380000000000001 -0.9380000000000001
probs:  [0.04373997842196572, 0.23743024035852012, 0.2896213938033799, 0.20504257214480195, 0.041491139267932416, 0.1826746760033999]
printing an ep nov before normalisation:  17.522169780628495
maxi score, test score, baseline:  -0.9790786447638604 -0.9380000000000001 -0.9380000000000001
probs:  [0.04380125283939876, 0.23776368038123416, 0.29002817069251985, 0.20454069255641052, 0.04154925370889881, 0.1823169498215377]
maxi score, test score, baseline:  -0.9791213114754099 -0.9380000000000001 -0.9380000000000001
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.24 ]
 [0.286]
 [0.286]
 [0.307]
 [0.286]
 [0.286]] [[36.708]
 [49.38 ]
 [36.708]
 [36.708]
 [38.941]
 [36.708]
 [36.708]] [[1.465]
 [1.826]
 [1.465]
 [1.465]
 [1.558]
 [1.465]
 [1.465]]
printing an ep nov before normalisation:  44.10842795850908
maxi score, test score, baseline:  -0.9791213114754099 -0.9380000000000001 -0.9380000000000001
probs:  [0.04382793602388531, 0.2379088835837127, 0.2902053099168895, 0.20466559497607947, 0.04157456081771521, 0.1818177146817179]
maxi score, test score, baseline:  -0.9791213114754099 -0.9380000000000001 -0.9380000000000001
probs:  [0.04376848733358565, 0.2381412465385074, 0.2897648259024438, 0.20484797477251218, 0.04151172405076921, 0.1819657414021818]
maxi score, test score, baseline:  -0.9791213114754099 -0.9380000000000001 -0.9380000000000001
probs:  [0.043795031620759726, 0.2382860320432285, 0.28994101516406523, 0.20497250724894964, 0.041536895499283355, 0.18146851842371345]
maxi score, test score, baseline:  -0.9791213114754099 -0.9380000000000001 -0.9380000000000001
probs:  [0.043795031620759726, 0.2382860320432285, 0.28994101516406523, 0.20497250724894964, 0.041536895499283355, 0.18146851842371345]
maxi score, test score, baseline:  -0.9791213114754099 -0.9380000000000001 -0.9380000000000001
probs:  [0.043795031620759726, 0.2382860320432285, 0.28994101516406523, 0.20497250724894964, 0.041536895499283355, 0.18146851842371345]
printing an ep nov before normalisation:  49.64104652404785
using explorer policy with actor:  1
printing an ep nov before normalisation:  57.631662623557446
siam score:  -0.8156511
maxi score, test score, baseline:  -0.9791213114754099 -0.9380000000000001 -0.9380000000000001
probs:  [0.04382143441421708, 0.23843004577212226, 0.2901162652528502, 0.20509637590786473, 0.04156193277196589, 0.18097394588097981]
maxi score, test score, baseline:  -0.9791213114754099 -0.9380000000000001 -0.9380000000000001
probs:  [0.04382143441421708, 0.23843004577212226, 0.2901162652528502, 0.20509637590786473, 0.04156193277196589, 0.18097394588097981]
maxi score, test score, baseline:  -0.9791213114754099 -0.9380000000000001 -0.9380000000000001
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 40.54527409604644
maxi score, test score, baseline:  -0.9791213114754099 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.9791213114754099 -0.9380000000000001 -0.9380000000000001
probs:  [0.04382143441421708, 0.23843004577212226, 0.2901162652528502, 0.20509637590786473, 0.04156193277196589, 0.18097394588097981]
actor:  1 policy actor:  1  step number:  39 total reward:  0.03999999999999937  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  32.60302049340447
actor:  1 policy actor:  1  step number:  37 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9791213114754099 -0.9380000000000001 -0.9380000000000001
probs:  [0.04375958848070875, 0.23276582294506012, 0.29007040618610863, 0.20114080699517517, 0.04155463885617619, 0.19070873653677112]
printing an ep nov before normalisation:  43.39950262006585
actor:  1 policy actor:  1  step number:  31 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9791213114754099 -0.9380000000000001 -0.9380000000000001
probs:  [0.043017327033397, 0.24638057516971257, 0.2850935767010489, 0.19740478192040964, 0.04084372114365257, 0.18726001803177925]
printing an ep nov before normalisation:  50.94761352342897
printing an ep nov before normalisation:  39.740654511545884
printing an ep nov before normalisation:  48.08957804908218
maxi score, test score, baseline:  -0.9791213114754099 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.9791213114754099 -0.9380000000000001 -0.9380000000000001
probs:  [0.04300698108747971, 0.24558230432686637, 0.2849371293663047, 0.19823444405204305, 0.04082154880610177, 0.18741759236120445]
actor:  1 policy actor:  1  step number:  40 total reward:  0.22999999999999954  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  23 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9791213114754099 -0.9380000000000001 -0.9380000000000001
probs:  [0.041634829964936476, 0.25753284604624077, 0.275822135805944, 0.20466048377681487, 0.03951934125170666, 0.18083036315435727]
Printing some Q and Qe and total Qs values:  [[0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]]
maxi score, test score, baseline:  -0.9791213114754099 -0.9380000000000001 -0.9380000000000001
probs:  [0.041634829964936476, 0.25753284604624077, 0.275822135805944, 0.20466048377681487, 0.03951934125170666, 0.18083036315435727]
printing an ep nov before normalisation:  59.97757732778073
maxi score, test score, baseline:  -0.9791213114754099 -0.9380000000000001 -0.9380000000000001
probs:  [0.04157606325008989, 0.2577781067167631, 0.27538933342970434, 0.20483128963598318, 0.039457595507622414, 0.1809676114598371]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.9791213114754099 -0.9380000000000001 -0.9380000000000001
probs:  [0.041616553027985545, 0.2570536044737348, 0.27565825948702916, 0.20503128026891723, 0.03949601553606028, 0.1811442872062728]
siam score:  -0.81302667
printing an ep nov before normalisation:  41.699801426727205
maxi score, test score, baseline:  -0.9791213114754099 -0.9380000000000001 -0.9380000000000001
probs:  [0.041616553027985545, 0.2570536044737348, 0.27565825948702916, 0.20503128026891723, 0.03949601553606028, 0.1811442872062728]
Printing some Q and Qe and total Qs values:  [[ 0.039]
 [-0.008]
 [ 0.129]
 [ 0.035]
 [ 0.019]
 [ 0.052]
 [ 0.07 ]] [[45.001]
 [44.614]
 [43.021]
 [45.9  ]
 [52.607]
 [39.855]
 [41.221]] [[0.225]
 [0.176]
 [0.302]
 [0.228]
 [0.26 ]
 [0.202]
 [0.229]]
maxi score, test score, baseline:  -0.9791638036809817 -0.9380000000000001 -0.9380000000000001
probs:  [0.04168141271160697, 0.2564848508163366, 0.27608904621647457, 0.20535164085999705, 0.03955755973360137, 0.18083548966198357]
printing an ep nov before normalisation:  66.19583428912308
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.36910933203335
maxi score, test score, baseline:  -0.9791638036809817 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.9791638036809817 -0.9380000000000001 -0.9380000000000001
probs:  [0.04134954124240544, 0.2565736886716535, 0.27618467784452805, 0.2054227586609761, 0.0395712221131637, 0.1808981114672731]
using another actor
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.9791638036809817 -0.9380000000000001 -0.9380000000000001
probs:  [0.04131469496256473, 0.25696882319916164, 0.27591529294621325, 0.20571570243319345, 0.039532823057744367, 0.18055266340112258]
maxi score, test score, baseline:  -0.9791638036809817 -0.9380000000000001 -0.9380000000000001
probs:  [0.041346899034233865, 0.25716966965545773, 0.27613095561708645, 0.20509508987769973, 0.039563633698508266, 0.18069375211701388]
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.414]
 [0.319]
 [0.316]
 [0.317]
 [0.314]
 [0.311]] [[64.694]
 [60.451]
 [61.588]
 [62.583]
 [64.326]
 [62.971]
 [59.299]] [[0.314]
 [0.414]
 [0.319]
 [0.316]
 [0.317]
 [0.314]
 [0.311]]
printing an ep nov before normalisation:  46.591241320917014
siam score:  -0.82482916
actions average: 
K:  2  action  0 :  tensor([0.2637, 0.0770, 0.1054, 0.1441, 0.1635, 0.0984, 0.1479],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0102, 0.9255, 0.0196, 0.0095, 0.0047, 0.0106, 0.0199],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1540, 0.0081, 0.4156, 0.1114, 0.1102, 0.1070, 0.0937],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1776, 0.0064, 0.1556, 0.1946, 0.1500, 0.1418, 0.1741],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1965, 0.0417, 0.1566, 0.1734, 0.1701, 0.1332, 0.1284],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1539, 0.0011, 0.1799, 0.1571, 0.1477, 0.2197, 0.1407],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1716, 0.0219, 0.1603, 0.1567, 0.1421, 0.1249, 0.2225],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9792061224489796 -0.9380000000000001 -0.9380000000000001
probs:  [0.041043041622930855, 0.2574085960271298, 0.27638750731536926, 0.2052856106993179, 0.039600285948900776, 0.18027495838635146]
actor:  0 policy actor:  1  step number:  32 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.729]
 [0.812]
 [0.666]
 [0.631]
 [0.608]
 [0.746]
 [0.825]] [[31.035]
 [34.537]
 [29.195]
 [29.005]
 [29.688]
 [30.389]
 [33.364]] [[1.306]
 [1.516]
 [1.177]
 [1.134]
 [1.136]
 [1.3  ]
 [1.487]]
maxi score, test score, baseline:  -0.9764580448065173 -0.9380000000000001 -0.9380000000000001
probs:  [0.041066920795566814, 0.2575587683751904, 0.2765487577066947, 0.20540535871598348, 0.03962332298104013, 0.17979687142552458]
printing an ep nov before normalisation:  40.02851595465279
maxi score, test score, baseline:  -0.9764580448065173 -0.9380000000000001 -0.9380000000000001
probs:  [0.041066920795566814, 0.2575587683751904, 0.2765487577066947, 0.20540535871598348, 0.03962332298104013, 0.17979687142552458]
actor:  1 policy actor:  1  step number:  41 total reward:  0.03999999999999937  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9764580448065173 -0.9380000000000001 -0.9380000000000001
probs:  [0.04154200856347942, 0.2556023365458316, 0.27999252533527863, 0.20403468583814477, 0.040114624460535146, 0.1787138192567305]
printing an ep nov before normalisation:  42.441501366789524
actor:  1 policy actor:  1  step number:  30 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  34 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  42.97744274139404
maxi score, test score, baseline:  -0.9764580448065173 -0.9380000000000001 -0.9380000000000001
probs:  [0.04024782043491748, 0.26335990857580055, 0.2712458159164268, 0.2131489375677239, 0.03886504772791659, 0.17313246977721458]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.833473371289294
printing an ep nov before normalisation:  43.460941314697266
actor:  1 policy actor:  1  step number:  32 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9765056910569107 -0.9380000000000001 -0.9380000000000001
probs:  [0.03958044059342341, 0.2586373389290473, 0.26670768192841054, 0.21008930907618756, 0.038216799839972004, 0.186768429632959]
printing an ep nov before normalisation:  49.29909887295973
maxi score, test score, baseline:  -0.9765056910569107 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.9765056910569107 -0.9380000000000001 -0.9380000000000001
probs:  [0.03961156451219841, 0.2588412991129713, 0.2669180096266794, 0.20946659377899376, 0.038246847843874554, 0.1869156851252826]
maxi score, test score, baseline:  -0.9765056910569107 -0.9380000000000001 -0.9380000000000001
probs:  [0.03955266924245931, 0.259077491676452, 0.2664923321938293, 0.20963632695073456, 0.03818611563679961, 0.18705506429972515]
printing an ep nov before normalisation:  39.17689085006714
Printing some Q and Qe and total Qs values:  [[1.214]
 [1.035]
 [1.049]
 [1.049]
 [1.049]
 [1.049]
 [1.049]] [[64.459]
 [68.513]
 [65.139]
 [65.139]
 [65.139]
 [65.139]
 [65.139]] [[1.74 ]
 [1.609]
 [1.584]
 [1.584]
 [1.584]
 [1.584]
 [1.584]]
maxi score, test score, baseline:  -0.9765056910569107 -0.9380000000000001 -0.9380000000000001
probs:  [0.03955266924245931, 0.259077491676452, 0.2664923321938293, 0.20963632695073456, 0.03818611563679961, 0.18705506429972515]
maxi score, test score, baseline:  -0.9765531440162272 -0.9380000000000001 -0.9380000000000001
probs:  [0.03959070243324048, 0.25836326767575674, 0.26674932432851023, 0.20983846251825358, 0.038222830331155845, 0.18723541271308308]
actor:  0 policy actor:  1  step number:  59 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  1.0
line 256 mcts: sample exp_bonus 35.674496544548
maxi score, test score, baseline:  -0.9741712550607288 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.9741712550607288 -0.9380000000000001 -0.9380000000000001
probs:  [0.039562930222012654, 0.25880150194362206, 0.26653396252092904, 0.2093873230419547, 0.03819214442157908, 0.18752213784990243]
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.408908367156982
maxi score, test score, baseline:  -0.9741712550607288 -0.9380000000000001 -0.9380000000000001
probs:  [0.039562930222012654, 0.25880150194362206, 0.26653396252092904, 0.2093873230419547, 0.03819214442157908, 0.18752213784990243]
printing an ep nov before normalisation:  63.1752617366
actor:  1 policy actor:  1  step number:  33 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9741712550607288 -0.9380000000000001 -0.9380000000000001
probs:  [0.03985797574389816, 0.2688076912890629, 0.2620332286446128, 0.2060940678183715, 0.038516153931663785, 0.18469088257239083]
actor:  1 policy actor:  1  step number:  35 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9741712550607288 -0.9380000000000001 -0.9380000000000001
probs:  [0.03989790281666244, 0.265708721903853, 0.26921884373310656, 0.20385490107633214, 0.03857447735719761, 0.1827451531128482]
printing an ep nov before normalisation:  35.72909194845024
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9741712550607288 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.9741712550607288 -0.9380000000000001 -0.9380000000000001
probs:  [0.03989790281666244, 0.265708721903853, 0.26921884373310656, 0.20385490107633214, 0.03857447735719761, 0.1827451531128482]
maxi score, test score, baseline:  -0.9741712550607288 -0.9380000000000001 -0.9380000000000001
printing an ep nov before normalisation:  23.321815479683146
maxi score, test score, baseline:  -0.9742232323232324 -0.9380000000000001 -0.9380000000000001
probs:  [0.039677933013273166, 0.26436235395868984, 0.2698262842470761, 0.20431478741268363, 0.03866125558521017, 0.18315738578306706]
actor:  1 policy actor:  1  step number:  40 total reward:  0.16999999999999948  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9742232323232324 -0.9380000000000001 -0.9380000000000001
probs:  [0.060903311490041113, 0.2587216441132377, 0.26406893152542643, 0.1992171226984754, 0.03783876703573364, 0.17925022313708563]
printing an ep nov before normalisation:  41.42074481557905
Printing some Q and Qe and total Qs values:  [[0.159]
 [0.072]
 [0.586]
 [0.193]
 [0.153]
 [0.163]
 [0.192]] [[29.411]
 [34.553]
 [33.672]
 [27.047]
 [28.35 ]
 [26.262]
 [31.895]] [[0.713]
 [0.836]
 [1.314]
 [0.651]
 [0.664]
 [0.588]
 [0.848]]
maxi score, test score, baseline:  -0.974275 -0.9380000000000001 -0.9380000000000001
probs:  [0.060903311490041113, 0.2587216441132377, 0.26406893152542643, 0.1992171226984754, 0.03783876703573364, 0.17925022313708563]
printing an ep nov before normalisation:  34.93755742122992
from probs:  [0.060903311490041113, 0.2587216441132377, 0.26406893152542643, 0.1992171226984754, 0.03783876703573364, 0.17925022313708563]
actor:  1 policy actor:  1  step number:  42 total reward:  0.1899999999999995  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  48.65771770477295
printing an ep nov before normalisation:  51.2597393052611
maxi score, test score, baseline:  -0.974275 -0.9380000000000001 -0.9380000000000001
probs:  [0.06033156697537726, 0.2553558368724313, 0.2615857394733089, 0.2076778027103685, 0.03748402117808665, 0.17756503279042724]
actor:  0 policy actor:  0  step number:  47 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.279]
 [0.366]
 [0.571]
 [0.399]
 [0.324]
 [0.418]
 [0.428]] [[47.887]
 [58.8  ]
 [59.982]
 [57.153]
 [64.079]
 [56.39 ]
 [53.172]] [[1.52 ]
 [1.893]
 [2.129]
 [1.883]
 [1.989]
 [1.882]
 [1.807]]
maxi score, test score, baseline:  -0.9719281124497992 -0.9380000000000001 -0.9380000000000001
probs:  [0.06033156697537726, 0.2553558368724313, 0.2615857394733089, 0.2076778027103685, 0.03748402117808665, 0.17756503279042724]
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.328]
 [0.328]
 [0.332]
 [0.328]
 [0.328]
 [0.328]] [[33.292]
 [29.953]
 [29.953]
 [29.036]
 [29.953]
 [29.953]
 [29.953]] [[0.314]
 [0.328]
 [0.328]
 [0.332]
 [0.328]
 [0.328]
 [0.328]]
printing an ep nov before normalisation:  25.061618675391106
maxi score, test score, baseline:  -0.9719841683366733 -0.9380000000000001 -0.9380000000000001
probs:  [0.06036549006974085, 0.25549966045053496, 0.2617330737391329, 0.20779475866311997, 0.03750506917574191, 0.17710194790172934]
maxi score, test score, baseline:  -0.97404 -0.9380000000000001 -0.9380000000000001
printing an ep nov before normalisation:  91.43729278179879
actor:  1 policy actor:  1  step number:  31 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.97404 -0.9380000000000001 -0.9380000000000001
probs:  [0.059603620328953104, 0.2522695650177717, 0.25842413270594206, 0.20516807607186516, 0.03703235780260362, 0.18750224807286425]
maxi score, test score, baseline:  -0.97404 -0.9380000000000001 -0.9380000000000001
probs:  [0.059603620328953104, 0.2522695650177717, 0.25842413270594206, 0.20516807607186516, 0.03703235780260362, 0.18750224807286425]
actor:  1 policy actor:  1  step number:  25 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  59.57645814120258
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  19 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  55.427026858032804
maxi score, test score, baseline:  -0.97404 -0.9380000000000001 -0.9380000000000001
probs:  [0.06063031505701044, 0.27535225706332367, 0.24722127613362044, 0.19724102675341174, 0.03944742242107203, 0.18010770257156186]
actor:  1 policy actor:  1  step number:  29 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  38 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9740400000000001 -0.9380000000000001 -0.9380000000000001
probs:  [0.04042779672374262, 0.28208394594732383, 0.2504242820031788, 0.1941746295941092, 0.04518241236561112, 0.18770693336603453]
actor:  1 policy actor:  1  step number:  38 total reward:  0.2699999999999996  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9740400000000001 -0.9380000000000001 -0.9380000000000001
probs:  [0.041422549055010055, 0.28905904561791296, 0.24655410522925217, 0.19160757258810532, 0.04606701586277137, 0.18528971164694819]
maxi score, test score, baseline:  -0.9740400000000001 -0.9380000000000001 -0.9380000000000001
probs:  [0.041454655503698805, 0.28928371343663334, 0.24674572133843883, 0.19097948713270982, 0.046102733834414066, 0.18543368875410518]
maxi score, test score, baseline:  -0.97404 -0.9380000000000001 -0.9380000000000001
probs:  [0.04148126763511054, 0.28946993426640294, 0.24690454653669464, 0.19110239734937248, 0.046132339455525584, 0.1849095147568938]
maxi score, test score, baseline:  -0.97404 -0.9380000000000001 -0.9380000000000001
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.421]
 [0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]] [[53.003]
 [61.196]
 [53.003]
 [53.003]
 [53.003]
 [53.003]
 [53.003]] [[0.341]
 [0.421]
 [0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  52.668415180962995
printing an ep nov before normalisation:  71.98241145526515
maxi score, test score, baseline:  -0.97404 -0.9380000000000001 -0.9380000000000001
probs:  [0.04150873043177458, 0.28966210770427786, 0.24640479339314278, 0.19122923643023812, 0.04616289142974062, 0.18503224061082607]
printing an ep nov before normalisation:  79.46388772753356
actor:  1 policy actor:  1  step number:  22 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.764806420425415
actor:  0 policy actor:  1  step number:  51 total reward:  0.05999999999999939  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  67.39058891007222
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.040819557918296, 0.28483955984710757, 0.24230276249356206, 0.18804623911074242, 0.04539619672202071, 0.1985956839082712]
actor:  1 policy actor:  1  step number:  36 total reward:  0.2899999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.040324406114539496, 0.2813746896970675, 0.23935556428227495, 0.1979195926528936, 0.04484534731924646, 0.19618039993397787]
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.040324406114539496, 0.2813746896970675, 0.23935556428227495, 0.1979195926528936, 0.04484534731924646, 0.19618039993397787]
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
actor:  1 policy actor:  1  step number:  39 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.272]
 [0.218]
 [0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]] [[36.162]
 [60.288]
 [36.162]
 [36.162]
 [36.162]
 [36.162]
 [36.162]] [[0.528]
 [1.132]
 [0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]]
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.03983604528424224, 0.27795605417983765, 0.23733215056778167, 0.19465026823546913, 0.04384343086898035, 0.20638205086368905]
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.03983604528424224, 0.27795605417983765, 0.23733215056778167, 0.19465026823546913, 0.04384343086898035, 0.20638205086368905]
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.03983604528424224, 0.27795605417983765, 0.23733215056778167, 0.19465026823546913, 0.04384343086898035, 0.20638205086368905]
printing an ep nov before normalisation:  48.795352060973705
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.03983604528424224, 0.27795605417983765, 0.23733215056778167, 0.19465026823546913, 0.04384343086898035, 0.20638205086368905]
using explorer policy with actor:  1
printing an ep nov before normalisation:  29.45535225600031
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.03983604528424224, 0.27795605417983765, 0.23733215056778167, 0.19465026823546913, 0.04384343086898035, 0.20638205086368905]
printing an ep nov before normalisation:  71.00332154767649
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.039771062442704445, 0.27750004687627067, 0.23782741350460235, 0.19502445362316673, 0.04378981595215622, 0.20608720760109964]
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.039771062442704445, 0.27750004687627067, 0.23782741350460235, 0.19502445362316673, 0.04378981595215622, 0.20608720760109964]
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.039771062442704445, 0.27750004687627067, 0.23782741350460235, 0.19502445362316673, 0.04378981595215622, 0.20608720760109964]
actor:  1 policy actor:  1  step number:  44 total reward:  0.3299999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.03927075908522009, 0.273999160691527, 0.23482726988892666, 0.2051763373619228, 0.04323878843583116, 0.20348768453657234]
printing an ep nov before normalisation:  51.17841917280796
actor:  1 policy actor:  1  step number:  37 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  61.63809886362376
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.038975740147542313, 0.27193475774938575, 0.2413932969115978, 0.20282754922461063, 0.04291385847070989, 0.2019547974961537]
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
printing an ep nov before normalisation:  54.135756492614746
actor:  1 policy actor:  1  step number:  36 total reward:  0.14999999999999947  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.03876758367277565, 0.27047817722504186, 0.2454550896021796, 0.20174131324416814, 0.04268459767607043, 0.20087323857976452]
printing an ep nov before normalisation:  49.99820999617711
printing an ep nov before normalisation:  32.376253604888916
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.03876758367277565, 0.27047817722504186, 0.2454550896021796, 0.20174131324416814, 0.04268459767607043, 0.20087323857976452]
printing an ep nov before normalisation:  50.439506106906464
printing an ep nov before normalisation:  59.447426993605575
printing an ep nov before normalisation:  61.00543415636233
printing an ep nov before normalisation:  49.22640800476074
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.03872779462033909, 0.27019849785447897, 0.246125129059893, 0.2022612261395346, 0.04265826085530938, 0.200029091470445]
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.03872779462033909, 0.27019849785447897, 0.246125129059893, 0.2022612261395346, 0.04265826085530938, 0.200029091470445]
actor:  1 policy actor:  1  step number:  30 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  1  action  0 :  tensor([0.3088, 0.0328, 0.1376, 0.1194, 0.1449, 0.1156, 0.1409],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0299, 0.9024, 0.0184, 0.0155, 0.0076, 0.0075, 0.0187],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1324, 0.0054, 0.3058, 0.1352, 0.1362, 0.1528, 0.1322],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1397, 0.0613, 0.1250, 0.2234, 0.1868, 0.1321, 0.1317],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1794, 0.0238, 0.1243, 0.1273, 0.3214, 0.1111, 0.1126],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1573, 0.0028, 0.1270, 0.1404, 0.1281, 0.3324, 0.1119],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1229, 0.0624, 0.1453, 0.1777, 0.1206, 0.1212, 0.2499],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.06490451740137984, 0.26469156334101673, 0.24070207631333013, 0.19699101372501884, 0.03794417246500645, 0.194766656754248]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  55.25982810447566
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.06490451740137984, 0.26469156334101673, 0.24070207631333013, 0.19699101372501884, 0.03794417246500645, 0.194766656754248]
actor:  1 policy actor:  1  step number:  36 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.0641842894882356, 0.2617497924305418, 0.23802705781748368, 0.20657716476393162, 0.03752373158368568, 0.19193796391612167]
actor:  1 policy actor:  1  step number:  25 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.52 ]
 [0.541]
 [0.497]
 [0.47 ]
 [0.46 ]
 [0.486]
 [0.5  ]] [[67.558]
 [65.646]
 [68.154]
 [70.902]
 [67.266]
 [68.467]
 [68.173]] [[1.518]
 [1.485]
 [1.511]
 [1.562]
 [1.45 ]
 [1.509]
 [1.515]]
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.06417623372206648, 0.2712515833558353, 0.23398161988841537, 0.20257751894836512, 0.0388790602062118, 0.18913398387910604]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.463]
 [0.478]
 [0.541]
 [0.527]
 [0.472]
 [0.523]] [[12.483]
 [14.669]
 [12.311]
 [12.024]
 [12.173]
 [12.472]
 [13.077]] [[0.693]
 [0.714]
 [0.688]
 [0.747]
 [0.735]
 [0.686]
 [0.746]]
actor:  1 policy actor:  1  step number:  31 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.06460818569102918, 0.27767113099550167, 0.23116670172275966, 0.19958265598145783, 0.039794722610003816, 0.18717660299924774]
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.06460818569102918, 0.27767113099550167, 0.23116670172275966, 0.19958265598145783, 0.039794722610003816, 0.18717660299924774]
printing an ep nov before normalisation:  49.54400104562122
Printing some Q and Qe and total Qs values:  [[0.205]
 [0.436]
 [0.194]
 [0.201]
 [0.244]
 [0.216]
 [0.297]] [[41.814]
 [45.501]
 [39.91 ]
 [37.768]
 [41.313]
 [37.882]
 [41.619]] [[0.987]
 [1.34 ]
 [0.914]
 [0.851]
 [1.01 ]
 [0.869]
 [1.073]]
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.06494617819852702, 0.27745448934028416, 0.23107111793218113, 0.19956929010418115, 0.03976339252349562, 0.187195531901331]
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.06532778207700052, 0.2774560923672065, 0.2311556622828483, 0.19893867262262757, 0.03976325741955667, 0.1873585332307604]
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.06540874669310577, 0.27780049749796926, 0.23082999821657182, 0.1991855675745802, 0.03981247364939741, 0.18696271636837547]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.06544956933171946, 0.27797414775244034, 0.23097427360402698, 0.1993100528606711, 0.03983728864218519, 0.186454667808957]
printing an ep nov before normalisation:  42.195763601379575
printing an ep nov before normalisation:  41.540025057615345
printing an ep nov before normalisation:  79.19025437250984
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.825]
 [1.002]
 [0.825]
 [0.825]
 [0.825]
 [0.825]
 [0.825]] [[63.717]
 [58.662]
 [63.717]
 [63.717]
 [63.717]
 [63.717]
 [63.717]] [[2.118]
 [2.105]
 [2.118]
 [2.118]
 [2.118]
 [2.118]
 [2.118]]
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.06544956933171946, 0.27797414775244034, 0.23097427360402698, 0.1993100528606711, 0.03983728864218519, 0.186454667808957]
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.69 ]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]] [[50.389]
 [58.383]
 [50.389]
 [50.389]
 [50.389]
 [50.389]
 [50.389]] [[0.841]
 [1.392]
 [0.841]
 [0.841]
 [0.841]
 [0.841]
 [0.841]]
printing an ep nov before normalisation:  0.21578850958547946
actor:  1 policy actor:  1  step number:  28 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  12.500602006912231
Printing some Q and Qe and total Qs values:  [[0.297]
 [0.362]
 [0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.297]
 [0.362]
 [0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]]
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.06457077316351925, 0.2742359479834285, 0.2272666908352895, 0.21067568282669308, 0.03930309189948614, 0.1839478132915835]
printing an ep nov before normalisation:  81.1570847780559
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.06457077316351925, 0.2742359479834285, 0.2272666908352895, 0.21067568282669308, 0.03930309189948614, 0.1839478132915835]
siam score:  -0.81544846
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
actor:  1 policy actor:  1  step number:  28 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.09244331184677775, 0.2662332802990755, 0.22005401550177847, 0.2045287124433134, 0.038159493570332534, 0.17858118633872228]
printing an ep nov before normalisation:  39.04469775856573
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.09251547049666889, 0.2664412902969019, 0.22022592724016418, 0.20390741449124672, 0.0381892186439385, 0.17872067883107978]
actor:  1 policy actor:  1  step number:  28 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.09179970244974217, 0.26437796302969335, 0.22779615939377237, 0.2007948085954384, 0.03789436476416409, 0.17733700176718956]
printing an ep nov before normalisation:  61.43556872803437
printing an ep nov before normalisation:  41.494863418401735
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.09179970244974217, 0.26437796302969335, 0.22779615939377237, 0.2007948085954384, 0.03789436476416409, 0.17733700176718956]
printing an ep nov before normalisation:  36.565842628479004
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.09179970244974217, 0.26437796302969335, 0.22779615939377237, 0.2007948085954384, 0.03789436476416409, 0.17733700176718956]
printing an ep nov before normalisation:  36.80062294006348
actor:  1 policy actor:  1  step number:  32 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.09176787524446924, 0.2711222527676901, 0.22525674337500437, 0.1972686666363291, 0.03885647831886008, 0.17572798365764708]
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.09176787524446924, 0.2711222527676901, 0.22525674337500437, 0.1972686666363291, 0.03885647831886008, 0.17572798365764708]
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.509]
 [0.505]
 [0.512]
 [0.513]
 [0.503]
 [0.488]] [[47.319]
 [43.734]
 [45.97 ]
 [48.137]
 [47.847]
 [47.216]
 [48.027]] [[0.517]
 [0.509]
 [0.505]
 [0.512]
 [0.513]
 [0.503]
 [0.488]]
printing an ep nov before normalisation:  26.908285479838966
actor:  1 policy actor:  1  step number:  40 total reward:  0.22999999999999954  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.762]
 [0.782]
 [0.718]
 [0.677]
 [0.634]
 [0.718]
 [0.718]] [[53.839]
 [51.46 ]
 [55.366]
 [56.407]
 [57.974]
 [55.366]
 [55.366]] [[2.146]
 [2.078]
 [2.158]
 [2.155]
 [2.169]
 [2.158]
 [2.158]]
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.09169296292817347, 0.2765627076438534, 0.22303488540815486, 0.19477417337978659, 0.03963255751283073, 0.17430271312720097]
printing an ep nov before normalisation:  55.250751295725344
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
printing an ep nov before normalisation:  41.5789035455525
printing an ep nov before normalisation:  37.95319000879924
actor:  1 policy actor:  1  step number:  37 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.25 ]
 [0.414]
 [0.25 ]
 [0.25 ]
 [0.25 ]
 [0.25 ]
 [0.25 ]] [[38.249]
 [57.667]
 [38.249]
 [38.249]
 [38.249]
 [38.249]
 [38.249]] [[0.563]
 [0.951]
 [0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.563]]
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.10702058919783577, 0.2716752733103936, 0.21986538067844014, 0.19127735247441685, 0.03893433969271222, 0.17122706464620135]
printing an ep nov before normalisation:  53.998011614540424
printing an ep nov before normalisation:  44.19250677682915
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
printing an ep nov before normalisation:  57.84867169008493
actor:  1 policy actor:  1  step number:  34 total reward:  0.2899999999999996  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  100.10259462729599
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
printing an ep nov before normalisation:  82.91939599215773
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.10594608351564318, 0.27783608666437515, 0.21672027260245647, 0.1897060437594041, 0.03981327419849461, 0.16997823925962638]
printing an ep nov before normalisation:  58.19018256207226
printing an ep nov before normalisation:  35.758343480356416
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.10594608351564318, 0.27783608666437515, 0.21672027260245647, 0.1897060437594041, 0.03981327419849461, 0.16997823925962638]
actor:  1 policy actor:  1  step number:  32 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  0.667
siam score:  -0.81948256
actor:  1 policy actor:  1  step number:  30 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
Printing some Q and Qe and total Qs values:  [[0.36 ]
 [0.42 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.349]
 [0.324]] [[ 0.   ]
 [33.674]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [29.18 ]
 [28.557]] [[0.051]
 [0.878]
 [0.051]
 [0.051]
 [0.051]
 [0.705]
 [0.666]]
printing an ep nov before normalisation:  51.14536520133125
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.10487362108192658, 0.28161856203103297, 0.21989267423943232, 0.18591736345835802, 0.04035239500572314, 0.1673453841835269]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.10487362108192658, 0.28161856203103297, 0.21989267423943232, 0.18591736345835802, 0.04035239500572314, 0.1673453841835269]
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.498]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]] [[29.309]
 [32.133]
 [29.309]
 [29.309]
 [29.309]
 [29.309]
 [29.309]] [[1.248]
 [1.498]
 [1.248]
 [1.248]
 [1.248]
 [1.248]
 [1.248]]
printing an ep nov before normalisation:  30.314960092501952
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.10497515428447182, 0.281169425838918, 0.21973297649934795, 0.18622685675164238, 0.040288365271069784, 0.16760722135455]
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.10497515428447182, 0.281169425838918, 0.21973297649934795, 0.18622685675164238, 0.040288365271069784, 0.16760722135455]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.10497515428447182, 0.281169425838918, 0.21973297649934795, 0.18622685675164238, 0.040288365271069784, 0.16760722135455]
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.1050316717209871, 0.28132092482328624, 0.21985135676197018, 0.18632717477485064, 0.04031001181597247, 0.16715886010293327]
actor:  1 policy actor:  1  step number:  33 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  54.37861520390953
Printing some Q and Qe and total Qs values:  [[0.257]
 [0.241]
 [0.263]
 [0.264]
 [0.264]
 [0.265]
 [0.264]] [[30.969]
 [32.19 ]
 [28.455]
 [28.233]
 [28.284]
 [27.948]
 [27.924]] [[0.257]
 [0.241]
 [0.263]
 [0.264]
 [0.264]
 [0.265]
 [0.264]]
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.10404796537180724, 0.2772606965297168, 0.2181511899601415, 0.19555454629165045, 0.03973016051773775, 0.16525544132894612]
actor:  1 policy actor:  1  step number:  30 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  43.65711770277099
printing an ep nov before normalisation:  52.358317117264065
printing an ep nov before normalisation:  50.502683696205835
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.10299297035259129, 0.27444715819832594, 0.21593776125561817, 0.19357053015013667, 0.03932815240556838, 0.17372342763775947]
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.10299297035259129, 0.27444715819832594, 0.21593776125561817, 0.19357053015013667, 0.03932815240556838, 0.17372342763775947]
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.10299297035259129, 0.27444715819832594, 0.21593776125561817, 0.19357053015013667, 0.03932815240556838, 0.17372342763775947]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
actor:  1 policy actor:  1  step number:  29 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.10230884300253999, 0.27262267705115173, 0.22114818355089855, 0.19228396511368614, 0.039067464213237445, 0.17256886706848612]
actor:  1 policy actor:  1  step number:  44 total reward:  0.3299999999999996  reward:  1.0 rdn_beta:  0.667
using another actor
printing an ep nov before normalisation:  63.80515098571777
Printing some Q and Qe and total Qs values:  [[0.238]
 [0.287]
 [0.243]
 [0.244]
 [0.244]
 [0.244]
 [0.241]] [[51.76 ]
 [58.83 ]
 [49.725]
 [50.233]
 [49.096]
 [48.573]
 [51.529]] [[0.238]
 [0.287]
 [0.243]
 [0.244]
 [0.244]
 [0.244]
 [0.241]]
actor:  1 policy actor:  1  step number:  26 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  69.35517117764086
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.1008550726925717, 0.27507952703588673, 0.21630530510143867, 0.19923146505890066, 0.039417236994458545, 0.16911139311674378]
printing an ep nov before normalisation:  58.25770322704586
printing an ep nov before normalisation:  63.923888206481934
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.1008550726925717, 0.27507952703588673, 0.21630530510143867, 0.19923146505890066, 0.039417236994458545, 0.16911139311674378]
printing an ep nov before normalisation:  73.3551777137876
printing an ep nov before normalisation:  63.47773141209403
line 256 mcts: sample exp_bonus 50.62987512064363
Printing some Q and Qe and total Qs values:  [[0.32 ]
 [0.32 ]
 [0.623]
 [0.32 ]
 [0.32 ]
 [0.32 ]
 [0.32 ]] [[45.071]
 [45.071]
 [ 0.123]
 [45.071]
 [45.071]
 [45.071]
 [45.071]] [[0.32 ]
 [0.32 ]
 [0.623]
 [0.32 ]
 [0.32 ]
 [0.32 ]
 [0.32 ]]
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.1008550726925717, 0.27507952703588673, 0.21630530510143867, 0.19923146505890066, 0.039417236994458545, 0.16911139311674378]
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
using another actor
actor:  1 policy actor:  1  step number:  34 total reward:  0.20999999999999952  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
using explorer policy with actor:  1
printing an ep nov before normalisation:  59.0630570386153
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.11440489071069772, 0.2713064031382269, 0.21333874750824713, 0.19580506495312663, 0.03887813740414252, 0.166266756285559]
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.11440489071069772, 0.2713064031382269, 0.21333874750824713, 0.19580506495312663, 0.03887813740414252, 0.166266756285559]
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
actor:  1 policy actor:  1  step number:  38 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.10085161585737236, 0.28462822083639133, 0.2167314951423599, 0.19538697776186095, 0.04080497759965037, 0.1615967128023652]
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.455]
 [0.291]
 [0.347]
 [0.324]
 [0.291]
 [0.337]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.326]
 [0.455]
 [0.291]
 [0.347]
 [0.324]
 [0.291]
 [0.337]]
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.10085161585737236, 0.28462822083639133, 0.2167314951423599, 0.19538697776186095, 0.04080497759965037, 0.1615967128023652]
printing an ep nov before normalisation:  65.82568899361631
printing an ep nov before normalisation:  66.19276672439894
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.10085161585737236, 0.28462822083639133, 0.2167314951423599, 0.19538697776186095, 0.04080497759965037, 0.1615967128023652]
printing an ep nov before normalisation:  49.976566468625826
printing an ep nov before normalisation:  73.4911280815923
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.805]
 [0.512]
 [0.569]
 [0.452]
 [0.512]
 [0.512]] [[52.733]
 [62.266]
 [52.733]
 [48.576]
 [48.582]
 [52.733]
 [52.733]] [[0.512]
 [0.805]
 [0.512]
 [0.569]
 [0.452]
 [0.512]
 [0.512]]
printing an ep nov before normalisation:  60.80768878492764
printing an ep nov before normalisation:  50.54067583355483
printing an ep nov before normalisation:  51.45514102994275
maxi score, test score, baseline:  -0.97192 -0.9380000000000001 -0.9380000000000001
probs:  [0.10085161585737236, 0.28462822083639133, 0.2167314951423599, 0.19538697776186095, 0.04080497759965037, 0.1615967128023652]
printing an ep nov before normalisation:  37.13346481323242
actor:  0 policy actor:  1  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  18 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  19 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  19 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  20 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.90834 0.5894999999999999 0.5894999999999999
printing an ep nov before normalisation:  18.126076459884644
siam score:  -0.8004113
maxi score, test score, baseline:  -0.90834 0.5894999999999999 0.5894999999999999
probs:  [0.08560486424087545, 0.5719335876961822, 0.08560486424087545, 0.08560486424087545, 0.08560486424087545, 0.08564695534031602]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  43 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.90834 0.5894999999999999 0.5894999999999999
probs:  [0.08561228214462753, 0.5718964853150178, 0.08561228214462753, 0.08561228214462753, 0.08561228214462753, 0.08565438610647219]
maxi score, test score, baseline:  -0.90834 0.5894999999999999 0.5894999999999999
probs:  [0.08561228214462753, 0.5718964853150178, 0.08561228214462753, 0.08561228214462753, 0.08561228214462753, 0.08565438610647219]
printing an ep nov before normalisation:  58.92202080441889
Printing some Q and Qe and total Qs values:  [[-0.067]
 [-0.067]
 [-0.067]
 [-0.067]
 [-0.067]
 [-0.067]
 [-0.067]] [[46.87]
 [46.87]
 [46.87]
 [46.87]
 [46.87]
 [46.87]
 [46.87]] [[0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]]
printing an ep nov before normalisation:  37.91016227736307
siam score:  -0.8052411
using another actor
maxi score, test score, baseline:  -0.90834 0.5894999999999999 0.5894999999999999
using another actor
from probs:  [0.08564189706498897, 0.5717478554542681, 0.08564189706498897, 0.08564189706498897, 0.08564189706498897, 0.08568455628577605]
printing an ep nov before normalisation:  54.27757740020752
maxi score, test score, baseline:  -0.90834 0.5894999999999999 0.5894999999999999
printing an ep nov before normalisation:  27.28447461484675
actions average: 
K:  0  action  0 :  tensor([0.2912, 0.0083, 0.1092, 0.1694, 0.1748, 0.1033, 0.1438],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0141, 0.9058, 0.0143, 0.0180, 0.0065, 0.0094, 0.0318],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1622, 0.0281, 0.2871, 0.1548, 0.1120, 0.1229, 0.1330],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1496, 0.0400, 0.1264, 0.2778, 0.1270, 0.1170, 0.1623],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1447, 0.0190, 0.1033, 0.1255, 0.3936, 0.0990, 0.1148],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1163, 0.0200, 0.1655, 0.1245, 0.0953, 0.3598, 0.1184],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1737, 0.0734, 0.1404, 0.1811, 0.1164, 0.1219, 0.1930],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  45.78214700150833
maxi score, test score, baseline:  -0.90834 0.5894999999999999 0.5894999999999999
maxi score, test score, baseline:  -0.90834 0.5894999999999999 0.5894999999999999
probs:  [0.08564929740314473, 0.5717107150123489, 0.08564929740314473, 0.08564929740314473, 0.08564929740314473, 0.08569209537507216]
printing an ep nov before normalisation:  49.76043534489318
actor:  0 policy actor:  1  step number:  41 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9055000000000001 0.5894999999999999 0.5894999999999999
probs:  [0.08564929740314473, 0.5717107150123489, 0.08564929740314473, 0.08564929740314473, 0.08564929740314473, 0.08569209537507216]
siam score:  -0.8072997
maxi score, test score, baseline:  -0.9055000000000001 0.5894999999999999 0.5894999999999999
probs:  [0.08564929740314473, 0.5717107150123489, 0.08564929740314473, 0.08564929740314473, 0.08564929740314473, 0.08569209537507216]
Printing some Q and Qe and total Qs values:  [[-0.206]
 [-0.202]
 [-0.207]
 [-0.207]
 [-0.207]
 [-0.207]
 [-0.207]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.206]
 [-0.202]
 [-0.207]
 [-0.207]
 [-0.207]
 [-0.207]
 [-0.207]]
maxi score, test score, baseline:  -0.9055000000000001 0.5894999999999999 0.5894999999999999
maxi score, test score, baseline:  -0.9055000000000001 0.5894999999999999 0.5894999999999999
probs:  [0.0856788851992147, 0.5715622212816176, 0.0856788851992147, 0.0856788851992147, 0.0856788851992147, 0.0857222379215233]
Printing some Q and Qe and total Qs values:  [[-0.208]
 [-0.192]
 [-0.206]
 [-0.206]
 [-0.205]
 [-0.204]
 [-0.206]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.208]
 [-0.192]
 [-0.206]
 [-0.206]
 [-0.205]
 [-0.204]
 [-0.206]]
printing an ep nov before normalisation:  62.845029660975285
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9055000000000001 0.5894999999999999 0.5894999999999999
maxi score, test score, baseline:  -0.9055000000000001 0.5894999999999999 0.5894999999999999
probs:  [0.08569367096853182, 0.5714880152122492, 0.08569367096853182, 0.08569367096853182, 0.08569367096853182, 0.0857373009136236]
printing an ep nov before normalisation:  49.95351113547018
maxi score, test score, baseline:  -0.9055000000000001 0.5894999999999999 0.5894999999999999
probs:  [0.08570845132267307, 0.5714138363202904, 0.08570845132267307, 0.08570845132267307, 0.08570845132267307, 0.08575235838901747]
actor:  0 policy actor:  1  step number:  33 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.112]
 [-0.112]
 [-0.112]
 [-0.112]
 [-0.112]
 [-0.112]
 [-0.112]] [[47.134]
 [47.134]
 [47.134]
 [47.134]
 [47.134]
 [47.134]
 [47.134]] [[0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]]
maxi score, test score, baseline:  -0.9027000000000001 0.5894999999999999 0.5894999999999999
probs:  [0.08570845132267307, 0.5714138363202904, 0.08570845132267307, 0.08570845132267307, 0.08570845132267307, 0.08575235838901747]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.172]
 [-0.152]
 [-0.171]
 [-0.174]
 [-0.177]
 [-0.177]
 [-0.177]] [[74.672]
 [70.143]
 [73.876]
 [75.759]
 [75.23 ]
 [75.23 ]
 [75.23 ]] [[1.323]
 [1.169]
 [1.293]
 [1.362]
 [1.339]
 [1.339]
 [1.339]]
maxi score, test score, baseline:  -0.9027000000000001 0.5894999999999999 0.5894999999999999
Printing some Q and Qe and total Qs values:  [[-0.087]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]] [[61.438]
 [61.438]
 [61.438]
 [61.438]
 [61.438]
 [61.438]
 [61.438]] [[1.066]
 [1.066]
 [1.066]
 [1.066]
 [1.066]
 [1.066]
 [1.066]]
printing an ep nov before normalisation:  64.2993174400212
actions average: 
K:  2  action  0 :  tensor([0.2805, 0.0825, 0.1410, 0.1315, 0.1308, 0.1064, 0.1272],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0205, 0.8358, 0.0493, 0.0243, 0.0153, 0.0302, 0.0246],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1593, 0.0441, 0.2800, 0.1356, 0.1239, 0.1151, 0.1420],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1054, 0.0265, 0.1141, 0.4052, 0.1326, 0.1034, 0.1127],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1670, 0.0707, 0.1147, 0.1516, 0.2437, 0.1084, 0.1439],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1446, 0.0834, 0.1744, 0.1684, 0.1313, 0.1453, 0.1525],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1427, 0.0583, 0.1533, 0.1588, 0.1330, 0.1429, 0.2110],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  31 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  60.69478932989701
actor:  1 policy actor:  1  step number:  37 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9027000000000001 0.5894999999999999 0.5894999999999999
probs:  [0.08573061170693619, 0.5713026189073448, 0.08573061170693619, 0.08573061170693619, 0.08573061170693619, 0.08577493426491059]
Printing some Q and Qe and total Qs values:  [[-0.217]
 [-0.182]
 [-0.196]
 [-0.195]
 [-0.206]
 [-0.196]
 [-0.199]] [[38.999]
 [40.32 ]
 [42.571]
 [35.79 ]
 [36.058]
 [42.571]
 [42.083]] [[1.074]
 [1.186]
 [1.302]
 [0.91 ]
 [0.914]
 [1.302]
 [1.271]]
printing an ep nov before normalisation:  66.63396181893772
Printing some Q and Qe and total Qs values:  [[-0.186]
 [-0.197]
 [-0.175]
 [-0.169]
 [-0.168]
 [-0.183]
 [-0.165]] [[27.644]
 [28.396]
 [27.714]
 [22.853]
 [22.291]
 [26.133]
 [23.34 ]] [[0.594]
 [0.604]
 [0.607]
 [0.476]
 [0.461]
 [0.555]
 [0.494]]
printing an ep nov before normalisation:  0.7132129032463297
from probs:  [0.08573061170693619, 0.5713026189073448, 0.08573061170693619, 0.08573061170693619, 0.08573061170693619, 0.08577493426491059]
maxi score, test score, baseline:  -0.9027000000000001 0.5894999999999999 0.5894999999999999
probs:  [0.08573799579732351, 0.5712655600089035, 0.08573799579732351, 0.08573799579732351, 0.08573799579732351, 0.0857824568018026]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
line 256 mcts: sample exp_bonus 47.54410743713379
printing an ep nov before normalisation:  41.51941532122822
maxi score, test score, baseline:  -0.9027000000000001 0.5894999999999999 0.5894999999999999
maxi score, test score, baseline:  -0.9027000000000001 0.5894999999999999 0.5894999999999999
probs:  [0.08573799579732351, 0.5712655600089035, 0.08573799579732351, 0.08573799579732351, 0.08573799579732351, 0.0857824568018026]
printing an ep nov before normalisation:  47.34315969759194
printing an ep nov before normalisation:  45.37324037703863
printing an ep nov before normalisation:  0.02090228404606857
actor:  0 policy actor:  0  step number:  33 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.89998 0.5894999999999999 0.5894999999999999
probs:  [0.08574537853614604, 0.5712285078936269, 0.08574537853614604, 0.08574537853614604, 0.08574537853614604, 0.08578997796178885]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  0.0699999999999994  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  45.20081078165621
maxi score, test score, baseline:  -0.89998 0.5894999999999999 0.5894999999999999
probs:  [0.08574538994303534, 0.5712285839858698, 0.08574538994303534, 0.08574538994303534, 0.08574538994303534, 0.08578985624198873]
Printing some Q and Qe and total Qs values:  [[0.437]
 [0.52 ]
 [0.434]
 [0.539]
 [0.443]
 [0.442]
 [0.438]] [[51.61 ]
 [53.634]
 [51.552]
 [51.662]
 [48.009]
 [47.909]
 [47.649]] [[0.437]
 [0.52 ]
 [0.434]
 [0.539]
 [0.443]
 [0.442]
 [0.438]]
maxi score, test score, baseline:  -0.89998 0.5894999999999999 0.5894999999999999
probs:  [0.08576015144023565, 0.5711545005596286, 0.08576015144023565, 0.08576015144023565, 0.08576015144023565, 0.08580489367942876]
maxi score, test score, baseline:  -0.89998 0.5894999999999999 0.5894999999999999
probs:  [0.08576015144023565, 0.5711545005596286, 0.08576015144023565, 0.08576015144023565, 0.08576015144023565, 0.08580489367942876]
printing an ep nov before normalisation:  47.046792760271885
maxi score, test score, baseline:  -0.89998 0.5894999999999999 0.5894999999999999
probs:  [0.08576015144023565, 0.5711545005596286, 0.08576015144023565, 0.08576015144023565, 0.08576015144023565, 0.08580489367942876]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.89998 0.5894999999999999 0.5894999999999999
probs:  [0.08576754161046374, 0.5711175453421966, 0.08576754161046374, 0.08576754161046374, 0.08576754161046374, 0.08581228821594841]
siam score:  -0.81125563
actor:  1 policy actor:  1  step number:  44 total reward:  0.1899999999999995  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.89998 0.5894999999999999 0.5894999999999999
printing an ep nov before normalisation:  44.43049492144538
actor:  1 policy actor:  1  step number:  39 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  38 total reward:  0.20999999999999952  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.005]
 [ 0.006]
 [-0.006]
 [-0.   ]
 [ 0.004]
 [ 0.003]
 [ 0.003]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.005]
 [ 0.006]
 [-0.006]
 [-0.   ]
 [ 0.004]
 [ 0.003]
 [ 0.003]]
printing an ep nov before normalisation:  59.08894538879395
printing an ep nov before normalisation:  56.70908035252372
maxi score, test score, baseline:  -0.89998 0.5894999999999999 0.5894999999999999
probs:  [0.08580441516114029, 0.5709324903572395, 0.08580441516114029, 0.08580441516114029, 0.08580441516114029, 0.08584984899819945]
Printing some Q and Qe and total Qs values:  [[-0.101]
 [-0.089]
 [-0.101]
 [-0.101]
 [-0.101]
 [-0.101]
 [-0.101]] [[59.447]
 [59.955]
 [59.447]
 [59.447]
 [59.447]
 [59.447]
 [59.447]] [[1.47 ]
 [1.505]
 [1.47 ]
 [1.47 ]
 [1.47 ]
 [1.47 ]
 [1.47 ]]
printing an ep nov before normalisation:  53.36480275886038
printing an ep nov before normalisation:  42.114629609085696
printing an ep nov before normalisation:  0.6632188328177335
maxi score, test score, baseline:  -0.89998 0.5894999999999999 0.5894999999999999
probs:  [0.08580441516114029, 0.5709324903572395, 0.08580441516114029, 0.08580441516114029, 0.08580441516114029, 0.08584984899819945]
maxi score, test score, baseline:  -0.89998 0.5894999999999999 0.5894999999999999
probs:  [0.08580441516114029, 0.5709324903572395, 0.08580441516114029, 0.08580441516114029, 0.08580441516114029, 0.08584984899819945]
maxi score, test score, baseline:  -0.89998 0.5894999999999999 0.5894999999999999
probs:  [0.08580441516114029, 0.5709324903572395, 0.08580441516114029, 0.08580441516114029, 0.08580441516114029, 0.08584984899819945]
actor:  1 policy actor:  1  step number:  30 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  0.667
from probs:  [0.08580442672044368, 0.5709325673731184, 0.08580442672044368, 0.08580442672044368, 0.08580442672044368, 0.08584972574510694]
maxi score, test score, baseline:  -0.89998 0.5894999999999999 0.5894999999999999
probs:  [0.08581179742035204, 0.5708955769102635, 0.08581179742035204, 0.08581179742035204, 0.08581179742035204, 0.08585723340832833]
actor:  0 policy actor:  0  step number:  41 total reward:  0.0799999999999994  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.452]
 [0.526]
 [0.356]
 [0.336]
 [0.333]
 [0.333]
 [0.337]] [[48.227]
 [46.243]
 [43.04 ]
 [44.166]
 [45.025]
 [44.291]
 [43.32 ]] [[0.452]
 [0.526]
 [0.356]
 [0.336]
 [0.333]
 [0.333]
 [0.337]]
maxi score, test score, baseline:  -0.8978200000000001 0.5894999999999999 0.5894999999999999
probs:  [0.08581179742035204, 0.5708955769102635, 0.08581179742035204, 0.08581179742035204, 0.08581179742035204, 0.08585723340832833]
printing an ep nov before normalisation:  1.3188553538319476
printing an ep nov before normalisation:  0.0001518322198990063
Printing some Q and Qe and total Qs values:  [[-0.192]
 [-0.155]
 [-0.258]
 [-0.169]
 [-0.18 ]
 [-0.195]
 [-0.165]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.192]
 [-0.155]
 [-0.258]
 [-0.169]
 [-0.18 ]
 [-0.195]
 [-0.165]]
maxi score, test score, baseline:  -0.8978200000000001 0.5894999999999999 0.5894999999999999
probs:  [0.08584126674506078, 0.5707476826838636, 0.08584126674506078, 0.08584126674506078, 0.08584126674506078, 0.08588725033589327]
printing an ep nov before normalisation:  62.069218091983345
printing an ep nov before normalisation:  54.22142311168514
printing an ep nov before normalisation:  59.43751190824973
printing an ep nov before normalisation:  47.6634150266287
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.01843547821045
maxi score, test score, baseline:  -0.8978200000000001 0.5894999999999999 0.5894999999999999
probs:  [0.08584863070935421, 0.5707107260242471, 0.08584863070935421, 0.08584863070935421, 0.08584863070935421, 0.08589475113833628]
actor:  0 policy actor:  1  step number:  32 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  61.572893027559594
actor:  1 policy actor:  1  step number:  45 total reward:  0.11999999999999944  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8949600000000001 0.5894999999999999 0.5894999999999999
probs:  [0.08585599332763252, 0.5706737761197173, 0.08585599332763252, 0.08585599332763252, 0.08585599332763252, 0.08590225056975281]
line 256 mcts: sample exp_bonus 36.775721119970356
actor:  0 policy actor:  1  step number:  24 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.89202 0.5894999999999999 0.5894999999999999
probs:  [0.08585599332763252, 0.5706737761197173, 0.08585599332763252, 0.08585599332763252, 0.08585599332763252, 0.08590225056975281]
siam score:  -0.8001024
printing an ep nov before normalisation:  21.375108352532664
maxi score, test score, baseline:  -0.89202 0.5894999999999999 0.5894999999999999
maxi score, test score, baseline:  -0.89202 0.5894999999999999 0.5894999999999999
probs:  [0.08586335460026485, 0.5706368329684226, 0.08586335460026485, 0.08586335460026485, 0.08586335460026485, 0.08590974863051806]
maxi score, test score, baseline:  -0.89202 0.5894999999999999 0.5894999999999999
probs:  [0.08586335460026485, 0.5706368329684226, 0.08586335460026485, 0.08586335460026485, 0.08586335460026485, 0.08590974863051806]
printing an ep nov before normalisation:  50.30261180851703
printing an ep nov before normalisation:  63.54054078323065
maxi score, test score, baseline:  -0.89202 0.5894999999999999 0.5894999999999999
probs:  [0.08586335460026485, 0.5706368329684226, 0.08586335460026485, 0.08586335460026485, 0.08586335460026485, 0.08590974863051806]
actor:  1 policy actor:  1  step number:  21 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.111]
 [-0.032]
 [-0.111]
 [-0.117]
 [-0.111]
 [-0.111]
 [-0.111]] [[64.304]
 [55.566]
 [64.304]
 [65.785]
 [64.304]
 [64.304]
 [64.304]] [[0.536]
 [0.495]
 [0.536]
 [0.55 ]
 [0.536]
 [0.536]
 [0.536]]
maxi score, test score, baseline:  -0.89202 0.5894999999999999 0.5894999999999999
printing an ep nov before normalisation:  46.15511156067846
maxi score, test score, baseline:  -0.89202 0.5894999999999999 0.5894999999999999
probs:  [0.06425231777039607, 0.30883826488561106, 0.4341290565479967, 0.06425231777039607, 0.06425231777039607, 0.06427572525520395]
actor:  1 policy actor:  1  step number:  40 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  68.43963991739322
actor:  1 policy actor:  1  step number:  27 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  65.05649518106776
printing an ep nov before normalisation:  46.51783154735082
printing an ep nov before normalisation:  46.72924518585205
maxi score, test score, baseline:  -0.89202 0.5894999999999999 0.5894999999999999
probs:  [0.06429507525596916, 0.308480638961629, 0.4343154777546678, 0.06429507525596916, 0.06429507525596916, 0.06431865751579571]
maxi score, test score, baseline:  -0.89202 0.5894999999999999 0.5894999999999999
maxi score, test score, baseline:  -0.89202 0.5894999999999999 0.5894999999999999
probs:  [0.06429507525596916, 0.308480638961629, 0.4343154777546678, 0.06429507525596916, 0.06429507525596916, 0.06431865751579571]
actor:  1 policy actor:  1  step number:  34 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  24 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.89202 0.5894999999999999 0.5894999999999999
probs:  [0.06430920772081548, 0.3083627404024958, 0.4343767882128359, 0.06430920772081548, 0.06430920772081548, 0.06433284822222173]
printing an ep nov before normalisation:  37.055114171072205
maxi score, test score, baseline:  -0.89202 0.5894999999999999 0.5894999999999999
probs:  [0.06430920772081548, 0.3083627404024958, 0.4343767882128359, 0.06430920772081548, 0.06430920772081548, 0.06433284822222173]
printing an ep nov before normalisation:  18.157371546492996
siam score:  -0.81052643
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  39 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.89202 0.5894999999999999 0.5894999999999999
probs:  [0.06430920772081548, 0.3083627404024958, 0.4343767882128359, 0.06430920772081548, 0.06430920772081548, 0.06433284822222173]
actor:  1 policy actor:  1  step number:  29 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.05 ]
 [ 0.009]
 [-0.04 ]
 [-0.048]
 [-0.053]
 [-0.046]
 [-0.049]] [[58.325]
 [63.115]
 [61.709]
 [63.471]
 [59.763]
 [57.773]
 [58.016]] [[0.429]
 [0.569]
 [0.497]
 [0.519]
 [0.449]
 [0.423]
 [0.424]]
maxi score, test score, baseline:  -0.89202 0.5894999999999999 0.5894999999999999
probs:  [0.06427625871361736, 0.30875174309681647, 0.43411954067451347, 0.06427625871361736, 0.06427625871361736, 0.06429994008781797]
Printing some Q and Qe and total Qs values:  [[-0.111]
 [-0.067]
 [-0.111]
 [-0.097]
 [-0.111]
 [-0.111]
 [-0.111]] [[73.642]
 [77.92 ]
 [73.642]
 [71.755]
 [73.642]
 [73.642]
 [73.642]] [[0.485]
 [0.564]
 [0.485]
 [0.484]
 [0.485]
 [0.485]
 [0.485]]
actions average: 
K:  2  action  0 :  tensor([0.3196, 0.0024, 0.1303, 0.1497, 0.1659, 0.1115, 0.1206],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0267, 0.9201, 0.0111, 0.0130, 0.0038, 0.0033, 0.0220],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1402, 0.0967, 0.2707, 0.1252, 0.1200, 0.1062, 0.1410],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1471, 0.0281, 0.1507, 0.3152, 0.1266, 0.1005, 0.1317],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1727, 0.0552, 0.1142, 0.1189, 0.3338, 0.0838, 0.1214],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1268, 0.0040, 0.1796, 0.1358, 0.1417, 0.2832, 0.1289],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1493, 0.0440, 0.1307, 0.1479, 0.1642, 0.1008, 0.2631],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.89202 0.5894999999999999 0.5894999999999999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.253]
 [-0.157]
 [-0.188]
 [-0.179]
 [-0.188]
 [-0.188]
 [-0.188]] [[30.097]
 [38.258]
 [32.866]
 [38.466]
 [32.866]
 [32.866]
 [32.866]] [[1.005]
 [1.464]
 [1.193]
 [1.451]
 [1.193]
 [1.193]
 [1.193]]
printing an ep nov before normalisation:  56.60723059102617
actor:  1 policy actor:  1  step number:  36 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.89202 0.5894999999999999 0.5894999999999999
probs:  [0.06425761643198166, 0.30902090569187657, 0.4339248481285184, 0.06425761643198166, 0.06425761643198166, 0.06428139688366008]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.89202 0.5894999999999999 0.5894999999999999
probs:  [0.06425761643198166, 0.30902090569187657, 0.4339248481285184, 0.06425761643198166, 0.06425761643198166, 0.06428139688366008]
maxi score, test score, baseline:  -0.89202 0.5894999999999999 0.5894999999999999
probs:  [0.06425761643198166, 0.30902090569187657, 0.4339248481285184, 0.06425761643198166, 0.06425761643198166, 0.06428139688366008]
maxi score, test score, baseline:  -0.89202 0.5894999999999999 0.5894999999999999
probs:  [0.06425761643198166, 0.30902090569187657, 0.4339248481285184, 0.06425761643198166, 0.06425761643198166, 0.06428139688366008]
using another actor
from probs:  [0.0642576209471502, 0.3090209274414782, 0.43392487867293833, 0.0642576209471502, 0.0642576209471502, 0.06428133104413299]
maxi score, test score, baseline:  -0.89202 0.5894999999999999 0.5894999999999999
actor:  0 policy actor:  1  step number:  25 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.0
from probs:  [0.0642576298978098, 0.30902097055687455, 0.4339249392227721, 0.0642576298978098, 0.0642576298978098, 0.06428120052692397]
printing an ep nov before normalisation:  0.029800497959229233
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.465]
 [0.465]
 [0.447]
 [0.443]
 [0.414]
 [0.433]] [[31.553]
 [32.609]
 [29.415]
 [30.992]
 [31.072]
 [28.442]
 [31.113]] [[0.492]
 [0.465]
 [0.465]
 [0.447]
 [0.443]
 [0.414]
 [0.433]]
actor:  0 policy actor:  1  step number:  25 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  31 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.88334 0.5894999999999999 0.5894999999999999
probs:  [0.0642576298978098, 0.30902097055687455, 0.4339249392227721, 0.0642576298978098, 0.0642576298978098, 0.06428120052692397]
printing an ep nov before normalisation:  62.69809703369341
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.502]
 [0.694]
 [0.502]
 [0.502]
 [0.502]
 [0.502]] [[65.599]
 [65.599]
 [54.142]
 [65.599]
 [65.599]
 [65.599]
 [65.599]] [[0.502]
 [0.502]
 [0.694]
 [0.502]
 [0.502]
 [0.502]
 [0.502]]
maxi score, test score, baseline:  -0.88334 0.5894999999999999 0.5894999999999999
maxi score, test score, baseline:  -0.88334 0.5894999999999999 0.5894999999999999
maxi score, test score, baseline:  -0.88334 0.5894999999999999 0.5894999999999999
probs:  [0.06422486153005925, 0.3094079989891054, 0.43366901307588185, 0.06422486153005925, 0.06422486153005925, 0.06424840334483505]
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.88334 0.5894999999999999 0.5894999999999999
probs:  [0.06422486153005925, 0.3094079989891054, 0.43366901307588185, 0.06422486153005925, 0.06422486153005925, 0.06424840334483505]
actor:  0 policy actor:  1  step number:  31 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8806999999999999 0.5894999999999999 0.5894999999999999
probs:  [0.06422486153005925, 0.3094079989891054, 0.43366901307588185, 0.06422486153005925, 0.06422486153005925, 0.06424840334483505]
printing an ep nov before normalisation:  33.22693347930908
maxi score, test score, baseline:  -0.8806999999999999 0.5894999999999999 0.5894999999999999
probs:  [0.06422486153005925, 0.3094079989891054, 0.43366901307588185, 0.06422486153005925, 0.06422486153005925, 0.06424840334483505]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.179]
 [-0.179]
 [-0.179]
 [-0.179]
 [-0.179]
 [-0.179]
 [-0.179]] [[31.123]
 [31.123]
 [31.123]
 [31.123]
 [31.123]
 [31.123]
 [31.123]] [[0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.821]]
maxi score, test score, baseline:  -0.8806999999999999 0.5894999999999999 0.5894999999999999
probs:  [0.06412678012511192, 0.3105662214261302, 0.43290299563178497, 0.06412678012511192, 0.06412678012511192, 0.06415044256674901]
maxi score, test score, baseline:  -0.8806999999999999 0.5894999999999999 0.5894999999999999
probs:  [0.06412678012511192, 0.3105662214261302, 0.43290299563178497, 0.06412678012511192, 0.06412678012511192, 0.06415044256674901]
Printing some Q and Qe and total Qs values:  [[0.61 ]
 [0.555]
 [0.74 ]
 [0.796]
 [0.664]
 [0.653]
 [0.601]] [[20.974]
 [38.714]
 [23.057]
 [22.543]
 [30.462]
 [30.828]
 [34.313]] [[1.064]
 [1.555]
 [1.258]
 [1.299]
 [1.41 ]
 [1.411]
 [1.466]]
maxi score, test score, baseline:  -0.8806999999999999 0.5894999999999999 0.5894999999999999
maxi score, test score, baseline:  -0.8806999999999999 0.5894999999999999 0.5894999999999999
probs:  [0.06412678012511192, 0.3105662214261302, 0.43290299563178497, 0.06412678012511192, 0.06412678012511192, 0.06415044256674901]
actor:  1 policy actor:  1  step number:  40 total reward:  0.2899999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8806999999999999 0.5894999999999999 0.5894999999999999
probs:  [0.06406158931366447, 0.3113360458443965, 0.43239385428342864, 0.06406158931366447, 0.06406158931366447, 0.06408533193118163]
using explorer policy with actor:  1
printing an ep nov before normalisation:  77.43279419036855
line 256 mcts: sample exp_bonus 53.36365923045126
Printing some Q and Qe and total Qs values:  [[-0.07 ]
 [-0.054]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]] [[54.888]
 [50.687]
 [54.888]
 [54.888]
 [54.888]
 [54.888]
 [54.888]] [[1.588]
 [1.405]
 [1.588]
 [1.588]
 [1.588]
 [1.588]
 [1.588]]
printing an ep nov before normalisation:  61.29201248824015
actor:  1 policy actor:  1  step number:  32 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.8806999999999999 0.5894999999999999 0.5894999999999999
probs:  [0.06406158931366447, 0.3113360458443965, 0.43239385428342864, 0.06406158931366447, 0.06406158931366447, 0.06408533193118163]
Printing some Q and Qe and total Qs values:  [[-0.063]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]] [[40.202]
 [40.202]
 [40.202]
 [40.202]
 [40.202]
 [40.202]
 [40.202]] [[1.604]
 [1.604]
 [1.604]
 [1.604]
 [1.604]
 [1.604]
 [1.604]]
printing an ep nov before normalisation:  47.759495909428296
maxi score, test score, baseline:  -0.8806999999999999 0.5894999999999999 0.5894999999999999
probs:  [0.06406158931366447, 0.3113360458443965, 0.43239385428342864, 0.06406158931366447, 0.06406158931366447, 0.06408533193118163]
actor:  1 policy actor:  1  step number:  33 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.8806999999999999 0.5894999999999999 0.5894999999999999
maxi score, test score, baseline:  -0.8806999999999999 0.5894999999999999 0.5894999999999999
probs:  [0.06406158931366447, 0.3113360458443965, 0.43239385428342864, 0.06406158931366447, 0.06406158931366447, 0.06408533193118163]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8806999999999999 0.5894999999999999 0.5894999999999999
probs:  [0.06406158931366447, 0.3113360458443965, 0.43239385428342864, 0.06406158931366447, 0.06406158931366447, 0.06408533193118163]
printing an ep nov before normalisation:  41.80434642233281
maxi score, test score, baseline:  -0.8806999999999999 0.5894999999999999 0.5894999999999999
maxi score, test score, baseline:  -0.8806999999999999 0.5894999999999999 0.5894999999999999
probs:  [0.06402905273240751, 0.3117202634079202, 0.4321397430293384, 0.06402905273240751, 0.06402905273240751, 0.064052835365519]
actor:  1 policy actor:  1  step number:  28 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8806999999999999 0.5894999999999999 0.5894999999999999
probs:  [0.06399655527315429, 0.31210401898802814, 0.43188593731876396, 0.06399655527315429, 0.06399655527315429, 0.06402037787374491]
maxi score, test score, baseline:  -0.8806999999999999 0.5894999999999999 0.5894999999999999
printing an ep nov before normalisation:  34.46997462687763
maxi score, test score, baseline:  -0.8806999999999999 0.5894999999999999 0.5894999999999999
probs:  [0.06404392663742338, 0.311594258282086, 0.4322061948969394, 0.06404392663742338, 0.06404392663742338, 0.06406776690870453]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8806999999999999 0.5894999999999999 0.5894999999999999
probs:  [0.06413803397739086, 0.31058157425247646, 0.43284241446231464, 0.06413803397739086, 0.06413803397739086, 0.06416190935303659]
printing an ep nov before normalisation:  36.82249364665082
maxi score, test score, baseline:  -0.8806999999999999 0.5894999999999999 0.5894999999999999
probs:  [0.06413803397739086, 0.31058157425247646, 0.43284241446231464, 0.06413803397739086, 0.06413803397739086, 0.06416190935303659]
printing an ep nov before normalisation:  63.61590818863527
printing an ep nov before normalisation:  22.272853803732243
actor:  1 policy actor:  1  step number:  42 total reward:  0.2899999999999996  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  55.51107853699879
line 256 mcts: sample exp_bonus 52.21071376321604
Printing some Q and Qe and total Qs values:  [[-0.18 ]
 [-0.185]
 [-0.192]
 [-0.18 ]
 [-0.192]
 [-0.193]
 [-0.193]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.18 ]
 [-0.185]
 [-0.192]
 [-0.18 ]
 [-0.192]
 [-0.193]
 [-0.193]]
actor:  1 policy actor:  1  step number:  39 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8806999999999999 0.5894999999999999 0.5894999999999999
probs:  [0.06421340892341645, 0.3098387503604368, 0.4332836060023118, 0.06421340892341645, 0.06421340892341645, 0.06423741686700205]
Printing some Q and Qe and total Qs values:  [[-0.189]
 [-0.181]
 [-0.19 ]
 [-0.19 ]
 [-0.192]
 [-0.195]
 [-0.195]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.189]
 [-0.181]
 [-0.19 ]
 [-0.19 ]
 [-0.192]
 [-0.195]
 [-0.195]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  67.68370798152984
printing an ep nov before normalisation:  36.98856830596924
printing an ep nov before normalisation:  25.19771846856545
maxi score, test score, baseline:  -0.8806999999999999 0.5894999999999999 0.5894999999999999
probs:  [0.06421340892341645, 0.3098387503604368, 0.4332836060023118, 0.06421340892341645, 0.06421340892341645, 0.06423741686700205]
line 256 mcts: sample exp_bonus 40.74030911612293
actor:  1 policy actor:  1  step number:  42 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8806999999999999 0.5894999999999999 0.5894999999999999
probs:  [0.06414934234308985, 0.3105962874377589, 0.4327822549410054, 0.06414934234308985, 0.06414934234308985, 0.06417343059196605]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
using another actor
from probs:  [0.06411736595009962, 0.3109743832119629, 0.4325320246574366, 0.06411736595009962, 0.06411736595009962, 0.06414149428030182]
printing an ep nov before normalisation:  42.135812215135175
maxi score, test score, baseline:  -0.8806999999999999 0.5894999999999999 0.5894999999999999
probs:  [0.06411736595009962, 0.3109743832119629, 0.4325320246574366, 0.06411736595009962, 0.06411736595009962, 0.06414149428030182]
actor:  0 policy actor:  0  step number:  26 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  30.04845676762863
maxi score, test score, baseline:  -0.87768 0.5894999999999999 0.5894999999999999
maxi score, test score, baseline:  -0.87768 0.5894999999999999 0.5894999999999999
probs:  [0.06416375606912994, 0.31047530517866406, 0.43284552472096455, 0.06416375606912994, 0.06416375606912994, 0.06418790189298164]
actor:  1 policy actor:  1  step number:  31 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.87768 0.5894999999999999 0.5894999999999999
probs:  [0.06413187134981006, 0.3108523990888896, 0.43259592959600485, 0.06413187134981006, 0.06413187134981006, 0.06415605726567532]
printing an ep nov before normalisation:  62.599853383359914
printing an ep nov before normalisation:  53.053496179717314
from probs:  [0.06413187587935465, 0.3108524210802352, 0.43259596020382635, 0.06413187587935465, 0.06413187587935465, 0.06415599107787441]
Printing some Q and Qe and total Qs values:  [[0.051]
 [0.113]
 [0.051]
 [0.051]
 [0.051]
 [0.051]
 [0.051]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.051]
 [0.113]
 [0.051]
 [0.051]
 [0.051]
 [0.051]
 [0.051]]
printing an ep nov before normalisation:  20.201812784758104
maxi score, test score, baseline:  -0.87768 0.5894999999999999 0.5894999999999999
probs:  [0.06410003330791283, 0.3112290918865452, 0.4323466901770869, 0.06410003330791283, 0.06410003330791283, 0.06412411801262939]
maxi score, test score, baseline:  -0.87768 0.5894999999999999 0.5894999999999999
probs:  [0.06410003330791283, 0.3112290918865452, 0.4323466901770869, 0.06410003330791283, 0.06410003330791283, 0.06412411801262939]
printing an ep nov before normalisation:  78.53265163618387
maxi score, test score, baseline:  -0.87768 0.5894999999999999 0.5894999999999999
probs:  [0.06406822828831134, 0.31160531838522043, 0.43209771412015585, 0.06406822828831134, 0.06406822828831134, 0.06409228262968981]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 37.61769026359202
actor:  1 policy actor:  1  step number:  36 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  50.37984848022461
actor:  1 policy actor:  1  step number:  27 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.87768 0.5894999999999999 0.5894999999999999
probs:  [0.06416071725640828, 0.31061044386901726, 0.43272259796673673, 0.06416071725640828, 0.06416071725640828, 0.06418480639502112]
printing an ep nov before normalisation:  30.817602011073966
maxi score, test score, baseline:  -0.87768 0.5894999999999999 0.5894999999999999
probs:  [0.06416071725640828, 0.31061044386901726, 0.43272259796673673, 0.06416071725640828, 0.06416071725640828, 0.06418480639502112]
actor:  1 policy actor:  1  step number:  39 total reward:  0.09999999999999942  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.87768 0.5894999999999999 0.5894999999999999
probs:  [0.06420665769897711, 0.31011627708874934, 0.4330329856925027, 0.06420665769897711, 0.06420665769897711, 0.06423076412181668]
maxi score, test score, baseline:  -0.87768 0.5894999999999999 0.5894999999999999
probs:  [0.06417504519134579, 0.3104903963915924, 0.4327852766464556, 0.06417504519134579, 0.06417504519134579, 0.06419919138791438]
maxi score, test score, baseline:  -0.87768 0.5894999999999999 0.5894999999999999
actor:  1 policy actor:  1  step number:  38 total reward:  0.14999999999999947  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  38 total reward:  0.24999999999999956  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  59.44502526861926
printing an ep nov before normalisation:  43.63766157110902
using explorer policy with actor:  1
printing an ep nov before normalisation:  54.6160262114574
maxi score, test score, baseline:  -0.87768 0.5894999999999999 0.5894999999999999
probs:  [0.06422084212122463, 0.30999786773721827, 0.43309467035315846, 0.06422084212122463, 0.06422084212122463, 0.06424493554594933]
Printing some Q and Qe and total Qs values:  [[-0.222]
 [-0.112]
 [-0.126]
 [-0.139]
 [-0.172]
 [-0.154]
 [-0.157]] [[55.653]
 [58.973]
 [56.227]
 [54.912]
 [53.227]
 [52.236]
 [59.425]] [[1.455]
 [1.726]
 [1.578]
 [1.502]
 [1.388]
 [1.358]
 [1.702]]
actor:  1 policy actor:  1  step number:  31 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.87768 0.5894999999999999 0.5894999999999999
maxi score, test score, baseline:  -0.87768 0.5894999999999999 0.5894999999999999
probs:  [0.06418932833812512, 0.3103710488257781, 0.4328476438205772, 0.06418932833812512, 0.06418932833812512, 0.06421332233926927]
actions average: 
K:  0  action  0 :  tensor([0.3495, 0.0113, 0.1110, 0.1356, 0.1727, 0.0951, 0.1248],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0193, 0.9630, 0.0024, 0.0049, 0.0011, 0.0013, 0.0080],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0938, 0.0115, 0.4327, 0.1161, 0.1002, 0.1410, 0.1048],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1423, 0.0505, 0.1401, 0.2621, 0.1384, 0.1146, 0.1519],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1668, 0.0119, 0.1387, 0.1662, 0.2322, 0.1332, 0.1510],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1418, 0.0020, 0.1490, 0.1744, 0.1486, 0.2337, 0.1505],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1616, 0.0851, 0.1212, 0.2427, 0.1184, 0.1098, 0.1611],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  38.348396640340695
maxi score, test score, baseline:  -0.87768 0.5894999999999999 0.5894999999999999
printing an ep nov before normalisation:  33.5408351959476
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]] [[1.303]
 [1.303]
 [1.303]
 [1.303]
 [1.303]
 [1.303]
 [1.303]] [[0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]]
from probs:  [0.06418932833812512, 0.3103710488257781, 0.4328476438205772, 0.06418932833812512, 0.06418932833812512, 0.06421332233926927]
actions average: 
K:  1  action  0 :  tensor([0.4499, 0.0472, 0.1024, 0.0840, 0.1491, 0.0717, 0.0956],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0044, 0.9309, 0.0073, 0.0321, 0.0020, 0.0025, 0.0208],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1172, 0.0137, 0.4251, 0.1154, 0.1175, 0.0940, 0.1170],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1221, 0.0817, 0.1436, 0.2570, 0.1366, 0.0977, 0.1612],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2227, 0.0483, 0.0970, 0.1183, 0.3515, 0.0705, 0.0915],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1454, 0.0027, 0.1559, 0.1427, 0.1425, 0.2966, 0.1142],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1324, 0.1554, 0.1594, 0.1343, 0.1413, 0.1145, 0.1626],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[-0.126]
 [-0.126]
 [-0.126]
 [-0.126]
 [-0.126]
 [-0.126]
 [-0.126]] [[60.37]
 [60.37]
 [60.37]
 [60.37]
 [60.37]
 [60.37]
 [60.37]] [[0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]]
maxi score, test score, baseline:  -0.87768 0.5894999999999999 0.5894999999999999
probs:  [0.06415784246944688, 0.3107437509928162, 0.43260084573422997, 0.06415784246944688, 0.06415784246944688, 0.06418187586461309]
printing an ep nov before normalisation:  74.27655696868896
actor:  1 policy actor:  1  step number:  31 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.87768 0.5894999999999999 0.5894999999999999
maxi score, test score, baseline:  -0.87768 0.5894999999999999 0.5894999999999999
probs:  [0.06424903961071063, 0.3097629180848897, 0.43321685584375325, 0.06424903961071063, 0.06424903961071063, 0.0642731072392251]
siam score:  -0.7885231
maxi score, test score, baseline:  -0.87768 0.5894999999999999 0.5894999999999999
printing an ep nov before normalisation:  28.88277106966581
printing an ep nov before normalisation:  32.082382711555724
actor:  0 policy actor:  1  step number:  27 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  47.32516898159922
printing an ep nov before normalisation:  57.16098908746403
Printing some Q and Qe and total Qs values:  [[-0.184]
 [-0.151]
 [-0.184]
 [-0.184]
 [-0.184]
 [-0.184]
 [-0.184]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.184]
 [-0.151]
 [-0.184]
 [-0.184]
 [-0.184]
 [-0.184]
 [-0.184]]
Printing some Q and Qe and total Qs values:  [[-0.062]
 [-0.056]
 [-0.037]
 [-0.08 ]
 [-0.079]
 [-0.08 ]
 [-0.062]] [[45.994]
 [53.239]
 [49.811]
 [45.708]
 [45.662]
 [45.096]
 [45.994]] [[0.426]
 [0.578]
 [0.528]
 [0.402]
 [0.402]
 [0.389]
 [0.426]]
printing an ep nov before normalisation:  0.00038179438092811324
maxi score, test score, baseline:  -0.8747199999999999 0.5894999999999999 0.5894999999999999
actor:  1 policy actor:  1  step number:  37 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.0
from probs:  [0.06416938970658376, 0.310755730077419, 0.43254246867257845, 0.06416938970658376, 0.06416938970658376, 0.06419363213025132]
printing an ep nov before normalisation:  58.130374017067005
actor:  1 policy actor:  1  step number:  25 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.87472 0.5894999999999999 0.5894999999999999
maxi score, test score, baseline:  -0.87472 0.5894999999999999 0.5894999999999999
probs:  [0.06418361121200047, 0.31063691111707425, 0.4326044148908164, 0.06418361121200047, 0.06418361121200047, 0.06420784035610799]
maxi score, test score, baseline:  -0.87472 0.5894999999999999 0.5894999999999999
probs:  [0.06415255109559538, 0.3110048944961128, 0.4323606327469136, 0.06415255109559538, 0.06415255109559538, 0.06417681947018739]
actor:  0 policy actor:  1  step number:  29 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.87176 0.5894999999999999 0.5894999999999999
probs:  [0.06415255109559538, 0.3110048944961128, 0.4323606327469136, 0.06415255109559538, 0.06415255109559538, 0.06417681947018739]
UNIT TEST: sample policy line 217 mcts : [0.02  0.816 0.02  0.02  0.02  0.041 0.061]
using explorer policy with actor:  1
printing an ep nov before normalisation:  69.71371420377326
printing an ep nov before normalisation:  38.31162579553839
actor:  0 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.86866 0.5894999999999999 0.5894999999999999
probs:  [0.06419777148011378, 0.3105186897275284, 0.432665938835331, 0.06419777148011378, 0.06419777148011378, 0.0642220569967992]
siam score:  -0.7927248
maxi score, test score, baseline:  -0.86866 0.5894999999999999 0.5894999999999999
probs:  [0.06421187543028718, 0.3104010825810299, 0.43272707381683917, 0.06421187543028718, 0.06421187543028718, 0.06423621731126936]
printing an ep nov before normalisation:  60.142656094141095
siam score:  -0.7917
actor:  1 policy actor:  1  step number:  39 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  28 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.86866 0.5894999999999999 0.5894999999999999
probs:  [0.06415013764620056, 0.31113283747998227, 0.4322421915978923, 0.06415013764620056, 0.06415013764620056, 0.06417455798352367]
maxi score, test score, baseline:  -0.86866 0.5894999999999999 0.5894999999999999
probs:  [0.06415013764620056, 0.31113283747998227, 0.4322421915978923, 0.06415013764620056, 0.06415013764620056, 0.06417455798352367]
maxi score, test score, baseline:  -0.86866 0.5894999999999999 0.5894999999999999
probs:  [0.06408854147927165, 0.31186291384365017, 0.4317584216255654, 0.06408854147927165, 0.06408854147927165, 0.06411304009296943]
using another actor
actor:  1 policy actor:  1  step number:  30 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.86866 0.5894999999999999 0.5894999999999999
probs:  [0.06410293280467144, 0.31174216580121783, 0.43182154796970895, 0.06410293280467144, 0.06410293280467144, 0.06412748781505877]
maxi score, test score, baseline:  -0.86866 0.5894999999999999 0.5894999999999999
actor:  1 policy actor:  1  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  0  action  0 :  tensor([0.2613, 0.0037, 0.1325, 0.1915, 0.1897, 0.0970, 0.1244],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0019,     0.9912,     0.0024,     0.0012,     0.0006,     0.0008,
            0.0019], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1261, 0.0351, 0.3156, 0.1556, 0.1235, 0.1429, 0.1011],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1412, 0.0067, 0.1714, 0.2262, 0.1642, 0.1407, 0.1496],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1582, 0.0038, 0.1524, 0.1917, 0.2612, 0.1122, 0.1205],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0994, 0.0250, 0.2442, 0.1566, 0.0938, 0.2734, 0.1076],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1702, 0.0276, 0.1343, 0.1687, 0.1500, 0.0881, 0.2612],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
siam score:  -0.7942011
printing an ep nov before normalisation:  78.48497000742279
printing an ep nov before normalisation:  57.84075579770106
maxi score, test score, baseline:  -0.86866 0.5894999999999999 0.5894999999999999
actor:  1 policy actor:  1  step number:  25 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  53.52607735354411
actor:  1 policy actor:  1  step number:  27 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.86866 0.5894999999999999 0.5894999999999999
maxi score, test score, baseline:  -0.86866 0.5894999999999999 0.5894999999999999
probs:  [0.058703962974105114, 0.28602966910901917, 0.3952630894963629, 0.058703962974105114, 0.058703962974105114, 0.14259535247230265]
maxi score, test score, baseline:  -0.86866 0.5894999999999999 0.5894999999999999
probs:  [0.058703962974105114, 0.28602966910901917, 0.3952630894963629, 0.058703962974105114, 0.058703962974105114, 0.14259535247230265]
actor:  1 policy actor:  1  step number:  37 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.86866 0.5894999999999999 0.5894999999999999
probs:  [0.058741523061095854, 0.2855722432779097, 0.3955164773308785, 0.058741523061095854, 0.058741523061095854, 0.14268671020792417]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.067]
 [-0.044]
 [-0.052]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.075]] [[57.248]
 [60.802]
 [58.07 ]
 [46.552]
 [46.57 ]
 [48.791]
 [51.587]] [[0.385]
 [0.453]
 [0.41 ]
 [0.241]
 [0.241]
 [0.27 ]
 [0.304]]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
Printing some Q and Qe and total Qs values:  [[-0.052]
 [ 0.083]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]] [[40.78 ]
 [55.145]
 [40.78 ]
 [40.78 ]
 [40.78 ]
 [40.78 ]
 [40.78 ]] [[0.1  ]
 [0.337]
 [0.1  ]
 [0.1  ]
 [0.1  ]
 [0.1  ]
 [0.1  ]]
from probs:  [0.058746911370328135, 0.2850829238319783, 0.39545947548880156, 0.058746911370328135, 0.058746911370328135, 0.14321686656823565]
maxi score, test score, baseline:  -0.86866 0.5894999999999999 0.5894999999999999
probs:  [0.05878391800932033, 0.2846319706638298, 0.3957090707133023, 0.05878391800932033, 0.05878391800932033, 0.143307204594907]
actor:  1 policy actor:  1  step number:  34 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.86866 0.5894999999999999 0.5894999999999999
probs:  [0.05879805621737946, 0.2847005513122033, 0.3958044273658019, 0.05879805621737946, 0.05879805621737946, 0.14310085266985634]
maxi score, test score, baseline:  -0.86866 0.5894999999999999 0.5894999999999999
probs:  [0.05879805621737946, 0.2847005513122033, 0.3958044273658019, 0.05879805621737946, 0.05879805621737946, 0.14310085266985634]
maxi score, test score, baseline:  -0.86866 0.5894999999999999 0.5894999999999999
probs:  [0.058834921406202036, 0.2842514731791245, 0.3960530685645439, 0.058834921406202036, 0.058834921406202036, 0.14319069403772533]
printing an ep nov before normalisation:  55.28839827131955
printing an ep nov before normalisation:  59.97146041379452
printing an ep nov before normalisation:  66.71305195974284
printing an ep nov before normalisation:  65.68164335683866
maxi score, test score, baseline:  -0.86866 0.5894999999999999 0.5894999999999999
actor:  1 policy actor:  1  step number:  31 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  33 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.86866 0.5894999999999999 0.5894999999999999
probs:  [0.05881532799604845, 0.28444901552624613, 0.3958586398867935, 0.05881532799604845, 0.05881532799604845, 0.14324636059881496]
printing an ep nov before normalisation:  43.46294097350835
printing an ep nov before normalisation:  36.31788572477583
printing an ep nov before normalisation:  51.599712616146874
maxi score, test score, baseline:  -0.86866 0.5894999999999999 0.5894999999999999
probs:  [0.05881532799604845, 0.28444901552624613, 0.3958586398867935, 0.05881532799604845, 0.05881532799604845, 0.14324636059881496]
printing an ep nov before normalisation:  0.004936202909675558
maxi score, test score, baseline:  -0.86866 0.5894999999999999 0.5894999999999999
probs:  [0.05885195651168825, 0.284002794781251, 0.39610564594446307, 0.05885195651168825, 0.05885195651168825, 0.14333568973922112]
siam score:  -0.7881195
printing an ep nov before normalisation:  60.42413714112136
actor:  1 policy actor:  1  step number:  28 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.112]
 [-0.112]
 [-0.112]
 [-0.112]
 [-0.112]
 [-0.112]
 [-0.112]] [[76.101]
 [76.101]
 [76.101]
 [76.101]
 [76.101]
 [76.101]
 [76.101]] [[0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]]
printing an ep nov before normalisation:  61.61447500594325
line 256 mcts: sample exp_bonus 0.0
actor:  1 policy actor:  1  step number:  35 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actions average: 
K:  1  action  0 :  tensor([0.2850, 0.0802, 0.1447, 0.1301, 0.1221, 0.1102, 0.1277],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0055, 0.9602, 0.0028, 0.0155, 0.0020, 0.0017, 0.0124],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1162, 0.0147, 0.5157, 0.0988, 0.0760, 0.0838, 0.0948],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1448, 0.0262, 0.1217, 0.3275, 0.1264, 0.1227, 0.1307],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1491, 0.0504, 0.1242, 0.1395, 0.3278, 0.1050, 0.1041],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1408, 0.0339, 0.2543, 0.1187, 0.0962, 0.2338, 0.1223],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1410, 0.0238, 0.1056, 0.1489, 0.0815, 0.0799, 0.4194],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  43.22739395895244
maxi score, test score, baseline:  -0.86866 0.5894999999999999 0.5894999999999999
probs:  [0.058781811521087454, 0.284576883881965, 0.3955703997487867, 0.058781811521087454, 0.058781811521087454, 0.1435072818059859]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  38 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  53.002683327859366
maxi score, test score, baseline:  -0.86564 0.5894999999999999 0.5894999999999999
probs:  [0.058781811521087454, 0.284576883881965, 0.3955703997487867, 0.058781811521087454, 0.058781811521087454, 0.1435072818059859]
actor:  1 policy actor:  1  step number:  42 total reward:  0.3299999999999996  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.199]
 [-0.168]
 [-0.199]
 [-0.183]
 [-0.185]
 [-0.199]
 [-0.199]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.199]
 [-0.168]
 [-0.199]
 [-0.183]
 [-0.185]
 [-0.199]
 [-0.199]]
line 256 mcts: sample exp_bonus 31.536970194827568
printing an ep nov before normalisation:  55.76990738979738
printing an ep nov before normalisation:  65.05015745788202
actor:  1 policy actor:  1  step number:  26 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.86564 0.5894999999999999 0.5894999999999999
maxi score, test score, baseline:  -0.86564 0.5894999999999999 0.5894999999999999
probs:  [0.05872463906413282, 0.2852115699498106, 0.3951227786899928, 0.05872463906413282, 0.05872463906413282, 0.14349173416779812]
Printing some Q and Qe and total Qs values:  [[0.856]
 [0.745]
 [0.856]
 [0.856]
 [0.856]
 [0.856]
 [0.856]] [[69.376]
 [57.689]
 [69.376]
 [69.376]
 [69.376]
 [69.376]
 [69.376]] [[1.511]
 [1.23 ]
 [1.511]
 [1.511]
 [1.511]
 [1.511]
 [1.511]]
maxi score, test score, baseline:  -0.86564 0.5894999999999999 0.5894999999999999
probs:  [0.0588117811692733, 0.2843892780657947, 0.3957102405671869, 0.0588117811692733, 0.0588117811692733, 0.1434651378591985]
printing an ep nov before normalisation:  28.00208053005451
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]] [[43.677]
 [43.677]
 [43.677]
 [43.677]
 [43.677]
 [43.677]
 [43.677]] [[0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]]
printing an ep nov before normalisation:  37.593322330050995
maxi score, test score, baseline:  -0.86564 0.5894999999999999 0.5894999999999999
probs:  [0.058848097194774414, 0.2839468004663746, 0.3959550622923302, 0.058848097194774414, 0.058848097194774414, 0.14355384565697218]
maxi score, test score, baseline:  -0.86564 0.5894999999999999 0.5894999999999999
probs:  [0.058848097194774414, 0.2839468004663746, 0.3959550622923302, 0.058848097194774414, 0.058848097194774414, 0.14355384565697218]
maxi score, test score, baseline:  -0.86564 0.5894999999999999 0.5894999999999999
printing an ep nov before normalisation:  46.0934568408784
printing an ep nov before normalisation:  15.780292197760708
Printing some Q and Qe and total Qs values:  [[0.604]
 [0.322]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]] [[44.295]
 [51.122]
 [44.295]
 [44.295]
 [44.295]
 [44.295]
 [44.295]] [[0.604]
 [0.322]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]]
maxi score, test score, baseline:  -0.86564 0.5894999999999999 0.5894999999999999
probs:  [0.058849497609288474, 0.2837903690142122, 0.3959333771702138, 0.058849497609288474, 0.058849497609288474, 0.14372776098770834]
printing an ep nov before normalisation:  0.05065885109161172
actor:  1 policy actor:  1  step number:  28 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.82802893404854
printing an ep nov before normalisation:  82.26106372085225
printing an ep nov before normalisation:  45.75767161435162
line 256 mcts: sample exp_bonus 52.91842600095039
printing an ep nov before normalisation:  64.29270290775119
printing an ep nov before normalisation:  41.300859451293945
Printing some Q and Qe and total Qs values:  [[-0.203]
 [-0.205]
 [-0.211]
 [-0.208]
 [-0.209]
 [-0.203]
 [-0.236]] [[44.859]
 [67.608]
 [65.541]
 [64.323]
 [60.007]
 [44.859]
 [64.615]] [[0.026]
 [0.356]
 [0.32 ]
 [0.305]
 [0.241]
 [0.026]
 [0.281]]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  82.7415128218355
from probs:  [0.05825679967259086, 0.27956028816361533, 0.3919072572531322, 0.06957183395111022, 0.05825679967259086, 0.14244702128696055]
printing an ep nov before normalisation:  55.93595992957967
Printing some Q and Qe and total Qs values:  [[-0.175]
 [-0.137]
 [-0.167]
 [-0.175]
 [-0.175]
 [-0.175]
 [-0.175]] [[45.586]
 [50.638]
 [51.606]
 [45.586]
 [45.586]
 [45.586]
 [45.586]] [[0.418]
 [0.586]
 [0.58 ]
 [0.418]
 [0.418]
 [0.418]
 [0.418]]
Printing some Q and Qe and total Qs values:  [[-0.123]
 [-0.105]
 [-0.123]
 [-0.134]
 [-0.123]
 [-0.123]
 [-0.123]] [[76.053]
 [65.75 ]
 [76.053]
 [63.738]
 [76.053]
 [76.053]
 [76.053]] [[0.877]
 [0.683]
 [0.877]
 [0.612]
 [0.877]
 [0.877]
 [0.877]]
actor:  0 policy actor:  1  step number:  29 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.86284 0.5894999999999999 0.5894999999999999
maxi score, test score, baseline:  -0.86284 0.5894999999999999 0.5894999999999999
probs:  [0.05825652360603742, 0.2794031221540674, 0.3918745962704516, 0.0695941785635338, 0.05825652360603742, 0.1426150557998724]
using explorer policy with actor:  1
from probs:  [0.05815154755765462, 0.28022267721229627, 0.3910748754865851, 0.06953660100444738, 0.05815154755765462, 0.14286275118136194]
maxi score, test score, baseline:  -0.86284 0.5894999999999999 0.5894999999999999
probs:  [0.058186324459329704, 0.2797916330392332, 0.3913092107636137, 0.06957820224848123, 0.058186324459329704, 0.1429483050300125]
maxi score, test score, baseline:  -0.86284 0.5894999999999999 0.5894999999999999
probs:  [0.058186324459329704, 0.2797916330392332, 0.3913092107636137, 0.06957820224848123, 0.058186324459329704, 0.1429483050300125]
printing an ep nov before normalisation:  60.49713701794713
from probs:  [0.05832398523046483, 0.27808538922798853, 0.39223680273949774, 0.06974287647355924, 0.05832398523046483, 0.1432869610980249]
printing an ep nov before normalisation:  37.86377177291564
maxi score, test score, baseline:  -0.86284 0.5894999999999999 0.5894999999999999
probs:  [0.05825451426385457, 0.27862555925863036, 0.3917071432489824, 0.06970508269920539, 0.05825451426385457, 0.14345318626547254]
maxi score, test score, baseline:  -0.86284 0.5894999999999999 0.5894999999999999
probs:  [0.05825451426385457, 0.27862555925863036, 0.3917071432489824, 0.06970508269920539, 0.05825451426385457, 0.14345318626547254]
maxi score, test score, baseline:  -0.86284 0.5894999999999999 0.5894999999999999
probs:  [0.05825451426385457, 0.27862555925863036, 0.3917071432489824, 0.06970508269920539, 0.05825451426385457, 0.14345318626547254]
actor:  0 policy actor:  1  step number:  22 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  2.0
from probs:  [0.05825451426385457, 0.27862555925863036, 0.3917071432489824, 0.06970508269920539, 0.05825451426385457, 0.14345318626547254]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.826]
 [0.826]
 [0.785]
 [0.826]
 [0.826]
 [0.826]
 [0.826]] [[53.654]
 [53.654]
 [50.928]
 [53.654]
 [53.654]
 [53.654]
 [53.654]] [[0.826]
 [0.826]
 [0.785]
 [0.826]
 [0.826]
 [0.826]
 [0.826]]
actor:  1 policy actor:  1  step number:  31 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.85962 0.5894999999999999 0.5894999999999999
probs:  [0.05830260581751311, 0.27826941524100635, 0.39203114498436453, 0.06976264884728338, 0.05830260581751311, 0.14333157929231938]
printing an ep nov before normalisation:  67.9039090753678
printing an ep nov before normalisation:  77.86259490500505
line 256 mcts: sample exp_bonus 57.890884575669
maxi score, test score, baseline:  -0.85962 0.5894999999999999 0.5894999999999999
probs:  [0.05830260581751311, 0.27826941524100635, 0.39203114498436453, 0.06976264884728338, 0.05830260581751311, 0.14333157929231938]
Printing some Q and Qe and total Qs values:  [[-0.052]
 [-0.004]
 [-0.055]
 [-0.051]
 [-0.057]
 [-0.058]
 [-0.058]] [[48.789]
 [51.143]
 [44.81 ]
 [45.839]
 [49.098]
 [45.005]
 [45.005]] [[0.716]
 [0.832]
 [0.599]
 [0.633]
 [0.72 ]
 [0.602]
 [0.602]]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.85962 0.5894999999999999 0.5894999999999999
probs:  [0.05824817379720823, 0.27886781665478405, 0.3916036588516686, 0.06974222877298748, 0.05824817379720823, 0.14328994812614348]
maxi score, test score, baseline:  -0.85962 0.5894999999999999 0.5894999999999999
probs:  [0.05821365427391651, 0.2791364876525453, 0.39134039984351204, 0.06972350516193623, 0.05821365427391651, 0.14337229879417346]
Printing some Q and Qe and total Qs values:  [[0.844]
 [0.812]
 [0.456]
 [0.54 ]
 [0.324]
 [0.304]
 [0.532]] [[61.715]
 [59.343]
 [72.337]
 [70.891]
 [65.716]
 [72.439]
 [71.832]] [[0.844]
 [0.812]
 [0.456]
 [0.54 ]
 [0.324]
 [0.304]
 [0.532]]
printing an ep nov before normalisation:  39.562955798087465
maxi score, test score, baseline:  -0.85962 0.5894999999999999 0.5894999999999999
maxi score, test score, baseline:  -0.85962 0.5894999999999999 0.5894999999999999
probs:  [0.0581791820908609, 0.27940479019364034, 0.3910775018699938, 0.06970480722853319, 0.0581791820908609, 0.1434545365261109]
maxi score, test score, baseline:  -0.85962 0.5894999999999999 0.5894999999999999
probs:  [0.0581791820908609, 0.27940479019364034, 0.3910775018699938, 0.06970480722853319, 0.0581791820908609, 0.1434545365261109]
printing an ep nov before normalisation:  46.55129909515381
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[-0.16 ]
 [-0.158]
 [-0.16 ]
 [-0.16 ]
 [-0.16 ]
 [-0.16 ]
 [-0.16 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.16 ]
 [-0.158]
 [-0.16 ]
 [-0.16 ]
 [-0.16 ]
 [-0.16 ]
 [-0.16 ]]
from probs:  [0.058144757150724216, 0.27967272503550467, 0.3908149641889359, 0.06968613491999237, 0.058144757150724216, 0.14353666155411868]
actor:  0 policy actor:  1  step number:  30 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  52.14520866931291
siam score:  -0.7761305
actor:  1 policy actor:  1  step number:  30 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8567600000000002 0.5894999999999999 0.5894999999999999
probs:  [0.05808991759565241, 0.2801169459688147, 0.39035371657016255, 0.06968806020366637, 0.05808991759565241, 0.14366144206605153]
Printing some Q and Qe and total Qs values:  [[-0.052]
 [-0.008]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]] [[57.368]
 [56.277]
 [57.368]
 [57.368]
 [57.368]
 [57.368]
 [57.368]] [[0.242]
 [0.276]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]]
maxi score, test score, baseline:  -0.8567600000000002 0.5894999999999999 0.5894999999999999
probs:  [0.05808991759565241, 0.2801169459688147, 0.39035371657016255, 0.06968806020366637, 0.05808991759565241, 0.14366144206605153]
maxi score, test score, baseline:  -0.8567600000000002 0.5894999999999999 0.5894999999999999
probs:  [0.05808991759565241, 0.2801169459688147, 0.39035371657016255, 0.06968806020366637, 0.05808991759565241, 0.14366144206605153]
Printing some Q and Qe and total Qs values:  [[0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.176]] [[53.466]
 [53.466]
 [53.466]
 [53.466]
 [53.466]
 [53.466]
 [53.466]] [[1.153]
 [1.153]
 [1.153]
 [1.153]
 [1.153]
 [1.153]
 [1.153]]
actor:  1 policy actor:  1  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8567600000000002 0.5894999999999999 0.5894999999999999
maxi score, test score, baseline:  -0.8567600000000002 0.5894999999999999 0.5894999999999999
probs:  [0.060854716505884, 0.2585961689119129, 0.41144382978618393, 0.07118424071437984, 0.060854716505884, 0.13706632757575515]
maxi score, test score, baseline:  -0.8567600000000002 0.5894999999999999 0.5894999999999999
probs:  [0.060854716505884, 0.2585961689119129, 0.41144382978618393, 0.07118424071437984, 0.060854716505884, 0.13706632757575515]
printing an ep nov before normalisation:  36.51453633198865
maxi score, test score, baseline:  -0.8567600000000002 0.5894999999999999 0.5894999999999999
printing an ep nov before normalisation:  0.0019675586179346283
actor:  1 policy actor:  1  step number:  45 total reward:  0.03999999999999937  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.8567600000000002 0.5894999999999999 0.5894999999999999
probs:  [0.06088652353207413, 0.25820815696697513, 0.4116592817979182, 0.07122145853415138, 0.06088652353207413, 0.13713805563680687]
maxi score, test score, baseline:  -0.8567600000000002 0.5894999999999999 0.5894999999999999
probs:  [0.06088652353207413, 0.25820815696697513, 0.4116592817979182, 0.07122145853415138, 0.06088652353207413, 0.13713805563680687]
printing an ep nov before normalisation:  0.007673689773355363
maxi score, test score, baseline:  -0.8567600000000001 0.5894999999999999 0.5894999999999999
probs:  [0.0609181957872981, 0.2578217890847178, 0.4118738209083453, 0.07125851865664241, 0.0609181957872981, 0.13720947977569833]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  -0.8567600000000001 0.5894999999999999 0.5894999999999999
probs:  [0.0609181957872981, 0.2578217890847178, 0.4118738209083453, 0.07125851865664241, 0.0609181957872981, 0.13720947977569833]
actor:  1 policy actor:  1  step number:  29 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  49.80619547889603
printing an ep nov before normalisation:  54.43720541038963
actor:  1 policy actor:  1  step number:  31 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.047]
 [0.047]
 [0.047]
 [0.047]
 [0.047]
 [0.047]
 [0.047]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.047]
 [0.047]
 [0.047]
 [0.047]
 [0.047]
 [0.047]
 [0.047]]
printing an ep nov before normalisation:  35.35237789154053
printing an ep nov before normalisation:  41.79283182210356
maxi score, test score, baseline:  -0.8567600000000001 0.5894999999999999 0.5894999999999999
probs:  [0.06093228766586325, 0.2576133122570496, 0.4119143710936644, 0.07128225411745018, 0.06093228766586325, 0.1373254872001093]
maxi score, test score, baseline:  -0.8567600000000002 0.5894999999999999 0.5894999999999999
probs:  [0.06093228766586325, 0.2576133122570496, 0.4119143710936644, 0.07128225411745018, 0.06093228766586325, 0.1373254872001093]
printing an ep nov before normalisation:  55.54229232239003
Printing some Q and Qe and total Qs values:  [[-0.06]
 [-0.06]
 [-0.06]
 [-0.06]
 [-0.06]
 [-0.06]
 [-0.06]] [[43.875]
 [43.875]
 [43.875]
 [43.875]
 [43.875]
 [43.875]
 [43.875]] [[0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.82387383905566
printing an ep nov before normalisation:  50.297433133579176
maxi score, test score, baseline:  -0.8567600000000002 0.5894999999999999 0.5894999999999999
probs:  [0.0609337979318967, 0.25748655987300173, 0.4118971534588624, 0.07127113571697744, 0.0609337979318967, 0.1374775550873651]
printing an ep nov before normalisation:  45.35093315168723
actor:  1 policy actor:  1  step number:  25 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8567600000000002 0.5894999999999999 0.5894999999999999
probs:  [0.060901993794786946, 0.2577334180317396, 0.4116543363452388, 0.07125398732040115, 0.060901993794786946, 0.13755427071304646]
actor:  0 policy actor:  0  step number:  29 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8538800000000001 0.5894999999999999 0.5894999999999999
printing an ep nov before normalisation:  67.90794289985179
using explorer policy with actor:  0
using explorer policy with actor:  1
printing an ep nov before normalisation:  60.342991394940086
printing an ep nov before normalisation:  64.99774525175809
printing an ep nov before normalisation:  22.12092638015747
printing an ep nov before normalisation:  50.60024324394416
maxi score, test score, baseline:  -0.8538800000000001 0.5894999999999999 0.5894999999999999
probs:  [0.06077716459274929, 0.25872644246059534, 0.4106995603787789, 0.07115489950145693, 0.06077716459274929, 0.1378647684736702]
maxi score, test score, baseline:  -0.8538800000000001 0.5894999999999999 0.5894999999999999
probs:  [0.06077716459274929, 0.25872644246059534, 0.4106995603787789, 0.07115489950145693, 0.06077716459274929, 0.1378647684736702]
printing an ep nov before normalisation:  53.41840877026667
maxi score, test score, baseline:  -0.8538800000000001 0.5894999999999999 0.5894999999999999
probs:  [0.06077716459274929, 0.25872644246059534, 0.4106995603787789, 0.07115489950145693, 0.06077716459274929, 0.1378647684736702]
maxi score, test score, baseline:  -0.8538800000000001 0.5894999999999999 0.5894999999999999
probs:  [0.06079019980565175, 0.25878202621293545, 0.4107878102477675, 0.07117016537398915, 0.06079019980565175, 0.13767959855400444]
maxi score, test score, baseline:  -0.8538800000000001 0.5894999999999999 0.5894999999999999
probs:  [0.06079019980565175, 0.25878202621293545, 0.4107878102477675, 0.07117016537398915, 0.06079019980565175, 0.13767959855400444]
maxi score, test score, baseline:  -0.8538800000000001 0.5894999999999999 0.5894999999999999
probs:  [0.06079019980565175, 0.25878202621293545, 0.4107878102477675, 0.07117016537398915, 0.06079019980565175, 0.13767959855400444]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.115]
 [-0.1  ]
 [-0.115]
 [-0.115]
 [-0.134]
 [-0.129]
 [-0.115]] [[28.376]
 [38.803]
 [28.376]
 [28.376]
 [30.358]
 [31.553]
 [28.376]] [[0.601]
 [1.206]
 [0.601]
 [0.601]
 [0.693]
 [0.767]
 [0.601]]
Printing some Q and Qe and total Qs values:  [[0.614]
 [0.695]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]] [[40.959]
 [42.467]
 [40.959]
 [40.959]
 [40.959]
 [40.959]
 [40.959]] [[1.052]
 [1.172]
 [1.052]
 [1.052]
 [1.052]
 [1.052]
 [1.052]]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.7309886881803
actor:  1 policy actor:  1  step number:  39 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  0.667
siam score:  -0.76991737
printing an ep nov before normalisation:  39.83509032866556
Printing some Q and Qe and total Qs values:  [[-0.274]
 [-0.274]
 [-0.274]
 [-0.274]
 [-0.274]
 [-0.274]
 [-0.274]] [[36.558]
 [36.558]
 [36.558]
 [36.558]
 [36.558]
 [36.558]
 [36.558]] [[0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]]
actor:  1 policy actor:  1  step number:  22 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  0.333
from probs:  [0.060878788291388375, 0.25812706493035303, 0.4113875643383728, 0.07127391362430742, 0.060878788291388375, 0.13745388052418991]
printing an ep nov before normalisation:  46.55807330756154
maxi score, test score, baseline:  -0.8538800000000001 0.5894999999999999 0.5894999999999999
maxi score, test score, baseline:  -0.8538800000000001 0.5894999999999999 0.5894999999999999
probs:  [0.130541069385969, 0.2386856756316059, 0.3811537173170699, 0.06604392223622182, 0.0564130067859932, 0.12716260864314033]
maxi score, test score, baseline:  -0.8538800000000001 0.5894999999999999 0.5894999999999999
printing an ep nov before normalisation:  44.9139878892272
maxi score, test score, baseline:  -0.8538800000000001 0.5894999999999999 0.5894999999999999
printing an ep nov before normalisation:  45.428729325433046
maxi score, test score, baseline:  -0.8538800000000001 0.5894999999999999 0.5894999999999999
maxi score, test score, baseline:  -0.8538800000000001 0.5894999999999999 0.5894999999999999
probs:  [0.13013034378709626, 0.2387984523463572, 0.38133384642234824, 0.0660750818061135, 0.056439613298435, 0.12722266233964963]
actor:  0 policy actor:  0  step number:  27 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.117]
 [-0.073]
 [-0.117]
 [-0.117]
 [-0.117]
 [-0.117]
 [-0.117]] [[36.412]
 [50.048]
 [36.412]
 [36.412]
 [36.412]
 [36.412]
 [36.412]] [[0.539]
 [1.051]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  0 policy actor:  0  step number:  24 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8477400000000002 0.5894999999999999 0.5894999999999999
probs:  [0.13015571775698073, 0.23884503710442515, 0.38140825248045146, 0.06608795291189214, 0.05645060366952249, 0.12705243607672806]
printing an ep nov before normalisation:  24.03103026506102
actor:  1 policy actor:  1  step number:  27 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.333
siam score:  -0.77275133
maxi score, test score, baseline:  -0.8477400000000002 0.5894999999999999 0.5894999999999999
probs:  [0.12975370079027682, 0.23896382752682593, 0.3815979867935138, 0.0660902609179078, 0.05647862894813211, 0.1271155950233436]
maxi score, test score, baseline:  -0.8477400000000002 0.5894999999999999 0.5894999999999999
probs:  [0.12975370079027682, 0.23896382752682593, 0.3815979867935138, 0.0660902609179078, 0.05647862894813211, 0.1271155950233436]
maxi score, test score, baseline:  -0.8477400000000002 0.5894999999999999 0.5894999999999999
probs:  [0.12975370079027682, 0.23896382752682593, 0.3815979867935138, 0.0660902609179078, 0.05647862894813211, 0.1271155950233436]
actor:  0 policy actor:  1  step number:  36 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.8448800000000001 0.5894999999999999 0.5894999999999999
maxi score, test score, baseline:  -0.8448800000000001 0.5894999999999999 0.5894999999999999
probs:  [0.12975370079027682, 0.23896382752682593, 0.3815979867935138, 0.0660902609179078, 0.05647862894813211, 0.1271155950233436]
maxi score, test score, baseline:  -0.8448800000000001 0.5894999999999999 0.5894999999999999
maxi score, test score, baseline:  -0.8448800000000001 0.5894999999999999 0.5894999999999999
probs:  [0.12975370079027682, 0.23896382752682593, 0.3815979867935138, 0.0660902609179078, 0.05647862894813211, 0.1271155950233436]
actor:  0 policy actor:  1  step number:  33 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  45.12272033070682
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.067]
 [-0.084]
 [-0.071]
 [-0.077]
 [-0.067]
 [-0.071]
 [-0.077]] [[43.222]
 [45.132]
 [44.639]
 [40.968]
 [43.222]
 [43.835]
 [45.241]] [[1.399]
 [1.506]
 [1.486]
 [1.243]
 [1.399]
 [1.435]
 [1.519]]
Printing some Q and Qe and total Qs values:  [[-0.089]
 [-0.083]
 [-0.079]
 [-0.079]
 [-0.077]
 [-0.08 ]
 [-0.077]] [[37.096]
 [35.593]
 [25.959]
 [32.881]
 [35.563]
 [27.329]
 [35.563]] [[1.411]
 [1.355]
 [0.97 ]
 [1.25 ]
 [1.36 ]
 [1.024]
 [1.36 ]]
printing an ep nov before normalisation:  0.009742814297482028
printing an ep nov before normalisation:  40.07314981540452
printing an ep nov before normalisation:  54.259567845318976
printing an ep nov before normalisation:  32.91175757168812
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  37 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.669]
 [0.71 ]
 [0.685]
 [0.659]
 [0.573]
 [0.661]
 [0.685]] [[36.009]
 [34.344]
 [36.812]
 [36.671]
 [37.639]
 [37.027]
 [35.407]] [[1.242]
 [1.231]
 [1.284]
 [1.252]
 [1.198]
 [1.266]
 [1.239]]
actions average: 
K:  3  action  0 :  tensor([0.3195, 0.0097, 0.1019, 0.1686, 0.1351, 0.1073, 0.1579],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0102, 0.8957, 0.0092, 0.0339, 0.0033, 0.0045, 0.0431],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1966, 0.1421, 0.1879, 0.1188, 0.1363, 0.0773, 0.1411],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1548, 0.0357, 0.1250, 0.3184, 0.1159, 0.1133, 0.1369],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1946, 0.0868, 0.1449, 0.1664, 0.1280, 0.1191, 0.1602],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1337, 0.0157, 0.2658, 0.1985, 0.1145, 0.1258, 0.1458],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1428, 0.1551, 0.1240, 0.1363, 0.0802, 0.0913, 0.2702],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  30 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.84216 0.5894999999999999 0.5894999999999999
probs:  [0.1298942238231695, 0.23874947439667227, 0.38201153708655616, 0.06613141221371156, 0.05653971364069267, 0.1266736388391978]
from probs:  [0.1298942238231695, 0.23874947439667227, 0.38201153708655616, 0.06613141221371156, 0.05653971364069267, 0.1266736388391978]
maxi score, test score, baseline:  -0.84216 0.5894999999999999 0.5894999999999999
probs:  [0.1298942238231695, 0.23874947439667227, 0.38201153708655616, 0.06613141221371156, 0.05653971364069267, 0.1266736388391978]
maxi score, test score, baseline:  -0.84216 0.5894999999999999 0.5894999999999999
probs:  [0.1298942238231695, 0.23874947439667227, 0.38201153708655616, 0.06613141221371156, 0.05653971364069267, 0.1266736388391978]
maxi score, test score, baseline:  -0.84216 0.5894999999999999 0.5894999999999999
probs:  [0.1298942238231695, 0.23874947439667227, 0.38201153708655616, 0.06613141221371156, 0.05653971364069267, 0.1266736388391978]
printing an ep nov before normalisation:  46.11225722372663
maxi score, test score, baseline:  -0.84216 0.5894999999999999 0.5894999999999999
probs:  [0.12995539640809728, 0.23839091585940025, 0.38219156408926336, 0.06616252541179417, 0.05656630507176643, 0.1267332931596786]
actor:  1 policy actor:  1  step number:  44 total reward:  0.2899999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.84216 0.5894999999999999 0.5894999999999999
probs:  [0.12955361371437998, 0.23850102649527463, 0.3823681321010779, 0.06619304080917297, 0.05659238558216426, 0.12679180129793016]
actor:  0 policy actor:  0  step number:  31 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.83916 0.5894999999999999 0.5894999999999999
probs:  [0.1291564887048427, 0.23860986066858628, 0.38254265324017184, 0.06622320245541809, 0.056618163753057206, 0.126849631177924]
maxi score, test score, baseline:  -0.83916 0.5894999999999999 0.5894999999999999
printing an ep nov before normalisation:  46.947689056396484
printing an ep nov before normalisation:  68.37788476743495
printing an ep nov before normalisation:  28.88878062986919
maxi score, test score, baseline:  -0.83916 0.5894999999999999 0.5894999999999999
probs:  [0.12918120390753174, 0.23865554234067105, 0.3826159061276187, 0.06623586239844334, 0.05662898379159701, 0.12668250143413806]
Printing some Q and Qe and total Qs values:  [[0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]] [[33.184]
 [33.184]
 [33.184]
 [33.184]
 [33.184]
 [33.184]
 [33.184]] [[1.529]
 [1.529]
 [1.529]
 [1.529]
 [1.529]
 [1.529]
 [1.529]]
actor:  1 policy actor:  1  step number:  32 total reward:  0.3299999999999996  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  36 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]] [[40.657]
 [40.657]
 [40.657]
 [40.657]
 [40.657]
 [40.657]
 [40.657]] [[1.549]
 [1.549]
 [1.549]
 [1.549]
 [1.549]
 [1.549]
 [1.549]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.0699999999999994  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.83916 0.5894999999999999 0.5894999999999999
probs:  [0.12920579418190548, 0.23870099310490517, 0.38268878874256795, 0.06624845834905933, 0.056639749137921476, 0.12651621648364053]
maxi score, test score, baseline:  -0.83916 0.5894999999999999 0.5894999999999999
probs:  [0.12920579418190548, 0.23870099310490517, 0.38268878874256795, 0.06624845834905933, 0.056639749137921476, 0.12651621648364053]
maxi score, test score, baseline:  -0.83916 0.5894999999999999 0.5894999999999999
probs:  [0.12881311850494837, 0.23880865514014016, 0.382861430298567, 0.0662782951560284, 0.056665249679327935, 0.1265732512209882]
printing an ep nov before normalisation:  61.97664737701416
maxi score, test score, baseline:  -0.83916 0.5894999999999999 0.5894999999999999
probs:  [0.12881311850494837, 0.23880865514014016, 0.382861430298567, 0.0662782951560284, 0.056665249679327935, 0.1265732512209882]
actor:  1 policy actor:  1  step number:  34 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  36 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.184]
 [-0.171]
 [-0.184]
 [-0.184]
 [-0.184]
 [-0.184]
 [-0.184]] [[ 0.   ]
 [44.745]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.662]
 [ 0.207]
 [-0.662]
 [-0.662]
 [-0.662]
 [-0.662]
 [-0.662]]
actions average: 
K:  4  action  0 :  tensor([0.4436, 0.0484, 0.1083, 0.0987, 0.1143, 0.0824, 0.1044],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0076, 0.9316, 0.0287, 0.0095, 0.0021, 0.0100, 0.0105],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1539, 0.0982, 0.2730, 0.1344, 0.1109, 0.1059, 0.1237],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1502, 0.0525, 0.1391, 0.3178, 0.1204, 0.0886, 0.1314],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1946, 0.0091, 0.1521, 0.1384, 0.2867, 0.1144, 0.1048],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2344, 0.0635, 0.1503, 0.1584, 0.1146, 0.1315, 0.1473],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1396, 0.0603, 0.1525, 0.1095, 0.1087, 0.0836, 0.3457],
       grad_fn=<DivBackward0>)
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  51.056864649173335
printing an ep nov before normalisation:  46.032522153861905
maxi score, test score, baseline:  -0.8365400000000001 0.5894999999999999 0.5894999999999999
probs:  [0.12893383289658572, 0.23809520770921674, 0.3832204669873928, 0.06634034573427687, 0.05671828227566233, 0.12669186439686553]
actor:  0 policy actor:  0  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8334199999999999 0.5894999999999999 0.5894999999999999
probs:  [0.12893383289658572, 0.23809520770921674, 0.3832204669873928, 0.06634034573427687, 0.05671828227566233, 0.12669186439686553]
maxi score, test score, baseline:  -0.8334199999999999 0.5894999999999999 0.5894999999999999
probs:  [0.12893383289658572, 0.23809520770921674, 0.3832204669873928, 0.06634034573427687, 0.05671828227566233, 0.12669186439686553]
actor:  1 policy actor:  1  step number:  34 total reward:  0.22999999999999954  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8302800000000001 0.5894999999999999 0.5894999999999999
probs:  [0.12893383289658572, 0.23809520770921674, 0.3832204669873928, 0.06634034573427687, 0.05671828227566233, 0.12669186439686553]
Printing some Q and Qe and total Qs values:  [[-0.093]
 [-0.08 ]
 [-0.093]
 [-0.093]
 [-0.093]
 [-0.093]
 [-0.093]] [[30.059]
 [34.908]
 [30.059]
 [30.059]
 [30.059]
 [30.059]
 [30.059]] [[0.192]
 [0.292]
 [0.192]
 [0.192]
 [0.192]
 [0.192]
 [0.192]]
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.555]
 [0.339]
 [0.303]
 [0.291]
 [0.299]
 [0.268]] [[37.499]
 [37.733]
 [37.753]
 [37.45 ]
 [37.077]
 [37.35 ]
 [38.239]] [[1.134]
 [1.296]
 [1.082]
 [1.033]
 [1.006]
 [1.025]
 [1.03 ]]
printing an ep nov before normalisation:  48.47410760134533
actor:  1 policy actor:  1  step number:  41 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8302800000000001 0.5894999999999999 0.5894999999999999
probs:  [0.12893383289658572, 0.23809520770921674, 0.3832204669873928, 0.06634034573427687, 0.05671828227566233, 0.12669186439686553]
actor:  1 policy actor:  1  step number:  31 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  35.37697715453641
maxi score, test score, baseline:  -0.8302800000000001 0.5894999999999999 0.5894999999999999
probs:  [0.12822095663473476, 0.23795172405587933, 0.3837391906083385, 0.06642999423436746, 0.05679490191163517, 0.12686323255504486]
maxi score, test score, baseline:  -0.8302800000000001 0.5894999999999999 0.5894999999999999
probs:  [0.12822095663473476, 0.23795172405587933, 0.3837391906083385, 0.06642999423436746, 0.05679490191163517, 0.12686323255504486]
line 256 mcts: sample exp_bonus 31.673284602032684
printing an ep nov before normalisation:  56.846395221173076
printing an ep nov before normalisation:  54.37531146766156
actor:  1 policy actor:  1  step number:  34 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.0
from probs:  [0.12822095663473476, 0.23795172405587933, 0.3837391906083385, 0.06642999423436746, 0.05679490191163517, 0.12686323255504486]
maxi score, test score, baseline:  -0.8302800000000001 0.5894999999999999 0.5894999999999999
using another actor
maxi score, test score, baseline:  -0.8302800000000001 0.5894999999999999 0.5894999999999999
printing an ep nov before normalisation:  52.418784319269236
maxi score, test score, baseline:  -0.8302800000000001 0.5894999999999999 0.5894999999999999
probs:  [0.12784109634140634, 0.2380554279536737, 0.3839064670570584, 0.06645890381573734, 0.056819609983699414, 0.1269184948484249]
printing an ep nov before normalisation:  52.724036328438004
actor:  1 policy actor:  1  step number:  33 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8302800000000001 0.5894999999999999 0.5894999999999999
probs:  [0.12784109634140634, 0.2380554279536737, 0.3839064670570584, 0.06645890381573734, 0.056819609983699414, 0.1269184948484249]
printing an ep nov before normalisation:  32.16550471184631
actor:  0 policy actor:  1  step number:  34 total reward:  0.20999999999999952  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  -0.82786 0.5894999999999999 0.5894999999999999
probs:  [0.12784109634140634, 0.2380554279536737, 0.3839064670570584, 0.06645890381573734, 0.056819609983699414, 0.1269184948484249]
printing an ep nov before normalisation:  33.85830876929121
maxi score, test score, baseline:  -0.82786 0.5894999999999999 0.5894999999999999
probs:  [0.12748973867906993, 0.23820324549757357, 0.3841448996939033, 0.06650011097770704, 0.056854828395010236, 0.12680717675673583]
actor:  1 policy actor:  1  step number:  31 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.82786 0.5894999999999999 0.5894999999999999
probs:  [0.12754888218988747, 0.2378497815370713, 0.38432323214184116, 0.06653093131406593, 0.05688116952676403, 0.12686600329037032]
line 256 mcts: sample exp_bonus 40.58305358780031
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.82786 0.5894999999999999 0.5894999999999999
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.82786 0.5894999999999999 0.5894999999999999
probs:  [0.127666429112135, 0.23714727672955965, 0.3846776654533258, 0.06659218631282385, 0.056933522167372734, 0.1269829202247829]
maxi score, test score, baseline:  -0.82786 0.5894999999999999 0.5894999999999999
probs:  [0.127666429112135, 0.23714727672955965, 0.3846776654533258, 0.06659218631282385, 0.056933522167372734, 0.1269829202247829]
actions average: 
K:  1  action  0 :  tensor([0.3784, 0.0088, 0.1023, 0.1169, 0.1763, 0.1063, 0.1111],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0034,     0.9691,     0.0013,     0.0084,     0.0003,     0.0003,
            0.0173], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0878, 0.0391, 0.5224, 0.0717, 0.0806, 0.0982, 0.1002],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1083, 0.0092, 0.1378, 0.3667, 0.1406, 0.1215, 0.1159],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2023, 0.0413, 0.1447, 0.1507, 0.1816, 0.1207, 0.1587],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1490, 0.0067, 0.1374, 0.1405, 0.1195, 0.3296, 0.1172],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0897, 0.0348, 0.0918, 0.1396, 0.0742, 0.0819, 0.4881],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.82786 0.5894999999999999 0.5894999999999999
probs:  [0.12731866292087965, 0.23729334765274518, 0.3849146588348241, 0.06663314473523771, 0.05696852798913435, 0.1268716578671789]
maxi score, test score, baseline:  -0.82786 0.5894999999999999 0.5894999999999999
probs:  [0.12743498713109105, 0.2365963829361536, 0.38526658117388607, 0.06669396577474972, 0.05702050973899273, 0.1269875732451268]
maxi score, test score, baseline:  -0.82786 0.5894999999999999 0.5894999999999999
maxi score, test score, baseline:  -0.82786 0.5894999999999999 0.5894999999999999
probs:  [0.12743498713109105, 0.2365963829361536, 0.38526658117388607, 0.06669396577474972, 0.05702050973899273, 0.1269875732451268]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  -0.82786 0.5894999999999999 0.5894999999999999
probs:  [0.12749278763846428, 0.23625006711619917, 0.3854414483809501, 0.06672418723025175, 0.05704633902689833, 0.12704517060723627]
printing an ep nov before normalisation:  45.69304409096272
maxi score, test score, baseline:  -0.82786 0.5894999999999999 0.5894999999999999
probs:  [0.12749278763846428, 0.23625006711619917, 0.3854414483809501, 0.06672418723025175, 0.05704633902689833, 0.12704517060723627]
Printing some Q and Qe and total Qs values:  [[-0.196]
 [-0.147]
 [-0.155]
 [-0.152]
 [-0.162]
 [-0.155]
 [-0.153]] [[41.913]
 [42.491]
 [53.32 ]
 [40.667]
 [46.522]
 [53.32 ]
 [43.982]] [[0.59 ]
 [0.65 ]
 [0.845]
 [0.61 ]
 [0.711]
 [0.845]
 [0.672]]
actor:  0 policy actor:  1  step number:  34 total reward:  0.2899999999999996  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.131]
 [-0.088]
 [-0.131]
 [-0.132]
 [-0.132]
 [-0.132]
 [-0.132]] [[24.977]
 [42.792]
 [25.055]
 [25.081]
 [25.153]
 [24.845]
 [25.096]] [[0.083]
 [0.593]
 [0.084]
 [0.085]
 [0.087]
 [0.079]
 [0.085]]
printing an ep nov before normalisation:  2.6805052581153177
actor:  1 policy actor:  1  step number:  27 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.8252800000000001 0.5894999999999999 0.5894999999999999
probs:  [0.12749278763846428, 0.23625006711619917, 0.3854414483809501, 0.06672418723025175, 0.05704633902689833, 0.12704517060723627]
actor:  1 policy actor:  1  step number:  32 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8252800000000001 0.5894999999999999 0.5894999999999999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8252800000000001 0.5894999999999999 0.5894999999999999
Printing some Q and Qe and total Qs values:  [[-0.094]
 [-0.095]
 [-0.078]
 [-0.078]
 [-0.087]
 [-0.091]
 [-0.09 ]] [[47.156]
 [54.29 ]
 [45.443]
 [47.84 ]
 [46.706]
 [46.745]
 [48.386]] [[0.769]
 [1.009]
 [0.726]
 [0.808]
 [0.76 ]
 [0.757]
 [0.814]]
printing an ep nov before normalisation:  48.44970703125
printing an ep nov before normalisation:  65.65946454525995
printing an ep nov before normalisation:  61.55265265694157
siam score:  -0.751964
maxi score, test score, baseline:  -0.8252800000000001 0.5894999999999999 0.5894999999999999
probs:  [0.12752072178856003, 0.23630185532285577, 0.38552595917559473, 0.06670835266306498, 0.05705882194855445, 0.12688428910136995]
maxi score, test score, baseline:  -0.8252800000000001 0.5894999999999999 0.5894999999999999
probs:  [0.12752072178856003, 0.23630185532285577, 0.38552595917559473, 0.06670835266306498, 0.05705882194855445, 0.12688428910136995]
actor:  0 policy actor:  0  step number:  20 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8220600000000001 0.5894999999999999 0.5894999999999999
probs:  [0.12752072178856003, 0.23630185532285577, 0.38552595917559473, 0.06670835266306498, 0.05705882194855445, 0.12688428910136995]
printing an ep nov before normalisation:  57.31247186784617
Printing some Q and Qe and total Qs values:  [[-0.031]
 [-0.031]
 [ 0.251]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]] [[63.256]
 [63.256]
 [51.971]
 [63.256]
 [63.256]
 [63.256]
 [63.256]] [[0.969]
 [0.969]
 [1.004]
 [0.969]
 [0.969]
 [0.969]
 [0.969]]
Printing some Q and Qe and total Qs values:  [[-0.014]
 [ 0.171]
 [ 0.097]
 [ 0.121]
 [ 0.088]
 [ 0.097]
 [ 0.114]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.014]
 [ 0.171]
 [ 0.097]
 [ 0.121]
 [ 0.088]
 [ 0.097]
 [ 0.114]]
actor:  0 policy actor:  1  step number:  22 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.81896 0.5894999999999999 0.5894999999999999
probs:  [0.12752072178856003, 0.23630185532285577, 0.38552595917559473, 0.06670835266306498, 0.05705882194855445, 0.12688428910136995]
using explorer policy with actor:  1
printing an ep nov before normalisation:  57.67978541503381
maxi score, test score, baseline:  -0.81896 0.5894999999999999 0.5894999999999999
probs:  [0.12752072178856003, 0.23630185532285577, 0.38552595917559473, 0.06670835266306498, 0.05705882194855445, 0.12688428910136995]
actor:  1 policy actor:  1  step number:  40 total reward:  0.14999999999999947  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.81896 0.5894999999999999 0.5894999999999999
probs:  [0.12752072178856003, 0.23630185532285577, 0.38552595917559473, 0.06670835266306498, 0.05705882194855445, 0.12688428910136995]
maxi score, test score, baseline:  -0.81896 0.5894999999999999 0.5894999999999999
probs:  [0.12752072178856003, 0.23630185532285577, 0.38552595917559473, 0.06670835266306498, 0.05705882194855445, 0.12688428910136995]
printing an ep nov before normalisation:  35.60108755904468
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  -0.81896 0.5894999999999999 0.5894999999999999
probs:  [0.12752072178856003, 0.23630185532285577, 0.38552595917559473, 0.06670835266306498, 0.05705882194855445, 0.12688428910136995]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.81896 0.5894999999999999 0.5894999999999999
probs:  [0.12752072178856003, 0.23630185532285577, 0.38552595917559473, 0.06670835266306498, 0.05705882194855445, 0.12688428910136995]
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.9674888445089
maxi score, test score, baseline:  -0.81896 0.5894999999999999 0.5894999999999999
probs:  [0.12752072178856003, 0.23630185532285577, 0.38552595917559473, 0.06670835266306498, 0.05705882194855445, 0.12688428910136995]
maxi score, test score, baseline:  -0.81896 0.5894999999999999 0.5894999999999999
probs:  [0.12752072178856003, 0.23630185532285577, 0.38552595917559473, 0.06670835266306498, 0.05705882194855445, 0.12688428910136995]
printing an ep nov before normalisation:  34.84315177078503
Printing some Q and Qe and total Qs values:  [[-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]] [[0.032]
 [0.032]
 [0.035]
 [0.034]
 [0.035]
 [0.035]
 [0.035]] [[-0.]
 [-0.]
 [ 0.]
 [-0.]
 [-0.]
 [ 0.]
 [ 0.]]
maxi score, test score, baseline:  -0.81896 0.5894999999999999 0.5894999999999999
probs:  [0.12752072178856003, 0.23630185532285577, 0.38552595917559473, 0.06670835266306498, 0.05705882194855445, 0.12688428910136995]
Printing some Q and Qe and total Qs values:  [[-0.173]
 [-0.158]
 [-0.173]
 [-0.173]
 [-0.173]
 [-0.173]
 [-0.173]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.173]
 [-0.158]
 [-0.173]
 [-0.173]
 [-0.173]
 [-0.173]
 [-0.173]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.0799999999999994  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.81896 0.5894999999999999 0.5894999999999999
probs:  [0.12715243001939758, 0.23640162321760552, 0.3856887658184514, 0.06673647689021178, 0.05708286979447074, 0.12693783425986313]
using explorer policy with actor:  1
printing an ep nov before normalisation:  65.54071088638663
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.118]
 [-0.092]
 [-0.093]
 [-0.093]
 [-0.115]
 [-0.093]
 [-0.114]] [[35.468]
 [47.338]
 [35.818]
 [35.818]
 [34.642]
 [35.818]
 [34.917]] [[0.611]
 [1.233]
 [0.653]
 [0.653]
 [0.572]
 [0.653]
 [0.587]]
maxi score, test score, baseline:  -0.81896 0.5894999999999999 0.5894999999999999
printing an ep nov before normalisation:  37.16211706805652
printing an ep nov before normalisation:  30.207081998071885
actor:  1 policy actor:  1  step number:  32 total reward:  0.5299999999999999  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  24 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  57.86894953572253
Printing some Q and Qe and total Qs values:  [[-0.13 ]
 [-0.106]
 [-0.13 ]
 [-0.13 ]
 [-0.13 ]
 [-0.13 ]
 [-0.13 ]] [[53.555]
 [72.119]
 [53.555]
 [53.555]
 [53.555]
 [53.555]
 [53.555]] [[0.197]
 [0.445]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]]
printing an ep nov before normalisation:  17.162763797190326
maxi score, test score, baseline:  -0.81896 0.5894999999999999 0.5894999999999999
probs:  [0.12717630552860554, 0.23644603404785927, 0.3857612378115538, 0.06674899615085174, 0.05709357448865762, 0.12677385197247187]
Printing some Q and Qe and total Qs values:  [[-0.137]
 [-0.123]
 [-0.137]
 [-0.137]
 [-0.137]
 [-0.137]
 [-0.137]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.137]
 [-0.123]
 [-0.137]
 [-0.137]
 [-0.137]
 [-0.137]
 [-0.137]]
maxi score, test score, baseline:  -0.81896 0.5894999999999999 0.5894999999999999
probs:  [0.12717630552860554, 0.23644603404785927, 0.3857612378115538, 0.06674899615085174, 0.05709357448865762, 0.12677385197247187]
printing an ep nov before normalisation:  60.17808903587749
maxi score, test score, baseline:  -0.81896 0.5894999999999999 0.5894999999999999
probs:  [0.12717630552860554, 0.23644603404785927, 0.3857612378115538, 0.06674899615085174, 0.05709357448865762, 0.12677385197247187]
actions average: 
K:  1  action  0 :  tensor([0.5643, 0.0634, 0.0753, 0.0834, 0.0834, 0.0592, 0.0709],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0116, 0.9178, 0.0097, 0.0190, 0.0058, 0.0059, 0.0303],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1142, 0.0612, 0.3936, 0.1120, 0.1311, 0.0970, 0.0909],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1488, 0.0078, 0.1549, 0.2866, 0.1503, 0.1175, 0.1341],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1766, 0.0517, 0.1375, 0.2047, 0.1900, 0.1141, 0.1256],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0851, 0.0437, 0.2454, 0.1115, 0.0684, 0.3504, 0.0953],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1653, 0.0939, 0.1505, 0.1518, 0.1467, 0.1310, 0.1609],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.81896 0.5894999999999999 0.5894999999999999
probs:  [0.12717630552860554, 0.23644603404785927, 0.3857612378115538, 0.06674899615085174, 0.05709357448865762, 0.12677385197247187]
Printing some Q and Qe and total Qs values:  [[-0.07 ]
 [-0.071]
 [-0.066]
 [-0.065]
 [-0.068]
 [-0.068]
 [-0.063]] [[43.598]
 [41.359]
 [29.794]
 [41.792]
 [42.164]
 [44.922]
 [44.809]] [[1.465]
 [1.386]
 [0.984]
 [1.407]
 [1.417]
 [1.514]
 [1.515]]
maxi score, test score, baseline:  -0.81896 0.5894999999999999 0.5894999999999999
probs:  [0.12720006192425076, 0.236490223314998, 0.38583334824598936, 0.06676145295360932, 0.05710422577773179, 0.12661068778342063]
actor:  0 policy actor:  0  step number:  34 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  56 total reward:  0.0699999999999994  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8160200000000001 0.5894999999999999 0.5894999999999999
probs:  [0.12725754949832124, 0.23614509761506874, 0.38600784668433635, 0.06679159689387294, 0.0571300005955425, 0.1266679087128583]
maxi score, test score, baseline:  -0.8160200000000001 0.5894999999999999 0.5894999999999999
probs:  [0.12731479983898092, 0.2358013961419941, 0.3861816250216824, 0.06682161643943295, 0.057155669048664294, 0.12672489350924543]
maxi score, test score, baseline:  -0.8160200000000001 0.5894999999999999 0.5894999999999999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8160200000000001 0.5894999999999999 0.5894999999999999
probs:  [0.12731479983898092, 0.2358013961419941, 0.3861816250216824, 0.06682161643943295, 0.057155669048664294, 0.12672489350924543]
actor:  0 policy actor:  0  step number:  31 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8129 0.5894999999999999 0.5894999999999999
maxi score, test score, baseline:  -0.8129 0.5894999999999999 0.5894999999999999
probs:  [0.12700706375811638, 0.23555754932422296, 0.3865162480976169, 0.06687942130065293, 0.05720509555871887, 0.126834621960672]
maxi score, test score, baseline:  -0.8129 0.5894999999999999 0.5894999999999999
probs:  [0.12700706375811638, 0.23555754932422296, 0.3865162480976169, 0.06687942130065293, 0.05720509555871887, 0.126834621960672]
printing an ep nov before normalisation:  42.05927682069182
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.585180788757356
maxi score, test score, baseline:  -0.8129 0.5894999999999999 0.5894999999999999
probs:  [0.1266463297941329, 0.2356549045228227, 0.3866760293613695, 0.06690702290508818, 0.05722869653189546, 0.12688701688469126]
actor:  1 policy actor:  1  step number:  41 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8129 0.5894999999999999 0.5894999999999999
probs:  [0.12665017133601145, 0.2356620560855201, 0.3866877666465074, 0.06687870997400681, 0.057230430222975366, 0.12689086573497887]
maxi score, test score, baseline:  -0.8129 0.5894999999999999 0.5894999999999999
actor:  1 policy actor:  1  step number:  29 total reward:  0.72  reward:  1.0 rdn_beta:  1.0
line 256 mcts: sample exp_bonus 50.65355716162232
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  39 total reward:  0.09999999999999942  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.183]
 [-0.176]
 [-0.183]
 [-0.178]
 [-0.194]
 [-0.183]
 [-0.178]] [[25.325]
 [32.436]
 [25.325]
 [34.123]
 [33.536]
 [25.325]
 [33.893]] [[0.139]
 [0.319]
 [0.139]
 [0.358]
 [0.328]
 [0.139]
 [0.352]]
maxi score, test score, baseline:  -0.8107000000000001 0.5894999999999999 0.5894999999999999
probs:  [0.0982803490981245, 0.1358391203607255, 0.5154782623573773, 0.07767640228740487, 0.0743625883552046, 0.09836327754116336]
printing an ep nov before normalisation:  40.89939221985522
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.297]
 [0.527]
 [0.202]
 [0.433]
 [0.318]
 [0.16 ]
 [0.482]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.297]
 [0.527]
 [0.202]
 [0.433]
 [0.318]
 [0.16 ]
 [0.482]]
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]]
printing an ep nov before normalisation:  44.79698093638713
printing an ep nov before normalisation:  44.79698093638713
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  48.78972424615271
maxi score, test score, baseline:  -0.8107000000000001 0.5894999999999999 0.5894999999999999
printing an ep nov before normalisation:  54.90277188921476
printing an ep nov before normalisation:  51.72726289967837
printing an ep nov before normalisation:  32.923793903079
printing an ep nov before normalisation:  67.83438140246719
printing an ep nov before normalisation:  51.44488419549009
maxi score, test score, baseline:  -0.8107000000000001 0.5894999999999999 0.5894999999999999
probs:  [0.09829544784978791, 0.13570628210302513, 0.5155575419789051, 0.07768833137022056, 0.07437400764769325, 0.09837838905036814]
Printing some Q and Qe and total Qs values:  [[0.699]
 [0.877]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]] [[33.776]
 [27.851]
 [30.914]
 [30.914]
 [30.914]
 [30.914]
 [30.914]] [[0.699]
 [0.877]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]]
Printing some Q and Qe and total Qs values:  [[0.698]
 [0.885]
 [0.718]
 [0.733]
 [0.54 ]
 [0.56 ]
 [0.74 ]] [[33.776]
 [21.793]
 [32.803]
 [34.214]
 [36.268]
 [34.102]
 [31.257]] [[0.698]
 [0.885]
 [0.718]
 [0.733]
 [0.54 ]
 [0.56 ]
 [0.74 ]]
actor:  0 policy actor:  0  step number:  16 total reward:  0.73  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
using another actor
actor:  0 policy actor:  0  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  18 total reward:  0.69  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  18 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  19 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  19 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  19 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  2
actions average: 
K:  2  action  0 :  tensor([0.5435, 0.0055, 0.0742, 0.0847, 0.1498, 0.0697, 0.0726],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0100, 0.9150, 0.0095, 0.0139, 0.0049, 0.0050, 0.0416],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0773, 0.0210, 0.5552, 0.1048, 0.0779, 0.0846, 0.0792],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1161, 0.0427, 0.1135, 0.4150, 0.1039, 0.0935, 0.1153],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1747, 0.0057, 0.1706, 0.1928, 0.1637, 0.1524, 0.1402],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1020, 0.0066, 0.3462, 0.1491, 0.1146, 0.1816, 0.0999],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2247, 0.0070, 0.1410, 0.2085, 0.1296, 0.1016, 0.1875],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[-0.069]
 [-0.042]
 [-0.069]
 [-0.09 ]
 [-0.069]
 [-0.089]
 [-0.048]] [[40.943]
 [46.764]
 [40.943]
 [43.205]
 [40.943]
 [43.305]
 [44.939]] [[0.51 ]
 [0.707]
 [0.51 ]
 [0.556]
 [0.51 ]
 [0.56 ]
 [0.648]]
printing an ep nov before normalisation:  1.8751245534730288
printing an ep nov before normalisation:  40.218342263405766
actor:  0 policy actor:  1  step number:  22 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  35 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.176]
 [-0.157]
 [-0.153]
 [-0.177]
 [-0.176]
 [-0.176]
 [-0.176]] [[65.742]
 [69.212]
 [57.869]
 [54.879]
 [65.742]
 [65.742]
 [65.742]] [[0.344]
 [0.412]
 [0.253]
 [0.186]
 [0.344]
 [0.344]
 [0.344]]
printing an ep nov before normalisation:  42.48567050150459
printing an ep nov before normalisation:  48.0090189886711
maxi score, test score, baseline:  -0.7405400000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
Printing some Q and Qe and total Qs values:  [[0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]] [[44.388]
 [44.388]
 [44.388]
 [44.388]
 [44.388]
 [44.388]
 [44.388]] [[1.493]
 [1.493]
 [1.493]
 [1.493]
 [1.493]
 [1.493]
 [1.493]]
actor:  0 policy actor:  1  step number:  24 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.73736 0.6734999999999999 0.6734999999999999
maxi score, test score, baseline:  -0.73736 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
printing an ep nov before normalisation:  53.19662951338283
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  40 total reward:  0.1899999999999995  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.73736 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
printing an ep nov before normalisation:  41.30543339505535
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.47501322173227
Printing some Q and Qe and total Qs values:  [[0.312]
 [0.484]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]] [[48.559]
 [56.687]
 [48.559]
 [48.559]
 [48.559]
 [48.559]
 [48.559]] [[0.858]
 [1.19 ]
 [0.858]
 [0.858]
 [0.858]
 [0.858]
 [0.858]]
maxi score, test score, baseline:  -0.73736 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
printing an ep nov before normalisation:  37.1907137073675
maxi score, test score, baseline:  -0.73736 0.6734999999999999 0.6734999999999999
actor:  1 policy actor:  1  step number:  32 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  34.23330307006836
printing an ep nov before normalisation:  58.571337242615705
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.184]
 [-0.184]
 [-0.184]
 [-0.184]
 [-0.184]
 [-0.184]
 [-0.184]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.184]
 [-0.184]
 [-0.184]
 [-0.184]
 [-0.184]
 [-0.184]
 [-0.184]]
maxi score, test score, baseline:  -0.73736 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
maxi score, test score, baseline:  -0.73736 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.73736 0.6734999999999999 0.6734999999999999
printing an ep nov before normalisation:  63.472043715955806
maxi score, test score, baseline:  -0.73736 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
printing an ep nov before normalisation:  47.976170770211766
Printing some Q and Qe and total Qs values:  [[0.841]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]] [[44.433]
 [40.461]
 [40.461]
 [40.461]
 [40.461]
 [40.461]
 [40.461]] [[0.841]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]]
actor:  1 policy actor:  1  step number:  34 total reward:  0.2699999999999996  reward:  1.0 rdn_beta:  1.0
from probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
maxi score, test score, baseline:  -0.73736 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
actor:  0 policy actor:  1  step number:  32 total reward:  0.24999999999999956  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.7348600000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
maxi score, test score, baseline:  -0.7348600000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
actor:  0 policy actor:  0  step number:  33 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  51.64169410140978
actor:  0 policy actor:  1  step number:  25 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.7293000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
maxi score, test score, baseline:  -0.7293000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
printing an ep nov before normalisation:  35.34829722705923
maxi score, test score, baseline:  -0.7293000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
maxi score, test score, baseline:  -0.7293000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
actor:  1 policy actor:  1  step number:  27 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.7293000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
actor:  0 policy actor:  1  step number:  24 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  49.93220690037341
maxi score, test score, baseline:  -0.7262000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
maxi score, test score, baseline:  -0.7262000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
actor:  1 policy actor:  1  step number:  31 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.7262000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
printing an ep nov before normalisation:  52.13741018193289
actor:  1 policy actor:  1  step number:  32 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.0
line 256 mcts: sample exp_bonus 42.53056400241893
maxi score, test score, baseline:  -0.7262000000000001 0.6734999999999999 0.6734999999999999
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[-0.072]
 [-0.054]
 [ 0.348]
 [-0.032]
 [-0.06 ]
 [ 0.168]
 [ 0.168]] [[56.132]
 [53.499]
 [49.817]
 [53.88 ]
 [50.553]
 [32.251]
 [32.251]] [[0.665]
 [0.624]
 [0.941]
 [0.654]
 [0.55 ]
 [0.359]
 [0.359]]
maxi score, test score, baseline:  -0.7262000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  37 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  1  step number:  36 total reward:  0.20999999999999952  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  4  action  0 :  tensor([0.4432, 0.0035, 0.1033, 0.1504, 0.1072, 0.0767, 0.1157],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0175, 0.8602, 0.0141, 0.0360, 0.0057, 0.0068, 0.0597],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0748, 0.0575, 0.5575, 0.0806, 0.0569, 0.0837, 0.0891],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1496, 0.1002, 0.0858, 0.4232, 0.0762, 0.0691, 0.0960],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1728, 0.0300, 0.2065, 0.1722, 0.1484, 0.1117, 0.1583],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0513, 0.0420, 0.2514, 0.1416, 0.0644, 0.3311, 0.1182],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1867, 0.1843, 0.0928, 0.0955, 0.2151, 0.0796, 0.1460],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.7237800000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
maxi score, test score, baseline:  -0.7237800000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
actor:  1 policy actor:  1  step number:  30 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.241]
 [-0.241]
 [-0.241]
 [-0.241]
 [-0.241]
 [-0.241]
 [-0.241]] [[59.492]
 [59.492]
 [59.492]
 [59.492]
 [59.492]
 [59.492]
 [59.492]] [[0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]]
using another actor
from probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
actor:  0 policy actor:  1  step number:  25 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.0
siam score:  -0.7733203
maxi score, test score, baseline:  -0.7205400000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
maxi score, test score, baseline:  -0.7205400000000001 0.6734999999999999 0.6734999999999999
maxi score, test score, baseline:  -0.7205400000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
printing an ep nov before normalisation:  31.086259996222076
siam score:  -0.7709719
printing an ep nov before normalisation:  40.07084257947252
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  27 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.7205400000000001 0.6734999999999999 0.6734999999999999
printing an ep nov before normalisation:  0.17513512618506866
actor:  0 policy actor:  0  step number:  42 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  36 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.7177600000000002 0.6734999999999999 0.6734999999999999
maxi score, test score, baseline:  -0.7177600000000002 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.7177600000000002 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
actor:  1 policy actor:  1  step number:  33 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  33 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 49.62773641225926
maxi score, test score, baseline:  -0.7149200000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
actions average: 
K:  0  action  0 :  tensor([0.3909, 0.0020, 0.1067, 0.1213, 0.1714, 0.0941, 0.1137],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0038, 0.9291, 0.0076, 0.0206, 0.0017, 0.0024, 0.0348],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0751, 0.0152, 0.5993, 0.1206, 0.0538, 0.0740, 0.0620],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1425, 0.0122, 0.1479, 0.3319, 0.1114, 0.1325, 0.1216],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1231, 0.0029, 0.1175, 0.1495, 0.3873, 0.1042, 0.1155],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1330, 0.0057, 0.1557, 0.1359, 0.1110, 0.3426, 0.1161],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1516, 0.0152, 0.1500, 0.1380, 0.0953, 0.1108, 0.3390],
       grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 34.750921385628835
maxi score, test score, baseline:  -0.7149200000000001 0.6734999999999999 0.6734999999999999
maxi score, test score, baseline:  -0.7149200000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
printing an ep nov before normalisation:  50.59844736963844
actor:  1 policy actor:  1  step number:  38 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.173]
 [-0.148]
 [-0.152]
 [-0.152]
 [-0.176]
 [-0.176]
 [-0.174]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.173]
 [-0.148]
 [-0.152]
 [-0.152]
 [-0.176]
 [-0.176]
 [-0.174]]
printing an ep nov before normalisation:  50.876692797631534
maxi score, test score, baseline:  -0.7149200000000001 0.6734999999999999 0.6734999999999999
Printing some Q and Qe and total Qs values:  [[-0.029]
 [ 0.168]
 [-0.03 ]
 [-0.041]
 [-0.036]
 [-0.029]
 [-0.029]] [[43.135]
 [47.197]
 [50.493]
 [52.594]
 [53.044]
 [43.135]
 [43.135]] [[0.524]
 [0.795]
 [0.657]
 [0.685]
 [0.698]
 [0.524]
 [0.524]]
maxi score, test score, baseline:  -0.7149200000000001 0.6734999999999999 0.6734999999999999
actor:  1 policy actor:  1  step number:  33 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  59.76830518670784
maxi score, test score, baseline:  -0.7149200000000001 0.6734999999999999 0.6734999999999999
maxi score, test score, baseline:  -0.7149200000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
maxi score, test score, baseline:  -0.7149200000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
actor:  1 policy actor:  1  step number:  33 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.7149200000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
maxi score, test score, baseline:  -0.7149200000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
actor:  0 policy actor:  0  step number:  32 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.588]
 [0.507]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]] [[52.721]
 [65.079]
 [54.121]
 [54.121]
 [54.121]
 [54.121]
 [54.121]] [[2.014]
 [2.507]
 [1.933]
 [1.933]
 [1.933]
 [1.933]
 [1.933]]
maxi score, test score, baseline:  -0.7119400000000001 0.6734999999999999 0.6734999999999999
printing an ep nov before normalisation:  42.38326549530029
actor:  1 policy actor:  1  step number:  22 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  0.0012120914789193193
printing an ep nov before normalisation:  0.0014036926825156115
using another actor
actor:  1 policy actor:  1  step number:  28 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  52.06021124789749
printing an ep nov before normalisation:  45.07024344723233
using explorer policy with actor:  0
printing an ep nov before normalisation:  42.74474185576503
printing an ep nov before normalisation:  41.70791401514702
actor:  0 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.7088800000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
maxi score, test score, baseline:  -0.7088800000000001 0.6734999999999999 0.6734999999999999
printing an ep nov before normalisation:  45.954904556274414
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.661]
 [0.477]
 [0.488]
 [0.469]
 [0.481]
 [0.482]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.449]
 [0.661]
 [0.477]
 [0.488]
 [0.469]
 [0.481]
 [0.482]]
actor:  1 policy actor:  1  step number:  35 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  35.175370961307
Sims:  50 1 epoch:  61794 pick best:  False frame count:  61794
actor:  0 policy actor:  0  step number:  20 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.70566 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
Printing some Q and Qe and total Qs values:  [[-0.167]
 [-0.175]
 [-0.172]
 [-0.172]
 [-0.171]
 [-0.166]
 [-0.166]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.167]
 [-0.175]
 [-0.172]
 [-0.172]
 [-0.171]
 [-0.166]
 [-0.166]]
maxi score, test score, baseline:  -0.70566 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
printing an ep nov before normalisation:  37.543620266370475
maxi score, test score, baseline:  -0.70566 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
actor:  1 policy actor:  1  step number:  32 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.69 ]
 [0.69 ]
 [0.689]
 [0.676]
 [0.69 ]
 [0.69 ]
 [0.676]] [[36.11 ]
 [41.18 ]
 [38.938]
 [42.436]
 [41.599]
 [40.433]
 [42.436]] [[2.025]
 [2.211]
 [2.127]
 [2.243]
 [2.227]
 [2.183]
 [2.243]]
actor:  1 policy actor:  1  step number:  31 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.264]
 [0.529]
 [0.272]
 [0.308]
 [0.253]
 [0.268]
 [0.263]] [[34.146]
 [40.422]
 [30.028]
 [36.706]
 [30.927]
 [35.027]
 [30.888]] [[0.264]
 [0.529]
 [0.272]
 [0.308]
 [0.253]
 [0.268]
 [0.263]]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  0.003213521516158835
actor:  0 policy actor:  0  step number:  31 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.667
from probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
actor:  1 policy actor:  1  step number:  48 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.7028200000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
maxi score, test score, baseline:  -0.7028200000000001 0.6734999999999999 0.6734999999999999
actor:  1 policy actor:  1  step number:  24 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  23 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  0.667
from probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
actor:  0 policy actor:  1  step number:  35 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.272]
 [0.328]
 [0.277]
 [0.303]
 [0.27 ]
 [0.27 ]
 [0.28 ]] [[22.176]
 [34.421]
 [22.086]
 [24.254]
 [14.161]
 [21.559]
 [14.91 ]] [[0.272]
 [0.328]
 [0.277]
 [0.303]
 [0.27 ]
 [0.27 ]
 [0.28 ]]
maxi score, test score, baseline:  -0.6970600000000001 0.6734999999999999 0.6734999999999999
maxi score, test score, baseline:  -0.6970600000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
Printing some Q and Qe and total Qs values:  [[0.235]
 [0.34 ]
 [0.76 ]
 [0.458]
 [0.308]
 [0.389]
 [0.389]] [[41.739]
 [45.752]
 [40.196]
 [38.834]
 [40.42 ]
 [35.354]
 [35.354]] [[0.235]
 [0.34 ]
 [0.76 ]
 [0.458]
 [0.308]
 [0.389]
 [0.389]]
printing an ep nov before normalisation:  44.54663194031689
maxi score, test score, baseline:  -0.6970600000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
actor:  0 policy actor:  1  step number:  38 total reward:  0.4099999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6942400000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
printing an ep nov before normalisation:  18.131937604414876
maxi score, test score, baseline:  -0.6942400000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  34 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.258]
 [0.31 ]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]] [[43.238]
 [61.974]
 [43.238]
 [43.238]
 [43.238]
 [43.238]
 [43.238]] [[0.258]
 [0.31 ]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]]
maxi score, test score, baseline:  -0.6942400000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
actor:  1 policy actor:  1  step number:  36 total reward:  0.44999999999999984  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  37.4492735200559
printing an ep nov before normalisation:  0.0018951085803564638
maxi score, test score, baseline:  -0.6942400000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
printing an ep nov before normalisation:  52.50383547204134
printing an ep nov before normalisation:  55.101189002900625
printing an ep nov before normalisation:  45.582764550458805
printing an ep nov before normalisation:  50.56009463404125
maxi score, test score, baseline:  -0.6942400000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
printing an ep nov before normalisation:  61.03034895285729
actor:  1 policy actor:  1  step number:  24 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.368]
 [0.046]
 [0.087]
 [0.3  ]
 [0.3  ]
 [0.387]] [[47.507]
 [47.033]
 [44.813]
 [41.464]
 [46.017]
 [46.017]
 [46.586]] [[1.171]
 [1.108]
 [0.72 ]
 [0.661]
 [1.01 ]
 [1.01 ]
 [1.114]]
printing an ep nov before normalisation:  55.34260785211161
maxi score, test score, baseline:  -0.6942400000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
from probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
maxi score, test score, baseline:  -0.6942400000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
actor:  1 policy actor:  1  step number:  39 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  0
printing an ep nov before normalisation:  42.16585041546436
printing an ep nov before normalisation:  20.144454070499965
printing an ep nov before normalisation:  33.22267532348633
Printing some Q and Qe and total Qs values:  [[-0.163]
 [-0.118]
 [-0.143]
 [-0.163]
 [-0.163]
 [-0.163]
 [-0.163]] [[47.157]
 [40.838]
 [44.317]
 [47.157]
 [47.157]
 [47.157]
 [47.157]] [[0.959]
 [0.732]
 [0.857]
 [0.959]
 [0.959]
 [0.959]
 [0.959]]
printing an ep nov before normalisation:  41.44454190060759
actor:  1 policy actor:  1  step number:  33 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  15.61872959136963
printing an ep nov before normalisation:  53.797200993397325
Printing some Q and Qe and total Qs values:  [[0.781]
 [0.781]
 [0.781]
 [0.781]
 [0.781]
 [0.781]
 [0.781]] [[49.733]
 [49.733]
 [49.733]
 [49.733]
 [49.733]
 [49.733]
 [49.733]] [[1.766]
 [1.766]
 [1.766]
 [1.766]
 [1.766]
 [1.766]
 [1.766]]
actor:  0 policy actor:  1  step number:  37 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  26 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.137]
 [-0.127]
 [-0.142]
 [-0.141]
 [-0.14 ]
 [-0.137]
 [-0.142]] [[37.847]
 [34.541]
 [38.098]
 [37.332]
 [36.626]
 [35.845]
 [38.098]] [[1.001]
 [0.817]
 [1.012]
 [0.968]
 [0.927]
 [0.883]
 [1.012]]
maxi score, test score, baseline:  -0.6913600000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
printing an ep nov before normalisation:  37.066661806786804
printing an ep nov before normalisation:  58.63794020527128
maxi score, test score, baseline:  -0.6913600000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
line 256 mcts: sample exp_bonus 40.07444122351903
maxi score, test score, baseline:  -0.6913600000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
actor:  1 policy actor:  1  step number:  30 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  47.30847912274493
printing an ep nov before normalisation:  42.6426581160662
actor:  0 policy actor:  0  step number:  39 total reward:  0.0799999999999994  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  54.599903364689695
printing an ep nov before normalisation:  50.426235041307045
actor:  1 policy actor:  1  step number:  36 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  1.667
from probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
maxi score, test score, baseline:  -0.6892 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
printing an ep nov before normalisation:  33.818709478981795
printing an ep nov before normalisation:  56.98823275680886
actor:  1 policy actor:  1  step number:  35 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.0
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  58.76693929581323
maxi score, test score, baseline:  -0.6892 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
maxi score, test score, baseline:  -0.6892 0.6734999999999999 0.6734999999999999
actor:  0 policy actor:  0  step number:  30 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  33 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6835 0.6734999999999999 0.6734999999999999
Printing some Q and Qe and total Qs values:  [[-0.18 ]
 [-0.147]
 [-0.189]
 [-0.184]
 [-0.19 ]
 [-0.184]
 [-0.183]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.18 ]
 [-0.147]
 [-0.189]
 [-0.184]
 [-0.19 ]
 [-0.184]
 [-0.183]]
actor:  1 policy actor:  1  step number:  34 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  38.49385942738708
actor:  1 policy actor:  1  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6835 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6835 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
from probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
Printing some Q and Qe and total Qs values:  [[-0.022]
 [-0.004]
 [-0.022]
 [-0.034]
 [-0.022]
 [-0.022]
 [-0.022]] [[46.358]
 [43.473]
 [46.358]
 [45.069]
 [46.358]
 [46.358]
 [46.358]] [[0.668]
 [0.622]
 [0.668]
 [0.628]
 [0.668]
 [0.668]
 [0.668]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6835 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
line 256 mcts: sample exp_bonus 41.64592325418661
maxi score, test score, baseline:  -0.6835 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
actions average: 
K:  2  action  0 :  tensor([0.5934, 0.0083, 0.0472, 0.0696, 0.1757, 0.0428, 0.0631],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0112, 0.9012, 0.0176, 0.0123, 0.0041, 0.0073, 0.0461],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1104, 0.0504, 0.4711, 0.0957, 0.0767, 0.1006, 0.0950],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1775, 0.0247, 0.1311, 0.3123, 0.1258, 0.1238, 0.1049],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2600, 0.0286, 0.1561, 0.1351, 0.1626, 0.1282, 0.1295],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1643, 0.0161, 0.1477, 0.2030, 0.1325, 0.2012, 0.1353],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0619, 0.2540, 0.0831, 0.0484, 0.0468, 0.0447, 0.4610],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.6835 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
actor:  0 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.68042 0.6734999999999999 0.6734999999999999
maxi score, test score, baseline:  -0.68042 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
from probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
maxi score, test score, baseline:  -0.68042 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
maxi score, test score, baseline:  -0.68042 0.6734999999999999 0.6734999999999999
actor:  1 policy actor:  1  step number:  24 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  26 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  2  action  0 :  tensor([0.5151, 0.0063, 0.0902, 0.1016, 0.1151, 0.0771, 0.0947],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0250, 0.9436, 0.0058, 0.0052, 0.0045, 0.0025, 0.0133],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0652, 0.0527, 0.5576, 0.0876, 0.0544, 0.1193, 0.0632],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0823, 0.1517, 0.0685, 0.4814, 0.0727, 0.0674, 0.0760],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1522, 0.1059, 0.1057, 0.1245, 0.3237, 0.0808, 0.1072],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1039, 0.0853, 0.1969, 0.1747, 0.0792, 0.2498, 0.1103],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2416, 0.0057, 0.0958, 0.1002, 0.1032, 0.0862, 0.3674],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.68042 0.6734999999999999 0.6734999999999999
maxi score, test score, baseline:  -0.68042 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
Printing some Q and Qe and total Qs values:  [[0.134]
 [0.224]
 [0.134]
 [0.078]
 [0.134]
 [0.134]
 [0.134]] [[45.986]
 [53.815]
 [44.575]
 [40.46 ]
 [44.575]
 [44.575]
 [44.575]] [[0.606]
 [0.893]
 [0.571]
 [0.412]
 [0.571]
 [0.571]
 [0.571]]
actor:  0 policy actor:  0  step number:  35 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  28 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6781 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
maxi score, test score, baseline:  -0.6781 0.6734999999999999 0.6734999999999999
Printing some Q and Qe and total Qs values:  [[-0.178]
 [-0.216]
 [-0.173]
 [-0.173]
 [-0.177]
 [-0.18 ]
 [-0.175]] [[35.101]
 [39.648]
 [34.414]
 [35.002]
 [34.699]
 [36.138]
 [34.764]] [[0.404]
 [0.512]
 [0.386]
 [0.405]
 [0.392]
 [0.435]
 [0.395]]
using another actor
from probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
maxi score, test score, baseline:  -0.6781 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
printing an ep nov before normalisation:  52.2206722667007
maxi score, test score, baseline:  -0.6781 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.62796660303931
printing an ep nov before normalisation:  43.32304433138629
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.521]
 [0.383]
 [0.4  ]
 [0.402]
 [0.4  ]
 [0.405]] [[28.901]
 [29.848]
 [28.347]
 [28.046]
 [28.885]
 [28.731]
 [27.94 ]] [[1.403]
 [1.546]
 [1.307]
 [1.304]
 [1.362]
 [1.35 ]
 [1.301]]
printing an ep nov before normalisation:  41.16841793060303
actor:  1 policy actor:  1  step number:  30 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.333
from probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
maxi score, test score, baseline:  -0.6781 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
maxi score, test score, baseline:  -0.6781 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.6781 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
maxi score, test score, baseline:  -0.6781 0.6734999999999999 0.6734999999999999
Printing some Q and Qe and total Qs values:  [[0.724]
 [0.764]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]] [[41.178]
 [40.661]
 [41.178]
 [41.178]
 [41.178]
 [41.178]
 [41.178]] [[0.724]
 [0.764]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]]
printing an ep nov before normalisation:  42.86823749542236
actor:  0 policy actor:  0  step number:  29 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6751400000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
actor:  0 policy actor:  0  step number:  34 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.196]
 [-0.148]
 [-0.16 ]
 [-0.147]
 [-0.171]
 [-0.132]
 [-0.17 ]] [[43.214]
 [45.625]
 [39.093]
 [41.129]
 [41.659]
 [38.151]
 [46.508]] [[0.478]
 [0.588]
 [0.409]
 [0.474]
 [0.464]
 [0.413]
 [0.589]]
printing an ep nov before normalisation:  46.401715238768844
line 256 mcts: sample exp_bonus 45.08902594447136
Printing some Q and Qe and total Qs values:  [[-0.092]
 [-0.012]
 [-0.092]
 [-0.092]
 [-0.092]
 [-0.092]
 [-0.092]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.092]
 [-0.012]
 [-0.092]
 [-0.092]
 [-0.092]
 [-0.092]
 [-0.092]]
maxi score, test score, baseline:  -0.6723600000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6723600000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
from probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
printing an ep nov before normalisation:  44.73690127533034
maxi score, test score, baseline:  -0.6723600000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
actor:  0 policy actor:  1  step number:  21 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6692000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
maxi score, test score, baseline:  -0.6692000000000001 0.6734999999999999 0.6734999999999999
maxi score, test score, baseline:  -0.6692000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
siam score:  -0.79444957
printing an ep nov before normalisation:  42.55964361200952
printing an ep nov before normalisation:  38.54308618265444
printing an ep nov before normalisation:  42.930246885483776
printing an ep nov before normalisation:  50.825484593709305
actor:  1 policy actor:  1  step number:  26 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.333
from probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
maxi score, test score, baseline:  -0.6692000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
maxi score, test score, baseline:  -0.6692000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6692000000000001 0.6734999999999999 0.6734999999999999
maxi score, test score, baseline:  -0.6692000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.259]
 [0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]] [[45.753]
 [46.047]
 [45.753]
 [45.753]
 [45.753]
 [45.753]
 [45.753]] [[1.696]
 [1.961]
 [1.696]
 [1.696]
 [1.696]
 [1.696]
 [1.696]]
printing an ep nov before normalisation:  39.06763501245522
printing an ep nov before normalisation:  56.858647738791234
actor:  1 policy actor:  1  step number:  46 total reward:  0.24999999999999967  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6692000000000001 0.6734999999999999 0.6734999999999999
actor:  0 policy actor:  1  step number:  40 total reward:  0.20999999999999952  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6667800000000002 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
maxi score, test score, baseline:  -0.6667800000000002 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
maxi score, test score, baseline:  -0.6667800000000002 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
printing an ep nov before normalisation:  24.791875197873225
printing an ep nov before normalisation:  0.0006284153374735979
actor:  1 policy actor:  1  step number:  46 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  53.52699479413333
maxi score, test score, baseline:  -0.6667800000000002 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
printing an ep nov before normalisation:  47.76716709377654
Printing some Q and Qe and total Qs values:  [[-0.131]
 [-0.13 ]
 [-0.133]
 [-0.132]
 [-0.131]
 [-0.134]
 [-0.131]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.131]
 [-0.13 ]
 [-0.133]
 [-0.132]
 [-0.131]
 [-0.134]
 [-0.131]]
printing an ep nov before normalisation:  59.2847694677341
actor:  1 policy actor:  1  step number:  26 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  60.226524542481904
printing an ep nov before normalisation:  47.1810908641282
actor:  1 policy actor:  1  step number:  22 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6667800000000002 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
actor:  1 policy actor:  1  step number:  25 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.333
from probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
actor:  1 policy actor:  1  step number:  39 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  45.80274569929762
printing an ep nov before normalisation:  41.91345702703959
actor:  1 policy actor:  1  step number:  30 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  26 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.005]
 [ 0.353]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[45.153]
 [48.74 ]
 [45.153]
 [45.153]
 [45.153]
 [45.153]
 [45.153]] [[1.153]
 [1.685]
 [1.153]
 [1.153]
 [1.153]
 [1.153]
 [1.153]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6637200000000001 0.6734999999999999 0.6734999999999999
maxi score, test score, baseline:  -0.6637200000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08774488309808626, 0.08774488309808626, 0.5612755845095687, 0.08774488309808626, 0.08774488309808626, 0.08774488309808626]
actor:  1 policy actor:  1  step number:  19 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6637200000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08671173019766626, 0.08671173019766626, 0.5546583616576775, 0.08671173019766626, 0.08671173019766626, 0.09849471755165759]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6637200000000001 0.6734999999999999 0.6734999999999999
Printing some Q and Qe and total Qs values:  [[0.179]
 [0.179]
 [0.595]
 [0.179]
 [0.179]
 [0.179]
 [0.179]] [[46.776]
 [46.776]
 [48.331]
 [46.776]
 [46.776]
 [46.776]
 [46.776]] [[0.891]
 [0.891]
 [1.343]
 [0.891]
 [0.891]
 [0.891]
 [0.891]]
Printing some Q and Qe and total Qs values:  [[0.619]
 [0.66 ]
 [0.619]
 [0.619]
 [0.619]
 [0.619]
 [0.619]] [[48.4]
 [47. ]
 [48.4]
 [48.4]
 [48.4]
 [48.4]
 [48.4]] [[1.269]
 [1.275]
 [1.269]
 [1.269]
 [1.269]
 [1.269]
 [1.269]]
siam score:  -0.80714643
maxi score, test score, baseline:  -0.6637200000000001 0.6734999999999999 0.6734999999999999
Printing some Q and Qe and total Qs values:  [[-0.045]
 [-0.018]
 [-0.045]
 [-0.045]
 [-0.045]
 [-0.045]
 [-0.045]] [[37.321]
 [53.118]
 [37.321]
 [37.321]
 [37.321]
 [37.321]
 [37.321]] [[0.436]
 [0.78 ]
 [0.436]
 [0.436]
 [0.436]
 [0.436]
 [0.436]]
maxi score, test score, baseline:  -0.6637200000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08671173019766626, 0.08671173019766626, 0.5546583616576775, 0.08671173019766626, 0.08671173019766626, 0.09849471755165759]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  39.83954191207886
printing an ep nov before normalisation:  42.54030227661133
Printing some Q and Qe and total Qs values:  [[0.594]
 [0.802]
 [0.594]
 [0.531]
 [0.594]
 [0.594]
 [0.594]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.594]
 [0.802]
 [0.594]
 [0.531]
 [0.594]
 [0.594]
 [0.594]]
printing an ep nov before normalisation:  57.27401702838648
maxi score, test score, baseline:  -0.6637200000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08671173019766626, 0.08671173019766626, 0.5546583616576775, 0.08671173019766626, 0.08671173019766626, 0.09849471755165759]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.6637200000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08671696625551104, 0.08671696625551104, 0.5546918979925316, 0.08671696625551104, 0.08671696625551104, 0.0984402369854242]
maxi score, test score, baseline:  -0.6637200000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08671696625551104, 0.08671696625551104, 0.5546918979925316, 0.08671696625551104, 0.08671696625551104, 0.0984402369854242]
actor:  1 policy actor:  1  step number:  26 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6637200000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08671696625551104, 0.08671696625551104, 0.5546918979925316, 0.08671696625551104, 0.08671696625551104, 0.0984402369854242]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  38.34784295823839
printing an ep nov before normalisation:  67.8150308517371
printing an ep nov before normalisation:  38.38613986968994
maxi score, test score, baseline:  -0.6637200000000001 0.6734999999999999 0.6734999999999999
actor:  1 policy actor:  1  step number:  24 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.6637200000000001 0.6734999999999999 0.6734999999999999
probs:  [0.086719564432335, 0.086719564432335, 0.5547085390096026, 0.086719564432335, 0.086719564432335, 0.0984132032610575]
printing an ep nov before normalisation:  19.108734032119198
maxi score, test score, baseline:  -0.6637200000000001 0.6734999999999999 0.6734999999999999
probs:  [0.086719564432335, 0.086719564432335, 0.5547085390096026, 0.086719564432335, 0.086719564432335, 0.0984132032610575]
actor:  1 policy actor:  1  step number:  37 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6637200000000001 0.6734999999999999 0.6734999999999999
maxi score, test score, baseline:  -0.6637200000000001 0.6734999999999999 0.6734999999999999
probs:  [0.086719564432335, 0.086719564432335, 0.5547085390096026, 0.086719564432335, 0.086719564432335, 0.0984132032610575]
maxi score, test score, baseline:  -0.6637200000000001 0.6734999999999999 0.6734999999999999
probs:  [0.086719564432335, 0.086719564432335, 0.5547085390096026, 0.086719564432335, 0.086719564432335, 0.0984132032610575]
printing an ep nov before normalisation:  56.98550016334357
printing an ep nov before normalisation:  1.7272220134145755e-05
printing an ep nov before normalisation:  49.50584286319912
maxi score, test score, baseline:  -0.6637200000000001 0.6734999999999999 0.6734999999999999
printing an ep nov before normalisation:  0.004502760868945188
printing an ep nov before normalisation:  47.4317892603886
actor:  1 policy actor:  1  step number:  26 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  0.667
siam score:  -0.80626875
printing an ep nov before normalisation:  44.10037168510207
printing an ep nov before normalisation:  23.968362404934005
printing an ep nov before normalisation:  9.026493210635351
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6637200000000001 0.6734999999999999 0.6734999999999999
printing an ep nov before normalisation:  0.003166328557426823
maxi score, test score, baseline:  -0.6637200000000001 0.6734999999999999 0.6734999999999999
actor:  1 policy actor:  1  step number:  29 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  51.20536062359888
printing an ep nov before normalisation:  41.068925857543945
maxi score, test score, baseline:  -0.6637200000000001 0.6734999999999999 0.6734999999999999
actor:  1 policy actor:  1  step number:  42 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6637200000000001 0.6734999999999999 0.6734999999999999
probs:  [0.086719564432335, 0.086719564432335, 0.5547085390096026, 0.086719564432335, 0.086719564432335, 0.0984132032610575]
maxi score, test score, baseline:  -0.6637200000000001 0.6734999999999999 0.6734999999999999
maxi score, test score, baseline:  -0.6637200000000001 0.6734999999999999 0.6734999999999999
probs:  [0.086719564432335, 0.086719564432335, 0.5547085390096026, 0.086719564432335, 0.086719564432335, 0.0984132032610575]
actor:  1 policy actor:  1  step number:  43 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  39.982519504875334
actor:  1 policy actor:  1  step number:  27 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6637200000000001 0.6734999999999999 0.6734999999999999
probs:  [0.086719564432335, 0.086719564432335, 0.5547085390096026, 0.086719564432335, 0.086719564432335, 0.0984132032610575]
actions average: 
K:  2  action  0 :  tensor([0.3167, 0.0013, 0.1404, 0.1763, 0.1245, 0.0998, 0.1411],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0080, 0.9336, 0.0093, 0.0146, 0.0042, 0.0039, 0.0264],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1499, 0.0526, 0.2491, 0.1467, 0.1404, 0.1274, 0.1338],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1194, 0.0994, 0.0907, 0.4188, 0.0974, 0.0809, 0.0934],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2273, 0.0262, 0.1202, 0.1932, 0.2134, 0.1067, 0.1130],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1151, 0.0042, 0.2208, 0.1416, 0.1148, 0.3047, 0.0988],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1790, 0.0014, 0.1527, 0.1586, 0.1259, 0.1016, 0.2808],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  27 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  26 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.0
siam score:  -0.7987701
printing an ep nov before normalisation:  50.09802244299033
printing an ep nov before normalisation:  46.68797573956664
printing an ep nov before normalisation:  51.29182380106272
maxi score, test score, baseline:  -0.6637200000000001 0.6734999999999999 0.6734999999999999
probs:  [0.086719564432335, 0.086719564432335, 0.5547085390096026, 0.086719564432335, 0.086719564432335, 0.0984132032610575]
maxi score, test score, baseline:  -0.6637200000000001 0.6734999999999999 0.6734999999999999
actor:  0 policy actor:  0  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  25 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.66074 0.6734999999999999 0.6734999999999999
probs:  [0.086719564432335, 0.086719564432335, 0.5547085390096026, 0.086719564432335, 0.086719564432335, 0.0984132032610575]
maxi score, test score, baseline:  -0.66074 0.6734999999999999 0.6734999999999999
probs:  [0.086719564432335, 0.086719564432335, 0.5547085390096026, 0.086719564432335, 0.086719564432335, 0.0984132032610575]
printing an ep nov before normalisation:  36.26505884948184
printing an ep nov before normalisation:  34.701679361101704
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.54 ]
 [0.475]
 [0.464]
 [0.484]
 [0.494]
 [0.464]] [[32.838]
 [31.575]
 [32.464]
 [32.   ]
 [32.51 ]
 [32.584]
 [32.   ]] [[0.8  ]
 [0.846]
 [0.797]
 [0.778]
 [0.808]
 [0.819]
 [0.778]]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  36.16630600566492
maxi score, test score, baseline:  -0.66074 0.6734999999999999 0.6734999999999999
probs:  [0.086719564432335, 0.086719564432335, 0.5547085390096026, 0.086719564432335, 0.086719564432335, 0.0984132032610575]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  57.829292752217825
printing an ep nov before normalisation:  66.47850386820603
Printing some Q and Qe and total Qs values:  [[-0.168]
 [-0.167]
 [-0.17 ]
 [-0.167]
 [-0.171]
 [-0.171]
 [-0.169]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.168]
 [-0.167]
 [-0.17 ]
 [-0.167]
 [-0.171]
 [-0.171]
 [-0.169]]
maxi score, test score, baseline:  -0.66074 0.6734999999999999 0.6734999999999999
probs:  [0.086719564432335, 0.086719564432335, 0.5547085390096026, 0.086719564432335, 0.086719564432335, 0.0984132032610575]
maxi score, test score, baseline:  -0.66074 0.6734999999999999 0.6734999999999999
printing an ep nov before normalisation:  75.0752372017691
Printing some Q and Qe and total Qs values:  [[-0.162]
 [-0.16 ]
 [-0.171]
 [-0.174]
 [-0.16 ]
 [-0.171]
 [-0.159]] [[44.614]
 [43.27 ]
 [43.766]
 [44.493]
 [42.553]
 [45.736]
 [44.529]] [[0.923]
 [0.856]
 [0.87 ]
 [0.904]
 [0.819]
 [0.971]
 [0.922]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.21764457632775
actor:  1 policy actor:  1  step number:  27 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  33 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.0
from probs:  [0.086719564432335, 0.086719564432335, 0.5547085390096026, 0.086719564432335, 0.086719564432335, 0.0984132032610575]
maxi score, test score, baseline:  -0.66074 0.6734999999999999 0.6734999999999999
probs:  [0.086719564432335, 0.086719564432335, 0.5547085390096026, 0.086719564432335, 0.086719564432335, 0.0984132032610575]
maxi score, test score, baseline:  -0.66074 0.6734999999999999 0.6734999999999999
actor:  1 policy actor:  1  step number:  34 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  29 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6580600000000001 0.6734999999999999 0.6734999999999999
probs:  [0.086719564432335, 0.086719564432335, 0.5547085390096026, 0.086719564432335, 0.086719564432335, 0.0984132032610575]
actor:  1 policy actor:  1  step number:  47 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  1.0
siam score:  -0.80000246
maxi score, test score, baseline:  -0.6580600000000001 0.6734999999999999 0.6734999999999999
maxi score, test score, baseline:  -0.6580600000000001 0.6734999999999999 0.6734999999999999
probs:  [0.086719564432335, 0.086719564432335, 0.5547085390096026, 0.086719564432335, 0.086719564432335, 0.0984132032610575]
printing an ep nov before normalisation:  56.58307832794794
actor:  1 policy actor:  1  step number:  26 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  26 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  21 total reward:  0.7  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  36.96166276931763
maxi score, test score, baseline:  -0.6580600000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08593769496964758, 0.08593769496964758, 0.5613447403470355, 0.08593769496964758, 0.08593769496964758, 0.09490447977437425]
actor:  1 policy actor:  1  step number:  36 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  34 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6580600000000001 0.6734999999999999 0.6734999999999999
probs:  [0.08593769496964758, 0.08593769496964758, 0.5613447403470355, 0.08593769496964758, 0.08593769496964758, 0.09490447977437425]
from probs:  [0.08593769496964758, 0.08593769496964758, 0.5613447403470355, 0.08593769496964758, 0.08593769496964758, 0.09490447977437425]
using explorer policy with actor:  1
printing an ep nov before normalisation:  66.46668835624547
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.576]
 [0.065]
 [0.485]
 [0.372]
 [0.04 ]
 [0.533]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.313]
 [0.576]
 [0.065]
 [0.485]
 [0.372]
 [0.04 ]
 [0.533]]
using another actor
actor:  1 policy actor:  1  step number:  29 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  55.27152419090271
siam score:  -0.8020229
printing an ep nov before normalisation:  69.5469823844203
actor:  0 policy actor:  1  step number:  28 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.65488 0.6734999999999999 0.6734999999999999
probs:  [0.08593769496964758, 0.08593769496964758, 0.5613447403470355, 0.08593769496964758, 0.08593769496964758, 0.09490447977437425]
printing an ep nov before normalisation:  49.487329827842395
maxi score, test score, baseline:  -0.65488 0.6734999999999999 0.6734999999999999
probs:  [0.08593769496964758, 0.08593769496964758, 0.5613447403470355, 0.08593769496964758, 0.08593769496964758, 0.09490447977437425]
Printing some Q and Qe and total Qs values:  [[-0.001]
 [-0.001]
 [-0.001]
 [-0.057]
 [-0.001]
 [-0.001]
 [-0.057]] [[ 0.487]
 [ 0.491]
 [ 0.458]
 [44.767]
 [ 0.436]
 [ 0.403]
 [44.767]] [[0.002]
 [0.002]
 [0.002]
 [0.21 ]
 [0.002]
 [0.001]
 [0.21 ]]
printing an ep nov before normalisation:  69.99265496220316
Printing some Q and Qe and total Qs values:  [[-0.119]
 [-0.107]
 [-0.076]
 [-0.118]
 [-0.119]
 [-0.069]
 [-0.119]] [[47.214]
 [51.997]
 [49.311]
 [48.749]
 [47.214]
 [48.877]
 [47.214]] [[0.957]
 [1.177]
 [1.091]
 [1.025]
 [0.957]
 [1.08 ]
 [0.957]]
maxi score, test score, baseline:  -0.65488 0.6734999999999999 0.6734999999999999
actor:  1 policy actor:  1  step number:  19 total reward:  0.78  reward:  1.0 rdn_beta:  1.667
using another actor
actor:  1 policy actor:  1  step number:  29 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.65488 0.6734999999999999 0.6734999999999999
probs:  [0.07992491829949562, 0.07992491829949562, 0.1241109476559488, 0.07992491829949562, 0.555355974129321, 0.08075832331624323]
from probs:  [0.07992491829949562, 0.07992491829949562, 0.1241109476559488, 0.07992491829949562, 0.555355974129321, 0.08075832331624323]
printing an ep nov before normalisation:  35.21411425277059
actor:  0 policy actor:  1  step number:  24 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.6516200000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07992491829949562, 0.07992491829949562, 0.1241109476559488, 0.07992491829949562, 0.555355974129321, 0.08075832331624323]
printing an ep nov before normalisation:  14.074867043866593
Printing some Q and Qe and total Qs values:  [[-0.056]
 [-0.059]
 [-0.056]
 [-0.058]
 [-0.056]
 [-0.056]
 [-0.056]] [[52.86 ]
 [52.038]
 [52.86 ]
 [55.05 ]
 [55.426]
 [52.86 ]
 [52.86 ]] [[1.299]
 [1.256]
 [1.299]
 [1.404]
 [1.425]
 [1.299]
 [1.299]]
actor:  0 policy actor:  0  step number:  34 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.64864 0.6734999999999999 0.6734999999999999
probs:  [0.0798715814964228, 0.0798715814964228, 0.12474236476900671, 0.0798715814964228, 0.5549249888940639, 0.0807179018476608]
maxi score, test score, baseline:  -0.64864 0.6734999999999999 0.6734999999999999
probs:  [0.0798715814964228, 0.0798715814964228, 0.12474236476900671, 0.0798715814964228, 0.5549249888940639, 0.0807179018476608]
maxi score, test score, baseline:  -0.64864 0.6734999999999999 0.6734999999999999
probs:  [0.0798715814964228, 0.0798715814964228, 0.12474236476900671, 0.0798715814964228, 0.5549249888940639, 0.0807179018476608]
actor:  1 policy actor:  1  step number:  34 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  -0.64864 0.6734999999999999 0.6734999999999999
probs:  [0.07981832935998126, 0.07981832935998126, 0.1253727795730245, 0.07981832935998126, 0.5544946878030732, 0.08067754454395853]
printing an ep nov before normalisation:  47.28331494478564
printing an ep nov before normalisation:  44.18866166944935
printing an ep nov before normalisation:  46.94982165581623
maxi score, test score, baseline:  -0.64864 0.6734999999999999 0.6734999999999999
printing an ep nov before normalisation:  33.468899726867676
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.64864 0.6734999999999999 0.6734999999999999
probs:  [0.07978287487320171, 0.07978287487320171, 0.12579250044862178, 0.07978287487320171, 0.5542081996852021, 0.08065067524657098]
Printing some Q and Qe and total Qs values:  [[-0.042]
 [-0.064]
 [-0.064]
 [-0.064]
 [-0.064]
 [-0.064]
 [-0.064]] [[43.688]
 [35.381]
 [35.381]
 [35.381]
 [35.381]
 [35.381]
 [35.381]] [[1.516]
 [0.921]
 [0.921]
 [0.921]
 [0.921]
 [0.921]
 [0.921]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.64864 0.6734999999999999 0.6734999999999999
probs:  [0.07978287487320171, 0.07978287487320171, 0.12579250044862178, 0.07978287487320171, 0.5542081996852021, 0.08065067524657098]
maxi score, test score, baseline:  -0.64864 0.6734999999999999 0.6734999999999999
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  34 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.64864 0.6734999999999999 0.6734999999999999
probs:  [0.07976516168873161, 0.07976516168873161, 0.12600219445270192, 0.07976516168873161, 0.5540650692286283, 0.08063725125247483]
printing an ep nov before normalisation:  38.3640456199646
actor:  0 policy actor:  0  step number:  24 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  36 total reward:  0.24999999999999956  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6429200000000002 0.6734999999999999 0.6734999999999999
probs:  [0.07974745786696638, 0.07974745786696638, 0.12621177761826627, 0.07974745786696638, 0.5539220144268995, 0.08062383435393501]
maxi score, test score, baseline:  -0.6429200000000002 0.6734999999999999 0.6734999999999999
probs:  [0.07972976340048471, 0.07972976340048471, 0.126421250033171, 0.07972976340048471, 0.5537790352200479, 0.08061042454532698]
actor:  1 policy actor:  1  step number:  35 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  33 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  55.61427654278122
printing an ep nov before normalisation:  66.21533730044435
maxi score, test score, baseline:  -0.6429200000000002 0.6734999999999999 0.6734999999999999
probs:  [0.0796767360586451, 0.0796767360586451, 0.12704900365109562, 0.0796767360586451, 0.5533505505700289, 0.08057023760294012]
Printing some Q and Qe and total Qs values:  [[0.123]
 [0.521]
 [0.183]
 [0.183]
 [0.183]
 [0.183]
 [0.183]] [[54.165]
 [51.451]
 [50.861]
 [50.861]
 [50.861]
 [50.861]
 [50.861]] [[0.838]
 [1.165]
 [0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]]
maxi score, test score, baseline:  -0.6429200000000002 0.6734999999999999 0.6734999999999999
probs:  [0.07964143113813084, 0.07964143113813084, 0.12746695391627047, 0.07964143113813084, 0.553065271014507, 0.08054348165483015]
maxi score, test score, baseline:  -0.6429200000000002 0.6734999999999999 0.6734999999999999
probs:  [0.07964143113813084, 0.07964143113813084, 0.12746695391627047, 0.07964143113813084, 0.553065271014507, 0.08054348165483015]
maxi score, test score, baseline:  -0.6429200000000002 0.6734999999999999 0.6734999999999999
probs:  [0.0796237926479406, 0.0796237926479406, 0.12767576366700487, 0.0796237926479406, 0.552922744121138, 0.08053011426803547]
actor:  0 policy actor:  1  step number:  28 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
using another actor
using another actor
actions average: 
K:  0  action  0 :  tensor([0.3942, 0.0029, 0.1041, 0.1145, 0.1943, 0.0799, 0.1101],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0009,     0.9898,     0.0018,     0.0023,     0.0003,     0.0007,
            0.0043], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1210, 0.0149, 0.3876, 0.1314, 0.1110, 0.1119, 0.1223],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1353, 0.0264, 0.1318, 0.3328, 0.1275, 0.1079, 0.1383],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1749, 0.0042, 0.1567, 0.1775, 0.2007, 0.1331, 0.1528],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1300, 0.0015, 0.1359, 0.1609, 0.1415, 0.3031, 0.1272],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1246, 0.1016, 0.1573, 0.2430, 0.1159, 0.0956, 0.1620],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  34 total reward:  0.3299999999999996  reward:  1.0 rdn_beta:  2.0
from probs:  [0.07960616346130353, 0.07960616346130353, 0.1278844632794798, 0.07960616346130353, 0.552780292404641, 0.08051675393196851]
actor:  0 policy actor:  1  step number:  37 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  22 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2.0
using another actor
Printing some Q and Qe and total Qs values:  [[-0.033]
 [-0.02 ]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.033]] [[33.908]
 [42.715]
 [33.908]
 [33.908]
 [33.908]
 [33.908]
 [33.908]] [[0.538]
 [0.922]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]]
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.513]
 [0.744]
 [0.513]
 [0.513]
 [0.513]
 [0.513]] [[44.352]
 [44.352]
 [49.753]
 [44.352]
 [44.352]
 [44.352]
 [44.352]] [[0.513]
 [0.513]
 [0.744]
 [0.513]
 [0.513]
 [0.513]
 [0.513]]
actor:  0 policy actor:  0  step number:  26 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.634 0.6734999999999999 0.6734999999999999
maxi score, test score, baseline:  -0.634 0.6734999999999999 0.6734999999999999
probs:  [0.07960653203905707, 0.07960653203905707, 0.1278850557612192, 0.07960653203905707, 0.5527828554584604, 0.0805124926631492]
printing an ep nov before normalisation:  34.006100683861824
maxi score, test score, baseline:  -0.634 0.6734999999999999 0.6734999999999999
probs:  [0.07960653203905707, 0.07960653203905707, 0.1278850557612192, 0.07960653203905707, 0.5527828554584604, 0.0805124926631492]
printing an ep nov before normalisation:  28.02618980407715
maxi score, test score, baseline:  -0.634 0.6734999999999999 0.6734999999999999
probs:  [0.07960653203905707, 0.07960653203905707, 0.1278850557612192, 0.07960653203905707, 0.5527828554584604, 0.0805124926631492]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  20.47639168288716
Printing some Q and Qe and total Qs values:  [[-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]] [[41.326]
 [41.326]
 [41.326]
 [41.326]
 [41.326]
 [41.326]
 [41.326]] [[0.296]
 [0.296]
 [0.296]
 [0.296]
 [0.296]
 [0.296]
 [0.296]]
maxi score, test score, baseline:  -0.634 0.6734999999999999 0.6734999999999999
probs:  [0.07958891379350459, 0.07958891379350459, 0.12809364907058057, 0.07958891379350459, 0.5526404902052909, 0.08049911934361487]
printing an ep nov before normalisation:  49.05842259100436
line 256 mcts: sample exp_bonus 0.08180751050531399
actor:  1 policy actor:  1  step number:  36 total reward:  0.20999999999999952  reward:  1.0 rdn_beta:  0.333
using another actor
from probs:  [0.07957130483516292, 0.07957130483516292, 0.1283021324229071, 0.07957130483516292, 0.5524981999979661, 0.08048575307363814]
Printing some Q and Qe and total Qs values:  [[0.204]
 [0.222]
 [0.211]
 [0.204]
 [0.212]
 [0.195]
 [0.194]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.204]
 [0.222]
 [0.211]
 [0.204]
 [0.212]
 [0.195]
 [0.194]]
maxi score, test score, baseline:  -0.634 0.6734999999999999 0.6734999999999999
probs:  [0.07957130483516292, 0.07957130483516292, 0.1283021324229071, 0.07957130483516292, 0.5524981999979661, 0.08048575307363814]
maxi score, test score, baseline:  -0.634 0.6734999999999999 0.6734999999999999
using another actor
printing an ep nov before normalisation:  15.771953270455015
actor:  1 policy actor:  1  step number:  29 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  49.276604652404785
printing an ep nov before normalisation:  43.83776664733887
printing an ep nov before normalisation:  44.94275229915202
maxi score, test score, baseline:  -0.634 0.6734999999999999 0.6734999999999999
probs:  [0.07953611475075378, 0.07953611475075378, 0.1287187696040462, 0.07953611475075378, 0.5522138444836193, 0.08045904166007306]
printing an ep nov before normalisation:  41.144493977531056
Printing some Q and Qe and total Qs values:  [[ 0.183]
 [ 0.229]
 [-0.002]
 [ 0.199]
 [-0.002]
 [-0.002]
 [-0.002]] [[48.538]
 [48.001]
 [49.987]
 [46.03 ]
 [49.987]
 [49.987]
 [49.987]] [[1.409]
 [1.432]
 [1.285]
 [1.319]
 [1.285]
 [1.285]
 [1.285]]
actor:  0 policy actor:  1  step number:  26 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[ 0.188]
 [ 0.244]
 [ 0.176]
 [ 0.118]
 [-0.013]
 [ 0.176]
 [ 0.176]] [[42.028]
 [42.318]
 [39.81 ]
 [40.876]
 [39.163]
 [39.81 ]
 [39.81 ]] [[1.241]
 [1.31 ]
 [1.13 ]
 [1.119]
 [0.911]
 [1.13 ]
 [1.13 ]]
maxi score, test score, baseline:  -0.6311000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07953611475075378, 0.07953611475075378, 0.1287187696040462, 0.07953611475075378, 0.5522138444836193, 0.08045904166007306]
maxi score, test score, baseline:  -0.6311000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07953611475075378, 0.07953611475075378, 0.1287187696040462, 0.07953611475075378, 0.5522138444836193, 0.08045904166007306]
printing an ep nov before normalisation:  34.48026063571416
maxi score, test score, baseline:  -0.6311000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07953611475075378, 0.07953611475075378, 0.1287187696040462, 0.07953611475075378, 0.5522138444836193, 0.08045904166007306]
actor:  0 policy actor:  1  step number:  30 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  34 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.126]
 [-0.125]
 [-0.126]
 [-0.126]
 [-0.126]
 [-0.126]
 [-0.126]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.126]
 [-0.125]
 [-0.126]
 [-0.126]
 [-0.126]
 [-0.126]
 [-0.126]]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.62812 0.6734999999999999 0.6734999999999999
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  80.89605812059676
actor:  1 policy actor:  1  step number:  26 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
from probs:  [0.07944830155296179, 0.07944830155296179, 0.12975844438201778, 0.07944830155296179, 0.5515042648550442, 0.08039238610405268]
actor:  1 policy actor:  1  step number:  20 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.034]
 [-0.034]
 [-0.034]
 [-0.034]
 [-0.034]
 [-0.034]
 [-0.034]] [[66.51]
 [66.51]
 [66.51]
 [66.51]
 [66.51]
 [66.51]
 [66.51]] [[1.064]
 [1.064]
 [1.064]
 [1.064]
 [1.064]
 [1.064]
 [1.064]]
maxi score, test score, baseline:  -0.6251600000000002 0.6734999999999999 0.6734999999999999
printing an ep nov before normalisation:  49.59766519940021
actor:  1 policy actor:  1  step number:  31 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6251600000000002 0.6734999999999999 0.6734999999999999
probs:  [0.07939591706505349, 0.07939591706505349, 0.13038125392158498, 0.07939591706505349, 0.5510807513652058, 0.08035024351804866]
actor:  1 policy actor:  1  step number:  30 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6251600000000002 0.6734999999999999 0.6734999999999999
probs:  [0.07937841058972674, 0.07937841058972674, 0.13058853536472018, 0.07937841058972674, 0.5509392883184338, 0.08033694454766598]
maxi score, test score, baseline:  -0.6251600000000002 0.6734999999999999 0.6734999999999999
probs:  [0.07937841058972674, 0.07937841058972674, 0.13058853536472018, 0.07937841058972674, 0.5509392883184338, 0.08033694454766598]
maxi score, test score, baseline:  -0.6251600000000002 0.6734999999999999 0.6734999999999999
probs:  [0.07937841058972674, 0.07937841058972674, 0.13058853536472018, 0.07937841058972674, 0.5509392883184338, 0.08033694454766598]
actor:  1 policy actor:  1  step number:  25 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6251600000000002 0.6734999999999999 0.6734999999999999
probs:  [0.07937860304580695, 0.07937860304580695, 0.1305888521905023, 0.07937860304580695, 0.5509406260145615, 0.08033471265751525]
maxi score, test score, baseline:  -0.6251600000000002 0.6734999999999999 0.6734999999999999
maxi score, test score, baseline:  -0.6251600000000002 0.6734999999999999 0.6734999999999999
probs:  [0.07937860304580695, 0.07937860304580695, 0.1305888521905023, 0.07937860304580695, 0.5509406260145615, 0.08033471265751525]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  39.1913386161605
maxi score, test score, baseline:  -0.6251600000000002 0.6734999999999999 0.6734999999999999
probs:  [0.07934361928691394, 0.07934361928691394, 0.13100309220698073, 0.07934361928691394, 0.5506579339006766, 0.0803081160316007]
Printing some Q and Qe and total Qs values:  [[-0.043]
 [ 0.051]
 [-0.047]
 [-0.032]
 [-0.045]
 [-0.047]
 [-0.038]] [[77.171]
 [57.75 ]
 [79.252]
 [78.515]
 [80.926]
 [76.549]
 [74.344]] [[0.559]
 [0.374]
 [0.585]
 [0.59 ]
 [0.611]
 [0.547]
 [0.524]]
Printing some Q and Qe and total Qs values:  [[-0.02]
 [-0.02]
 [-0.02]
 [-0.02]
 [-0.02]
 [-0.02]
 [-0.02]] [[58.825]
 [58.825]
 [58.825]
 [58.825]
 [58.825]
 [58.825]
 [58.825]] [[39.216]
 [39.216]
 [39.216]
 [39.216]
 [39.216]
 [39.216]
 [39.216]]
maxi score, test score, baseline:  -0.6251600000000002 0.6734999999999999 0.6734999999999999
probs:  [0.07934361928691394, 0.07934361928691394, 0.13100309220698073, 0.07934361928691394, 0.5506579339006766, 0.0803081160316007]
maxi score, test score, baseline:  -0.6251600000000002 0.6734999999999999 0.6734999999999999
probs:  [0.07934361928691394, 0.07934361928691394, 0.13100309220698073, 0.07934361928691394, 0.5506579339006766, 0.0803081160316007]
maxi score, test score, baseline:  -0.6251600000000002 0.6734999999999999 0.6734999999999999
maxi score, test score, baseline:  -0.6251600000000002 0.6734999999999999 0.6734999999999999
probs:  [0.07934361928691394, 0.07934361928691394, 0.13100309220698073, 0.07934361928691394, 0.5506579339006766, 0.0803081160316007]
actor:  1 policy actor:  1  step number:  32 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  28 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  25 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.6251600000000002 0.6734999999999999 0.6734999999999999
probs:  [0.07934361928691394, 0.07934361928691394, 0.13100309220698073, 0.07934361928691394, 0.5506579339006766, 0.0803081160316007]
maxi score, test score, baseline:  -0.6251600000000002 0.6734999999999999 0.6734999999999999
probs:  [0.07934361928691394, 0.07934361928691394, 0.13100309220698073, 0.07934361928691394, 0.5506579339006766, 0.0803081160316007]
maxi score, test score, baseline:  -0.6251600000000002 0.6734999999999999 0.6734999999999999
probs:  [0.07934361928691394, 0.07934361928691394, 0.13100309220698073, 0.07934361928691394, 0.5506579339006766, 0.0803081160316007]
Printing some Q and Qe and total Qs values:  [[-0.113]
 [-0.131]
 [-0.103]
 [-0.113]
 [-0.113]
 [-0.113]
 [-0.113]] [[51.298]
 [57.971]
 [60.637]
 [51.298]
 [51.298]
 [51.298]
 [51.298]] [[0.833]
 [0.941]
 [1.019]
 [0.833]
 [0.833]
 [0.833]
 [0.833]]
printing an ep nov before normalisation:  55.69706887320408
maxi score, test score, baseline:  -0.6251600000000002 0.6734999999999999 0.6734999999999999
probs:  [0.07934361928691394, 0.07934361928691394, 0.13100309220698073, 0.07934361928691394, 0.5506579339006766, 0.0803081160316007]
actor:  0 policy actor:  0  step number:  31 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6222800000000002 0.6734999999999999 0.6734999999999999
probs:  [0.07932614118639816, 0.07932614118639816, 0.13121004905995398, 0.07932614118639816, 0.5505166991866879, 0.08029482819416357]
printing an ep nov before normalisation:  33.325300380488976
maxi score, test score, baseline:  -0.6222800000000002 0.6734999999999999 0.6734999999999999
probs:  [0.07932614118639816, 0.07932614118639816, 0.13121004905995398, 0.07932614118639816, 0.5505166991866879, 0.08029482819416357]
actions average: 
K:  1  action  0 :  tensor([0.4051, 0.0170, 0.1118, 0.1283, 0.1270, 0.1036, 0.1072],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0015,     0.9842,     0.0015,     0.0020,     0.0004,     0.0005,
            0.0100], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1103, 0.0220, 0.4948, 0.0927, 0.0776, 0.0942, 0.1084],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0916, 0.0104, 0.0826, 0.5565, 0.1042, 0.0818, 0.0729],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1974, 0.0035, 0.1224, 0.1686, 0.2603, 0.1246, 0.1233],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0975, 0.0181, 0.2634, 0.0990, 0.0783, 0.3757, 0.0680],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1490, 0.1530, 0.1123, 0.1297, 0.1154, 0.0922, 0.2484],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[-0.029]
 [ 0.383]
 [-0.029]
 [-0.041]
 [-0.044]
 [-0.029]
 [-0.029]] [[51.507]
 [63.253]
 [51.507]
 [61.593]
 [63.574]
 [51.507]
 [51.507]] [[0.368]
 [0.927]
 [0.368]
 [0.483]
 [0.504]
 [0.368]
 [0.368]]
actor:  1 policy actor:  1  step number:  31 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6222800000000002 0.6734999999999999 0.6734999999999999
actor:  1 policy actor:  1  step number:  26 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6222800000000002 0.6734999999999999 0.6734999999999999
probs:  [0.07929121250706665, 0.07929121250706665, 0.13162363688355275, 0.07929121250706665, 0.5502344521524103, 0.0802682734428371]
actions average: 
K:  3  action  0 :  tensor([0.5954, 0.0230, 0.0878, 0.0705, 0.0830, 0.0486, 0.0918],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0137, 0.8780, 0.0188, 0.0316, 0.0033, 0.0052, 0.0495],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0897, 0.0131, 0.4738, 0.1459, 0.0856, 0.1082, 0.0838],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1211, 0.1185, 0.1025, 0.3181, 0.1095, 0.1043, 0.1261],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2319, 0.0038, 0.1557, 0.1634, 0.1847, 0.1337, 0.1267],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1376, 0.1736, 0.1471, 0.1292, 0.1017, 0.1347, 0.1761],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1961, 0.0094, 0.1295, 0.2444, 0.1055, 0.1064, 0.2088],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.6222800000000002 0.6734999999999999 0.6734999999999999
probs:  [0.07929140797232047, 0.07929140797232047, 0.13162396157355263, 0.07929140797232047, 0.550235810520421, 0.08026600398906486]
actor:  0 policy actor:  0  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  46 total reward:  0.3299999999999996  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  52.89900914023022
actions average: 
K:  3  action  0 :  tensor([0.4824, 0.0008, 0.0848, 0.0849, 0.1858, 0.0827, 0.0786],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0159, 0.9366, 0.0198, 0.0051, 0.0047, 0.0053, 0.0126],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1082, 0.0922, 0.4422, 0.0997, 0.0907, 0.0804, 0.0866],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1124, 0.0503, 0.1104, 0.4022, 0.1160, 0.1062, 0.1026],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1667, 0.0122, 0.1636, 0.1936, 0.1720, 0.1645, 0.1275],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1501, 0.0257, 0.1691, 0.1347, 0.0997, 0.2996, 0.1210],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1427, 0.1457, 0.1806, 0.1205, 0.1322, 0.1105, 0.1677],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.61914 0.6734999999999999 0.6734999999999999
printing an ep nov before normalisation:  56.67647849335946
actor:  1 policy actor:  1  step number:  18 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.61914 0.6734999999999999 0.6734999999999999
probs:  [0.07927395817275058, 0.07927395817275058, 0.13183059461789307, 0.07927395817275058, 0.5500948035501371, 0.08025272731371824]
actor:  0 policy actor:  1  step number:  36 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  76.78854977626327
printing an ep nov before normalisation:  58.17778495316955
printing an ep nov before normalisation:  45.31709671020508
maxi score, test score, baseline:  -0.6162400000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07927395817275058, 0.07927395817275058, 0.13183059461789307, 0.07927395817275058, 0.5500948035501371, 0.08025272731371824]
actor:  1 policy actor:  1  step number:  47 total reward:  0.01999999999999935  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6162400000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07927395817275058, 0.07927395817275058, 0.13183059461789307, 0.07927395817275058, 0.5500948035501371, 0.08025272731371824]
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.10879325866699
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  31 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.061]
 [-0.036]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]] [[51.476]
 [73.553]
 [51.476]
 [51.476]
 [51.476]
 [51.476]
 [51.476]] [[0.795]
 [1.519]
 [0.795]
 [0.795]
 [0.795]
 [0.795]
 [0.795]]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6162400000000001 0.6734999999999999 0.6734999999999999
maxi score, test score, baseline:  -0.6162400000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07923908602802515, 0.07923908602802515, 0.13224353560317026, 0.07923908602802515, 0.5498130114610454, 0.0802261948517088]
maxi score, test score, baseline:  -0.6162400000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07923928287627463, 0.07923928287627463, 0.13224386434851568, 0.07923928287627463, 0.5498143792927632, 0.0802239077298972]
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.280615943063665
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  33 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  37.46877964471964
Sims:  50 1 epoch:  68080 pick best:  False frame count:  68080
printing an ep nov before normalisation:  75.65073794637212
actor:  1 policy actor:  1  step number:  41 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.028]
 [0.085]
 [0.028]
 [0.015]
 [0.027]
 [0.029]
 [0.031]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.028]
 [0.085]
 [0.028]
 [0.015]
 [0.027]
 [0.029]
 [0.031]]
maxi score, test score, baseline:  -0.6162400000000001 0.6734999999999999 0.6734999999999999
actor:  0 policy actor:  1  step number:  18 total reward:  0.71  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  35.52813529968262
actor:  1 policy actor:  1  step number:  27 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  63.14158867201337
printing an ep nov before normalisation:  80.3422380920004
line 256 mcts: sample exp_bonus 69.30128805326551
actor:  1 policy actor:  1  step number:  24 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.61282 0.6734999999999999 0.6734999999999999
probs:  [0.07918704554705547, 0.07918704554705547, 0.13286247006518243, 0.07918704554705547, 0.5493922611638639, 0.08018413212978732]
Printing some Q and Qe and total Qs values:  [[-0.119]
 [-0.062]
 [-0.119]
 [-0.119]
 [-0.119]
 [-0.119]
 [-0.119]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.119]
 [-0.062]
 [-0.119]
 [-0.119]
 [-0.119]
 [-0.119]
 [-0.119]]
Printing some Q and Qe and total Qs values:  [[-0.158]
 [-0.08 ]
 [-0.149]
 [-0.158]
 [-0.178]
 [-0.157]
 [-0.149]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.158]
 [-0.08 ]
 [-0.149]
 [-0.158]
 [-0.178]
 [-0.157]
 [-0.149]]
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.623]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]] [[53.588]
 [63.606]
 [53.588]
 [53.588]
 [53.588]
 [53.588]
 [53.588]] [[1.33 ]
 [1.847]
 [1.33 ]
 [1.33 ]
 [1.33 ]
 [1.33 ]
 [1.33 ]]
printing an ep nov before normalisation:  30.54401159286499
actor:  1 policy actor:  1  step number:  36 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.61282 0.6734999999999999 0.6734999999999999
actor:  1 policy actor:  1  step number:  35 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.61282 0.6734999999999999 0.6734999999999999
probs:  [0.0791348902723694, 0.0791348902723694, 0.13348010407437502, 0.0791348902723694, 0.548970806099223, 0.0801444190092938]
maxi score, test score, baseline:  -0.61282 0.6734999999999999 0.6734999999999999
probs:  [0.0791348902723694, 0.0791348902723694, 0.13348010407437502, 0.0791348902723694, 0.548970806099223, 0.0801444190092938]
Printing some Q and Qe and total Qs values:  [[0.618]
 [0.513]
 [0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]] [[61.756]
 [70.925]
 [61.142]
 [61.142]
 [61.142]
 [61.142]
 [61.142]] [[0.88 ]
 [0.832]
 [0.725]
 [0.725]
 [0.725]
 [0.725]
 [0.725]]
printing an ep nov before normalisation:  73.03405516631129
printing an ep nov before normalisation:  59.34644752674192
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  24 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  32 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.61282 0.6734999999999999 0.6734999999999999
probs:  [0.07911752338174047, 0.07911752338174047, 0.13368576653806455, 0.07911752338174047, 0.5488304681553299, 0.0801311951613841]
printing an ep nov before normalisation:  46.68783854986594
maxi score, test score, baseline:  -0.61282 0.6734999999999999 0.6734999999999999
probs:  [0.07911752338174047, 0.07911752338174047, 0.13368576653806455, 0.07911752338174047, 0.5488304681553299, 0.0801311951613841]
maxi score, test score, baseline:  -0.61282 0.6734999999999999 0.6734999999999999
printing an ep nov before normalisation:  34.45367683853606
actor:  0 policy actor:  1  step number:  40 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  31 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.61 0.6734999999999999 0.6734999999999999
probs:  [0.07910036768533592, 0.07910036768533592, 0.13389166370960268, 0.07910036768533592, 0.5486916076146683, 0.08011562561972134]
printing an ep nov before normalisation:  60.73289277696159
maxi score, test score, baseline:  -0.61 0.6734999999999999 0.6734999999999999
probs:  [0.07910036768533592, 0.07910036768533592, 0.13389166370960268, 0.07910036768533592, 0.5486916076146683, 0.08011562561972134]
printing an ep nov before normalisation:  56.64458785564675
maxi score, test score, baseline:  -0.61 0.6734999999999999 0.6734999999999999
probs:  [0.07910036768533592, 0.07910036768533592, 0.13389166370960268, 0.07910036768533592, 0.5486916076146683, 0.08011562561972134]
printing an ep nov before normalisation:  0.0008951232371146034
actor:  1 policy actor:  1  step number:  23 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.61 0.6734999999999999 0.6734999999999999
probs:  [0.07910036768533592, 0.07910036768533592, 0.13389166370960268, 0.07910036768533592, 0.5486916076146683, 0.08011562561972134]
actions average: 
K:  3  action  0 :  tensor([0.5474, 0.0334, 0.0643, 0.0736, 0.1108, 0.0580, 0.1124],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0148, 0.9097, 0.0151, 0.0180, 0.0094, 0.0103, 0.0226],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0632, 0.0073, 0.6449, 0.0913, 0.0513, 0.0852, 0.0570],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1572, 0.0743, 0.1468, 0.2191, 0.1369, 0.1161, 0.1496],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.3063, 0.0059, 0.1300, 0.1253, 0.1985, 0.0932, 0.1408],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1317, 0.0087, 0.1568, 0.2794, 0.1184, 0.1577, 0.1472],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1119, 0.1816, 0.1319, 0.1124, 0.0733, 0.0808, 0.3082],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.61 0.6734999999999999 0.6734999999999999
probs:  [0.07910036768533592, 0.07910036768533592, 0.13389166370960268, 0.07910036768533592, 0.5486916076146683, 0.08011562561972134]
actor:  1 policy actor:  1  step number:  28 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  36 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.61 0.6734999999999999 0.6734999999999999
probs:  [0.07904835106861413, 0.07904835106861413, 0.13450768905644714, 0.07904835106861413, 0.5482712702192674, 0.08007598751844305]
maxi score, test score, baseline:  -0.61 0.6734999999999999 0.6734999999999999
probs:  [0.07904835106861413, 0.07904835106861413, 0.13450768905644714, 0.07904835106861413, 0.5482712702192674, 0.08007598751844305]
printing an ep nov before normalisation:  58.17924732035385
maxi score, test score, baseline:  -0.61 0.6734999999999999 0.6734999999999999
actor:  1 policy actor:  1  step number:  21 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.61 0.6734999999999999 0.6734999999999999
probs:  [0.07904835106861413, 0.07904835106861413, 0.13450768905644714, 0.07904835106861413, 0.5482712702192674, 0.08007598751844305]
Printing some Q and Qe and total Qs values:  [[0.169]
 [0.567]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]] [[47.594]
 [59.189]
 [47.594]
 [47.594]
 [47.594]
 [47.594]
 [47.594]] [[0.93]
 [1.68]
 [0.93]
 [0.93]
 [0.93]
 [0.93]
 [0.93]]
Printing some Q and Qe and total Qs values:  [[-0.004]
 [ 0.019]
 [-0.1  ]
 [-0.1  ]
 [-0.1  ]
 [-0.1  ]
 [-0.1  ]] [[52.888]
 [52.666]
 [55.266]
 [55.266]
 [55.266]
 [55.266]
 [55.266]] [[1.562]
 [1.578]
 [1.544]
 [1.544]
 [1.544]
 [1.544]
 [1.544]]
actor:  1 policy actor:  1  step number:  25 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.61 0.6734999999999999 0.6734999999999999
probs:  [0.07899641598571942, 0.07899641598571942, 0.13512274880984732, 0.07899641598571942, 0.5478515916847971, 0.08003641154819732]
siam score:  -0.79849577
maxi score, test score, baseline:  -0.61 0.6734999999999999 0.6734999999999999
probs:  [0.07899641598571942, 0.07899641598571942, 0.13512274880984732, 0.07899641598571942, 0.5478515916847971, 0.08003641154819732]
printing an ep nov before normalisation:  55.4440200605559
siam score:  -0.79821444
maxi score, test score, baseline:  -0.61 0.6734999999999999 0.6734999999999999
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]] [[36.957]
 [36.957]
 [36.957]
 [36.957]
 [36.957]
 [36.957]
 [36.957]] [[1.352]
 [1.352]
 [1.352]
 [1.352]
 [1.352]
 [1.352]
 [1.352]]
Printing some Q and Qe and total Qs values:  [[0.52 ]
 [0.644]
 [0.532]
 [0.532]
 [0.446]
 [0.532]
 [0.618]] [[56.363]
 [59.149]
 [64.253]
 [64.253]
 [52.681]
 [64.253]
 [60.204]] [[1.286]
 [1.472]
 [1.471]
 [1.471]
 [1.131]
 [1.471]
 [1.468]]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.61 0.6734999999999999 0.6734999999999999
maxi score, test score, baseline:  -0.61 0.6734999999999999 0.6734999999999999
probs:  [0.07899641598571942, 0.07899641598571942, 0.13512274880984732, 0.07899641598571942, 0.5478515916847971, 0.08003641154819732]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.333
from probs:  [0.07899641598571942, 0.07899641598571942, 0.13512274880984732, 0.07899641598571942, 0.5478515916847971, 0.08003641154819732]
maxi score, test score, baseline:  -0.61 0.6734999999999999 0.6734999999999999
probs:  [0.07899641598571942, 0.07899641598571942, 0.13512274880984732, 0.07899641598571942, 0.5478515916847971, 0.08003641154819732]
printing an ep nov before normalisation:  67.57776463139584
using another actor
maxi score, test score, baseline:  -0.6100000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07899641598571942, 0.07899641598571942, 0.13512274880984732, 0.07899641598571942, 0.5478515916847971, 0.08003641154819732]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.171]
 [-0.098]
 [-0.135]
 [-0.118]
 [-0.153]
 [-0.259]
 [-0.129]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.171]
 [-0.098]
 [-0.135]
 [-0.118]
 [-0.153]
 [-0.259]
 [-0.129]]
siam score:  -0.7999157
maxi score, test score, baseline:  -0.6100000000000001 0.6734999999999999 0.6734999999999999
line 256 mcts: sample exp_bonus 53.970552512577605
printing an ep nov before normalisation:  66.80515039075357
printing an ep nov before normalisation:  54.138913553486866
maxi score, test score, baseline:  -0.6100000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07894456224510071, 0.07894456224510071, 0.13573684523831514, 0.07894456224510071, 0.5474325704633662, 0.07999689756301656]
maxi score, test score, baseline:  -0.6100000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07894456224510071, 0.07894456224510071, 0.13573684523831514, 0.07894456224510071, 0.5474325704633662, 0.07999689756301656]
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]] [[61.102]
 [72.378]
 [72.378]
 [72.378]
 [72.378]
 [72.378]
 [72.378]] [[0.896]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]]
siam score:  -0.7971196
maxi score, test score, baseline:  -0.6100000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07892729570793458, 0.07892729570793458, 0.13594133036666264, 0.07892729570793458, 0.5472930425256147, 0.0799837399839189]
maxi score, test score, baseline:  -0.6100000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07892729570793458, 0.07892729570793458, 0.13594133036666264, 0.07892729570793458, 0.5472930425256147, 0.0799837399839189]
printing an ep nov before normalisation:  44.72290537955635
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6100000000000001 0.6734999999999999 0.6734999999999999
maxi score, test score, baseline:  -0.6100000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07892729570793458, 0.07892729570793458, 0.13594133036666264, 0.07892729570793458, 0.5472930425256147, 0.0799837399839189]
maxi score, test score, baseline:  -0.6100000000000001 0.6734999999999999 0.6734999999999999
maxi score, test score, baseline:  -0.6100000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07892729570793458, 0.07892729570793458, 0.13594133036666264, 0.07892729570793458, 0.5472930425256147, 0.0799837399839189]
printing an ep nov before normalisation:  51.87479349216197
Printing some Q and Qe and total Qs values:  [[-0.034]
 [ 0.33 ]
 [-0.029]
 [-0.041]
 [-0.036]
 [-0.026]
 [-0.031]] [[34.869]
 [51.813]
 [35.255]
 [36.495]
 [36.644]
 [35.184]
 [34.807]] [[0.393]
 [1.256]
 [0.409]
 [0.434]
 [0.443]
 [0.41 ]
 [0.394]]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6100000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07889278965580643, 0.07889278965580643, 0.1363499806032621, 0.07889278965580643, 0.5470142050119277, 0.07995744541739076]
Printing some Q and Qe and total Qs values:  [[-0.118]
 [-0.087]
 [-0.118]
 [-0.118]
 [-0.118]
 [-0.118]
 [-0.116]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.118]
 [-0.087]
 [-0.118]
 [-0.118]
 [-0.118]
 [-0.118]
 [-0.116]]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6100000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07887555012675146, 0.07887555012675146, 0.13655414587841525, 0.07887555012675146, 0.5468748953221093, 0.07994430841922108]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6100000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07885831958631949, 0.07885831958631949, 0.1367582047025925, 0.07885831958631949, 0.5467356582678183, 0.07993117827063077]
using explorer policy with actor:  1
printing an ep nov before normalisation:  62.61700219059358
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]] [[69.143]
 [69.143]
 [69.143]
 [69.143]
 [69.143]
 [69.143]
 [69.143]] [[0.803]
 [0.803]
 [0.803]
 [0.803]
 [0.803]
 [0.803]
 [0.803]]
printing an ep nov before normalisation:  0.004630759890460467
maxi score, test score, baseline:  -0.6100000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07885831958631949, 0.07885831958631949, 0.1367582047025925, 0.07885831958631949, 0.5467356582678183, 0.07993117827063077]
maxi score, test score, baseline:  -0.6100000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07885831958631949, 0.07885831958631949, 0.1367582047025925, 0.07885831958631949, 0.5467356582678183, 0.07993117827063077]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  77.67711368951767
actor:  1 policy actor:  1  step number:  39 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6100000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07885831958631949, 0.07885831958631949, 0.1367582047025925, 0.07885831958631949, 0.5467356582678183, 0.07993117827063077]
actor:  1 policy actor:  1  step number:  38 total reward:  0.4099999999999998  reward:  1.0 rdn_beta:  1.0
line 256 mcts: sample exp_bonus 51.305125749600634
maxi score, test score, baseline:  -0.6100000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07885831958631949, 0.07885831958631949, 0.1367582047025925, 0.07885831958631949, 0.5467356582678183, 0.07993117827063077]
maxi score, test score, baseline:  -0.6100000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07885831958631949, 0.07885831958631949, 0.1367582047025925, 0.07885831958631949, 0.5467356582678183, 0.07993117827063077]
actor:  1 policy actor:  1  step number:  32 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.05 ]
 [ 0.465]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]] [[48.728]
 [59.793]
 [48.728]
 [48.728]
 [48.728]
 [48.728]
 [48.728]] [[0.319]
 [0.973]
 [0.319]
 [0.319]
 [0.319]
 [0.319]
 [0.319]]
actor:  1 policy actor:  1  step number:  30 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  22 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6100000000000001 0.6734999999999999 0.6734999999999999
maxi score, test score, baseline:  -0.6100000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07882388544321942, 0.07882388544321942, 0.1371660033308649, 0.07882388544321942, 0.5464574018387053, 0.07990493850077156]
maxi score, test score, baseline:  -0.6100000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07882409881931733, 0.07882409881931733, 0.13716637490634323, 0.07882409881931733, 0.5464588832409786, 0.07990244539472624]
printing an ep nov before normalisation:  77.30389406817987
actor:  1 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6100000000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07882409881931733, 0.07882409881931733, 0.13716637490634323, 0.07882409881931733, 0.5464588832409786, 0.07990244539472624]
actor:  0 policy actor:  0  step number:  28 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  19 total reward:  0.7  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6036600000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07882409881931733, 0.07882409881931733, 0.13716637490634323, 0.07882409881931733, 0.5464588832409786, 0.07990244539472624]
actor:  1 policy actor:  1  step number:  26 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6036600000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07882431112968322, 0.07882431112968322, 0.1371667446259443, 0.07882431112968322, 0.5464603572442139, 0.07989996474079197]
Printing some Q and Qe and total Qs values:  [[0.848]
 [0.852]
 [0.824]
 [0.804]
 [0.78 ]
 [0.799]
 [0.839]] [[51.437]
 [48.862]
 [46.064]
 [48.207]
 [52.104]
 [46.005]
 [50.87 ]] [[0.848]
 [0.852]
 [0.824]
 [0.804]
 [0.78 ]
 [0.799]
 [0.839]]
Printing some Q and Qe and total Qs values:  [[ 0.502]
 [ 0.525]
 [-0.06 ]
 [ 0.251]
 [ 0.252]
 [ 0.283]
 [ 0.283]] [[56.068]
 [49.83 ]
 [45.389]
 [46.201]
 [45.211]
 [52.17 ]
 [52.17 ]] [[1.57 ]
 [1.401]
 [0.678]
 [1.014]
 [0.985]
 [1.231]
 [1.231]]
using another actor
printing an ep nov before normalisation:  68.02404151711106
maxi score, test score, baseline:  -0.6036600000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07882431112968322, 0.07882431112968322, 0.1371667446259443, 0.07882431112968322, 0.5464603572442139, 0.07989996474079197]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6036600000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07880710903166126, 0.07880710903166126, 0.13737048850989983, 0.07880710903166126, 0.5463213481933749, 0.07988683620174147]
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.503]
 [0.79 ]
 [0.503]
 [0.235]
 [0.503]
 [0.503]] [[41.127]
 [41.127]
 [57.139]
 [41.127]
 [52.438]
 [41.127]
 [41.127]] [[0.503]
 [0.503]
 [0.79 ]
 [0.503]
 [0.235]
 [0.503]
 [0.503]]
actor:  1 policy actor:  1  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  1  action  0 :  tensor([0.5028, 0.0029, 0.0889, 0.0987, 0.1243, 0.0769, 0.1054],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0044, 0.9400, 0.0052, 0.0162, 0.0020, 0.0026, 0.0297],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0976, 0.0052, 0.5296, 0.0766, 0.0797, 0.1277, 0.0836],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1309, 0.0041, 0.1077, 0.4248, 0.1154, 0.1178, 0.0992],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2042, 0.0007, 0.1066, 0.1281, 0.4076, 0.0778, 0.0749],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1696, 0.0072, 0.1635, 0.1642, 0.1415, 0.2189, 0.1351],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.3288, 0.0129, 0.1223, 0.1207, 0.1464, 0.1148, 0.1541],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  72.61694535073251
maxi score, test score, baseline:  -0.6036600000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07878991589269853, 0.07878991589269853, 0.1375741262815944, 0.07878991589269853, 0.5461824115401186, 0.07987371450019137]
printing an ep nov before normalisation:  14.520901445990292
actor:  0 policy actor:  1  step number:  34 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.60068 0.6734999999999999 0.6734999999999999
probs:  [0.07878991589269853, 0.07878991589269853, 0.1375741262815944, 0.07878991589269853, 0.5461824115401186, 0.07987371450019137]
printing an ep nov before normalisation:  60.597113929846856
maxi score, test score, baseline:  -0.60068 0.6734999999999999 0.6734999999999999
probs:  [0.07878991589269853, 0.07878991589269853, 0.1375741262815944, 0.07878991589269853, 0.5461824115401186, 0.07987371450019137]
actor:  0 policy actor:  1  step number:  46 total reward:  0.3299999999999996  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5980200000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07878991589269853, 0.07878991589269853, 0.1375741262815944, 0.07878991589269853, 0.5461824115401186, 0.07987371450019137]
printing an ep nov before normalisation:  70.45459296770792
printing an ep nov before normalisation:  88.22207681239469
actions average: 
K:  4  action  0 :  tensor([0.4881, 0.0250, 0.1075, 0.0972, 0.1010, 0.0815, 0.0997],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0082, 0.9406, 0.0075, 0.0154, 0.0031, 0.0033, 0.0218],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0229, 0.0880, 0.7336, 0.0697, 0.0244, 0.0263, 0.0350],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1098, 0.1150, 0.0837, 0.4497, 0.0908, 0.0572, 0.0939],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2072, 0.0047, 0.1357, 0.1473, 0.2872, 0.0893, 0.1286],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1521, 0.0251, 0.1631, 0.1336, 0.1141, 0.2953, 0.1168],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1734, 0.1947, 0.1205, 0.1236, 0.0687, 0.0629, 0.2563],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  25 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.5980200000000001 0.6734999999999999 0.6734999999999999
probs:  [0.0787727317057978, 0.0787727317057978, 0.13777765802390338, 0.0787727317057978, 0.5460435472279012, 0.0798605996308021]
maxi score, test score, baseline:  -0.5980200000000001 0.6734999999999999 0.6734999999999999
probs:  [0.0787727317057978, 0.0787727317057978, 0.13777765802390338, 0.0787727317057978, 0.5460435472279012, 0.0798605996308021]
printing an ep nov before normalisation:  83.99790765485923
actor:  1 policy actor:  1  step number:  39 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.59802 0.6734999999999999 0.6734999999999999
probs:  [0.0787555564639692, 0.0787555564639692, 0.13798108381961577, 0.0787555564639692, 0.545904755200238, 0.0798474915882386]
maxi score, test score, baseline:  -0.5980200000000001 0.6734999999999999 0.6734999999999999
probs:  [0.0787555564639692, 0.0787555564639692, 0.13798108381961577, 0.0787555564639692, 0.545904755200238, 0.0798474915882386]
printing an ep nov before normalisation:  57.39377413361551
Printing some Q and Qe and total Qs values:  [[0.212]
 [0.212]
 [0.571]
 [0.212]
 [0.212]
 [0.212]
 [0.212]] [[61.54 ]
 [61.54 ]
 [71.834]
 [61.54 ]
 [61.54 ]
 [61.54 ]
 [61.54 ]] [[1.11 ]
 [1.11 ]
 [1.707]
 [1.11 ]
 [1.11 ]
 [1.11 ]
 [1.11 ]]
printing an ep nov before normalisation:  47.84766267145375
maxi score, test score, baseline:  -0.5980200000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07873839016023011, 0.07873839016023011, 0.13818440375143426, 0.07873839016023011, 0.5457660354007032, 0.07983439036717208]
actor:  1 policy actor:  1  step number:  20 total reward:  0.69  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5980200000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07882311449073473, 0.07882311449073473, 0.13718091777312344, 0.07882311449073473, 0.5464506872816192, 0.07989905147305312]
maxi score, test score, baseline:  -0.5980200000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07882311449073473, 0.07882311449073473, 0.13718091777312344, 0.07882311449073473, 0.5464506872816192, 0.07989905147305312]
maxi score, test score, baseline:  -0.5980200000000001 0.6734999999999999 0.6734999999999999
probs:  [0.07882311449073473, 0.07882311449073473, 0.13718091777312344, 0.07882311449073473, 0.5464506872816192, 0.07989905147305312]
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.549]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]] [[69.618]
 [74.174]
 [69.618]
 [69.618]
 [69.618]
 [69.618]
 [69.618]] [[1.726]
 [1.919]
 [1.726]
 [1.726]
 [1.726]
 [1.726]
 [1.726]]
actor:  0 policy actor:  1  step number:  40 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.132]
 [-0.12 ]
 [-0.132]
 [-0.13 ]
 [-0.137]
 [-0.136]
 [-0.133]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.132]
 [-0.12 ]
 [-0.132]
 [-0.13 ]
 [-0.137]
 [-0.136]
 [-0.133]]
actor:  1 policy actor:  1  step number:  34 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  51.294338513127684
printing an ep nov before normalisation:  47.92819420496623
printing an ep nov before normalisation:  52.37290820222837
printing an ep nov before normalisation:  45.25454317309373
printing an ep nov before normalisation:  56.10112972878438
line 256 mcts: sample exp_bonus 56.904950906022805
siam score:  -0.79963166
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.953]
 [1.026]
 [0.953]
 [0.953]
 [0.953]
 [0.953]
 [0.953]] [[54.544]
 [52.627]
 [54.544]
 [54.544]
 [54.544]
 [54.544]
 [54.544]] [[0.953]
 [1.026]
 [0.953]
 [0.953]
 [0.953]
 [0.953]
 [0.953]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  48.3594224928324
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  0.0005681943935087475
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  18 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  18 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  18 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  18 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  18 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  18 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  39.99581158537748
actor:  0 policy actor:  0  step number:  19 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  20 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  20 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.52886 0.6635 0.6635
probs:  [0.07701824380871741, 0.07701824380871741, 0.15159028440841174, 0.07701824380871741, 0.5347417549907415, 0.08261322917469445]
actor:  1 policy actor:  1  step number:  35 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  1  action  0 :  tensor([0.4267, 0.0028, 0.1166, 0.1232, 0.1380, 0.0872, 0.1055],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0018,     0.9775,     0.0021,     0.0049,     0.0006,     0.0008,
            0.0124], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0899, 0.0059, 0.6405, 0.0666, 0.0528, 0.0844, 0.0599],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1245, 0.0221, 0.1202, 0.3581, 0.0956, 0.0945, 0.1851],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1648, 0.0074, 0.1177, 0.1327, 0.3593, 0.1055, 0.1126],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1037, 0.0324, 0.1584, 0.1305, 0.0921, 0.3726, 0.1102],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1441, 0.2054, 0.1068, 0.1518, 0.0977, 0.0921, 0.2021],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  56.740777409578016
actor:  0 policy actor:  1  step number:  29 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  63.74581557809852
maxi score, test score, baseline:  -0.5260200000000002 0.6635 0.6635
probs:  [0.07701931475567879, 0.07701931475567879, 0.15159239408441735, 0.07701931475567879, 0.534749201661117, 0.08260045998742949]
maxi score, test score, baseline:  -0.5260200000000002 0.6635 0.6635
probs:  [0.07701931475567879, 0.07701931475567879, 0.15159239408441735, 0.07701931475567879, 0.534749201661117, 0.08260045998742949]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5260200000000002 0.6635 0.6635
probs:  [0.07699670326098683, 0.07699670326098683, 0.15183543862262122, 0.07699670326098683, 0.5345767210622822, 0.0825977305321361]
maxi score, test score, baseline:  -0.5260200000000002 0.6635 0.6635
probs:  [0.07699670326098683, 0.07699670326098683, 0.15183543862262122, 0.07699670326098683, 0.5345767210622822, 0.0825977305321361]
actions average: 
K:  0  action  0 :  tensor([0.4166, 0.0043, 0.1176, 0.1116, 0.1572, 0.1005, 0.0922],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0018,     0.9810,     0.0012,     0.0015,     0.0004,     0.0005,
            0.0137], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0735, 0.0040, 0.6052, 0.0681, 0.0760, 0.1085, 0.0646],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0930, 0.0138, 0.0810, 0.5159, 0.0997, 0.0914, 0.1051],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1983, 0.0032, 0.1772, 0.1445, 0.1758, 0.1383, 0.1626],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1310, 0.0081, 0.3068, 0.1290, 0.1193, 0.2091, 0.0967],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0895, 0.1348, 0.1800, 0.1022, 0.0845, 0.0746, 0.3343],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.5260200000000002 0.6635 0.6635
probs:  [0.07699670326098683, 0.07699670326098683, 0.15183543862262122, 0.07699670326098683, 0.5345767210622822, 0.0825977305321361]
printing an ep nov before normalisation:  36.646780952383885
actor:  0 policy actor:  0  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5227800000000001 0.6635 0.6635
probs:  [0.07699670326098683, 0.07699670326098683, 0.15183543862262122, 0.07699670326098683, 0.5345767210622822, 0.0825977305321361]
maxi score, test score, baseline:  -0.5227800000000001 0.6635 0.6635
probs:  [0.07699670326098683, 0.07699670326098683, 0.15183543862262122, 0.07699670326098683, 0.5345767210622822, 0.0825977305321361]
maxi score, test score, baseline:  -0.5227800000000001 0.6635 0.6635
probs:  [0.07697410656827909, 0.07697410656827909, 0.15207832405849886, 0.07697410656827909, 0.5344043533730596, 0.08259500286360444]
printing an ep nov before normalisation:  57.89155739102062
actor:  0 policy actor:  0  step number:  36 total reward:  0.1899999999999995  reward:  1.0 rdn_beta:  1.0
using another actor
printing an ep nov before normalisation:  41.53357982635498
maxi score, test score, baseline:  -0.5204000000000001 0.6635 0.6635
probs:  [0.07695152466302571, 0.07695152466302571, 0.1523210505482267, 0.07695152466302571, 0.5342320984826157, 0.08259227698008055]
printing an ep nov before normalisation:  42.05084846215927
maxi score, test score, baseline:  -0.5204000000000001 0.6635 0.6635
probs:  [0.07695152466302571, 0.07695152466302571, 0.1523210505482267, 0.07695152466302571, 0.5342320984826157, 0.08259227698008055]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  77.9549153697701
Printing some Q and Qe and total Qs values:  [[-0.118]
 [-0.072]
 [-0.118]
 [-0.118]
 [-0.118]
 [-0.118]
 [-0.118]] [[ 0.   ]
 [39.842]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.561]
 [ 0.984]
 [-0.561]
 [-0.561]
 [-0.561]
 [-0.561]
 [-0.561]]
maxi score, test score, baseline:  -0.5204000000000001 0.6635 0.6635
probs:  [0.07695152466302571, 0.07695152466302571, 0.1523210505482267, 0.07695152466302571, 0.5342320984826157, 0.08259227698008055]
printing an ep nov before normalisation:  78.66370764758133
printing an ep nov before normalisation:  73.80065395752253
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  36 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  0  action  0 :  tensor([0.4492, 0.0255, 0.1025, 0.1347, 0.1020, 0.0889, 0.0971],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0034, 0.9634, 0.0027, 0.0066, 0.0016, 0.0016, 0.0207],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0814, 0.0273, 0.5791, 0.0884, 0.0682, 0.0838, 0.0717],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1275, 0.0338, 0.1143, 0.3043, 0.1251, 0.1322, 0.1626],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1984, 0.0024, 0.0655, 0.1015, 0.4749, 0.0916, 0.0657],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1332, 0.0024, 0.1184, 0.1786, 0.1168, 0.3372, 0.1133],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1569, 0.1384, 0.0932, 0.1115, 0.0873, 0.0878, 0.3248],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  32 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5204000000000001 0.6635 0.6635
probs:  [0.0769300370926017, 0.0769300370926017, 0.15256576104971706, 0.0769300370926017, 0.5340674619962851, 0.08257666567619269]
maxi score, test score, baseline:  -0.5204000000000001 0.6635 0.6635
probs:  [0.0769300370926017, 0.0769300370926017, 0.15256576104971706, 0.0769300370926017, 0.5340674619962851, 0.08257666567619269]
actor:  0 policy actor:  0  step number:  41 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.5177200000000001 0.6635 0.6635
probs:  [0.0769300370926017, 0.0769300370926017, 0.15256576104971706, 0.0769300370926017, 0.5340674619962851, 0.08257666567619269]
Printing some Q and Qe and total Qs values:  [[-0.101]
 [-0.05 ]
 [-0.088]
 [-0.095]
 [-0.114]
 [-0.088]
 [-0.088]] [[64.393]
 [58.621]
 [53.243]
 [57.373]
 [61.874]
 [53.243]
 [60.869]] [[0.193]
 [0.203]
 [0.127]
 [0.149]
 [0.162]
 [0.127]
 [0.181]]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5177200000000001 0.6635 0.6635
probs:  [0.0769074881825487, 0.0769074881825487, 0.15280818104119628, 0.0769074881825487, 0.533895456238893, 0.08257389817226465]
using explorer policy with actor:  1
printing an ep nov before normalisation:  16.83744572238393
actor:  1 policy actor:  1  step number:  48 total reward:  0.2699999999999996  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  27 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  19 total reward:  0.7  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.5177200000000001 0.6635 0.6635
probs:  [0.07481155162646642, 0.07481155162646642, 0.14891947469027655, 0.07481155162646642, 0.5193089364915288, 0.10733693393879536]
printing an ep nov before normalisation:  83.73262986492587
maxi score, test score, baseline:  -0.5177200000000001 0.6635 0.6635
probs:  [0.07478263420003935, 0.07478263420003935, 0.1491410522883337, 0.07478263420003935, 0.5190930883107218, 0.10741795680082655]
Printing some Q and Qe and total Qs values:  [[-0.031]
 [ 0.34 ]
 [-0.031]
 [-0.032]
 [-0.032]
 [-0.03 ]
 [-0.03 ]] [[37.816]
 [50.635]
 [37.298]
 [37.557]
 [37.66 ]
 [37.706]
 [37.949]] [[0.443]
 [1.158]
 [0.429]
 [0.435]
 [0.439]
 [0.442]
 [0.448]]
siam score:  -0.8103772
printing an ep nov before normalisation:  62.721615268152284
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.672]
 [0.574]
 [0.467]
 [0.552]
 [0.437]
 [0.638]] [[78.572]
 [66.036]
 [73.701]
 [78.24 ]
 [78.525]
 [75.485]
 [73.481]] [[1.98 ]
 [1.803]
 [1.89 ]
 [1.893]
 [1.984]
 [1.796]
 [1.949]]
maxi score, test score, baseline:  -0.5177200000000001 0.6635 0.6635
probs:  [0.0747537410854944, 0.0747537410854944, 0.1493624435984281, 0.0747537410854944, 0.5188774216009576, 0.10749891154413105]
actor:  0 policy actor:  0  step number:  24 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.53014659881592
printing an ep nov before normalisation:  43.27210243300724
actor:  1 policy actor:  1  step number:  26 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  32.904404401779175
printing an ep nov before normalisation:  83.62710170407232
actor:  0 policy actor:  0  step number:  22 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  82.5577576436977
maxi score, test score, baseline:  -0.5114400000000001 0.6635 0.6635
probs:  [0.07472487225218474, 0.07472487225218474, 0.14958364885538955, 0.07472487225218474, 0.5186619361334791, 0.10757979825457703]
maxi score, test score, baseline:  -0.5114400000000001 0.6635 0.6635
probs:  [0.07473092870329373, 0.07473092870329373, 0.14959578344725455, 0.07473092870329373, 0.5187040379500554, 0.10750739249280877]
maxi score, test score, baseline:  -0.5114400000000001 0.6635 0.6635
probs:  [0.07473092870329373, 0.07473092870329373, 0.14959578344725455, 0.07473092870329373, 0.5187040379500554, 0.10750739249280877]
printing an ep nov before normalisation:  80.70895851656822
Printing some Q and Qe and total Qs values:  [[0.218]
 [0.218]
 [0.218]
 [0.218]
 [0.218]
 [0.218]
 [0.218]] [[60.651]
 [60.651]
 [60.651]
 [60.651]
 [60.651]
 [60.651]
 [60.651]] [[1.218]
 [1.218]
 [1.218]
 [1.218]
 [1.218]
 [1.218]
 [1.218]]
maxi score, test score, baseline:  -0.5114400000000001 0.6635 0.6635
actor:  1 policy actor:  1  step number:  21 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  30 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  86.97744405563688
maxi score, test score, baseline:  -0.5114400000000001 0.6635 0.6635
probs:  [0.0746732994615513, 0.0746732994615513, 0.15003775383313128, 0.0746732994615513, 0.5182738556074111, 0.1076684921748038]
printing an ep nov before normalisation:  20.74882178401016
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5114400000000001 0.6635 0.6635
probs:  [0.07464452109447779, 0.07464452109447779, 0.15025846098831513, 0.07464452109447779, 0.5180590350581404, 0.1077489406701111]
actor:  1 policy actor:  1  step number:  40 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  37.87597550286187
actor:  1 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  22 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5114400000000001 0.6635 0.6635
actor:  1 policy actor:  1  step number:  29 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
from probs:  [0.07456446449230589, 0.07456446449230589, 0.15093190035581197, 0.07456446449230589, 0.5174582857276793, 0.10791642043959099]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5114400000000001 0.6635 0.6635
probs:  [0.07453580006150126, 0.07453580006150126, 0.15115192737201516, 0.07453580006150126, 0.5172443054005375, 0.10799636704294369]
line 256 mcts: sample exp_bonus 52.16410277751801
actor:  0 policy actor:  1  step number:  28 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5084200000000001 0.6635 0.6635
probs:  [0.07453580006150126, 0.07453580006150126, 0.15115192737201516, 0.07453580006150126, 0.5172443054005375, 0.10799636704294369]
printing an ep nov before normalisation:  85.10375903064549
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5084200000000001 0.6635 0.6635
probs:  [0.07453580006150126, 0.07453580006150126, 0.15115192737201516, 0.07453580006150126, 0.5172443054005375, 0.10799636704294369]
actor:  0 policy actor:  1  step number:  22 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  33 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5053200000000001 0.6635 0.6635
probs:  [0.07447854310245015, 0.07447854310245015, 0.15159142948328136, 0.07447854310245015, 0.5168168814997314, 0.10815605970963679]
printing an ep nov before normalisation:  66.7730757505359
printing an ep nov before normalisation:  4.4090398887419724e-05
printing an ep nov before normalisation:  35.555076599121094
actor:  1 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  36.094897985458374
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.616]
 [0.493]
 [0.493]
 [0.392]
 [0.493]
 [0.493]] [[38.147]
 [43.881]
 [40.397]
 [40.397]
 [47.948]
 [40.397]
 [40.397]] [[1.739]
 [1.917]
 [1.691]
 [1.691]
 [1.814]
 [1.691]
 [1.691]]
using explorer policy with actor:  1
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.041 0.327 0.122 0.082 0.102 0.041 0.286]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5053200000000001 0.6635 0.6635
probs:  [0.0743073452868323, 0.0743073452868323, 0.1529055370198177, 0.0743073452868323, 0.5155388877086963, 0.10863353941098901]
printing an ep nov before normalisation:  55.7392139425898
siam score:  -0.8172409
maxi score, test score, baseline:  -0.5053200000000001 0.6635 0.6635
probs:  [0.07422206724929102, 0.07422206724929102, 0.15356012780083664, 0.07422206724929102, 0.5149022861136578, 0.10887138433763228]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  28 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  27 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5024600000000001 0.6635 0.6635
probs:  [0.07420001876562252, 0.07420001876562252, 0.1537910938790407, 0.07420001876562252, 0.5147344198553963, 0.10887442996869547]
actor:  0 policy actor:  1  step number:  19 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.048]
 [0.048]
 [0.534]
 [0.048]
 [0.048]
 [0.048]
 [0.048]] [[68.463]
 [68.463]
 [69.914]
 [68.463]
 [68.463]
 [68.463]
 [68.463]] [[1.141]
 [1.141]
 [1.666]
 [1.141]
 [1.141]
 [1.141]
 [1.141]]
printing an ep nov before normalisation:  0.00043873086497114855
maxi score, test score, baseline:  -0.4991800000000001 0.6635 0.6635
probs:  [0.07414336649501901, 0.07414336649501901, 0.1542263358897048, 0.07414336649501901, 0.5143114897267024, 0.10903207489853577]
printing an ep nov before normalisation:  44.70608285372247
actor:  1 policy actor:  1  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  51.742948519501084
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  32 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.4991800000000001 0.6635 0.6635
probs:  [0.07426784399404382, 0.07426784399404382, 0.1532700134620094, 0.07426784399404382, 0.5152407602417449, 0.10868569431411414]
printing an ep nov before normalisation:  56.873115883198764
siam score:  -0.8179822
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.37483721063424
actor:  1 policy actor:  1  step number:  23 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  60.75989791581147
maxi score, test score, baseline:  -0.4991800000000001 0.6635 0.6635
probs:  [0.07421214044918038, 0.07421214044918038, 0.15369796670393993, 0.07421214044918038, 0.5148249127011815, 0.10884069924733743]
actor:  1 policy actor:  1  step number:  45 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  3  action  0 :  tensor([0.4283, 0.0301, 0.1036, 0.1266, 0.1253, 0.0822, 0.1038],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0171, 0.9156, 0.0127, 0.0167, 0.0048, 0.0068, 0.0264],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0900, 0.0038, 0.6026, 0.0662, 0.0700, 0.0898, 0.0776],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1591, 0.0419, 0.1281, 0.3511, 0.1087, 0.0930, 0.1180],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1858, 0.0400, 0.1553, 0.1823, 0.1743, 0.1224, 0.1398],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1163, 0.0010, 0.1447, 0.1295, 0.1168, 0.3802, 0.1114],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1155, 0.0488, 0.1035, 0.1521, 0.0878, 0.0991, 0.3932],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[-0.152]
 [-0.086]
 [-0.039]
 [-0.081]
 [-0.1  ]
 [-0.059]
 [-0.168]] [[82.404]
 [77.097]
 [78.315]
 [81.877]
 [78.541]
 [79.592]
 [86.175]] [[1.317]
 [1.248]
 [1.326]
 [1.374]
 [1.27 ]
 [1.338]
 [1.398]]
actor:  1 policy actor:  1  step number:  34 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  22 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  75.64834655891332
maxi score, test score, baseline:  -0.4991800000000001 0.6635 0.6635
probs:  [0.07416285422789481, 0.07416285422789481, 0.15413838244109587, 0.07416285422789481, 0.5144536965654956, 0.10891935830972413]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
from probs:  [0.07416285422789481, 0.07416285422789481, 0.15413838244109587, 0.07416285422789481, 0.5144536965654956, 0.10891935830972413]
actor:  1 policy actor:  1  step number:  19 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4991800000000001 0.6635 0.6635
probs:  [0.0741073656525671, 0.0741073656525671, 0.1545650565105872, 0.0741073656525671, 0.5140394340996028, 0.1090734124321087]
Printing some Q and Qe and total Qs values:  [[-0.133]
 [-0.109]
 [-0.134]
 [-0.133]
 [-0.134]
 [-0.136]
 [-0.129]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.133]
 [-0.109]
 [-0.134]
 [-0.133]
 [-0.134]
 [-0.136]
 [-0.129]]
printing an ep nov before normalisation:  59.27054909885679
Printing some Q and Qe and total Qs values:  [[0.677]
 [0.749]
 [0.611]
 [0.679]
 [0.649]
 [0.693]
 [0.708]] [[30.568]
 [32.059]
 [28.836]
 [30.999]
 [28.659]
 [30.797]
 [32.715]] [[0.677]
 [0.749]
 [0.611]
 [0.679]
 [0.649]
 [0.693]
 [0.708]]
printing an ep nov before normalisation:  43.293426605829524
actor:  0 policy actor:  0  step number:  36 total reward:  0.16999999999999948  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  35.53844106223153
maxi score, test score, baseline:  -0.4968400000000001 0.6635 0.6635
probs:  [0.0741073656525671, 0.0741073656525671, 0.1545650565105872, 0.0741073656525671, 0.5140394340996028, 0.1090734124321087]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  28 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4934800000000001 0.6635 0.6635
probs:  [0.0740796552625155, 0.0740796552625155, 0.15477813289289988, 0.0740796552625155, 0.513832555936921, 0.10915034538263267]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4934800000000001 0.6635 0.6635
probs:  [0.0740519674340731, 0.0740519674340731, 0.15499103578988585, 0.0740519674340731, 0.513625846213018, 0.10922721569487688]
maxi score, test score, baseline:  -0.4934800000000001 0.6635 0.6635
probs:  [0.07402430213969691, 0.07402430213969691, 0.15520376541333453, 0.07402430213969691, 0.5134193047222652, 0.10930402344530953]
maxi score, test score, baseline:  -0.4934800000000001 0.6635 0.6635
probs:  [0.07399665935188864, 0.07399665935188864, 0.15541632197469113, 0.07399665935188864, 0.5132129312593682, 0.10938076871027475]
actor:  0 policy actor:  1  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4901400000000001 0.6635 0.6635
probs:  [0.07399665935188864, 0.07399665935188864, 0.15541632197469113, 0.07399665935188864, 0.5132129312593682, 0.10938076871027475]
actor:  0 policy actor:  1  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4869000000000001 0.6635 0.6635
probs:  [0.07396903904319481, 0.07396903904319481, 0.15562870568505674, 0.07396903904319481, 0.5130067256193667, 0.1094574515659921]
maxi score, test score, baseline:  -0.4869000000000001 0.6635 0.6635
probs:  [0.07396903904319481, 0.07396903904319481, 0.15562870568505674, 0.07396903904319481, 0.5130067256193667, 0.1094574515659921]
maxi score, test score, baseline:  -0.4869000000000001 0.6635 0.6635
probs:  [0.07396903904319481, 0.07396903904319481, 0.15562870568505674, 0.07396903904319481, 0.5130067256193667, 0.1094574515659921]
actor:  1 policy actor:  1  step number:  33 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.4869000000000001 0.6635 0.6635
probs:  [0.07396903904319481, 0.07396903904319481, 0.15562870568505674, 0.07396903904319481, 0.5130067256193667, 0.1094574515659921]
printing an ep nov before normalisation:  0.04780554691365069
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  42 total reward:  0.2899999999999996  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  46.1534318332831
actor:  1 policy actor:  1  step number:  32 total reward:  0.5299999999999999  reward:  1.0 rdn_beta:  1.667
using another actor
from probs:  [0.07394144118620655, 0.07394144118620655, 0.1558409167551893, 0.07394144118620655, 0.5128006875976335, 0.10953407208855752]
maxi score, test score, baseline:  -0.4869000000000001 0.6635 0.6635
probs:  [0.07388631271793379, 0.07388631271793379, 0.1562648218160751, 0.07388631271793379, 0.5123891135921254, 0.10968712643799802]
maxi score, test score, baseline:  -0.4869000000000001 0.6635 0.6635
probs:  [0.07388631271793379, 0.07388631271793379, 0.1562648218160751, 0.07388631271793379, 0.5123891135921254, 0.10968712643799802]
printing an ep nov before normalisation:  34.74737715546923
actor:  0 policy actor:  1  step number:  21 total reward:  0.72  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.4834600000000001 0.6635 0.6635
probs:  [0.07388631271793379, 0.07388631271793379, 0.1562648218160751, 0.07388631271793379, 0.5123891135921254, 0.10968712643799802]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.4834600000000001 0.6635 0.6635
probs:  [0.07388631271793379, 0.07388631271793379, 0.1562648218160751, 0.07388631271793379, 0.5123891135921254, 0.10968712643799802]
printing an ep nov before normalisation:  52.085561837390735
maxi score, test score, baseline:  -0.4834600000000001 0.6635 0.6635
maxi score, test score, baseline:  -0.4834600000000001 0.6635 0.6635
probs:  [0.07385878205205387, 0.07385878205205387, 0.15647651622663472, 0.07385878205205387, 0.5121835772007561, 0.10976356041644761]
maxi score, test score, baseline:  -0.4834600000000001 0.6635 0.6635
probs:  [0.07385878205205387, 0.07385878205205387, 0.15647651622663472, 0.07385878205205387, 0.5121835772007561, 0.10976356041644761]
printing an ep nov before normalisation:  56.2452640465088
printing an ep nov before normalisation:  56.06734481135078
actor:  0 policy actor:  0  step number:  28 total reward:  0.57  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  35 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[ 0.184]
 [ 0.446]
 [-0.026]
 [-0.037]
 [-0.035]
 [ 0.184]
 [ 0.184]] [[38.138]
 [45.297]
 [35.506]
 [36.559]
 [36.013]
 [38.138]
 [38.138]] [[0.31 ]
 [0.622]
 [0.081]
 [0.078]
 [0.076]
 [0.31 ]
 [0.31 ]]
printing an ep nov before normalisation:  62.68007932439251
actor:  1 policy actor:  1  step number:  21 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  20 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  0.667
using another actor
from probs:  [0.07385878205205387, 0.07385878205205387, 0.15647651622663472, 0.07385878205205387, 0.5121835772007561, 0.10976356041644761]
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  22 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4771000000000002 0.6635 0.6635
probs:  [0.07383127372868852, 0.07383127372868852, 0.15668803883657484, 0.07383127372868852, 0.511978207612465, 0.1098399323648946]
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.801]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.676]
 [0.801]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]]
printing an ep nov before normalisation:  58.990555995754434
printing an ep nov before normalisation:  50.9918726067302
siam score:  -0.8164312
maxi score, test score, baseline:  -0.47710000000000014 0.6635 0.6635
printing an ep nov before normalisation:  57.699559171755496
maxi score, test score, baseline:  -0.4771000000000001 0.6635 0.6635
probs:  [0.07383127372868852, 0.07383127372868852, 0.15668803883657484, 0.07383127372868852, 0.511978207612465, 0.1098399323648946]
printing an ep nov before normalisation:  51.49194435504242
printing an ep nov before normalisation:  31.80769920349121
actor:  1 policy actor:  1  step number:  39 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  32.69251108169556
maxi score, test score, baseline:  -0.4771000000000001 0.6635 0.6635
probs:  [0.07383776792151062, 0.07383776792151062, 0.15670183427863366, 0.07383776792151062, 0.5120233108429725, 0.1097615511138621]
actor:  1 policy actor:  1  step number:  31 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.4771000000000001 0.6635 0.6635
probs:  [0.07381029820580634, 0.07381029820580634, 0.15691324374399632, 0.07381029820580634, 0.5118182197432527, 0.10983764189533189]
printing an ep nov before normalisation:  52.953281836243846
printing an ep nov before normalisation:  36.44908673754214
siam score:  -0.81102556
maxi score, test score, baseline:  -0.4771000000000001 0.6635 0.6635
probs:  [0.07378285075113726, 0.07378285075113726, 0.15712448188637912, 0.07378285075113726, 0.5116132948462065, 0.10991367101400253]
maxi score, test score, baseline:  -0.4771000000000001 0.6635 0.6635
probs:  [0.07378285075113726, 0.07378285075113726, 0.15712448188637912, 0.07378285075113726, 0.5116132948462065, 0.10991367101400253]
actor:  1 policy actor:  1  step number:  31 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  32 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.4771000000000001 0.6635 0.6635
probs:  [0.07376193775655493, 0.07376193775655493, 0.15734945419092786, 0.07376193775655493, 0.5114537606303247, 0.10991097190908251]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  29 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.063231231219575
printing an ep nov before normalisation:  47.69459050469446
printing an ep nov before normalisation:  47.03528310779979
Printing some Q and Qe and total Qs values:  [[-0.118]
 [-0.143]
 [-0.165]
 [-0.162]
 [-0.269]
 [-0.166]
 [-0.163]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.118]
 [-0.143]
 [-0.165]
 [-0.162]
 [-0.269]
 [-0.166]
 [-0.163]]
actor:  0 policy actor:  1  step number:  28 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  22 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  53.348966114937824
printing an ep nov before normalisation:  63.18605687994766
actions average: 
K:  2  action  0 :  tensor([0.5836, 0.0127, 0.1020, 0.0663, 0.1190, 0.0527, 0.0636],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0099, 0.9218, 0.0091, 0.0267, 0.0038, 0.0032, 0.0255],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0843, 0.0065, 0.5400, 0.0769, 0.0834, 0.1098, 0.0991],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1152, 0.0097, 0.1324, 0.3820, 0.1464, 0.1103, 0.1039],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1316, 0.0137, 0.1268, 0.1492, 0.3168, 0.1304, 0.1315],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0867, 0.0748, 0.1729, 0.1037, 0.0857, 0.3677, 0.1085],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1748, 0.0039, 0.1100, 0.0860, 0.1255, 0.0719, 0.4280],
       grad_fn=<DivBackward0>)
actions average: 
K:  1  action  0 :  tensor([0.3854, 0.0741, 0.0870, 0.1221, 0.1650, 0.0885, 0.0779],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0073, 0.9565, 0.0127, 0.0043, 0.0019, 0.0023, 0.0150],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0712, 0.0368, 0.5240, 0.0852, 0.0847, 0.1012, 0.0969],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1363, 0.1710, 0.1332, 0.1527, 0.1447, 0.1135, 0.1486],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1434, 0.0177, 0.1461, 0.1539, 0.2929, 0.1150, 0.1310],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1059, 0.0249, 0.1540, 0.1196, 0.1106, 0.3366, 0.1483],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1031, 0.1821, 0.1100, 0.1081, 0.1095, 0.0830, 0.3043],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  1  step number:  25 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  51.42331489516227
actor:  1 policy actor:  1  step number:  34 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.166]
 [-0.166]
 [-0.166]
 [-0.166]
 [-0.166]
 [-0.166]
 [-0.166]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.166]
 [-0.166]
 [-0.166]
 [-0.166]
 [-0.166]
 [-0.166]
 [-0.166]]
printing an ep nov before normalisation:  47.12978195335634
actor:  1 policy actor:  1  step number:  40 total reward:  0.2899999999999996  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  44.201178020901146
printing an ep nov before normalisation:  59.981469344368875
Printing some Q and Qe and total Qs values:  [[-0.052]
 [ 0.453]
 [-0.018]
 [-0.022]
 [ 0.203]
 [ 0.203]
 [ 0.203]] [[44.501]
 [50.831]
 [43.078]
 [39.626]
 [42.767]
 [42.767]
 [42.767]] [[0.765]
 [1.585]
 [0.727]
 [0.551]
 [0.932]
 [0.932]
 [0.932]]
Printing some Q and Qe and total Qs values:  [[0.245]
 [0.405]
 [0.233]
 [0.22 ]
 [0.227]
 [0.413]
 [0.451]] [[44.256]
 [45.638]
 [43.309]
 [43.116]
 [43.243]
 [43.336]
 [42.86 ]] [[1.512]
 [1.745]
 [1.45 ]
 [1.427]
 [1.44 ]
 [1.632]
 [1.644]]
actor:  0 policy actor:  0  step number:  27 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  27 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  59.69831121690113
printing an ep nov before normalisation:  47.27887662081584
actor:  1 policy actor:  1  step number:  33 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4677600000000001 0.6635 0.6635
probs:  [0.07361758921951193, 0.07361758921951193, 0.15865499540458622, 0.07361758921951193, 0.5103657192699725, 0.11012651766690533]
actor:  1 policy actor:  1  step number:  27 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4677600000000001 0.6635 0.6635
probs:  [0.07359038236617862, 0.07359038236617862, 0.15886510366269788, 0.07359038236617862, 0.5101625524339353, 0.11020119680483079]
printing an ep nov before normalisation:  42.604679902544035
printing an ep nov before normalisation:  39.87079386596726
maxi score, test score, baseline:  -0.4677600000000001 0.6635 0.6635
probs:  [0.07359038236617862, 0.07359038236617862, 0.15886510366269788, 0.07359038236617862, 0.5101625524339353, 0.11020119680483079]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4677600000000001 0.6635 0.6635
probs:  [0.07353603437422576, 0.07353603437422576, 0.15928481268899353, 0.07353603437422576, 0.5097567094856614, 0.11035037470266759]
Printing some Q and Qe and total Qs values:  [[0.361]
 [0.466]
 [0.292]
 [0.293]
 [0.285]
 [0.288]
 [0.287]] [[38.186]
 [40.015]
 [33.079]
 [35.155]
 [37.18 ]
 [36.614]
 [34.49 ]] [[0.361]
 [0.466]
 [0.292]
 [0.293]
 [0.285]
 [0.288]
 [0.287]]
printing an ep nov before normalisation:  38.86449337005615
maxi score, test score, baseline:  -0.4677600000000001 0.6635 0.6635
probs:  [0.07353603437422576, 0.07353603437422576, 0.15928481268899353, 0.07353603437422576, 0.5097567094856614, 0.11035037470266759]
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.598]
 [0.61 ]
 [0.556]
 [0.502]
 [0.565]
 [0.606]] [[47.152]
 [46.231]
 [44.822]
 [43.842]
 [44.942]
 [45.286]
 [46.482]] [[2.319]
 [2.261]
 [2.192]
 [2.081]
 [2.091]
 [2.173]
 [2.284]]
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.975]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]] [[35.377]
 [34.986]
 [35.377]
 [35.377]
 [35.377]
 [35.377]
 [35.377]] [[1.76 ]
 [2.287]
 [1.76 ]
 [1.76 ]
 [1.76 ]
 [1.76 ]
 [1.76 ]]
actor:  1 policy actor:  1  step number:  19 total reward:  0.7  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  34 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.4677600000000001 0.6635 0.6635
probs:  [0.0713683990689066, 0.0713683990689066, 0.15458495708742206, 0.0713683990689066, 0.49470717814787335, 0.13660266755798486]
actor:  0 policy actor:  1  step number:  22 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.003]
 [ 0.193]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[49.306]
 [40.843]
 [49.306]
 [49.306]
 [49.306]
 [49.306]
 [49.306]] [[0.709]
 [0.729]
 [0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.709]]
maxi score, test score, baseline:  -0.4646200000000001 0.6635 0.6635
probs:  [0.0713683990689066, 0.0713683990689066, 0.15458495708742206, 0.0713683990689066, 0.49470717814787335, 0.13660266755798486]
maxi score, test score, baseline:  -0.4646200000000001 0.6635 0.6635
probs:  [0.0713683990689066, 0.0713683990689066, 0.15458495708742206, 0.0713683990689066, 0.49470717814787335, 0.13660266755798486]
printing an ep nov before normalisation:  53.63556981977252
using explorer policy with actor:  0
printing an ep nov before normalisation:  53.42612266540527
printing an ep nov before normalisation:  53.898589539073114
printing an ep nov before normalisation:  47.0848276157375
actor:  0 policy actor:  1  step number:  30 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  23 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4617200000000001 0.6635 0.6635
probs:  [0.07133625413714884, 0.07133625413714884, 0.1547757604711842, 0.07133625413714884, 0.49447018314010216, 0.13674529397726726]
Printing some Q and Qe and total Qs values:  [[0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]] [[29.989]
 [29.989]
 [29.989]
 [29.989]
 [29.989]
 [29.989]
 [29.989]] [[1.424]
 [1.424]
 [1.424]
 [1.424]
 [1.424]
 [1.424]
 [1.424]]
actor:  1 policy actor:  1  step number:  19 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
from probs:  [0.07133625413714884, 0.07133625413714884, 0.1547757604711842, 0.07133625413714884, 0.49447018314010216, 0.13674529397726726]
actor:  1 policy actor:  1  step number:  34 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  47.35697957876227
printing an ep nov before normalisation:  65.19570222438692
printing an ep nov before normalisation:  46.779255457731615
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.4617200000000001 0.6635 0.6635
probs:  [0.07146052391452498, 0.07146052391452498, 0.15403812959981036, 0.07146052391452498, 0.4953863871960611, 0.13619391146055357]
actor:  0 policy actor:  0  step number:  31 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.45 ]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]] [[44.242]
 [47.792]
 [44.242]
 [44.242]
 [44.242]
 [44.242]
 [44.242]] [[0.606]
 [0.693]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]]
printing an ep nov before normalisation:  54.010000458038
actor:  1 policy actor:  1  step number:  20 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4590800000000001 0.6635 0.6635
probs:  [0.07142888169927535, 0.07142888169927535, 0.1542259489980551, 0.07142888169927535, 0.4951530985673746, 0.13633430733674426]
maxi score, test score, baseline:  -0.4590800000000001 0.6635 0.6635
probs:  [0.07142888169927535, 0.07142888169927535, 0.1542259489980551, 0.07142888169927535, 0.4951530985673746, 0.13633430733674426]
maxi score, test score, baseline:  -0.4590800000000001 0.6635 0.6635
probs:  [0.07139726957186047, 0.07139726957186047, 0.15441358980327222, 0.07139726957186047, 0.4949200317673326, 0.13647456971381378]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4590800000000001 0.6635 0.6635
probs:  [0.07136568748938603, 0.07136568748938603, 0.1546010522700705, 0.07136568748938603, 0.49468718647968785, 0.13661469878208363]
actor:  1 policy actor:  1  step number:  38 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.919]
 [0.95 ]
 [0.925]
 [0.925]
 [0.925]
 [0.925]
 [0.925]] [[64.567]
 [47.813]
 [61.376]
 [61.376]
 [61.376]
 [61.376]
 [61.376]] [[0.919]
 [0.95 ]
 [0.925]
 [0.925]
 [0.925]
 [0.925]
 [0.925]]
maxi score, test score, baseline:  -0.4590800000000001 0.6635 0.6635
probs:  [0.07136568748938603, 0.07136568748938603, 0.1546010522700705, 0.07136568748938603, 0.49468718647968785, 0.13661469878208363]
maxi score, test score, baseline:  -0.4590800000000001 0.6635 0.6635
maxi score, test score, baseline:  -0.4590800000000001 0.6635 0.6635
probs:  [0.07136568748938603, 0.07136568748938603, 0.1546010522700705, 0.07136568748938603, 0.49468718647968785, 0.13661469878208363]
printing an ep nov before normalisation:  42.49170637490616
actor:  1 policy actor:  1  step number:  30 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  50.21202743003256
actor:  0 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  29 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.45594000000000007 0.6635 0.6635
probs:  [0.07130261328808855, 0.07130261328808855, 0.15497544320442802, 0.07130261328808855, 0.49422215917960505, 0.1368945577517012]
actions average: 
K:  1  action  0 :  tensor([0.4952, 0.0062, 0.0921, 0.1021, 0.1231, 0.0738, 0.1075],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0024,     0.9731,     0.0034,     0.0036,     0.0006,     0.0013,
            0.0157], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0882, 0.0039, 0.5532, 0.0934, 0.0838, 0.0525, 0.1249],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1065, 0.0071, 0.1098, 0.3642, 0.1097, 0.1179, 0.1848],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1417, 0.0030, 0.1296, 0.1587, 0.3098, 0.1136, 0.1436],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1011, 0.0024, 0.1815, 0.1293, 0.1036, 0.3858, 0.0962],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2511, 0.0167, 0.1148, 0.1400, 0.1294, 0.0950, 0.2530],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  53.74953312616336
actions average: 
K:  0  action  0 :  tensor([0.3967, 0.0022, 0.1194, 0.1411, 0.1403, 0.0952, 0.1051],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0055,     0.9499,     0.0043,     0.0052,     0.0004,     0.0009,
            0.0338], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1389, 0.0029, 0.3372, 0.1594, 0.1265, 0.1165, 0.1185],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1580, 0.0345, 0.1218, 0.2763, 0.1500, 0.0999, 0.1594],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1659, 0.0077, 0.1760, 0.2012, 0.1705, 0.1285, 0.1503],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1252, 0.0014, 0.1096, 0.1521, 0.1210, 0.3825, 0.1082],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1521, 0.0393, 0.0879, 0.1660, 0.1134, 0.0902, 0.3511],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.45594000000000007 0.6635 0.6635
probs:  [0.07123965875385478, 0.07123965875385478, 0.15534912382834384, 0.07123965875385478, 0.4937580141491412, 0.13717388576095071]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.45594000000000007 0.6635 0.6635
probs:  [0.07120822625551339, 0.07120822625551339, 0.15553569840528883, 0.07120822625551339, 0.4935262717007572, 0.13731335112741386]
Printing some Q and Qe and total Qs values:  [[-0.118]
 [-0.107]
 [-0.118]
 [-0.118]
 [-0.149]
 [-0.141]
 [-0.118]] [[39.019]
 [45.146]
 [39.019]
 [39.019]
 [41.792]
 [43.418]
 [39.019]] [[1.124]
 [1.337]
 [1.124]
 [1.124]
 [1.184]
 [1.245]
 [1.124]]
maxi score, test score, baseline:  -0.45594000000000007 0.6635 0.6635
printing an ep nov before normalisation:  48.68817548079166
maxi score, test score, baseline:  -0.45594000000000007 0.6635 0.6635
probs:  [0.07120822625551339, 0.07120822625551339, 0.15553569840528883, 0.07120822625551339, 0.4935262717007572, 0.13731335112741386]
printing an ep nov before normalisation:  58.494886988525394
maxi score, test score, baseline:  -0.45594000000000007 0.6635 0.6635
probs:  [0.07120822625551339, 0.07120822625551339, 0.15553569840528883, 0.07120822625551339, 0.4935262717007572, 0.13731335112741386]
maxi score, test score, baseline:  -0.45594000000000007 0.6635 0.6635
probs:  [0.07120822625551339, 0.07120822625551339, 0.15553569840528883, 0.07120822625551339, 0.4935262717007572, 0.13731335112741386]
printing an ep nov before normalisation:  37.35583423630645
actor:  1 policy actor:  1  step number:  31 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  2.0
from probs:  [0.07120822625551339, 0.07120822625551339, 0.15553569840528883, 0.07120822625551339, 0.4935262717007572, 0.13731335112741386]
maxi score, test score, baseline:  -0.45594000000000007 0.6635 0.6635
probs:  [0.0712195586173961, 0.0712195586173961, 0.15556047613386487, 0.0712195586173961, 0.49360493942604483, 0.13717590858790185]
maxi score, test score, baseline:  -0.45594000000000007 0.6635 0.6635
probs:  [0.0712195586173961, 0.0712195586173961, 0.15556047613386487, 0.0712195586173961, 0.49360493942604483, 0.13717590858790185]
actor:  1 policy actor:  1  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.45594000000000007 0.6635 0.6635
probs:  [0.0712195586173961, 0.0712195586173961, 0.15556047613386487, 0.0712195586173961, 0.49360493942604483, 0.13717590858790185]
printing an ep nov before normalisation:  54.36328081554158
maxi score, test score, baseline:  -0.45594000000000007 0.6635 0.6635
probs:  [0.0712195586173961, 0.0712195586173961, 0.15556047613386487, 0.0712195586173961, 0.49360493942604483, 0.13717590858790185]
actor:  0 policy actor:  0  step number:  21 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
using another actor
maxi score, test score, baseline:  -0.45274000000000003 0.6635 0.6635
Printing some Q and Qe and total Qs values:  [[0.57 ]
 [0.861]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.373]] [[39.585]
 [44.441]
 [40.209]
 [40.209]
 [40.209]
 [40.209]
 [39.629]] [[0.57 ]
 [0.861]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.373]]
actor:  1 policy actor:  1  step number:  27 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.153]
 [-0.153]
 [-0.153]
 [-0.153]
 [-0.153]
 [-0.153]
 [-0.153]] [[54.242]
 [54.242]
 [54.242]
 [54.242]
 [54.242]
 [54.242]
 [54.242]] [[1.242]
 [1.242]
 [1.242]
 [1.242]
 [1.242]
 [1.242]
 [1.242]]
printing an ep nov before normalisation:  44.96112572441736
printing an ep nov before normalisation:  46.57503128051758
printing an ep nov before normalisation:  65.43902575216768
printing an ep nov before normalisation:  74.70380929049564
maxi score, test score, baseline:  -0.45274000000000003 0.6635 0.6635
probs:  [0.07107436465772385, 0.07107436465772385, 0.15651630547621942, 0.07107436465772385, 0.49252948150791365, 0.13773111904269528]
maxi score, test score, baseline:  -0.45274000000000003 0.6635 0.6635
probs:  [0.07107436465772385, 0.07107436465772385, 0.15651630547621942, 0.07107436465772385, 0.49252948150791365, 0.13773111904269528]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
from probs:  [0.07107436465772385, 0.07107436465772385, 0.15651630547621942, 0.07107436465772385, 0.49252948150791365, 0.13773111904269528]
printing an ep nov before normalisation:  0.003370335120962409
printing an ep nov before normalisation:  34.86272438909673
UNIT TEST: sample policy line 217 mcts : [0.061 0.224 0.143 0.143 0.082 0.082 0.265]
maxi score, test score, baseline:  -0.45274000000000003 0.6635 0.6635
probs:  [0.07101198115370852, 0.07101198115370852, 0.15688754078774597, 0.07101198115370852, 0.492069496430416, 0.13800701932071244]
from probs:  [0.07101198115370852, 0.07101198115370852, 0.15688754078774597, 0.07101198115370852, 0.492069496430416, 0.13800701932071244]
siam score:  -0.80860305
maxi score, test score, baseline:  -0.45274000000000003 0.6635 0.6635
probs:  [0.07101198115370852, 0.07101198115370852, 0.15688754078774597, 0.07101198115370852, 0.492069496430416, 0.13800701932071244]
printing an ep nov before normalisation:  50.042722078084594
maxi score, test score, baseline:  -0.45274000000000003 0.6635 0.6635
probs:  [0.07101198115370852, 0.07101198115370852, 0.15688754078774597, 0.07101198115370852, 0.492069496430416, 0.13800701932071244]
maxi score, test score, baseline:  -0.45274000000000003 0.6635 0.6635
probs:  [0.07101198115370852, 0.07101198115370852, 0.15688754078774597, 0.07101198115370852, 0.492069496430416, 0.13800701932071244]
printing an ep nov before normalisation:  30.79227592882603
actor:  1 policy actor:  1  step number:  26 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  21 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
using another actor
line 256 mcts: sample exp_bonus 56.359112931161356
printing an ep nov before normalisation:  57.42215086612914
maxi score, test score, baseline:  -0.4495400000000001 0.6635 0.6635
printing an ep nov before normalisation:  47.91843250574986
printing an ep nov before normalisation:  45.0053855509225
Printing some Q and Qe and total Qs values:  [[0.575]
 [0.72 ]
 [0.55 ]
 [0.55 ]
 [0.577]
 [0.579]
 [0.612]] [[33.18 ]
 [35.567]
 [36.312]
 [38.113]
 [35.084]
 [34.473]
 [34.32 ]] [[1.676]
 [1.9  ]
 [1.756]
 [1.816]
 [1.742]
 [1.724]
 [1.751]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.20665998966073
maxi score, test score, baseline:  -0.4495400000000001 0.6635 0.6635
probs:  [0.07099225553230587, 0.07099225553230587, 0.15709819762570176, 0.07099225553230587, 0.4919191018671149, 0.13800593391026575]
printing an ep nov before normalisation:  1.5177547538769431e-05
actor:  0 policy actor:  1  step number:  22 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  31 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.44628000000000007 0.6635 0.6635
probs:  [0.07099225553230587, 0.07099225553230587, 0.15709819762570176, 0.07099225553230587, 0.4919191018671149, 0.13800593391026575]
actor:  1 policy actor:  1  step number:  42 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.44628000000000007 0.6635 0.6635
actor:  1 policy actor:  1  step number:  20 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.44628000000000007 0.6635 0.6635
probs:  [0.07093009559184243, 0.07093009559184243, 0.15746857107537415, 0.07093009559184243, 0.4914607403834719, 0.13828040176562675]
Printing some Q and Qe and total Qs values:  [[0.596]
 [0.668]
 [0.494]
 [0.596]
 [0.596]
 [0.518]
 [0.508]] [[42.682]
 [45.374]
 [40.15 ]
 [42.682]
 [42.682]
 [40.405]
 [40.068]] [[1.034]
 [1.157]
 [0.884]
 [1.034]
 [1.034]
 [0.913]
 [0.896]]
siam score:  -0.81575346
actor:  1 policy actor:  1  step number:  38 total reward:  0.24999999999999956  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  63.69150760225115
maxi score, test score, baseline:  -0.44628000000000007 0.6635 0.6635
probs:  [0.07089905948246099, 0.07089905948246099, 0.15765349645995963, 0.07089905948246099, 0.49123188306738486, 0.13841744202527254]
printing an ep nov before normalisation:  55.172637785885506
printing an ep nov before normalisation:  56.6490998278774
maxi score, test score, baseline:  -0.44628000000000007 0.6635 0.6635
probs:  [0.07089905948246099, 0.07089905948246099, 0.15765349645995963, 0.07089905948246099, 0.49123188306738486, 0.13841744202527254]
printing an ep nov before normalisation:  54.78695557445623
maxi score, test score, baseline:  -0.44628000000000007 0.6635 0.6635
probs:  [0.07089905948246099, 0.07089905948246099, 0.15765349645995963, 0.07089905948246099, 0.49123188306738486, 0.13841744202527254]
actor:  1 policy actor:  1  step number:  30 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  47.16858071624349
maxi score, test score, baseline:  -0.44628000000000007 0.6635 0.6635
probs:  [0.07089905948246099, 0.07089905948246099, 0.15765349645995963, 0.07089905948246099, 0.49123188306738486, 0.13841744202527254]
siam score:  -0.81690675
actor:  0 policy actor:  1  step number:  30 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.131]
 [-0.09 ]
 [-0.131]
 [-0.129]
 [-0.125]
 [-0.129]
 [-0.129]] [[75.244]
 [72.729]
 [75.617]
 [73.54 ]
 [74.615]
 [73.54 ]
 [73.54 ]] [[0.73 ]
 [0.718]
 [0.738]
 [0.696]
 [0.723]
 [0.696]
 [0.696]]
maxi score, test score, baseline:  -0.4434600000000001 0.6635 0.6635
probs:  [0.07089905948246099, 0.07089905948246099, 0.15765349645995963, 0.07089905948246099, 0.49123188306738486, 0.13841744202527254]
maxi score, test score, baseline:  -0.4434600000000001 0.6635 0.6635
probs:  [0.07086805255865224, 0.07086805255865224, 0.15783824794538837, 0.07086805255865224, 0.4910032409629325, 0.13855435341572248]
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.571]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]] [[65.75 ]
 [65.062]
 [65.75 ]
 [65.75 ]
 [65.75 ]
 [65.75 ]
 [65.75 ]] [[1.666]
 [1.75 ]
 [1.666]
 [1.666]
 [1.666]
 [1.666]
 [1.666]]
printing an ep nov before normalisation:  36.643696554879824
actor:  1 policy actor:  1  step number:  34 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  22 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4434600000000001 0.6635 0.6635
probs:  [0.07083707477926743, 0.07083707477926743, 0.15802282577684068, 0.07083707477926743, 0.490774813766688, 0.13869113611866912]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  62.63849639778193
maxi score, test score, baseline:  -0.4434600000000001 0.6635 0.6635
probs:  [0.07084856320375417, 0.07084856320375417, 0.1580484807133999, 0.07084856320375417, 0.4908545364264829, 0.13855129324885465]
printing an ep nov before normalisation:  57.17790860226464
Printing some Q and Qe and total Qs values:  [[0.718]
 [0.772]
 [0.653]
 [0.709]
 [0.709]
 [0.686]
 [0.668]] [[43.318]
 [41.437]
 [35.925]
 [44.399]
 [43.404]
 [40.469]
 [35.588]] [[0.718]
 [0.772]
 [0.653]
 [0.709]
 [0.709]
 [0.686]
 [0.668]]
actor:  0 policy actor:  1  step number:  24 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  20.911581879386755
maxi score, test score, baseline:  -0.4373000000000001 0.6635 0.6635
actor:  1 policy actor:  1  step number:  34 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.4373000000000001 0.6635 0.6635
maxi score, test score, baseline:  -0.4373000000000001 0.6635 0.6635
probs:  [0.07079127666805873, 0.07079127666805873, 0.15792055347318823, 0.07079127666805873, 0.4904570028045883, 0.13924861371804736]
printing an ep nov before normalisation:  28.581392765045166
maxi score, test score, baseline:  -0.4373000000000001 0.6635 0.6635
probs:  [0.07079127666805873, 0.07079127666805873, 0.15792055347318823, 0.07079127666805873, 0.4904570028045883, 0.13924861371804736]
Printing some Q and Qe and total Qs values:  [[0.117]
 [0.117]
 [0.539]
 [0.117]
 [0.117]
 [0.117]
 [0.117]] [[53.495]
 [53.495]
 [61.325]
 [53.495]
 [53.495]
 [53.495]
 [53.495]] [[0.336]
 [0.336]
 [0.804]
 [0.336]
 [0.336]
 [0.336]
 [0.336]]
maxi score, test score, baseline:  -0.4373000000000001 0.6635 0.6635
probs:  [0.07079127666805873, 0.07079127666805873, 0.15792055347318823, 0.07079127666805873, 0.4904570028045883, 0.13924861371804736]
actor:  1 policy actor:  1  step number:  19 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8197717
maxi score, test score, baseline:  -0.4373000000000001 0.6635 0.6635
probs:  [0.07076023503824816, 0.07076023503824816, 0.1581045856997791, 0.07076023503824816, 0.49022815390332164, 0.1393865552821549]
using explorer policy with actor:  1
printing an ep nov before normalisation:  70.46010970571923
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  34 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  27.27549234314239
actor:  1 policy actor:  1  step number:  29 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]] [[54.188]
 [54.188]
 [54.188]
 [54.188]
 [54.188]
 [54.188]
 [54.188]] [[1.867]
 [1.867]
 [1.867]
 [1.867]
 [1.867]
 [1.867]
 [1.867]]
printing an ep nov before normalisation:  62.19949972633343
printing an ep nov before normalisation:  59.367793542309116
actions average: 
K:  3  action  0 :  tensor([0.3902, 0.0392, 0.1450, 0.1257, 0.0883, 0.1136, 0.0979],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0010,     0.9806,     0.0016,     0.0039,     0.0001,     0.0005,
            0.0123], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0726, 0.0081, 0.5998, 0.0734, 0.0532, 0.1326, 0.0603],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1164, 0.1738, 0.1406, 0.2154, 0.0844, 0.1356, 0.1338],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1424, 0.0033, 0.0641, 0.1080, 0.5955, 0.0543, 0.0324],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1436, 0.0644, 0.1658, 0.1417, 0.1040, 0.2694, 0.1111],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1213, 0.2279, 0.1140, 0.1284, 0.0797, 0.0832, 0.2455],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  52.472406543018785
actor:  1 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.107]
 [-0.107]
 [-0.107]
 [-0.107]
 [-0.107]
 [-0.107]
 [-0.107]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.107]
 [-0.107]
 [-0.107]
 [-0.107]
 [-0.107]
 [-0.107]
 [-0.107]]
actor:  1 policy actor:  1  step number:  34 total reward:  0.46999999999999986  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[ 0.068]
 [ 0.043]
 [ 0.068]
 [-0.003]
 [ 0.068]
 [ 0.068]
 [ 0.068]] [[64.825]
 [58.02 ]
 [64.825]
 [60.876]
 [64.825]
 [64.825]
 [64.825]] [[2.068]
 [1.729]
 [2.068]
 [1.815]
 [2.068]
 [2.068]
 [2.068]]
Printing some Q and Qe and total Qs values:  [[-0.005]
 [ 0.193]
 [-0.005]
 [-0.026]
 [-0.005]
 [-0.005]
 [-0.005]] [[48.626]
 [47.308]
 [48.626]
 [51.538]
 [48.626]
 [48.626]
 [48.626]] [[1.276]
 [1.413]
 [1.276]
 [1.389]
 [1.276]
 [1.276]
 [1.276]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.144]
 [0.371]
 [0.144]
 [0.144]
 [0.144]
 [0.144]
 [0.144]] [[51.647]
 [61.181]
 [51.647]
 [51.647]
 [51.647]
 [51.647]
 [51.647]] [[1.434]
 [2.038]
 [1.434]
 [1.434]
 [1.434]
 [1.434]
 [1.434]]
maxi score, test score, baseline:  -0.4373000000000001 0.6635 0.6635
probs:  [0.07066728547663857, 0.07066728547663857, 0.15865564293768947, 0.07066728547663857, 0.4895428997727431, 0.13979960085965157]
Printing some Q and Qe and total Qs values:  [[0.147]
 [0.401]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]] [[48.226]
 [49.091]
 [42.709]
 [42.709]
 [42.709]
 [42.709]
 [42.709]] [[1.15 ]
 [1.437]
 [0.966]
 [0.966]
 [0.966]
 [0.966]
 [0.966]]
printing an ep nov before normalisation:  47.63493957548179
printing an ep nov before normalisation:  41.43273497820392
Printing some Q and Qe and total Qs values:  [[0.74]
 [0.74]
 [0.74]
 [0.74]
 [0.74]
 [0.74]
 [0.74]] [[56.878]
 [56.878]
 [56.878]
 [56.878]
 [56.878]
 [56.878]
 [56.878]] [[2.74]
 [2.74]
 [2.74]
 [2.74]
 [2.74]
 [2.74]
 [2.74]]
printing an ep nov before normalisation:  41.002349853515625
actor:  1 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  36.0706285533107
actor:  1 policy actor:  1  step number:  28 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.029]
 [1.029]
 [1.029]
 [1.029]
 [1.029]
 [1.029]
 [1.029]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.029]
 [1.029]
 [1.029]
 [1.029]
 [1.029]
 [1.029]
 [1.029]]
maxi score, test score, baseline:  -0.4373000000000001 0.6635 0.6635
probs:  [0.07059792757935734, 0.07059792757935734, 0.15925782930734603, 0.07059792757935734, 0.48902143982028934, 0.13992694813429268]
maxi score, test score, baseline:  -0.4373000000000001 0.6635 0.6635
probs:  [0.07060951516440388, 0.07060951516440388, 0.1592839965917231, 0.07060951516440388, 0.4891018351626473, 0.1397856227524181]
printing an ep nov before normalisation:  59.76066516760761
maxi score, test score, baseline:  -0.4373000000000001 0.6635 0.6635
Printing some Q and Qe and total Qs values:  [[0.442]
 [0.438]
 [0.412]
 [0.41 ]
 [0.394]
 [0.416]
 [0.434]] [[43.998]
 [53.513]
 [46.232]
 [43.7  ]
 [35.191]
 [48.012]
 [45.06 ]] [[0.442]
 [0.438]
 [0.412]
 [0.41 ]
 [0.394]
 [0.416]
 [0.434]]
maxi score, test score, baseline:  -0.4373000000000001 0.6635 0.6635
probs:  [0.07048661250343347, 0.07048661250343347, 0.16001538660821038, 0.07048661250343347, 0.4881956110449892, 0.14032916483649996]
Printing some Q and Qe and total Qs values:  [[-0.025]
 [-0.004]
 [-0.025]
 [-0.025]
 [-0.025]
 [-0.025]
 [-0.025]] [[57.302]
 [65.151]
 [57.302]
 [57.302]
 [57.302]
 [57.302]
 [57.302]] [[0.723]
 [0.894]
 [0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]]
printing an ep nov before normalisation:  63.26163402897063
printing an ep nov before normalisation:  62.633642731086304
maxi score, test score, baseline:  -0.4373000000000001 0.6635 0.6635
probs:  [0.07045595869379757, 0.07045595869379757, 0.160197806501791, 0.07045595869379757, 0.48796958484367836, 0.1404647325731381]
actor:  0 policy actor:  1  step number:  21 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
using another actor
maxi score, test score, baseline:  -0.43410000000000004 0.6635 0.6635
probs:  [0.07045595869379757, 0.07045595869379757, 0.160197806501791, 0.07045595869379757, 0.48796958484367836, 0.1404647325731381]
actor:  1 policy actor:  1  step number:  31 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  30 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
siam score:  -0.824901
maxi score, test score, baseline:  -0.43410000000000004 0.6635 0.6635
probs:  [0.07039473701920575, 0.07039473701920575, 0.1605621348347626, 0.07039473701920575, 0.487518166155096, 0.1407354879525243]
maxi score, test score, baseline:  -0.43410000000000004 0.6635 0.6635
probs:  [0.07039473701920575, 0.07039473701920575, 0.1605621348347626, 0.07039473701920575, 0.487518166155096, 0.1407354879525243]
maxi score, test score, baseline:  -0.43410000000000004 0.6635 0.6635
actions average: 
K:  4  action  0 :  tensor([0.5894, 0.0468, 0.0760, 0.0724, 0.0830, 0.0722, 0.0602],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0057, 0.9644, 0.0071, 0.0066, 0.0036, 0.0037, 0.0089],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1041, 0.0039, 0.4886, 0.0927, 0.1183, 0.1039, 0.0886],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1042, 0.0943, 0.0903, 0.4153, 0.1001, 0.1102, 0.0856],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.3837, 0.0010, 0.0882, 0.1199, 0.2576, 0.0786, 0.0710],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1367, 0.0723, 0.1335, 0.1552, 0.1447, 0.2403, 0.1174],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1309, 0.1073, 0.1385, 0.1683, 0.1377, 0.1113, 0.2061],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.43410000000000004 0.6635 0.6635
probs:  [0.07039473701920575, 0.07039473701920575, 0.1605621348347626, 0.07039473701920575, 0.487518166155096, 0.1407354879525243]
printing an ep nov before normalisation:  0.17185675456801164
actor:  1 policy actor:  1  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  31 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  59.336264763641644
printing an ep nov before normalisation:  74.57338704290636
printing an ep nov before normalisation:  63.314253375515534
printing an ep nov before normalisation:  73.75773256019288
actor:  1 policy actor:  1  step number:  26 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  53.49305095284484
maxi score, test score, baseline:  -0.4311400000000001 0.6635 0.6635
actor:  1 policy actor:  1  step number:  29 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.088]
 [0.088]
 [0.687]
 [0.088]
 [0.088]
 [0.088]
 [0.088]] [[44.983]
 [44.983]
 [52.522]
 [44.983]
 [44.983]
 [44.983]
 [44.983]] [[0.442]
 [0.442]
 [1.156]
 [0.442]
 [0.442]
 [0.442]
 [0.442]]
maxi score, test score, baseline:  -0.4311400000000001 0.6635 0.6635
probs:  [0.07025398927064538, 0.07025398927064538, 0.16149715383808394, 0.07025398927064538, 0.4864751927785274, 0.1412656855714524]
printing an ep nov before normalisation:  62.327685220619614
printing an ep nov before normalisation:  46.25748344362559
actor:  0 policy actor:  1  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.42780000000000007 0.6635 0.6635
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  32 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.283]] [[59.887]
 [59.887]
 [59.887]
 [59.887]
 [59.887]
 [59.887]
 [59.887]] [[1.283]
 [1.283]
 [1.283]
 [1.283]
 [1.283]
 [1.283]
 [1.283]]
Printing some Q and Qe and total Qs values:  [[0.889]
 [0.965]
 [0.936]
 [0.936]
 [0.936]
 [0.936]
 [0.936]] [[50.31 ]
 [53.333]
 [49.784]
 [49.784]
 [49.784]
 [49.784]
 [49.784]] [[0.889]
 [0.965]
 [0.936]
 [0.936]
 [0.936]
 [0.936]
 [0.936]]
actor:  1 policy actor:  1  step number:  43 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  44.53890244528892
maxi score, test score, baseline:  -0.42498000000000014 0.6635 0.6635
probs:  [0.07025398927064538, 0.07025398927064538, 0.16149715383808394, 0.07025398927064538, 0.4864751927785274, 0.1412656855714524]
printing an ep nov before normalisation:  37.25806213814568
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  27 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.42174000000000006 0.6635 0.6635
probs:  [0.07025398927064538, 0.07025398927064538, 0.16149715383808394, 0.07025398927064538, 0.4864751927785274, 0.1412656855714524]
maxi score, test score, baseline:  -0.42174000000000006 0.6635 0.6635
probs:  [0.07025398927064538, 0.07025398927064538, 0.16149715383808394, 0.07025398927064538, 0.4864751927785274, 0.1412656855714524]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  57.81727339487879
printing an ep nov before normalisation:  56.49694623765262
maxi score, test score, baseline:  -0.42174000000000006 0.6635 0.6635
probs:  [0.07022358587326996, 0.07022358587326996, 0.16167830944776812, 0.07022358587326996, 0.4862510010113723, 0.14139993192104963]
maxi score, test score, baseline:  -0.4217400000000001 0.6635 0.6635
probs:  [0.07022358587326996, 0.07022358587326996, 0.16167830944776812, 0.07022358587326996, 0.4862510010113723, 0.14139993192104963]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4217400000000001 0.6635 0.6635
probs:  [0.07022358587326996, 0.07022358587326996, 0.16167830944776812, 0.07022358587326996, 0.4862510010113723, 0.14139993192104963]
printing an ep nov before normalisation:  27.13019847869873
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  44 total reward:  0.2699999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.4217400000000001 0.6635 0.6635
probs:  [0.07023536324500784, 0.07023536324500784, 0.1617054540724381, 0.07023536324500784, 0.4863326839961311, 0.14125577219640725]
maxi score, test score, baseline:  -0.4217400000000001 0.6635 0.6635
probs:  [0.07023536324500784, 0.07023536324500784, 0.1617054540724381, 0.07023536324500784, 0.4863326839961311, 0.14125577219640725]
actor:  1 policy actor:  1  step number:  37 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.4217400000000001 0.6635 0.6635
probs:  [0.07021675815272678, 0.07021675815272678, 0.1619136530835917, 0.07021675815272678, 0.4861903283274481, 0.14124574413077995]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4217400000000001 0.6635 0.6635
probs:  [0.07018645535130123, 0.07018645535130123, 0.16209465788265567, 0.07018645535130123, 0.48596685455054744, 0.1413791215128932]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  -0.4217400000000001 0.6635 0.6635
probs:  [0.07015618068101226, 0.07015618068101226, 0.1622754946487166, 0.07015618068101226, 0.4857435882320679, 0.14151237507617864]
maxi score, test score, baseline:  -0.4217400000000001 0.6635 0.6635
probs:  [0.07015618068101226, 0.07015618068101226, 0.1622754946487166, 0.07015618068101226, 0.4857435882320679, 0.14151237507617864]
printing an ep nov before normalisation:  59.91025209263461
maxi score, test score, baseline:  -0.4217400000000001 0.6635 0.6635
probs:  [0.07015618068101226, 0.07015618068101226, 0.1622754946487166, 0.07015618068101226, 0.4857435882320679, 0.14151237507617864]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  71.3400407447201
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.629]
 [0.541]
 [0.537]
 [0.502]
 [0.553]
 [0.539]] [[74.716]
 [56.834]
 [75.983]
 [71.617]
 [77.926]
 [75.065]
 [73.997]] [[0.507]
 [0.629]
 [0.541]
 [0.537]
 [0.502]
 [0.553]
 [0.539]]
actor:  1 policy actor:  1  step number:  22 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4217400000000001 0.6635 0.6635
printing an ep nov before normalisation:  51.64822153691258
UNIT TEST: sample policy line 217 mcts : [0.061 0.224 0.143 0.122 0.265 0.122 0.061]
printing an ep nov before normalisation:  36.164761515314304
actor:  1 policy actor:  1  step number:  29 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.42174000000000006 0.6635 0.6635
probs:  [0.07008908506718557, 0.07008908506718557, 0.162871806806627, 0.07008908506718557, 0.48523840671936336, 0.141622531272453]
printing an ep nov before normalisation:  61.06602096087379
maxi score, test score, baseline:  -0.42174000000000006 0.6635 0.6635
actor:  1 policy actor:  1  step number:  26 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  32 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  53.21820138259137
maxi score, test score, baseline:  -0.42174000000000006 0.6635 0.6635
probs:  [0.07002887436325136, 0.07002887436325136, 0.16323234430557443, 0.07002887436325136, 0.48479432438754155, 0.14188670821712993]
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]] [[61.002]
 [61.002]
 [61.002]
 [61.002]
 [61.002]
 [61.002]
 [61.002]] [[1.928]
 [1.928]
 [1.928]
 [1.928]
 [1.928]
 [1.928]
 [1.928]]
actor:  1 policy actor:  1  step number:  35 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  70.27888090198167
printing an ep nov before normalisation:  55.12594506563617
maxi score, test score, baseline:  -0.42174000000000006 0.6635 0.6635
probs:  [0.06998056398806371, 0.06998056398806371, 0.16361980952075253, 0.06998056398806371, 0.48443280435773767, 0.14200569415731862]
actor:  1 policy actor:  1  step number:  33 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  28.01671990320772
Printing some Q and Qe and total Qs values:  [[0.914]
 [0.931]
 [0.914]
 [0.914]
 [0.914]
 [0.914]
 [0.914]] [[61.672]
 [64.868]
 [61.672]
 [61.672]
 [61.672]
 [61.672]
 [61.672]] [[0.914]
 [0.931]
 [0.914]
 [0.914]
 [0.914]
 [0.914]
 [0.914]]
printing an ep nov before normalisation:  76.02467195734428
actor:  1 policy actor:  1  step number:  29 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.42174000000000006 0.6635 0.6635
probs:  [0.06995057721212442, 0.06995057721212442, 0.16379958819619864, 0.06995057721212442, 0.48421162607874585, 0.14213705408868238]
actor:  1 policy actor:  1  step number:  37 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.42174000000000006 0.6635 0.6635
maxi score, test score, baseline:  -0.42174000000000006 0.6635 0.6635
probs:  [0.06992061808990553, 0.06992061808990553, 0.1639792010802559, 0.06992061808990553, 0.48399065176974054, 0.14226829288028697]
printing an ep nov before normalisation:  59.72225150154169
maxi score, test score, baseline:  -0.4217400000000001 0.6635 0.6635
probs:  [0.06992061808990553, 0.06992061808990553, 0.1639792010802559, 0.06992061808990553, 0.48399065176974054, 0.14226829288028697]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4217400000000001 0.6635 0.6635
probs:  [0.06989068658317137, 0.06989068658317137, 0.16415864840215755, 0.06989068658317137, 0.48376988114870084, 0.1423994106996275]
maxi score, test score, baseline:  -0.4217400000000001 0.6635 0.6635
probs:  [0.06989068658317137, 0.06989068658317137, 0.16415864840215755, 0.06989068658317137, 0.48376988114870084, 0.1423994106996275]
actor:  1 policy actor:  1  step number:  35 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.667
using another actor
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.496]
 [0.461]
 [0.161]
 [0.461]
 [0.461]
 [0.461]] [[64.685]
 [60.941]
 [59.576]
 [57.763]
 [59.576]
 [59.576]
 [59.576]] [[1.793]
 [1.685]
 [1.609]
 [1.254]
 [1.609]
 [1.609]
 [1.609]]
printing an ep nov before normalisation:  54.52591591909282
maxi score, test score, baseline:  -0.4217400000000001 0.6635 0.6635
probs:  [0.06986078265375678, 0.06986078265375678, 0.16433793039071382, 0.06986078265375678, 0.4835493139341258, 0.14253040771388997]
maxi score, test score, baseline:  -0.4217400000000001 0.6635 0.6635
probs:  [0.06986078265375678, 0.06986078265375678, 0.16433793039071382, 0.06986078265375678, 0.4835493139341258, 0.14253040771388997]
actor:  1 policy actor:  1  step number:  32 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4217400000000001 0.6635 0.6635
probs:  [0.06983090626356683, 0.06983090626356683, 0.16451704727431374, 0.06983090626356683, 0.4833289498450327, 0.142661284089953]
actor:  0 policy actor:  1  step number:  19 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4184600000000001 0.6635 0.6635
probs:  [0.06983090626356683, 0.06983090626356683, 0.16451704727431374, 0.06983090626356683, 0.4833289498450327, 0.142661284089953]
printing an ep nov before normalisation:  0.13235486338771807
actor:  1 policy actor:  1  step number:  33 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.4184600000000001 0.6635 0.6635
probs:  [0.06983090626356683, 0.06983090626356683, 0.16451704727431374, 0.06983090626356683, 0.4833289498450327, 0.142661284089953]
maxi score, test score, baseline:  -0.4184600000000001 0.6635 0.6635
probs:  [0.06983090626356683, 0.06983090626356683, 0.16451704727431374, 0.06983090626356683, 0.4833289498450327, 0.142661284089953]
printing an ep nov before normalisation:  47.29989364337575
printing an ep nov before normalisation:  50.73577787745568
printing an ep nov before normalisation:  28.91660451889038
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  31 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  51.250908600607204
printing an ep nov before normalisation:  48.334747238346495
maxi score, test score, baseline:  -0.41846000000000005 0.6635 0.6635
probs:  [0.06980105737457683, 0.06980105737457683, 0.1646959992809257, 0.06980105737457683, 0.4831087886009564, 0.14279203999438733]
maxi score, test score, baseline:  -0.41846000000000005 0.6635 0.6635
probs:  [0.06977123594883193, 0.06977123594883193, 0.16487478663809893, 0.06977123594883193, 0.48288882992194754, 0.1429226755934577]
printing an ep nov before normalisation:  52.59628622568917
printing an ep nov before normalisation:  31.79600781078875
printing an ep nov before normalisation:  47.28132645800695
Printing some Q and Qe and total Qs values:  [[ 0.161]
 [ 0.161]
 [ 0.449]
 [ 0.161]
 [-0.061]
 [ 0.161]
 [ 0.161]] [[46.188]
 [46.188]
 [46.584]
 [46.188]
 [50.001]
 [46.188]
 [46.188]] [[1.364]
 [1.364]
 [1.67 ]
 [1.364]
 [1.321]
 [1.364]
 [1.364]]
maxi score, test score, baseline:  -0.41846000000000005 0.6635 0.6635
probs:  [0.06977123594883193, 0.06977123594883193, 0.16487478663809893, 0.06977123594883193, 0.48288882992194754, 0.1429226755934577]
printing an ep nov before normalisation:  48.55506553074055
actor:  1 policy actor:  1  step number:  41 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.41846000000000005 0.6635 0.6635
probs:  [0.06974144194844717, 0.06974144194844717, 0.16505340957296374, 0.06974144194844717, 0.48266907352857186, 0.1430531910531229]
printing an ep nov before normalisation:  43.37602123199436
actor:  1 policy actor:  1  step number:  40 total reward:  0.24999999999999956  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.52 ]
 [0.483]
 [0.529]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]] [[60.005]
 [59.949]
 [61.095]
 [60.005]
 [60.005]
 [60.005]
 [60.005]] [[1.895]
 [1.856]
 [1.94 ]
 [1.895]
 [1.895]
 [1.895]
 [1.895]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  25.46323416735867
actor:  1 policy actor:  1  step number:  38 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.41846000000000005 0.6635 0.6635
probs:  [0.06969388475028289, 0.06969388475028289, 0.1654385581576191, 0.06969388475028289, 0.482312992189696, 0.14316679540183636]
actor:  1 policy actor:  1  step number:  21 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.11 ]
 [-0.097]
 [-0.118]
 [-0.107]
 [-0.11 ]
 [-0.111]
 [-0.11 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.11 ]
 [-0.097]
 [-0.118]
 [-0.107]
 [-0.11 ]
 [-0.111]
 [-0.11 ]]
maxi score, test score, baseline:  -0.41846000000000005 0.6635 0.6635
probs:  [0.06969388475028289, 0.06969388475028289, 0.1654385581576191, 0.06969388475028289, 0.482312992189696, 0.14316679540183636]
printing an ep nov before normalisation:  64.70574683351624
printing an ep nov before normalisation:  67.17994995547649
printing an ep nov before normalisation:  66.33955655517528
printing an ep nov before normalisation:  53.19165627299906
maxi score, test score, baseline:  -0.41846000000000005 0.6635 0.6635
probs:  [0.06969388475028289, 0.06969388475028289, 0.1654385581576191, 0.06969388475028289, 0.482312992189696, 0.14316679540183636]
printing an ep nov before normalisation:  41.508030155205695
maxi score, test score, baseline:  -0.41846000000000005 0.6635 0.6635
probs:  [0.06969388475028289, 0.06969388475028289, 0.1654385581576191, 0.06969388475028289, 0.482312992189696, 0.14316679540183636]
actor:  1 policy actor:  1  step number:  30 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.41846000000000005 0.6635 0.6635
probs:  [0.06966419363084436, 0.06966419363084436, 0.16561678151543982, 0.06966419363084436, 0.482093983109423, 0.1432966544826042]
printing an ep nov before normalisation:  84.78062578981549
actor:  1 policy actor:  1  step number:  28 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  36 total reward:  0.1899999999999995  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  71.96503852171415
printing an ep nov before normalisation:  55.57624051829332
maxi score, test score, baseline:  -0.41608000000000006 0.6635 0.6635
probs:  [0.0696345297446784, 0.0696345297446784, 0.1657948414033284, 0.0696345297446784, 0.4818751749085406, 0.14342639445409594]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8207733
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  30 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  57.39882069654786
maxi score, test score, baseline:  -0.41608000000000006 0.6635 0.6635
probs:  [0.06961685236321259, 0.06961685236321259, 0.16600128674868067, 0.06961685236321259, 0.48173945990095285, 0.1434086962607288]
maxi score, test score, baseline:  -0.41608000000000006 0.6635 0.6635
maxi score, test score, baseline:  -0.41608000000000006 0.6635 0.6635
probs:  [0.06962876026605132, 0.06962876026605132, 0.16602971273717956, 0.06962876026605132, 0.48182199618457966, 0.14326201028008684]
printing an ep nov before normalisation:  0.0052409659872409975
actor:  1 policy actor:  1  step number:  24 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
from probs:  [0.06962876026605132, 0.06962876026605132, 0.16602971273717956, 0.06962876026605132, 0.48182199618457966, 0.14326201028008684]
maxi score, test score, baseline:  -0.41608000000000006 0.6635 0.6635
actor:  1 policy actor:  1  step number:  26 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  65.16332743561293
maxi score, test score, baseline:  -0.41608000000000006 0.6635 0.6635
probs:  [0.06953853019252193, 0.06953853019252193, 0.16606260436036818, 0.06953853019252193, 0.4811834216323323, 0.14413838342973367]
actor:  0 policy actor:  1  step number:  17 total reward:  0.72  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  67.14306547554106
actor:  0 policy actor:  0  step number:  23 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  65.94025611877441
maxi score, test score, baseline:  -0.40952000000000005 0.6635 0.6635
probs:  [0.06953853019252193, 0.06953853019252193, 0.16606260436036818, 0.06953853019252193, 0.4811834216323323, 0.14413838342973367]
maxi score, test score, baseline:  -0.40952000000000005 0.6635 0.6635
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  34 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  27 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.40952000000000005 0.6635 0.6635
probs:  [0.06949127660927559, 0.06949127660927559, 0.16644581733582922, 0.06949127660927559, 0.48082958583383617, 0.14425076700250786]
maxi score, test score, baseline:  -0.40952000000000005 0.6635 0.6635
probs:  [0.06949127660927559, 0.06949127660927559, 0.16644581733582922, 0.06949127660927559, 0.48082958583383617, 0.14425076700250786]
printing an ep nov before normalisation:  65.0553814860368
maxi score, test score, baseline:  -0.40952000000000005 0.6635 0.6635
probs:  [0.06949127660927559, 0.06949127660927559, 0.16644581733582922, 0.06949127660927559, 0.48082958583383617, 0.14425076700250786]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.40952000000000005 0.6635 0.6635
probs:  [0.06940264583070258, 0.06940264583070258, 0.1669764963315691, 0.06940264583070258, 0.48017589395689786, 0.14463967221942534]
maxi score, test score, baseline:  -0.40952000000000005 0.6635 0.6635
probs:  [0.06940264583070258, 0.06940264583070258, 0.1669764963315691, 0.06940264583070258, 0.48017589395689786, 0.14463967221942534]
maxi score, test score, baseline:  -0.40952000000000005 0.6635 0.6635
probs:  [0.06940264583070258, 0.06940264583070258, 0.1669764963315691, 0.06940264583070258, 0.48017589395689786, 0.14463967221942534]
printing an ep nov before normalisation:  65.10658010950912
line 256 mcts: sample exp_bonus 52.89375431437188
actor:  1 policy actor:  1  step number:  24 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  28 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.4066600000000001 0.6635 0.6635
probs:  [0.06940264583070258, 0.06940264583070258, 0.1669764963315691, 0.06940264583070258, 0.48017589395689786, 0.14463967221942534]
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [ 0.136]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]] [[66.381]
 [65.938]
 [66.381]
 [66.381]
 [66.381]
 [66.381]
 [66.381]] [[0.743]
 [0.882]
 [0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]]
maxi score, test score, baseline:  -0.4066600000000001 0.6635 0.6635
probs:  [0.06940264583070258, 0.06940264583070258, 0.1669764963315691, 0.06940264583070258, 0.48017589395689786, 0.14463967221942534]
printing an ep nov before normalisation:  65.50087502311112
siam score:  -0.83641434
maxi score, test score, baseline:  -0.4066600000000001 0.6635 0.6635
probs:  [0.06940264583070258, 0.06940264583070258, 0.1669764963315691, 0.06940264583070258, 0.48017589395689786, 0.14463967221942534]
Printing some Q and Qe and total Qs values:  [[-0.026]
 [ 0.076]
 [-0.026]
 [-0.028]
 [-0.026]
 [-0.026]
 [-0.026]] [[68.74 ]
 [60.522]
 [68.74 ]
 [81.608]
 [68.74 ]
 [68.74 ]
 [68.74 ]] [[1.203]
 [1.025]
 [1.203]
 [1.639]
 [1.203]
 [1.203]
 [1.203]]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  25 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4066600000000001 0.6635 0.6635
probs:  [0.069343693802676, 0.069343693802676, 0.16732947306869878, 0.069343693802676, 0.47974109622104977, 0.14489834930222342]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  45.26797048908828
maxi score, test score, baseline:  -0.4066600000000001 0.6635 0.6635
probs:  [0.06931425822472141, 0.06931425822472141, 0.16750571932535438, 0.06931425822472141, 0.47952399558725534, 0.14502751041322604]
printing an ep nov before normalisation:  2.822391954985619
actor:  1 policy actor:  1  step number:  21 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.0
from probs:  [0.06931425822472141, 0.06931425822472141, 0.16750571932535438, 0.06931425822472141, 0.47952399558725534, 0.14502751041322604]
actor:  1 policy actor:  1  step number:  26 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4066600000000001 0.6635 0.6635
probs:  [0.069255467756194, 0.069255467756194, 0.16785772872095628, 0.069255467756194, 0.4790903894254172, 0.14528547858504468]
printing an ep nov before normalisation:  48.827574472589745
actor:  0 policy actor:  1  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  26 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4036800000000001 0.6635 0.6635
probs:  [0.06920894565520698, 0.06920894565520698, 0.16823869100407945, 0.06920894565520698, 0.4787418357019168, 0.14539263632838276]
printing an ep nov before normalisation:  51.3452421373871
actor:  1 policy actor:  1  step number:  22 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.087]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.087]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]]
maxi score, test score, baseline:  -0.4036800000000001 0.6635 0.6635
probs:  [0.06915040949203222, 0.06915040949203222, 0.16858960116247768, 0.06915040949203222, 0.4783100826962489, 0.14564908766517673]
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.685]
 [0.473]
 [0.544]
 [0.487]
 [0.458]
 [0.483]] [[37.293]
 [39.869]
 [38.304]
 [38.776]
 [40.795]
 [37.576]
 [37.141]] [[1.232]
 [1.406]
 [1.166]
 [1.245]
 [1.224]
 [1.137]
 [1.154]]
printing an ep nov before normalisation:  37.71203279495239
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  38 total reward:  0.3299999999999996  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  0.004546862060124113
from probs:  [0.06912118140124718, 0.06912118140124718, 0.1687648165064768, 0.06912118140124718, 0.47809450115892316, 0.14577713813085852]
Printing some Q and Qe and total Qs values:  [[-0.045]
 [-0.037]
 [-0.045]
 [-0.045]
 [-0.045]
 [-0.045]
 [-0.045]] [[44.69]
 [53.47]
 [44.69]
 [44.69]
 [44.69]
 [44.69]
 [44.69]] [[0.594]
 [0.745]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]]
maxi score, test score, baseline:  -0.4036800000000001 0.6635 0.6635
maxi score, test score, baseline:  -0.4036800000000001 0.6635 0.6635
probs:  [0.06909197992244862, 0.06909197992244862, 0.16893987231804528, 0.06909197992244862, 0.4778791159071842, 0.14590507200742472]
maxi score, test score, baseline:  -0.4036800000000001 0.6635 0.6635
actor:  1 policy actor:  1  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  27 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  68.12784609316716
using another actor
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.30694389343262
actor:  1 policy actor:  1  step number:  21 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4006800000000001 0.6635 0.6635
probs:  [0.06906280501930795, 0.06906280501930795, 0.16911476881496432, 0.06906280501930795, 0.4776639266730787, 0.1460328894540332]
actor:  0 policy actor:  1  step number:  23 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.39736000000000005 0.6635 0.6635
probs:  [0.06906280501930795, 0.06906280501930795, 0.16911476881496432, 0.06906280501930795, 0.4776639266730787, 0.1460328894540332]
printing an ep nov before normalisation:  66.22713930217338
printing an ep nov before normalisation:  34.269125782745085
maxi score, test score, baseline:  -0.39736000000000005 0.6635 0.6635
probs:  [0.06900453479501654, 0.06900453479501654, 0.16946408473399616, 0.06900453479501654, 0.4772341351883918, 0.14628817569256253]
printing an ep nov before normalisation:  70.77687715283996
maxi score, test score, baseline:  -0.39736000000000005 0.6635 0.6635
actor:  1 policy actor:  1  step number:  44 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.667
from probs:  [0.06900453479501654, 0.06900453479501654, 0.16946408473399616, 0.06900453479501654, 0.4772341351883918, 0.14628817569256253]
actor:  1 policy actor:  1  step number:  20 total reward:  0.73  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  73.23528857596982
actor:  1 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.39736000000000005 0.6635 0.6635
probs:  [0.07177032844419892, 0.07177032844419892, 0.15288381990465166, 0.07177032844419892, 0.4976341692242148, 0.13417102553853694]
actor:  0 policy actor:  0  step number:  26 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.39692000000000005 0.6635 0.6635
actor:  1 policy actor:  1  step number:  27 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  68.78350832694456
Printing some Q and Qe and total Qs values:  [[-0.071]
 [-0.053]
 [-0.071]
 [-0.071]
 [-0.071]
 [-0.071]
 [-0.071]] [[72.449]
 [78.647]
 [72.449]
 [72.449]
 [72.449]
 [72.449]
 [72.449]] [[1.332]
 [1.47 ]
 [1.332]
 [1.332]
 [1.332]
 [1.332]
 [1.332]]
maxi score, test score, baseline:  -0.39692000000000005 0.6635 0.6635
probs:  [0.0717215696850939, 0.0717215696850939, 0.1531761168874545, 0.0717215696850939, 0.49727453272509237, 0.13438464133217146]
actions average: 
K:  0  action  0 :  tensor([0.4598, 0.0017, 0.1113, 0.1243, 0.1006, 0.0787, 0.1238],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0006,     0.9858,     0.0009,     0.0011,     0.0006,     0.0005,
            0.0105], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0690, 0.0957, 0.5417, 0.0957, 0.0562, 0.0629, 0.0788],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1246, 0.0039, 0.1416, 0.3569, 0.1359, 0.1139, 0.1231],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1520, 0.0021, 0.1393, 0.1556, 0.3252, 0.1091, 0.1166],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0900, 0.0134, 0.3383, 0.0792, 0.0612, 0.3220, 0.0958],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1526, 0.0077, 0.1629, 0.1733, 0.1552, 0.1132, 0.2352],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  63.689616316284344
actions average: 
K:  3  action  0 :  tensor([0.4773, 0.0013, 0.1092, 0.1178, 0.1401, 0.0717, 0.0826],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0094, 0.9365, 0.0139, 0.0158, 0.0050, 0.0069, 0.0124],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0614, 0.0063, 0.6012, 0.0824, 0.0626, 0.0953, 0.0908],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1828, 0.0900, 0.1463, 0.2372, 0.1218, 0.0963, 0.1256],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1929, 0.0053, 0.0835, 0.0989, 0.4723, 0.0684, 0.0788],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1072, 0.1186, 0.1421, 0.1562, 0.0969, 0.2852, 0.0939],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1576, 0.0266, 0.1483, 0.2594, 0.1263, 0.1153, 0.1664],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.39692000000000005 0.6635 0.6635
probs:  [0.0717215696850939, 0.0717215696850939, 0.1531761168874545, 0.0717215696850939, 0.49727453272509237, 0.13438464133217146]
printing an ep nov before normalisation:  33.21852207183838
maxi score, test score, baseline:  -0.39692000000000005 0.6635 0.6635
probs:  [0.0717215696850939, 0.0717215696850939, 0.1531761168874545, 0.0717215696850939, 0.49727453272509237, 0.13438464133217146]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  32 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  56.32707413328563
printing an ep nov before normalisation:  73.25547293715762
maxi score, test score, baseline:  -0.39692000000000005 0.6635 0.6635
probs:  [0.0716728820611932, 0.0716728820611932, 0.15346798743189077, 0.0716728820611932, 0.4969154209074081, 0.13459794547712156]
printing an ep nov before normalisation:  61.50394793485085
actor:  1 policy actor:  1  step number:  42 total reward:  0.2699999999999996  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  1  action  0 :  tensor([0.8419, 0.0011, 0.0343, 0.0311, 0.0348, 0.0240, 0.0329],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0045, 0.9518, 0.0050, 0.0113, 0.0024, 0.0020, 0.0230],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0962, 0.0026, 0.5897, 0.0940, 0.0788, 0.0756, 0.0631],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1456, 0.0891, 0.1903, 0.1908, 0.1071, 0.0966, 0.1804],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2397, 0.0013, 0.0728, 0.0831, 0.4601, 0.0610, 0.0820],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1410, 0.0133, 0.1636, 0.1127, 0.1042, 0.3637, 0.1015],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1590, 0.0446, 0.1690, 0.1544, 0.1451, 0.1194, 0.2086],
       grad_fn=<DivBackward0>)
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  64.866191216737
printing an ep nov before normalisation:  56.232443053771064
printing an ep nov before normalisation:  53.06881125225372
Printing some Q and Qe and total Qs values:  [[0.94 ]
 [0.941]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [0.94 ]] [[37.305]
 [32.824]
 [37.305]
 [37.305]
 [37.305]
 [37.305]
 [37.305]] [[0.94 ]
 [0.941]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [0.94 ]]
printing an ep nov before normalisation:  34.989470619825994
siam score:  -0.83181643
actor:  1 policy actor:  1  step number:  38 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  27 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.39392000000000005 0.6635 0.6635
probs:  [0.07164856487631502, 0.07164856487631502, 0.1536137630812431, 0.07164856487631502, 0.4967360613954224, 0.13470448089438938]
printing an ep nov before normalisation:  30.316835641860962
maxi score, test score, baseline:  -0.39392000000000005 0.6635 0.6635
probs:  [0.07164856487631502, 0.07164856487631502, 0.1536137630812431, 0.07164856487631502, 0.4967360613954224, 0.13470448089438938]
printing an ep nov before normalisation:  34.81468677520752
maxi score, test score, baseline:  -0.39392000000000005 0.6635 0.6635
probs:  [0.07164856487631502, 0.07164856487631502, 0.1536137630812431, 0.07164856487631502, 0.4967360613954224, 0.13470448089438938]
maxi score, test score, baseline:  -0.39392000000000005 0.6635 0.6635
probs:  [0.07164856487631502, 0.07164856487631502, 0.1536137630812431, 0.07164856487631502, 0.4967360613954224, 0.13470448089438938]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  28 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
from probs:  [0.07164856487631502, 0.07164856487631502, 0.1536137630812431, 0.07164856487631502, 0.4967360613954224, 0.13470448089438938]
Printing some Q and Qe and total Qs values:  [[-0.069]
 [-0.066]
 [-0.069]
 [-0.07 ]
 [-0.069]
 [-0.069]
 [-0.07 ]] [[30.909]
 [33.876]
 [30.656]
 [31.869]
 [30.656]
 [30.656]
 [32.67 ]] [[0.185]
 [0.237]
 [0.181]
 [0.2  ]
 [0.181]
 [0.181]
 [0.213]]
maxi score, test score, baseline:  -0.39392000000000005 0.6635 0.6635
probs:  [0.07165894126147088, 0.07165894126147088, 0.15363603205459425, 0.07165894126147088, 0.4968081150613478, 0.13457902909964528]
actor:  1 policy actor:  1  step number:  40 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.39392000000000005 0.6635 0.6635
maxi score, test score, baseline:  -0.39392000000000005 0.6635 0.6635
actor:  0 policy actor:  0  step number:  19 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  44.95104692020263
actor:  1 policy actor:  1  step number:  21 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  67.52491692274681
printing an ep nov before normalisation:  58.98480962889036
printing an ep nov before normalisation:  59.164219014818016
maxi score, test score, baseline:  -0.39068 0.6635 0.6635
probs:  [0.07156192136671494, 0.07156192136671494, 0.15421834290086678, 0.07156192136671494, 0.4960924753190832, 0.13500341767990523]
printing an ep nov before normalisation:  48.18768783681922
maxi score, test score, baseline:  -0.39068 0.6635 0.6635
probs:  [0.07156192136671494, 0.07156192136671494, 0.15421834290086678, 0.07156192136671494, 0.4960924753190832, 0.13500341767990523]
actor:  0 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.38754000000000005 0.6635 0.6635
actor:  1 policy actor:  1  step number:  22 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.38754000000000005 0.6635 0.6635
probs:  [0.07157232464430247, 0.07157232464430247, 0.1542407847254592, 0.07157232464430247, 0.49616470961879666, 0.13487753172283665]
printing an ep nov before normalisation:  44.592344508934154
maxi score, test score, baseline:  -0.38754000000000005 0.6635 0.6635
probs:  [0.07157232464430247, 0.07157232464430247, 0.1542407847254592, 0.07157232464430247, 0.49616470961879666, 0.13487753172283665]
maxi score, test score, baseline:  -0.38754000000000005 0.6635 0.6635
probs:  [0.07157232464430247, 0.07157232464430247, 0.1542407847254592, 0.07157232464430247, 0.49616470961879666, 0.13487753172283665]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.38754000000000005 0.6635 0.6635
actor:  0 policy actor:  0  step number:  41 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  42.12847852804271
actor:  1 policy actor:  1  step number:  30 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.108]
 [-0.1  ]
 [-0.117]
 [-0.113]
 [-0.115]
 [-0.115]
 [-0.117]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.108]
 [-0.1  ]
 [-0.117]
 [-0.113]
 [-0.115]
 [-0.115]
 [-0.117]]
printing an ep nov before normalisation:  57.80202895541942
printing an ep nov before normalisation:  73.11829608635185
maxi score, test score, baseline:  -0.3847000000000001 0.6635 0.6635
probs:  [0.0715102099458036, 0.0715102099458036, 0.15469915544983673, 0.0715102099458036, 0.4957019996713608, 0.13506821504139158]
Printing some Q and Qe and total Qs values:  [[ 0.002]
 [ 0.316]
 [-0.03 ]
 [-0.03 ]
 [ 0.069]
 [ 0.069]
 [ 0.069]] [[56.029]
 [52.617]
 [45.982]
 [45.866]
 [51.286]
 [51.286]
 [51.286]] [[1.044]
 [1.24 ]
 [0.666]
 [0.662]
 [0.948]
 [0.948]
 [0.948]]
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.435]
 [0.45 ]
 [0.372]
 [0.38 ]
 [0.38 ]
 [0.41 ]] [[59.377]
 [55.934]
 [60.809]
 [59.65 ]
 [61.582]
 [58.61 ]
 [58.976]] [[1.599]
 [1.477]
 [1.68 ]
 [1.557]
 [1.639]
 [1.525]
 [1.569]]
maxi score, test score, baseline:  -0.3847000000000001 0.6635 0.6635
probs:  [0.0715102099458036, 0.0715102099458036, 0.15469915544983673, 0.0715102099458036, 0.4957019996713608, 0.13506821504139158]
actor:  1 policy actor:  1  step number:  42 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  53.58758329282418
printing an ep nov before normalisation:  41.99607770590921
actor:  1 policy actor:  1  step number:  27 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  70.5786898805408
printing an ep nov before normalisation:  39.74653720855713
maxi score, test score, baseline:  -0.3847000000000001 0.6635 0.6635
probs:  [0.07149647205938164, 0.07149647205938164, 0.15486680333571842, 0.07149647205938164, 0.4955961470005491, 0.13504763348558735]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.3847000000000001 0.6635 0.6635
probs:  [0.07147238435842497, 0.07147238435842497, 0.1550118958318621, 0.07147238435842497, 0.49541844338794927, 0.13515250770491383]
printing an ep nov before normalisation:  76.30510436492624
actor:  0 policy actor:  1  step number:  27 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.3816600000000001 0.6635 0.6635
probs:  [0.07148274240123596, 0.07148274240123596, 0.1550343833399665, 0.07148274240123596, 0.4954903560043818, 0.13502703345194386]
actor:  1 policy actor:  1  step number:  32 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.3816600000000001 0.6635 0.6635
probs:  [0.07145868960641741, 0.07145868960641741, 0.15517943737259182, 0.07145868960641741, 0.49531290079415763, 0.13513159301399852]
maxi score, test score, baseline:  -0.3816600000000001 0.6635 0.6635
probs:  [0.07145868960641741, 0.07145868960641741, 0.15517943737259182, 0.07145868960641741, 0.49531290079415763, 0.13513159301399852]
actor:  1 policy actor:  1  step number:  32 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.3816600000000001 0.6635 0.6635
probs:  [0.07146902094927908, 0.07146902094927908, 0.1552018954891915, 0.07146902094927908, 0.49538462652555726, 0.135006415137414]
Printing some Q and Qe and total Qs values:  [[-0.077]
 [-0.058]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.077]
 [-0.058]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]]
actor:  0 policy actor:  0  step number:  22 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  46.048468840813875
actor:  1 policy actor:  1  step number:  28 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.3784400000000001 0.6635 0.6635
probs:  [0.07146902094927908, 0.07146902094927908, 0.1552018954891915, 0.07146902094927908, 0.49538462652555726, 0.135006415137414]
maxi score, test score, baseline:  -0.3784400000000001 0.6635 0.6635
probs:  [0.07146902094927908, 0.07146902094927908, 0.1552018954891915, 0.07146902094927908, 0.49538462652555726, 0.135006415137414]
Printing some Q and Qe and total Qs values:  [[0.604]
 [0.607]
 [0.632]
 [0.47 ]
 [0.378]
 [0.492]
 [0.65 ]] [[52.525]
 [51.514]
 [48.446]
 [49.859]
 [53.466]
 [53.054]
 [52.822]] [[1.613]
 [1.586]
 [1.522]
 [1.401]
 [1.414]
 [1.516]
 [1.667]]
printing an ep nov before normalisation:  43.726535410821405
actor:  0 policy actor:  0  step number:  28 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  41 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 41.806841135025024
actor:  1 policy actor:  1  step number:  22 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  49.04624990368952
maxi score, test score, baseline:  -0.3756600000000001 0.6635 0.6635
probs:  [0.07139701889995219, 0.07139701889995219, 0.15563662775183446, 0.07139701889995219, 0.49485338708432947, 0.13531892846397953]
maxi score, test score, baseline:  -0.3756600000000001 0.6635 0.6635
probs:  [0.07139701889995219, 0.07139701889995219, 0.15563662775183446, 0.07139701889995219, 0.49485338708432947, 0.13531892846397953]
actions average: 
K:  1  action  0 :  tensor([0.4668, 0.0045, 0.0772, 0.1110, 0.1894, 0.0741, 0.0771],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0014,     0.9684,     0.0032,     0.0093,     0.0004,     0.0007,
            0.0166], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1190, 0.0020, 0.4653, 0.1229, 0.1185, 0.0913, 0.0811],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1742, 0.0041, 0.1284, 0.2530, 0.1730, 0.1235, 0.1438],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1466, 0.0061, 0.1514, 0.1690, 0.2583, 0.1315, 0.1371],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0644, 0.0015, 0.1542, 0.1017, 0.1012, 0.4800, 0.0969],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0994, 0.0958, 0.0880, 0.0817, 0.0783, 0.0576, 0.4993],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.3756600000000001 0.6635 0.6635
probs:  [0.07140735818196216, 0.07140735818196216, 0.155659188907619, 0.07140735818196216, 0.4949251633833378, 0.13519357316315678]
maxi score, test score, baseline:  -0.37566000000000016 0.6635 0.6635
probs:  [0.0713594778363957, 0.0713594778363957, 0.15594861997688875, 0.0713594778363957, 0.4945718786114598, 0.1354010679024643]
maxi score, test score, baseline:  -0.37566000000000016 0.6635 0.6635
probs:  [0.07133556354965344, 0.07133556354965344, 0.15609317903344427, 0.07133556354965344, 0.4943954272254807, 0.13550470309211482]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.37566000000000016 0.6635 0.6635
probs:  [0.07133556354965344, 0.07133556354965344, 0.15609317903344427, 0.07133556354965344, 0.4943954272254807, 0.13550470309211482]
maxi score, test score, baseline:  -0.37566000000000016 0.6635 0.6635
probs:  [0.07133556354965344, 0.07133556354965344, 0.15609317903344427, 0.07133556354965344, 0.4943954272254807, 0.13550470309211482]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  0.0008140188219840638
actor:  1 policy actor:  1  step number:  33 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.37566000000000016 0.6635 0.6635
Printing some Q and Qe and total Qs values:  [[0.579]
 [0.533]
 [0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]] [[53.371]
 [55.953]
 [53.371]
 [53.371]
 [53.371]
 [53.371]
 [53.371]] [[1.831]
 [1.897]
 [1.831]
 [1.831]
 [1.831]
 [1.831]
 [1.831]]
printing an ep nov before normalisation:  37.323784828186035
maxi score, test score, baseline:  -0.37566000000000016 0.6635 0.6635
probs:  [0.0712981676398384, 0.0712981676398384, 0.15640478042564515, 0.0712981676398384, 0.49411496395669924, 0.13558575269814035]
maxi score, test score, baseline:  -0.37566000000000016 0.6635 0.6635
maxi score, test score, baseline:  -0.37566000000000016 0.6635 0.6635
probs:  [0.07127432201780855, 0.07127432201780855, 0.15654909344185516, 0.07127432201780855, 0.49393901024668113, 0.13568893025803816]
actor:  1 policy actor:  1  step number:  41 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.667
from probs:  [0.07127432201780855, 0.07127432201780855, 0.15654909344185516, 0.07127432201780855, 0.49393901024668113, 0.13568893025803816]
maxi score, test score, baseline:  -0.37566000000000016 0.6635 0.6635
probs:  [0.07125049354650508, 0.07125049354650508, 0.15669330266236395, 0.07125049354650508, 0.49376318308961825, 0.13579203360850237]
maxi score, test score, baseline:  -0.37566000000000016 0.6635 0.6635
probs:  [0.07125049354650508, 0.07125049354650508, 0.15669330266236395, 0.07125049354650508, 0.49376318308961825, 0.13579203360850237]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.37566000000000016 0.6635 0.6635
actor:  1 policy actor:  1  step number:  33 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([0.5783, 0.0019, 0.0772, 0.1029, 0.0997, 0.0538, 0.0861],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0101, 0.9291, 0.0194, 0.0089, 0.0019, 0.0019, 0.0286],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0814, 0.0044, 0.5195, 0.0762, 0.0849, 0.1494, 0.0841],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1067, 0.0320, 0.1039, 0.3816, 0.1463, 0.0994, 0.1302],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1806, 0.0041, 0.1134, 0.1229, 0.3597, 0.1005, 0.1189],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1320, 0.0026, 0.2110, 0.1296, 0.1358, 0.2675, 0.1214],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1590, 0.0045, 0.1676, 0.1621, 0.1878, 0.1419, 0.1771],
       grad_fn=<DivBackward0>)
actions average: 
K:  3  action  0 :  tensor([0.3791, 0.0394, 0.1111, 0.1190, 0.1419, 0.0870, 0.1225],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0070, 0.9555, 0.0181, 0.0068, 0.0020, 0.0019, 0.0086],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0663, 0.0436, 0.6300, 0.0809, 0.0598, 0.0578, 0.0615],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1501, 0.0559, 0.1324, 0.2249, 0.1267, 0.0830, 0.2270],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1804, 0.0685, 0.1055, 0.1711, 0.2608, 0.0720, 0.1417],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0701, 0.0644, 0.2306, 0.0838, 0.0702, 0.3930, 0.0879],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1180, 0.1093, 0.1053, 0.0996, 0.1152, 0.0932, 0.3596],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  27 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  42 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  2.0
from probs:  [0.07122668220743143, 0.07122668220743143, 0.1568374081991127, 0.07122668220743143, 0.4935874823490268, 0.13589506282956618]
printing an ep nov before normalisation:  44.25776958465576
maxi score, test score, baseline:  -0.3730400000000001 0.6635 0.6635
probs:  [0.07120288798211757, 0.07120288798211757, 0.1569814101638811, 0.07120288798211757, 0.49341190788861894, 0.1359980180011473]
printing an ep nov before normalisation:  50.35441489197018
printing an ep nov before normalisation:  22.145980051249637
maxi score, test score, baseline:  -0.3730400000000001 0.6635 0.6635
probs:  [0.07117911085212007, 0.07117911085212007, 0.1571253086682883, 0.07117911085212007, 0.493236459572303, 0.1361008992030485]
actor:  0 policy actor:  0  step number:  28 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  21 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.36682000000000015 0.6635 0.6635
probs:  [0.07117911085212007, 0.07117911085212007, 0.1571253086682883, 0.07117911085212007, 0.493236459572303, 0.1361008992030485]
maxi score, test score, baseline:  -0.36682000000000015 0.6635 0.6635
probs:  [0.0711316078044327, 0.0711316078044327, 0.15741279574169526, 0.0711316078044327, 0.4928859408285564, 0.1363064400164502]
printing an ep nov before normalisation:  69.21812646682527
maxi score, test score, baseline:  -0.36682000000000015 0.6635 0.6635
probs:  [0.0711316078044327, 0.0711316078044327, 0.15741279574169526, 0.0711316078044327, 0.4928859408285564, 0.1363064400164502]
printing an ep nov before normalisation:  25.756702423095703
actor:  1 policy actor:  1  step number:  33 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  27 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  27 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.36682000000000015 0.6635 0.6635
probs:  [0.0711078818499882, 0.0711078818499882, 0.1575563845331329, 0.0711078818499882, 0.49271087012991815, 0.1364090997869844]
printing an ep nov before normalisation:  86.64327490875677
maxi score, test score, baseline:  -0.36682000000000015 0.6635 0.6635
actor:  1 policy actor:  1  step number:  30 total reward:  0.5099999999999999  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  34.506540298461914
Printing some Q and Qe and total Qs values:  [[ 0.631]
 [-0.008]
 [ 0.131]
 [ 0.217]
 [ 0.262]
 [-0.022]
 [ 0.205]] [[47.78 ]
 [44.419]
 [39.772]
 [42.17 ]
 [46.218]
 [39.777]
 [42.119]] [[1.782]
 [1.002]
 [0.946]
 [1.133]
 [1.348]
 [0.793]
 [1.118]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.36682000000000015 0.6635 0.6635
probs:  [0.07108417291735071, 0.07108417291735071, 0.15769987030908586, 0.07108417291735071, 0.4925359250329559, 0.13651168590590607]
actions average: 
K:  0  action  0 :  tensor([0.5425, 0.0377, 0.0690, 0.0969, 0.0992, 0.0561, 0.0986],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0011,     0.9789,     0.0021,     0.0024,     0.0001,     0.0003,
            0.0151], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0588, 0.0039, 0.6832, 0.0668, 0.0545, 0.0857, 0.0470],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1287, 0.0668, 0.1364, 0.2501, 0.1605, 0.1168, 0.1408],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1583, 0.0005, 0.1284, 0.1478, 0.3380, 0.1096, 0.1174],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1474, 0.0036, 0.1471, 0.1529, 0.1242, 0.3207, 0.1042],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1497, 0.0042, 0.1322, 0.1752, 0.1381, 0.1352, 0.2654],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  28 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  28 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  81.05335787849502
maxi score, test score, baseline:  -0.36682000000000015 0.6635 0.6635
probs:  [0.07103680604427702, 0.07103680604427702, 0.1579865332576615, 0.07103680604427702, 0.4921864111037811, 0.13671663750572635]
Printing some Q and Qe and total Qs values:  [[-0.073]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.043]
 [-0.073]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [64.645]
 [ 0.   ]] [[-0.928]
 [-0.928]
 [-0.928]
 [-0.928]
 [-0.928]
 [ 0.983]
 [-0.928]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  65.13370227187546
actor:  1 policy actor:  1  step number:  24 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  62.17779619580051
maxi score, test score, baseline:  -0.36682000000000015 0.6635 0.6635
probs:  [0.07098950703903473, 0.07098950703903473, 0.15827278547208065, 0.07098950703903473, 0.49183739796241155, 0.13692129544840353]
printing an ep nov before normalisation:  51.010503378710986
printing an ep nov before normalisation:  64.60741582433272
Printing some Q and Qe and total Qs values:  [[-0.109]
 [-0.109]
 [-0.109]
 [-0.109]
 [-0.109]
 [-0.109]
 [-0.109]] [[62.26]
 [62.26]
 [62.26]
 [62.26]
 [62.26]
 [62.26]
 [62.26]] [[1.206]
 [1.206]
 [1.206]
 [1.206]
 [1.206]
 [1.206]
 [1.206]]
maxi score, test score, baseline:  -0.36682000000000015 0.6635 0.6635
probs:  [0.07096588294128457, 0.07096588294128457, 0.15841575782974443, 0.07096588294128457, 0.49166307885092986, 0.1370235144954722]
Printing some Q and Qe and total Qs values:  [[0.148]
 [0.148]
 [0.595]
 [0.148]
 [0.148]
 [0.148]
 [0.148]] [[60.378]
 [60.378]
 [60.37 ]
 [60.378]
 [60.378]
 [60.378]
 [60.378]] [[1.659]
 [1.659]
 [2.105]
 [1.659]
 [1.659]
 [1.659]
 [1.659]]
printing an ep nov before normalisation:  63.56142181297901
actor:  1 policy actor:  1  step number:  24 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.36682000000000015 0.6635 0.6635
maxi score, test score, baseline:  -0.36682000000000015 0.6635 0.6635
probs:  [0.07094227575586545, 0.07094227575586545, 0.15855862783446872, 0.07094227575586545, 0.4914888845333156, 0.13712566036461935]
printing an ep nov before normalisation:  4.795643917532289e-05
actor:  1 policy actor:  1  step number:  27 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.36682000000000015 0.6635 0.6635
probs:  [0.0709292908260747, 0.0709292908260747, 0.15872515291543796, 0.0709292908260747, 0.4913884056720134, 0.13709856893432457]
maxi score, test score, baseline:  -0.36682000000000015 0.6635 0.6635
probs:  [0.07090573395930945, 0.07090573395930945, 0.15886788495628337, 0.07090573395930945, 0.49121457381386524, 0.1372003393519229]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.36682000000000015 0.6635 0.6635
printing an ep nov before normalisation:  54.521119594573975
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.36682000000000015 0.6635 0.6635
printing an ep nov before normalisation:  31.206135749816895
Printing some Q and Qe and total Qs values:  [[0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]] [[51.251]
 [51.251]
 [51.251]
 [51.251]
 [51.251]
 [51.251]
 [51.251]] [[0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]]
maxi score, test score, baseline:  -0.36682000000000015 0.6635 0.6635
probs:  [0.07088219392530186, 0.07088219392530186, 0.15901051500674676, 0.07088219392530186, 0.49104086616873593, 0.13730203704861177]
actor:  1 policy actor:  1  step number:  35 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  35 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.36682000000000015 0.6635 0.6635
probs:  [0.07086928083172399, 0.07086928083172399, 0.15917689909199642, 0.07086928083172399, 0.4909409017953088, 0.13727435661752282]
actor:  0 policy actor:  0  step number:  22 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  51.490000640710136
printing an ep nov before normalisation:  60.559921015826696
maxi score, test score, baseline:  -0.36364000000000013 0.6635 0.6635
probs:  [0.07084579081887508, 0.07084579081887508, 0.15931939173561702, 0.07084579081887508, 0.49076755448042325, 0.1373756813273345]
actor:  1 policy actor:  1  step number:  33 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.36364000000000013 0.6635 0.6635
maxi score, test score, baseline:  -0.36364000000000013 0.6635 0.6635
probs:  [0.07082231755981024, 0.07082231755981024, 0.15946178274919598, 0.07082231755981024, 0.4905943308020592, 0.13747693376931405]
actor:  1 policy actor:  1  step number:  33 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 61.010294269177166
maxi score, test score, baseline:  -0.36364000000000013 0.6635 0.6635
probs:  [0.07079886103661186, 0.07079886103661186, 0.15960407224142326, 0.07079886103661186, 0.4904212306279918, 0.13757811402074935]
Printing some Q and Qe and total Qs values:  [[-0.084]
 [-0.067]
 [-0.084]
 [-0.095]
 [-0.084]
 [-0.084]
 [-0.097]] [[42.98 ]
 [61.653]
 [42.98 ]
 [55.088]
 [42.98 ]
 [42.98 ]
 [53.621]] [[0.131]
 [0.361]
 [0.131]
 [0.259]
 [0.131]
 [0.131]
 [0.239]]
actor:  0 policy actor:  1  step number:  25 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  74.70609286248593
siam score:  -0.83094907
maxi score, test score, baseline:  -0.3607200000000001 0.6635 0.6635
probs:  [0.07077542123138796, 0.07077542123138796, 0.15974626032083336, 0.07077542123138796, 0.490248253826185, 0.13767922215881775]
actor:  1 policy actor:  1  step number:  29 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.3607200000000001 0.6635 0.6635
probs:  [0.07077542123138796, 0.07077542123138796, 0.15974626032083336, 0.07077542123138796, 0.490248253826185, 0.13767922215881775]
actor:  1 policy actor:  1  step number:  21 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.3607200000000001 0.6635 0.6635
probs:  [0.07089998991521182, 0.07089998991521182, 0.15899061488187316, 0.07089998991521182, 0.49116752309174894, 0.1371418922807425]
maxi score, test score, baseline:  -0.3607200000000001 0.6635 0.6635
probs:  [0.07089998991521182, 0.07089998991521182, 0.15899061488187316, 0.07089998991521182, 0.49116752309174894, 0.1371418922807425]
actor:  1 policy actor:  1  step number:  20 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.3607200000000001 0.6635 0.6635
probs:  [0.07083672161559056, 0.07173004671080595, 0.15884859006146315, 0.07083672161559056, 0.4907285189096095, 0.13701940108694025]
maxi score, test score, baseline:  -0.3607200000000001 0.6635 0.6635
probs:  [0.07083672161559056, 0.07173004671080595, 0.15884859006146315, 0.07083672161559056, 0.4907285189096095, 0.13701940108694025]
maxi score, test score, baseline:  -0.3607200000000001 0.6635 0.6635
probs:  [0.07083672161559056, 0.07173004671080595, 0.15884859006146315, 0.07083672161559056, 0.4907285189096095, 0.13701940108694025]
maxi score, test score, baseline:  -0.3607200000000001 0.6635 0.6635
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.539]
 [0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]] [[46.839]
 [52.265]
 [46.839]
 [46.839]
 [46.839]
 [46.839]
 [46.839]] [[0.62 ]
 [0.779]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]]
maxi score, test score, baseline:  -0.3607200000000001 0.6635 0.6635
maxi score, test score, baseline:  -0.3607200000000001 0.6635 0.6635
probs:  [0.07084724698046599, 0.07174070506164776, 0.15887221742406243, 0.07084724698046599, 0.4908015519961081, 0.13689103155724977]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
from probs:  [0.07084724698046599, 0.07174070506164776, 0.15887221742406243, 0.07084724698046599, 0.4908015519961081, 0.13689103155724977]
maxi score, test score, baseline:  -0.3607200000000001 0.6635 0.6635
probs:  [0.07082398227561477, 0.07171910066224227, 0.1590125287625408, 0.07082398227561477, 0.4906299105763604, 0.13699049544762695]
maxi score, test score, baseline:  -0.3607200000000001 0.6635 0.6635
probs:  [0.07082398227561477, 0.07171910066224227, 0.1590125287625408, 0.07082398227561477, 0.4906299105763604, 0.13699049544762695]
maxi score, test score, baseline:  -0.3607200000000001 0.6635 0.6635
probs:  [0.07082398227561477, 0.07171910066224227, 0.1590125287625408, 0.07082398227561477, 0.4906299105763604, 0.13699049544762695]
actor:  1 policy actor:  1  step number:  41 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.333
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.554]
 [0.691]
 [0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.554]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.554]
 [0.691]
 [0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.554]]
Printing some Q and Qe and total Qs values:  [[0.663]
 [0.729]
 [0.663]
 [0.663]
 [0.663]
 [0.663]
 [0.663]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.663]
 [0.729]
 [0.663]
 [0.663]
 [0.663]
 [0.663]
 [0.663]]
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.67 ]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.536]
 [0.67 ]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]]
printing an ep nov before normalisation:  47.03336018248959
maxi score, test score, baseline:  -0.3607200000000001 0.6635 0.6635
probs:  [0.07080073400455757, 0.07169751152381947, 0.15915274098746265, 0.07080073400455757, 0.49045839040120853, 0.13708988907839426]
printing an ep nov before normalisation:  40.79296980897281
printing an ep nov before normalisation:  62.86514286183472
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.878]
 [0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]] [[38.745]
 [50.937]
 [38.745]
 [38.745]
 [38.745]
 [38.745]
 [38.745]] [[0.681]
 [0.878]
 [0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.681]]
printing an ep nov before normalisation:  62.13261909469788
maxi score, test score, baseline:  -0.3607200000000001 0.6635 0.6635
probs:  [0.07080073400455757, 0.07169751152381947, 0.15915274098746265, 0.07080073400455757, 0.49045839040120853, 0.13708988907839426]
printing an ep nov before normalisation:  61.513682752707744
actor:  1 policy actor:  1  step number:  33 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  64.31501102603842
printing an ep nov before normalisation:  50.633460210882404
printing an ep nov before normalisation:  53.21071623565082
printing an ep nov before normalisation:  2.2298936553966087
actor:  0 policy actor:  0  step number:  18 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  18 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  18 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  18 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  18 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  18 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  18 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  18 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  19 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  19 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  19 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  19 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  19 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  19 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  19 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  19 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  19 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  2
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  20 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  20 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  21 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[0.082]
 [0.263]
 [0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]] [[66.886]
 [54.638]
 [62.034]
 [62.034]
 [62.034]
 [62.034]
 [62.034]] [[0.741]
 [0.761]
 [0.717]
 [0.717]
 [0.717]
 [0.717]
 [0.717]]
from probs:  [0.06069156173996033, 0.0786948734390085, 0.17254051692666822, 0.07116434562624802, 0.4226731818834408, 0.19423552038467407]
printing an ep nov before normalisation:  48.19146323748185
maxi score, test score, baseline:  -0.29560000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.06065556925238583, 0.07868745785793986, 0.17268206383787643, 0.07114497673387518, 0.422418428241973, 0.19441150407594962]
actions average: 
K:  3  action  0 :  tensor([0.6752, 0.0517, 0.0599, 0.0543, 0.0446, 0.0544, 0.0600],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0036, 0.9622, 0.0049, 0.0068, 0.0013, 0.0021, 0.0190],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0795, 0.0422, 0.5967, 0.0806, 0.0586, 0.0820, 0.0603],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2265, 0.0226, 0.1459, 0.2102, 0.1441, 0.1255, 0.1252],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2508, 0.0067, 0.1331, 0.1196, 0.2955, 0.0925, 0.1018],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0997, 0.0549, 0.1710, 0.1345, 0.0871, 0.3425, 0.1103],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1150, 0.1522, 0.0975, 0.1891, 0.0771, 0.0819, 0.2873],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.29560000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.06065556925238583, 0.07868745785793986, 0.17268206383787643, 0.07114497673387518, 0.422418428241973, 0.19441150407594962]
actor:  1 policy actor:  1  step number:  31 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  53.02692423136179
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.396]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]] [[53.523]
 [48.345]
 [53.523]
 [53.523]
 [53.523]
 [53.523]
 [53.523]] [[1.592]
 [1.498]
 [1.592]
 [1.592]
 [1.592]
 [1.592]
 [1.592]]
printing an ep nov before normalisation:  34.949469566345215
printing an ep nov before normalisation:  52.41677888653311
printing an ep nov before normalisation:  42.338251007106926
actor:  1 policy actor:  1  step number:  27 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.333
using another actor
actor:  0 policy actor:  0  step number:  26 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.2927000000000001 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.2927000000000001 0.6279999999999999 0.6279999999999999
probs:  [0.060677797725959416, 0.07867717994043141, 0.17274543708330714, 0.07114254343367113, 0.42257352394802183, 0.19418351786860902]
printing an ep nov before normalisation:  48.3887733337837
actor:  1 policy actor:  1  step number:  25 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.2927000000000001 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.2927000000000001 0.6279999999999999 0.6279999999999999
probs:  [0.06064423380239859, 0.07863379857683743, 0.17289371211566573, 0.07112595942013931, 0.4223357181358785, 0.19436657794908035]
maxi score, test score, baseline:  -0.2927000000000001 0.6279999999999999 0.6279999999999999
probs:  [0.06060835242448133, 0.07862632484147813, 0.17303508589079003, 0.07110662992207424, 0.42208174701159085, 0.19454185990958545]
printing an ep nov before normalisation:  56.78455811780321
line 256 mcts: sample exp_bonus 29.431215163036427
maxi score, test score, baseline:  -0.2927000000000001 0.6279999999999999 0.6279999999999999
probs:  [0.06060835242448133, 0.07862632484147813, 0.17303508589079003, 0.07110662992207424, 0.42208174701159085, 0.19454185990958545]
printing an ep nov before normalisation:  50.935235023498535
actor:  1 policy actor:  1  step number:  29 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
using another actor
from probs:  [0.06060835242448133, 0.07862632484147813, 0.17303508589079003, 0.07110662992207424, 0.42208174701159085, 0.19454185990958545]
maxi score, test score, baseline:  -0.2927000000000001 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.2927000000000001 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.2927000000000001 0.6279999999999999 0.6279999999999999
probs:  [0.060592990635838735, 0.07860646022091594, 0.17323491487746828, 0.07111136239753228, 0.4219709488023393, 0.19448332306590563]
maxi score, test score, baseline:  -0.2927000000000001 0.6279999999999999 0.6279999999999999
probs:  [0.060592990635838735, 0.07860646022091594, 0.17323491487746828, 0.07111136239753228, 0.4219709488023393, 0.19448332306590563]
from probs:  [0.060592990635838735, 0.07860646022091594, 0.17323491487746828, 0.07111136239753228, 0.4219709488023393, 0.19448332306590563]
maxi score, test score, baseline:  -0.2927000000000001 0.6279999999999999 0.6279999999999999
probs:  [0.060592990635838735, 0.07860646022091594, 0.17323491487746828, 0.07111136239753228, 0.4219709488023393, 0.19448332306590563]
printing an ep nov before normalisation:  48.473821367536274
actor:  1 policy actor:  1  step number:  28 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  33 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.2927000000000001 0.6279999999999999 0.6279999999999999
probs:  [0.060597064753759144, 0.0785729234386486, 0.173246579439887, 0.07108763830896751, 0.4219993745935186, 0.19449641946521917]
line 256 mcts: sample exp_bonus 36.2528828153915
actor:  1 policy actor:  1  step number:  26 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  63.36780171135146
actor:  1 policy actor:  1  step number:  22 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  57.60996673697903
printing an ep nov before normalisation:  80.43156122981817
printing an ep nov before normalisation:  70.71969985961914
printing an ep nov before normalisation:  49.841867737170276
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  -0.2927000000000001 0.6279999999999999 0.6279999999999999
actor:  1 policy actor:  1  step number:  25 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.2927000000000001 0.6279999999999999 0.6279999999999999
probs:  [0.060597361858659, 0.07861220678275632, 0.17349119204610416, 0.07111068748469855, 0.4219978317909844, 0.1941907200367976]
maxi score, test score, baseline:  -0.2927000000000001 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.2927000000000001 0.6279999999999999 0.6279999999999999
probs:  [0.06052601915651341, 0.07859730974194135, 0.17377357800118143, 0.07107228603851423, 0.42149284352878635, 0.1945379635330632]
actor:  0 policy actor:  0  step number:  30 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
from probs:  [0.06052601915651341, 0.07859730974194135, 0.17377357800118143, 0.07107228603851423, 0.42149284352878635, 0.1945379635330632]
actor:  0 policy actor:  0  step number:  25 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  25 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.2867600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.06809278586256111, 0.07036867481040865, 0.1713926113278318, 0.06238131061961598, 0.4343318483090407, 0.19343276907054177]
from probs:  [0.06809278586256111, 0.07036867481040865, 0.1713926113278318, 0.06238131061961598, 0.4343318483090407, 0.19343276907054177]
maxi score, test score, baseline:  -0.2867600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.06815997873472301, 0.07043576213993685, 0.17145501374848368, 0.06238842488197082, 0.434382057120854, 0.19317876337403178]
Printing some Q and Qe and total Qs values:  [[-0.029]
 [ 0.009]
 [-0.029]
 [-0.029]
 [-0.029]
 [-0.029]
 [-0.029]] [[39.232]
 [63.23 ]
 [39.232]
 [39.232]
 [39.232]
 [39.232]
 [39.232]] [[0.548]
 [1.16 ]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]]
printing an ep nov before normalisation:  59.95122434471016
actor:  1 policy actor:  1  step number:  25 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  0.00023703202853337442
actor:  1 policy actor:  1  step number:  31 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
siam score:  -0.8368574
printing an ep nov before normalisation:  36.23791908947667
maxi score, test score, baseline:  -0.2867600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.06805875739708028, 0.07042149645339646, 0.17196312842197242, 0.062302462453124756, 0.43377055579150864, 0.1934835994829175]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  22 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  60.92594257359975
actor:  1 policy actor:  1  step number:  18 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.2867600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.06851524661581959, 0.07081051739660357, 0.1694526168140412, 0.06289425585828311, 0.43796879487576595, 0.1903585684394865]
printing an ep nov before normalisation:  57.12654624727551
printing an ep nov before normalisation:  40.16345024108887
actor:  1 policy actor:  1  step number:  30 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.2867600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.06336999439879025, 0.0657175088920745, 0.1666048464424251, 0.07508815766414213, 0.441232844567275, 0.18798664803529316]
siam score:  -0.83606017
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.553]
 [0.472]
 [0.523]
 [0.553]
 [0.595]
 [0.614]] [[30.204]
 [30.204]
 [30.826]
 [32.385]
 [30.204]
 [33.534]
 [33.192]] [[1.63 ]
 [1.63 ]
 [1.599]
 [1.773]
 [1.63 ]
 [1.936]
 [1.928]]
maxi score, test score, baseline:  -0.2867600000000001 0.6279999999999999 0.6279999999999999
actor:  0 policy actor:  1  step number:  24 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  34 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.79074848418082
maxi score, test score, baseline:  -0.2836600000000002 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.2836600000000002 0.6279999999999999 0.6279999999999999
probs:  [0.06330286080175834, 0.06565825646621684, 0.16688429709117059, 0.07506036476162205, 0.44075633834088723, 0.1883378825383448]
Printing some Q and Qe and total Qs values:  [[0.612]
 [0.679]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]] [[46.303]
 [48.6  ]
 [46.303]
 [46.303]
 [46.303]
 [46.303]
 [46.303]] [[1.442]
 [1.591]
 [1.442]
 [1.442]
 [1.442]
 [1.442]
 [1.442]]
printing an ep nov before normalisation:  59.588848085113604
Printing some Q and Qe and total Qs values:  [[0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.229]] [[53.256]
 [53.256]
 [53.256]
 [53.256]
 [53.256]
 [53.256]
 [53.256]] [[1.229]
 [1.229]
 [1.229]
 [1.229]
 [1.229]
 [1.229]
 [1.229]]
maxi score, test score, baseline:  -0.2836600000000002 0.6279999999999999 0.6279999999999999
probs:  [0.06330286080175834, 0.06565825646621684, 0.16688429709117059, 0.07506036476162205, 0.44075633834088723, 0.1883378825383448]
actor:  1 policy actor:  1  step number:  31 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.333
from probs:  [0.06330286080175834, 0.06565825646621684, 0.16688429709117059, 0.07506036476162205, 0.44075633834088723, 0.1883378825383448]
maxi score, test score, baseline:  -0.2836600000000002 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.2836600000000002 0.6279999999999999 0.6279999999999999
probs:  [0.06331248723002023, 0.0656682418335122, 0.16690970830718216, 0.07491951460964381, 0.44082348495077417, 0.18836656306886754]
printing an ep nov before normalisation:  47.62516590834529
siam score:  -0.8327244
actor:  0 policy actor:  1  step number:  36 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.55882555444478
from probs:  [0.06331248723002023, 0.0656682418335122, 0.16690970830718216, 0.07491951460964381, 0.44082348495077417, 0.18836656306886754]
maxi score, test score, baseline:  -0.2806400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.06331248723002023, 0.0656682418335122, 0.16690970830718216, 0.07491951460964381, 0.44082348495077417, 0.18836656306886754]
printing an ep nov before normalisation:  56.96382522583008
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  13.489243557158511
maxi score, test score, baseline:  -0.28064000000000006 0.6279999999999999 0.6279999999999999
probs:  [0.06331248723002023, 0.0656682418335122, 0.16690970830718216, 0.07491951460964381, 0.44082348495077417, 0.18836656306886754]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.253]
 [0.034]
 [0.005]
 [0.034]
 [0.034]
 [0.012]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.034]
 [0.253]
 [0.034]
 [0.005]
 [0.034]
 [0.034]
 [0.012]]
printing an ep nov before normalisation:  41.70271150480853
Printing some Q and Qe and total Qs values:  [[0.397]
 [0.611]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]] [[39.721]
 [42.73 ]
 [39.721]
 [39.721]
 [39.721]
 [39.721]
 [39.721]] [[1.039]
 [1.342]
 [1.039]
 [1.039]
 [1.039]
 [1.039]
 [1.039]]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  65.48775067893615
actor:  0 policy actor:  0  step number:  32 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  22 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.667
from probs:  [0.06331248723002023, 0.0656682418335122, 0.16690970830718216, 0.07491951460964381, 0.44082348495077417, 0.18836656306886754]
maxi score, test score, baseline:  -0.2779400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.06339787295432583, 0.06574359733207742, 0.16655400245431629, 0.07495548045608084, 0.44142954761600967, 0.18791949918719]
maxi score, test score, baseline:  -0.2779400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.06339787295432583, 0.06574359733207742, 0.16655400245431629, 0.07495548045608084, 0.44142954761600967, 0.18791949918719]
printing an ep nov before normalisation:  47.04256646307826
maxi score, test score, baseline:  -0.2779400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.06339787295432583, 0.06574359733207742, 0.16655400245431629, 0.07495548045608084, 0.44142954761600967, 0.18791949918719]
line 256 mcts: sample exp_bonus 22.051028143917637
printing an ep nov before normalisation:  38.266563415527344
actor:  1 policy actor:  1  step number:  38 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.2779400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.0633313718251211, 0.06568490806303241, 0.16683103753333778, 0.0749274691061705, 0.44095752665542454, 0.18826768681691364]
actor:  1 policy actor:  1  step number:  25 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.2779400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.06335117467394975, 0.06570544838164413, 0.16688327152759738, 0.07495090553888356, 0.4410956569274583, 0.18801354295046696]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.2779400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.06337090261778405, 0.06572591100575516, 0.16693530794487813, 0.0749742533224393, 0.44123326471672514, 0.1877603603924182]
maxi score, test score, baseline:  -0.2779400000000001 0.6279999999999999 0.6279999999999999
printing an ep nov before normalisation:  2.87851544622697
actor:  0 policy actor:  0  step number:  58 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.27814000000000005 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.27814000000000005 0.6279999999999999 0.6279999999999999
probs:  [0.06337351820701398, 0.06568729581505996, 0.1669422070872189, 0.07497734884074184, 0.44125150916503586, 0.1877681208849295]
printing an ep nov before normalisation:  52.063382476734866
maxi score, test score, baseline:  -0.27814000000000005 0.6279999999999999 0.6279999999999999
probs:  [0.06337351820701398, 0.06568729581505996, 0.1669422070872189, 0.07497734884074184, 0.44125150916503586, 0.1877681208849295]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.27814000000000005 0.6279999999999999 0.6279999999999999
probs:  [0.06334035950197899, 0.0656579745206519, 0.16708081752108211, 0.07496343514129133, 0.44101614212398393, 0.1879412711910117]
printing an ep nov before normalisation:  76.11976853950881
actor:  1 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.27814000000000005 0.6279999999999999 0.6279999999999999
actor:  1 policy actor:  1  step number:  25 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.27814000000000005 0.6279999999999999 0.6279999999999999
probs:  [0.06327199963491778, 0.06559215406903494, 0.16712612628283213, 0.07490781070161995, 0.440535245517254, 0.18856666379434117]
printing an ep nov before normalisation:  30.320228470696343
Printing some Q and Qe and total Qs values:  [[0.901]
 [0.96 ]
 [0.901]
 [0.901]
 [0.901]
 [0.901]
 [0.901]] [[57.273]
 [52.982]
 [57.273]
 [57.273]
 [57.273]
 [57.273]
 [57.273]] [[0.901]
 [0.96 ]
 [0.901]
 [0.901]
 [0.901]
 [0.901]
 [0.901]]
actor:  1 policy actor:  1  step number:  34 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.843]
 [0.829]
 [0.858]
 [0.852]
 [0.852]
 [0.893]
 [0.903]] [[31.585]
 [32.34 ]
 [15.156]
 [27.801]
 [27.801]
 [30.284]
 [15.54 ]] [[0.843]
 [0.829]
 [0.858]
 [0.852]
 [0.852]
 [0.893]
 [0.903]]
actor:  0 policy actor:  0  step number:  33 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.005]
 [1.008]
 [0.974]
 [0.915]
 [0.914]
 [0.925]
 [0.974]] [[36.419]
 [39.465]
 [33.496]
 [37.759]
 [40.769]
 [37.964]
 [33.496]] [[1.005]
 [1.008]
 [0.974]
 [0.915]
 [0.914]
 [0.925]
 [0.974]]
maxi score, test score, baseline:  -0.2754200000000001 0.6279999999999999 0.6279999999999999
probs:  [0.0632561370146137, 0.0656497106966582, 0.16713833032092953, 0.07496120626726353, 0.440425324799598, 0.188569290900937]
actor:  0 policy actor:  1  step number:  27 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  29 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  52.93322294867938
actor:  1 policy actor:  1  step number:  19 total reward:  0.72  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.2725400000000001 0.6279999999999999 0.6279999999999999
actor:  0 policy actor:  0  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]] [[60.642]
 [60.642]
 [60.642]
 [60.642]
 [60.642]
 [60.642]
 [60.642]] [[1.408]
 [1.408]
 [1.408]
 [1.408]
 [1.408]
 [1.408]
 [1.408]]
maxi score, test score, baseline:  -0.26934000000000013 0.6279999999999999 0.6279999999999999
probs:  [0.05916635138581953, 0.06147843353001, 0.1565794678799792, 0.07020387351912526, 0.4118953017242483, 0.24067657196081768]
printing an ep nov before normalisation:  40.469070368757755
maxi score, test score, baseline:  -0.26934000000000013 0.6279999999999999 0.6279999999999999
actor:  0 policy actor:  0  step number:  22 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  38 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[ 0.338]
 [ 0.156]
 [ 0.338]
 [-0.02 ]
 [-0.026]
 [ 0.338]
 [ 0.338]] [[47.69 ]
 [50.99 ]
 [47.69 ]
 [47.189]
 [46.866]
 [47.69 ]
 [47.69 ]] [[1.775]
 [1.781]
 [1.775]
 [1.388]
 [1.364]
 [1.775]
 [1.775]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  40 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  30 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  60.19258231321973
printing an ep nov before normalisation:  54.25288803035024
maxi score, test score, baseline:  -0.2662800000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05911738838105054, 0.06143757974746838, 0.1568721642801336, 0.07019362268556699, 0.41154617677992006, 0.24083306812586047]
actor:  1 policy actor:  1  step number:  29 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
using another actor
actor:  1 policy actor:  1  step number:  23 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.333
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0985],
        [-0.6371],
        [-0.2381],
        [-0.0781],
        [ 0.8553],
        [-0.0000],
        [-0.2682],
        [ 0.8065],
        [-0.4324],
        [-0.3974]], dtype=torch.float64)
-0.125759551797 -0.22426412604215304
-0.048519850599000006 -0.685651091361611
-0.048519850599000006 -0.28662983935271724
-0.145559551797 -0.22361687803338415
-0.126539750799 0.7287348972574985
-0.7114271570999999 -0.7114271570999999
-0.048519850599000006 -0.31673143354900146
-0.106157551797 0.7003574328757959
-0.048519850599000006 -0.4809340756174286
-0.107327830599 -0.504719249952759
maxi score, test score, baseline:  -0.2662800000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05852679413469643, 0.06082871845667282, 0.15551193919144574, 0.07889770246162107, 0.4074230333489212, 0.2388118124066427]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  43.99514198303223
actor:  0 policy actor:  1  step number:  21 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8240094
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  20 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[ 0.324]
 [ 0.33 ]
 [ 0.235]
 [ 0.051]
 [-0.011]
 [-0.009]
 [-0.037]] [[46.841]
 [52.291]
 [45.32 ]
 [35.162]
 [38.275]
 [36.888]
 [39.905]] [[1.268]
 [1.451]
 [1.13 ]
 [0.617]
 [0.656]
 [0.613]
 [0.683]]
maxi score, test score, baseline:  -0.26296000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.05849479972969153, 0.060800425374651475, 0.15563588968133749, 0.07880306994116058, 0.40719611238028963, 0.2390697028928693]
printing an ep nov before normalisation:  39.611377827906495
printing an ep nov before normalisation:  71.89239934096793
actor:  1 policy actor:  1  step number:  28 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  20 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  57.997805389575625
printing an ep nov before normalisation:  54.597765651098406
actor:  1 policy actor:  1  step number:  33 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  2.0
deleting a thread, now have 2 threads
Frames:  82366 train batches done:  9652 episodes:  3476
printing an ep nov before normalisation:  40.540832051016196
actions average: 
K:  1  action  0 :  tensor([0.5910, 0.0036, 0.0661, 0.0762, 0.1269, 0.0477, 0.0885],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0031, 0.9458, 0.0046, 0.0088, 0.0021, 0.0028, 0.0328],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0887, 0.0226, 0.5791, 0.0810, 0.0819, 0.0749, 0.0718],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0522, 0.1262, 0.0465, 0.5232, 0.0611, 0.0423, 0.1485],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2110, 0.0017, 0.1081, 0.1459, 0.3372, 0.0923, 0.1038],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1074, 0.0021, 0.1266, 0.1213, 0.1168, 0.3893, 0.1366],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1495, 0.0978, 0.1196, 0.1294, 0.1117, 0.0891, 0.3029],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  26 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  27 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.26296000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.05810591496685776, 0.06042577140699514, 0.15584657968357726, 0.07853953199445945, 0.4044612617689575, 0.24262094017915278]
maxi score, test score, baseline:  -0.26296000000000014 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.26296000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.05810591496685776, 0.06042577140699514, 0.15584657968357726, 0.07853953199445945, 0.4044612617689575, 0.24262094017915278]
siam score:  -0.82144
actions average: 
K:  2  action  0 :  tensor([0.6089, 0.0019, 0.0607, 0.0593, 0.1431, 0.0529, 0.0734],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0096, 0.9130, 0.0121, 0.0341, 0.0023, 0.0042, 0.0247],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0788, 0.0025, 0.5770, 0.0879, 0.0851, 0.0769, 0.0919],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1121, 0.1032, 0.1118, 0.3841, 0.0948, 0.0893, 0.1046],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1653, 0.0183, 0.1538, 0.1600, 0.2404, 0.1267, 0.1356],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1134, 0.0023, 0.1625, 0.1571, 0.1183, 0.3160, 0.1304],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0474, 0.1207, 0.0957, 0.0951, 0.0374, 0.0966, 0.5070],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.26296000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.05810591496685776, 0.06042577140699514, 0.15584657968357726, 0.07853953199445945, 0.4044612617689575, 0.24262094017915278]
actor:  0 policy actor:  0  step number:  31 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  62.702827273019665
maxi score, test score, baseline:  -0.26032000000000016 0.6279999999999999 0.6279999999999999
probs:  [0.05810591496685776, 0.06042577140699514, 0.15584657968357726, 0.07853953199445945, 0.4044612617689575, 0.24262094017915278]
actor:  0 policy actor:  0  step number:  23 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.2572400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05810591496685776, 0.06042577140699514, 0.15584657968357726, 0.07853953199445945, 0.4044612617689575, 0.24262094017915278]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.2572400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.058093397624706264, 0.06048105274776164, 0.15586250199843044, 0.07858734179955953, 0.4043746358881778, 0.2426010699413643]
printing an ep nov before normalisation:  45.586093003995785
actions average: 
K:  2  action  0 :  tensor([0.5209, 0.0199, 0.0893, 0.0881, 0.1247, 0.0487, 0.1085],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0045, 0.9517, 0.0084, 0.0105, 0.0023, 0.0063, 0.0162],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0784, 0.0212, 0.5238, 0.1153, 0.0918, 0.0958, 0.0737],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1112, 0.0672, 0.1058, 0.3310, 0.1194, 0.0982, 0.1672],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1893, 0.0035, 0.1316, 0.1275, 0.3608, 0.1138, 0.0735],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1436, 0.0058, 0.2140, 0.1369, 0.1446, 0.2077, 0.1475],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1017, 0.1817, 0.1131, 0.1344, 0.0902, 0.0818, 0.2970],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.149]
 [0.149]
 [0.581]
 [0.149]
 [0.149]
 [0.149]
 [0.149]] [[45.169]
 [45.169]
 [45.982]
 [45.169]
 [45.169]
 [45.169]
 [45.169]] [[1.31 ]
 [1.31 ]
 [1.779]
 [1.31 ]
 [1.31 ]
 [1.31 ]
 [1.31 ]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  0.012464180963434046
actor:  0 policy actor:  1  step number:  25 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.2542400000000001 0.6279999999999999 0.6279999999999999
actor:  1 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.2542400000000001 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.2542400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.058118542679616504, 0.06050723364708488, 0.1559300625511038, 0.07862137781086011, 0.40455000929161433, 0.2422727740197203]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.2542400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05808115529615561, 0.060473383435429705, 0.15603751439342542, 0.07861435101039252, 0.40428551377971406, 0.2425080820848826]
actor:  1 policy actor:  1  step number:  23 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  57.73980981931095
maxi score, test score, baseline:  -0.2542400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05841012652687591, 0.0607712311251457, 0.15509204665699813, 0.07867617976073221, 0.40661280726792626, 0.24043760866232175]
printing an ep nov before normalisation:  45.26784896850586
maxi score, test score, baseline:  -0.2542400000000001 0.6279999999999999 0.6279999999999999
actor:  1 policy actor:  1  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  47.41936891229415
maxi score, test score, baseline:  -0.2542400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05837315951227274, 0.06073776151179633, 0.1551982903535755, 0.0786692319668121, 0.4063512856382804, 0.24067027101726293]
maxi score, test score, baseline:  -0.2542400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05837315951227274, 0.06073776151179633, 0.1551982903535755, 0.0786692319668121, 0.4063512856382804, 0.24067027101726293]
maxi score, test score, baseline:  -0.2542400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05837315951227274, 0.06073776151179633, 0.1551982903535755, 0.0786692319668121, 0.4063512856382804, 0.24067027101726293]
printing an ep nov before normalisation:  53.84739875793457
maxi score, test score, baseline:  -0.2542400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05837315951227274, 0.06073776151179633, 0.1551982903535755, 0.0786692319668121, 0.4063512856382804, 0.24067027101726293]
printing an ep nov before normalisation:  30.354934807893645
actor:  0 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.2511400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05833623345472459, 0.060704328980608326, 0.15530441633902653, 0.07866229187059705, 0.40609005375764146, 0.240902675597402]
maxi score, test score, baseline:  -0.2511400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05833623345472459, 0.060704328980608326, 0.15530441633902653, 0.07866229187059705, 0.40609005375764146, 0.240902675597402]
actor:  1 policy actor:  1  step number:  35 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  40.30163167959551
maxi score, test score, baseline:  -0.2511400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05829934828620251, 0.06067093346998869, 0.15541042480886733, 0.07865535945930142, 0.40582911114474146, 0.2411348228308985]
maxi score, test score, baseline:  -0.2511400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05826250393882814, 0.06063757491848086, 0.1555163159581813, 0.07864843472016764, 0.4055684573193781, 0.24136671314496408]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.023]
 [ 0.126]
 [-0.023]
 [-0.022]
 [-0.023]
 [-0.023]
 [-0.023]] [[28.529]
 [43.335]
 [29.313]
 [28.904]
 [28.907]
 [29.059]
 [28.902]] [[0.416]
 [1.101]
 [0.444]
 [0.43 ]
 [0.43 ]
 [0.436]
 [0.429]]
maxi score, test score, baseline:  -0.2511400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05825069818161552, 0.060630274619251835, 0.15568899856680976, 0.07867530063019262, 0.40548244304797043, 0.24127228495415975]
maxi score, test score, baseline:  -0.2511400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05821395600025718, 0.060597013525054234, 0.15579479907123087, 0.07866843764087157, 0.4052225083420677, 0.2415032854205184]
maxi score, test score, baseline:  -0.2511400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05821395600025718, 0.060597013525054234, 0.15579479907123087, 0.07866843764087157, 0.4052225083420677, 0.2415032854205184]
printing an ep nov before normalisation:  44.2529837945446
printing an ep nov before normalisation:  72.22675085218417
actor:  0 policy actor:  0  step number:  21 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.24794000000000013 0.6279999999999999 0.6279999999999999
probs:  [0.05821395600025718, 0.060597013525054234, 0.15579479907123087, 0.07866843764087157, 0.4052225083420677, 0.2415032854205184]
maxi score, test score, baseline:  -0.24794000000000013 0.6279999999999999 0.6279999999999999
probs:  [0.05821395600025718, 0.060597013525054234, 0.15579479907123087, 0.07866843764087157, 0.4052225083420677, 0.2415032854205184]
maxi score, test score, baseline:  -0.24794000000000013 0.6279999999999999 0.6279999999999999
probs:  [0.05817950520539411, 0.06052738541786038, 0.15590652317059955, 0.07866462740614596, 0.40497855902919294, 0.24174339977080708]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.333
siam score:  -0.8328749
actor:  1 policy actor:  1  step number:  25 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.24794000000000013 0.6279999999999999 0.6279999999999999
probs:  [0.05817337278673498, 0.060525907625840585, 0.1560941326552875, 0.07860352238216103, 0.4049321189709251, 0.24167094557905092]
Printing some Q and Qe and total Qs values:  [[ 0.214]
 [ 0.528]
 [ 0.445]
 [ 0.394]
 [ 0.368]
 [-0.002]
 [ 0.403]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.214]
 [ 0.528]
 [ 0.445]
 [ 0.394]
 [ 0.368]
 [-0.002]
 [ 0.403]]
maxi score, test score, baseline:  -0.24794000000000013 0.6279999999999999 0.6279999999999999
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  54.21327381046257
maxi score, test score, baseline:  -0.2479400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05817337278673498, 0.060525907625840585, 0.1560941326552875, 0.07860352238216103, 0.4049321189709251, 0.24167094557905092]
line 256 mcts: sample exp_bonus 47.122501927073664
printing an ep nov before normalisation:  24.89443302154541
actor:  1 policy actor:  1  step number:  33 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.2479400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05813677894122778, 0.060492730893728926, 0.15619977106695657, 0.07859660381996764, 0.40467322890983953, 0.24190088636827953]
actor:  1 policy actor:  1  step number:  32 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  38.692180552246654
actor:  1 policy actor:  1  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.2479400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.058186522166552096, 0.060544494558222944, 0.15633361206438567, 0.0786638931720484, 0.40502015808435565, 0.24125131995443513]
printing an ep nov before normalisation:  48.22079111195958
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.2479400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.058186522166552096, 0.060544494558222944, 0.15633361206438567, 0.0786638931720484, 0.40502015808435565, 0.24125131995443513]
printing an ep nov before normalisation:  15.223925602821097
actor:  1 policy actor:  1  step number:  29 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  39 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[ 0.026]
 [ 0.03 ]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]] [[60.681]
 [60.739]
 [49.613]
 [49.613]
 [49.613]
 [49.613]
 [49.613]] [[0.794]
 [0.798]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]]
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.593]
 [0.49 ]
 [0.49 ]
 [0.521]
 [0.49 ]
 [0.49 ]] [[36.356]
 [37.09 ]
 [36.356]
 [36.356]
 [43.183]
 [36.356]
 [36.356]] [[1.288]
 [1.412]
 [1.288]
 [1.288]
 [1.521]
 [1.288]
 [1.288]]
maxi score, test score, baseline:  -0.2479400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05821680793145813, 0.06057601045135857, 0.15641510008524687, 0.07860951138623944, 0.4052313831376154, 0.2409511870080814]
maxi score, test score, baseline:  -0.2479400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05821680793145813, 0.06057601045135857, 0.15641510008524687, 0.07860951138623944, 0.4052313831376154, 0.2409511870080814]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.116]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.116]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.116]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.116]]
actor:  1 policy actor:  1  step number:  34 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.2479400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05821680793145813, 0.06057601045135857, 0.15641510008524687, 0.07860951138623944, 0.4052313831376154, 0.2409511870080814]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.2479400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05821680793145813, 0.06057601045135857, 0.15641510008524687, 0.07860951138623944, 0.4052313831376154, 0.2409511870080814]
from probs:  [0.05821680793145813, 0.06057601045135857, 0.15641510008524687, 0.07860951138623944, 0.4052313831376154, 0.2409511870080814]
maxi score, test score, baseline:  -0.2479400000000001 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.2479400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.0582414714357181, 0.060601675722386686, 0.15648146063964855, 0.07864283405920625, 0.4054033962935337, 0.24062916184950672]
printing an ep nov before normalisation:  65.05669031211703
maxi score, test score, baseline:  -0.2479400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.0582414714357181, 0.060601675722386686, 0.15648146063964855, 0.07864283405920625, 0.4054033962935337, 0.24062916184950672]
actor:  0 policy actor:  1  step number:  19 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.2446600000000001 0.6279999999999999 0.6279999999999999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.2446600000000001 0.6279999999999999 0.6279999999999999
printing an ep nov before normalisation:  46.02394221137309
maxi score, test score, baseline:  -0.2446600000000001 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.2446600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05827157408885236, 0.0606330010662313, 0.15656245597307317, 0.07858856624952305, 0.4056133442519648, 0.2403310583703553]
actor:  1 policy actor:  1  step number:  34 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.2446600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05827157408885236, 0.0606330010662313, 0.15656245597307317, 0.07858856624952305, 0.4056133442519648, 0.2403310583703553]
maxi score, test score, baseline:  -0.2446600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05827157408885236, 0.0606330010662313, 0.15656245597307317, 0.07858856624952305, 0.4056133442519648, 0.2403310583703553]
actor:  0 policy actor:  1  step number:  22 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
using another actor
maxi score, test score, baseline:  -0.2415600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05827157408885236, 0.0606330010662313, 0.15656245597307317, 0.07858856624952305, 0.4056133442519648, 0.2403310583703553]
actor:  1 policy actor:  1  step number:  29 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.2415600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.058296073972055405, 0.06065849607034476, 0.15662837628249043, 0.07862162784728755, 0.4057842162491053, 0.24001120957871666]
printing an ep nov before normalisation:  58.5603532702874
maxi score, test score, baseline:  -0.2415600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.058296073972055405, 0.06065849607034476, 0.15662837628249043, 0.07862162784728755, 0.4057842162491053, 0.24001120957871666]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  -0.2415600000000001 0.6279999999999999 0.6279999999999999
printing an ep nov before normalisation:  36.70421224628253
Printing some Q and Qe and total Qs values:  [[0.604]
 [0.604]
 [0.604]
 [0.606]
 [0.604]
 [0.629]
 [0.604]] [[24.164]
 [24.164]
 [24.164]
 [23.866]
 [24.164]
 [25.57 ]
 [24.164]] [[1.935]
 [1.935]
 [1.935]
 [1.906]
 [1.935]
 [2.109]
 [1.935]]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.2415600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.058296073972055405, 0.06065849607034476, 0.15662837628249043, 0.07862162784728755, 0.4057842162491053, 0.24001120957871666]
maxi score, test score, baseline:  -0.2415600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.058296073972055405, 0.06065849607034476, 0.15662837628249043, 0.07862162784728755, 0.4057842162491053, 0.24001120957871666]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.2415600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.058296073972055384, 0.060658496070344736, 0.15662837628249043, 0.07862162784728753, 0.4057842162491052, 0.24001120957871666]
maxi score, test score, baseline:  -0.2415600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.058296073972055405, 0.06065849607034476, 0.15662837628249043, 0.07862162784728755, 0.4057842162491053, 0.24001120957871666]
printing an ep nov before normalisation:  40.04088878631592
actor:  1 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.2415600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05832275202840263, 0.06064741979362278, 0.15670015726668887, 0.07865762880128313, 0.4059702796980654, 0.23970176241193716]
Printing some Q and Qe and total Qs values:  [[-0.056]
 [ 0.586]
 [ 0.485]
 [ 0.485]
 [-0.022]
 [ 0.485]
 [ 0.593]] [[41.518]
 [34.105]
 [32.192]
 [32.192]
 [44.677]
 [32.192]
 [39.798]] [[1.442]
 [1.816]
 [1.646]
 [1.646]
 [1.589]
 [1.646]
 [2.029]]
printing an ep nov before normalisation:  56.96572669025585
actor:  1 policy actor:  1  step number:  30 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.059]
 [0.348]
 [0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]] [[42.493]
 [47.099]
 [42.493]
 [42.493]
 [42.493]
 [42.493]
 [42.493]] [[0.791]
 [1.24 ]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]]
actor:  1 policy actor:  1  step number:  32 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.2415600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05835259712717564, 0.06067845720215842, 0.15678045961463893, 0.07860332972241538, 0.4061784313692359, 0.23940672496437573]
maxi score, test score, baseline:  -0.2415600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05835259712717564, 0.06067845720215842, 0.15678045961463893, 0.07860332972241538, 0.4061784313692359, 0.23940672496437573]
actor:  1 policy actor:  1  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.2415600000000001 0.6279999999999999 0.6279999999999999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.2415600000000001 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.2415600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05835259712717564, 0.06067845720215842, 0.15678045961463893, 0.07860332972241538, 0.4061784313692359, 0.23940672496437573]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.2415600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.058345487394125886, 0.06073920442293975, 0.15681070495018318, 0.07856433764489798, 0.4061295200990712, 0.23941074548878186]
printing an ep nov before normalisation:  76.59206234670388
maxi score, test score, baseline:  -0.2415600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.058345487394125886, 0.06073920442293975, 0.15681070495018318, 0.07856433764489798, 0.4061295200990712, 0.23941074548878186]
maxi score, test score, baseline:  -0.2415600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.058345487394125886, 0.06073920442293975, 0.15681070495018318, 0.07856433764489798, 0.4061295200990712, 0.23941074548878186]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [ 0.5560],
        [-0.0000],
        [ 0.2391],
        [ 0.0958],
        [-0.1467],
        [-0.3672],
        [-0.0000],
        [-0.0941],
        [ 0.7856]], dtype=torch.float64)
-0.9602999999999999 -0.9602999999999999
-0.067925830599 0.4880527142704402
0.99 0.99
-0.107327830599 0.13174698848092237
-0.125759551797 -0.02999952197746583
-0.145559551797 -0.2922483372111915
-0.06831985059899999 -0.4355488609322147
-0.5145049750499997 -0.5145049750499997
-0.106157551797 -0.20029758987472607
-0.145559551797 0.6400703474069083
maxi score, test score, baseline:  -0.2415600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.058345487394125886, 0.06073920442293975, 0.15681070495018318, 0.07856433764489798, 0.4061295200990712, 0.23941074548878186]
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.633]
 [0.475]
 [0.518]
 [0.522]
 [0.568]
 [0.566]] [[31.329]
 [29.095]
 [29.558]
 [30.821]
 [31.655]
 [32.158]
 [32.671]] [[2.034]
 [1.877]
 [1.765]
 [1.932]
 [2.018]
 [2.113]
 [2.161]]
maxi score, test score, baseline:  -0.2415600000000001 0.6279999999999999 0.6279999999999999
actor:  1 policy actor:  1  step number:  33 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  40 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
siam score:  -0.82905334
maxi score, test score, baseline:  -0.2415600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05837198923372183, 0.0607280989161731, 0.1568820342775396, 0.07860004436195872, 0.4063143548490729, 0.23910347836153367]
actor:  1 policy actor:  1  step number:  18 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  60.60263207983571
actor:  1 policy actor:  1  step number:  20 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.2415600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.058186658769704816, 0.0605352707155608, 0.1563832199484762, 0.07835034311320445, 0.40502178382356846, 0.24152272362948535]
actor:  1 policy actor:  1  step number:  22 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.2415600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.058186658769704816, 0.0605352707155608, 0.1563832199484762, 0.07835034311320445, 0.40502178382356846, 0.24152272362948532]
maxi score, test score, baseline:  -0.2415600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.058186658769704816, 0.0605352707155608, 0.1563832199484762, 0.07835034311320445, 0.40502178382356846, 0.24152272362948532]
maxi score, test score, baseline:  -0.2415600000000001 0.6279999999999999 0.6279999999999999
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
using another actor
printing an ep nov before normalisation:  55.59839454435483
maxi score, test score, baseline:  -0.2415600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.058186658769704816, 0.0605352707155608, 0.1563832199484762, 0.07835034311320445, 0.40502178382356846, 0.24152272362948532]
Printing some Q and Qe and total Qs values:  [[0.612]
 [0.689]
 [0.544]
 [0.583]
 [0.571]
 [0.586]
 [0.615]] [[29.326]
 [25.59 ]
 [26.933]
 [27.595]
 [29.225]
 [28.186]
 [27.634]] [[1.824]
 [1.617]
 [1.574]
 [1.664]
 [1.776]
 [1.712]
 [1.698]]
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.331]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]] [[38.29 ]
 [57.607]
 [38.29 ]
 [38.29 ]
 [38.29 ]
 [38.29 ]
 [38.29 ]] [[0.242]
 [0.331]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]]
printing an ep nov before normalisation:  38.82206439971924
actor:  1 policy actor:  1  step number:  34 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.2415600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.058186658769704816, 0.0605352707155608, 0.1563832199484762, 0.07835034311320445, 0.40502178382356846, 0.24152272362948532]
printing an ep nov before normalisation:  39.43768811038255
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[-0.05 ]
 [ 0.011]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]] [[53.226]
 [58.188]
 [53.226]
 [53.226]
 [53.226]
 [53.226]
 [53.226]] [[1.236]
 [1.512]
 [1.236]
 [1.236]
 [1.236]
 [1.236]
 [1.236]]
printing an ep nov before normalisation:  56.945585904002805
printing an ep nov before normalisation:  37.247211933135986
maxi score, test score, baseline:  -0.24156000000000008 0.6279999999999999 0.6279999999999999
probs:  [0.058186658769704816, 0.0605352707155608, 0.1563832199484762, 0.07835034311320445, 0.40502178382356846, 0.24152272362948535]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.24156000000000008 0.6279999999999999 0.6279999999999999
probs:  [0.05821100995260501, 0.0605606070508813, 0.1564487608136799, 0.07838315218650542, 0.40519161900754913, 0.24120485098877928]
maxi score, test score, baseline:  -0.24156000000000008 0.6279999999999999 0.6279999999999999
probs:  [0.05819862080410816, 0.060615078549641474, 0.15646422068083687, 0.07843037266897447, 0.40510587904587464, 0.24118582825056445]
printing an ep nov before normalisation:  57.521736508618595
maxi score, test score, baseline:  -0.24156000000000008 0.6279999999999999 0.6279999999999999
probs:  [0.05819862080410816, 0.060615078549641474, 0.15646422068083687, 0.07843037266897447, 0.40510587904587464, 0.24118582825056445]
Printing some Q and Qe and total Qs values:  [[1.001]
 [1.013]
 [1.001]
 [1.001]
 [1.015]
 [1.001]
 [1.001]] [[41.114]
 [48.145]
 [41.114]
 [41.114]
 [54.1  ]
 [41.114]
 [41.114]] [[1.001]
 [1.013]
 [1.001]
 [1.001]
 [1.015]
 [1.001]
 [1.001]]
maxi score, test score, baseline:  -0.24156000000000008 0.6279999999999999 0.6279999999999999
probs:  [0.05819862080410816, 0.060615078549641474, 0.15646422068083687, 0.07843037266897447, 0.40510587904587464, 0.24118582825056445]
actor:  0 policy actor:  0  step number:  24 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.23846000000000012 0.6279999999999999 0.6279999999999999
probs:  [0.05822287492688173, 0.06064034203545809, 0.15652952069613787, 0.07846307766686329, 0.405275037571172, 0.24086914710348706]
actor:  1 policy actor:  1  step number:  30 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.001]
 [ 0.04 ]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[74.373]
 [59.305]
 [74.373]
 [74.373]
 [74.373]
 [74.373]
 [74.373]] [[0.599]
 [0.495]
 [0.599]
 [0.599]
 [0.599]
 [0.599]
 [0.599]]
actor:  1 policy actor:  1  step number:  36 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.23846000000000012 0.6279999999999999 0.6279999999999999
probs:  [0.05824928338247643, 0.060629406041452026, 0.15660062087858984, 0.07849868763274566, 0.40545922132605305, 0.2405627807386829]
actor:  1 policy actor:  1  step number:  43 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.23846000000000012 0.6279999999999999 0.6279999999999999
probs:  [0.05824928338247643, 0.060629406041452026, 0.15660062087858984, 0.07849868763274566, 0.40545922132605305, 0.2405627807386829]
Printing some Q and Qe and total Qs values:  [[0.335]
 [0.335]
 [0.655]
 [0.335]
 [0.335]
 [0.335]
 [0.335]] [[65.671]
 [65.671]
 [57.239]
 [65.671]
 [65.671]
 [65.671]
 [65.671]] [[1.32 ]
 [1.32 ]
 [1.461]
 [1.32 ]
 [1.32 ]
 [1.32 ]
 [1.32 ]]
siam score:  -0.8321106
maxi score, test score, baseline:  -0.2384600000000001 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.2384600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05824928338247643, 0.060629406041452026, 0.15660062087858984, 0.07849868763274566, 0.40545922132605305, 0.2405627807386829]
printing an ep nov before normalisation:  0.17538814361358845
siam score:  -0.834301
maxi score, test score, baseline:  -0.2384600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05824928338247643, 0.060629406041452026, 0.15660062087858984, 0.07849868763274566, 0.40545922132605305, 0.2405627807386829]
actor:  1 policy actor:  1  step number:  37 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.2384600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.058273376667595266, 0.06065448605858893, 0.15666548786642517, 0.07853117575224206, 0.4056272581014218, 0.2402482155537268]
printing an ep nov before normalisation:  57.96213843515089
Printing some Q and Qe and total Qs values:  [[0.629]
 [0.829]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]] [[57.417]
 [53.089]
 [56.526]
 [56.526]
 [56.526]
 [56.526]
 [56.526]] [[0.629]
 [0.829]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]]
maxi score, test score, baseline:  -0.2384600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.058273376667595266, 0.06065448605858893, 0.15666548786642517, 0.07853117575224206, 0.4056272581014218, 0.2402482155537268]
maxi score, test score, baseline:  -0.2384600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.058273376667595266, 0.06065448605858893, 0.15666548786642517, 0.07853117575224206, 0.4056272581014218, 0.2402482155537268]
actor:  1 policy actor:  1  step number:  31 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.23846000000000012 0.6279999999999999 0.6279999999999999
probs:  [0.05827560505475951, 0.06061850849256651, 0.1566714874120398, 0.07853418057736734, 0.4056427998173607, 0.24025741864590605]
maxi score, test score, baseline:  -0.23846000000000012 0.6279999999999999 0.6279999999999999
probs:  [0.05827560505475951, 0.06061850849256651, 0.1566714874120398, 0.07853418057736734, 0.4056427998173607, 0.24025741864590605]
actor:  1 policy actor:  1  step number:  32 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  38 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.2384600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05828104214322712, 0.06062416467377907, 0.15668612582891606, 0.07844807351573628, 0.40568072037526665, 0.24027987346307478]
maxi score, test score, baseline:  -0.2384600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05828104214322712, 0.06062416467377907, 0.15668612582891606, 0.07844807351573628, 0.40568072037526665, 0.24027987346307478]
maxi score, test score, baseline:  -0.2384600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05828104214322712, 0.06062416467377907, 0.15668612582891606, 0.07844807351573628, 0.40568072037526665, 0.24027987346307478]
printing an ep nov before normalisation:  59.09622344552044
actor:  0 policy actor:  0  step number:  21 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
siam score:  -0.83476275
actor:  1 policy actor:  1  step number:  21 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 71.4329040035812
maxi score, test score, baseline:  -0.23526000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.05828326165282136, 0.0605883340037051, 0.15669210147321438, 0.07845106280351248, 0.40569620017527225, 0.24028903989147463]
maxi score, test score, baseline:  -0.23526000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.05828326165282136, 0.0605883340037051, 0.15669210147321438, 0.07845106280351248, 0.40569620017527225, 0.24028903989147463]
using another actor
maxi score, test score, baseline:  -0.23526000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.05828326165282134, 0.06058833400370507, 0.15669210147321438, 0.07845106280351245, 0.405696200175272, 0.24028903989147457]
actor:  0 policy actor:  1  step number:  26 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  65.33121435110755
maxi score, test score, baseline:  -0.23232000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.05830728236733365, 0.0606133069013926, 0.15675677307728922, 0.07848341446734594, 0.40586373081249294, 0.2399754923741457]
maxi score, test score, baseline:  -0.23232000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.05830728236733365, 0.0606133069013926, 0.15675677307728922, 0.07848341446734594, 0.40586373081249294, 0.2399754923741457]
maxi score, test score, baseline:  -0.23232000000000014 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.23232000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.05830728236733365, 0.0606133069013926, 0.15675677307728922, 0.07848341446734594, 0.40586373081249294, 0.2399754923741457]
maxi score, test score, baseline:  -0.23232000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.05830728236733365, 0.0606133069013926, 0.15675677307728922, 0.07848341446734594, 0.40586373081249294, 0.2399754923741457]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  1  action  0 :  tensor([0.4277, 0.0835, 0.0945, 0.0816, 0.1597, 0.0755, 0.0775],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0048, 0.9656, 0.0031, 0.0115, 0.0023, 0.0026, 0.0100],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0474, 0.0034, 0.7808, 0.0397, 0.0380, 0.0463, 0.0444],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1489, 0.0355, 0.1191, 0.3805, 0.1201, 0.1058, 0.0901],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1601, 0.0079, 0.0931, 0.1514, 0.4056, 0.0891, 0.0928],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0703, 0.0025, 0.1098, 0.0476, 0.0637, 0.6050, 0.1011],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1222, 0.1713, 0.1131, 0.2822, 0.0871, 0.0883, 0.1358],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  38 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.2323200000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05835508312785724, 0.06066300248814662, 0.15688546832727734, 0.07854779365700945, 0.40619711356287114, 0.2393515388368382]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.2323200000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05835508312785724, 0.06066300248814662, 0.15688546832727734, 0.07854779365700945, 0.40619711356287114, 0.2393515388368382]
actor:  1 policy actor:  1  step number:  37 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.2323200000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05835508312785722, 0.06066300248814668, 0.15688546832727737, 0.07854779365700952, 0.406197113562871, 0.2393515388368382]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.2323200000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05835508312785722, 0.06066300248814668, 0.15688546832727737, 0.07854779365700952, 0.406197113562871, 0.2393515388368382]
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]] [[61.367]
 [61.367]
 [61.367]
 [61.367]
 [61.367]
 [61.367]
 [61.367]] [[1.749]
 [1.749]
 [1.749]
 [1.749]
 [1.749]
 [1.749]
 [1.749]]
line 256 mcts: sample exp_bonus 34.41743656592677
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.714]
 [0.592]
 [0.631]
 [0.639]
 [0.62 ]
 [0.657]] [[29.311]
 [27.14 ]
 [27.618]
 [29.788]
 [30.329]
 [29.704]
 [29.242]] [[1.608]
 [1.55 ]
 [1.458]
 [1.633]
 [1.675]
 [1.617]
 [1.625]]
actor:  1 policy actor:  1  step number:  29 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  25 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.93 ]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.593]
 [0.93 ]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]]
maxi score, test score, baseline:  -0.2323200000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05836590330746135, 0.06067425158059465, 0.15691459978258934, 0.07837667113872365, 0.4062725780784247, 0.2393959961122065]
printing an ep nov before normalisation:  55.912551940537504
actor:  0 policy actor:  0  step number:  23 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.22924000000000008 0.6279999999999999 0.6279999999999999
printing an ep nov before normalisation:  30.994091033935547
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.618]
 [0.593]
 [0.593]
 [0.627]
 [0.593]
 [0.593]] [[23.532]
 [25.931]
 [21.926]
 [21.926]
 [27.142]
 [21.926]
 [21.926]] [[1.836]
 [1.989]
 [1.752]
 [1.752]
 [2.063]
 [1.752]
 [1.752]]
actor:  1 policy actor:  1  step number:  35 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.22924000000000008 0.6279999999999999 0.6279999999999999
probs:  [0.05838969299758247, 0.06069898429606435, 0.15697864939359438, 0.07840863579166545, 0.40643849745368876, 0.23908554006740462]
using another actor
from probs:  [0.05838969299758247, 0.06069898429606435, 0.15697864939359438, 0.07840863579166545, 0.40643849745368876, 0.23908554006740462]
maxi score, test score, baseline:  -0.22924000000000008 0.6279999999999999 0.6279999999999999
probs:  [0.05841340350448695, 0.060723634689488, 0.157042485817534, 0.07844049405129042, 0.40660386457166126, 0.23877611736553941]
maxi score, test score, baseline:  -0.22924000000000008 0.6279999999999999 0.6279999999999999
probs:  [0.05841340350448695, 0.060723634689488, 0.157042485817534, 0.07844049405129042, 0.40660386457166126, 0.23877611736553941]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  44.809318672915765
maxi score, test score, baseline:  -0.2292400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05841878191231469, 0.06072922629786144, 0.15705696624697374, 0.07835550906329851, 0.40664137586592985, 0.23879814061362178]
maxi score, test score, baseline:  -0.2292400000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05841878191231469, 0.06072922629786144, 0.15705696624697374, 0.07835550906329851, 0.40664137586592985, 0.23879814061362178]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  48.68817455101318
printing an ep nov before normalisation:  51.591642495138586
printing an ep nov before normalisation:  38.796839923160825
printing an ep nov before normalisation:  60.09537267459885
printing an ep nov before normalisation:  56.96436687397713
siam score:  -0.84563905
actor:  1 policy actor:  1  step number:  48 total reward:  0.3299999999999996  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  71.81509994527413
maxi score, test score, baseline:  -0.22924000000000014 0.6279999999999999 0.6279999999999999
printing an ep nov before normalisation:  0.3341284434966951
actor:  1 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  30.695719718933105
printing an ep nov before normalisation:  37.451255321502686
maxi score, test score, baseline:  -0.22924000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.05841878191231469, 0.06072922629786144, 0.15705696624697374, 0.07835550906329851, 0.40664137586592985, 0.23879814061362178]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.22924000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.058442417993493465, 0.06075379931531723, 0.15712060229256644, 0.07838722992809273, 0.40680622390736754, 0.23848972656316253]
Printing some Q and Qe and total Qs values:  [[0.735]
 [0.77 ]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]] [[46.457]
 [51.384]
 [46.457]
 [46.457]
 [46.457]
 [46.457]
 [46.457]] [[2.038]
 [2.276]
 [2.038]
 [2.038]
 [2.038]
 [2.038]
 [2.038]]
maxi score, test score, baseline:  -0.22924000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.058442417993493465, 0.06075379931531723, 0.15712060229256644, 0.07838722992809273, 0.40680622390736754, 0.23848972656316253]
actor:  1 policy actor:  1  step number:  25 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.81]
 [0.81]
 [0.81]
 [0.81]
 [0.81]
 [0.81]
 [0.81]] [[54.321]
 [54.321]
 [54.321]
 [54.321]
 [54.321]
 [54.321]
 [54.321]] [[2.477]
 [2.477]
 [2.477]
 [2.477]
 [2.477]
 [2.477]
 [2.477]]
printing an ep nov before normalisation:  52.247714159623584
maxi score, test score, baseline:  -0.22924000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.05844777400421422, 0.06075936763876054, 0.15713502242168362, 0.07830263605957372, 0.4068435789947256, 0.2385116208810423]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.22924000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.05844777400421422, 0.06075936763876054, 0.15713502242168362, 0.07830263605957372, 0.4068435789947256, 0.2385116208810423]
maxi score, test score, baseline:  -0.22924000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.05844777400421422, 0.06075936763876054, 0.15713502242168362, 0.07830263605957372, 0.4068435789947256, 0.2385116208810423]
maxi score, test score, baseline:  -0.22924000000000014 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.22924000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.05844777400421422, 0.06075936763876054, 0.15713502242168362, 0.07830263605957372, 0.4068435789947256, 0.2385116208810423]
actions average: 
K:  2  action  0 :  tensor([    0.6478,     0.0003,     0.0518,     0.0582,     0.1415,     0.0427,
            0.0577], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0027,     0.9673,     0.0041,     0.0079,     0.0007,     0.0011,
            0.0163], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0575, 0.0017, 0.6828, 0.0635, 0.0566, 0.0908, 0.0471],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1249, 0.0283, 0.1253, 0.3370, 0.1398, 0.1238, 0.1209],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2124, 0.0022, 0.1749, 0.1634, 0.1395, 0.1508, 0.1569],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1439, 0.0614, 0.1405, 0.1377, 0.1294, 0.2613, 0.1259],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2001, 0.0025, 0.1693, 0.1285, 0.1414, 0.1228, 0.2355],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  59.19584711203495
printing an ep nov before normalisation:  35.65394222053682
actor:  1 policy actor:  1  step number:  36 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.22924000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.058449996801580174, 0.060723591644235485, 0.15714100691773658, 0.07830561567212828, 0.4068590817250436, 0.23852070723927593]
actor:  1 policy actor:  1  step number:  42 total reward:  0.2699999999999996  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.22924000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.058452210368613276, 0.06068796421221183, 0.15714696656272073, 0.07830858291161817, 0.40687452007911074, 0.23852975586572542]
maxi score, test score, baseline:  -0.22924000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.058452210368613276, 0.06068796421221183, 0.15714696656272073, 0.07830858291161817, 0.40687452007911074, 0.23852975586572542]
maxi score, test score, baseline:  -0.22924000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.058452210368613276, 0.06068796421221183, 0.15714696656272073, 0.07830858291161817, 0.40687452007911074, 0.23852975586572542]
actor:  1 policy actor:  1  step number:  21 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.22924000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.05880784913316985, 0.061011688928529574, 0.1560937967417, 0.07838078384020715, 0.40939098592940326, 0.23631489542699022]
maxi score, test score, baseline:  -0.22924000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.05880784913316985, 0.061011688928529574, 0.1560937967417, 0.07838078384020715, 0.40939098592940326, 0.23631489542699022]
maxi score, test score, baseline:  -0.22924000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.05880784913316985, 0.061011688928529574, 0.1560937967417, 0.07838078384020715, 0.40939098592940326, 0.23631489542699022]
maxi score, test score, baseline:  -0.22924000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.05880784913316985, 0.061011688928529574, 0.1560937967417, 0.07838078384020715, 0.40939098592940326, 0.23631489542699022]
maxi score, test score, baseline:  -0.22924000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.05880784913316985, 0.061011688928529574, 0.1560937967417, 0.07838078384020715, 0.40939098592940326, 0.23631489542699022]
actor:  1 policy actor:  1  step number:  32 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  22.18624022814077
maxi score, test score, baseline:  -0.22924000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.05881313534163115, 0.06101717368887074, 0.1561078477922437, 0.07829781185369765, 0.4094278574377594, 0.23633617388579728]
maxi score, test score, baseline:  -0.22924000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.05880073883874009, 0.061070438451402885, 0.15612315298671522, 0.07834417829978887, 0.4093420389601364, 0.23631945246321656]
printing an ep nov before normalisation:  24.08397462632921
maxi score, test score, baseline:  -0.22924000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.05880073883874009, 0.061070438451402885, 0.15612315298671522, 0.07834417829978887, 0.4093420389601364, 0.23631945246321656]
Printing some Q and Qe and total Qs values:  [[0.601]
 [0.627]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]] [[37.528]
 [35.777]
 [37.528]
 [37.528]
 [37.528]
 [37.528]
 [37.528]] [[0.601]
 [0.627]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]]
printing an ep nov before normalisation:  47.747800869859034
maxi score, test score, baseline:  -0.22924000000000014 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.22924000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.05880599567297999, 0.06107589865943882, 0.15613713028277132, 0.07826165408913192, 0.4093787056401911, 0.23634061565548695]
printing an ep nov before normalisation:  38.59161298101688
line 256 mcts: sample exp_bonus 24.674635766159025
actor:  0 policy actor:  1  step number:  39 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.102]
 [0.155]
 [0.446]
 [0.102]
 [0.102]
 [0.102]
 [0.102]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.102]
 [0.155]
 [0.446]
 [0.102]
 [0.102]
 [0.102]
 [0.102]]
printing an ep nov before normalisation:  46.15196465431296
actor:  1 policy actor:  1  step number:  41 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  0.333
using another actor
maxi score, test score, baseline:  -0.22908000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.05879371216433748, 0.061128666816232954, 0.1561522878723297, 0.07830762277574195, 0.40929366929017824, 0.2363240410811796]
actor:  1 policy actor:  1  step number:  27 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  21.415643463866704
Printing some Q and Qe and total Qs values:  [[-0.001]
 [ 0.389]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[46.145]
 [44.753]
 [46.145]
 [46.145]
 [46.145]
 [46.145]
 [46.145]] [[1.026]
 [1.367]
 [1.026]
 [1.026]
 [1.026]
 [1.026]
 [1.026]]
printing an ep nov before normalisation:  58.69508489468441
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  -0.22908000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.05879371216433748, 0.061128666816232954, 0.1561522878723297, 0.07830762277574195, 0.40929366929017824, 0.2363240410811796]
printing an ep nov before normalisation:  49.05527946649023
actor:  0 policy actor:  0  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
using another actor
maxi score, test score, baseline:  -0.2261000000000001 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.2261000000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05879371216433748, 0.061128666816232954, 0.1561522878723297, 0.07830762277574195, 0.40929366929017824, 0.2363240410811796]
maxi score, test score, baseline:  -0.2261000000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05879371216433748, 0.061128666816232954, 0.1561522878723297, 0.07830762277574195, 0.40929366929017824, 0.2363240410811796]
actor:  1 policy actor:  1  step number:  39 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.2261000000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05879371216433748, 0.061128666816232954, 0.1561522878723297, 0.07830762277574195, 0.40929366929017824, 0.2363240410811796]
actor:  0 policy actor:  1  step number:  24 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.2231600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05879371216433748, 0.061128666816232954, 0.1561522878723297, 0.07830762277574195, 0.40929366929017824, 0.2363240410811796]
Printing some Q and Qe and total Qs values:  [[0.335]
 [0.472]
 [0.307]
 [0.318]
 [0.336]
 [0.258]
 [0.35 ]] [[36.467]
 [34.949]
 [34.561]
 [36.29 ]
 [38.365]
 [36.662]
 [36.154]] [[1.468]
 [1.515]
 [1.327]
 [1.441]
 [1.582]
 [1.403]
 [1.465]]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  31 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  53.10761193718864
maxi score, test score, baseline:  -0.2231600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05879371216433748, 0.061128666816232954, 0.1561522878723297, 0.07830762277574195, 0.40929366929017824, 0.2363240410811796]
maxi score, test score, baseline:  -0.2231600000000001 0.6279999999999999 0.6279999999999999
printing an ep nov before normalisation:  50.22373425824834
printing an ep nov before normalisation:  31.056489944458008
Printing some Q and Qe and total Qs values:  [[0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]] [[25.108]
 [25.108]
 [25.108]
 [25.108]
 [25.108]
 [25.108]
 [25.108]] [[1.634]
 [1.634]
 [1.634]
 [1.634]
 [1.634]
 [1.634]
 [1.634]]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.2231600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05879371216433748, 0.061128666816232954, 0.1561522878723297, 0.07830762277574195, 0.40929366929017824, 0.2363240410811796]
actor:  1 policy actor:  1  step number:  29 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8524879
printing an ep nov before normalisation:  18.114597304130868
actor:  1 policy actor:  1  step number:  36 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  22.812379889671405
actor:  1 policy actor:  1  step number:  32 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  29 total reward:  0.5399999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.2231600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.058840335981567966, 0.061177146473867135, 0.1562762930546771, 0.07836975640328846, 0.4096188732423231, 0.23571759484427618]
maxi score, test score, baseline:  -0.2231600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.058840335981567966, 0.061177146473867135, 0.1562762930546771, 0.07836975640328846, 0.4096188732423231, 0.23571759484427618]
printing an ep nov before normalisation:  60.74941632471788
siam score:  -0.855096
maxi score, test score, baseline:  -0.2231600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05882813908762323, 0.06122944492429496, 0.1562912725240642, 0.07841530809828202, 0.40953443555302116, 0.23570139981271435]
actor:  1 policy actor:  1  step number:  22 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  68.40556326444583
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.2231600000000001 0.6279999999999999 0.6279999999999999
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.355]
 [0.029]
 [0.01 ]
 [0.041]
 [0.041]
 [0.021]] [[45.152]
 [47.194]
 [45.641]
 [46.621]
 [36.559]
 [36.559]
 [51.084]] [[0.343]
 [0.708]
 [0.365]
 [0.357]
 [0.269]
 [0.269]
 [0.421]]
maxi score, test score, baseline:  -0.2231600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.058835558831357715, 0.06119994329397731, 0.15631101278153461, 0.078336117830684, 0.40958618879590053, 0.23573117846654584]
line 256 mcts: sample exp_bonus 37.07351764768994
maxi score, test score, baseline:  -0.2231600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.058835558831357715, 0.06119994329397731, 0.15631101278153461, 0.078336117830684, 0.40958618879590053, 0.23573117846654584]
printing an ep nov before normalisation:  53.57203324995874
maxi score, test score, baseline:  -0.2231600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.058837736642227544, 0.06116513925498311, 0.15631680685594343, 0.07833901909712801, 0.40960137918237816, 0.23573991896733978]
maxi score, test score, baseline:  -0.2231600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.058837736642227544, 0.06116513925498311, 0.15631680685594343, 0.07833901909712801, 0.40960137918237816, 0.23573991896733978]
printing an ep nov before normalisation:  9.671643510955619e-05
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  34 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.2231600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05883990546321064, 0.06113047888519924, 0.1563225770127232, 0.07834190838729929, 0.4096165068637532, 0.2357486233878144]
actor:  1 policy actor:  1  step number:  24 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.686]
 [0.561]
 [0.531]
 [0.545]
 [0.534]
 [0.639]] [[13.378]
 [14.509]
 [13.184]
 [13.077]
 [13.211]
 [12.968]
 [15.547]] [[1.526]
 [1.652]
 [1.439]
 [1.402]
 [1.424]
 [1.398]
 [1.674]]
printing an ep nov before normalisation:  82.32255481792255
actor:  1 policy actor:  1  step number:  49 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.183]
 [0.183]
 [0.295]
 [0.183]
 [0.183]
 [0.183]
 [0.183]] [[66.519]
 [66.519]
 [67.659]
 [66.519]
 [66.519]
 [66.519]
 [66.519]] [[1.194]
 [1.194]
 [1.328]
 [1.194]
 [1.194]
 [1.194]
 [1.194]]
maxi score, test score, baseline:  -0.2231600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05882781482109224, 0.061182340615950945, 0.1563374081910223, 0.07838707472257697, 0.4095328045047132, 0.23573255714464422]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.736]
 [0.786]
 [0.736]
 [0.736]
 [0.785]
 [0.736]
 [0.736]] [[51.743]
 [49.653]
 [51.743]
 [51.743]
 [59.552]
 [51.743]
 [51.743]] [[1.894]
 [1.897]
 [1.894]
 [1.894]
 [2.118]
 [1.894]
 [1.894]]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.2231600000000001 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.2231600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05885620156167435, 0.06121186608695322, 0.1564129539563513, 0.0783362745221447, 0.40973080435428016, 0.23545189951859619]
siam score:  -0.8541295
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.241]
 [0.025]
 [0.025]
 [0.025]
 [0.025]
 [0.025]] [[44.638]
 [51.2  ]
 [44.638]
 [44.638]
 [44.638]
 [44.638]
 [44.638]] [[0.49 ]
 [0.816]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  74.71817279702192
maxi score, test score, baseline:  -0.2231600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.058879306324838496, 0.06123589769469782, 0.15647444277050132, 0.07836704378925523, 0.4098919619770636, 0.23515134744364352]
maxi score, test score, baseline:  -0.2231600000000001 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.2231600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.058879306324838496, 0.06123589769469782, 0.15647444277050132, 0.07836704378925523, 0.4098919619770636, 0.23515134744364352]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  30 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.667
from probs:  [0.058879306324838496, 0.06123589769469782, 0.15647444277050132, 0.07836704378925523, 0.4098919619770636, 0.23515134744364352]
maxi score, test score, baseline:  -0.2231600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05890233466698988, 0.06125984981581063, 0.1565357282050531, 0.0783977112843398, 0.4100525865569995, 0.23485178947080712]
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]]
maxi score, test score, baseline:  -0.2231600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05890233466698988, 0.06125984981581063, 0.1565357282050531, 0.0783977112843398, 0.4100525865569995, 0.23485178947080712]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.2231600000000001 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.2231600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05890233466698988, 0.06125984981581063, 0.1565357282050531, 0.0783977112843398, 0.4100525865569995, 0.23485178947080712]
printing an ep nov before normalisation:  21.059777932949352
maxi score, test score, baseline:  -0.2231600000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05890233466698988, 0.06125984981581063, 0.1565357282050531, 0.0783977112843398, 0.4100525865569995, 0.23485178947080712]
actor:  1 policy actor:  1  step number:  31 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  48 total reward:  0.3299999999999996  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  68.54283881827442
printing an ep nov before normalisation:  73.22517259927407
line 256 mcts: sample exp_bonus 53.493682296164906
actor:  1 policy actor:  1  step number:  21 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  22 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.21998000000000012 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.21998000000000012 0.6279999999999999 0.6279999999999999
probs:  [0.05893048286541544, 0.061289127175568635, 0.15661063913702644, 0.07834689012366705, 0.4102489225554113, 0.23457393814291105]
maxi score, test score, baseline:  -0.21998000000000012 0.6279999999999999 0.6279999999999999
probs:  [0.05893048286541544, 0.061289127175568635, 0.15661063913702644, 0.07834689012366705, 0.4102489225554113, 0.23457393814291105]
actor:  0 policy actor:  0  step number:  26 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  42 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.21708000000000013 0.6279999999999999 0.6279999999999999
probs:  [0.05893048286541544, 0.061289127175568635, 0.15661063913702644, 0.07834689012366705, 0.4102489225554113, 0.23457393814291105]
siam score:  -0.85466665
maxi score, test score, baseline:  -0.21708000000000013 0.6279999999999999 0.6279999999999999
probs:  [0.05893048286541544, 0.061289127175568635, 0.15661063913702644, 0.07834689012366705, 0.4102489225554113, 0.23457393814291105]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.21708000000000013 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.21708000000000013 0.6279999999999999 0.6279999999999999
probs:  [0.05899889935586326, 0.061360288184665476, 0.15679271626268484, 0.07843789954666856, 0.41072613320160684, 0.23368406344851103]
maxi score, test score, baseline:  -0.21708000000000013 0.6279999999999999 0.6279999999999999
probs:  [0.05899889935586326, 0.061360288184665476, 0.15679271626268484, 0.07843789954666856, 0.41072613320160684, 0.23368406344851103]
Printing some Q and Qe and total Qs values:  [[0.099]
 [0.441]
 [0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]] [[52.928]
 [49.173]
 [52.928]
 [52.928]
 [52.928]
 [52.928]
 [52.928]] [[0.73 ]
 [1.002]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]]
printing an ep nov before normalisation:  39.337648168746114
maxi score, test score, baseline:  -0.21708000000000013 0.6279999999999999 0.6279999999999999
probs:  [0.05899889935586326, 0.061360288184665476, 0.15679271626268484, 0.07843789954666856, 0.41072613320160684, 0.23368406344851103]
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.728]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]] [[25.859]
 [26.894]
 [25.859]
 [25.859]
 [25.859]
 [25.859]
 [25.859]] [[2.14 ]
 [2.251]
 [2.14 ]
 [2.14 ]
 [2.14 ]
 [2.14 ]
 [2.14 ]]
actor:  1 policy actor:  1  step number:  30 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.21708000000000013 0.6279999999999999 0.6279999999999999
probs:  [0.05899889935586326, 0.061360288184665476, 0.15679271626268484, 0.07843789954666856, 0.41072613320160684, 0.23368406344851103]
maxi score, test score, baseline:  -0.21708000000000013 0.6279999999999999 0.6279999999999999
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.706]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]] [[66.233]
 [60.53 ]
 [66.233]
 [66.233]
 [66.233]
 [66.233]
 [66.233]] [[1.073]
 [1.213]
 [1.073]
 [1.073]
 [1.073]
 [1.073]
 [1.073]]
actor:  0 policy actor:  0  step number:  22 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.21394000000000013 0.6279999999999999 0.6279999999999999
probs:  [0.05899889935586326, 0.061360288184665476, 0.15679271626268484, 0.07843789954666856, 0.41072613320160684, 0.23368406344851103]
maxi score, test score, baseline:  -0.21394000000000013 0.6279999999999999 0.6279999999999999
probs:  [0.05899889935586326, 0.061360288184665476, 0.15679271626268484, 0.07843789954666856, 0.41072613320160684, 0.23368406344851103]
maxi score, test score, baseline:  -0.21394000000000013 0.6279999999999999 0.6279999999999999
printing an ep nov before normalisation:  52.40657189415509
maxi score, test score, baseline:  -0.21394000000000013 0.6279999999999999 0.6279999999999999
probs:  [0.05899889935586326, 0.061360288184665476, 0.15679271626268484, 0.07843789954666856, 0.41072613320160684, 0.23368406344851103]
printing an ep nov before normalisation:  56.17549543545412
actor:  1 policy actor:  1  step number:  41 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  1  action  0 :  tensor([0.5037, 0.0013, 0.1071, 0.1042, 0.1032, 0.0787, 0.1017],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0082, 0.9471, 0.0125, 0.0039, 0.0011, 0.0013, 0.0259],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0621, 0.0032, 0.6998, 0.0586, 0.0577, 0.0526, 0.0660],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1025, 0.0903, 0.0902, 0.4215, 0.0971, 0.0821, 0.1163],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1959, 0.0039, 0.1552, 0.1411, 0.2748, 0.0995, 0.1295],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1302, 0.0057, 0.3332, 0.1464, 0.1246, 0.1049, 0.1549],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1747, 0.0045, 0.1623, 0.1602, 0.1522, 0.1290, 0.2171],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  57.533100685764246
maxi score, test score, baseline:  -0.21394000000000013 0.6279999999999999 0.6279999999999999
probs:  [0.05900107056007982, 0.06132569192342197, 0.1567984944986098, 0.07844078773987564, 0.4107412775295375, 0.23369267774847538]
maxi score, test score, baseline:  -0.21394000000000013 0.6279999999999999 0.6279999999999999
probs:  [0.05900107056007982, 0.06132569192342197, 0.1567984944986098, 0.07844078773987564, 0.4107412775295375, 0.23369267774847538]
actor:  1 policy actor:  1  step number:  33 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  37 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.21394000000000013 0.6279999999999999 0.6279999999999999
printing an ep nov before normalisation:  25.320758819580078
printing an ep nov before normalisation:  41.67884145399735
maxi score, test score, baseline:  -0.21394000000000013 0.6279999999999999 0.6279999999999999
actor:  1 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.21394000000000013 0.6279999999999999 0.6279999999999999
probs:  [0.059046311449710905, 0.06137271932215088, 0.15691889428954242, 0.0785009683664242, 0.4110568364544984, 0.23310427011767312]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  0 policy actor:  0  step number:  26 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  52.2888130373097
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.680164337158203
maxi score, test score, baseline:  -0.2110000000000001 0.6279999999999999 0.6279999999999999
probs:  [0.059046311449710905, 0.06137271932215088, 0.15691889428954242, 0.0785009683664242, 0.4110568364544984, 0.23310427011767312]
actions average: 
K:  3  action  0 :  tensor([0.4196, 0.0089, 0.1434, 0.1264, 0.1029, 0.0970, 0.1019],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0027,     0.9793,     0.0018,     0.0030,     0.0003,     0.0003,
            0.0125], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1245, 0.0063, 0.4180, 0.1114, 0.1033, 0.1379, 0.0985],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0901, 0.0293, 0.1624, 0.3792, 0.1022, 0.1041, 0.1328],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1376, 0.0148, 0.1244, 0.1655, 0.2587, 0.1649, 0.1340],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1006, 0.0125, 0.1812, 0.1718, 0.1022, 0.3326, 0.0991],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1470, 0.0053, 0.1892, 0.1919, 0.1608, 0.1568, 0.1490],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  36 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  94.2286148789282
maxi score, test score, baseline:  -0.2110000000000001 0.6279999999999999 0.6279999999999999
probs:  [0.059046311449710905, 0.06137271932215088, 0.15691889428954242, 0.0785009683664242, 0.4110568364544984, 0.23310427011767312]
printing an ep nov before normalisation:  1.2397215738447187
actor:  1 policy actor:  1  step number:  24 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  2  action  0 :  tensor([0.5928, 0.0007, 0.0658, 0.0789, 0.1416, 0.0652, 0.0549],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0074, 0.9488, 0.0134, 0.0072, 0.0026, 0.0040, 0.0166],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0552, 0.0806, 0.5694, 0.0705, 0.0602, 0.1053, 0.0589],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1075, 0.0600, 0.1336, 0.3257, 0.1091, 0.1007, 0.1635],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1370, 0.0058, 0.1152, 0.1499, 0.3792, 0.1229, 0.0900],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1227, 0.0006, 0.1615, 0.1485, 0.1163, 0.3553, 0.0951],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0659, 0.1014, 0.0450, 0.0535, 0.0492, 0.0267, 0.6584],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.2110000000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05905150227697571, 0.06137811512903051, 0.15693270866239287, 0.07841983385212868, 0.41109304290146914, 0.23312479717800302]
printing an ep nov before normalisation:  30.26493753249319
maxi score, test score, baseline:  -0.2110000000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05905150227697571, 0.06137811512903051, 0.15693270866239287, 0.07841983385212868, 0.41109304290146914, 0.23312479717800302]
siam score:  -0.8501979
actor:  1 policy actor:  1  step number:  39 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 37.96498537513826
maxi score, test score, baseline:  -0.2110000000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05907401574831339, 0.061401517630661415, 0.15699262386884277, 0.07844974822537487, 0.41125007621514603, 0.23283201831166145]
from probs:  [0.05907401574831339, 0.061401517630661415, 0.15699262386884277, 0.07844974822537487, 0.41125007621514603, 0.23283201831166145]
maxi score, test score, baseline:  -0.2110000000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05907401574831339, 0.061401517630661415, 0.15699262386884277, 0.07844974822537487, 0.41125007621514603, 0.23283201831166145]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  40.25995866645533
maxi score, test score, baseline:  -0.2110000000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05909645573038048, 0.061424843741017386, 0.15705234349795352, 0.07847956495106419, 0.411406596935118, 0.23254019514446653]
maxi score, test score, baseline:  -0.2110000000000001 0.6279999999999999 0.6279999999999999
probs:  [0.05909645573038048, 0.061424843741017386, 0.15705234349795352, 0.07847956495106419, 0.411406596935118, 0.23254019514446653]
maxi score, test score, baseline:  -0.2110000000000001 0.6279999999999999 0.6279999999999999
actor:  1 policy actor:  1  step number:  31 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  43.227157365452186
actor:  0 policy actor:  1  step number:  32 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  1.667
siam score:  -0.849492
maxi score, test score, baseline:  -0.20838000000000012 0.6279999999999999 0.6279999999999999
probs:  [0.059118822582420044, 0.06144809383352774, 0.1571118685057803, 0.07850928450653462, 0.41156260756713525, 0.23224932300460208]
maxi score, test score, baseline:  -0.20838000000000012 0.6279999999999999 0.6279999999999999
probs:  [0.059118822582420044, 0.06144809383352774, 0.1571118685057803, 0.07850928450653462, 0.41156260756713525, 0.23224932300460208]
maxi score, test score, baseline:  -0.20838000000000012 0.6279999999999999 0.6279999999999999
probs:  [0.059118822582420044, 0.06144809383352774, 0.1571118685057803, 0.07850928450653462, 0.41156260756713525, 0.23224932300460208]
printing an ep nov before normalisation:  55.662408658270685
printing an ep nov before normalisation:  53.86651343448524
actor:  1 policy actor:  1  step number:  27 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
siam score:  -0.85322416
Printing some Q and Qe and total Qs values:  [[ 0.648]
 [ 0.604]
 [ 0.077]
 [ 0.556]
 [ 0.453]
 [-0.067]
 [ 0.569]] [[59.999]
 [60.842]
 [67.123]
 [66.672]
 [68.163]
 [68.172]
 [65.561]] [[1.853]
 [1.833]
 [1.485]
 [1.951]
 [1.891]
 [1.371]
 [1.932]]
actor:  1 policy actor:  1  step number:  20 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  46.835624337961406
printing an ep nov before normalisation:  47.53767974388653
siam score:  -0.85586303
actor:  1 policy actor:  1  step number:  27 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  59.11594503596995
maxi score, test score, baseline:  -0.20838000000000012 0.6279999999999999 0.6279999999999999
probs:  [0.05970549007741538, 0.06194280405278629, 0.15533971127576002, 0.07862919744170287, 0.41571429120434134, 0.22866850594799407]
actor:  1 policy actor:  1  step number:  24 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
actions average: 
K:  4  action  0 :  tensor([0.5201, 0.0150, 0.1072, 0.0992, 0.1131, 0.0710, 0.0744],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0140, 0.9472, 0.0084, 0.0071, 0.0045, 0.0048, 0.0140],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1028, 0.0544, 0.4918, 0.0948, 0.0578, 0.1180, 0.0804],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1290, 0.0175, 0.1264, 0.3482, 0.1345, 0.0908, 0.1536],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1661, 0.1008, 0.1327, 0.0734, 0.3443, 0.0939, 0.0888],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1937, 0.0073, 0.1882, 0.1733, 0.1378, 0.1507, 0.1490],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1296, 0.0208, 0.1664, 0.1559, 0.1238, 0.1311, 0.2723],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[ 0.016]
 [ 0.286]
 [-0.005]
 [-0.013]
 [-0.012]
 [-0.008]
 [-0.001]] [[45.629]
 [48.658]
 [40.225]
 [40.036]
 [40.298]
 [40.704]
 [44.939]] [[0.36 ]
 [0.673]
 [0.264]
 [0.253]
 [0.257]
 [0.267]
 [0.334]]
actor:  1 policy actor:  1  step number:  20 total reward:  0.69  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  0.10632859551719775
actor:  1 policy actor:  1  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  0.667
from probs:  [0.06088644437756568, 0.06301209158783183, 0.151747457956373, 0.07886564899953696, 0.4240720240591453, 0.22141633301954716]
line 256 mcts: sample exp_bonus 40.874863164016844
printing an ep nov before normalisation:  54.77517696908552
maxi score, test score, baseline:  -0.20838000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.06088850897170067, 0.06298027229463284, 0.15175261030656548, 0.07886832458617354, 0.4240864308936921, 0.22142385294723535]
printing an ep nov before normalisation:  62.308625303767506
printing an ep nov before normalisation:  61.25887925955877
maxi score, test score, baseline:  -0.20838000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.06088850897170067, 0.06298027229463284, 0.15175261030656548, 0.07886832458617354, 0.4240864308936921, 0.22142385294723535]
actions average: 
K:  1  action  0 :  tensor([0.3444, 0.0025, 0.1248, 0.1387, 0.1545, 0.1118, 0.1232],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0110, 0.9554, 0.0061, 0.0076, 0.0018, 0.0017, 0.0165],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0826, 0.0007, 0.5672, 0.0885, 0.0903, 0.0887, 0.0820],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0970, 0.0037, 0.0524, 0.6456, 0.0768, 0.0590, 0.0655],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1162, 0.0010, 0.1060, 0.0966, 0.5015, 0.0858, 0.0929],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1522, 0.0031, 0.1497, 0.1566, 0.1589, 0.2340, 0.1456],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1655, 0.0496, 0.1250, 0.1288, 0.1511, 0.1253, 0.2546],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.20838000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.06088850897170067, 0.06298027229463284, 0.15175261030656548, 0.07886832458617354, 0.4240864308936921, 0.22142385294723535]
actor:  1 policy actor:  1  step number:  29 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.114]
 [0.301]
 [0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.114]
 [0.301]
 [0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]]
Printing some Q and Qe and total Qs values:  [[ 0.038]
 [ 0.328]
 [ 0.065]
 [ 0.008]
 [-0.002]
 [ 0.021]
 [ 0.039]] [[35.864]
 [48.57 ]
 [36.018]
 [36.184]
 [35.913]
 [35.89 ]
 [35.79 ]] [[0.658]
 [1.369]
 [0.69 ]
 [0.639]
 [0.62 ]
 [0.642]
 [0.657]]
siam score:  -0.85123926
actor:  1 policy actor:  1  step number:  22 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  33 total reward:  0.5199999999999999  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8464723
maxi score, test score, baseline:  -0.20838000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.060912749242980256, 0.06300534714207684, 0.15181310372959517, 0.07885126427102548, 0.42425558064171026, 0.221161954972612]
Printing some Q and Qe and total Qs values:  [[0.872]
 [0.765]
 [0.865]
 [0.774]
 [0.633]
 [0.865]
 [0.57 ]] [[48.875]
 [53.104]
 [52.555]
 [49.749]
 [51.106]
 [52.555]
 [50.729]] [[0.872]
 [0.765]
 [0.865]
 [0.774]
 [0.633]
 [0.865]
 [0.57 ]]
printing an ep nov before normalisation:  45.35273696895157
actor:  1 policy actor:  1  step number:  25 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  45.3158091685448
maxi score, test score, baseline:  -0.20838000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.060912749242980256, 0.06300534714207684, 0.15181310372959517, 0.07885126427102548, 0.42425558064171026, 0.221161954972612]
maxi score, test score, baseline:  -0.20838000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.060912749242980256, 0.06300534714207684, 0.15181310372959517, 0.07885126427102548, 0.42425558064171026, 0.221161954972612]
actor:  0 policy actor:  1  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  49.87398585018487
actions average: 
K:  4  action  0 :  tensor([0.7733, 0.0013, 0.0312, 0.0376, 0.0706, 0.0300, 0.0559],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0130, 0.9187, 0.0193, 0.0120, 0.0063, 0.0113, 0.0194],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0484, 0.0750, 0.6532, 0.0735, 0.0516, 0.0455, 0.0528],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1655, 0.0946, 0.1137, 0.2712, 0.1316, 0.1115, 0.1119],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1606, 0.0014, 0.0728, 0.1109, 0.5043, 0.0793, 0.0708],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1236, 0.0106, 0.1289, 0.1442, 0.1051, 0.3722, 0.1155],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1401, 0.0716, 0.1115, 0.1192, 0.1188, 0.0888, 0.3499],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.20540000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.06093397279234365, 0.06302730140390701, 0.15186606869582941, 0.07887875175507605, 0.42440367956294184, 0.22089022578990214]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  0.667
using another actor
from probs:  [0.06093397279234365, 0.06302730140390701, 0.15186606869582941, 0.07887875175507605, 0.42440367956294184, 0.22089022578990214]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.20540000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.06093603200190824, 0.06299559042074865, 0.15187120760842077, 0.07888141872133583, 0.4244180488237042, 0.2208977024238823]
actor:  0 policy actor:  1  step number:  22 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  54.4017648845319
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.51682501961168
maxi score, test score, baseline:  -0.20222000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.060940964398044174, 0.06300068989072174, 0.1518835167745419, 0.07880675052014365, 0.42445246731489583, 0.2209156111016527]
maxi score, test score, baseline:  -0.20222000000000015 0.6279999999999999 0.6279999999999999
Printing some Q and Qe and total Qs values:  [[0.633]
 [0.664]
 [0.551]
 [0.488]
 [0.58 ]
 [0.551]
 [0.446]] [[48.416]
 [44.961]
 [44.775]
 [44.577]
 [48.985]
 [44.775]
 [46.881]] [[1.342]
 [1.281]
 [1.162]
 [1.094]
 [1.305]
 [1.162]
 [1.114]]
actor:  1 policy actor:  1  step number:  37 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.20222000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.060945872740107654, 0.06300576449184474, 0.15189576591191314, 0.07873244645724686, 0.4244867179556436, 0.22093343244324395]
actor:  1 policy actor:  1  step number:  34 total reward:  0.2899999999999996  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.68 ]
 [0.691]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]] [[56.115]
 [53.707]
 [53.002]
 [53.002]
 [53.002]
 [53.002]
 [53.002]] [[1.841]
 [1.782]
 [1.764]
 [1.764]
 [1.764]
 [1.764]
 [1.764]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.20222000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.060945872740107654, 0.06300576449184474, 0.15189576591191314, 0.07873244645724686, 0.4244867179556436, 0.22093343244324395]
printing an ep nov before normalisation:  48.42963220568645
maxi score, test score, baseline:  -0.20222000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.060945872740107654, 0.06300576449184474, 0.15189576591191314, 0.07873244645724686, 0.4244867179556436, 0.22093343244324395]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  -0.20222000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.060945872740107654, 0.06300576449184474, 0.15189576591191314, 0.07873244645724686, 0.4244867179556436, 0.22093343244324395]
printing an ep nov before normalisation:  25.7141785615258
actor:  1 policy actor:  1  step number:  35 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  39 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.20222000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.060945872740107654, 0.06300576449184474, 0.15189576591191314, 0.07873244645724686, 0.4244867179556436, 0.22093343244324395]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  24 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.20222000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.060945872740107654, 0.06300576449184474, 0.15189576591191314, 0.07873244645724686, 0.4244867179556436, 0.22093343244324395]
maxi score, test score, baseline:  -0.20222000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.060945872740107654, 0.06300576449184474, 0.15189576591191314, 0.07873244645724686, 0.4244867179556436, 0.22093343244324395]
maxi score, test score, baseline:  -0.20222000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.060945872740107654, 0.06300576449184474, 0.15189576591191314, 0.07873244645724686, 0.4244867179556436, 0.22093343244324395]
printing an ep nov before normalisation:  66.06785007753912
actor:  1 policy actor:  1  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.20222000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.060945872740107654, 0.06300576449184474, 0.15189576591191314, 0.07873244645724686, 0.4244867179556436, 0.22093343244324395]
maxi score, test score, baseline:  -0.20222000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.060945872740107654, 0.06300576449184474, 0.15189576591191314, 0.07873244645724686, 0.4244867179556436, 0.22093343244324395]
maxi score, test score, baseline:  -0.20222000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.060945872740107654, 0.06300576449184474, 0.15189576591191314, 0.07873244645724686, 0.4244867179556436, 0.22093343244324395]
printing an ep nov before normalisation:  46.895502512469946
Printing some Q and Qe and total Qs values:  [[0.75 ]
 [0.779]
 [0.75 ]
 [0.68 ]
 [0.729]
 [0.711]
 [0.75 ]] [[55.578]
 [53.946]
 [55.578]
 [55.316]
 [54.323]
 [55.495]
 [55.578]] [[2.196]
 [2.144]
 [2.196]
 [2.113]
 [2.113]
 [2.153]
 [2.196]]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
using another actor
siam score:  -0.8490364
printing an ep nov before normalisation:  0.0003042799426111742
maxi score, test score, baseline:  -0.20222000000000015 0.6279999999999999 0.6279999999999999
actor:  1 policy actor:  1  step number:  42 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.20222000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.060945872740107654, 0.06300576449184474, 0.15189576591191314, 0.07873244645724686, 0.4244867179556436, 0.22093343244324395]
actor:  0 policy actor:  1  step number:  27 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.19922000000000012 0.6279999999999999 0.6279999999999999
probs:  [0.060945872740107654, 0.06300576449184474, 0.15189576591191314, 0.07873244645724686, 0.4244867179556436, 0.22093343244324395]
maxi score, test score, baseline:  -0.19922000000000012 0.6279999999999999 0.6279999999999999
actor:  1 policy actor:  1  step number:  33 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.19922000000000012 0.6279999999999999 0.6279999999999999
probs:  [0.060945872740107654, 0.06300576449184474, 0.15189576591191314, 0.07873244645724686, 0.4244867179556436, 0.22093343244324395]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.19922000000000012 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.19922000000000012 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.19922000000000012 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.19922000000000012 0.6279999999999999 0.6279999999999999
probs:  [0.060945872740107654, 0.06300576449184474, 0.15189576591191314, 0.07873244645724686, 0.4244867179556436, 0.22093343244324395]
printing an ep nov before normalisation:  68.73969329398825
maxi score, test score, baseline:  -0.19922000000000012 0.6279999999999999 0.6279999999999999
probs:  [0.060945872740107654, 0.06300576449184474, 0.15189576591191314, 0.07873244645724686, 0.4244867179556436, 0.22093343244324395]
actor:  1 policy actor:  1  step number:  24 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.19922000000000012 0.6279999999999999 0.6279999999999999
probs:  [0.060945872740107654, 0.06300576449184474, 0.15189576591191314, 0.07873244645724686, 0.4244867179556436, 0.22093343244324395]
printing an ep nov before normalisation:  34.47762489318848
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.404]
 [0.504]
 [0.404]
 [0.404]
 [0.404]
 [0.404]] [[34.489]
 [34.489]
 [44.6  ]
 [34.489]
 [34.489]
 [34.489]
 [34.489]] [[1.081]
 [1.081]
 [1.609]
 [1.081]
 [1.081]
 [1.081]
 [1.081]]
actor:  0 policy actor:  1  step number:  28 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.19636000000000012 0.6279999999999999 0.6279999999999999
probs:  [0.060945872740107654, 0.06300576449184474, 0.15189576591191314, 0.07873244645724686, 0.4244867179556436, 0.22093343244324395]
actor:  1 policy actor:  1  step number:  19 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.19636000000000012 0.6279999999999999 0.6279999999999999
probs:  [0.06170706628465118, 0.06369702397245208, 0.14956917795594285, 0.0788897794433447, 0.4298739614016211, 0.216262990941988]
maxi score, test score, baseline:  -0.19636000000000012 0.6279999999999999 0.6279999999999999
probs:  [0.06170706628465118, 0.06369702397245208, 0.14956917795594285, 0.0788897794433447, 0.4298739614016211, 0.216262990941988]
printing an ep nov before normalisation:  47.081442267238316
maxi score, test score, baseline:  -0.19636000000000012 0.6279999999999999 0.6279999999999999
actor:  1 policy actor:  1  step number:  25 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.0
UNIT TEST: sample policy line 217 mcts : [0.102 0.204 0.082 0.122 0.143 0.102 0.245]
maxi score, test score, baseline:  -0.19636000000000012 0.6279999999999999 0.6279999999999999
Printing some Q and Qe and total Qs values:  [[ 0.011]
 [ 0.079]
 [ 0.006]
 [ 0.015]
 [ 0.015]
 [-0.004]
 [ 0.018]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.011]
 [ 0.079]
 [ 0.006]
 [ 0.015]
 [ 0.015]
 [-0.004]
 [ 0.018]]
actor:  1 policy actor:  1  step number:  32 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.333
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  -0.19636000000000012 0.6279999999999999 0.6279999999999999
actor:  0 policy actor:  0  step number:  34 total reward:  0.1899999999999995  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.1939800000000001 0.6279999999999999 0.6279999999999999
probs:  [0.06171184396912889, 0.06370195606324949, 0.1495807731046277, 0.07881835964137261, 0.4299073061735723, 0.21627976104804894]
printing an ep nov before normalisation:  55.53909697629958
actor:  1 policy actor:  1  step number:  31 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.1939800000000001 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.1939800000000001 0.6279999999999999 0.6279999999999999
actor:  1 policy actor:  1  step number:  39 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.1939800000000001 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.1939800000000001 0.6279999999999999 0.6279999999999999
probs:  [0.06171184396912889, 0.06370195606324949, 0.1495807731046277, 0.07881835964137261, 0.4299073061735723, 0.21627976104804894]
actor:  1 policy actor:  1  step number:  33 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.1939800000000001 0.6279999999999999 0.6279999999999999
probs:  [0.06171184396912889, 0.06370195606324949, 0.1495807731046277, 0.07881835964137261, 0.4299073061735723, 0.21627976104804894]
maxi score, test score, baseline:  -0.1939800000000001 0.6279999999999999 0.6279999999999999
probs:  [0.06171184396912889, 0.06370195606324949, 0.1495807731046277, 0.07881835964137261, 0.4299073061735723, 0.21627976104804894]
maxi score, test score, baseline:  -0.1939800000000001 0.6279999999999999 0.6279999999999999
probs:  [0.06171184396912889, 0.06370195606324949, 0.1495807731046277, 0.07881835964137261, 0.4299073061735723, 0.21627976104804894]
actor:  1 policy actor:  1  step number:  22 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.1939800000000001 0.6279999999999999 0.6279999999999999
probs:  [0.06171029695596905, 0.06370035905335802, 0.14960212108672147, 0.07881638286856471, 0.4298965091439104, 0.2162743308914763]
maxi score, test score, baseline:  -0.1939800000000001 0.6279999999999999 0.6279999999999999
probs:  [0.06171029695596905, 0.06370035905335802, 0.14960212108672147, 0.07881638286856471, 0.4298965091439104, 0.2162743308914763]
maxi score, test score, baseline:  -0.1939800000000001 0.6279999999999999 0.6279999999999999
probs:  [0.06171029695596905, 0.06370035905335802, 0.14960212108672147, 0.07881638286856471, 0.4298965091439104, 0.2162743308914763]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  0.04999999999999938  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.037]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.037]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]]
printing an ep nov before normalisation:  52.09811006300516
actor:  1 policy actor:  1  step number:  26 total reward:  0.63  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.1939800000000001 0.6279999999999999 0.6279999999999999
probs:  [0.06169958860040271, 0.06374677212502891, 0.1496143765911388, 0.07885678527656757, 0.4298224022307203, 0.21626007517614174]
printing an ep nov before normalisation:  56.6364661240727
maxi score, test score, baseline:  -0.1939800000000001 0.6279999999999999 0.6279999999999999
probs:  [0.06169958860040271, 0.06374677212502891, 0.1496143765911388, 0.07885678527656757, 0.4298224022307203, 0.21626007517614174]
actor:  1 policy actor:  1  step number:  33 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
from probs:  [0.06169958860040271, 0.06374677212502891, 0.1496143765911388, 0.07885678527656757, 0.4298224022307203, 0.21626007517614174]
maxi score, test score, baseline:  -0.1939800000000001 0.6279999999999999 0.6279999999999999
probs:  [0.06169958860040271, 0.06374677212502891, 0.1496143765911388, 0.07885678527656757, 0.4298224022307203, 0.21626007517614174]
using explorer policy with actor:  0
actions average: 
K:  1  action  0 :  tensor([0.4521, 0.0029, 0.0915, 0.1069, 0.1651, 0.0965, 0.0850],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0039, 0.9626, 0.0044, 0.0107, 0.0010, 0.0012, 0.0162],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0563, 0.0016, 0.7187, 0.0520, 0.0533, 0.0603, 0.0579],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1020, 0.1192, 0.0969, 0.3846, 0.0793, 0.0796, 0.1385],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1470, 0.0006, 0.1269, 0.1521, 0.3312, 0.1270, 0.1153],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0869, 0.0238, 0.1317, 0.1418, 0.1399, 0.3906, 0.0853],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0796, 0.2019, 0.0971, 0.1655, 0.0950, 0.0942, 0.2667],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.1939800000000001 0.6279999999999999 0.6279999999999999
probs:  [0.06169958860040271, 0.06374677212502891, 0.1496143765911388, 0.07885678527656757, 0.4298224022307203, 0.21626007517614174]
Printing some Q and Qe and total Qs values:  [[0.87 ]
 [0.843]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [0.87 ]] [[41.468]
 [50.343]
 [41.468]
 [41.468]
 [41.468]
 [41.468]
 [41.468]] [[0.87 ]
 [0.843]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [0.87 ]]
actor:  0 policy actor:  0  step number:  28 total reward:  0.57  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.19084000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.06169958860040271, 0.06374677212502891, 0.1496143765911388, 0.07885678527656757, 0.4298224022307203, 0.21626007517614174]
actor:  0 policy actor:  1  step number:  30 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.18790000000000012 0.6279999999999999 0.6279999999999999
probs:  [0.06169958860040271, 0.06374677212502891, 0.1496143765911388, 0.07885678527656757, 0.4298224022307203, 0.21626007517614174]
printing an ep nov before normalisation:  50.28741151833309
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.01147870176285
Printing some Q and Qe and total Qs values:  [[0.624]
 [0.694]
 [0.627]
 [0.624]
 [0.636]
 [0.637]
 [0.652]] [[27.659]
 [26.82 ]
 [26.159]
 [26.984]
 [27.785]
 [27.634]
 [27.259]] [[2.198]
 [2.175]
 [2.033]
 [2.123]
 [2.225]
 [2.208]
 [2.181]]
actor:  1 policy actor:  1  step number:  24 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  34 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  49.8586378364623
maxi score, test score, baseline:  -0.18790000000000012 0.6279999999999999 0.6279999999999999
probs:  [0.06170067634268442, 0.06374775709344131, 0.1496110507853355, 0.07885701168352367, 0.42983010050986364, 0.21625340358515155]
maxi score, test score, baseline:  -0.18790000000000012 0.6279999999999999 0.6279999999999999
probs:  [0.06170067634268442, 0.06374775709344131, 0.1496110507853355, 0.07885701168352367, 0.42983010050986364, 0.21625340358515155]
printing an ep nov before normalisation:  44.72736013418041
printing an ep nov before normalisation:  41.78968661283599
maxi score, test score, baseline:  -0.18790000000000012 0.6279999999999999 0.6279999999999999
probs:  [0.06170067634268442, 0.06374775709344131, 0.1496110507853355, 0.07885701168352367, 0.42983010050986364, 0.21625340358515155]
actor:  1 policy actor:  1  step number:  35 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  40 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.18790000000000012 0.6279999999999999 0.6279999999999999
probs:  [0.06170067634268442, 0.06374775709344131, 0.1496110507853355, 0.07885701168352367, 0.42983010050986364, 0.21625340358515155]
siam score:  -0.8723446
maxi score, test score, baseline:  -0.18790000000000012 0.6279999999999999 0.6279999999999999
probs:  [0.06170067634268442, 0.06374775709344131, 0.1496110507853355, 0.07885701168352367, 0.42983010050986364, 0.21625340358515155]
printing an ep nov before normalisation:  31.456252640778935
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.601]
 [0.617]
 [0.589]
 [0.594]
 [0.594]
 [0.635]] [[ 7.964]
 [11.091]
 [13.216]
 [11.517]
 [12.515]
 [12.523]
 [13.349]] [[0.74 ]
 [0.866]
 [0.961]
 [0.87 ]
 [0.913]
 [0.912]
 [0.985]]
maxi score, test score, baseline:  -0.18790000000000012 0.6279999999999999 0.6279999999999999
actor:  1 policy actor:  1  step number:  26 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  34 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.141]
 [-0.012]
 [ 0.048]
 [-0.035]
 [-0.12 ]
 [-0.025]
 [-0.048]] [[56.779]
 [54.882]
 [61.853]
 [57.225]
 [57.527]
 [53.708]
 [61.307]] [[0.516]
 [0.615]
 [0.785]
 [0.629]
 [0.549]
 [0.583]
 [0.681]]
maxi score, test score, baseline:  -0.18790000000000012 0.6279999999999999 0.6279999999999999
probs:  [0.06171015603691629, 0.06375755198181741, 0.1496340662598386, 0.07871528355954384, 0.4298962620124546, 0.21628668014942926]
printing an ep nov before normalisation:  62.08493699969732
siam score:  -0.8687302
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.287]
 [0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.628]] [[31.583]
 [ 0.003]
 [31.583]
 [31.583]
 [31.583]
 [31.583]
 [31.583]] [[0.628]
 [0.287]
 [0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.628]]
actor:  1 policy actor:  1  step number:  32 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.18790000000000012 0.6279999999999999 0.6279999999999999
probs:  [0.06171015603691629, 0.06375755198181741, 0.1496340662598386, 0.07871528355954384, 0.4298962620124546, 0.21628668014942926]
actor:  0 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.18482000000000015 0.6279999999999999 0.6279999999999999
Printing some Q and Qe and total Qs values:  [[-0.002]
 [ 0.094]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.002]
 [ 0.094]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]]
maxi score, test score, baseline:  -0.18482000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.06171015603691629, 0.06375755198181741, 0.1496340662598386, 0.07871528355954384, 0.4298962620124546, 0.21628668014942926]
printing an ep nov before normalisation:  72.18017534220074
printing an ep nov before normalisation:  50.109089604659786
maxi score, test score, baseline:  -0.18482000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.06171015603691629, 0.06375755198181741, 0.1496340662598386, 0.07871528355954384, 0.4298962620124546, 0.21628668014942926]
actor:  1 policy actor:  1  step number:  27 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  31 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.654]
 [0.438]
 [0.433]
 [0.45 ]
 [0.461]
 [0.522]] [[31.078]
 [39.514]
 [31.239]
 [32.597]
 [34.745]
 [34.621]
 [31.078]] [[0.522]
 [0.654]
 [0.438]
 [0.433]
 [0.45 ]
 [0.461]
 [0.522]]
maxi score, test score, baseline:  -0.18482000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.06132142268535949, 0.06335589347960977, 0.15507406344135422, 0.07814333011176491, 0.42718318055464327, 0.21492210972726825]
actor:  0 policy actor:  1  step number:  23 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.18178000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.06132142268535949, 0.06335589347960977, 0.15507406344135422, 0.07814333011176491, 0.42718318055464327, 0.21492210972726825]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.18178000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.06132142268535949, 0.06335589347960977, 0.15507406344135422, 0.07814333011176491, 0.42718318055464327, 0.21492210972726825]
printing an ep nov before normalisation:  62.769921448421464
maxi score, test score, baseline:  -0.18178000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.06132142268535949, 0.06335589347960977, 0.15507406344135422, 0.07814333011176491, 0.42718318055464327, 0.21492210972726825]
printing an ep nov before normalisation:  0.0067788669618096264
actor:  0 policy actor:  0  step number:  34 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
from probs:  [0.06132142268535949, 0.06335589347960977, 0.15507406344135422, 0.07814333011176491, 0.42718318055464327, 0.21492210972726825]
printing an ep nov before normalisation:  48.697957953717705
printing an ep nov before normalisation:  63.37955506613837
Printing some Q and Qe and total Qs values:  [[0.655]
 [0.823]
 [0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.655]] [[40.41 ]
 [44.495]
 [40.41 ]
 [40.41 ]
 [40.41 ]
 [40.41 ]
 [40.41 ]] [[0.655]
 [0.823]
 [0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.655]]
printing an ep nov before normalisation:  53.18308726675553
maxi score, test score, baseline:  -0.17876000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.06132142268535949, 0.06335589347960977, 0.15507406344135422, 0.07814333011176491, 0.42718318055464327, 0.21492210972726825]
printing an ep nov before normalisation:  0.04714601232649329
actor:  1 policy actor:  1  step number:  23 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  2  action  0 :  tensor([0.4840, 0.0091, 0.1008, 0.0881, 0.0985, 0.0835, 0.1360],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0044, 0.9581, 0.0074, 0.0079, 0.0023, 0.0053, 0.0146],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0525, 0.0052, 0.7190, 0.0538, 0.0508, 0.0639, 0.0547],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1553, 0.0079, 0.1683, 0.2030, 0.1297, 0.1459, 0.1899],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1734, 0.0069, 0.1466, 0.1321, 0.2920, 0.1317, 0.1172],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1155, 0.1689, 0.1451, 0.1079, 0.0852, 0.2503, 0.1271],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1415, 0.0696, 0.1658, 0.1357, 0.1182, 0.1458, 0.2234],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.17876000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.061257736379373276, 0.06329008964216926, 0.15491279688683943, 0.0791021297864492, 0.4267386955749118, 0.2146985517302571]
actor:  1 policy actor:  1  step number:  28 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.17876000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.06125773637937327, 0.06329008964216924, 0.15491279688683948, 0.07910212978644919, 0.42673869557491173, 0.21469855173025712]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.17876000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.06125773637937327, 0.06329008964216924, 0.15491279688683948, 0.07910212978644919, 0.42673869557491173, 0.21469855173025712]
Printing some Q and Qe and total Qs values:  [[0.645]
 [0.613]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]] [[47.562]
 [52.628]
 [53.356]
 [53.356]
 [53.356]
 [53.356]
 [53.356]] [[2.18 ]
 [2.42 ]
 [2.456]
 [2.456]
 [2.456]
 [2.456]
 [2.456]]
maxi score, test score, baseline:  -0.17876000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.06125773637937327, 0.06329008964216924, 0.15491279688683948, 0.07910212978644919, 0.42673869557491173, 0.21469855173025712]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.172]
 [0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]] [[54.018]
 [67.802]
 [54.018]
 [54.018]
 [54.018]
 [54.018]
 [54.018]] [[0.304]
 [0.602]
 [0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]]
printing an ep nov before normalisation:  39.1353528973854
maxi score, test score, baseline:  -0.17876000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.06125773637937327, 0.06329008964216924, 0.15491279688683948, 0.07910212978644919, 0.42673869557491173, 0.21469855173025712]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.17876000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.06125971322937919, 0.06325981670981214, 0.15491780266923347, 0.07910468374847496, 0.42675249257759873, 0.2147054910655015]
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.17876000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.06125971322937919, 0.06325981670981214, 0.15491780266923347, 0.07910468374847496, 0.42675249257759873, 0.2147054910655015]
actor:  1 policy actor:  1  step number:  30 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.0
line 256 mcts: sample exp_bonus 41.832162473581356
maxi score, test score, baseline:  -0.17876000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.06125971322937919, 0.06325981670981214, 0.15491780266923347, 0.07910468374847496, 0.42675249257759873, 0.2147054910655015]
actions average: 
K:  2  action  0 :  tensor([0.5056, 0.0030, 0.0806, 0.0934, 0.1300, 0.0761, 0.1114],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0052, 0.9479, 0.0101, 0.0097, 0.0014, 0.0025, 0.0232],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0833, 0.0193, 0.5244, 0.1004, 0.0864, 0.0902, 0.0961],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1469, 0.0735, 0.1657, 0.1598, 0.1818, 0.1263, 0.1459],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1484, 0.0140, 0.0709, 0.0865, 0.5306, 0.0668, 0.0829],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0760, 0.1469, 0.1631, 0.0851, 0.0570, 0.3979, 0.0741],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0888, 0.1691, 0.1190, 0.1140, 0.1196, 0.0726, 0.3167],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.17570000000000013 0.6279999999999999 0.6279999999999999
probs:  [0.06125971322937919, 0.06325981670981214, 0.15491780266923347, 0.07910468374847496, 0.42675249257759873, 0.2147054910655015]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  28.206788223394508
printing an ep nov before normalisation:  66.28946746384113
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.17570000000000013 0.6279999999999999 0.6279999999999999
printing an ep nov before normalisation:  54.80171810201233
from probs:  [0.06125971322937919, 0.06325981670981214, 0.15491780266923347, 0.07910468374847496, 0.42675249257759873, 0.2147054910655015]
maxi score, test score, baseline:  -0.17570000000000013 0.6279999999999999 0.6279999999999999
probs:  [0.06125971322937919, 0.06325981670981214, 0.15491780266923347, 0.07910468374847496, 0.42675249257759873, 0.2147054910655015]
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  32 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  59.48658548980037
maxi score, test score, baseline:  -0.17570000000000013 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.17570000000000013 0.6279999999999999 0.6279999999999999
probs:  [0.06125971322937919, 0.06325981670981214, 0.15491780266923347, 0.07910468374847496, 0.42675249257759873, 0.2147054910655015]
from probs:  [0.06125971322937919, 0.06325981670981214, 0.15491780266923347, 0.07910468374847496, 0.42675249257759873, 0.2147054910655015]
siam score:  -0.87210464
actor:  0 policy actor:  0  step number:  38 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.17284000000000013 0.6279999999999999 0.6279999999999999
probs:  [0.06125971322937919, 0.06325981670981214, 0.15491780266923347, 0.07910468374847496, 0.42675249257759873, 0.2147054910655015]
maxi score, test score, baseline:  -0.17284000000000013 0.6279999999999999 0.6279999999999999
probs:  [0.06125971322937919, 0.06325981670981214, 0.15491780266923347, 0.07910468374847496, 0.42675249257759873, 0.2147054910655015]
siam score:  -0.8721916
actor:  1 policy actor:  1  step number:  29 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
from probs:  [0.06125971322937919, 0.06325981670981214, 0.15491780266923347, 0.07910468374847496, 0.42675249257759873, 0.2147054910655015]
printing an ep nov before normalisation:  45.98146017376717
actor:  0 policy actor:  1  step number:  44 total reward:  0.22999999999999954  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.17038000000000014 0.6279999999999999 0.6279999999999999
probs:  [0.06125971322937919, 0.06325981670981214, 0.15491780266923347, 0.07910468374847496, 0.42675249257759873, 0.2147054910655015]
actor:  0 policy actor:  0  step number:  21 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.16722000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.06125971322937919, 0.06325981670981214, 0.15491780266923347, 0.07910468374847496, 0.42675249257759873, 0.2147054910655015]
maxi score, test score, baseline:  -0.16722000000000015 0.6279999999999999 0.6279999999999999
printing an ep nov before normalisation:  46.05936760759059
printing an ep nov before normalisation:  50.70991709834384
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.573]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]] [[49.753]
 [43.96 ]
 [49.753]
 [49.753]
 [49.753]
 [49.753]
 [49.753]] [[2.294]
 [2.157]
 [2.294]
 [2.294]
 [2.294]
 [2.294]
 [2.294]]
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.586]
 [0.527]
 [0.504]
 [0.51 ]
 [0.502]
 [0.505]] [[38.673]
 [41.028]
 [36.834]
 [37.162]
 [38.074]
 [38.486]
 [37.071]] [[1.797]
 [1.878]
 [1.589]
 [1.584]
 [1.64 ]
 [1.654]
 [1.58 ]]
maxi score, test score, baseline:  -0.16722000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.06125971322937919, 0.06325981670981214, 0.15491780266923347, 0.07910468374847496, 0.42675249257759873, 0.2147054910655015]
from probs:  [0.06125971322937919, 0.06325981670981214, 0.15491780266923347, 0.07910468374847496, 0.42675249257759873, 0.2147054910655015]
actor:  1 policy actor:  1  step number:  44 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.16722000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.06125971322937919, 0.06325981670981214, 0.15491780266923347, 0.07910468374847496, 0.42675249257759873, 0.2147054910655015]
Printing some Q and Qe and total Qs values:  [[0.678]
 [0.631]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]] [[35.395]
 [42.881]
 [35.395]
 [35.395]
 [35.395]
 [35.395]
 [35.395]] [[0.678]
 [0.631]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]]
maxi score, test score, baseline:  -0.16722000000000015 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.16722000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.06125971322937919, 0.06325981670981214, 0.15491780266923347, 0.07910468374847496, 0.42675249257759873, 0.2147054910655015]
maxi score, test score, baseline:  -0.16722000000000015 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.16722000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.06125971322937919, 0.06325981670981214, 0.15491780266923347, 0.07910468374847496, 0.42675249257759873, 0.2147054910655015]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.16722000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.06125971322937919, 0.06325981670981214, 0.15491780266923347, 0.07910468374847496, 0.42675249257759873, 0.2147054910655015]
actor:  1 policy actor:  1  step number:  42 total reward:  0.20999999999999963  reward:  1.0 rdn_beta:  0.333
from probs:  [0.06125971322937919, 0.06325981670981214, 0.15491780266923347, 0.07910468374847496, 0.42675249257759873, 0.2147054910655015]
printing an ep nov before normalisation:  26.231138706207275
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.16722000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.061248259532194566, 0.06330466957415495, 0.15493131796605314, 0.07914411930427095, 0.42667306847736564, 0.21469856514596058]
printing an ep nov before normalisation:  45.52208362136396
maxi score, test score, baseline:  -0.16722000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.061248259532194566, 0.06330466957415495, 0.15493131796605314, 0.07914411930427095, 0.42667306847736564, 0.21469856514596058]
printing an ep nov before normalisation:  34.91036327407626
printing an ep nov before normalisation:  42.37963206715192
actor:  1 policy actor:  1  step number:  37 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  25 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
siam score:  -0.86559516
maxi score, test score, baseline:  -0.16628000000000012 0.6279999999999999 0.6279999999999999
probs:  [0.061248259532194566, 0.06330466957415495, 0.15493131796605314, 0.07914411930427095, 0.42667306847736564, 0.21469856514596058]
siam score:  -0.86541826
maxi score, test score, baseline:  -0.16628000000000012 0.6279999999999999 0.6279999999999999
probs:  [0.061248259532194566, 0.06330466957415495, 0.15493131796605314, 0.07914411930427095, 0.42667306847736564, 0.21469856514596058]
actor:  1 policy actor:  1  step number:  28 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  51.29957579167881
actions average: 
K:  1  action  0 :  tensor([0.3946, 0.0362, 0.1288, 0.1481, 0.1094, 0.0822, 0.1007],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0046, 0.9722, 0.0052, 0.0061, 0.0013, 0.0015, 0.0090],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0604,     0.0006,     0.6796,     0.0735,     0.0542,     0.0768,
            0.0548], grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0792, 0.0799, 0.1002, 0.4809, 0.0666, 0.0786, 0.1145],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1971, 0.0020, 0.1071, 0.1247, 0.3612, 0.1035, 0.1043],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([    0.0740,     0.0005,     0.1338,     0.0982,     0.0683,     0.5405,
            0.0847], grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1441, 0.1819, 0.1365, 0.1363, 0.1048, 0.0924, 0.2039],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  31 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.0
from probs:  [0.061248259532194566, 0.06330466957415495, 0.15493131796605314, 0.07914411930427095, 0.42667306847736564, 0.21469856514596058]
actions average: 
K:  3  action  0 :  tensor([0.5384, 0.0877, 0.0792, 0.0808, 0.0793, 0.0595, 0.0752],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0017,     0.9926,     0.0019,     0.0012,     0.0007,     0.0009,
            0.0011], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0834,     0.0003,     0.6363,     0.0718,     0.0748,     0.0688,
            0.0646], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1253, 0.0161, 0.1468, 0.3454, 0.1062, 0.1356, 0.1247],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1398, 0.0021, 0.0425, 0.0526, 0.6700, 0.0388, 0.0543],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1481, 0.0055, 0.1758, 0.1561, 0.1359, 0.2396, 0.1391],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1269, 0.0899, 0.1633, 0.1043, 0.0905, 0.1051, 0.3199],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  17 total reward:  0.78  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.16272000000000011 0.6279999999999999 0.6279999999999999
probs:  [0.061248259532194566, 0.06330466957415495, 0.15493131796605314, 0.07914411930427095, 0.42667306847736564, 0.21469856514596058]
maxi score, test score, baseline:  -0.16272000000000011 0.6279999999999999 0.6279999999999999
probs:  [0.061248259532194566, 0.06330466957415495, 0.15493131796605314, 0.07914411930427095, 0.42667306847736564, 0.21469856514596058]
actor:  1 policy actor:  1  step number:  30 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.16272000000000011 0.6279999999999999 0.6279999999999999
probs:  [0.061248259532194566, 0.06330466957415495, 0.15493131796605314, 0.07914411930427095, 0.42667306847736564, 0.21469856514596058]
line 256 mcts: sample exp_bonus 48.73264039888023
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.704]
 [0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]] [[40.979]
 [45.429]
 [40.979]
 [40.979]
 [40.979]
 [40.979]
 [40.979]] [[0.514]
 [0.704]
 [0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]]
maxi score, test score, baseline:  -0.16272000000000011 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.16272000000000011 0.6279999999999999 0.6279999999999999
probs:  [0.061248259532194566, 0.06330466957415495, 0.15493131796605314, 0.07914411930427095, 0.42667306847736564, 0.21469856514596058]
maxi score, test score, baseline:  -0.16272000000000011 0.6279999999999999 0.6279999999999999
actor:  0 policy actor:  1  step number:  31 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  32 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  51.06976992613942
actor:  1 policy actor:  1  step number:  30 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.15992000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.06125301727085445, 0.06330958740209418, 0.15494336882865664, 0.07907248353588059, 0.42670627413892936, 0.21471526882358466]
Printing some Q and Qe and total Qs values:  [[-0.074]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.074]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.074]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.074]]
printing an ep nov before normalisation:  20.502201369602517
maxi score, test score, baseline:  -0.15992000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.06125301727085445, 0.06330958740209418, 0.15494336882865664, 0.07907248353588059, 0.42670627413892936, 0.21471526882358466]
maxi score, test score, baseline:  -0.15992000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.06125301727085445, 0.06330958740209418, 0.15494336882865664, 0.07907248353588059, 0.42670627413892936, 0.21471526882358466]
actor:  1 policy actor:  1  step number:  26 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.15992000000000015 0.6279999999999999 0.6279999999999999
maxi score, test score, baseline:  -0.15992000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.06138555036865401, 0.06342979490995006, 0.15451438990338554, 0.07909821967555003, 0.42764398466618314, 0.21392806047627722]
printing an ep nov before normalisation:  48.115134614367044
maxi score, test score, baseline:  -0.15992000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.06138555036865401, 0.06342979490995006, 0.15451438990338554, 0.07909821967555003, 0.42764398466618314, 0.21392806047627722]
printing an ep nov before normalisation:  47.65590212984909
maxi score, test score, baseline:  -0.15992000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.06138555036865401, 0.06342979490995006, 0.15451438990338554, 0.07909821967555003, 0.42764398466618314, 0.21392806047627722]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.064]
 [-0.046]
 [-0.066]
 [-0.06 ]
 [-0.065]
 [-0.065]
 [-0.057]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.064]
 [-0.046]
 [-0.066]
 [-0.06 ]
 [-0.065]
 [-0.065]
 [-0.057]]
Printing some Q and Qe and total Qs values:  [[-0.008]
 [-0.004]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]] [[70.438]
 [62.729]
 [70.438]
 [70.438]
 [70.438]
 [70.438]
 [70.438]] [[1.992]
 [1.674]
 [1.992]
 [1.992]
 [1.992]
 [1.992]
 [1.992]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.15992000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.06138751085408595, 0.06339983969863411, 0.15451933113668825, 0.07910074708527291, 0.42765766787954385, 0.21393490334577492]
actor:  1 policy actor:  1  step number:  30 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.15992000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.06137618098154232, 0.06344399187598317, 0.15453278376531082, 0.07913960938705578, 0.4275790960296979, 0.21392833796040994]
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.435]
 [0.562]
 [0.302]
 [0.435]
 [0.435]
 [0.197]] [[36.729]
 [36.729]
 [43.697]
 [38.921]
 [36.729]
 [36.729]
 [36.53 ]] [[1.229]
 [1.229]
 [1.629]
 [1.182]
 [1.229]
 [1.229]
 [0.983]]
Printing some Q and Qe and total Qs values:  [[0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]]
maxi score, test score, baseline:  -0.15992000000000015 0.6279999999999999 0.6279999999999999
probs:  [0.06137618098154232, 0.06344399187598317, 0.15453278376531082, 0.07913960938705578, 0.4275790960296979, 0.21392833796040994]
Starting evaluation
printing an ep nov before normalisation:  33.80525469312581
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.555]
 [0.805]
 [0.555]
 [0.555]
 [0.555]
 [0.555]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.555]
 [0.555]
 [0.805]
 [0.555]
 [0.555]
 [0.555]
 [0.555]]
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.783]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]] [[50.408]
 [45.013]
 [50.408]
 [50.408]
 [50.408]
 [50.408]
 [50.408]] [[0.617]
 [0.783]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]]
maxi score, test score, baseline:  -0.15992000000000015 0.6279999999999999 0.6279999999999999
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  47.15474933976123
printing an ep nov before normalisation:  50.65715789794922
printing an ep nov before normalisation:  36.068663597106934
Printing some Q and Qe and total Qs values:  [[0.965]
 [0.962]
 [0.942]
 [0.942]
 [0.942]
 [0.942]
 [0.942]] [[42.271]
 [40.577]
 [44.478]
 [44.478]
 [44.478]
 [44.478]
 [44.478]] [[0.965]
 [0.962]
 [0.942]
 [0.942]
 [0.942]
 [0.942]
 [0.942]]
printing an ep nov before normalisation:  35.80365899464041
printing an ep nov before normalisation:  35.808546837013886
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  38.354551681437684
actor:  1 policy actor:  1  step number:  40 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  19 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  2
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.10310000000000012 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.10310000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07143374879501939, 0.07143374879501939, 0.1398055493105159, 0.07143374879501939, 0.4918462149242354, 0.15404698938019043]
maxi score, test score, baseline:  -0.10310000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07143374879501939, 0.07143374879501939, 0.1398055493105159, 0.07143374879501939, 0.4918462149242354, 0.15404698938019043]
actor:  1 policy actor:  1  step number:  43 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.10310000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07143374879501939, 0.07143374879501939, 0.1398055493105159, 0.07143374879501939, 0.4918462149242354, 0.15404698938019043]
maxi score, test score, baseline:  -0.10310000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07143374879501939, 0.07143374879501939, 0.1398055493105159, 0.07143374879501939, 0.4918462149242354, 0.15404698938019043]
printing an ep nov before normalisation:  52.05208282384809
maxi score, test score, baseline:  -0.10310000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07143374879501939, 0.07143374879501939, 0.1398055493105159, 0.07143374879501939, 0.4918462149242354, 0.15404698938019043]
Printing some Q and Qe and total Qs values:  [[0.136]
 [0.479]
 [0.16 ]
 [0.143]
 [0.09 ]
 [0.127]
 [0.118]] [[32.248]
 [41.902]
 [33.905]
 [33.438]
 [33.009]
 [32.529]
 [32.943]] [[0.783]
 [1.556]
 [0.88 ]
 [0.842]
 [0.77 ]
 [0.786]
 [0.796]]
Printing some Q and Qe and total Qs values:  [[0.57 ]
 [0.571]
 [0.535]
 [0.519]
 [0.519]
 [0.531]
 [0.535]] [[40.549]
 [42.05 ]
 [44.352]
 [40.261]
 [42.536]
 [41.62 ]
 [44.352]] [[2.191]
 [2.291]
 [2.408]
 [2.12 ]
 [2.272]
 [2.223]
 [2.408]]
maxi score, test score, baseline:  -0.10310000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07143374879501939, 0.07143374879501939, 0.1398055493105159, 0.07143374879501939, 0.4918462149242354, 0.15404698938019043]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.10310000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07143374879501939, 0.07143374879501939, 0.1398055493105159, 0.07143374879501939, 0.4918462149242354, 0.15404698938019043]
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.381]
 [0.572]
 [0.381]
 [0.381]
 [0.381]
 [0.381]] [[54.702]
 [54.702]
 [57.466]
 [54.702]
 [54.702]
 [54.702]
 [54.702]] [[1.721]
 [1.721]
 [2.017]
 [1.721]
 [1.721]
 [1.721]
 [1.721]]
maxi score, test score, baseline:  -0.1031000000000001 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.1031000000000001 0.6789999999999999 0.6789999999999999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.1031000000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07143374879501939, 0.07143374879501939, 0.1398055493105159, 0.07143374879501939, 0.4918462149242354, 0.15404698938019043]
printing an ep nov before normalisation:  22.275924554094203
actor:  1 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.1031000000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07143374879501939, 0.07143374879501939, 0.1398055493105159, 0.07143374879501939, 0.4918462149242354, 0.15404698938019043]
printing an ep nov before normalisation:  38.319819745013355
actor:  1 policy actor:  1  step number:  45 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  53.061041030325036
maxi score, test score, baseline:  -0.1031000000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07143374879501939, 0.07143374879501939, 0.1398055493105159, 0.07143374879501939, 0.4918462149242354, 0.15404698938019043]
maxi score, test score, baseline:  -0.1031000000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07143374879501939, 0.07143374879501939, 0.1398055493105159, 0.07143374879501939, 0.4918462149242354, 0.15404698938019043]
printing an ep nov before normalisation:  33.57161780626665
maxi score, test score, baseline:  -0.1031000000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07143374879501939, 0.07143374879501939, 0.1398055493105159, 0.07143374879501939, 0.4918462149242354, 0.15404698938019043]
actor:  1 policy actor:  1  step number:  33 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.1031000000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07143374879501939, 0.07143374879501939, 0.1398055493105159, 0.07143374879501939, 0.4918462149242354, 0.15404698938019043]
maxi score, test score, baseline:  -0.1031000000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07143374879501939, 0.07143374879501939, 0.1398055493105159, 0.07143374879501939, 0.4918462149242354, 0.15404698938019043]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.067]
 [0.238]
 [0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.067]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.067]
 [0.238]
 [0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.067]]
printing an ep nov before normalisation:  43.397746086120605
Printing some Q and Qe and total Qs values:  [[0.588]
 [0.652]
 [0.622]
 [0.485]
 [0.47 ]
 [0.669]
 [0.583]] [[50.679]
 [53.434]
 [54.252]
 [51.921]
 [51.316]
 [51.939]
 [50.932]] [[2.028]
 [2.212]
 [2.218]
 [1.979]
 [1.937]
 [2.163]
 [2.034]]
maxi score, test score, baseline:  -0.1031000000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07143374879501939, 0.07143374879501939, 0.1398055493105159, 0.07143374879501939, 0.4918462149242354, 0.15404698938019043]
maxi score, test score, baseline:  -0.1031000000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07143374879501939, 0.07143374879501939, 0.1398055493105159, 0.07143374879501939, 0.4918462149242354, 0.15404698938019043]
actor:  1 policy actor:  1  step number:  31 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.1031000000000001 0.6789999999999999 0.6789999999999999
actor:  1 policy actor:  1  step number:  26 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.1031000000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07143374879501939, 0.07143374879501939, 0.1398055493105159, 0.07143374879501939, 0.4918462149242354, 0.15404698938019043]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.1031000000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07143374879501939, 0.07143374879501939, 0.1398055493105159, 0.07143374879501939, 0.4918462149242354, 0.15404698938019043]
printing an ep nov before normalisation:  47.12891606757057
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.1031000000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07143374879501939, 0.07143374879501939, 0.1398055493105159, 0.07143374879501939, 0.4918462149242354, 0.15404698938019043]
actor:  0 policy actor:  0  step number:  25 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.1026200000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07143374879501939, 0.07143374879501939, 0.1398055493105159, 0.07143374879501939, 0.4918462149242354, 0.15404698938019043]
maxi score, test score, baseline:  -0.1026200000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07143374879501939, 0.07143374879501939, 0.1398055493105159, 0.07143374879501939, 0.4918462149242354, 0.15404698938019043]
maxi score, test score, baseline:  -0.1026200000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07143374879501939, 0.07143374879501939, 0.1398055493105159, 0.07143374879501939, 0.4918462149242354, 0.15404698938019043]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.1026200000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07143374879501939, 0.07143374879501939, 0.1398055493105159, 0.07143374879501939, 0.4918462149242354, 0.15404698938019043]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.1026200000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07143374879501939, 0.07143374879501939, 0.1398055493105159, 0.07143374879501939, 0.4918462149242354, 0.15404698938019043]
printing an ep nov before normalisation:  65.67340607957698
maxi score, test score, baseline:  -0.1026200000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07143374879501939, 0.07143374879501939, 0.1398055493105159, 0.07143374879501939, 0.4918462149242354, 0.15404698938019043]
maxi score, test score, baseline:  -0.1026200000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07143374879501939, 0.07143374879501939, 0.1398055493105159, 0.07143374879501939, 0.4918462149242354, 0.15404698938019043]
maxi score, test score, baseline:  -0.1026200000000001 0.6789999999999999 0.6789999999999999
actor:  1 policy actor:  1  step number:  21 total reward:  0.7  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8642351
maxi score, test score, baseline:  -0.1026200000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.10262000000000009 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
printing an ep nov before normalisation:  46.297286601343785
maxi score, test score, baseline:  -0.10262000000000009 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
printing an ep nov before normalisation:  49.37706546027284
maxi score, test score, baseline:  -0.10262000000000009 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
printing an ep nov before normalisation:  62.136468271291385
actor:  1 policy actor:  1  step number:  34 total reward:  0.2699999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.10262000000000009 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actor:  0 policy actor:  1  step number:  31 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.333
using another actor
printing an ep nov before normalisation:  57.667949980275345
printing an ep nov before normalisation:  62.87233263514622
maxi score, test score, baseline:  -0.0998600000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.0998600000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.716]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]] [[45.275]
 [54.19 ]
 [45.275]
 [45.275]
 [45.275]
 [45.275]
 [45.275]] [[1.591]
 [1.916]
 [1.591]
 [1.591]
 [1.591]
 [1.591]
 [1.591]]
actor:  0 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.09676000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
line 256 mcts: sample exp_bonus 65.19249670628231
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.666]
 [0.597]
 [0.58 ]
 [0.587]
 [0.55 ]
 [0.547]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.581]
 [0.666]
 [0.597]
 [0.58 ]
 [0.587]
 [0.55 ]
 [0.547]]
maxi score, test score, baseline:  -0.09676000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
printing an ep nov before normalisation:  43.93521293816332
actor:  1 policy actor:  1  step number:  24 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.09676000000000011 0.6789999999999999 0.6789999999999999
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.0967600000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.0967600000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
printing an ep nov before normalisation:  55.79910957754483
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  27 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.0967600000000001 0.6789999999999999 0.6789999999999999
Printing some Q and Qe and total Qs values:  [[0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.537]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.537]]
printing an ep nov before normalisation:  48.095581025509105
actor:  0 policy actor:  0  step number:  26 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.09370000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
printing an ep nov before normalisation:  46.41302527964149
Printing some Q and Qe and total Qs values:  [[ 0.006]
 [ 0.19 ]
 [-0.004]
 [-0.006]
 [-0.015]
 [-0.011]
 [-0.023]] [[32.906]
 [41.582]
 [31.675]
 [32.626]
 [33.945]
 [31.296]
 [31.109]] [[0.632]
 [1.116]
 [0.579]
 [0.611]
 [0.647]
 [0.559]
 [0.541]]
maxi score, test score, baseline:  -0.09370000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
printing an ep nov before normalisation:  47.94236421585083
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]] [[55.834]
 [55.834]
 [55.834]
 [55.834]
 [55.834]
 [55.834]
 [55.834]] [[1.207]
 [1.207]
 [1.207]
 [1.207]
 [1.207]
 [1.207]
 [1.207]]
maxi score, test score, baseline:  -0.09370000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actor:  1 policy actor:  1  step number:  40 total reward:  0.2899999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.09370000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
siam score:  -0.86271214
maxi score, test score, baseline:  -0.09370000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.09370000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.09370000000000012 0.6789999999999999 0.6789999999999999
actor:  1 policy actor:  1  step number:  36 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.09370000000000012 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.09370000000000012 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.09370000000000012 0.6789999999999999 0.6789999999999999
printing an ep nov before normalisation:  65.51080723735369
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.635]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]] [[38.688]
 [43.799]
 [38.688]
 [38.688]
 [38.688]
 [38.688]
 [38.688]] [[1.505]
 [1.913]
 [1.505]
 [1.505]
 [1.505]
 [1.505]
 [1.505]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]] [[34.872]
 [34.872]
 [34.872]
 [34.872]
 [34.872]
 [34.872]
 [34.872]] [[1.319]
 [1.319]
 [1.319]
 [1.319]
 [1.319]
 [1.319]
 [1.319]]
printing an ep nov before normalisation:  42.107086181640625
maxi score, test score, baseline:  -0.09370000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
printing an ep nov before normalisation:  39.772078610859
actor:  1 policy actor:  1  step number:  29 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.61 ]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]] [[66.744]
 [70.034]
 [66.744]
 [66.744]
 [66.744]
 [66.744]
 [66.744]] [[1.518]
 [1.64 ]
 [1.518]
 [1.518]
 [1.518]
 [1.518]
 [1.518]]
maxi score, test score, baseline:  -0.09370000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.09370000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actions average: 
K:  1  action  0 :  tensor([0.4787, 0.0128, 0.1336, 0.1066, 0.0864, 0.0897, 0.0922],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0048, 0.9422, 0.0064, 0.0138, 0.0037, 0.0052, 0.0239],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0713, 0.0034, 0.6363, 0.0779, 0.0709, 0.0639, 0.0764],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0988, 0.0034, 0.1049, 0.3915, 0.1104, 0.1456, 0.1455],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1557, 0.0061, 0.1488, 0.1517, 0.2265, 0.1547, 0.1566],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1201, 0.0424, 0.2019, 0.1807, 0.1229, 0.1846, 0.1473],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1536, 0.0165, 0.2050, 0.1459, 0.1259, 0.1644, 0.1889],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.09370000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
siam score:  -0.84945875
maxi score, test score, baseline:  -0.09370000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
printing an ep nov before normalisation:  47.30969794424475
printing an ep nov before normalisation:  41.09800185562451
printing an ep nov before normalisation:  40.607662200927734
siam score:  -0.8495682
actor:  1 policy actor:  1  step number:  32 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.09370000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
siam score:  -0.8474829
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  56.70795945837891
printing an ep nov before normalisation:  54.45276923764624
printing an ep nov before normalisation:  45.1726799499292
maxi score, test score, baseline:  -0.09370000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actor:  0 policy actor:  0  step number:  26 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  26 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.09068000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
printing an ep nov before normalisation:  40.12977365707765
printing an ep nov before normalisation:  1.9752619095925184e-05
maxi score, test score, baseline:  -0.09068000000000011 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.09068000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
printing an ep nov before normalisation:  46.931761208836846
maxi score, test score, baseline:  -0.09068000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
printing an ep nov before normalisation:  44.474124664234054
maxi score, test score, baseline:  -0.09068000000000011 0.6789999999999999 0.6789999999999999
actor:  0 policy actor:  1  step number:  23 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.0875600000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.0875600000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.0875600000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actor:  0 policy actor:  0  step number:  25 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  22 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.08676000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.08676000000000013 0.6789999999999999 0.6789999999999999
Printing some Q and Qe and total Qs values:  [[0.683]
 [0.683]
 [0.726]
 [0.683]
 [0.683]
 [0.683]
 [0.683]] [[36.394]
 [36.394]
 [35.704]
 [36.394]
 [36.394]
 [36.394]
 [36.394]] [[2.127]
 [2.127]
 [2.113]
 [2.127]
 [2.127]
 [2.127]
 [2.127]]
maxi score, test score, baseline:  -0.08676000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actor:  1 policy actor:  1  step number:  26 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.71 ]
 [0.787]
 [0.725]
 [0.704]
 [0.713]
 [0.712]
 [0.705]] [[32.138]
 [30.356]
 [31.625]
 [32.48 ]
 [32.084]
 [32.542]
 [32.628]] [[1.993]
 [1.927]
 [1.966]
 [2.014]
 [1.991]
 [2.027]
 [2.026]]
maxi score, test score, baseline:  -0.08676000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.765]
 [0.841]
 [0.78 ]
 [0.78 ]
 [0.78 ]
 [0.78 ]
 [0.78 ]] [[34.421]
 [39.201]
 [37.434]
 [37.434]
 [37.434]
 [37.434]
 [37.434]] [[0.765]
 [0.841]
 [0.78 ]
 [0.78 ]
 [0.78 ]
 [0.78 ]
 [0.78 ]]
Printing some Q and Qe and total Qs values:  [[0.942]
 [0.915]
 [0.942]
 [0.942]
 [0.942]
 [0.942]
 [0.942]] [[38.211]
 [40.287]
 [38.211]
 [38.211]
 [38.211]
 [38.211]
 [38.211]] [[0.942]
 [0.915]
 [0.942]
 [0.942]
 [0.942]
 [0.942]
 [0.942]]
maxi score, test score, baseline:  -0.08676000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.08676000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actor:  0 policy actor:  1  step number:  32 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.667
using another actor
maxi score, test score, baseline:  -0.08394000000000011 0.6789999999999999 0.6789999999999999
using explorer policy with actor:  0
printing an ep nov before normalisation:  58.588762770553316
maxi score, test score, baseline:  -0.08394000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.757]
 [0.757]
 [0.757]
 [0.755]
 [0.73 ]
 [0.731]
 [0.757]] [[56.168]
 [56.168]
 [56.168]
 [54.717]
 [56.535]
 [56.738]
 [56.168]] [[2.292]
 [2.292]
 [2.292]
 [2.227]
 [2.281]
 [2.292]
 [2.292]]
maxi score, test score, baseline:  -0.08394000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.08394000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actor:  1 policy actor:  1  step number:  34 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  34.13677846580393
actions average: 
K:  1  action  0 :  tensor([0.3939, 0.0260, 0.1137, 0.0934, 0.1122, 0.1320, 0.1288],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0058, 0.9618, 0.0054, 0.0104, 0.0023, 0.0027, 0.0116],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0793, 0.0120, 0.5473, 0.0929, 0.0722, 0.0914, 0.1048],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1421, 0.0384, 0.1237, 0.3144, 0.1206, 0.1364, 0.1244],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2055, 0.0140, 0.1021, 0.0942, 0.4260, 0.0828, 0.0753],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1333, 0.0015, 0.1711, 0.1363, 0.1404, 0.2836, 0.1338],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1486, 0.1030, 0.0963, 0.1026, 0.0992, 0.1096, 0.3408],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  33 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
using another actor
maxi score, test score, baseline:  -0.08394000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.08394000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.08394000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
Printing some Q and Qe and total Qs values:  [[-0.047]
 [-0.011]
 [-0.24 ]
 [-0.033]
 [-0.056]
 [-0.252]
 [-0.03 ]] [[44.91 ]
 [50.271]
 [40.697]
 [36.668]
 [38.336]
 [35.844]
 [38.483]] [[ 0.127]
 [ 0.198]
 [-0.092]
 [ 0.089]
 [ 0.076]
 [-0.136]
 [ 0.103]]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.0839400000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
from probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.0839400000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.0839400000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.0839400000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.0839400000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actor:  1 policy actor:  1  step number:  30 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.0839400000000001 0.6789999999999999 0.6789999999999999
printing an ep nov before normalisation:  45.73825485661194
maxi score, test score, baseline:  -0.0839400000000001 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.0839400000000001 0.6789999999999999 0.6789999999999999
printing an ep nov before normalisation:  29.60073471069336
actor:  1 policy actor:  1  step number:  35 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  66.07872767364606
maxi score, test score, baseline:  -0.0839400000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
printing an ep nov before normalisation:  57.30323374694412
actor:  0 policy actor:  0  step number:  21 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.0807800000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.0807800000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
printing an ep nov before normalisation:  51.48915406455016
maxi score, test score, baseline:  -0.0807800000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
printing an ep nov before normalisation:  49.91741462539333
maxi score, test score, baseline:  -0.0807800000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  30 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.0807800000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.0807800000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.0807800000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.0807800000000001 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.0807800000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
printing an ep nov before normalisation:  40.16775106864199
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.313]
 [0.541]
 [0.313]
 [0.313]
 [0.313]
 [0.253]] [[41.504]
 [41.504]
 [39.914]
 [41.504]
 [41.504]
 [41.504]
 [50.28 ]] [[0.514]
 [0.514]
 [0.729]
 [0.514]
 [0.514]
 [0.514]
 [0.523]]
maxi score, test score, baseline:  -0.0807800000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.0807800000000001 0.6789999999999999 0.6789999999999999
actor:  0 policy actor:  0  step number:  20 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.226]
 [0.382]
 [0.226]
 [0.226]
 [0.226]
 [0.226]
 [0.226]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.226]
 [0.382]
 [0.226]
 [0.226]
 [0.226]
 [0.226]
 [0.226]]
actor:  1 policy actor:  1  step number:  31 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.0775200000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.07752000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actor:  1 policy actor:  1  step number:  22 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.07752000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.07752000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.07752000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actor:  1 policy actor:  1  step number:  27 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.07752000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
printing an ep nov before normalisation:  47.247012801577704
maxi score, test score, baseline:  -0.07752000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actor:  0 policy actor:  0  step number:  25 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.07448000000000012 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.07448000000000012 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.07448000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
printing an ep nov before normalisation:  45.43310416210205
UNIT TEST: sample policy line 217 mcts : [0.061 0.245 0.122 0.163 0.082 0.082 0.245]
Printing some Q and Qe and total Qs values:  [[-0.024]
 [-0.003]
 [-0.024]
 [-0.024]
 [-0.031]
 [-0.024]
 [-0.007]] [[ 0.   ]
 [58.038]
 [ 0.   ]
 [ 0.   ]
 [46.163]
 [ 0.   ]
 [49.742]] [[-0.133]
 [ 0.472]
 [-0.133]
 [-0.133]
 [ 0.325]
 [-0.133]
 [ 0.386]]
actor:  1 policy actor:  1  step number:  42 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.07448000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.07448000000000012 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.07448000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actor:  1 policy actor:  1  step number:  36 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.627]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]] [[73.555]
 [87.767]
 [87.767]
 [87.767]
 [87.767]
 [87.767]
 [87.767]] [[1.821]
 [2.112]
 [2.112]
 [2.112]
 [2.112]
 [2.112]
 [2.112]]
maxi score, test score, baseline:  -0.07448000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actor:  0 policy actor:  0  step number:  36 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  54.972597888737205
using explorer policy with actor:  1
printing an ep nov before normalisation:  27.985777854919434
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.07178000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
Printing some Q and Qe and total Qs values:  [[-0.02 ]
 [-0.01 ]
 [-0.02 ]
 [-0.02 ]
 [-0.043]
 [-0.02 ]
 [-0.02 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.02 ]
 [-0.01 ]
 [-0.02 ]
 [-0.02 ]
 [-0.043]
 [-0.02 ]
 [-0.02 ]]
maxi score, test score, baseline:  -0.07178000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
printing an ep nov before normalisation:  53.76716591015747
actor:  0 policy actor:  1  step number:  35 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  26.05286289121366
printing an ep nov before normalisation:  22.92952460908637
maxi score, test score, baseline:  -0.06894000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
printing an ep nov before normalisation:  31.027061939239502
maxi score, test score, baseline:  -0.06894000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  24 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.06894000000000011 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.06894000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
printing an ep nov before normalisation:  61.41239246839612
Printing some Q and Qe and total Qs values:  [[0.164]
 [0.23 ]
 [0.094]
 [0.164]
 [0.164]
 [0.1  ]
 [0.164]] [[40.944]
 [46.973]
 [37.957]
 [40.944]
 [40.944]
 [38.   ]
 [40.944]] [[0.933]
 [1.241]
 [0.743]
 [0.933]
 [0.933]
 [0.751]
 [0.933]]
maxi score, test score, baseline:  -0.0689400000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.0689400000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.0689400000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.0689400000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actor:  1 policy actor:  1  step number:  32 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.05 ]
 [-0.023]
 [-0.053]
 [-0.052]
 [-0.053]
 [-0.052]
 [-0.052]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.05 ]
 [-0.023]
 [-0.053]
 [-0.052]
 [-0.053]
 [-0.052]
 [-0.052]]
actor:  1 policy actor:  1  step number:  33 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.0689400000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
Printing some Q and Qe and total Qs values:  [[0.25 ]
 [0.439]
 [0.25 ]
 [0.25 ]
 [0.25 ]
 [0.25 ]
 [0.25 ]] [[50.121]
 [43.48 ]
 [50.121]
 [50.121]
 [50.121]
 [50.121]
 [50.121]] [[0.983]
 [0.994]
 [0.983]
 [0.983]
 [0.983]
 [0.983]
 [0.983]]
printing an ep nov before normalisation:  66.21251055910537
printing an ep nov before normalisation:  39.055785594561854
maxi score, test score, baseline:  -0.0689400000000001 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.0689400000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  33 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.0689400000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actor:  1 policy actor:  1  step number:  36 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.0689400000000001 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.0689400000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
printing an ep nov before normalisation:  46.79705330532938
maxi score, test score, baseline:  -0.0689400000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actor:  1 policy actor:  1  step number:  37 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.0689400000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.0689400000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actor:  1 policy actor:  1  step number:  34 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.0689400000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.0689400000000001 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
printing an ep nov before normalisation:  49.87853954422942
printing an ep nov before normalisation:  60.974105377867524
maxi score, test score, baseline:  -0.06894000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
printing an ep nov before normalisation:  30.27707301946725
printing an ep nov before normalisation:  19.475207328796387
actions average: 
K:  2  action  0 :  tensor([0.5306, 0.0086, 0.0948, 0.0975, 0.0996, 0.0805, 0.0884],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0014,     0.9737,     0.0015,     0.0071,     0.0002,     0.0006,
            0.0155], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0700, 0.0520, 0.5660, 0.0828, 0.0630, 0.0839, 0.0823],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0856, 0.1710, 0.1200, 0.2054, 0.0711, 0.1112, 0.2357],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1472, 0.0072, 0.1118, 0.1204, 0.4023, 0.1011, 0.1101],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1711, 0.0993, 0.1328, 0.1501, 0.1162, 0.1670, 0.1635],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1276, 0.0703, 0.1261, 0.1647, 0.1218, 0.1406, 0.2489],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.06894000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
Printing some Q and Qe and total Qs values:  [[0.642]
 [0.734]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]] [[13.033]
 [18.136]
 [13.033]
 [13.033]
 [13.033]
 [13.033]
 [13.033]] [[1.298]
 [1.646]
 [1.298]
 [1.298]
 [1.298]
 [1.298]
 [1.298]]
printing an ep nov before normalisation:  0.031045441120625128
actor:  1 policy actor:  1  step number:  37 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  49 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  3  action  0 :  tensor([0.7681, 0.0088, 0.0344, 0.0472, 0.0770, 0.0240, 0.0405],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0038,     0.9718,     0.0044,     0.0021,     0.0006,     0.0009,
            0.0164], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0798, 0.1314, 0.5013, 0.0777, 0.0519, 0.0854, 0.0726],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1290, 0.0914, 0.1211, 0.2939, 0.0890, 0.1231, 0.1526],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1562, 0.0464, 0.1250, 0.1405, 0.2743, 0.1245, 0.1330],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0790, 0.1388, 0.2750, 0.1367, 0.0762, 0.1969, 0.0975],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1288, 0.0954, 0.1146, 0.1223, 0.0957, 0.0900, 0.3533],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.585]
 [0.634]
 [0.542]
 [0.547]
 [0.56 ]
 [0.581]
 [0.574]] [[26.562]
 [24.609]
 [27.607]
 [28.525]
 [28.358]
 [28.135]
 [26.295]] [[2.272]
 [2.077]
 [2.359]
 [2.478]
 [2.47 ]
 [2.463]
 [2.227]]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.06894000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actions average: 
K:  4  action  0 :  tensor([0.6781, 0.0027, 0.0521, 0.0697, 0.0846, 0.0590, 0.0539],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0037,     0.9661,     0.0070,     0.0054,     0.0006,     0.0083,
            0.0090], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0693, 0.1024, 0.4722, 0.0738, 0.0601, 0.1419, 0.0803],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0890, 0.0941, 0.0839, 0.4670, 0.0702, 0.0710, 0.1249],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1440, 0.0031, 0.1097, 0.1289, 0.3850, 0.1142, 0.1150],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2310, 0.0207, 0.1573, 0.1678, 0.1355, 0.1357, 0.1519],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1606, 0.0840, 0.1251, 0.2035, 0.1444, 0.1477, 0.1346],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.06894000000000011 0.6789999999999999 0.6789999999999999
printing an ep nov before normalisation:  64.24705581251466
printing an ep nov before normalisation:  52.1360791753598
actor:  1 policy actor:  1  step number:  31 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
UNIT TEST: sample policy line 217 mcts : [0.    0.939 0.    0.02  0.02  0.    0.02 ]
maxi score, test score, baseline:  -0.06894000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.06894000000000011 0.6789999999999999 0.6789999999999999
Printing some Q and Qe and total Qs values:  [[0.689]
 [0.733]
 [0.607]
 [0.64 ]
 [0.655]
 [0.688]
 [0.699]] [[23.077]
 [21.349]
 [22.231]
 [22.571]
 [22.516]
 [22.993]
 [19.232]] [[2.271]
 [2.197]
 [2.131]
 [2.188]
 [2.199]
 [2.264]
 [2.018]]
actor:  1 policy actor:  1  step number:  31 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.06894000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
printing an ep nov before normalisation:  42.502361536026
printing an ep nov before normalisation:  40.457295430291175
maxi score, test score, baseline:  -0.06894000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actor:  0 policy actor:  0  step number:  35 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.06970000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.0
from probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.06970000000000011 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.06970000000000011 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.06970000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.06970000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  21 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.291993486374935
Printing some Q and Qe and total Qs values:  [[0.71 ]
 [0.765]
 [0.685]
 [0.684]
 [0.71 ]
 [0.71 ]
 [0.71 ]] [[31.098]
 [35.602]
 [29.738]
 [29.709]
 [31.098]
 [31.098]
 [31.098]] [[1.778]
 [2.116]
 [1.668]
 [1.665]
 [1.778]
 [1.778]
 [1.778]]
maxi score, test score, baseline:  -0.06986000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  28 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.06986000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.626]
 [0.759]
 [0.626]
 [0.626]
 [0.626]
 [0.626]] [[58.644]
 [58.644]
 [62.085]
 [58.644]
 [58.644]
 [58.644]
 [58.644]] [[0.626]
 [0.626]
 [0.759]
 [0.626]
 [0.626]
 [0.626]
 [0.626]]
maxi score, test score, baseline:  -0.06986000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actor:  0 policy actor:  1  step number:  34 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  31 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.07048000000000013 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.07048000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
printing an ep nov before normalisation:  44.83922783615862
siam score:  -0.87269914
actor:  0 policy actor:  1  step number:  27 total reward:  0.6  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  27 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.07064000000000013 0.6789999999999999 0.6789999999999999
actions average: 
K:  4  action  0 :  tensor([0.4669, 0.0081, 0.0818, 0.0902, 0.1764, 0.0752, 0.1015],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0065, 0.9518, 0.0090, 0.0083, 0.0049, 0.0029, 0.0166],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0644, 0.0117, 0.5950, 0.1014, 0.0715, 0.0671, 0.0889],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1337, 0.0186, 0.1259, 0.3456, 0.1215, 0.1272, 0.1275],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1288, 0.0025, 0.0804, 0.0986, 0.5457, 0.0738, 0.0703],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1107, 0.0542, 0.1494, 0.1313, 0.1418, 0.2123, 0.2002],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1426, 0.0779, 0.1038, 0.0712, 0.0671, 0.1132, 0.4243],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.07064000000000013 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.07064000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
Printing some Q and Qe and total Qs values:  [[0.211]
 [0.271]
 [0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.211]
 [0.271]
 [0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]]
printing an ep nov before normalisation:  64.43836801859287
printing an ep nov before normalisation:  49.578661025584324
actions average: 
K:  1  action  0 :  tensor([0.7802, 0.0034, 0.0385, 0.0378, 0.0494, 0.0354, 0.0553],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0039, 0.9786, 0.0045, 0.0031, 0.0015, 0.0026, 0.0057],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0600, 0.0012, 0.6717, 0.0618, 0.0587, 0.0821, 0.0645],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1179, 0.1055, 0.1077, 0.2169, 0.1432, 0.1588, 0.1501],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1350, 0.0041, 0.1033, 0.1130, 0.3839, 0.1210, 0.1397],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1517, 0.0013, 0.1660, 0.1383, 0.1370, 0.2487, 0.1571],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1215, 0.0203, 0.0947, 0.1096, 0.1096, 0.0981, 0.4461],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  34 total reward:  0.44999999999999984  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.07064000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.12731587378789
maxi score, test score, baseline:  -0.07064000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actor:  1 policy actor:  1  step number:  43 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  29 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.07064000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.07064000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
using explorer policy with actor:  1
from probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.07064000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.07064000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.07400000000000012 0.6789999999999999 0.6789999999999999
actor:  1 policy actor:  1  step number:  28 total reward:  0.61  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.07400000000000012 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.07400000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.07400000000000012 0.6789999999999999 0.6789999999999999
printing an ep nov before normalisation:  47.2378756785831
printing an ep nov before normalisation:  50.52002564141015
maxi score, test score, baseline:  -0.07400000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actor:  1 policy actor:  1  step number:  29 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  45.97071826547917
maxi score, test score, baseline:  -0.07400000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
maxi score, test score, baseline:  -0.07400000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
printing an ep nov before normalisation:  36.2729308433744
actor:  1 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
using another actor
maxi score, test score, baseline:  -0.07736000000000014 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.07736000000000014 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
Printing some Q and Qe and total Qs values:  [[0.142]
 [0.278]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]] [[52.226]
 [61.778]
 [52.226]
 [52.226]
 [52.226]
 [52.226]
 [52.226]] [[0.992]
 [1.423]
 [0.992]
 [0.992]
 [0.992]
 [0.992]
 [0.992]]
maxi score, test score, baseline:  -0.07736000000000014 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.07736000000000014 0.6789999999999999 0.6789999999999999
probs:  [0.07176362076711519, 0.07176362076711519, 0.13824535834032498, 0.07176362076711519, 0.49437066983640254, 0.15209310952192676]
actor:  0 policy actor:  1  step number:  36 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.542]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]] [[41.667]
 [51.029]
 [41.667]
 [41.667]
 [41.667]
 [41.667]
 [41.667]] [[0.665]
 [0.841]
 [0.665]
 [0.665]
 [0.665]
 [0.665]
 [0.665]]
maxi score, test score, baseline:  -0.07794000000000012 0.6789999999999999 0.6789999999999999
actor:  1 policy actor:  1  step number:  46 total reward:  0.1899999999999995  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  27 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.07794000000000012 0.6789999999999999 0.6789999999999999
actor:  1 policy actor:  1  step number:  23 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.222]
 [0.394]
 [0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]] [[48.426]
 [48.893]
 [48.426]
 [48.426]
 [48.426]
 [48.426]
 [48.426]] [[1.115]
 [1.303]
 [1.115]
 [1.115]
 [1.115]
 [1.115]
 [1.115]]
actor:  1 policy actor:  1  step number:  24 total reward:  0.7499999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  58.5128843721122
printing an ep nov before normalisation:  39.840390137174936
maxi score, test score, baseline:  -0.07794000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.061987031008358895, 0.061987031008358895, 0.11939490149429856, 0.061987031008358895, 0.426913875958802, 0.26773012952182274]
actor:  1 policy actor:  1  step number:  22 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.07794000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.061987031008358895, 0.061987031008358895, 0.11939490149429856, 0.061987031008358895, 0.426913875958802, 0.26773012952182274]
siam score:  -0.86747587
printing an ep nov before normalisation:  0.00654857523869623
maxi score, test score, baseline:  -0.07794000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.061987031008358895, 0.061987031008358895, 0.11939490149429856, 0.061987031008358895, 0.426913875958802, 0.26773012952182274]
printing an ep nov before normalisation:  58.2803208848316
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([0.4015, 0.0012, 0.1098, 0.0964, 0.1710, 0.1003, 0.1197],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0067, 0.9548, 0.0095, 0.0105, 0.0021, 0.0010, 0.0153],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0723, 0.0021, 0.5477, 0.0778, 0.0822, 0.1464, 0.0715],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1663, 0.0266, 0.1576, 0.2219, 0.1457, 0.1340, 0.1479],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1641, 0.0025, 0.1574, 0.1174, 0.2681, 0.1355, 0.1549],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1745, 0.0047, 0.1591, 0.1493, 0.1611, 0.1548, 0.1965],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1446, 0.0094, 0.1447, 0.1424, 0.1599, 0.1285, 0.2705],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  69.6328059298018
maxi score, test score, baseline:  -0.07794000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.061987031008358895, 0.061987031008358895, 0.11939490149429856, 0.061987031008358895, 0.426913875958802, 0.26773012952182274]
actor:  0 policy actor:  1  step number:  41 total reward:  0.0799999999999994  reward:  1.0 rdn_beta:  0.333
from probs:  [0.061987031008358895, 0.061987031008358895, 0.11939490149429856, 0.061987031008358895, 0.426913875958802, 0.26773012952182274]
actor:  1 policy actor:  1  step number:  37 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.08250000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.061987031008358895, 0.061987031008358895, 0.11939490149429856, 0.061987031008358895, 0.426913875958802, 0.26773012952182274]
Printing some Q and Qe and total Qs values:  [[0.261]
 [0.261]
 [0.417]
 [0.261]
 [0.261]
 [0.261]
 [0.261]] [[61.515]
 [61.515]
 [63.225]
 [61.515]
 [61.515]
 [61.515]
 [61.515]] [[1.865]
 [1.865]
 [2.082]
 [1.865]
 [1.865]
 [1.865]
 [1.865]]
maxi score, test score, baseline:  -0.08250000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.061987031008358895, 0.061987031008358895, 0.11939490149429856, 0.061987031008358895, 0.426913875958802, 0.26773012952182274]
printing an ep nov before normalisation:  49.580087661743164
maxi score, test score, baseline:  -0.08250000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.061987031008358895, 0.061987031008358895, 0.11939490149429856, 0.061987031008358895, 0.426913875958802, 0.26773012952182274]
actor:  0 policy actor:  1  step number:  41 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  35 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  22 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  21 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8695834
maxi score, test score, baseline:  -0.08342000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.061987031008358895, 0.061987031008358895, 0.11939490149429856, 0.061987031008358895, 0.426913875958802, 0.26773012952182274]
printing an ep nov before normalisation:  75.76414642753814
Printing some Q and Qe and total Qs values:  [[-0.016]
 [-0.022]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]] [[ 0.   ]
 [39.529]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.448]
 [ 0.98 ]
 [-0.448]
 [-0.448]
 [-0.448]
 [-0.448]
 [-0.448]]
maxi score, test score, baseline:  -0.08342000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.061987031008358895, 0.061987031008358895, 0.11939490149429856, 0.061987031008358895, 0.426913875958802, 0.26773012952182274]
maxi score, test score, baseline:  -0.08342000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.061987031008358895, 0.061987031008358895, 0.11939490149429856, 0.061987031008358895, 0.426913875958802, 0.26773012952182274]
maxi score, test score, baseline:  -0.08342000000000012 0.6789999999999999 0.6789999999999999
actor:  1 policy actor:  1  step number:  32 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.201515197753906
Printing some Q and Qe and total Qs values:  [[0.187]
 [0.935]
 [0.755]
 [0.342]
 [0.442]
 [0.507]
 [0.621]] [[46.729]
 [50.453]
 [43.914]
 [44.544]
 [46.523]
 [44.845]
 [47.359]] [[0.187]
 [0.935]
 [0.755]
 [0.342]
 [0.442]
 [0.507]
 [0.621]]
maxi score, test score, baseline:  -0.08342000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.061987031008358895, 0.061987031008358895, 0.11939490149429856, 0.061987031008358895, 0.426913875958802, 0.26773012952182274]
actor:  0 policy actor:  0  step number:  30 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.08400000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.061987031008358895, 0.061987031008358895, 0.11939490149429856, 0.061987031008358895, 0.426913875958802, 0.26773012952182274]
printing an ep nov before normalisation:  56.15175996359287
actor:  1 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.08400000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.061987031008358895, 0.061987031008358895, 0.11939490149429856, 0.061987031008358895, 0.426913875958802, 0.26773012952182274]
maxi score, test score, baseline:  -0.08400000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.061987031008358895, 0.061987031008358895, 0.11939490149429856, 0.061987031008358895, 0.426913875958802, 0.26773012952182274]
printing an ep nov before normalisation:  30.698516368865967
maxi score, test score, baseline:  -0.08400000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.061987031008358895, 0.061987031008358895, 0.11939490149429856, 0.061987031008358895, 0.426913875958802, 0.26773012952182274]
actor:  1 policy actor:  1  step number:  34 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.546]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.474]] [[34.109]
 [33.368]
 [30.95 ]
 [30.95 ]
 [30.95 ]
 [33.787]
 [33.785]] [[1.656]
 [1.702]
 [1.547]
 [1.547]
 [1.547]
 [1.646]
 [1.645]]
actions average: 
K:  0  action  0 :  tensor([0.4419, 0.0030, 0.1076, 0.1130, 0.1355, 0.0879, 0.1112],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0016,     0.9624,     0.0010,     0.0132,     0.0005,     0.0003,
            0.0210], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0874, 0.0140, 0.5559, 0.0816, 0.0779, 0.1084, 0.0748],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1355, 0.0645, 0.1056, 0.3114, 0.1266, 0.1027, 0.1537],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1636, 0.0084, 0.1489, 0.1698, 0.2348, 0.1290, 0.1456],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1555, 0.0053, 0.1375, 0.1265, 0.1341, 0.2810, 0.1601],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1425, 0.0169, 0.0996, 0.1246, 0.0971, 0.0809, 0.4384],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  0.019017666010086032
actor:  1 policy actor:  1  step number:  30 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.333
from probs:  [0.061987031008358895, 0.061987031008358895, 0.11939490149429856, 0.061987031008358895, 0.426913875958802, 0.26773012952182274]
Printing some Q and Qe and total Qs values:  [[0.244]
 [0.591]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.244]
 [0.591]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]]
printing an ep nov before normalisation:  40.08411834647252
maxi score, test score, baseline:  -0.08400000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.061987031008358895, 0.061987031008358895, 0.11939490149429856, 0.061987031008358895, 0.426913875958802, 0.26773012952182274]
siam score:  -0.8738159
actor:  1 policy actor:  1  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  59.65158383999997
line 256 mcts: sample exp_bonus 32.153800241756635
maxi score, test score, baseline:  -0.08400000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.061987031008358895, 0.061987031008358895, 0.11939490149429856, 0.061987031008358895, 0.426913875958802, 0.26773012952182274]
Printing some Q and Qe and total Qs values:  [[0.353]
 [0.353]
 [0.592]
 [0.353]
 [0.353]
 [0.353]
 [0.353]] [[33.887]
 [33.887]
 [44.618]
 [33.887]
 [33.887]
 [33.887]
 [33.887]] [[0.992]
 [0.992]
 [1.617]
 [0.992]
 [0.992]
 [0.992]
 [0.992]]
actor:  1 policy actor:  1  step number:  34 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.08400000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.061987031008358895, 0.061987031008358895, 0.11939490149429856, 0.061987031008358895, 0.426913875958802, 0.26773012952182274]
maxi score, test score, baseline:  -0.08400000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.061987031008358895, 0.061987031008358895, 0.11939490149429856, 0.061987031008358895, 0.426913875958802, 0.26773012952182274]
actor:  1 policy actor:  1  step number:  27 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  51.760041861264504
actions average: 
K:  4  action  0 :  tensor([0.5650, 0.0522, 0.1157, 0.0754, 0.0601, 0.0590, 0.0726],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0102, 0.9262, 0.0124, 0.0194, 0.0048, 0.0046, 0.0223],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0528, 0.1286, 0.6534, 0.0407, 0.0263, 0.0555, 0.0427],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1480, 0.0248, 0.0938, 0.2698, 0.1690, 0.0968, 0.1978],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1273, 0.0290, 0.1375, 0.1356, 0.2800, 0.1294, 0.1614],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2047, 0.0335, 0.1526, 0.1723, 0.1469, 0.1562, 0.1337],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1300, 0.0071, 0.1397, 0.1351, 0.1157, 0.1087, 0.3636],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  19.509778916835785
maxi score, test score, baseline:  -0.08400000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.061987031008358895, 0.061987031008358895, 0.11939490149429856, 0.061987031008358895, 0.426913875958802, 0.26773012952182274]
printing an ep nov before normalisation:  33.71239374096855
actor:  1 policy actor:  1  step number:  26 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.118]
 [0.313]
 [0.124]
 [0.109]
 [0.116]
 [0.099]
 [0.072]] [[27.916]
 [46.817]
 [38.348]
 [28.225]
 [28.579]
 [26.4  ]
 [26.136]] [[0.184]
 [0.474]
 [0.243]
 [0.177]
 [0.186]
 [0.157]
 [0.13 ]]
printing an ep nov before normalisation:  53.776191834529364
siam score:  -0.864409
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.08400000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.061987031008358895, 0.061987031008358895, 0.11939490149429856, 0.061987031008358895, 0.426913875958802, 0.26773012952182274]
actor:  1 policy actor:  1  step number:  25 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.762]
 [0.761]
 [0.75 ]
 [0.747]
 [0.768]
 [0.752]
 [0.751]] [[40.123]
 [43.872]
 [40.324]
 [40.414]
 [41.766]
 [39.59 ]
 [37.591]] [[0.762]
 [0.761]
 [0.75 ]
 [0.747]
 [0.768]
 [0.752]
 [0.751]]
actor:  0 policy actor:  0  step number:  34 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.08764000000000012 0.6789999999999999 0.6789999999999999
Printing some Q and Qe and total Qs values:  [[0.096]
 [0.152]
 [0.097]
 [0.094]
 [0.091]
 [0.092]
 [0.099]] [[25.807]
 [40.595]
 [24.372]
 [24.413]
 [25.838]
 [27.128]
 [22.867]] [[0.64 ]
 [1.371]
 [0.576]
 [0.575]
 [0.636]
 [0.696]
 [0.509]]
maxi score, test score, baseline:  -0.08764000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.061987031008358895, 0.061987031008358895, 0.11939490149429856, 0.061987031008358895, 0.426913875958802, 0.26773012952182274]
printing an ep nov before normalisation:  52.50781801787322
actor:  1 policy actor:  1  step number:  22 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.08764000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.061987031008358895, 0.061987031008358895, 0.11939490149429856, 0.061987031008358895, 0.426913875958802, 0.26773012952182274]
maxi score, test score, baseline:  -0.08764000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.061987031008358895, 0.061987031008358895, 0.11939490149429856, 0.061987031008358895, 0.426913875958802, 0.26773012952182274]
actor:  1 policy actor:  1  step number:  40 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.08764000000000012 0.6789999999999999 0.6789999999999999
Printing some Q and Qe and total Qs values:  [[0.054]
 [0.116]
 [0.054]
 [0.054]
 [0.054]
 [0.054]
 [0.054]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.054]
 [0.116]
 [0.054]
 [0.054]
 [0.054]
 [0.054]
 [0.054]]
maxi score, test score, baseline:  -0.08764000000000012 0.6789999999999999 0.6789999999999999
printing an ep nov before normalisation:  72.62613455223666
maxi score, test score, baseline:  -0.08764000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.061987031008358895, 0.061987031008358895, 0.11939490149429856, 0.061987031008358895, 0.426913875958802, 0.26773012952182274]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 57.26683966419633
maxi score, test score, baseline:  -0.08764000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.061987031008358895, 0.061987031008358895, 0.11939490149429856, 0.061987031008358895, 0.426913875958802, 0.26773012952182274]
siam score:  -0.8669745
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  42 total reward:  0.20999999999999952  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.08764000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.061987031008358895, 0.061987031008358895, 0.11939490149429856, 0.061987031008358895, 0.426913875958802, 0.26773012952182274]
printing an ep nov before normalisation:  62.90698411537604
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.08764000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.061987031008358895, 0.061987031008358895, 0.11939490149429856, 0.061987031008358895, 0.426913875958802, 0.26773012952182274]
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.736]
 [0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]] [[51.266]
 [49.88 ]
 [51.266]
 [51.266]
 [51.266]
 [51.266]
 [51.266]] [[2.04 ]
 [2.074]
 [2.04 ]
 [2.04 ]
 [2.04 ]
 [2.04 ]
 [2.04 ]]
printing an ep nov before normalisation:  57.79531186930416
actor:  1 policy actor:  1  step number:  18 total reward:  0.73  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  60.270192480635444
maxi score, test score, baseline:  -0.08764000000000012 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.08764000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
printing an ep nov before normalisation:  40.78521415148147
actor:  1 policy actor:  1  step number:  30 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  19 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.08764000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.08764000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
maxi score, test score, baseline:  -0.08764000000000012 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.08764000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
Printing some Q and Qe and total Qs values:  [[0.743]
 [0.844]
 [0.743]
 [0.743]
 [0.743]
 [0.753]
 [0.755]] [[41.913]
 [34.313]
 [41.913]
 [41.913]
 [41.913]
 [41.374]
 [40.79 ]] [[2.259]
 [1.847]
 [2.259]
 [2.259]
 [2.259]
 [2.233]
 [2.196]]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  34 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.08764000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
maxi score, test score, baseline:  -0.08764000000000012 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.08764000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
actor:  0 policy actor:  1  step number:  21 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.104]
 [0.09 ]
 [0.104]
 [0.104]
 [0.104]
 [0.104]
 [0.104]] [[56.534]
 [55.427]
 [56.534]
 [56.534]
 [56.534]
 [56.534]
 [56.534]] [[1.534]
 [1.481]
 [1.534]
 [1.534]
 [1.534]
 [1.534]
 [1.534]]
Printing some Q and Qe and total Qs values:  [[0.06 ]
 [0.195]
 [0.06 ]
 [0.06 ]
 [0.06 ]
 [0.06 ]
 [0.06 ]] [[31.491]
 [51.867]
 [31.491]
 [31.491]
 [31.491]
 [31.491]
 [31.491]] [[0.565]
 [1.323]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]]
maxi score, test score, baseline:  -0.08774000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
maxi score, test score, baseline:  -0.08774000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
from probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
line 256 mcts: sample exp_bonus 35.305739636930035
actor:  0 policy actor:  0  step number:  38 total reward:  0.2899999999999996  reward:  1.0 rdn_beta:  1.667
siam score:  -0.85881937
actor:  1 policy actor:  1  step number:  36 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  56.46917863505994
maxi score, test score, baseline:  -0.09180000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
maxi score, test score, baseline:  -0.09180000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
actor:  1 policy actor:  1  step number:  32 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.09180000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
maxi score, test score, baseline:  -0.09180000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.09180000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
actions average: 
K:  0  action  0 :  tensor([0.5039, 0.0184, 0.0926, 0.1046, 0.1168, 0.0898, 0.0740],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0015,     0.9877,     0.0019,     0.0032,     0.0003,     0.0006,
            0.0049], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0701, 0.0017, 0.5972, 0.0857, 0.0782, 0.0958, 0.0713],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1137, 0.1430, 0.1030, 0.3083, 0.1211, 0.1027, 0.1081],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1234, 0.0086, 0.1589, 0.1685, 0.2921, 0.1363, 0.1122],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1515, 0.0038, 0.1205, 0.1811, 0.1751, 0.2163, 0.1517],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0923, 0.1497, 0.1082, 0.1315, 0.1135, 0.1036, 0.3013],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.09180000000000012 0.6789999999999999 0.6789999999999999
printing an ep nov before normalisation:  58.84397029876709
maxi score, test score, baseline:  -0.09180000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
actor:  0 policy actor:  0  step number:  33 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  36 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  -0.09212000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
printing an ep nov before normalisation:  0.21412706088653977
maxi score, test score, baseline:  -0.09212000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
maxi score, test score, baseline:  -0.09212000000000012 0.6789999999999999 0.6789999999999999
Printing some Q and Qe and total Qs values:  [[0.394]
 [0.471]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]] [[36.119]
 [52.868]
 [36.119]
 [36.119]
 [36.119]
 [36.119]
 [36.119]] [[0.996]
 [1.667]
 [0.996]
 [0.996]
 [0.996]
 [0.996]
 [0.996]]
maxi score, test score, baseline:  -0.09212000000000012 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.09212000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
actor:  1 policy actor:  1  step number:  31 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
from probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
maxi score, test score, baseline:  -0.09212000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
printing an ep nov before normalisation:  0.00028751576280683366
actor:  0 policy actor:  0  step number:  19 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.09216000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
Printing some Q and Qe and total Qs values:  [[-0.037]
 [-0.015]
 [-0.03 ]
 [-0.043]
 [-0.057]
 [-0.056]
 [-0.058]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.037]
 [-0.015]
 [-0.03 ]
 [-0.043]
 [-0.057]
 [-0.056]
 [-0.058]]
actor:  0 policy actor:  0  step number:  31 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.09262000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
maxi score, test score, baseline:  -0.09262000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
siam score:  -0.8635706
actions average: 
K:  0  action  0 :  tensor([0.5309, 0.0206, 0.0950, 0.0838, 0.1009, 0.0677, 0.1011],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0026,     0.9812,     0.0022,     0.0017,     0.0002,     0.0003,
            0.0118], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1182, 0.0028, 0.5036, 0.0974, 0.0926, 0.0852, 0.1003],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1301, 0.0235, 0.1293, 0.3273, 0.1143, 0.0966, 0.1789],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1197, 0.0010, 0.0776, 0.0876, 0.5607, 0.0660, 0.0875],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1216, 0.0037, 0.1448, 0.1073, 0.0862, 0.4138, 0.1226],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1481, 0.0813, 0.1184, 0.1273, 0.1132, 0.0854, 0.3263],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.65 ]
 [0.757]
 [0.65 ]
 [0.65 ]
 [0.679]
 [0.676]
 [0.65 ]] [[35.258]
 [35.731]
 [35.258]
 [35.258]
 [35.232]
 [34.998]
 [35.258]] [[2.425]
 [2.58 ]
 [2.425]
 [2.425]
 [2.451]
 [2.424]
 [2.425]]
maxi score, test score, baseline:  -0.09262000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
printing an ep nov before normalisation:  35.753350257873535
actor:  0 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.08956000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
maxi score, test score, baseline:  -0.08956000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
actor:  1 policy actor:  1  step number:  46 total reward:  0.3299999999999996  reward:  1.0 rdn_beta:  2.0
from probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
maxi score, test score, baseline:  -0.08956000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
Printing some Q and Qe and total Qs values:  [[0.209]
 [0.423]
 [0.209]
 [0.209]
 [0.209]
 [0.209]
 [0.209]] [[27.335]
 [39.963]
 [27.335]
 [27.335]
 [27.335]
 [27.335]
 [27.335]] [[0.318]
 [0.641]
 [0.318]
 [0.318]
 [0.318]
 [0.318]
 [0.318]]
maxi score, test score, baseline:  -0.08956000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
printing an ep nov before normalisation:  32.9209566116333
maxi score, test score, baseline:  -0.08956000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
printing an ep nov before normalisation:  34.82166051864624
maxi score, test score, baseline:  -0.08956000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
actor:  0 policy actor:  1  step number:  26 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.08670000000000014 0.6789999999999999 0.6789999999999999
actor:  1 policy actor:  1  step number:  44 total reward:  0.2699999999999996  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.615]
 [0.574]
 [0.381]
 [0.459]
 [0.574]
 [0.348]] [[62.857]
 [65.384]
 [67.072]
 [62.741]
 [62.063]
 [67.072]
 [68.478]] [[1.975]
 [2.091]
 [2.101]
 [1.778]
 [1.836]
 [2.101]
 [1.917]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.08670000000000014 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
actor:  1 policy actor:  1  step number:  36 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  52.698965072631836
maxi score, test score, baseline:  -0.08670000000000014 0.6789999999999999 0.6789999999999999
actor:  1 policy actor:  1  step number:  33 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.08670000000000014 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
maxi score, test score, baseline:  -0.08670000000000014 0.6789999999999999 0.6789999999999999
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.08670000000000014 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
printing an ep nov before normalisation:  37.109080716582866
printing an ep nov before normalisation:  23.279947278853545
maxi score, test score, baseline:  -0.08670000000000014 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
actor:  0 policy actor:  1  step number:  31 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.4  ]
 [0.606]
 [0.736]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.4  ]] [[39.267]
 [39.453]
 [37.187]
 [39.267]
 [39.267]
 [39.267]
 [39.267]] [[0.4  ]
 [0.606]
 [0.736]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.4  ]]
printing an ep nov before normalisation:  70.82373491419555
maxi score, test score, baseline:  -0.08654000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
printing an ep nov before normalisation:  39.0865281822183
printing an ep nov before normalisation:  39.92963700638721
maxi score, test score, baseline:  -0.08654000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
maxi score, test score, baseline:  -0.08654000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
actor:  1 policy actor:  1  step number:  34 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  32 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.08654000000000013 0.6789999999999999 0.6789999999999999
printing an ep nov before normalisation:  32.85713099158744
actor:  0 policy actor:  1  step number:  33 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.08378000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
actor:  1 policy actor:  1  step number:  28 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  2.0
from probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
maxi score, test score, baseline:  -0.08378000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.563]
 [0.545]
 [0.552]
 [0.71 ]
 [0.55 ]
 [0.71 ]] [[40.893]
 [36.334]
 [39.832]
 [40.053]
 [39.592]
 [39.138]
 [39.592]] [[0.56 ]
 [0.563]
 [0.545]
 [0.552]
 [0.71 ]
 [0.55 ]
 [0.71 ]]
maxi score, test score, baseline:  -0.08378000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
printing an ep nov before normalisation:  53.99089477025678
actor:  0 policy actor:  0  step number:  27 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.08374000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
siam score:  -0.8711291
actor:  0 policy actor:  0  step number:  27 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  19 total reward:  0.72  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.449]
 [0.7  ]
 [0.449]
 [0.449]
 [0.449]
 [0.449]] [[71.983]
 [71.983]
 [73.147]
 [71.983]
 [71.983]
 [71.983]
 [71.983]] [[2.077]
 [2.077]
 [2.367]
 [2.077]
 [2.077]
 [2.077]
 [2.077]]
maxi score, test score, baseline:  -0.07726000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
actor:  1 policy actor:  1  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.07726000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
maxi score, test score, baseline:  -0.07726000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
maxi score, test score, baseline:  -0.07726000000000012 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.07726000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
printing an ep nov before normalisation:  0.03063460943735663
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  -0.07726000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  55.70795152785884
printing an ep nov before normalisation:  52.72947726390986
printing an ep nov before normalisation:  31.616350644746888
Printing some Q and Qe and total Qs values:  [[-0.07 ]
 [-0.015]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]] [[35.615]
 [26.891]
 [35.615]
 [35.615]
 [35.615]
 [35.615]
 [35.615]] [[0.535]
 [0.442]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]]
using another actor
from probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
printing an ep nov before normalisation:  19.190955810322624
printing an ep nov before normalisation:  52.86107267495001
maxi score, test score, baseline:  -0.07726000000000012 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.07726000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
actor:  0 policy actor:  0  step number:  45 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.639]
 [0.831]
 [0.639]
 [0.639]
 [0.639]
 [0.639]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.639]
 [0.639]
 [0.831]
 [0.639]
 [0.639]
 [0.639]
 [0.639]]
maxi score, test score, baseline:  -0.07742000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
printing an ep nov before normalisation:  39.42286491394043
actor:  0 policy actor:  1  step number:  20 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.07628000000000013 0.6789999999999999 0.6789999999999999
printing an ep nov before normalisation:  30.97377061843872
maxi score, test score, baseline:  -0.07628000000000013 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.07628000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
maxi score, test score, baseline:  -0.07628000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
actor:  0 policy actor:  0  step number:  26 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.07326000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.07326000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
actor:  1 policy actor:  1  step number:  27 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  2.0
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.464]
 [0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.283]] [[52.163]
 [52.458]
 [52.163]
 [52.163]
 [52.163]
 [52.163]
 [52.163]] [[1.337]
 [1.526]
 [1.337]
 [1.337]
 [1.337]
 [1.337]
 [1.337]]
maxi score, test score, baseline:  -0.07326000000000012 0.6789999999999999 0.6789999999999999
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  28 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.07308000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.07308000000000012 0.6789999999999999 0.6789999999999999
actor:  1 policy actor:  1  step number:  26 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.07308000000000012 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.07308000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
actor:  1 policy actor:  1  step number:  32 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  67.93972283460113
maxi score, test score, baseline:  -0.07308000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
printing an ep nov before normalisation:  57.38197955529539
actor:  0 policy actor:  1  step number:  33 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.07290000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  35 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  2  action  0 :  tensor([0.7082, 0.0012, 0.0481, 0.0480, 0.0939, 0.0435, 0.0571],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0030,     0.9718,     0.0018,     0.0053,     0.0008,     0.0006,
            0.0168], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0613, 0.0224, 0.6587, 0.0633, 0.0542, 0.0608, 0.0793],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1468, 0.0914, 0.1170, 0.2685, 0.1179, 0.1002, 0.1582],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2375, 0.0014, 0.0858, 0.0925, 0.3783, 0.0903, 0.1142],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1022, 0.0043, 0.2081, 0.1703, 0.0950, 0.2999, 0.1202],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0957, 0.1088, 0.1118, 0.1137, 0.0998, 0.0981, 0.3721],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  40.082387924194336
maxi score, test score, baseline:  -0.07290000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
printing an ep nov before normalisation:  66.62464187311795
printing an ep nov before normalisation:  50.141418523417656
maxi score, test score, baseline:  -0.07290000000000013 0.6789999999999999 0.6789999999999999
actor:  1 policy actor:  1  step number:  31 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.07290000000000013 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.07290000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.333
using another actor
maxi score, test score, baseline:  -0.07290000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
maxi score, test score, baseline:  -0.07290000000000013 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.07290000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
maxi score, test score, baseline:  -0.07290000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
maxi score, test score, baseline:  -0.07290000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
actor:  1 policy actor:  1  step number:  31 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  72.8359593748478
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  24 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  34.99392509460449
maxi score, test score, baseline:  -0.07290000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
actions average: 
K:  0  action  0 :  tensor([0.4702, 0.0213, 0.0740, 0.0950, 0.1494, 0.0829, 0.1071],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0018,     0.9793,     0.0018,     0.0068,     0.0004,     0.0006,
            0.0092], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0842, 0.0046, 0.5517, 0.0755, 0.0986, 0.1097, 0.0757],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1133, 0.0861, 0.0811, 0.2648, 0.1289, 0.1352, 0.1906],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1556, 0.0032, 0.1358, 0.1199, 0.3592, 0.1110, 0.1152],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0678, 0.0087, 0.1477, 0.0666, 0.1182, 0.5474, 0.0436],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0532, 0.0288, 0.0948, 0.1903, 0.0381, 0.0666, 0.5283],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.07290000000000013 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.07290000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
actor:  1 policy actor:  1  step number:  21 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [ 0.057]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.01 ]
 [ 0.057]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]]
maxi score, test score, baseline:  -0.07290000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.06462557673195578, 0.06462557673195578, 0.11494850956879733, 0.06462557673195578, 0.44619767350140976, 0.24497708673392546]
printing an ep nov before normalisation:  45.84659521624168
actor:  1 policy actor:  1  step number:  25 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.07290000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
printing an ep nov before normalisation:  39.24729824066162
printing an ep nov before normalisation:  57.77284582242269
maxi score, test score, baseline:  -0.07290000000000013 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.07290000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
printing an ep nov before normalisation:  47.63825967487313
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([0.5165, 0.0474, 0.0809, 0.0916, 0.0964, 0.0720, 0.0953],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0013,     0.9696,     0.0009,     0.0089,     0.0003,     0.0003,
            0.0187], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0657, 0.0224, 0.7038, 0.0537, 0.0400, 0.0599, 0.0545],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1211, 0.0136, 0.1287, 0.3614, 0.0952, 0.1240, 0.1560],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1594, 0.0009, 0.0987, 0.1346, 0.4112, 0.0862, 0.1090],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1120, 0.0652, 0.1850, 0.0966, 0.0782, 0.3410, 0.1220],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1346, 0.0071, 0.1244, 0.1372, 0.1328, 0.1245, 0.3395],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  28 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  56.39028058763172
printing an ep nov before normalisation:  33.68035554885864
maxi score, test score, baseline:  -0.07290000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
maxi score, test score, baseline:  -0.07290000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
from probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.0
UNIT TEST: sample policy line 217 mcts : [0.204 0.245 0.143 0.102 0.082 0.102 0.122]
printing an ep nov before normalisation:  47.610346575549514
printing an ep nov before normalisation:  0.06230350639697235
maxi score, test score, baseline:  -0.07290000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
maxi score, test score, baseline:  -0.07290000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
maxi score, test score, baseline:  -0.07290000000000013 0.6789999999999999 0.6789999999999999
printing an ep nov before normalisation:  57.91223253016351
maxi score, test score, baseline:  -0.07290000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
maxi score, test score, baseline:  -0.07290000000000013 0.6789999999999999 0.6789999999999999
printing an ep nov before normalisation:  0.1731560851823133
actor:  1 policy actor:  1  step number:  22 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.07290000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
maxi score, test score, baseline:  -0.07290000000000013 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.07290000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  39 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  25 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[ 0.638]
 [ 0.521]
 [-0.047]
 [ 0.201]
 [ 0.568]
 [-0.069]
 [-0.02 ]] [[38.679]
 [44.95 ]
 [39.634]
 [41.593]
 [42.675]
 [39.932]
 [39.435]] [[1.09 ]
 [1.116]
 [0.426]
 [0.719]
 [1.11 ]
 [0.411]
 [0.449]]
maxi score, test score, baseline:  -0.06990000000000014 0.6789999999999999 0.6789999999999999
printing an ep nov before normalisation:  39.80467467448877
maxi score, test score, baseline:  -0.06990000000000014 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  23.462098669075946
main train batch thing paused
add a thread
Adding thread: now have 3 threads
actor:  1 policy actor:  1  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  50 total reward:  0.1899999999999995  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  59.36510714375775
printing an ep nov before normalisation:  60.4510916154375
maxi score, test score, baseline:  -0.06990000000000014 0.6789999999999999 0.6789999999999999
actor:  0 policy actor:  0  step number:  24 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  26 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.086]
 [0.189]
 [0.099]
 [0.085]
 [0.1  ]
 [0.094]
 [0.095]] [[32.677]
 [52.955]
 [33.132]
 [33.409]
 [35.228]
 [32.972]
 [32.844]] [[0.448]
 [1.026]
 [0.471]
 [0.464]
 [0.521]
 [0.463]
 [0.461]]
printing an ep nov before normalisation:  53.607912928157084
maxi score, test score, baseline:  -0.06996000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
siam score:  -0.87025774
printing an ep nov before normalisation:  49.47940830685509
maxi score, test score, baseline:  -0.06996000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
printing an ep nov before normalisation:  30.59864044189453
actor:  1 policy actor:  1  step number:  28 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  42 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.0
from probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.06996000000000012 0.6789999999999999 0.6789999999999999
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.06996000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
actor:  0 policy actor:  1  step number:  26 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.06978000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
actor:  0 policy actor:  1  step number:  26 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.06940000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
printing an ep nov before normalisation:  45.514023283047564
line 256 mcts: sample exp_bonus 49.689362997096644
actor:  1 policy actor:  1  step number:  23 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
from probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  34 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.06940000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
maxi score, test score, baseline:  -0.06940000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
actor:  0 policy actor:  1  step number:  22 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  21 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.06618000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
siam score:  -0.8572383
maxi score, test score, baseline:  -0.06882000000000013 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.06882000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
maxi score, test score, baseline:  -0.06882000000000013 0.6789999999999999 0.6789999999999999
printing an ep nov before normalisation:  40.71507569437915
maxi score, test score, baseline:  -0.06882000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
Printing some Q and Qe and total Qs values:  [[0.29 ]
 [0.345]
 [0.29 ]
 [0.29 ]
 [0.29 ]
 [0.29 ]
 [0.29 ]] [[67.976]
 [58.685]
 [67.976]
 [67.976]
 [67.976]
 [67.976]
 [67.976]] [[1.396]
 [1.228]
 [1.396]
 [1.396]
 [1.396]
 [1.396]
 [1.396]]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  23 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.06882000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
Printing some Q and Qe and total Qs values:  [[0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]] [[53.507]
 [53.507]
 [53.507]
 [53.507]
 [53.507]
 [53.507]
 [53.507]] [[2.383]
 [2.383]
 [2.383]
 [2.383]
 [2.383]
 [2.383]
 [2.383]]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.06882000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
maxi score, test score, baseline:  -0.06882000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.06882000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
maxi score, test score, baseline:  -0.06882000000000013 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
actor:  1 policy actor:  1  step number:  27 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.673]
 [0.673]
 [0.877]
 [0.673]
 [0.673]
 [0.673]
 [0.673]] [[38.945]
 [38.945]
 [38.786]
 [38.945]
 [38.945]
 [38.945]
 [38.945]] [[0.673]
 [0.673]
 [0.877]
 [0.673]
 [0.673]
 [0.673]
 [0.673]]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  46.48884781601319
printing an ep nov before normalisation:  51.90555156193493
printing an ep nov before normalisation:  43.065444343212604
printing an ep nov before normalisation:  46.660419969431175
Printing some Q and Qe and total Qs values:  [[0.119]
 [0.228]
 [0.132]
 [0.119]
 [0.119]
 [0.119]
 [0.119]] [[41.785]
 [44.729]
 [42.789]
 [41.785]
 [41.785]
 [41.785]
 [41.785]] [[1.009]
 [1.212]
 [1.054]
 [1.009]
 [1.009]
 [1.009]
 [1.009]]
actor:  0 policy actor:  1  step number:  23 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
from probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
actor:  0 policy actor:  0  step number:  27 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.06580000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
printing an ep nov before normalisation:  45.77530406066281
actor:  0 policy actor:  0  step number:  24 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.06250000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
maxi score, test score, baseline:  -0.06250000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.06250000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
using another actor
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.001]
 [ 1.145]
 [ 1.145]
 [ 1.145]
 [ 1.145]
 [ 1.145]
 [ 1.145]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.001]
 [ 1.145]
 [ 1.145]
 [ 1.145]
 [ 1.145]
 [ 1.145]
 [ 1.145]]
printing an ep nov before normalisation:  47.275582652990984
printing an ep nov before normalisation:  63.234104450966996
actor:  1 policy actor:  1  step number:  31 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  29 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  47.22969170141908
printing an ep nov before normalisation:  40.89841727737796
maxi score, test score, baseline:  -0.06250000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
actor:  0 policy actor:  1  step number:  26 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.059480000000000116 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
Printing some Q and Qe and total Qs values:  [[-0.04 ]
 [-0.015]
 [-0.037]
 [-0.036]
 [-0.033]
 [-0.033]
 [-0.034]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.04 ]
 [-0.015]
 [-0.037]
 [-0.036]
 [-0.033]
 [-0.033]
 [-0.034]]
maxi score, test score, baseline:  -0.059480000000000116 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
actor:  1 policy actor:  1  step number:  22 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  63.5247150282625
actions average: 
K:  0  action  0 :  tensor([0.5024, 0.0009, 0.0810, 0.0754, 0.1664, 0.0795, 0.0944],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0014,     0.9724,     0.0013,     0.0017,     0.0001,     0.0001,
            0.0229], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0573, 0.0073, 0.6673, 0.0649, 0.0708, 0.0624, 0.0700],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1091, 0.0071, 0.1086, 0.4276, 0.1259, 0.1019, 0.1198],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1303, 0.0011, 0.1293, 0.1192, 0.3814, 0.1196, 0.1192],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1127, 0.0018, 0.1858, 0.1232, 0.1264, 0.3210, 0.1291],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1632, 0.0062, 0.1232, 0.1492, 0.1556, 0.1173, 0.2853],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.059480000000000116 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.059480000000000116 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
actor:  1 policy actor:  1  step number:  22 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  21 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  32 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.059480000000000116 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
maxi score, test score, baseline:  -0.059480000000000116 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.728]
 [0.71 ]
 [0.737]
 [0.737]
 [0.737]
 [0.73 ]
 [0.745]] [[49.399]
 [52.024]
 [52.211]
 [52.211]
 [52.211]
 [57.851]
 [54.045]] [[1.916]
 [1.993]
 [2.027]
 [2.027]
 [2.027]
 [2.225]
 [2.101]]
maxi score, test score, baseline:  -0.059480000000000116 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.059480000000000116 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
Printing some Q and Qe and total Qs values:  [[-0.069]
 [-0.05 ]
 [ 0.603]
 [-0.026]
 [-0.077]
 [ 0.058]
 [-0.004]] [[29.576]
 [30.521]
 [38.403]
 [31.404]
 [32.928]
 [31.178]
 [29.401]] [[0.47 ]
 [0.522]
 [1.453]
 [0.577]
 [0.58 ]
 [0.654]
 [0.529]]
maxi score, test score, baseline:  -0.059480000000000116 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
maxi score, test score, baseline:  -0.059480000000000116 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
actor:  0 policy actor:  1  step number:  31 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
using another actor
actor:  1 policy actor:  1  step number:  34 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.117]
 [0.151]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.117]
 [0.151]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]]
Printing some Q and Qe and total Qs values:  [[0.111]
 [0.146]
 [0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]] [[54.755]
 [57.156]
 [54.755]
 [54.755]
 [54.755]
 [54.755]
 [54.755]] [[1.222]
 [1.32 ]
 [1.222]
 [1.222]
 [1.222]
 [1.222]
 [1.222]]
actor:  0 policy actor:  1  step number:  22 total reward:  0.73  reward:  1.0 rdn_beta:  2.0
from probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
maxi score, test score, baseline:  -0.05306000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
printing an ep nov before normalisation:  58.39057396167127
maxi score, test score, baseline:  -0.05306000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
printing an ep nov before normalisation:  43.69487208083139
actor:  1 policy actor:  1  step number:  27 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  38.43657865698944
Printing some Q and Qe and total Qs values:  [[0.744]
 [0.793]
 [0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]] [[67.39 ]
 [69.405]
 [67.39 ]
 [67.39 ]
 [67.39 ]
 [67.39 ]
 [67.39 ]] [[1.702]
 [1.793]
 [1.702]
 [1.702]
 [1.702]
 [1.702]
 [1.702]]
actor:  0 policy actor:  1  step number:  26 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.05000000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
maxi score, test score, baseline:  -0.05000000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
actor:  1 policy actor:  1  step number:  27 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  39 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.05000000000000012 0.6789999999999999 0.6789999999999999
using explorer policy with actor:  1
from probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
maxi score, test score, baseline:  -0.05000000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
printing an ep nov before normalisation:  54.6109231502221
printing an ep nov before normalisation:  76.71860746717101
printing an ep nov before normalisation:  41.226296043225126
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.754]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]] [[34.086]
 [48.149]
 [34.086]
 [34.086]
 [34.086]
 [34.086]
 [34.086]] [[0.573]
 [0.754]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]]
actions average: 
K:  3  action  0 :  tensor([0.5975, 0.0028, 0.0907, 0.0908, 0.0824, 0.0572, 0.0786],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0046, 0.9472, 0.0061, 0.0139, 0.0012, 0.0012, 0.0259],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0793, 0.0141, 0.5600, 0.0992, 0.0635, 0.0973, 0.0865],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1391, 0.1044, 0.1456, 0.2597, 0.1116, 0.1159, 0.1237],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1610, 0.0044, 0.0915, 0.0972, 0.4872, 0.0810, 0.0778],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1000, 0.1898, 0.1596, 0.1072, 0.0816, 0.2567, 0.1052],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0801, 0.0855, 0.0475, 0.0156, 0.0038, 0.0052, 0.7622],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  35.67983627319336
maxi score, test score, baseline:  -0.05000000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
maxi score, test score, baseline:  -0.05000000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
siam score:  -0.859863
printing an ep nov before normalisation:  62.474539396632146
actor:  0 policy actor:  0  step number:  30 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.05002000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
maxi score, test score, baseline:  -0.05002000000000011 0.6789999999999999 0.6789999999999999
printing an ep nov before normalisation:  61.675063209343264
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]] [[48.796]
 [48.796]
 [48.796]
 [48.796]
 [48.796]
 [48.796]
 [48.796]] [[2.148]
 [2.148]
 [2.148]
 [2.148]
 [2.148]
 [2.148]
 [2.148]]
actor:  1 policy actor:  1  step number:  36 total reward:  0.46999999999999986  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  43.75841517806223
printing an ep nov before normalisation:  30.876494923270517
maxi score, test score, baseline:  -0.05002000000000011 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.05002000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
actor:  1 policy actor:  1  step number:  22 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  36 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  57.0581973553329
printing an ep nov before normalisation:  45.540835245170214
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.05002000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
maxi score, test score, baseline:  -0.05002000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
actor:  1 policy actor:  1  step number:  31 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  37 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.05002000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.05578982831789436, 0.05578982831789436, 0.14618966860071778, 0.05578982831789436, 0.3066685556626384, 0.3797722907829607]
actor:  1 policy actor:  1  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  0.333
using another actor
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.05002000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.05586601214140038, 0.05578533448131628, 0.14617787568195564, 0.05578533448131628, 0.3066438053251264, 0.37974163788888504]
Printing some Q and Qe and total Qs values:  [[0.922]
 [0.913]
 [0.901]
 [0.904]
 [0.852]
 [0.896]
 [0.915]] [[58.415]
 [59.676]
 [56.687]
 [57.352]
 [57.975]
 [58.751]
 [59.675]] [[0.922]
 [0.913]
 [0.901]
 [0.904]
 [0.852]
 [0.896]
 [0.915]]
from probs:  [0.055865724678398725, 0.05578535143782608, 0.14617792017995637, 0.05578535143782608, 0.30664389871511366, 0.37974175355087897]
maxi score, test score, baseline:  -0.05002000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.055865724678398725, 0.05578535143782608, 0.14617792017995637, 0.05578535143782608, 0.30664389871511366, 0.37974175355087897]
actor:  0 policy actor:  1  step number:  22 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.04692000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.055865724678398725, 0.05578535143782608, 0.14617792017995637, 0.05578535143782608, 0.30664389871511366, 0.37974175355087897]
printing an ep nov before normalisation:  0.010284299092973015
printing an ep nov before normalisation:  57.06558203517612
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.04692000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.055865724678398725, 0.05578535143782608, 0.14617792017995637, 0.05578535143782608, 0.30664389871511366, 0.37974175355087897]
printing an ep nov before normalisation:  29.339942932128906
actor:  0 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  22 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  28 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  48.59543735295493
maxi score, test score, baseline:  -0.04372000000000012 0.6789999999999999 0.6789999999999999
actor:  1 policy actor:  1  step number:  32 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  29.878209243162367
printing an ep nov before normalisation:  23.5805344581604
actor:  1 policy actor:  1  step number:  35 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  19 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.04372000000000012 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.04372000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.055865724678398725, 0.05578535143782608, 0.14617792017995637, 0.05578535143782608, 0.30664389871511366, 0.37974175355087897]
printing an ep nov before normalisation:  25.559302738734655
maxi score, test score, baseline:  -0.04372000000000012 0.6789999999999999 0.6789999999999999
probs:  [0.055865724678398725, 0.05578535143782608, 0.14617792017995637, 0.05578535143782608, 0.30664389871511366, 0.37974175355087897]
printing an ep nov before normalisation:  0.32056062862295676
printing an ep nov before normalisation:  40.43871850534039
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  -0.04682000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.055865724678398725, 0.05578535143782608, 0.14617792017995637, 0.05578535143782608, 0.30664389871511366, 0.37974175355087897]
Printing some Q and Qe and total Qs values:  [[0.234]
 [0.283]
 [0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.234]
 [0.283]
 [0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]]
maxi score, test score, baseline:  -0.04682000000000011 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.04682000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.055865724678398725, 0.05578535143782608, 0.14617792017995637, 0.05578535143782608, 0.30664389871511366, 0.37974175355087897]
actor:  1 policy actor:  1  step number:  42 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  2.0
from probs:  [0.055865724678398725, 0.05578535143782608, 0.14617792017995637, 0.05578535143782608, 0.30664389871511366, 0.37974175355087897]
actor:  0 policy actor:  1  step number:  29 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  23 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.002]
 [ 0.076]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]] [[35.524]
 [55.99 ]
 [35.524]
 [35.524]
 [35.524]
 [35.524]
 [35.524]] [[0.399]
 [0.779]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]]
maxi score, test score, baseline:  -0.04402000000000011 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.04402000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.055865724678398725, 0.05578535143782608, 0.14617792017995637, 0.05578535143782608, 0.30664389871511366, 0.37974175355087897]
actor:  0 policy actor:  1  step number:  20 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.040840000000000105 0.6789999999999999 0.6789999999999999
actor:  1 policy actor:  1  step number:  21 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.040840000000000105 0.6789999999999999 0.6789999999999999
probs:  [0.055865724678398725, 0.05578535143782608, 0.14617792017995637, 0.05578535143782608, 0.30664389871511366, 0.37974175355087897]
using explorer policy with actor:  1
printing an ep nov before normalisation:  29.33738946914673
actor:  1 policy actor:  1  step number:  29 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  27.834738363703764
actor:  1 policy actor:  1  step number:  27 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8537108
using another actor
from probs:  [0.055865724678398725, 0.05578535143782608, 0.14617792017995637, 0.05578535143782608, 0.30664389871511366, 0.37974175355087897]
actor:  1 policy actor:  1  step number:  37 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[ 0.367]
 [ 0.65 ]
 [-0.011]
 [ 0.556]
 [ 0.455]
 [ 0.523]
 [ 0.623]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.367]
 [ 0.65 ]
 [-0.011]
 [ 0.556]
 [ 0.455]
 [ 0.523]
 [ 0.623]]
printing an ep nov before normalisation:  69.29230848685206
Printing some Q and Qe and total Qs values:  [[0.953]
 [0.953]
 [0.953]
 [0.953]
 [0.953]
 [0.953]
 [0.953]] [[82.805]
 [82.805]
 [82.805]
 [82.805]
 [82.805]
 [82.805]
 [82.805]] [[2.953]
 [2.953]
 [2.953]
 [2.953]
 [2.953]
 [2.953]
 [2.953]]
maxi score, test score, baseline:  -0.040840000000000105 0.6789999999999999 0.6789999999999999
probs:  [0.055865724678398725, 0.05578535143782608, 0.14617792017995637, 0.05578535143782608, 0.30664389871511366, 0.37974175355087897]
printing an ep nov before normalisation:  39.907806451724205
Printing some Q and Qe and total Qs values:  [[0.67 ]
 [0.466]
 [0.479]
 [0.486]
 [0.482]
 [0.454]
 [0.494]] [[27.905]
 [31.882]
 [18.69 ]
 [18.86 ]
 [18.713]
 [19.019]
 [29.022]] [[0.67 ]
 [0.466]
 [0.479]
 [0.486]
 [0.482]
 [0.454]
 [0.494]]
actor:  1 policy actor:  1  step number:  34 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  27 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.040840000000000105 0.6789999999999999 0.6789999999999999
probs:  [0.055865724678398725, 0.05578535143782608, 0.14617792017995637, 0.05578535143782608, 0.30664389871511366, 0.37974175355087897]
from probs:  [0.055865724678398725, 0.05578535143782608, 0.14617792017995637, 0.05578535143782608, 0.30664389871511366, 0.37974175355087897]
maxi score, test score, baseline:  -0.040840000000000105 0.6789999999999999 0.6789999999999999
probs:  [0.055865724678398725, 0.05578535143782608, 0.14617792017995637, 0.05578535143782608, 0.30664389871511366, 0.37974175355087897]
siam score:  -0.8537012
printing an ep nov before normalisation:  40.84221458685463
maxi score, test score, baseline:  -0.040840000000000105 0.6789999999999999 0.6789999999999999
actor:  1 policy actor:  1  step number:  37 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  27 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  40 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  0  action  0 :  tensor([0.4811, 0.0097, 0.1114, 0.0930, 0.1273, 0.0787, 0.0988],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0009,     0.9794,     0.0015,     0.0065,     0.0001,     0.0004,
            0.0112], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0400, 0.0138, 0.7783, 0.0416, 0.0396, 0.0351, 0.0515],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0944, 0.0782, 0.0916, 0.4243, 0.1033, 0.0728, 0.1355],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1418, 0.0650, 0.1511, 0.1384, 0.2731, 0.1036, 0.1270],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1048, 0.0029, 0.2284, 0.0896, 0.1073, 0.3546, 0.1124],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1214, 0.1610, 0.1293, 0.1355, 0.1185, 0.0975, 0.2369],
       grad_fn=<DivBackward0>)
siam score:  -0.8558376
maxi score, test score, baseline:  -0.038040000000000115 0.6789999999999999 0.6789999999999999
probs:  [0.055865724678398725, 0.05578535143782608, 0.14617792017995637, 0.05578535143782608, 0.30664389871511366, 0.37974175355087897]
printing an ep nov before normalisation:  0.0036214565852787928
actor:  0 policy actor:  1  step number:  21 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.037900000000000114 0.6789999999999999 0.6789999999999999
probs:  [0.055865724678398725, 0.05578535143782608, 0.14617792017995637, 0.05578535143782608, 0.30664389871511366, 0.37974175355087897]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.037900000000000114 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.037900000000000114 0.6789999999999999 0.6789999999999999
probs:  [0.055865724678398725, 0.05578535143782608, 0.14617792017995637, 0.05578535143782608, 0.30664389871511366, 0.37974175355087897]
Printing some Q and Qe and total Qs values:  [[0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]] [[48.98]
 [48.98]
 [48.98]
 [48.98]
 [48.98]
 [48.98]
 [48.98]] [[2.499]
 [2.499]
 [2.499]
 [2.499]
 [2.499]
 [2.499]
 [2.499]]
actor:  1 policy actor:  1  step number:  20 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.0
from probs:  [0.055865724678398725, 0.05578535143782608, 0.14617792017995637, 0.05578535143782608, 0.30664389871511366, 0.37974175355087897]
maxi score, test score, baseline:  -0.037900000000000114 0.6789999999999999 0.6789999999999999
probs:  [0.055865724678398725, 0.05578535143782608, 0.14617792017995637, 0.05578535143782608, 0.30664389871511366, 0.37974175355087897]
actor:  1 policy actor:  1  step number:  24 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.037900000000000114 0.6789999999999999 0.6789999999999999
probs:  [0.055865724678398725, 0.05578535143782608, 0.14617792017995637, 0.05578535143782608, 0.30664389871511366, 0.37974175355087897]
maxi score, test score, baseline:  -0.037900000000000114 0.6789999999999999 0.6789999999999999
actions average: 
K:  2  action  0 :  tensor([0.6229, 0.0444, 0.0548, 0.0755, 0.0982, 0.0444, 0.0599],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0052,     0.9633,     0.0052,     0.0021,     0.0008,     0.0014,
            0.0220], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0622, 0.0719, 0.5937, 0.0600, 0.0455, 0.0850, 0.0815],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0982, 0.0888, 0.0693, 0.4537, 0.0841, 0.0733, 0.1327],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1979, 0.0141, 0.1026, 0.1260, 0.3261, 0.0980, 0.1352],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1357, 0.0370, 0.1860, 0.1563, 0.1167, 0.2397, 0.1286],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1194, 0.0106, 0.1716, 0.1824, 0.0966, 0.1011, 0.3183],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  34 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  21 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
from probs:  [0.055865724678398725, 0.05578535143782608, 0.14617792017995637, 0.05578535143782608, 0.30664389871511366, 0.37974175355087897]
maxi score, test score, baseline:  -0.031760000000000115 0.6789999999999999 0.6789999999999999
probs:  [0.05586543937659967, 0.05578536826685358, 0.1461779643434125, 0.05578536826685358, 0.306643991402977, 0.3797418683433037]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.031760000000000115 0.6789999999999999 0.6789999999999999
probs:  [0.05586543937659967, 0.05578536826685358, 0.1461779643434125, 0.05578536826685358, 0.306643991402977, 0.3797418683433037]
printing an ep nov before normalisation:  35.43022632598877
actor:  1 policy actor:  1  step number:  41 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  30 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  38.22755335874568
maxi score, test score, baseline:  -0.03176000000000011 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.03176000000000011 0.6789999999999999 0.6789999999999999
probs:  [0.05586543937659967, 0.05578536826685358, 0.1461779643434125, 0.05578536826685358, 0.306643991402977, 0.3797418683433037]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  44.53840933915465
using another actor
actor:  1 policy actor:  1  step number:  34 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.031760000000000115 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.031760000000000115 0.6789999999999999 0.6789999999999999
probs:  [0.05586515621172203, 0.05578538496983102, 0.14617800817608245, 0.05578538496983102, 0.30664408339660454, 0.37974198227592887]
printing an ep nov before normalisation:  63.22035178104851
printing an ep nov before normalisation:  48.50979804992676
printing an ep nov before normalisation:  42.66691207885742
actor:  0 policy actor:  1  step number:  24 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  54.322705720326574
printing an ep nov before normalisation:  41.73122765287107
actor:  1 policy actor:  1  step number:  24 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  19 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  44.4654622705575
maxi score, test score, baseline:  -0.028500000000000122 0.6789999999999999 0.6789999999999999
probs:  [0.05586487515984743, 0.055785401548169265, 0.1461780516816688, 0.055785401548169265, 0.3066441747037672, 0.37974209535837794]
actor:  1 policy actor:  1  step number:  36 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.028500000000000122 0.6789999999999999 0.6789999999999999
actor:  1 policy actor:  1  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  0.667
using another actor
maxi score, test score, baseline:  -0.028500000000000122 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  -0.028500000000000122 0.6789999999999999 0.6789999999999999
probs:  [0.05586459619741161, 0.0557854180032583, 0.14617809486381947, 0.0557854180032583, 0.3066442653321201, 0.3797422076001323]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.076]
 [0.076]
 [0.076]
 [0.076]
 [0.076]
 [0.076]
 [0.076]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.076]
 [0.076]
 [0.076]
 [0.076]
 [0.076]
 [0.076]
 [0.076]]
maxi score, test score, baseline:  -0.028500000000000122 0.6789999999999999 0.6789999999999999
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.714]
 [0.625]
 [0.6  ]
 [0.625]
 [0.596]
 [0.604]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.598]
 [0.714]
 [0.625]
 [0.6  ]
 [0.625]
 [0.596]
 [0.604]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.658]
 [0.789]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]] [[56.886]
 [66.686]
 [56.886]
 [56.886]
 [56.886]
 [56.886]
 [56.886]] [[0.658]
 [0.789]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]]
printing an ep nov before normalisation:  38.69156380104885
printing an ep nov before normalisation:  65.31869699707177
maxi score, test score, baseline:  -0.028500000000000122 0.6789999999999999 0.6789999999999999
probs:  [0.05586459619741161, 0.0557854180032583, 0.14617809486381947, 0.0557854180032583, 0.3066442653321201, 0.3797422076001323]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.028500000000000122 0.6789999999999999 0.6789999999999999
probs:  [0.05586459619741161, 0.0557854180032583, 0.14617809486381947, 0.0557854180032583, 0.3066442653321201, 0.3797422076001323]
printing an ep nov before normalisation:  51.426182798066854
Printing some Q and Qe and total Qs values:  [[-0.029]
 [ 0.224]
 [-0.139]
 [ 0.094]
 [ 0.016]
 [ 0.092]
 [ 0.095]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.029]
 [ 0.224]
 [-0.139]
 [ 0.094]
 [ 0.016]
 [ 0.092]
 [ 0.095]]
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  0.026939999999999874 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  19 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.333
siam score:  -0.8732367
actor:  0 policy actor:  0  step number:  21 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.03017999999999988 0.6799999999999999 0.6799999999999999
actor:  1 policy actor:  1  step number:  32 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.08895934256295
maxi score, test score, baseline:  0.03017999999999988 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.03017999999999988 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.03017999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
maxi score, test score, baseline:  0.03017999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
actor:  0 policy actor:  1  step number:  24 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.030239999999999868 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
maxi score, test score, baseline:  0.030239999999999868 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.543]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]] [[56.576]
 [62.515]
 [56.576]
 [56.576]
 [56.576]
 [56.576]
 [56.576]] [[1.777]
 [2.145]
 [1.777]
 [1.777]
 [1.777]
 [1.777]
 [1.777]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  45 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  22 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  0.9237349490184954
actor:  1 policy actor:  1  step number:  23 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.030399999999999882 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
maxi score, test score, baseline:  0.030399999999999882 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
Printing some Q and Qe and total Qs values:  [[0.252]
 [0.384]
 [0.241]
 [0.246]
 [0.248]
 [0.26 ]
 [0.251]] [[68.112]
 [59.648]
 [67.458]
 [69.434]
 [66.212]
 [69.126]
 [67.563]] [[1.703]
 [1.603]
 [1.674]
 [1.734]
 [1.646]
 [1.739]
 [1.687]]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  26 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.030399999999999882 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
maxi score, test score, baseline:  0.030399999999999882 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
printing an ep nov before normalisation:  34.58484871136274
using explorer policy with actor:  1
maxi score, test score, baseline:  0.030399999999999882 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.030399999999999882 0.6799999999999999 0.6799999999999999
actor:  1 policy actor:  1  step number:  31 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
from probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
maxi score, test score, baseline:  0.030399999999999882 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
maxi score, test score, baseline:  0.030399999999999882 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
printing an ep nov before normalisation:  19.966775357653578
actor:  1 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.030399999999999882 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.030399999999999882 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  72.76637246087816
maxi score, test score, baseline:  0.030399999999999882 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
actor:  0 policy actor:  0  step number:  22 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  64.75664161428318
actor:  1 policy actor:  1  step number:  29 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  22 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.0
from probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
maxi score, test score, baseline:  0.031059999999999876 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
maxi score, test score, baseline:  0.031059999999999876 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
printing an ep nov before normalisation:  17.11999183491995
actor:  1 policy actor:  1  step number:  35 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  82.19044354970366
maxi score, test score, baseline:  0.031059999999999876 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
from probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
maxi score, test score, baseline:  0.031059999999999876 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
actor:  0 policy actor:  1  step number:  29 total reward:  0.5199999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  60.94790043945713
actor:  1 policy actor:  1  step number:  31 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.03409999999999987 0.6799999999999999 0.6799999999999999
printing an ep nov before normalisation:  49.51433292664956
actions average: 
K:  1  action  0 :  tensor([0.6492, 0.0089, 0.0617, 0.0562, 0.0629, 0.0460, 0.1151],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0042, 0.9557, 0.0085, 0.0090, 0.0015, 0.0037, 0.0174],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0321, 0.0376, 0.7153, 0.0354, 0.0208, 0.1102, 0.0486],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1187, 0.1341, 0.1133, 0.2798, 0.1007, 0.1110, 0.1424],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1578, 0.0039, 0.0952, 0.0906, 0.4500, 0.0903, 0.1121],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0725, 0.0023, 0.1018, 0.0857, 0.0789, 0.5663, 0.0923],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1442, 0.0953, 0.1257, 0.1582, 0.1342, 0.1376, 0.2047],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.822]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]] [[44.778]
 [51.93 ]
 [44.778]
 [44.778]
 [44.778]
 [44.778]
 [44.778]] [[0.661]
 [0.822]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]]
maxi score, test score, baseline:  0.03409999999999987 0.6799999999999999 0.6799999999999999
actor:  0 policy actor:  0  step number:  22 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.19 ]
 [0.82 ]
 [0.867]
 [0.556]
 [0.231]
 [0.367]
 [0.534]] [[40.22 ]
 [44.719]
 [48.573]
 [28.49 ]
 [29.144]
 [28.877]
 [33.255]] [[0.19 ]
 [0.82 ]
 [0.867]
 [0.556]
 [0.231]
 [0.367]
 [0.534]]
actor:  0 policy actor:  0  step number:  34 total reward:  0.24999999999999956  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  25.713470247056748
maxi score, test score, baseline:  0.03669999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
actor:  0 policy actor:  1  step number:  35 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.0
from probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
Printing some Q and Qe and total Qs values:  [[0.638]
 [0.675]
 [0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]] [[26.86]
 [38.69]
 [26.86]
 [26.86]
 [26.86]
 [26.86]
 [26.86]] [[0.638]
 [0.675]
 [0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]]
maxi score, test score, baseline:  0.03683999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
maxi score, test score, baseline:  0.03683999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
printing an ep nov before normalisation:  58.87291268532213
actor:  1 policy actor:  1  step number:  41 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  53.08392298503098
maxi score, test score, baseline:  0.03683999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
actor:  0 policy actor:  0  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.03999999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.03999999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
maxi score, test score, baseline:  0.03999999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
line 256 mcts: sample exp_bonus 39.99204874038696
printing an ep nov before normalisation:  39.649879355592184
maxi score, test score, baseline:  0.03999999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
actor:  1 policy actor:  1  step number:  35 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  0  action  0 :  tensor([0.5712, 0.0463, 0.0721, 0.0612, 0.1207, 0.0459, 0.0827],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0015,     0.9712,     0.0018,     0.0113,     0.0005,     0.0004,
            0.0134], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0611, 0.0008, 0.6694, 0.0596, 0.0702, 0.0629, 0.0760],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1907, 0.0015, 0.1419, 0.1520, 0.1726, 0.1354, 0.2059],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1867, 0.0017, 0.0933, 0.1953, 0.3177, 0.0813, 0.1240],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1846, 0.0112, 0.1680, 0.1495, 0.1715, 0.1345, 0.1808],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1455, 0.0040, 0.1422, 0.2377, 0.1370, 0.0963, 0.2372],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.904]
 [0.809]
 [0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]] [[47.811]
 [51.108]
 [53.344]
 [53.344]
 [53.344]
 [53.344]
 [53.344]] [[0.904]
 [0.809]
 [0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]]
printing an ep nov before normalisation:  65.20341511505444
maxi score, test score, baseline:  0.03999999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
line 256 mcts: sample exp_bonus 43.55411444970116
maxi score, test score, baseline:  0.03999999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
actor:  0 policy actor:  0  step number:  35 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.039519999999999875 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.039519999999999875 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
maxi score, test score, baseline:  0.039519999999999875 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
printing an ep nov before normalisation:  74.11359978232812
actor:  1 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  21 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.0
from probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
using another actor
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  21 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.03957999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
actor:  0 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.04265999999999988 0.6799999999999999 0.6799999999999999
actor:  1 policy actor:  1  step number:  33 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.563]
 [0.67 ]
 [0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.563]] [[71.491]
 [72.628]
 [71.491]
 [71.491]
 [71.491]
 [71.491]
 [71.491]] [[2.237]
 [2.382]
 [2.237]
 [2.237]
 [2.237]
 [2.237]
 [2.237]]
maxi score, test score, baseline:  0.04265999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
actor:  0 policy actor:  0  step number:  24 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  47.125845596425435
printing an ep nov before normalisation:  45.31708190350558
printing an ep nov before normalisation:  57.866691009429616
using another actor
maxi score, test score, baseline:  0.04325999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
maxi score, test score, baseline:  0.04325999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
printing an ep nov before normalisation:  0.11233443899527629
actor:  1 policy actor:  1  step number:  19 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.04325999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
Printing some Q and Qe and total Qs values:  [[0.65 ]
 [0.756]
 [0.637]
 [0.628]
 [0.648]
 [0.651]
 [0.641]] [[52.134]
 [47.987]
 [49.37 ]
 [50.947]
 [51.471]
 [52.153]
 [50.509]] [[0.65 ]
 [0.756]
 [0.637]
 [0.628]
 [0.648]
 [0.651]
 [0.641]]
maxi score, test score, baseline:  0.04325999999999988 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.04325999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.481]
 [0.175]
 [0.175]
 [0.175]
 [0.175]
 [0.175]] [[ 0.005]
 [ 0.005]
 [42.021]
 [42.021]
 [42.021]
 [42.021]
 [42.021]] [[0.481]
 [0.481]
 [0.175]
 [0.175]
 [0.175]
 [0.175]
 [0.175]]
maxi score, test score, baseline:  0.04325999999999989 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
printing an ep nov before normalisation:  61.731247660336535
maxi score, test score, baseline:  0.04325999999999989 0.6799999999999999 0.6799999999999999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.287]
 [0.426]
 [0.287]
 [0.287]
 [0.287]
 [0.287]
 [0.287]] [[38.221]
 [39.836]
 [38.221]
 [38.221]
 [38.221]
 [38.221]
 [38.221]] [[0.952]
 [1.146]
 [0.952]
 [0.952]
 [0.952]
 [0.952]
 [0.952]]
maxi score, test score, baseline:  0.04325999999999989 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
UNIT TEST: sample policy line 217 mcts : [0.02  0.898 0.    0.02  0.02  0.041 0.   ]
maxi score, test score, baseline:  0.04325999999999989 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  39 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.04325999999999989 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.04325999999999989 0.6799999999999999 0.6799999999999999
printing an ep nov before normalisation:  65.99268610707631
actor:  1 policy actor:  1  step number:  40 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.04325999999999989 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.04325999999999989 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.04325999999999989 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
Printing some Q and Qe and total Qs values:  [[0.26 ]
 [0.516]
 [0.26 ]
 [0.26 ]
 [0.26 ]
 [0.26 ]
 [0.26 ]] [[47.984]
 [47.112]
 [47.984]
 [47.984]
 [47.984]
 [47.984]
 [47.984]] [[0.821]
 [1.06 ]
 [0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.821]]
printing an ep nov before normalisation:  28.813910484313965
actor:  1 policy actor:  1  step number:  29 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  56.99832832208724
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  43.1845761919435
printing an ep nov before normalisation:  53.98157994614211
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.4010179635952227
actor:  1 policy actor:  1  step number:  30 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  31 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.04325999999999989 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
maxi score, test score, baseline:  0.04325999999999989 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.04325999999999989 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
maxi score, test score, baseline:  0.04325999999999989 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.332]
 [0.332]
 [0.332]
 [0.332]
 [0.332]
 [0.332]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.332]
 [0.332]
 [0.332]
 [0.332]
 [0.332]
 [0.332]
 [0.332]]
printing an ep nov before normalisation:  44.36186119733414
maxi score, test score, baseline:  0.04325999999999989 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
Printing some Q and Qe and total Qs values:  [[-0.043]
 [ 0.109]
 [-0.182]
 [ 0.101]
 [ 0.053]
 [-0.185]
 [ 0.12 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.043]
 [ 0.109]
 [-0.182]
 [ 0.101]
 [ 0.053]
 [-0.185]
 [ 0.12 ]]
printing an ep nov before normalisation:  29.40735633359609
maxi score, test score, baseline:  0.04325999999999989 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
printing an ep nov before normalisation:  35.886898040771484
actor:  1 policy actor:  1  step number:  40 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  25.841864286832834
siam score:  -0.87095535
actor:  1 policy actor:  1  step number:  31 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.04325999999999989 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.04325999999999989 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
printing an ep nov before normalisation:  53.09886321808516
maxi score, test score, baseline:  0.04325999999999989 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.04325999999999989 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
maxi score, test score, baseline:  0.04325999999999989 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.04325999999999989 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
maxi score, test score, baseline:  0.04325999999999989 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
actor:  1 policy actor:  1  step number:  27 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  21 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  29 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  0.46768309570192435
maxi score, test score, baseline:  0.04325999999999989 0.6799999999999999 0.6799999999999999
actor:  1 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  51.053329092818124
maxi score, test score, baseline:  0.04325999999999989 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
actor:  1 policy actor:  1  step number:  19 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  0.07821479317271951
maxi score, test score, baseline:  0.04325999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
maxi score, test score, baseline:  0.04325999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.04325999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
printing an ep nov before normalisation:  40.34935445723059
actions average: 
K:  3  action  0 :  tensor([0.6235, 0.0019, 0.0807, 0.0750, 0.0951, 0.0554, 0.0684],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0011,     0.9717,     0.0028,     0.0039,     0.0002,     0.0005,
            0.0199], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0627, 0.0155, 0.6673, 0.0690, 0.0677, 0.0534, 0.0644],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1573, 0.0151, 0.1731, 0.2296, 0.1390, 0.1093, 0.1766],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1734, 0.0025, 0.1963, 0.1465, 0.1694, 0.1399, 0.1721],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0851, 0.1555, 0.1439, 0.0727, 0.0605, 0.4120, 0.0703],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0867, 0.1698, 0.0279, 0.0332, 0.0281, 0.0187, 0.6356],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.04325999999999988 0.6799999999999999 0.6799999999999999
line 256 mcts: sample exp_bonus 0.0
actor:  1 policy actor:  1  step number:  38 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.04325999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
actor:  1 policy actor:  1  step number:  35 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.04325999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
Printing some Q and Qe and total Qs values:  [[0.232]
 [0.266]
 [0.255]
 [0.243]
 [0.231]
 [0.235]
 [0.252]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.232]
 [0.266]
 [0.255]
 [0.243]
 [0.231]
 [0.235]
 [0.252]]
maxi score, test score, baseline:  0.04325999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
maxi score, test score, baseline:  0.04325999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
maxi score, test score, baseline:  0.04325999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
maxi score, test score, baseline:  0.04325999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.04325999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
printing an ep nov before normalisation:  28.87559167768862
maxi score, test score, baseline:  0.04325999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
line 256 mcts: sample exp_bonus 6.575743545119508
using another actor
actor:  1 policy actor:  1  step number:  40 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  52.08445433153477
maxi score, test score, baseline:  0.043259999999999875 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
printing an ep nov before normalisation:  52.37817765167984
using explorer policy with actor:  1
maxi score, test score, baseline:  0.043259999999999875 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.043259999999999875 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
actor:  1 policy actor:  1  step number:  41 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  30 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  31 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
using another actor
Printing some Q and Qe and total Qs values:  [[0.191]
 [0.217]
 [0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]] [[37.927]
 [42.872]
 [37.927]
 [37.927]
 [37.927]
 [37.927]
 [37.927]] [[1.536]
 [1.923]
 [1.536]
 [1.536]
 [1.536]
 [1.536]
 [1.536]]
maxi score, test score, baseline:  0.043259999999999875 0.6799999999999999 0.6799999999999999
printing an ep nov before normalisation:  43.519728962131055
printing an ep nov before normalisation:  49.82517780875735
printing an ep nov before normalisation:  43.9814027962188
maxi score, test score, baseline:  0.043259999999999875 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.043259999999999875 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  53.332285461102565
actor:  0 policy actor:  0  step number:  36 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  38 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.043459999999999874 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
actor:  1 policy actor:  1  step number:  30 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.166]
 [0.109]
 [0.166]
 [0.166]
 [0.166]
 [0.166]
 [0.166]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.166]
 [0.109]
 [0.166]
 [0.166]
 [0.166]
 [0.166]
 [0.166]]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.043459999999999874 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
maxi score, test score, baseline:  0.043459999999999874 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
printing an ep nov before normalisation:  25.685014724731445
actor:  1 policy actor:  1  step number:  41 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.043459999999999874 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
actor:  1 policy actor:  1  step number:  38 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.043459999999999874 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.043459999999999874 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.043459999999999874 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
printing an ep nov before normalisation:  42.268131060801444
maxi score, test score, baseline:  0.043459999999999874 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
Printing some Q and Qe and total Qs values:  [[0.579]
 [0.636]
 [0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]] [[31.733]
 [38.231]
 [31.733]
 [38.556]
 [31.733]
 [31.733]
 [31.733]] [[1.308]
 [1.514]
 [1.308]
 [1.465]
 [1.308]
 [1.308]
 [1.308]]
actor:  1 policy actor:  1  step number:  31 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.043459999999999874 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
Printing some Q and Qe and total Qs values:  [[ 0.004]
 [ 0.134]
 [-0.019]
 [ 0.037]
 [-0.004]
 [-0.005]
 [ 0.004]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.004]
 [ 0.134]
 [-0.019]
 [ 0.037]
 [-0.004]
 [-0.005]
 [ 0.004]]
actor:  1 policy actor:  1  step number:  43 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.043459999999999874 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
printing an ep nov before normalisation:  37.41978039315264
maxi score, test score, baseline:  0.043459999999999874 0.6799999999999999 0.6799999999999999
actor:  1 policy actor:  1  step number:  31 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  0.09678813018354049
actor:  1 policy actor:  1  step number:  24 total reward:  0.69  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]] [[41.439]
 [41.439]
 [41.439]
 [41.439]
 [41.439]
 [41.439]
 [41.439]] [[1.567]
 [1.567]
 [1.567]
 [1.567]
 [1.567]
 [1.567]
 [1.567]]
maxi score, test score, baseline:  0.040239999999999866 0.6799999999999999 0.6799999999999999
probs:  [0.05581175357691834, 0.05581175357691834, 0.14449568404871702, 0.05581175357691834, 0.308560955421545, 0.379508099798983]
siam score:  -0.85425943
maxi score, test score, baseline:  0.040239999999999866 0.6799999999999999 0.6799999999999999
probs:  [0.05581175357691834, 0.05581175357691834, 0.14449568404871702, 0.05581175357691834, 0.308560955421545, 0.379508099798983]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.126]
 [0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.126]
 [0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]]
actor:  0 policy actor:  1  step number:  24 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.04039999999999989 0.6799999999999999 0.6799999999999999
probs:  [0.05581175357691834, 0.05581175357691834, 0.14449568404871702, 0.05581175357691834, 0.308560955421545, 0.379508099798983]
printing an ep nov before normalisation:  57.29337067466103
maxi score, test score, baseline:  0.04039999999999989 0.6799999999999999 0.6799999999999999
probs:  [0.05581175357691834, 0.05581175357691834, 0.14449568404871702, 0.05581175357691834, 0.308560955421545, 0.379508099798983]
printing an ep nov before normalisation:  4.625326255336404e-06
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  54 total reward:  0.2899999999999996  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  30 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 0.0
actions average: 
K:  0  action  0 :  tensor([0.5758, 0.0211, 0.0788, 0.0889, 0.0799, 0.0822, 0.0734],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0009,     0.9866,     0.0013,     0.0048,     0.0001,     0.0001,
            0.0061], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0854, 0.0054, 0.6152, 0.0668, 0.0785, 0.0689, 0.0799],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0477, 0.0985, 0.1065, 0.4852, 0.0696, 0.0532, 0.1394],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1603, 0.0062, 0.1400, 0.1815, 0.1650, 0.1655, 0.1816],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1157, 0.0072, 0.1821, 0.1400, 0.1237, 0.2968, 0.1343],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1488, 0.0271, 0.0952, 0.1379, 0.1326, 0.1013, 0.3571],
       grad_fn=<DivBackward0>)
siam score:  -0.86136055
maxi score, test score, baseline:  0.04039999999999989 0.6799999999999999 0.6799999999999999
probs:  [0.05581175357691834, 0.05581175357691834, 0.14449568404871702, 0.05581175357691834, 0.308560955421545, 0.379508099798983]
Printing some Q and Qe and total Qs values:  [[0.243]
 [0.734]
 [0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.243]
 [0.734]
 [0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]]
printing an ep nov before normalisation:  27.265506245093256
actor:  1 policy actor:  1  step number:  31 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  72.63599185353038
maxi score, test score, baseline:  0.04039999999999989 0.6799999999999999 0.6799999999999999
probs:  [0.05581175357691834, 0.05581175357691834, 0.14449568404871702, 0.05581175357691834, 0.308560955421545, 0.379508099798983]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  41.241711563486795
actor:  1 policy actor:  1  step number:  33 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  34 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.04039999999999989 0.6799999999999999 0.6799999999999999
probs:  [0.05581175357691834, 0.05581175357691834, 0.14449568404871702, 0.05581175357691834, 0.308560955421545, 0.379508099798983]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  21 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.04039999999999989 0.6799999999999999 0.6799999999999999
probs:  [0.05581175357691834, 0.05581175357691834, 0.14449568404871702, 0.05581175357691834, 0.308560955421545, 0.379508099798983]
maxi score, test score, baseline:  0.04039999999999989 0.6799999999999999 0.6799999999999999
probs:  [0.05581175357691834, 0.05581175357691834, 0.14449568404871702, 0.05581175357691834, 0.308560955421545, 0.379508099798983]
maxi score, test score, baseline:  0.04039999999999989 0.6799999999999999 0.6799999999999999
probs:  [0.05581175357691834, 0.05581175357691834, 0.14449568404871702, 0.05581175357691834, 0.308560955421545, 0.379508099798983]
printing an ep nov before normalisation:  38.88106107711792
printing an ep nov before normalisation:  38.36194160126535
maxi score, test score, baseline:  0.04039999999999989 0.6799999999999999 0.6799999999999999
probs:  [0.05581175357691834, 0.05581175357691834, 0.14449568404871702, 0.05581175357691834, 0.308560955421545, 0.379508099798983]
actor:  0 policy actor:  1  step number:  23 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.04343999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.05581175357691834, 0.05581175357691834, 0.14449568404871702, 0.05581175357691834, 0.308560955421545, 0.379508099798983]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  29.905031264217136
maxi score, test score, baseline:  0.04343999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.05581175357691834, 0.05581175357691834, 0.14449568404871702, 0.05581175357691834, 0.308560955421545, 0.379508099798983]
printing an ep nov before normalisation:  28.327939561981175
maxi score, test score, baseline:  0.04343999999999988 0.6799999999999999 0.6799999999999999
printing an ep nov before normalisation:  52.7953445792607
actor:  1 policy actor:  1  step number:  34 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.04343999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.05581175357691834, 0.05581175357691834, 0.14449568404871702, 0.05581175357691834, 0.308560955421545, 0.379508099798983]
maxi score, test score, baseline:  0.04343999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.05581175357691834, 0.05581175357691834, 0.14449568404871702, 0.05581175357691834, 0.308560955421545, 0.379508099798983]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.04343999999999988 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.04343999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.05581175357691834, 0.05581175357691834, 0.14449568404871702, 0.05581175357691834, 0.308560955421545, 0.379508099798983]
printing an ep nov before normalisation:  38.6587744490537
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.642]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]] [[49.017]
 [49.75 ]
 [46.157]
 [46.157]
 [46.157]
 [46.157]
 [46.157]] [[0.9  ]
 [0.881]
 [0.747]
 [0.747]
 [0.747]
 [0.747]
 [0.747]]
actor:  0 policy actor:  1  step number:  26 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.046299999999999876 0.6799999999999999 0.6799999999999999
probs:  [0.05581175357691834, 0.05581175357691834, 0.14449568404871702, 0.05581175357691834, 0.308560955421545, 0.379508099798983]
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.599]
 [0.544]
 [0.568]
 [0.585]
 [0.547]
 [0.585]] [[44.559]
 [40.356]
 [42.289]
 [41.728]
 [40.068]
 [43.098]
 [40.068]] [[0.876]
 [0.875]
 [0.846]
 [0.863]
 [0.857]
 [0.86 ]
 [0.857]]
actor:  1 policy actor:  1  step number:  40 total reward:  0.3299999999999996  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  26 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.522]
 [0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]] [[53.826]
 [56.402]
 [53.826]
 [53.826]
 [53.826]
 [53.826]
 [53.826]] [[1.339]
 [1.62 ]
 [1.339]
 [1.339]
 [1.339]
 [1.339]
 [1.339]]
maxi score, test score, baseline:  0.046299999999999876 0.6799999999999999 0.6799999999999999
probs:  [0.05581175357691834, 0.05581175357691834, 0.14449568404871702, 0.05581175357691834, 0.308560955421545, 0.379508099798983]
maxi score, test score, baseline:  0.046299999999999876 0.6799999999999999 0.6799999999999999
probs:  [0.05581175357691834, 0.05581175357691834, 0.14449568404871702, 0.05581175357691834, 0.308560955421545, 0.379508099798983]
printing an ep nov before normalisation:  69.98955092119269
maxi score, test score, baseline:  0.046299999999999876 0.6799999999999999 0.6799999999999999
probs:  [0.05581175357691834, 0.05581175357691834, 0.14449568404871702, 0.05581175357691834, 0.308560955421545, 0.379508099798983]
printing an ep nov before normalisation:  69.77701453775701
actor:  1 policy actor:  1  step number:  39 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
using another actor
actor:  1 policy actor:  1  step number:  37 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.046299999999999876 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.046299999999999876 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
printing an ep nov before normalisation:  41.22270584106445
printing an ep nov before normalisation:  42.99707749842969
printing an ep nov before normalisation:  50.471234767217446
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.774]
 [0.686]
 [0.752]
 [0.658]
 [0.644]
 [0.731]] [[44.799]
 [44.606]
 [49.712]
 [42.686]
 [44.257]
 [42.752]
 [41.134]] [[1.715]
 [2.039]
 [2.194]
 [1.925]
 [1.907]
 [1.821]
 [1.831]]
actor:  1 policy actor:  1  step number:  38 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  31 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  26 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.049019999999999876 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
maxi score, test score, baseline:  0.049019999999999876 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.049019999999999876 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
maxi score, test score, baseline:  0.049019999999999876 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
printing an ep nov before normalisation:  45.62972068786621
maxi score, test score, baseline:  0.049019999999999876 0.6799999999999999 0.6799999999999999
actor:  0 policy actor:  0  step number:  27 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  33.50667612195895
actor:  1 policy actor:  1  step number:  38 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.051979999999999894 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
maxi score, test score, baseline:  0.051979999999999894 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
actor:  0 policy actor:  0  step number:  20 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
using another actor
using explorer policy with actor:  0
printing an ep nov before normalisation:  40.70199196707813
actor:  1 policy actor:  1  step number:  25 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.052219999999999864 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
actor:  1 policy actor:  1  step number:  24 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.052219999999999864 0.6799999999999999 0.6799999999999999
probs:  [0.05605994201535234, 0.05605994201535234, 0.14513918334527007, 0.05605994201535234, 0.30548181773912164, 0.3811991728695512]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.742]
 [0.742]
 [0.862]
 [0.742]
 [0.742]
 [0.742]
 [0.742]] [[40.17 ]
 [40.17 ]
 [49.133]
 [40.17 ]
 [40.17 ]
 [40.17 ]
 [40.17 ]] [[0.742]
 [0.742]
 [0.862]
 [0.742]
 [0.742]
 [0.742]
 [0.742]]
actions average: 
K:  4  action  0 :  tensor([0.6954, 0.0251, 0.0564, 0.0555, 0.0663, 0.0482, 0.0531],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0017, 0.9630, 0.0062, 0.0155, 0.0012, 0.0062, 0.0062],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0459, 0.0720, 0.6524, 0.0496, 0.0511, 0.0652, 0.0639],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1182, 0.0155, 0.1202, 0.3523, 0.1257, 0.1419, 0.1261],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2703, 0.0230, 0.1156, 0.1006, 0.2415, 0.0946, 0.1544],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0637, 0.0192, 0.3059, 0.1037, 0.0818, 0.3450, 0.0807],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1260, 0.1674, 0.1027, 0.1527, 0.1013, 0.1098, 0.2401],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  20 total reward:  0.69  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.05221999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.05630113602919519, 0.05630113602919519, 0.14459356053917238, 0.05630113602919519, 0.3035199246571316, 0.3829831067161104]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.049]
 [-0.049]
 [ 0.37 ]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]] [[40.691]
 [40.691]
 [51.888]
 [40.691]
 [40.691]
 [40.691]
 [40.691]] [[0.83 ]
 [0.83 ]
 [1.638]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]]
Printing some Q and Qe and total Qs values:  [[-0.105]
 [-0.095]
 [-0.13 ]
 [-0.163]
 [-0.168]
 [-0.214]
 [-0.22 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.105]
 [-0.095]
 [-0.13 ]
 [-0.163]
 [-0.168]
 [-0.214]
 [-0.22 ]]
Printing some Q and Qe and total Qs values:  [[0.106]
 [0.111]
 [0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.106]] [[ 0.   ]
 [42.559]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.479]
 [ 1.024]
 [-0.479]
 [-0.479]
 [-0.479]
 [-0.479]
 [-0.479]]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  26 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  60.33917903624972
maxi score, test score, baseline:  0.04909999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.05630113602919519, 0.05630113602919519, 0.14459356053917238, 0.05630113602919519, 0.3035199246571316, 0.3829831067161104]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.197]
 [0.166]
 [0.196]
 [0.187]
 [0.195]
 [0.204]
 [0.208]] [[29.48 ]
 [50.218]
 [29.048]
 [27.384]
 [27.46 ]
 [28.059]
 [27.603]] [[0.715]
 [1.553]
 [0.696]
 [0.618]
 [0.629]
 [0.662]
 [0.647]]
printing an ep nov before normalisation:  62.39361666279769
printing an ep nov before normalisation:  41.802987842985075
printing an ep nov before normalisation:  66.38955813031548
actor:  1 policy actor:  1  step number:  30 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  28 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.323]
 [0.524]
 [0.323]
 [0.323]
 [0.323]
 [0.323]
 [0.323]] [[47.604]
 [53.227]
 [47.604]
 [47.604]
 [47.604]
 [47.604]
 [47.604]] [[1.722]
 [2.155]
 [1.722]
 [1.722]
 [1.722]
 [1.722]
 [1.722]]
maxi score, test score, baseline:  0.04909999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.05630113602919519, 0.05630113602919519, 0.14459356053917238, 0.05630113602919519, 0.3035199246571316, 0.3829831067161104]
Printing some Q and Qe and total Qs values:  [[0.633]
 [0.741]
 [0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]] [[40.178]
 [45.735]
 [40.178]
 [40.178]
 [40.178]
 [40.178]
 [40.178]] [[0.633]
 [0.741]
 [0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]]
maxi score, test score, baseline:  0.04909999999999987 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.04909999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.05630113602919519, 0.05630113602919519, 0.14459356053917238, 0.05630113602919519, 0.3035199246571316, 0.3829831067161104]
actor:  0 policy actor:  1  step number:  21 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  28 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  51.41781163521589
maxi score, test score, baseline:  0.05335999999999988 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.05335999999999988 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.05335999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.05630113602919519, 0.05630113602919519, 0.14459356053917238, 0.05630113602919519, 0.3035199246571316, 0.3829831067161104]
Printing some Q and Qe and total Qs values:  [[0.579]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]] [[63.323]
 [71.303]
 [71.303]
 [71.303]
 [71.303]
 [71.303]
 [71.303]] [[1.629]
 [1.624]
 [1.624]
 [1.624]
 [1.624]
 [1.624]
 [1.624]]
printing an ep nov before normalisation:  60.62735076405783
maxi score, test score, baseline:  0.05335999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.05630113602919519, 0.05630113602919519, 0.14459356053917238, 0.05630113602919519, 0.3035199246571316, 0.3829831067161104]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  58.51135685305921
actor:  1 policy actor:  1  step number:  24 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  25 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
from probs:  [0.05630113602919519, 0.05630113602919519, 0.14459356053917238, 0.05630113602919519, 0.3035199246571316, 0.3829831067161104]
maxi score, test score, baseline:  0.05335999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.05630113602919519, 0.05630113602919519, 0.14459356053917238, 0.05630113602919519, 0.3035199246571316, 0.3829831067161104]
actor:  0 policy actor:  0  step number:  24 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.049719999999999875 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.049719999999999875 0.6799999999999999 0.6799999999999999
probs:  [0.05630113602919519, 0.05630113602919519, 0.14459356053917238, 0.05630113602919519, 0.3035199246571316, 0.3829831067161104]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
Printing some Q and Qe and total Qs values:  [[0.214]
 [0.303]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]] [[58.164]
 [65.677]
 [58.164]
 [58.164]
 [58.164]
 [58.164]
 [58.164]] [[0.718]
 [0.882]
 [0.718]
 [0.718]
 [0.718]
 [0.718]
 [0.718]]
actor:  1 policy actor:  1  step number:  32 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  29 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.049719999999999875 0.6799999999999999 0.6799999999999999
probs:  [0.05630113602919519, 0.05630113602919519, 0.14459356053917238, 0.05630113602919519, 0.3035199246571316, 0.3829831067161104]
maxi score, test score, baseline:  0.049719999999999875 0.6799999999999999 0.6799999999999999
probs:  [0.05630113602919519, 0.05630113602919519, 0.14459356053917238, 0.05630113602919519, 0.3035199246571316, 0.3829831067161104]
siam score:  -0.8618013
printing an ep nov before normalisation:  51.03102146980426
maxi score, test score, baseline:  0.04635999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.05630113602919519, 0.05630113602919519, 0.14459356053917238, 0.05630113602919519, 0.3035199246571316, 0.3829831067161104]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.04635999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.05630113602919519, 0.05630113602919519, 0.14459356053917238, 0.05630113602919519, 0.3035199246571316, 0.3829831067161104]
maxi score, test score, baseline:  0.04635999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.05630113602919519, 0.05630113602919519, 0.14459356053917238, 0.05630113602919519, 0.3035199246571316, 0.3829831067161104]
maxi score, test score, baseline:  0.04635999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.05630113602919519, 0.05630113602919519, 0.14459356053917238, 0.05630113602919519, 0.3035199246571316, 0.3829831067161104]
Printing some Q and Qe and total Qs values:  [[0.848]
 [0.847]
 [0.847]
 [0.847]
 [0.847]
 [0.844]
 [0.847]] [[50.149]
 [49.902]
 [49.902]
 [49.902]
 [49.902]
 [53.244]
 [49.902]] [[2.312]
 [2.301]
 [2.301]
 [2.301]
 [2.301]
 [2.444]
 [2.301]]
actor:  1 policy actor:  1  step number:  40 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 44.259086804737336
actor:  1 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  23 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.04635999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.05630113602919519, 0.05630113602919519, 0.14459356053917238, 0.05630113602919519, 0.3035199246571316, 0.3829831067161104]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.04635999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.05630113602919519, 0.05630113602919519, 0.14459356053917238, 0.05630113602919519, 0.3035199246571316, 0.3829831067161104]
actions average: 
K:  3  action  0 :  tensor([0.6421, 0.0051, 0.0616, 0.0563, 0.1115, 0.0532, 0.0701],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0044, 0.9616, 0.0081, 0.0067, 0.0023, 0.0029, 0.0141],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0293, 0.0231, 0.7700, 0.0242, 0.0265, 0.0765, 0.0505],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0777, 0.2564, 0.1173, 0.3292, 0.0363, 0.0592, 0.1240],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1072, 0.0204, 0.1022, 0.0976, 0.5082, 0.0942, 0.0702],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1271, 0.0143, 0.2176, 0.1230, 0.1098, 0.3095, 0.0987],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1887, 0.1331, 0.1150, 0.0950, 0.1208, 0.0735, 0.2741],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.04635999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.05630113602919519, 0.05630113602919519, 0.14459356053917238, 0.05630113602919519, 0.3035199246571316, 0.3829831067161104]
actor:  1 policy actor:  1  step number:  20 total reward:  0.75  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  22 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  34 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.04635999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
maxi score, test score, baseline:  0.04635999999999987 0.6799999999999999 0.6799999999999999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.747]
 [0.84 ]
 [0.747]
 [0.747]
 [0.747]
 [0.747]
 [0.747]] [[25.672]
 [29.804]
 [25.672]
 [25.672]
 [25.672]
 [25.672]
 [25.672]] [[1.38 ]
 [1.674]
 [1.38 ]
 [1.38 ]
 [1.38 ]
 [1.38 ]
 [1.38 ]]
printing an ep nov before normalisation:  30.70426926818376
actor:  1 policy actor:  1  step number:  28 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.04635999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  1 policy actor:  1  step number:  25 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.745]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.652]
 [0.745]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]]
printing an ep nov before normalisation:  61.761265614789764
actor:  1 policy actor:  1  step number:  33 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.042999999999999865 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.042999999999999865 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8691772
maxi score, test score, baseline:  0.042999999999999865 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]] [[44.779]
 [44.779]
 [44.779]
 [44.779]
 [44.779]
 [44.779]
 [44.779]] [[1.301]
 [1.301]
 [1.301]
 [1.301]
 [1.301]
 [1.301]
 [1.301]]
printing an ep nov before normalisation:  50.157114498966756
maxi score, test score, baseline:  0.042999999999999865 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  59.053854798721304
maxi score, test score, baseline:  0.042999999999999865 0.6799999999999999 0.6799999999999999
actor:  1 policy actor:  1  step number:  28 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  1.0
from probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  1 policy actor:  1  step number:  35 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.461]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]] [[46.07 ]
 [48.259]
 [46.07 ]
 [46.07 ]
 [46.07 ]
 [46.07 ]
 [46.07 ]] [[1.308]
 [1.509]
 [1.308]
 [1.308]
 [1.308]
 [1.308]
 [1.308]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.23629188537598
Printing some Q and Qe and total Qs values:  [[0.232]
 [0.44 ]
 [0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]] [[48.363]
 [45.272]
 [48.363]
 [48.363]
 [48.363]
 [48.363]
 [48.363]] [[0.232]
 [0.44 ]
 [0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]]
maxi score, test score, baseline:  0.042999999999999865 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
printing an ep nov before normalisation:  32.83434867858887
maxi score, test score, baseline:  0.042999999999999865 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.042999999999999865 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
maxi score, test score, baseline:  0.042999999999999865 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  0 policy actor:  0  step number:  24 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
from probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actions average: 
K:  1  action  0 :  tensor([0.6289, 0.0086, 0.0835, 0.0678, 0.0782, 0.0598, 0.0733],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0059, 0.9220, 0.0110, 0.0235, 0.0060, 0.0053, 0.0263],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0903, 0.0020, 0.5631, 0.0837, 0.0859, 0.0860, 0.0890],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0941, 0.0310, 0.1110, 0.4344, 0.1095, 0.1081, 0.1117],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1332, 0.0041, 0.0908, 0.0942, 0.4920, 0.0754, 0.1102],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1309, 0.0064, 0.1782, 0.1314, 0.1349, 0.2784, 0.1399],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1007, 0.0768, 0.0778, 0.0943, 0.0562, 0.0527, 0.5414],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  32 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.648]
 [0.722]
 [0.648]
 [0.648]
 [0.697]
 [0.648]
 [0.648]] [[45.894]
 [44.417]
 [45.894]
 [45.894]
 [46.456]
 [45.894]
 [45.894]] [[2.18 ]
 [2.204]
 [2.18 ]
 [2.18 ]
 [2.248]
 [2.18 ]
 [2.18 ]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.03925999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
maxi score, test score, baseline:  0.03925999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
maxi score, test score, baseline:  0.03925999999999987 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.03925999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
printing an ep nov before normalisation:  0.02783318918829991
maxi score, test score, baseline:  0.03925999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  1 policy actor:  1  step number:  38 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  62.152119773384925
printing an ep nov before normalisation:  72.70047447763274
maxi score, test score, baseline:  0.03925999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  1 policy actor:  1  step number:  37 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]] [[48.413]
 [48.413]
 [48.413]
 [48.413]
 [48.413]
 [48.413]
 [48.413]] [[1.269]
 [1.269]
 [1.269]
 [1.269]
 [1.269]
 [1.269]
 [1.269]]
actor:  1 policy actor:  1  step number:  26 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  34 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.782]
 [0.782]
 [0.782]
 [0.782]
 [0.782]
 [0.782]
 [0.782]] [[35.077]
 [35.077]
 [35.077]
 [35.077]
 [35.077]
 [35.077]
 [35.077]] [[0.782]
 [0.782]
 [0.782]
 [0.782]
 [0.782]
 [0.782]
 [0.782]]
maxi score, test score, baseline:  0.03925999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
maxi score, test score, baseline:  0.03925999999999987 0.6799999999999999 0.6799999999999999
using another actor
actor:  0 policy actor:  0  step number:  26 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  22 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  39.048755168914795
printing an ep nov before normalisation:  59.64289452948093
maxi score, test score, baseline:  0.03571999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
printing an ep nov before normalisation:  56.39903749693596
maxi score, test score, baseline:  0.03571999999999988 0.6799999999999999 0.6799999999999999
printing an ep nov before normalisation:  61.56452660586152
maxi score, test score, baseline:  0.03571999999999988 0.6799999999999999 0.6799999999999999
printing an ep nov before normalisation:  56.3950920735603
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.526]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]] [[54.71 ]
 [56.805]
 [54.71 ]
 [54.71 ]
 [54.71 ]
 [54.71 ]
 [54.71 ]] [[1.024]
 [1.264]
 [1.024]
 [1.024]
 [1.024]
 [1.024]
 [1.024]]
maxi score, test score, baseline:  0.03571999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
maxi score, test score, baseline:  0.03571999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
maxi score, test score, baseline:  0.03571999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
printing an ep nov before normalisation:  47.51484162741848
printing an ep nov before normalisation:  32.8094119149884
actor:  1 policy actor:  1  step number:  28 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  31 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  43.59565178553264
printing an ep nov before normalisation:  40.60540985771724
maxi score, test score, baseline:  0.03571999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
maxi score, test score, baseline:  0.03571999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  1 policy actor:  1  step number:  24 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.03571999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[ 0.21 ]
 [ 0.085]
 [-0.113]
 [ 0.077]
 [-0.005]
 [-0.149]
 [ 0.053]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.21 ]
 [ 0.085]
 [-0.113]
 [ 0.077]
 [-0.005]
 [-0.149]
 [ 0.053]]
maxi score, test score, baseline:  0.03571999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
maxi score, test score, baseline:  0.03571999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
printing an ep nov before normalisation:  54.60009804660026
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  21 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  46.068387031555176
maxi score, test score, baseline:  0.03571999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.758]
 [0.752]
 [0.66 ]
 [0.66 ]
 [0.675]
 [0.66 ]
 [0.66 ]] [[53.99 ]
 [51.135]
 [46.288]
 [46.288]
 [49.32 ]
 [46.288]
 [46.288]] [[1.901]
 [1.809]
 [1.571]
 [1.571]
 [1.677]
 [1.571]
 [1.571]]
maxi score, test score, baseline:  0.03571999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
maxi score, test score, baseline:  0.03571999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
maxi score, test score, baseline:  0.03571999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.03571999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  1 policy actor:  1  step number:  33 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  43.28977584838867
actions average: 
K:  0  action  0 :  tensor([0.5469, 0.0025, 0.0582, 0.0769, 0.1530, 0.0717, 0.0908],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0056,     0.9797,     0.0027,     0.0039,     0.0006,     0.0007,
            0.0068], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0795, 0.0011, 0.5964, 0.0809, 0.0705, 0.0961, 0.0755],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1159, 0.0406, 0.0832, 0.4191, 0.0903, 0.1255, 0.1254],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2099, 0.0017, 0.0962, 0.0976, 0.3535, 0.0880, 0.1530],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1333, 0.0014, 0.2128, 0.1308, 0.1348, 0.2413, 0.1456],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0671, 0.3132, 0.0571, 0.1138, 0.0630, 0.0726, 0.3133],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  27 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  70.00231237781301
printing an ep nov before normalisation:  45.098472250512565
actor:  1 policy actor:  1  step number:  27 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.03571999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
maxi score, test score, baseline:  0.03571999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
printing an ep nov before normalisation:  52.371625922920664
actor:  1 policy actor:  1  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  4  action  0 :  tensor([0.6164, 0.0030, 0.0759, 0.0842, 0.0941, 0.0604, 0.0659],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0045, 0.9742, 0.0065, 0.0048, 0.0013, 0.0022, 0.0065],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0281, 0.0009, 0.8301, 0.0352, 0.0323, 0.0427, 0.0307],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1011, 0.1402, 0.1031, 0.3180, 0.1115, 0.1254, 0.1008],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2993, 0.0813, 0.1080, 0.1140, 0.1897, 0.0901, 0.1176],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1508, 0.0038, 0.1855, 0.1943, 0.1609, 0.1494, 0.1552],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1240, 0.1937, 0.1286, 0.1546, 0.1410, 0.1234, 0.1348],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  19 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  0
maxi score, test score, baseline:  0.03571999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
maxi score, test score, baseline:  0.03571999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
printing an ep nov before normalisation:  40.91406868135763
printing an ep nov before normalisation:  49.47441742373053
printing an ep nov before normalisation:  56.178783484342404
maxi score, test score, baseline:  0.03571999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
maxi score, test score, baseline:  0.03571999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
Printing some Q and Qe and total Qs values:  [[1.019]
 [1.019]
 [1.019]
 [1.019]
 [0.999]
 [1.005]
 [1.019]] [[65.561]
 [65.561]
 [65.561]
 [65.561]
 [64.968]
 [63.181]
 [65.561]] [[1.019]
 [1.019]
 [1.019]
 [1.019]
 [0.999]
 [1.005]
 [1.019]]
printing an ep nov before normalisation:  48.0703490140485
Printing some Q and Qe and total Qs values:  [[0.89 ]
 [0.915]
 [0.896]
 [0.877]
 [0.901]
 [0.89 ]
 [0.877]] [[42.82 ]
 [43.053]
 [42.633]
 [37.41 ]
 [43.997]
 [44.219]
 [37.41 ]] [[0.89 ]
 [0.915]
 [0.896]
 [0.877]
 [0.901]
 [0.89 ]
 [0.877]]
maxi score, test score, baseline:  0.03235999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
printing an ep nov before normalisation:  38.52388558460804
actor:  1 policy actor:  1  step number:  39 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  26.71172515038942
maxi score, test score, baseline:  0.028999999999999873 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  0 policy actor:  1  step number:  33 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.028479999999999873 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
printing an ep nov before normalisation:  71.70175006504002
maxi score, test score, baseline:  0.028479999999999873 0.6799999999999999 0.6799999999999999
Printing some Q and Qe and total Qs values:  [[0.619]
 [0.643]
 [0.619]
 [0.619]
 [0.619]
 [0.619]
 [0.619]] [[30.107]
 [34.202]
 [30.107]
 [30.107]
 [30.107]
 [30.107]
 [30.107]] [[0.619]
 [0.643]
 [0.619]
 [0.619]
 [0.619]
 [0.619]
 [0.619]]
line 256 mcts: sample exp_bonus 47.9278520251298
printing an ep nov before normalisation:  40.82250189343143
maxi score, test score, baseline:  0.028479999999999873 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  0 policy actor:  0  step number:  26 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  0
maxi score, test score, baseline:  0.024819999999999873 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.024819999999999873 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  1 policy actor:  1  step number:  32 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  37.727557134051686
actor:  0 policy actor:  1  step number:  25 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.02479999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
maxi score, test score, baseline:  0.02479999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
printing an ep nov before normalisation:  39.221289481535614
printing an ep nov before normalisation:  38.30946207046509
printing an ep nov before normalisation:  41.8787565206339
printing an ep nov before normalisation:  32.42813194656898
actor:  0 policy actor:  0  step number:  28 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.024239999999999876 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
using another actor
printing an ep nov before normalisation:  33.1956204724204
actor:  1 policy actor:  1  step number:  29 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  0.333
using another actor
maxi score, test score, baseline:  0.024239999999999876 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
Printing some Q and Qe and total Qs values:  [[-0.   ]
 [ 0.113]
 [-0.   ]
 [-0.   ]
 [-0.   ]
 [-0.   ]
 [-0.   ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.   ]
 [ 0.113]
 [-0.   ]
 [-0.   ]
 [-0.   ]
 [-0.   ]
 [-0.   ]]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.024239999999999876 0.6799999999999999 0.6799999999999999
printing an ep nov before normalisation:  34.39115095318398
actor:  0 policy actor:  1  step number:  24 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.02077999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actions average: 
K:  1  action  0 :  tensor([0.4771, 0.0043, 0.0683, 0.1128, 0.1525, 0.0738, 0.1113],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0049,     0.9417,     0.0069,     0.0123,     0.0008,     0.0178,
            0.0156], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0624, 0.0053, 0.5965, 0.0921, 0.0736, 0.0985, 0.0717],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1947, 0.0089, 0.0858, 0.3576, 0.1478, 0.0987, 0.1065],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1070, 0.0090, 0.1058, 0.1081, 0.4727, 0.1073, 0.0901],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1577, 0.0039, 0.1722, 0.1712, 0.1460, 0.1789, 0.1700],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1075, 0.1309, 0.0942, 0.1450, 0.1093, 0.0851, 0.3281],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  20.26318251939827
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  62.313240627235245
printing an ep nov before normalisation:  32.28912088606093
printing an ep nov before normalisation:  64.20510890302988
maxi score, test score, baseline:  0.02077999999999987 0.6799999999999999 0.6799999999999999
actor:  1 policy actor:  1  step number:  24 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.02077999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
Printing some Q and Qe and total Qs values:  [[0.116]
 [0.106]
 [0.147]
 [0.116]
 [0.105]
 [0.116]
 [0.118]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.116]
 [0.106]
 [0.147]
 [0.116]
 [0.105]
 [0.116]
 [0.118]]
printing an ep nov before normalisation:  68.56093689423889
actor:  1 policy actor:  1  step number:  30 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  2.0
from probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
maxi score, test score, baseline:  0.02077999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
Printing some Q and Qe and total Qs values:  [[0.096]
 [0.14 ]
 [0.096]
 [0.079]
 [0.096]
 [0.096]
 [0.096]] [[40.195]
 [47.591]
 [40.195]
 [42.902]
 [40.195]
 [40.195]
 [40.195]] [[1.012]
 [1.363]
 [1.012]
 [1.107]
 [1.012]
 [1.012]
 [1.012]]
printing an ep nov before normalisation:  44.00385985513406
printing an ep nov before normalisation:  54.0044974734924
maxi score, test score, baseline:  0.02077999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
printing an ep nov before normalisation:  39.10929451426695
maxi score, test score, baseline:  0.02077999999999987 0.6799999999999999 0.6799999999999999
actor:  0 policy actor:  1  step number:  23 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.020539999999999878 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]] [[45.319]
 [45.319]
 [45.319]
 [45.319]
 [45.319]
 [45.319]
 [45.319]] [[1.1]
 [1.1]
 [1.1]
 [1.1]
 [1.1]
 [1.1]
 [1.1]]
maxi score, test score, baseline:  0.020539999999999878 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actions average: 
K:  2  action  0 :  tensor([0.7567, 0.0046, 0.0365, 0.0479, 0.0525, 0.0555, 0.0463],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0012,     0.9681,     0.0032,     0.0090,     0.0003,     0.0013,
            0.0168], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0922, 0.0201, 0.5236, 0.0862, 0.0986, 0.0876, 0.0917],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1084, 0.1517, 0.1000, 0.2316, 0.1210, 0.1111, 0.1761],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1136, 0.0705, 0.0731, 0.0792, 0.4908, 0.0778, 0.0950],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1119, 0.0984, 0.1913, 0.1063, 0.0972, 0.2720, 0.1229],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1147, 0.2264, 0.1056, 0.0984, 0.0982, 0.1071, 0.2496],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  29 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.020539999999999878 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.020539999999999878 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
Printing some Q and Qe and total Qs values:  [[0.058]
 [0.047]
 [0.058]
 [0.058]
 [0.058]
 [0.058]
 [0.058]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.058]
 [0.047]
 [0.058]
 [0.058]
 [0.058]
 [0.058]
 [0.058]]
actor:  1 policy actor:  1  step number:  37 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.333
from probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  1 policy actor:  1  step number:  28 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]] [[49.582]
 [49.582]
 [49.582]
 [49.582]
 [49.582]
 [49.582]
 [49.582]] [[0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]]
maxi score, test score, baseline:  0.020539999999999878 0.6799999999999999 0.6799999999999999
actor:  0 policy actor:  0  step number:  18 total reward:  0.73  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  30 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.795]
 [0.795]
 [0.795]
 [0.795]
 [0.795]
 [0.789]
 [0.795]] [[51.677]
 [51.677]
 [51.677]
 [51.677]
 [51.677]
 [54.167]
 [51.677]] [[2.576]
 [2.576]
 [2.576]
 [2.576]
 [2.576]
 [2.71 ]
 [2.576]]
maxi score, test score, baseline:  0.020679999999999872 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
printing an ep nov before normalisation:  54.40790087483834
printing an ep nov before normalisation:  55.85266513505732
maxi score, test score, baseline:  0.020679999999999872 0.6799999999999999 0.6799999999999999
printing an ep nov before normalisation:  37.902173301684705
printing an ep nov before normalisation:  48.55926990509033
printing an ep nov before normalisation:  49.2702109198131
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  33 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  33 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.020679999999999872 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.351]
 [0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]] [[42.012]
 [38.018]
 [42.012]
 [42.012]
 [42.012]
 [42.012]
 [42.012]] [[1.545]
 [1.351]
 [1.545]
 [1.545]
 [1.545]
 [1.545]
 [1.545]]
printing an ep nov before normalisation:  53.41359270263946
maxi score, test score, baseline:  0.020679999999999872 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  54.45464517562132
Printing some Q and Qe and total Qs values:  [[0.711]
 [0.735]
 [0.669]
 [0.67 ]
 [0.672]
 [0.712]
 [0.672]] [[19.627]
 [25.226]
 [14.104]
 [14.149]
 [14.442]
 [19.46 ]
 [14.267]] [[1.009]
 [1.119]
 [0.883]
 [0.885]
 [0.891]
 [1.007]
 [0.888]]
actor:  1 policy actor:  1  step number:  39 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  35.04930019378662
maxi score, test score, baseline:  0.020679999999999872 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
printing an ep nov before normalisation:  48.818711177191354
printing an ep nov before normalisation:  37.828971633238034
maxi score, test score, baseline:  0.020679999999999872 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
maxi score, test score, baseline:  0.020679999999999872 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  1 policy actor:  1  step number:  28 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.020679999999999872 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
Printing some Q and Qe and total Qs values:  [[0.318]
 [0.353]
 [0.318]
 [0.318]
 [0.318]
 [0.318]
 [0.318]] [[57.407]
 [67.648]
 [57.407]
 [57.407]
 [57.407]
 [57.407]
 [57.407]] [[1.589]
 [1.939]
 [1.589]
 [1.589]
 [1.589]
 [1.589]
 [1.589]]
Printing some Q and Qe and total Qs values:  [[0.712]
 [0.827]
 [0.712]
 [0.729]
 [0.717]
 [0.712]
 [0.715]] [[32.546]
 [36.977]
 [32.546]
 [41.323]
 [40.719]
 [32.546]
 [40.582]] [[1.496]
 [1.919]
 [1.496]
 [2.124]
 [2.07 ]
 [1.496]
 [2.059]]
printing an ep nov before normalisation:  28.650460846558687
printing an ep nov before normalisation:  0.010719588526626467
printing an ep nov before normalisation:  60.27471599199684
actor:  1 policy actor:  1  step number:  40 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.020679999999999872 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  1 policy actor:  1  step number:  29 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  56.76671707763013
maxi score, test score, baseline:  0.020679999999999872 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.651]
 [0.638]
 [0.639]
 [0.651]
 [0.617]
 [0.641]] [[32.381]
 [49.794]
 [31.367]
 [31.577]
 [33.454]
 [33.255]
 [32.814]] [[1.316]
 [2.122]
 [1.274]
 [1.284]
 [1.381]
 [1.338]
 [1.342]]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
from probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  1 policy actor:  1  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  12.499253098628303
maxi score, test score, baseline:  0.020679999999999872 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
printing an ep nov before normalisation:  52.85236250084026
printing an ep nov before normalisation:  43.53473501484229
maxi score, test score, baseline:  0.020679999999999872 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  1 policy actor:  1  step number:  34 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  37.42784726767144
maxi score, test score, baseline:  0.017439999999999876 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
printing an ep nov before normalisation:  50.471204765721204
actions average: 
K:  4  action  0 :  tensor([0.5956, 0.0246, 0.0691, 0.0662, 0.1084, 0.0570, 0.0790],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0016,     0.9839,     0.0021,     0.0032,     0.0007,     0.0007,
            0.0078], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0410, 0.0012, 0.7242, 0.0566, 0.0606, 0.0751, 0.0413],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0368, 0.2201, 0.0394, 0.5271, 0.0370, 0.0347, 0.1048],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1747, 0.0092, 0.0873, 0.0614, 0.4922, 0.0499, 0.1254],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0956, 0.0465, 0.2588, 0.1439, 0.1383, 0.1976, 0.1194],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0853, 0.2166, 0.0953, 0.1133, 0.1004, 0.0884, 0.3007],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.017439999999999876 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
printing an ep nov before normalisation:  20.362460613250732
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  37 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  37 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.017439999999999876 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
printing an ep nov before normalisation:  36.22671127319336
maxi score, test score, baseline:  0.017439999999999876 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
maxi score, test score, baseline:  0.017439999999999876 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
printing an ep nov before normalisation:  36.55177856853392
maxi score, test score, baseline:  0.017439999999999876 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  1 policy actor:  1  step number:  20 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  40 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.2  ]
 [0.258]
 [0.2  ]
 [0.2  ]
 [0.2  ]
 [0.2  ]
 [0.2  ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.2  ]
 [0.258]
 [0.2  ]
 [0.2  ]
 [0.2  ]
 [0.2  ]
 [0.2  ]]
siam score:  -0.86215746
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  28.33199977874756
siam score:  -0.8633189
printing an ep nov before normalisation:  27.17618055200089
maxi score, test score, baseline:  0.017439999999999876 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  27.963886260986328
actor:  1 policy actor:  1  step number:  29 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.017439999999999876 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
printing an ep nov before normalisation:  14.76806173287343
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.621]
 [0.621]
 [0.636]
 [0.636]
 [0.636]
 [0.637]] [[22.518]
 [22.843]
 [22.843]
 [13.392]
 [17.47 ]
 [15.996]
 [19.928]] [[2.104]
 [2.11 ]
 [2.11 ]
 [1.509]
 [1.775]
 [1.679]
 [1.936]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.017439999999999876 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  1 policy actor:  1  step number:  31 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.017439999999999876 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.65755033493042
actor:  0 policy actor:  0  step number:  35 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.013639999999999874 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.465]
 [0.465]
 [0.45 ]
 [0.451]
 [0.449]
 [0.462]] [[50.87 ]
 [46.092]
 [46.092]
 [53.541]
 [53.453]
 [54.034]
 [56.284]] [[2.018]
 [1.785]
 [1.785]
 [2.119]
 [2.117]
 [2.141]
 [2.26 ]]
maxi score, test score, baseline:  0.013639999999999874 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.013639999999999874 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
UNIT TEST: sample policy line 217 mcts : [0.    0.939 0.    0.    0.    0.061 0.   ]
actor:  1 policy actor:  1  step number:  20 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  31 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  47.19885227019654
Printing some Q and Qe and total Qs values:  [[0.138]
 [0.195]
 [0.258]
 [0.255]
 [0.157]
 [0.178]
 [0.208]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.138]
 [0.195]
 [0.258]
 [0.255]
 [0.157]
 [0.178]
 [0.208]]
siam score:  -0.86713225
siam score:  -0.867348
Printing some Q and Qe and total Qs values:  [[0.23 ]
 [0.184]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]] [[45.342]
 [52.982]
 [45.342]
 [45.342]
 [45.342]
 [45.342]
 [45.342]] [[1.076]
 [1.257]
 [1.076]
 [1.076]
 [1.076]
 [1.076]
 [1.076]]
Printing some Q and Qe and total Qs values:  [[ 0.499]
 [ 0.464]
 [-0.029]
 [ 0.301]
 [ 0.322]
 [ 0.167]
 [ 0.266]] [[51.333]
 [55.886]
 [62.168]
 [58.881]
 [53.078]
 [52.341]
 [59.977]] [[1.595]
 [1.718]
 [1.443]
 [1.66 ]
 [1.478]
 [1.298]
 [1.662]]
maxi score, test score, baseline:  0.013639999999999874 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
maxi score, test score, baseline:  0.013639999999999874 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
maxi score, test score, baseline:  0.013639999999999874 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.855]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]] [[35.349]
 [39.572]
 [35.349]
 [35.349]
 [35.349]
 [35.349]
 [35.349]] [[0.668]
 [0.855]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]]
maxi score, test score, baseline:  0.013639999999999874 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  1 policy actor:  1  step number:  33 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  65.55803838893716
maxi score, test score, baseline:  0.013639999999999878 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  1 policy actor:  1  step number:  37 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  24 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.013639999999999878 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
printing an ep nov before normalisation:  41.85237553177161
maxi score, test score, baseline:  0.013639999999999878 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
printing an ep nov before normalisation:  0.11632870318862842
actor:  1 policy actor:  1  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.013639999999999878 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  0 policy actor:  1  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.014479999999999877 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
Printing some Q and Qe and total Qs values:  [[0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]] [[24.132]
 [24.132]
 [24.132]
 [24.132]
 [24.132]
 [24.132]
 [24.132]] [[2.263]
 [2.263]
 [2.263]
 [2.263]
 [2.263]
 [2.263]
 [2.263]]
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  39 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.014479999999999877 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
printing an ep nov before normalisation:  31.50621578773184
printing an ep nov before normalisation:  27.91329792567662
maxi score, test score, baseline:  0.011839999999999877 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.011839999999999877 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  1 policy actor:  1  step number:  33 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.011839999999999877 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  31 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  34 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.011839999999999877 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
from probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
maxi score, test score, baseline:  0.011839999999999877 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
maxi score, test score, baseline:  0.011839999999999877 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
printing an ep nov before normalisation:  56.42153502026605
maxi score, test score, baseline:  0.011839999999999877 0.6799999999999999 0.6799999999999999
actor:  0 policy actor:  0  step number:  30 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  35 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.011819999999999874 0.6799999999999999 0.6799999999999999
actor:  1 policy actor:  1  step number:  37 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.0
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  17.628846168518066
printing an ep nov before normalisation:  60.628285031501385
maxi score, test score, baseline:  0.011819999999999874 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.011819999999999874 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
from probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
maxi score, test score, baseline:  0.011819999999999874 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  0 policy actor:  0  step number:  24 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.009279999999999868 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
printing an ep nov before normalisation:  91.000819525122
maxi score, test score, baseline:  0.009279999999999868 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actions average: 
K:  4  action  0 :  tensor([    0.6032,     0.0005,     0.0730,     0.0743,     0.1073,     0.0755,
            0.0662], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0066, 0.9639, 0.0071, 0.0069, 0.0026, 0.0034, 0.0096],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0688, 0.0069, 0.6605, 0.0614, 0.0570, 0.0854, 0.0601],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1423, 0.0093, 0.1607, 0.2177, 0.1436, 0.2064, 0.1201],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1583, 0.0588, 0.1104, 0.1053, 0.4014, 0.0716, 0.0942],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1760, 0.0030, 0.1578, 0.1635, 0.1645, 0.1485, 0.1868],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0711, 0.1632, 0.0741, 0.1474, 0.0640, 0.0658, 0.4143],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.691]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]] [[39.833]
 [51.378]
 [39.833]
 [39.833]
 [39.833]
 [39.833]
 [39.833]] [[0.771]
 [0.907]
 [0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]]
actor:  1 policy actor:  1  step number:  23 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  0.47715175534023047
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.009279999999999868 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
printing an ep nov before normalisation:  69.43373096857577
maxi score, test score, baseline:  0.009279999999999868 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.676]
 [0.858]
 [0.676]
 [0.676]
 [0.676]
 [0.676]] [[42.878]
 [42.878]
 [44.734]
 [42.878]
 [42.878]
 [42.878]
 [42.878]] [[0.676]
 [0.676]
 [0.858]
 [0.676]
 [0.676]
 [0.676]
 [0.676]]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  69.20549870712024
maxi score, test score, baseline:  0.009279999999999868 0.6799999999999999 0.6799999999999999
Printing some Q and Qe and total Qs values:  [[1.023]
 [1.023]
 [1.01 ]
 [1.01 ]
 [1.01 ]
 [1.023]
 [1.023]] [[38.694]
 [38.099]
 [41.084]
 [41.084]
 [41.084]
 [41.104]
 [37.961]] [[1.023]
 [1.023]
 [1.01 ]
 [1.01 ]
 [1.01 ]
 [1.023]
 [1.023]]
actor:  0 policy actor:  0  step number:  25 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  37 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.771]
 [0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]] [[18.342]
 [16.589]
 [17.822]
 [17.822]
 [17.892]
 [17.822]
 [17.822]] [[2.128]
 [2.067]
 [2.087]
 [2.087]
 [2.094]
 [2.087]
 [2.087]]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.014999999999999868 0.6799999999999999 0.6799999999999999
printing an ep nov before normalisation:  47.24840310818684
actions average: 
K:  3  action  0 :  tensor([0.7496, 0.0033, 0.0279, 0.0381, 0.0821, 0.0549, 0.0441],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0012,     0.9711,     0.0042,     0.0069,     0.0001,     0.0003,
            0.0162], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0878, 0.0171, 0.7299, 0.0305, 0.0447, 0.0497, 0.0403],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0671, 0.1771, 0.0754, 0.3642, 0.0956, 0.0790, 0.1417],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1331, 0.0021, 0.1039, 0.0947, 0.4412, 0.1068, 0.1182],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1521, 0.0013, 0.1127, 0.1200, 0.1207, 0.3276, 0.1656],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1061, 0.0658, 0.1044, 0.1130, 0.0865, 0.1109, 0.4134],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  25 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  21 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]
 [0.272]] [[41.216]
 [41.216]
 [41.216]
 [41.216]
 [41.216]
 [41.216]
 [41.216]] [[1.939]
 [1.939]
 [1.939]
 [1.939]
 [1.939]
 [1.939]
 [1.939]]
maxi score, test score, baseline:  0.01507999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  1 policy actor:  1  step number:  30 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  30 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  50.742329996025695
maxi score, test score, baseline:  0.01507999999999987 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.01507999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
rdn beta is 0 so we're just using the maxi policy
actions average: 
K:  4  action  0 :  tensor([0.5127, 0.0205, 0.0643, 0.0563, 0.2209, 0.0648, 0.0605],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0061, 0.9233, 0.0214, 0.0186, 0.0042, 0.0056, 0.0209],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1039, 0.0286, 0.5016, 0.0867, 0.1047, 0.0973, 0.0772],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1110, 0.0497, 0.1053, 0.3786, 0.1014, 0.1119, 0.1420],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2507, 0.0216, 0.1460, 0.1243, 0.2254, 0.1108, 0.1212],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1391, 0.1019, 0.1573, 0.1142, 0.1104, 0.2653, 0.1117],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1409, 0.3102, 0.1003, 0.1034, 0.0998, 0.1165, 0.1289],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.01507999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
printing an ep nov before normalisation:  44.66016525197479
actor:  1 policy actor:  1  step number:  30 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  0 policy actor:  0  step number:  19 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.015539999999999868 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.015539999999999868 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  1 policy actor:  1  step number:  33 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.015539999999999868 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  1 policy actor:  1  step number:  26 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  22 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.244]
 [0.256]
 [0.27 ]
 [0.27 ]
 [0.264]
 [0.25 ]
 [0.256]] [[62.482]
 [54.61 ]
 [66.06 ]
 [66.06 ]
 [63.806]
 [63.617]
 [62.748]] [[1.026]
 [0.894]
 [1.117]
 [1.117]
 [1.069]
 [1.053]
 [1.042]]
Printing some Q and Qe and total Qs values:  [[-0.014]
 [-0.02 ]
 [-0.018]
 [-0.029]
 [-0.028]
 [-0.024]
 [-0.016]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.014]
 [-0.02 ]
 [-0.018]
 [-0.029]
 [-0.028]
 [-0.024]
 [-0.016]]
Printing some Q and Qe and total Qs values:  [[0.331]
 [0.331]
 [0.293]
 [0.331]
 [0.331]
 [0.331]
 [0.378]] [[55.637]
 [55.637]
 [62.25 ]
 [55.637]
 [55.637]
 [55.637]
 [62.682]] [[1.522]
 [1.522]
 [1.691]
 [1.522]
 [1.522]
 [1.522]
 [1.789]]
Printing some Q and Qe and total Qs values:  [[0.672]
 [0.594]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]] [[64.388]
 [71.397]
 [76.052]
 [76.052]
 [76.052]
 [76.052]
 [76.052]] [[0.926]
 [0.887]
 [0.938]
 [0.938]
 [0.938]
 [0.938]
 [0.938]]
maxi score, test score, baseline:  0.015539999999999868 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.015539999999999868 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  22 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  25.20648693009595
maxi score, test score, baseline:  0.015539999999999868 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
actor:  1 policy actor:  1  step number:  31 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.015539999999999868 0.6799999999999999 0.6799999999999999
probs:  [0.04630535531626825, 0.04630535531626825, 0.29670526938192143, 0.04630535531626825, 0.24952847397824762, 0.31485019069102627]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  21 total reward:  0.7  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.015539999999999868 0.6799999999999999 0.6799999999999999
actor:  1 policy actor:  1  step number:  21 total reward:  0.7  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8656659
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]]
maxi score, test score, baseline:  0.015539999999999868 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.015539999999999868 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
Printing some Q and Qe and total Qs values:  [[0.065]
 [0.195]
 [0.065]
 [0.065]
 [0.065]
 [0.065]
 [0.065]] [[29.932]
 [36.605]
 [29.932]
 [29.932]
 [29.932]
 [29.932]
 [29.932]] [[0.843]
 [1.222]
 [0.843]
 [0.843]
 [0.843]
 [0.843]
 [0.843]]
siam score:  -0.8643215
printing an ep nov before normalisation:  42.79330483599164
line 256 mcts: sample exp_bonus 33.82196358379578
maxi score, test score, baseline:  0.015539999999999868 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
actor:  0 policy actor:  0  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  34 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.018679999999999863 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
maxi score, test score, baseline:  0.018679999999999863 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
printing an ep nov before normalisation:  33.92513620964171
actor:  1 policy actor:  1  step number:  24 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  24 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.018679999999999863 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
maxi score, test score, baseline:  0.018679999999999863 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
actor:  0 policy actor:  0  step number:  25 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  25 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.01875999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  35 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.01875999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
maxi score, test score, baseline:  0.01875999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
printing an ep nov before normalisation:  44.83067617407903
printing an ep nov before normalisation:  59.2938102718261
maxi score, test score, baseline:  0.015779999999999864 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.02  0.49  0.224 0.184 0.02  0.02  0.041]
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.273]
 [0.355]
 [0.315]
 [0.315]
 [0.315]
 [0.315]] [[57.447]
 [55.627]
 [56.167]
 [57.447]
 [57.447]
 [57.447]
 [57.447]] [[1.513]
 [1.401]
 [1.504]
 [1.513]
 [1.513]
 [1.513]
 [1.513]]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.763]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]] [[51.402]
 [50.617]
 [49.248]
 [49.248]
 [49.248]
 [49.248]
 [49.248]] [[2.167]
 [2.295]
 [2.097]
 [2.097]
 [2.097]
 [2.097]
 [2.097]]
maxi score, test score, baseline:  0.015779999999999864 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.015779999999999864 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
printing an ep nov before normalisation:  36.628280602417
actor:  0 policy actor:  1  step number:  20 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2.0
from probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  22.848920822143555
maxi score, test score, baseline:  0.016019999999999868 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
actor:  1 policy actor:  1  step number:  33 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  2.0
siam score:  -0.863796
maxi score, test score, baseline:  0.016019999999999868 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
actor:  0 policy actor:  1  step number:  17 total reward:  0.72  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.016239999999999866 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
maxi score, test score, baseline:  0.016239999999999866 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
actions average: 
K:  2  action  0 :  tensor([0.6620, 0.0197, 0.0559, 0.0573, 0.1046, 0.0581, 0.0424],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0017, 0.9547, 0.0043, 0.0132, 0.0012, 0.0031, 0.0218],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0608, 0.0109, 0.6244, 0.0648, 0.0595, 0.1231, 0.0565],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1220, 0.1020, 0.0883, 0.3629, 0.1075, 0.0975, 0.1197],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1586, 0.0182, 0.1016, 0.1392, 0.3944, 0.1017, 0.0863],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1348, 0.0661, 0.1466, 0.1153, 0.1232, 0.2847, 0.1294],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1081, 0.1942, 0.0832, 0.2176, 0.1149, 0.0781, 0.2039],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.016239999999999866 0.6799999999999999 0.6799999999999999
actor:  1 policy actor:  1  step number:  40 total reward:  0.3299999999999996  reward:  1.0 rdn_beta:  1.333
siam score:  -0.86802596
actor:  1 policy actor:  1  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  22 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.016239999999999866 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
maxi score, test score, baseline:  0.016239999999999866 0.6799999999999999 0.6799999999999999
Printing some Q and Qe and total Qs values:  [[0.406]
 [0.535]
 [0.487]
 [0.294]
 [0.406]
 [0.406]
 [0.406]] [[37.215]
 [41.385]
 [40.189]
 [33.093]
 [37.215]
 [37.215]
 [37.215]] [[0.873]
 [1.096]
 [1.02 ]
 [0.667]
 [0.873]
 [0.873]
 [0.873]]
printing an ep nov before normalisation:  49.57246553694284
maxi score, test score, baseline:  0.016239999999999866 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
maxi score, test score, baseline:  0.016239999999999866 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
printing an ep nov before normalisation:  38.23782921113133
maxi score, test score, baseline:  0.016239999999999866 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
actor:  1 policy actor:  1  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  30.097065172338898
maxi score, test score, baseline:  0.016239999999999866 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
actor:  1 policy actor:  1  step number:  36 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  66.19601452486107
actor:  1 policy actor:  1  step number:  38 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.658]
 [0.647]
 [0.541]
 [0.516]
 [0.647]
 [0.551]] [[51.022]
 [47.083]
 [51.126]
 [56.214]
 [53.719]
 [51.126]
 [49.532]] [[1.397]
 [1.378]
 [1.454]
 [1.459]
 [1.38 ]
 [1.454]
 [1.324]]
Printing some Q and Qe and total Qs values:  [[0.443]
 [0.531]
 [0.293]
 [0.443]
 [0.443]
 [0.268]
 [0.296]] [[35.609]
 [39.587]
 [30.365]
 [35.609]
 [35.609]
 [30.693]
 [31.661]] [[0.944]
 [1.131]
 [0.664]
 [0.944]
 [0.944]
 [0.648]
 [0.7  ]]
maxi score, test score, baseline:  0.016239999999999866 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.301]
 [0.243]
 [0.242]
 [0.24 ]
 [0.239]
 [0.242]] [[27.683]
 [42.113]
 [26.647]
 [26.985]
 [28.172]
 [27.788]
 [27.15 ]] [[0.712]
 [1.322]
 [0.673]
 [0.686]
 [0.728]
 [0.714]
 [0.692]]
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.672]
 [0.589]
 [0.563]
 [0.559]
 [0.563]
 [0.564]] [[36.741]
 [39.045]
 [37.873]
 [34.972]
 [36.741]
 [34.797]
 [34.669]] [[1.271]
 [1.429]
 [1.323]
 [1.24 ]
 [1.271]
 [1.237]
 [1.235]]
printing an ep nov before normalisation:  44.56696019104532
printing an ep nov before normalisation:  26.772802809200478
actor:  1 policy actor:  1  step number:  34 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  53.71812243754522
maxi score, test score, baseline:  0.016239999999999866 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
actions average: 
K:  1  action  0 :  tensor([0.6921, 0.0039, 0.0626, 0.0563, 0.0637, 0.0516, 0.0698],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0009,     0.9837,     0.0009,     0.0076,     0.0004,     0.0003,
            0.0062], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0431, 0.0153, 0.7334, 0.0522, 0.0396, 0.0600, 0.0564],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1193, 0.0614, 0.1350, 0.3179, 0.1254, 0.1102, 0.1307],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1346, 0.0045, 0.1264, 0.1389, 0.3389, 0.1116, 0.1452],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1270, 0.0030, 0.1423, 0.1236, 0.0979, 0.2941, 0.2120],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0896, 0.1564, 0.0747, 0.0891, 0.0892, 0.0701, 0.4309],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  31 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.016239999999999866 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
Printing some Q and Qe and total Qs values:  [[0.731]
 [0.705]
 [0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]] [[49.467]
 [56.554]
 [49.467]
 [49.467]
 [49.467]
 [49.467]
 [49.467]] [[2.167]
 [2.499]
 [2.167]
 [2.167]
 [2.167]
 [2.167]
 [2.167]]
printing an ep nov before normalisation:  51.74292652129317
using explorer policy with actor:  1
actions average: 
K:  0  action  0 :  tensor([0.4587, 0.0217, 0.0888, 0.0914, 0.1300, 0.0826, 0.1267],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0025,     0.9684,     0.0021,     0.0077,     0.0010,     0.0004,
            0.0179], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0359, 0.0014, 0.7556, 0.0420, 0.0430, 0.0755, 0.0464],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1279, 0.0038, 0.1695, 0.2574, 0.1554, 0.1269, 0.1590],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1681, 0.0041, 0.1409, 0.1865, 0.1832, 0.1334, 0.1838],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1108, 0.0014, 0.1815, 0.1637, 0.1219, 0.2649, 0.1559],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1129, 0.1203, 0.1086, 0.2227, 0.0910, 0.0787, 0.2658],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  22 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  0.667
from probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
actor:  1 policy actor:  1  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.016239999999999866 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
printing an ep nov before normalisation:  52.765633385057654
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  25 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  26 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  53.12822358083862
actor:  0 policy actor:  0  step number:  28 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.45 ]
 [0.594]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]] [[42.743]
 [49.411]
 [42.743]
 [42.743]
 [42.743]
 [42.743]
 [42.743]] [[1.165]
 [1.488]
 [1.165]
 [1.165]
 [1.165]
 [1.165]
 [1.165]]
maxi score, test score, baseline:  0.016119999999999867 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
maxi score, test score, baseline:  0.016119999999999867 0.6799999999999999 0.6799999999999999
printing an ep nov before normalisation:  59.206647886424115
actor:  1 policy actor:  1  step number:  29 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.016119999999999867 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.57449994906133
actor:  1 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.577]
 [0.557]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]] [[46.227]
 [56.569]
 [46.227]
 [46.227]
 [46.227]
 [46.227]
 [46.227]] [[1.169]
 [1.392]
 [1.169]
 [1.169]
 [1.169]
 [1.169]
 [1.169]]
using another actor
line 256 mcts: sample exp_bonus 0.0
actor:  1 policy actor:  1  step number:  32 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.016119999999999867 0.6799999999999999 0.6799999999999999
line 256 mcts: sample exp_bonus 26.04373413889955
actor:  0 policy actor:  1  step number:  25 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.0
line 256 mcts: sample exp_bonus 46.681726284715566
maxi score, test score, baseline:  0.019159999999999868 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.019159999999999868 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.019159999999999868 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.019159999999999868 0.6799999999999999 0.6799999999999999
using explorer policy with actor:  1
maxi score, test score, baseline:  0.019159999999999868 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
printing an ep nov before normalisation:  60.112200341724865
maxi score, test score, baseline:  0.019159999999999868 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
actor:  1 policy actor:  1  step number:  30 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.175]
 [0.175]
 [0.175]
 [0.175]
 [0.175]
 [0.175]
 [0.175]] [[47.671]
 [47.671]
 [47.671]
 [47.671]
 [47.671]
 [47.671]
 [47.671]] [[1.101]
 [1.101]
 [1.101]
 [1.101]
 [1.101]
 [1.101]
 [1.101]]
printing an ep nov before normalisation:  72.39492734438198
printing an ep nov before normalisation:  61.6514850215347
maxi score, test score, baseline:  0.019159999999999868 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
actor:  1 policy actor:  1  step number:  33 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.20667654771234
actor:  1 policy actor:  1  step number:  29 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  55.89545972363326
using explorer policy with actor:  1
maxi score, test score, baseline:  0.019159999999999868 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
Printing some Q and Qe and total Qs values:  [[0.729]
 [0.783]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]] [[62.406]
 [51.131]
 [62.406]
 [62.406]
 [62.406]
 [62.406]
 [62.406]] [[0.729]
 [0.783]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]]
maxi score, test score, baseline:  0.019159999999999868 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
maxi score, test score, baseline:  0.019159999999999868 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
actor:  0 policy actor:  1  step number:  20 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  35 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  37 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.667
from probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
printing an ep nov before normalisation:  43.38801860809326
maxi score, test score, baseline:  0.019819999999999866 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
actor:  1 policy actor:  1  step number:  21 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  56.17009531302377
maxi score, test score, baseline:  0.019819999999999866 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
actor:  0 policy actor:  0  step number:  21 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  36.16328239440918
actor:  1 policy actor:  1  step number:  23 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.004]
 [-0.005]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.004]
 [-0.005]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]]
printing an ep nov before normalisation:  50.85784107956492
printing an ep nov before normalisation:  57.60999961771044
siam score:  -0.8668749
printing an ep nov before normalisation:  50.157457715968604
actor:  1 policy actor:  1  step number:  29 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  4  action  0 :  tensor([0.6515, 0.0014, 0.0600, 0.0508, 0.1112, 0.0701, 0.0550],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0051, 0.9630, 0.0125, 0.0047, 0.0017, 0.0046, 0.0083],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0714, 0.0070, 0.5290, 0.1074, 0.1019, 0.1045, 0.0788],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0957, 0.0105, 0.1592, 0.3877, 0.1414, 0.1363, 0.0692],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1940, 0.0254, 0.1605, 0.1227, 0.1506, 0.2386, 0.1083],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1081, 0.0005, 0.1609, 0.1102, 0.1311, 0.3841, 0.1051],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0616, 0.3164, 0.0612, 0.1180, 0.0751, 0.0799, 0.2878],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  56.0820561667933
Printing some Q and Qe and total Qs values:  [[0.756]
 [0.755]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]] [[27.802]
 [20.443]
 [29.531]
 [29.531]
 [29.531]
 [29.531]
 [29.531]] [[1.874]
 [1.577]
 [1.923]
 [1.923]
 [1.923]
 [1.923]
 [1.923]]
actor:  1 policy actor:  1  step number:  32 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  30 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.002]
 [ 0.013]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.002]
 [ 0.013]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]]
maxi score, test score, baseline:  0.020279999999999875 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
maxi score, test score, baseline:  0.020279999999999875 0.6799999999999999 0.6799999999999999
printing an ep nov before normalisation:  38.53519833680424
siam score:  -0.86402833
actor:  1 policy actor:  1  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.020279999999999875 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
printing an ep nov before normalisation:  32.11474401713579
actor:  1 policy actor:  1  step number:  30 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  22.346813678741455
actor:  1 policy actor:  1  step number:  37 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.020279999999999875 0.6799999999999999 0.6799999999999999
probs:  [0.04669404129005677, 0.04669404129005677, 0.2866392920432768, 0.04669404129005677, 0.2553420854232915, 0.31793649866326135]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]] [[47.771]
 [47.771]
 [47.771]
 [47.771]
 [47.771]
 [47.771]
 [47.771]] [[0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]]
printing an ep nov before normalisation:  25.17171303431193
using explorer policy with actor:  1
printing an ep nov before normalisation:  61.6371596660179
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.69 ]
 [0.589]
 [0.586]
 [0.591]
 [0.591]
 [0.591]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.586]
 [0.69 ]
 [0.589]
 [0.586]
 [0.591]
 [0.591]
 [0.591]]
printing an ep nov before normalisation:  59.92156071788201
printing an ep nov before normalisation:  35.03550164951203
printing an ep nov before normalisation:  37.48967993370579
printing an ep nov before normalisation:  61.83342427061169
printing an ep nov before normalisation:  60.06252834711119
Printing some Q and Qe and total Qs values:  [[0.918]
 [0.934]
 [0.918]
 [0.918]
 [0.918]
 [0.918]
 [0.918]] [[46.855]
 [53.419]
 [46.855]
 [46.855]
 [46.855]
 [46.855]
 [46.855]] [[0.918]
 [0.934]
 [0.918]
 [0.918]
 [0.918]
 [0.918]
 [0.918]]
siam score:  -0.8622531
printing an ep nov before normalisation:  47.386554149304175
printing an ep nov before normalisation:  42.552823328355416
printing an ep nov before normalisation:  0.01848504387396588
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  1 policy actor:  1  step number:  28 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  18 total reward:  0.71  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  58.834629757490454
maxi score, test score, baseline:  0.06249999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  33 total reward:  0.5199999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.06249999999999989 0.6815 0.6815
printing an ep nov before normalisation:  0.00017851792193823712
actor:  1 policy actor:  1  step number:  29 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.06249999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.06249999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
printing an ep nov before normalisation:  36.84041951877635
printing an ep nov before normalisation:  36.21846751536274
printing an ep nov before normalisation:  39.28461912730933
maxi score, test score, baseline:  0.06249999999999989 0.6815 0.6815
maxi score, test score, baseline:  0.06249999999999989 0.6815 0.6815
printing an ep nov before normalisation:  15.92987436125639
maxi score, test score, baseline:  0.06249999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.06249999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
Printing some Q and Qe and total Qs values:  [[-0.077]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]] [[28.358]
 [28.358]
 [28.358]
 [28.358]
 [28.358]
 [28.358]
 [28.358]] [[-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.06249999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  38 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
using another actor
maxi score, test score, baseline:  0.06249999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.06249999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
Printing some Q and Qe and total Qs values:  [[-0.011]
 [-0.041]
 [-0.009]
 [-0.006]
 [-0.006]
 [-0.009]
 [-0.011]] [[21.23 ]
 [32.144]
 [17.39 ]
 [17.639]
 [18.996]
 [19.897]
 [20.682]] [[0.58 ]
 [0.871]
 [0.469]
 [0.48 ]
 [0.519]
 [0.543]
 [0.564]]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.552]
 [0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.515]] [[40.96 ]
 [37.093]
 [40.96 ]
 [40.96 ]
 [40.96 ]
 [40.96 ]
 [40.96 ]] [[1.765]
 [1.632]
 [1.765]
 [1.765]
 [1.765]
 [1.765]
 [1.765]]
maxi score, test score, baseline:  0.06249999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
printing an ep nov before normalisation:  50.1723881549832
Printing some Q and Qe and total Qs values:  [[0.176]
 [0.22 ]
 [0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.176]] [[33.666]
 [39.763]
 [33.666]
 [33.666]
 [33.666]
 [33.666]
 [33.666]] [[0.646]
 [0.852]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]]
maxi score, test score, baseline:  0.06249999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.686]
 [0.737]
 [0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]] [[41.726]
 [46.523]
 [41.726]
 [41.726]
 [41.726]
 [41.726]
 [41.726]] [[1.518]
 [1.737]
 [1.518]
 [1.518]
 [1.518]
 [1.518]
 [1.518]]
actor:  0 policy actor:  0  step number:  30 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  30 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.018]
 [-0.022]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.018]
 [-0.022]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.171]
 [0.108]
 [0.065]
 [0.104]
 [0.013]
 [0.013]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.171]
 [0.108]
 [0.065]
 [0.104]
 [0.013]
 [0.013]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.05995999999999989 0.6815 0.6815
maxi score, test score, baseline:  0.05995999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
Printing some Q and Qe and total Qs values:  [[0.751]
 [0.767]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.747]] [[38.129]
 [36.969]
 [39.732]
 [39.732]
 [39.732]
 [39.732]
 [36.895]] [[1.371]
 [1.35 ]
 [1.313]
 [1.313]
 [1.313]
 [1.313]
 [1.327]]
actor:  1 policy actor:  1  step number:  20 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  0.667
from probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
printing an ep nov before normalisation:  19.99580413022242
actor:  1 policy actor:  1  step number:  36 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  32 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
from probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
siam score:  -0.8590152
printing an ep nov before normalisation:  36.366000175476074
line 256 mcts: sample exp_bonus 39.19773185611886
maxi score, test score, baseline:  0.05995999999999989 0.6815 0.6815
printing an ep nov before normalisation:  36.92835088465732
actor:  0 policy actor:  0  step number:  36 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  32 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  53.945119975357024
actor:  1 policy actor:  1  step number:  22 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0655999999999999 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.0655999999999999 0.6815 0.6815
actor:  1 policy actor:  1  step number:  24 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8591289
maxi score, test score, baseline:  0.0655999999999999 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  42.62656702234727
printing an ep nov before normalisation:  29.532673358917236
printing an ep nov before normalisation:  44.676186868792136
actor:  1 policy actor:  1  step number:  33 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.176]
 [0.219]
 [0.176]
 [0.176]
 [0.176]
 [0.224]
 [0.176]] [[38.099]
 [51.912]
 [38.099]
 [38.099]
 [38.099]
 [39.706]
 [38.099]] [[0.986]
 [1.661]
 [0.986]
 [0.986]
 [0.986]
 [1.108]
 [0.986]]
maxi score, test score, baseline:  0.0655999999999999 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.0655999999999999 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.0655999999999999 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
Printing some Q and Qe and total Qs values:  [[0.672]
 [0.52 ]
 [0.635]
 [0.445]
 [0.424]
 [0.635]
 [0.635]] [[35.627]
 [38.687]
 [38.664]
 [37.923]
 [39.456]
 [38.664]
 [38.664]] [[2.382]
 [2.462]
 [2.575]
 [2.329]
 [2.424]
 [2.575]
 [2.575]]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0655999999999999 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.589]
 [0.565]
 [0.304]
 [0.565]
 [0.565]
 [0.565]] [[56.287]
 [53.029]
 [53.66 ]
 [59.8  ]
 [53.66 ]
 [53.66 ]
 [53.66 ]] [[1.846]
 [1.751]
 [1.747]
 [1.692]
 [1.747]
 [1.747]
 [1.747]]
printing an ep nov before normalisation:  36.38493515424376
maxi score, test score, baseline:  0.0655999999999999 0.6815 0.6815
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.0655999999999999 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  34 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  41 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0655999999999999 0.6815 0.6815
actor:  1 policy actor:  1  step number:  37 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0655999999999999 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
line 256 mcts: sample exp_bonus 53.14150272250659
maxi score, test score, baseline:  0.0655999999999999 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
printing an ep nov before normalisation:  48.095583202969046
UNIT TEST: sample policy line 217 mcts : [0.061 0.755 0.02  0.041 0.02  0.02  0.082]
actor:  1 policy actor:  1  step number:  32 total reward:  0.2699999999999996  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]]
maxi score, test score, baseline:  0.0655999999999999 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  30 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0655999999999999 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0655999999999999 0.6815 0.6815
printing an ep nov before normalisation:  51.012409183396365
printing an ep nov before normalisation:  55.730929904266034
actor:  1 policy actor:  1  step number:  20 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  38 total reward:  0.4299999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0655999999999999 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  31 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  42.580552781016266
printing an ep nov before normalisation:  48.424764093397535
maxi score, test score, baseline:  0.0655999999999999 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.0655999999999999 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
printing an ep nov before normalisation:  44.44679260253906
actor:  1 policy actor:  1  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  29 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0655999999999999 0.6815 0.6815
maxi score, test score, baseline:  0.0655999999999999 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
printing an ep nov before normalisation:  61.62003967647866
maxi score, test score, baseline:  0.0655999999999999 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
printing an ep nov before normalisation:  1.2730443059740537
actor:  0 policy actor:  1  step number:  28 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  49.814978461119786
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.06849999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
Printing some Q and Qe and total Qs values:  [[0.784]
 [0.809]
 [0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]] [[30.235]
 [31.146]
 [28.119]
 [28.119]
 [28.119]
 [28.119]
 [28.119]] [[0.784]
 [0.809]
 [0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]]
maxi score, test score, baseline:  0.06849999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  0 policy actor:  0  step number:  28 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.257]
 [0.272]
 [0.257]
 [0.257]
 [0.262]
 [0.257]
 [0.257]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.257]
 [0.272]
 [0.257]
 [0.257]
 [0.262]
 [0.257]
 [0.257]]
Printing some Q and Qe and total Qs values:  [[0.232]
 [0.904]
 [0.232]
 [0.232]
 [0.237]
 [0.232]
 [0.232]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.232]
 [0.904]
 [0.232]
 [0.232]
 [0.237]
 [0.232]
 [0.232]]
actor:  0 policy actor:  1  step number:  43 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  1.667
from probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.07371999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
printing an ep nov before normalisation:  61.36226585108716
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  41.97539806365967
actor:  1 policy actor:  1  step number:  35 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.07371999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
printing an ep nov before normalisation:  43.88817867128579
maxi score, test score, baseline:  0.07371999999999988 0.6815 0.6815
maxi score, test score, baseline:  0.07371999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  0 policy actor:  1  step number:  29 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.593]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.529]
 [0.593]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]]
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.631]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]] [[52.557]
 [54.552]
 [52.557]
 [52.557]
 [52.557]
 [52.557]
 [52.557]] [[1.77 ]
 [1.911]
 [1.77 ]
 [1.77 ]
 [1.77 ]
 [1.77 ]
 [1.77 ]]
printing an ep nov before normalisation:  56.54796563231305
maxi score, test score, baseline:  0.0767199999999999 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  25 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  61.58971338572185
maxi score, test score, baseline:  0.0767199999999999 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  0 policy actor:  1  step number:  19 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  40.30804863810152
maxi score, test score, baseline:  0.07999999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
printing an ep nov before normalisation:  38.03959846496582
actor:  1 policy actor:  1  step number:  44 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8551601
actor:  1 policy actor:  1  step number:  22 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[ 0.729]
 [ 0.699]
 [-0.049]
 [ 0.682]
 [ 0.474]
 [-0.103]
 [ 0.71 ]] [[59.342]
 [62.444]
 [66.083]
 [64.522]
 [69.858]
 [66.524]
 [64.361]] [[2.29 ]
 [2.376]
 [1.766]
 [2.437]
 [2.43 ]
 [1.728]
 [2.459]]
actions average: 
K:  4  action  0 :  tensor([0.6693, 0.0443, 0.0446, 0.0544, 0.0996, 0.0348, 0.0530],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0056, 0.9394, 0.0102, 0.0120, 0.0031, 0.0071, 0.0226],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0634, 0.0778, 0.6254, 0.0681, 0.0502, 0.0466, 0.0686],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1599, 0.0740, 0.1289, 0.1365, 0.1251, 0.1886, 0.1869],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.3663, 0.0033, 0.0145, 0.1245, 0.3005, 0.0337, 0.1573],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1356, 0.1179, 0.1394, 0.1745, 0.1701, 0.1219, 0.1407],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0949, 0.0050, 0.1027, 0.1357, 0.0922, 0.0899, 0.4796],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.768]
 [0.759]
 [0.759]
 [0.759]
 [0.758]
 [0.759]
 [0.759]] [[56.76 ]
 [51.478]
 [51.478]
 [51.478]
 [59.654]
 [51.478]
 [51.478]] [[2.345]
 [2.104]
 [2.104]
 [2.104]
 [2.461]
 [2.104]
 [2.104]]
actor:  1 policy actor:  1  step number:  35 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  32 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  24 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.07999999999999989 0.6815 0.6815
maxi score, test score, baseline:  0.07999999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.07999999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
printing an ep nov before normalisation:  72.44196503655087
printing an ep nov before normalisation:  62.66334694874015
maxi score, test score, baseline:  0.07999999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.07999999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  32 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.0
from probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.07999999999999989 0.6815 0.6815
from probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.07999999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.07999999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
printing an ep nov before normalisation:  61.41662278934813
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.07999999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  0 policy actor:  1  step number:  26 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  56.20988333102359
actor:  0 policy actor:  1  step number:  39 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.08019999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
Printing some Q and Qe and total Qs values:  [[-0.003]
 [-0.003]
 [ 0.206]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[46.885]
 [46.885]
 [54.574]
 [46.885]
 [46.885]
 [46.885]
 [46.885]] [[1.108]
 [1.108]
 [1.641]
 [1.108]
 [1.108]
 [1.108]
 [1.108]]
actor:  1 policy actor:  1  step number:  22 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.08019999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  0 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.08323999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
printing an ep nov before normalisation:  63.087613750367055
maxi score, test score, baseline:  0.08323999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  27 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  56.2316407057951
maxi score, test score, baseline:  0.08323999999999987 0.6815 0.6815
printing an ep nov before normalisation:  37.952936752660094
actor:  1 policy actor:  1  step number:  36 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  43.42102617403595
Printing some Q and Qe and total Qs values:  [[0.273]
 [0.355]
 [0.273]
 [0.273]
 [0.273]
 [0.273]
 [0.273]] [[ 0.   ]
 [46.441]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.049]
 [0.669]
 [0.049]
 [0.049]
 [0.049]
 [0.049]
 [0.049]]
Printing some Q and Qe and total Qs values:  [[0.119]
 [0.123]
 [0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.119]
 [0.123]
 [0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]]
Printing some Q and Qe and total Qs values:  [[0.236]
 [0.295]
 [0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]] [[41.983]
 [45.782]
 [41.983]
 [41.983]
 [41.983]
 [41.983]
 [41.983]] [[0.487]
 [0.588]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.08323999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.08323999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
printing an ep nov before normalisation:  66.14791629054946
maxi score, test score, baseline:  0.08323999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.08323999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  32 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.08323999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.08323999999999988 0.6815 0.6815
printing an ep nov before normalisation:  51.454127274605455
maxi score, test score, baseline:  0.08323999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  21.693045664445243
actor:  1 policy actor:  1  step number:  35 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  32 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[ 0.081]
 [-0.005]
 [ 0.077]
 [ 0.072]
 [ 0.08 ]
 [ 0.122]
 [ 0.061]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.081]
 [-0.005]
 [ 0.077]
 [ 0.072]
 [ 0.08 ]
 [ 0.122]
 [ 0.061]]
maxi score, test score, baseline:  0.08323999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
printing an ep nov before normalisation:  49.2364594915247
printing an ep nov before normalisation:  54.98360094813813
maxi score, test score, baseline:  0.08323999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.503]] [[51.518]
 [51.518]
 [51.518]
 [51.518]
 [51.518]
 [51.518]
 [51.518]] [[1.305]
 [1.305]
 [1.305]
 [1.305]
 [1.305]
 [1.305]
 [1.305]]
siam score:  -0.86941546
printing an ep nov before normalisation:  24.27541902599045
maxi score, test score, baseline:  0.08323999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  26 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  35 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  39.829701763286245
maxi score, test score, baseline:  0.08323999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actions average: 
K:  0  action  0 :  tensor([0.5402, 0.0032, 0.0768, 0.0832, 0.1330, 0.0836, 0.0799],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0015,     0.9763,     0.0007,     0.0068,     0.0005,     0.0010,
            0.0133], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0445, 0.0016, 0.7421, 0.0547, 0.0446, 0.0679, 0.0447],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0614, 0.1014, 0.0533, 0.4807, 0.0592, 0.0595, 0.1844],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1894, 0.0034, 0.1474, 0.1575, 0.1826, 0.1603, 0.1594],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1381, 0.0052, 0.1135, 0.1091, 0.1323, 0.3896, 0.1123],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1319, 0.0055, 0.1003, 0.1418, 0.1807, 0.1028, 0.3369],
       grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 40.73344881987393
actor:  0 policy actor:  1  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.0
siam score:  -0.86718714
actor:  1 policy actor:  1  step number:  33 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.08339999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
Printing some Q and Qe and total Qs values:  [[0.093]
 [0.309]
 [0.093]
 [0.093]
 [0.316]
 [0.302]
 [0.093]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.093]
 [0.309]
 [0.093]
 [0.093]
 [0.316]
 [0.302]
 [0.093]]
actor:  1 policy actor:  1  step number:  28 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  51.63967595907604
actor:  1 policy actor:  1  step number:  23 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  28 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.08339999999999989 0.6815 0.6815
maxi score, test score, baseline:  0.08339999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.08339999999999989 0.6815 0.6815
printing an ep nov before normalisation:  0.5162568182868199
actor:  1 policy actor:  1  step number:  22 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  0.16941934078118948
actor:  0 policy actor:  0  step number:  20 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.378]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.386]
 [0.378]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]]
maxi score, test score, baseline:  0.08665999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  53.45299906985466
printing an ep nov before normalisation:  32.05357690693744
actions average: 
K:  3  action  0 :  tensor([0.7630, 0.0044, 0.0409, 0.0436, 0.0588, 0.0446, 0.0447],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0024, 0.9661, 0.0032, 0.0096, 0.0015, 0.0015, 0.0157],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0403, 0.0627, 0.7069, 0.0509, 0.0472, 0.0513, 0.0406],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0649, 0.0011, 0.0374, 0.6675, 0.1061, 0.0620, 0.0609],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1705, 0.0453, 0.1226, 0.1640, 0.1743, 0.1731, 0.1502],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0662, 0.0699, 0.2639, 0.0892, 0.0725, 0.3804, 0.0579],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0922, 0.1675, 0.0866, 0.1648, 0.0945, 0.0824, 0.3120],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.08665999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
Printing some Q and Qe and total Qs values:  [[0.054]
 [0.245]
 [0.116]
 [0.246]
 [0.116]
 [0.116]
 [0.264]] [[46.554]
 [49.773]
 [38.294]
 [43.178]
 [38.294]
 [38.294]
 [45.628]] [[1.009]
 [1.336]
 [0.723]
 [1.059]
 [0.723]
 [0.723]
 [1.18 ]]
actor:  1 policy actor:  1  step number:  34 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  26 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.08665999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.08665999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  29 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  66.5162493659659
actor:  1 policy actor:  1  step number:  29 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.08665999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.08665999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.08665999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
siam score:  -0.87341696
Printing some Q and Qe and total Qs values:  [[0.884]
 [0.884]
 [0.884]
 [0.884]
 [0.884]
 [0.884]
 [0.884]] [[35.558]
 [35.558]
 [35.558]
 [35.558]
 [35.558]
 [35.558]
 [35.558]] [[0.884]
 [0.884]
 [0.884]
 [0.884]
 [0.884]
 [0.884]
 [0.884]]
maxi score, test score, baseline:  0.08665999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.08665999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
printing an ep nov before normalisation:  55.0955032985174
actor:  0 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.194]
 [0.233]
 [0.123]
 [0.218]
 [0.217]
 [0.213]
 [0.131]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.194]
 [0.233]
 [0.123]
 [0.218]
 [0.217]
 [0.213]
 [0.131]]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  45.41735861048843
printing an ep nov before normalisation:  35.7514062020778
maxi score, test score, baseline:  0.08969999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  31 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.08969999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.08969999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.08969999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.08969999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
printing an ep nov before normalisation:  56.318888239233935
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  46.69856685436303
actor:  1 policy actor:  1  step number:  33 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.08969999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
using explorer policy with actor:  1
siam score:  -0.862067
actor:  1 policy actor:  1  step number:  39 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  53.82466514578843
actor:  1 policy actor:  1  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  26 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.08969999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.08969999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
printing an ep nov before normalisation:  37.60013474524675
actor:  1 policy actor:  1  step number:  22 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.08969999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.08969999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.08969999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
printing an ep nov before normalisation:  46.59407242483574
printing an ep nov before normalisation:  50.99487882000924
maxi score, test score, baseline:  0.08969999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  35.88543880538472
printing an ep nov before normalisation:  29.020824432373047
actor:  1 policy actor:  1  step number:  35 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  34 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.08917999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
printing an ep nov before normalisation:  71.39013750147066
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.499]
 [0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]] [[40.234]
 [54.414]
 [40.234]
 [40.234]
 [40.234]
 [40.234]
 [40.234]] [[1.165]
 [1.88 ]
 [1.165]
 [1.165]
 [1.165]
 [1.165]
 [1.165]]
maxi score, test score, baseline:  0.08917999999999987 0.6815 0.6815
maxi score, test score, baseline:  0.08917999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.08917999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.08917999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.08917999999999987 0.6815 0.6815
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  31 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  4  action  0 :  tensor([0.2687, 0.0023, 0.0850, 0.1050, 0.3219, 0.1018, 0.1153],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0041, 0.9465, 0.0047, 0.0214, 0.0026, 0.0027, 0.0179],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0283, 0.0038, 0.8147, 0.0369, 0.0361, 0.0527, 0.0275],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0922, 0.0918, 0.0730, 0.4462, 0.1070, 0.0725, 0.1172],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1682, 0.0046, 0.0786, 0.1143, 0.4255, 0.0867, 0.1222],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1452, 0.0121, 0.1534, 0.1255, 0.1246, 0.2739, 0.1654],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1444, 0.1205, 0.1266, 0.1627, 0.1307, 0.1443, 0.1707],
       grad_fn=<DivBackward0>)
siam score:  -0.86697656
maxi score, test score, baseline:  0.08619999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  33 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.08619999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
printing an ep nov before normalisation:  60.05775817466689
maxi score, test score, baseline:  0.08619999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  21 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  22 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  2  action  0 :  tensor([0.7907, 0.0018, 0.0338, 0.0323, 0.0741, 0.0282, 0.0390],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0012,     0.9694,     0.0149,     0.0039,     0.0009,     0.0011,
            0.0087], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0374, 0.0015, 0.7605, 0.0333, 0.0398, 0.0790, 0.0485],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0781, 0.0943, 0.1157, 0.3997, 0.1073, 0.1038, 0.1012],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.1871,     0.0002,     0.0590,     0.0652,     0.5430,     0.0719,
            0.0737], grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1164, 0.0247, 0.1980, 0.0931, 0.1445, 0.2672, 0.1560],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0953, 0.0879, 0.1007, 0.1133, 0.1085, 0.0932, 0.4012],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  32 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  20 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  17.337640570783293
maxi score, test score, baseline:  0.0862199999999999 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
from probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.53 ]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.454]
 [0.53 ]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]]
maxi score, test score, baseline:  0.0862199999999999 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.0862199999999999 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  37.40036310068342
maxi score, test score, baseline:  0.0862199999999999 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
Printing some Q and Qe and total Qs values:  [[0.754]
 [0.771]
 [0.732]
 [0.699]
 [0.719]
 [0.732]
 [0.719]] [[29.561]
 [29.515]
 [28.658]
 [32.144]
 [32.035]
 [28.658]
 [26.969]] [[0.754]
 [0.771]
 [0.732]
 [0.699]
 [0.719]
 [0.732]
 [0.719]]
printing an ep nov before normalisation:  37.42256306328035
maxi score, test score, baseline:  0.08371999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.94273194610295
actor:  0 policy actor:  1  step number:  24 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  22 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.66 ]
 [0.694]
 [0.705]
 [0.709]
 [0.711]
 [0.66 ]
 [0.71 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.66 ]
 [0.694]
 [0.705]
 [0.709]
 [0.711]
 [0.66 ]
 [0.71 ]]
printing an ep nov before normalisation:  25.051613715018668
Printing some Q and Qe and total Qs values:  [[0.154]
 [0.332]
 [0.154]
 [0.09 ]
 [0.154]
 [0.154]
 [0.129]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.154]
 [0.332]
 [0.154]
 [0.09 ]
 [0.154]
 [0.154]
 [0.129]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  40.70363491075851
maxi score, test score, baseline:  0.08379999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  0 policy actor:  0  step number:  22 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  24 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.08397999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  38.847333146471755
maxi score, test score, baseline:  0.08397999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  20 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.08397999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.08397999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
printing an ep nov before normalisation:  35.87639163915845
maxi score, test score, baseline:  0.08397999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  26 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.32 ]
 [0.366]
 [0.32 ]
 [0.32 ]
 [0.32 ]
 [0.32 ]
 [0.32 ]] [[39.72 ]
 [43.266]
 [39.72 ]
 [39.72 ]
 [39.72 ]
 [39.72 ]
 [39.72 ]] [[1.132]
 [1.316]
 [1.132]
 [1.132]
 [1.132]
 [1.132]
 [1.132]]
printing an ep nov before normalisation:  49.666983585014954
actor:  1 policy actor:  1  step number:  31 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.08397999999999989 0.6815 0.6815
actor:  1 policy actor:  1  step number:  21 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  30 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.08397999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
printing an ep nov before normalisation:  32.33780860900879
printing an ep nov before normalisation:  54.48008206745629
actor:  0 policy actor:  0  step number:  30 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.667
using another actor
printing an ep nov before normalisation:  48.16122225725539
actor:  1 policy actor:  1  step number:  23 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.08683999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.08683999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
printing an ep nov before normalisation:  37.10517576411262
actor:  1 policy actor:  1  step number:  40 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.106]
 [0.098]
 [0.092]
 [0.097]
 [0.132]
 [0.131]
 [0.082]] [[18.376]
 [40.541]
 [29.496]
 [31.913]
 [32.086]
 [34.27 ]
 [32.654]] [[0.106]
 [0.098]
 [0.092]
 [0.097]
 [0.132]
 [0.131]
 [0.082]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.57 ]
 [0.652]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]] [[51.723]
 [46.863]
 [44.752]
 [44.752]
 [44.752]
 [44.752]
 [44.752]] [[0.57 ]
 [0.652]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]]
maxi score, test score, baseline:  0.08683999999999988 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
printing an ep nov before normalisation:  37.439185850732194
actor:  0 policy actor:  1  step number:  39 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  23 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  2.0
siam score:  -0.85112864
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.08661999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  45 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.0
from probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.08661999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.08661999999999989 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.08661999999999989 0.6815 0.6815
printing an ep nov before normalisation:  54.83547944095787
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.817]
 [0.707]
 [0.652]
 [0.631]
 [0.628]
 [0.621]] [[28.816]
 [35.818]
 [32.113]
 [28.354]
 [29.039]
 [29.403]
 [29.786]] [[0.639]
 [0.817]
 [0.707]
 [0.652]
 [0.631]
 [0.628]
 [0.621]]
actor:  1 policy actor:  1  step number:  21 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  32 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.08653999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
Printing some Q and Qe and total Qs values:  [[0.294]
 [0.356]
 [0.294]
 [0.294]
 [0.294]
 [0.294]
 [0.294]] [[49.047]
 [50.866]
 [49.047]
 [49.047]
 [49.047]
 [49.047]
 [49.047]] [[1.232]
 [1.356]
 [1.232]
 [1.232]
 [1.232]
 [1.232]
 [1.232]]
actor:  0 policy actor:  0  step number:  34 total reward:  0.22999999999999954  reward:  1.0 rdn_beta:  1.667
siam score:  -0.84737474
actor:  1 policy actor:  1  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.08603999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
printing an ep nov before normalisation:  40.77378273010254
printing an ep nov before normalisation:  68.26236440546747
printing an ep nov before normalisation:  48.527291012037985
maxi score, test score, baseline:  0.08603999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  51.55640509288445
actor:  1 policy actor:  1  step number:  35 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  37.5119976245594
actor:  1 policy actor:  1  step number:  42 total reward:  0.2899999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.08603999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.552]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]] [[46.721]
 [26.393]
 [46.721]
 [46.721]
 [46.721]
 [46.721]
 [46.721]] [[1.413]
 [1.043]
 [1.413]
 [1.413]
 [1.413]
 [1.413]
 [1.413]]
actor:  1 policy actor:  1  step number:  37 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.08603999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.08603999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.08603999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.0
from probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.08603999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.08603999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  27 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.08603999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
printing an ep nov before normalisation:  48.310321960965304
actor:  1 policy actor:  1  step number:  31 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  46.39087473290802
maxi score, test score, baseline:  0.08603999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  0 policy actor:  1  step number:  32 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.08909999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
Printing some Q and Qe and total Qs values:  [[0.294]
 [0.413]
 [0.294]
 [0.294]
 [0.294]
 [0.294]
 [0.294]] [[50.52 ]
 [48.702]
 [50.52 ]
 [50.52 ]
 [50.52 ]
 [50.52 ]
 [50.52 ]] [[1.108]
 [1.178]
 [1.108]
 [1.108]
 [1.108]
 [1.108]
 [1.108]]
actor:  1 policy actor:  1  step number:  36 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  42.670208183288246
maxi score, test score, baseline:  0.08909999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  36 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.08909999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]] [[44.11]
 [44.11]
 [44.11]
 [44.11]
 [44.11]
 [44.11]
 [44.11]] [[1.973]
 [1.973]
 [1.973]
 [1.973]
 [1.973]
 [1.973]
 [1.973]]
Printing some Q and Qe and total Qs values:  [[0.184]
 [0.28 ]
 [0.184]
 [0.184]
 [0.184]
 [0.184]
 [0.184]] [[42.091]
 [49.943]
 [42.091]
 [42.091]
 [42.091]
 [42.091]
 [42.091]] [[0.799]
 [1.113]
 [0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.08909999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  42 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.18 ]
 [0.198]
 [0.18 ]
 [0.18 ]
 [0.18 ]
 [0.18 ]
 [0.18 ]] [[39.555]
 [42.503]
 [39.555]
 [39.555]
 [39.555]
 [39.555]
 [39.555]] [[1.371]
 [1.519]
 [1.371]
 [1.371]
 [1.371]
 [1.371]
 [1.371]]
printing an ep nov before normalisation:  41.28011856835512
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]] [[46.169]
 [46.169]
 [46.169]
 [46.169]
 [46.169]
 [46.169]
 [46.169]] [[2.345]
 [2.345]
 [2.345]
 [2.345]
 [2.345]
 [2.345]
 [2.345]]
Printing some Q and Qe and total Qs values:  [[0.692]
 [0.912]
 [0.848]
 [0.706]
 [0.7  ]
 [0.848]
 [0.848]] [[47.774]
 [47.322]
 [53.689]
 [47.898]
 [52.307]
 [53.689]
 [53.689]] [[0.692]
 [0.912]
 [0.848]
 [0.706]
 [0.7  ]
 [0.848]
 [0.848]]
maxi score, test score, baseline:  0.08909999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  21 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.08909999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  0 policy actor:  1  step number:  20 total reward:  0.69  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.08959999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  35 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8556914
maxi score, test score, baseline:  0.08959999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.08959999999999987 0.6815 0.6815
maxi score, test score, baseline:  0.08959999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  19 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.08959999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  0 policy actor:  1  step number:  21 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.527]
 [0.708]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]] [[57.508]
 [56.612]
 [57.508]
 [57.508]
 [57.508]
 [57.508]
 [57.508]] [[1.432]
 [1.593]
 [1.432]
 [1.432]
 [1.432]
 [1.432]
 [1.432]]
line 256 mcts: sample exp_bonus 28.786114197559154
maxi score, test score, baseline:  0.08671999999999987 0.6815 0.6815
maxi score, test score, baseline:  0.08671999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
Printing some Q and Qe and total Qs values:  [[0.206]
 [0.206]
 [0.429]
 [0.206]
 [0.206]
 [0.206]
 [0.206]] [[34.687]
 [34.687]
 [42.85 ]
 [34.687]
 [34.687]
 [34.687]
 [34.687]] [[1.023]
 [1.023]
 [1.612]
 [1.023]
 [1.023]
 [1.023]
 [1.023]]
maxi score, test score, baseline:  0.08671999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.08671999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.08671999999999987 0.6815 0.6815
Printing some Q and Qe and total Qs values:  [[0.308]
 [0.49 ]
 [0.308]
 [0.308]
 [0.308]
 [0.308]
 [0.308]] [[55.154]
 [58.076]
 [55.154]
 [55.154]
 [55.154]
 [55.154]
 [55.154]] [[1.622]
 [1.904]
 [1.622]
 [1.622]
 [1.622]
 [1.622]
 [1.622]]
maxi score, test score, baseline:  0.08671999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.08671999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5099999999999999  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8491608
maxi score, test score, baseline:  0.08671999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
printing an ep nov before normalisation:  58.75196542877051
actor:  1 policy actor:  1  step number:  21 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  61.32152342198185
maxi score, test score, baseline:  0.08671999999999987 0.6815 0.6815
maxi score, test score, baseline:  0.08671999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
printing an ep nov before normalisation:  54.23555748276487
actor:  1 policy actor:  1  step number:  20 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.321]
 [0.235]
 [0.722]
 [0.454]
 [0.377]
 [0.538]
 [0.251]] [[47.842]
 [48.209]
 [42.047]
 [38.142]
 [41.55 ]
 [40.725]
 [43.233]] [[0.321]
 [0.235]
 [0.722]
 [0.454]
 [0.377]
 [0.538]
 [0.251]]
maxi score, test score, baseline:  0.08671999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.08671999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
maxi score, test score, baseline:  0.08671999999999987 0.6815 0.6815
printing an ep nov before normalisation:  36.48971929757674
actor:  1 policy actor:  1  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  0.20919662768093872
maxi score, test score, baseline:  0.08671999999999987 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  0 policy actor:  1  step number:  27 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.08645999999999986 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  0 policy actor:  0  step number:  19 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  56.71248880377389
printing an ep nov before normalisation:  32.87562353526123
maxi score, test score, baseline:  0.08977999999999986 0.6815 0.6815
probs:  [0.046729977969820306, 0.046729977969820306, 0.28941498975019675, 0.046729977969820306, 0.2527985723898472, 0.3175965039504951]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.251]
 [0.473]
 [0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.251]] [[31.154]
 [37.142]
 [31.154]
 [31.154]
 [31.154]
 [31.154]
 [31.154]] [[0.525]
 [0.848]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]]
actor:  1 policy actor:  1  step number:  20 total reward:  0.69  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]] [[39.26]
 [39.26]
 [39.26]
 [39.26]
 [39.26]
 [39.26]
 [39.26]] [[26.713]
 [26.713]
 [26.713]
 [26.713]
 [26.713]
 [26.713]
 [26.713]]
maxi score, test score, baseline:  0.08977999999999986 0.6815 0.6815
probs:  [0.04692615031681635, 0.04692615031681635, 0.2883116967973963, 0.04692615031681635, 0.2518913433118348, 0.31901850894031974]
maxi score, test score, baseline:  0.08977999999999986 0.6815 0.6815
printing an ep nov before normalisation:  56.79478880738488
actor:  1 policy actor:  1  step number:  34 total reward:  0.24999999999999956  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  50.96260062650569
printing an ep nov before normalisation:  45.766146035646244
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  32.554670355743745
actor:  0 policy actor:  0  step number:  39 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  31 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.0004932110573463433
actor:  1 policy actor:  1  step number:  48 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  4  action  0 :  tensor([0.6894, 0.0078, 0.0392, 0.0306, 0.1060, 0.0335, 0.0936],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0084,     0.9662,     0.0043,     0.0076,     0.0015,     0.0007,
            0.0112], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0525, 0.0044, 0.7435, 0.0471, 0.0522, 0.0553, 0.0450],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0956, 0.0715, 0.1258, 0.3876, 0.1069, 0.1091, 0.1035],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1278, 0.0717, 0.1065, 0.1036, 0.3920, 0.0781, 0.1203],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1355, 0.0020, 0.2435, 0.1492, 0.1711, 0.2008, 0.0979],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0573, 0.2016, 0.0560, 0.0945, 0.0601, 0.0569, 0.4736],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  25 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.09233999999999988 0.6815 0.6815
probs:  [0.04692615031681635, 0.04692615031681635, 0.2883116967973963, 0.04692615031681635, 0.2518913433118348, 0.31901850894031974]
printing an ep nov before normalisation:  62.27847049215402
actor:  1 policy actor:  1  step number:  36 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  48.18231799931766
maxi score, test score, baseline:  0.09233999999999988 0.6815 0.6815
probs:  [0.04692615031681635, 0.04692615031681635, 0.2883116967973963, 0.04692615031681635, 0.2518913433118348, 0.31901850894031974]
actions average: 
K:  2  action  0 :  tensor([0.6783, 0.0059, 0.0577, 0.0597, 0.0644, 0.0683, 0.0658],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0022,     0.9709,     0.0028,     0.0034,     0.0008,     0.0008,
            0.0192], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0573, 0.0220, 0.6802, 0.0475, 0.0580, 0.0867, 0.0484],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0947, 0.0068, 0.1048, 0.4491, 0.1115, 0.0880, 0.1452],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1302, 0.0033, 0.0535, 0.0535, 0.5659, 0.0590, 0.1344],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0801, 0.0025, 0.1811, 0.1045, 0.0804, 0.4631, 0.0883],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0875, 0.0766, 0.1235, 0.1003, 0.1201, 0.0936, 0.3983],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  53.62534955602246
printing an ep nov before normalisation:  29.35393071916388
actor:  1 policy actor:  1  step number:  27 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  30.943021006175265
maxi score, test score, baseline:  0.08951999999999988 0.6815 0.6815
maxi score, test score, baseline:  0.08951999999999988 0.6815 0.6815
probs:  [0.04692615031681635, 0.04692615031681635, 0.2883116967973963, 0.04692615031681635, 0.2518913433118348, 0.31901850894031974]
actor:  1 policy actor:  1  step number:  24 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.08951999999999988 0.6815 0.6815
probs:  [0.04692615031681635, 0.04692615031681635, 0.2883116967973963, 0.04692615031681635, 0.2518913433118348, 0.31901850894031974]
actor:  1 policy actor:  1  step number:  47 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.08951999999999988 0.6815 0.6815
probs:  [0.04692615031681635, 0.04692615031681635, 0.2883116967973963, 0.04692615031681635, 0.2518913433118348, 0.31901850894031974]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.08951999999999988 0.6815 0.6815
printing an ep nov before normalisation:  42.36630345550241
actor:  0 policy actor:  0  step number:  29 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  30 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09243999999999988 0.6815 0.6815
probs:  [0.04692615031681635, 0.04692615031681635, 0.2883116967973963, 0.04692615031681635, 0.2518913433118348, 0.31901850894031974]
Printing some Q and Qe and total Qs values:  [[0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.667]
 [0.665]
 [0.671]] [[53.099]
 [53.099]
 [53.099]
 [53.099]
 [55.944]
 [57.628]
 [53.099]] [[2.37 ]
 [2.37 ]
 [2.37 ]
 [2.37 ]
 [2.517]
 [2.605]
 [2.37 ]]
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]] [[61.81]
 [61.81]
 [61.81]
 [61.81]
 [61.81]
 [61.81]
 [61.81]] [[2.658]
 [2.658]
 [2.658]
 [2.658]
 [2.658]
 [2.658]
 [2.658]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09243999999999988 0.6815 0.6815
printing an ep nov before normalisation:  51.475801192749486
actor:  1 policy actor:  1  step number:  24 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09243999999999988 0.6815 0.6815
probs:  [0.04692615031681635, 0.04692615031681635, 0.2883116967973963, 0.04692615031681635, 0.2518913433118348, 0.31901850894031974]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.154]
 [0.357]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.27 ]] [[22.004]
 [27.07 ]
 [22.004]
 [22.004]
 [22.004]
 [22.004]
 [27.102]] [[0.982]
 [1.378]
 [0.982]
 [0.982]
 [0.982]
 [0.982]
 [1.292]]
maxi score, test score, baseline:  0.09243999999999988 0.6815 0.6815
probs:  [0.04692615031681635, 0.04692615031681635, 0.2883116967973963, 0.04692615031681635, 0.2518913433118348, 0.31901850894031974]
printing an ep nov before normalisation:  33.38479214021272
actor:  1 policy actor:  1  step number:  32 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
using another actor
maxi score, test score, baseline:  0.09243999999999988 0.6815 0.6815
probs:  [0.04692615031681635, 0.04692615031681635, 0.2883116967973963, 0.04692615031681635, 0.2518913433118348, 0.31901850894031974]
maxi score, test score, baseline:  0.09243999999999988 0.6815 0.6815
probs:  [0.04692615031681635, 0.04692615031681635, 0.2883116967973963, 0.04692615031681635, 0.2518913433118348, 0.31901850894031974]
printing an ep nov before normalisation:  36.13121509552002
actor:  1 policy actor:  1  step number:  37 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.0
using another actor
maxi score, test score, baseline:  0.08949999999999989 0.6815 0.6815
probs:  [0.04692615031681635, 0.04692615031681635, 0.2883116967973963, 0.04692615031681635, 0.2518913433118348, 0.31901850894031974]
printing an ep nov before normalisation:  41.318451596108964
maxi score, test score, baseline:  0.08949999999999989 0.6815 0.6815
probs:  [0.04692615031681635, 0.04692615031681635, 0.2883116967973963, 0.04692615031681635, 0.2518913433118348, 0.31901850894031974]
printing an ep nov before normalisation:  45.148635241155546
Printing some Q and Qe and total Qs values:  [[0.879]
 [0.908]
 [0.837]
 [0.837]
 [0.769]
 [0.837]
 [0.837]] [[39.966]
 [42.829]
 [53.927]
 [53.927]
 [42.45 ]
 [53.927]
 [53.927]] [[0.879]
 [0.908]
 [0.837]
 [0.837]
 [0.769]
 [0.837]
 [0.837]]
printing an ep nov before normalisation:  46.05111095582814
actor:  1 policy actor:  1  step number:  20 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  30 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.257]
 [0.257]
 [0.491]
 [0.257]
 [0.257]
 [0.257]
 [0.257]] [[38.507]
 [38.507]
 [44.294]
 [38.507]
 [38.507]
 [38.507]
 [38.507]] [[0.71 ]
 [0.71 ]
 [1.084]
 [0.71 ]
 [0.71 ]
 [0.71 ]
 [0.71 ]]
maxi score, test score, baseline:  0.08919999999999988 0.6815 0.6815
probs:  [0.04692615031681635, 0.04692615031681635, 0.2883116967973963, 0.04692615031681635, 0.2518913433118348, 0.31901850894031974]
maxi score, test score, baseline:  0.08919999999999988 0.6815 0.6815
probs:  [0.04692615031681635, 0.04692615031681635, 0.2883116967973963, 0.04692615031681635, 0.2518913433118348, 0.31901850894031974]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.07 ]
 [0.164]
 [0.063]
 [0.062]
 [0.056]
 [0.063]
 [0.069]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.07 ]
 [0.164]
 [0.063]
 [0.062]
 [0.056]
 [0.063]
 [0.069]]
maxi score, test score, baseline:  0.08919999999999988 0.6815 0.6815
probs:  [0.04692615031681635, 0.04692615031681635, 0.2883116967973963, 0.04692615031681635, 0.2518913433118348, 0.31901850894031974]
actor:  1 policy actor:  1  step number:  31 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.291]
 [0.362]
 [0.274]
 [0.266]
 [0.291]
 [0.291]
 [0.291]] [[37.577]
 [41.007]
 [35.063]
 [34.296]
 [37.577]
 [37.577]
 [37.577]] [[1.281]
 [1.518]
 [1.142]
 [1.098]
 [1.281]
 [1.281]
 [1.281]]
Printing some Q and Qe and total Qs values:  [[ 0.539]
 [ 0.54 ]
 [-0.08 ]
 [ 0.436]
 [ 0.392]
 [-0.037]
 [ 0.534]] [[41.061]
 [41.12 ]
 [47.978]
 [43.183]
 [44.678]
 [39.699]
 [42.474]] [[1.757]
 [1.762]
 [1.635]
 [1.806]
 [1.871]
 [1.083]
 [1.854]]
maxi score, test score, baseline:  0.08919999999999988 0.6815 0.6815
printing an ep nov before normalisation:  47.98563991713477
Printing some Q and Qe and total Qs values:  [[0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.678]
 [0.678]
 [0.683]] [[44.337]
 [44.337]
 [44.337]
 [44.337]
 [48.623]
 [50.14 ]
 [44.337]] [[2.082]
 [2.082]
 [2.082]
 [2.082]
 [2.325]
 [2.411]
 [2.082]]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.08919999999999988 0.6815 0.6815
probs:  [0.04692615031681635, 0.04692615031681635, 0.2883116967973963, 0.04692615031681635, 0.2518913433118348, 0.31901850894031974]
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.58 ]
 [0.351]
 [0.437]
 [0.351]
 [0.351]
 [0.351]] [[35.782]
 [35.708]
 [31.288]
 [34.332]
 [31.288]
 [31.288]
 [31.288]] [[1.118]
 [1.105]
 [0.737]
 [0.918]
 [0.737]
 [0.737]
 [0.737]]
maxi score, test score, baseline:  0.08919999999999988 0.6815 0.6815
probs:  [0.04692615031681635, 0.04692615031681635, 0.2883116967973963, 0.04692615031681635, 0.2518913433118348, 0.31901850894031974]
Printing some Q and Qe and total Qs values:  [[0.19 ]
 [0.259]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]] [[39.608]
 [40.184]
 [39.608]
 [39.608]
 [39.608]
 [39.608]
 [39.608]] [[0.752]
 [0.832]
 [0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]]
actor:  1 policy actor:  1  step number:  43 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.08919999999999988 0.6815 0.6815
probs:  [0.04692615031681635, 0.04692615031681635, 0.2883116967973963, 0.04692615031681635, 0.2518913433118348, 0.31901850894031974]
printing an ep nov before normalisation:  41.8947738351944
maxi score, test score, baseline:  0.08919999999999988 0.6815 0.6815
probs:  [0.04692615031681635, 0.04692615031681635, 0.2883116967973963, 0.04692615031681635, 0.2518913433118348, 0.31901850894031974]
printing an ep nov before normalisation:  40.51875978177849
maxi score, test score, baseline:  0.08919999999999988 0.6815 0.6815
probs:  [0.04692615031681635, 0.04692615031681635, 0.2883116967973963, 0.04692615031681635, 0.2518913433118348, 0.31901850894031974]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.08919999999999988 0.6815 0.6815
actor:  1 policy actor:  1  step number:  33 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  27 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.08919999999999988 0.6815 0.6815
maxi score, test score, baseline:  0.08919999999999988 0.6815 0.6815
probs:  [0.04692615031681635, 0.04692615031681635, 0.2883116967973963, 0.04692615031681635, 0.2518913433118348, 0.31901850894031974]
printing an ep nov before normalisation:  15.859065922968329
actions average: 
K:  4  action  0 :  tensor([0.6555, 0.0103, 0.0812, 0.0472, 0.0650, 0.0585, 0.0824],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0026,     0.9699,     0.0043,     0.0060,     0.0003,     0.0011,
            0.0158], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0906, 0.0476, 0.4973, 0.0874, 0.0798, 0.1282, 0.0691],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0720, 0.0085, 0.1323, 0.4894, 0.1339, 0.0986, 0.0653],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1153, 0.0127, 0.1155, 0.1373, 0.3948, 0.1246, 0.0998],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1529, 0.0436, 0.0850, 0.1286, 0.2487, 0.0959, 0.2454],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0815, 0.1080, 0.0702, 0.0987, 0.0788, 0.0879, 0.4749],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.08919999999999988 0.6815 0.6815
probs:  [0.04692615031681635, 0.04692615031681635, 0.2883116967973963, 0.04692615031681635, 0.2518913433118348, 0.31901850894031974]
actor:  1 policy actor:  1  step number:  45 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  32 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  0  action  0 :  tensor([0.5010, 0.0051, 0.0752, 0.0803, 0.1200, 0.1008, 0.1177],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0013,     0.9687,     0.0038,     0.0079,     0.0003,     0.0088,
            0.0091], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0429, 0.0024, 0.7731, 0.0348, 0.0347, 0.0670, 0.0451],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1569, 0.1365, 0.1011, 0.1996, 0.1101, 0.1185, 0.1773],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1322, 0.0034, 0.0793, 0.0945, 0.4972, 0.0885, 0.1048],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0915, 0.0041, 0.1368, 0.0949, 0.1000, 0.4681, 0.1046],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1390, 0.1050, 0.1129, 0.1333, 0.1168, 0.1383, 0.2547],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  29 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.08919999999999988 0.6815 0.6815
probs:  [0.04692615031681635, 0.04692615031681635, 0.2883116967973963, 0.04692615031681635, 0.2518913433118348, 0.31901850894031974]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]] [[40.628]
 [40.628]
 [40.628]
 [40.628]
 [40.628]
 [40.628]
 [40.628]] [[0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]]
maxi score, test score, baseline:  0.08919999999999988 0.6815 0.6815
probs:  [0.04692615031681635, 0.04692615031681635, 0.2883116967973963, 0.04692615031681635, 0.2518913433118348, 0.31901850894031974]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.08919999999999988 0.6815 0.6815
probs:  [0.04692615031681635, 0.04692615031681635, 0.2883116967973963, 0.04692615031681635, 0.2518913433118348, 0.31901850894031974]
Printing some Q and Qe and total Qs values:  [[0.186]
 [0.234]
 [0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.186]
 [0.234]
 [0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]]
maxi score, test score, baseline:  0.08919999999999988 0.6815 0.6815
probs:  [0.04692615031681635, 0.04692615031681635, 0.2883116967973963, 0.04692615031681635, 0.2518913433118348, 0.31901850894031974]
actor:  1 policy actor:  1  step number:  23 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  35.308358669281006
maxi score, test score, baseline:  0.08919999999999988 0.6815 0.6815
Printing some Q and Qe and total Qs values:  [[0.771]
 [0.885]
 [0.812]
 [0.812]
 [0.812]
 [0.812]
 [0.812]] [[55.425]
 [45.789]
 [42.507]
 [42.507]
 [42.507]
 [42.507]
 [42.507]] [[0.771]
 [0.885]
 [0.812]
 [0.812]
 [0.812]
 [0.812]
 [0.812]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  37.10427814059787
maxi score, test score, baseline:  0.0862199999999999 0.6815 0.6815
probs:  [0.04692615031681635, 0.04692615031681635, 0.2883116967973963, 0.04692615031681635, 0.2518913433118348, 0.31901850894031974]
printing an ep nov before normalisation:  44.30637949470973
siam score:  -0.8522164
actor:  1 policy actor:  1  step number:  35 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  25 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.191]
 [0.261]
 [0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]] [[36.19 ]
 [44.051]
 [36.19 ]
 [36.19 ]
 [36.19 ]
 [36.19 ]
 [36.19 ]] [[1.003]
 [1.408]
 [1.003]
 [1.003]
 [1.003]
 [1.003]
 [1.003]]
UNIT TEST: sample policy line 217 mcts : [0.041 0.408 0.082 0.041 0.102 0.286 0.041]
using explorer policy with actor:  1
printing an ep nov before normalisation:  60.228473139612056
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  0.0014818194452459466
actor:  1 policy actor:  1  step number:  43 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.34 ]
 [0.442]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]] [[42.503]
 [44.933]
 [42.503]
 [42.503]
 [42.503]
 [42.503]
 [42.503]] [[1.9  ]
 [2.143]
 [1.9  ]
 [1.9  ]
 [1.9  ]
 [1.9  ]
 [1.9  ]]
maxi score, test score, baseline:  0.0862199999999999 0.6815 0.6815
probs:  [0.04692615031681635, 0.04692615031681635, 0.2883116967973963, 0.04692615031681635, 0.2518913433118348, 0.31901850894031974]
siam score:  -0.84860086
maxi score, test score, baseline:  0.0862199999999999 0.6815 0.6815
probs:  [0.04692615031681635, 0.04692615031681635, 0.2883116967973963, 0.04692615031681635, 0.2518913433118348, 0.31901850894031974]
actor:  1 policy actor:  1  step number:  33 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  3  action  0 :  tensor([0.5902, 0.0235, 0.1103, 0.0721, 0.0754, 0.0549, 0.0736],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0088, 0.9361, 0.0132, 0.0147, 0.0015, 0.0061, 0.0196],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0331, 0.0095, 0.7565, 0.0400, 0.0357, 0.0706, 0.0548],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0860, 0.0172, 0.1050, 0.4079, 0.0973, 0.0888, 0.1976],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1897, 0.0528, 0.0886, 0.1039, 0.4157, 0.0754, 0.0740],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0610, 0.0026, 0.0634, 0.2610, 0.0871, 0.4092, 0.1157],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0856, 0.3167, 0.0811, 0.1405, 0.0968, 0.0877, 0.1915],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  21 total reward:  0.76  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  65.68672499143376
Printing some Q and Qe and total Qs values:  [[0.296]
 [0.449]
 [0.27 ]
 [0.296]
 [0.296]
 [0.296]
 [0.269]] [[45.475]
 [40.736]
 [39.06 ]
 [45.475]
 [45.475]
 [45.475]
 [37.877]] [[1.464]
 [1.404]
 [1.149]
 [1.464]
 [1.464]
 [1.464]
 [1.094]]
using another actor
actor:  1 policy actor:  1  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.0
using another actor
from probs:  [0.05835072097054796, 0.05835072097054796, 0.22405876571895952, 0.05835072097054796, 0.19905666574598585, 0.40183240562341077]
Printing some Q and Qe and total Qs values:  [[0.164]
 [0.318]
 [0.173]
 [0.191]
 [0.156]
 [0.365]
 [0.146]] [[36.162]
 [40.952]
 [37.752]
 [36.558]
 [36.351]
 [39.836]
 [36.898]] [[0.561]
 [0.814]
 [0.602]
 [0.596]
 [0.557]
 [0.837]
 [0.558]]
maxi score, test score, baseline:  0.0862199999999999 0.6815 0.6815
probs:  [0.05835072097054796, 0.05835072097054796, 0.22405876571895952, 0.05835072097054796, 0.19905666574598585, 0.40183240562341077]
printing an ep nov before normalisation:  62.95443120524186
Printing some Q and Qe and total Qs values:  [[0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]] [[59.081]
 [59.081]
 [59.081]
 [59.081]
 [59.081]
 [59.081]
 [59.081]] [[0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]]
actor:  0 policy actor:  1  step number:  22 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.333
siam score:  -0.8513092
maxi score, test score, baseline:  0.0867799999999999 0.6815 0.6815
probs:  [0.05835072097054796, 0.05835072097054796, 0.22405876571895952, 0.05835072097054796, 0.19905666574598585, 0.40183240562341077]
actor:  0 policy actor:  1  step number:  26 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  27 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.751]
 [0.777]
 [0.724]
 [0.717]
 [0.74 ]
 [0.713]
 [0.713]] [[53.944]
 [48.677]
 [52.721]
 [54.592]
 [54.147]
 [53.627]
 [53.627]] [[1.948]
 [1.78 ]
 [1.876]
 [1.938]
 [1.945]
 [1.899]
 [1.899]]
maxi score, test score, baseline:  0.08967999999999987 0.6815 0.6815
probs:  [0.05835072097054796, 0.05835072097054796, 0.22405876571895952, 0.05835072097054796, 0.19905666574598585, 0.40183240562341077]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  33 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
from probs:  [0.05835072097054796, 0.05835072097054796, 0.22405876571895952, 0.05835072097054796, 0.19905666574598585, 0.40183240562341077]
maxi score, test score, baseline:  0.09001999999999988 0.6815 0.6815
probs:  [0.05835072097054796, 0.05835072097054796, 0.22405876571895952, 0.05835072097054796, 0.19905666574598585, 0.40183240562341077]
actor:  1 policy actor:  1  step number:  22 total reward:  0.69  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.09001999999999988 0.6815 0.6815
maxi score, test score, baseline:  0.09001999999999988 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
actor:  0 policy actor:  1  step number:  21 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09317999999999987 0.6815 0.6815
UNIT TEST: sample policy line 217 mcts : [0.245 0.224 0.02  0.041 0.245 0.204 0.02 ]
actor:  1 policy actor:  1  step number:  42 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  36.29281772040611
maxi score, test score, baseline:  0.09317999999999987 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
printing an ep nov before normalisation:  41.311192359638675
printing an ep nov before normalisation:  48.40162772174526
actor:  1 policy actor:  1  step number:  23 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  27 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09317999999999987 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.09317999999999987 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
actor:  1 policy actor:  1  step number:  26 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  33 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09317999999999987 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
printing an ep nov before normalisation:  45.87265273455333
maxi score, test score, baseline:  0.09317999999999987 0.6815 0.6815
maxi score, test score, baseline:  0.09317999999999987 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
maxi score, test score, baseline:  0.09317999999999987 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09317999999999987 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.61 ]
 [0.617]
 [0.536]
 [0.462]
 [0.545]
 [0.606]] [[35.559]
 [37.353]
 [36.889]
 [38.566]
 [43.069]
 [39.044]
 [36.666]] [[0.806]
 [0.834]
 [0.837]
 [0.774]
 [0.751]
 [0.788]
 [0.823]]
actor:  1 policy actor:  1  step number:  21 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09317999999999987 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
maxi score, test score, baseline:  0.09317999999999987 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
actor:  1 policy actor:  1  step number:  42 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  31 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09317999999999987 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  26.25904100263113
maxi score, test score, baseline:  0.09317999999999987 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09317999999999987 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
actor:  1 policy actor:  1  step number:  34 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  31 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09317999999999987 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
maxi score, test score, baseline:  0.09317999999999987 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
printing an ep nov before normalisation:  21.96394144479984
maxi score, test score, baseline:  0.09317999999999987 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
Printing some Q and Qe and total Qs values:  [[0.211]
 [0.233]
 [0.211]
 [0.211]
 [0.138]
 [0.211]
 [0.293]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.211]
 [0.233]
 [0.211]
 [0.211]
 [0.138]
 [0.211]
 [0.293]]
actor:  1 policy actor:  1  step number:  35 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.084]
 [-0.095]
 [-0.084]
 [-0.084]
 [-0.084]
 [-0.084]
 [-0.084]] [[24.914]
 [36.67 ]
 [24.914]
 [24.914]
 [24.914]
 [24.914]
 [24.914]] [[1.101]
 [1.649]
 [1.101]
 [1.101]
 [1.101]
 [1.101]
 [1.101]]
maxi score, test score, baseline:  0.09317999999999987 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
actor:  1 policy actor:  1  step number:  30 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  36 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.937]
 [0.959]
 [0.937]
 [0.937]
 [0.937]
 [0.937]
 [0.937]] [[39.196]
 [37.534]
 [39.196]
 [39.196]
 [39.196]
 [39.196]
 [39.196]] [[0.937]
 [0.959]
 [0.937]
 [0.937]
 [0.937]
 [0.937]
 [0.937]]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  20 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  56.225825514230664
maxi score, test score, baseline:  0.09303999999999987 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
Printing some Q and Qe and total Qs values:  [[0.751]
 [0.772]
 [0.772]
 [0.756]
 [0.76 ]
 [0.744]
 [0.752]] [[58.493]
 [58.455]
 [58.455]
 [61.296]
 [60.253]
 [58.405]
 [62.316]] [[2.567]
 [2.586]
 [2.586]
 [2.707]
 [2.661]
 [2.556]
 [2.752]]
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]] [[38.794]
 [34.456]
 [34.456]
 [34.456]
 [34.456]
 [34.456]
 [34.456]] [[1.875]
 [1.401]
 [1.401]
 [1.401]
 [1.401]
 [1.401]
 [1.401]]
maxi score, test score, baseline:  0.09303999999999987 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
printing an ep nov before normalisation:  0.02209807378989126
Printing some Q and Qe and total Qs values:  [[0.275]
 [0.302]
 [0.275]
 [0.275]
 [0.275]
 [0.275]
 [0.275]] [[22.859]
 [27.58 ]
 [22.859]
 [22.859]
 [22.859]
 [22.859]
 [22.859]] [[0.82 ]
 [0.997]
 [0.82 ]
 [0.82 ]
 [0.82 ]
 [0.82 ]
 [0.82 ]]
actor:  1 policy actor:  1  step number:  20 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  56.71298981091174
printing an ep nov before normalisation:  40.484449065044934
Printing some Q and Qe and total Qs values:  [[0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.775]] [[54.938]
 [54.938]
 [54.938]
 [54.938]
 [54.938]
 [54.938]
 [54.938]] [[1.775]
 [1.775]
 [1.775]
 [1.775]
 [1.775]
 [1.775]
 [1.775]]
actor:  1 policy actor:  1  step number:  36 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.478]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.429]
 [0.478]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]]
printing an ep nov before normalisation:  30.575511411837443
maxi score, test score, baseline:  0.09303999999999987 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
printing an ep nov before normalisation:  60.111532690745534
actor:  1 policy actor:  1  step number:  33 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.66 ]
 [0.584]
 [0.631]
 [0.596]
 [0.593]
 [0.596]] [[15.408]
 [17.491]
 [15.231]
 [16.004]
 [15.408]
 [15.782]
 [15.449]] [[1.121]
 [1.257]
 [1.104]
 [1.177]
 [1.122]
 [1.132]
 [1.124]]
actor:  1 policy actor:  1  step number:  40 total reward:  0.4099999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09303999999999987 0.6815 0.6815
actor:  1 policy actor:  1  step number:  31 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  19 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09295999999999988 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.09295999999999988 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
siam score:  -0.8519396
printing an ep nov before normalisation:  40.75785145665664
maxi score, test score, baseline:  0.09295999999999988 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
actor:  0 policy actor:  0  step number:  21 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  45.25611839551279
printing an ep nov before normalisation:  44.51341892130796
actor:  0 policy actor:  1  step number:  31 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.0
siam score:  -0.85285413
maxi score, test score, baseline:  0.09219999999999988 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
actor:  0 policy actor:  0  step number:  23 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.084]
 [0.162]
 [0.143]
 [0.078]
 [0.078]
 [0.143]
 [0.143]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.084]
 [0.162]
 [0.143]
 [0.078]
 [0.078]
 [0.143]
 [0.143]]
actor:  1 policy actor:  1  step number:  21 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  0.667
using another actor
printing an ep nov before normalisation:  0.012210736206460146
maxi score, test score, baseline:  0.09203999999999989 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
maxi score, test score, baseline:  0.09203999999999989 0.6815 0.6815
maxi score, test score, baseline:  0.09203999999999989 0.6815 0.6815
printing an ep nov before normalisation:  44.104637921606304
maxi score, test score, baseline:  0.08867999999999988 0.6815 0.6815
maxi score, test score, baseline:  0.08867999999999988 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
line 256 mcts: sample exp_bonus 0.0
actor:  1 policy actor:  1  step number:  20 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  39.10770110424091
actions average: 
K:  3  action  0 :  tensor([0.6683, 0.0227, 0.0426, 0.0906, 0.0842, 0.0418, 0.0498],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0035, 0.9701, 0.0050, 0.0039, 0.0027, 0.0051, 0.0096],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0711, 0.0047, 0.6118, 0.0906, 0.0881, 0.0841, 0.0495],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1443, 0.0104, 0.1329, 0.2711, 0.1734, 0.1438, 0.1240],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.1193,     0.0004,     0.0699,     0.0996,     0.5835,     0.0895,
            0.0379], grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.2162, 0.0069, 0.1407, 0.1583, 0.1810, 0.1381, 0.1589],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1015, 0.0445, 0.1314, 0.0819, 0.0666, 0.0912, 0.4829],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  56.077945960437766
actor:  0 policy actor:  1  step number:  31 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8464432
from probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
maxi score, test score, baseline:  0.08495999999999987 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.036]
 [0.01 ]
 [0.013]
 [0.013]
 [0.013]
 [0.051]] [[18.106]
 [26.845]
 [21.028]
 [18.337]
 [18.084]
 [18.154]
 [25.618]] [[0.258]
 [0.6  ]
 [0.361]
 [0.265]
 [0.256]
 [0.258]
 [0.57 ]]
printing an ep nov before normalisation:  27.269467617051703
actor:  0 policy actor:  0  step number:  28 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
using another actor
maxi score, test score, baseline:  0.08451999999999987 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
printing an ep nov before normalisation:  53.532324573365315
maxi score, test score, baseline:  0.08451999999999987 0.6815 0.6815
siam score:  -0.8465727
line 256 mcts: sample exp_bonus 52.770805757499126
maxi score, test score, baseline:  0.08451999999999987 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
actions average: 
K:  2  action  0 :  tensor([0.6712, 0.0278, 0.0335, 0.0505, 0.1079, 0.0348, 0.0743],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0062,     0.9786,     0.0039,     0.0019,     0.0009,     0.0007,
            0.0077], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0304, 0.0642, 0.6392, 0.0550, 0.0404, 0.1126, 0.0582],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0868, 0.1932, 0.0980, 0.2922, 0.0956, 0.0798, 0.1544],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1866, 0.0160, 0.0821, 0.1047, 0.3974, 0.0671, 0.1462],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0664, 0.0266, 0.2037, 0.0874, 0.0759, 0.4240, 0.1160],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1093, 0.1067, 0.1506, 0.1218, 0.1273, 0.1191, 0.2651],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.08451999999999987 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
printing an ep nov before normalisation:  30.865607261657715
actor:  1 policy actor:  1  step number:  38 total reward:  0.34999999999999976  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  20.915125758294636
actor:  1 policy actor:  1  step number:  37 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.08451999999999987 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
actor:  0 policy actor:  1  step number:  44 total reward:  0.2699999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.08375999999999988 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
printing an ep nov before normalisation:  71.12375691220396
maxi score, test score, baseline:  0.08375999999999988 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
printing an ep nov before normalisation:  56.044589549572294
actions average: 
K:  4  action  0 :  tensor([0.4784, 0.0230, 0.0741, 0.0712, 0.1439, 0.1185, 0.0909],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0175, 0.9237, 0.0119, 0.0087, 0.0057, 0.0061, 0.0263],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0674, 0.0959, 0.5753, 0.0720, 0.0637, 0.0767, 0.0490],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1432, 0.0140, 0.1355, 0.2771, 0.1467, 0.1462, 0.1373],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1834, 0.0022, 0.0996, 0.0848, 0.4500, 0.0901, 0.0899],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1380, 0.0007, 0.3429, 0.0953, 0.1096, 0.1499, 0.1637],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1114, 0.1184, 0.1161, 0.1195, 0.1171, 0.1247, 0.2927],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.262]
 [0.328]
 [0.262]
 [0.262]
 [0.262]
 [0.262]
 [0.262]] [[39.334]
 [50.092]
 [39.334]
 [39.334]
 [39.334]
 [39.334]
 [39.334]] [[1.136]
 [1.796]
 [1.136]
 [1.136]
 [1.136]
 [1.136]
 [1.136]]
actor:  0 policy actor:  1  step number:  28 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  32 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.08347999999999989 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
Printing some Q and Qe and total Qs values:  [[0.231]
 [0.272]
 [0.231]
 [0.231]
 [0.231]
 [0.231]
 [0.231]] [[40.798]
 [43.28 ]
 [40.798]
 [40.798]
 [40.798]
 [40.798]
 [40.798]] [[1.166]
 [1.29 ]
 [1.166]
 [1.166]
 [1.166]
 [1.166]
 [1.166]]
actor:  1 policy actor:  1  step number:  37 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  4  action  0 :  tensor([0.5837, 0.0035, 0.0686, 0.1268, 0.1137, 0.0642, 0.0395],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0118, 0.9309, 0.0210, 0.0090, 0.0052, 0.0037, 0.0184],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0871, 0.0020, 0.5106, 0.1059, 0.0736, 0.1030, 0.1179],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0492, 0.0120, 0.0613, 0.6193, 0.0451, 0.1138, 0.0992],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1472, 0.0024, 0.0830, 0.1180, 0.5323, 0.0717, 0.0454],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1806, 0.0249, 0.3027, 0.1233, 0.1368, 0.1257, 0.1060],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1425, 0.0341, 0.1036, 0.1619, 0.1380, 0.1317, 0.2882],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.08347999999999989 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 30.857719465279576
maxi score, test score, baseline:  0.08347999999999989 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
maxi score, test score, baseline:  0.08347999999999989 0.6815 0.6815
siam score:  -0.8437243
maxi score, test score, baseline:  0.08347999999999989 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
printing an ep nov before normalisation:  40.725154876708984
actor:  1 policy actor:  1  step number:  27 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  26 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  26 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.08307999999999988 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.08307999999999988 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
printing an ep nov before normalisation:  0.10331370809581131
actor:  1 policy actor:  1  step number:  37 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  31 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.08307999999999988 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
printing an ep nov before normalisation:  46.42708821407525
printing an ep nov before normalisation:  57.48052339162437
Printing some Q and Qe and total Qs values:  [[0.316]
 [0.373]
 [0.316]
 [0.316]
 [0.316]
 [0.316]
 [0.316]] [[49.707]
 [57.324]
 [49.707]
 [49.707]
 [49.707]
 [49.707]
 [49.707]] [[1.051]
 [1.265]
 [1.051]
 [1.051]
 [1.051]
 [1.051]
 [1.051]]
printing an ep nov before normalisation:  54.11409507900565
maxi score, test score, baseline:  0.08307999999999988 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
maxi score, test score, baseline:  0.08307999999999988 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
maxi score, test score, baseline:  0.08307999999999988 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
actor:  1 policy actor:  1  step number:  33 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.132]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.132]]
actor:  1 policy actor:  1  step number:  39 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.266]
 [0.267]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]] [[26.518]
 [35.622]
 [26.518]
 [26.518]
 [26.518]
 [26.518]
 [26.518]] [[0.998]
 [1.478]
 [0.998]
 [0.998]
 [0.998]
 [0.998]
 [0.998]]
printing an ep nov before normalisation:  47.97505658189943
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.08287999999999986 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
actor:  1 policy actor:  1  step number:  29 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.08287999999999986 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
actor:  1 policy actor:  1  step number:  32 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  56.372042268698856
printing an ep nov before normalisation:  43.31887330215762
actor:  1 policy actor:  1  step number:  24 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.08287999999999986 0.6815 0.6815
probs:  [0.05824387616589952, 0.05824387616589952, 0.2236478019564713, 0.05824387616589952, 0.20052539026426078, 0.4010951792815694]
actor:  1 policy actor:  1  step number:  19 total reward:  0.74  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.08287999999999986 0.6815 0.6815
probs:  [0.05359628832860963, 0.05359628832860963, 0.20577149818678825, 0.05359628832860963, 0.2644126913822232, 0.36902694544515957]
Printing some Q and Qe and total Qs values:  [[0.351]
 [0.351]
 [0.54 ]
 [0.351]
 [0.351]
 [0.351]
 [0.351]] [[54.624]
 [54.624]
 [53.974]
 [54.624]
 [54.624]
 [54.624]
 [54.624]] [[1.227]
 [1.227]
 [1.4  ]
 [1.227]
 [1.227]
 [1.227]
 [1.227]]
Printing some Q and Qe and total Qs values:  [[ 0.603]
 [ 0.617]
 [-0.062]
 [ 0.515]
 [ 0.517]
 [-0.054]
 [ 0.566]] [[58.397]
 [58.537]
 [64.46 ]
 [63.999]
 [63.245]
 [60.555]
 [61.02 ]] [[1.604]
 [1.622]
 [1.098]
 [1.663]
 [1.646]
 [1.003]
 [1.636]]
printing an ep nov before normalisation:  53.842287040629934
maxi score, test score, baseline:  0.08287999999999986 0.6815 0.6815
probs:  [0.05359628832860963, 0.05359628832860963, 0.20577149818678825, 0.05359628832860963, 0.2644126913822232, 0.36902694544515957]
printing an ep nov before normalisation:  46.92093384194437
actor:  1 policy actor:  1  step number:  35 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]] [[42.168]
 [42.168]
 [42.168]
 [42.168]
 [42.168]
 [42.168]
 [42.168]] [[1.581]
 [1.581]
 [1.581]
 [1.581]
 [1.581]
 [1.581]
 [1.581]]
maxi score, test score, baseline:  0.08287999999999986 0.6815 0.6815
probs:  [0.05359628832860963, 0.05359628832860963, 0.20577149818678825, 0.05359628832860963, 0.2644126913822232, 0.36902694544515957]
actor:  1 policy actor:  1  step number:  36 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  34 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  0  action  0 :  tensor([0.5296, 0.0016, 0.1070, 0.0848, 0.1247, 0.0755, 0.0767],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0016,     0.9601,     0.0141,     0.0111,     0.0003,     0.0013,
            0.0113], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0229,     0.0001,     0.8337,     0.0464,     0.0224,     0.0511,
            0.0234], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1193, 0.1319, 0.1250, 0.2438, 0.1215, 0.1230, 0.1355],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1595, 0.0024, 0.1361, 0.1109, 0.4019, 0.0912, 0.0980],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1195, 0.0037, 0.2502, 0.1661, 0.1613, 0.1719, 0.1272],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1115, 0.0329, 0.1250, 0.1698, 0.1261, 0.1119, 0.3227],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.08287999999999986 0.6815 0.6815
probs:  [0.05359628832860963, 0.05359628832860963, 0.20577149818678825, 0.05359628832860963, 0.2644126913822232, 0.36902694544515957]
printing an ep nov before normalisation:  48.191830555318745
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.08287999999999986 0.6815 0.6815
probs:  [0.05359628832860963, 0.05359628832860963, 0.20577149818678825, 0.05359628832860963, 0.2644126913822232, 0.36902694544515957]
printing an ep nov before normalisation:  0.07265535449676008
actor:  0 policy actor:  1  step number:  29 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.203]
 [0.276]
 [0.203]
 [0.203]
 [0.203]
 [0.203]
 [0.203]] [[41.052]
 [57.107]
 [41.052]
 [41.052]
 [41.052]
 [41.052]
 [41.052]] [[0.918]
 [1.578]
 [0.918]
 [0.918]
 [0.918]
 [0.918]
 [0.918]]
actor:  0 policy actor:  1  step number:  27 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.667
from probs:  [0.05359628832860963, 0.05359628832860963, 0.20577149818678825, 0.05359628832860963, 0.2644126913822232, 0.36902694544515957]
maxi score, test score, baseline:  0.08259999999999988 0.6815 0.6815
probs:  [0.05359628832860963, 0.05359628832860963, 0.20577149818678825, 0.05359628832860963, 0.2644126913822232, 0.36902694544515957]
printing an ep nov before normalisation:  27.197347322997125
Printing some Q and Qe and total Qs values:  [[0.731]
 [0.731]
 [0.88 ]
 [0.731]
 [0.731]
 [0.731]
 [0.731]] [[41.257]
 [41.257]
 [43.026]
 [41.257]
 [41.257]
 [41.257]
 [41.257]] [[0.731]
 [0.731]
 [0.88 ]
 [0.731]
 [0.731]
 [0.731]
 [0.731]]
Printing some Q and Qe and total Qs values:  [[0.745]
 [0.775]
 [0.716]
 [0.719]
 [0.74 ]
 [0.732]
 [0.731]] [[35.267]
 [38.712]
 [31.011]
 [31.563]
 [31.584]
 [30.619]
 [31.159]] [[1.986]
 [2.256]
 [1.66 ]
 [1.701]
 [1.724]
 [1.648]
 [1.685]]
printing an ep nov before normalisation:  0.0024871161394912633
actor:  1 policy actor:  1  step number:  45 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  30 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.07925999999999987 0.6815 0.6815
probs:  [0.05359628832860963, 0.05359628832860963, 0.20577149818678825, 0.05359628832860963, 0.2644126913822232, 0.36902694544515957]
maxi score, test score, baseline:  0.07925999999999987 0.6815 0.6815
probs:  [0.05359628832860963, 0.05359628832860963, 0.20577149818678825, 0.05359628832860963, 0.2644126913822232, 0.36902694544515957]
maxi score, test score, baseline:  0.07925999999999987 0.6815 0.6815
probs:  [0.05359628832860963, 0.05359628832860963, 0.20577149818678825, 0.05359628832860963, 0.2644126913822232, 0.36902694544515957]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]] [[40.672]
 [40.672]
 [40.672]
 [40.672]
 [40.672]
 [40.672]
 [40.672]] [[2.54]
 [2.54]
 [2.54]
 [2.54]
 [2.54]
 [2.54]
 [2.54]]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  58.97362411064243
printing an ep nov before normalisation:  72.36011334950715
actor:  1 policy actor:  1  step number:  34 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  49.65133510506034
actor:  1 policy actor:  1  step number:  22 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[ 0.576]
 [ 0.571]
 [-0.071]
 [ 0.382]
 [ 0.431]
 [-0.078]
 [ 0.454]] [[44.169]
 [43.628]
 [47.475]
 [48.215]
 [50.176]
 [45.463]
 [46.726]] [[1.935]
 [1.901]
 [1.462]
 [1.955]
 [2.107]
 [1.349]
 [1.948]]
maxi score, test score, baseline:  0.07925999999999987 0.6815 0.6815
probs:  [0.05359628832860963, 0.05359628832860963, 0.20577149818678825, 0.05359628832860963, 0.2644126913822232, 0.36902694544515957]
actor:  1 policy actor:  1  step number:  31 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  25 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.674]
 [0.672]
 [0.674]
 [0.674]
 [0.674]
 [0.674]
 [0.674]] [[39.317]
 [51.915]
 [39.317]
 [39.317]
 [39.317]
 [39.317]
 [39.317]] [[1.617]
 [2.19 ]
 [1.617]
 [1.617]
 [1.617]
 [1.617]
 [1.617]]
maxi score, test score, baseline:  0.07925999999999987 0.6815 0.6815
probs:  [0.05359628832860963, 0.05359628832860963, 0.20577149818678825, 0.05359628832860963, 0.2644126913822232, 0.36902694544515957]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  21 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  52.41012665356387
actor:  1 policy actor:  1  step number:  22 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  53.31412728734131
maxi score, test score, baseline:  0.07925999999999987 0.6815 0.6815
probs:  [0.05359628832860961, 0.05359628832860961, 0.20577149818678836, 0.05359628832860961, 0.2644126913822234, 0.36902694544515946]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.07925999999999987 0.6815 0.6815
probs:  [0.05359628832860961, 0.05359628832860961, 0.20577149818678836, 0.05359628832860961, 0.2644126913822234, 0.36902694544515946]
actor:  1 policy actor:  1  step number:  33 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  40.44815349386289
maxi score, test score, baseline:  0.07925999999999987 0.6815 0.6815
probs:  [0.05359628832860961, 0.05359628832860961, 0.20577149818678836, 0.05359628832860961, 0.2644126913822234, 0.36902694544515946]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  3  action  0 :  tensor([0.4977, 0.0459, 0.0915, 0.0842, 0.1151, 0.0743, 0.0913],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0014,     0.9792,     0.0049,     0.0031,     0.0004,     0.0013,
            0.0096], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0167, 0.0419, 0.7929, 0.0245, 0.0128, 0.0271, 0.0841],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1569, 0.0078, 0.1642, 0.1897, 0.1713, 0.1826, 0.1277],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1982, 0.0070, 0.0900, 0.1311, 0.3745, 0.0920, 0.1072],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1452, 0.0115, 0.1407, 0.1677, 0.1085, 0.3437, 0.0828],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1458, 0.1077, 0.1659, 0.1393, 0.1164, 0.1582, 0.1668],
       grad_fn=<DivBackward0>)
using another actor
printing an ep nov before normalisation:  57.40439496775091
actions average: 
K:  1  action  0 :  tensor([    0.6115,     0.0005,     0.0426,     0.0356,     0.1429,     0.0322,
            0.1347], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0040, 0.9495, 0.0034, 0.0174, 0.0014, 0.0019, 0.0224],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0349,     0.0008,     0.7951,     0.0573,     0.0360,     0.0424,
            0.0336], grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1223, 0.0359, 0.1147, 0.3069, 0.1535, 0.1056, 0.1611],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1217, 0.0059, 0.1326, 0.1706, 0.3297, 0.1089, 0.1305],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1493, 0.0014, 0.1572, 0.2056, 0.1389, 0.2081, 0.1397],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1141, 0.0729, 0.1210, 0.1288, 0.1180, 0.0948, 0.3503],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.913]
 [0.893]
 [0.913]
 [0.913]
 [0.913]
 [0.913]
 [0.913]] [[44.084]
 [53.528]
 [44.084]
 [44.084]
 [44.084]
 [44.084]
 [44.084]] [[0.913]
 [0.893]
 [0.913]
 [0.913]
 [0.913]
 [0.913]
 [0.913]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.232]
 [0.336]
 [0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]] [[41.205]
 [42.395]
 [41.205]
 [41.205]
 [41.205]
 [41.205]
 [41.205]] [[1.145]
 [1.287]
 [1.145]
 [1.145]
 [1.145]
 [1.145]
 [1.145]]
using another actor
from probs:  [0.05359628832860961, 0.05359628832860961, 0.20577149818678836, 0.05359628832860961, 0.2644126913822234, 0.36902694544515946]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  0 policy actor:  1  step number:  21 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.4  ]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]] [[62.473]
 [65.518]
 [62.473]
 [62.473]
 [62.473]
 [62.473]
 [62.473]] [[1.898]
 [1.985]
 [1.898]
 [1.898]
 [1.898]
 [1.898]
 [1.898]]
maxi score, test score, baseline:  0.07599999999999989 0.6815 0.6815
probs:  [0.05359628832860961, 0.05359628832860961, 0.20577149818678836, 0.05359628832860961, 0.2644126913822234, 0.36902694544515946]
maxi score, test score, baseline:  0.07599999999999989 0.6815 0.6815
probs:  [0.05359628832860961, 0.05359628832860961, 0.20577149818678836, 0.05359628832860961, 0.2644126913822234, 0.36902694544515946]
Printing some Q and Qe and total Qs values:  [[ 0.553]
 [ 0.638]
 [-0.085]
 [ 0.025]
 [ 0.556]
 [-0.094]
 [-0.016]] [[40.331]
 [40.054]
 [37.135]
 [37.772]
 [42.322]
 [37.83 ]
 [38.197]] [[1.824]
 [1.893]
 [0.99 ]
 [1.138]
 [1.95 ]
 [1.023]
 [1.124]]
printing an ep nov before normalisation:  31.671161651611328
printing an ep nov before normalisation:  49.3997558733163
maxi score, test score, baseline:  0.07599999999999989 0.6815 0.6815
probs:  [0.05359628832860961, 0.05359628832860961, 0.20577149818678836, 0.05359628832860961, 0.2644126913822234, 0.36902694544515946]
printing an ep nov before normalisation:  32.15402841567993
actor:  0 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  34 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  4  action  0 :  tensor([    0.7610,     0.0003,     0.0240,     0.0308,     0.1014,     0.0412,
            0.0412], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0167,     0.9494,     0.0209,     0.0011,     0.0003,     0.0040,
            0.0076], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0317, 0.0228, 0.7792, 0.0492, 0.0294, 0.0392, 0.0485],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0449, 0.0148, 0.0735, 0.6278, 0.0646, 0.1110, 0.0635],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1674, 0.0041, 0.0696, 0.1209, 0.4458, 0.0864, 0.1058],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0963, 0.2299, 0.1319, 0.1145, 0.0957, 0.1346, 0.1970],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1048, 0.1705, 0.0955, 0.1461, 0.1063, 0.1233, 0.2535],
       grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 0.0007648031862311199
actor:  1 policy actor:  1  step number:  32 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  57.455441734098805
maxi score, test score, baseline:  0.07581999999999987 0.6815 0.6815
probs:  [0.05359628832860961, 0.05359628832860961, 0.20577149818678836, 0.05359628832860961, 0.2644126913822234, 0.36902694544515946]
printing an ep nov before normalisation:  42.56596443035795
actor:  1 policy actor:  1  step number:  34 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  35 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.07581999999999987 0.6815 0.6815
probs:  [0.05359628832860961, 0.05359628832860961, 0.20577149818678836, 0.05359628832860961, 0.2644126913822234, 0.36902694544515946]
maxi score, test score, baseline:  0.07581999999999987 0.6815 0.6815
probs:  [0.05359628832860961, 0.05359628832860961, 0.20577149818678836, 0.05359628832860961, 0.2644126913822234, 0.36902694544515946]
actor:  0 policy actor:  0  step number:  20 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.07299999999999988 0.6815 0.6815
probs:  [0.05359628832860961, 0.05359628832860961, 0.20577149818678836, 0.05359628832860961, 0.2644126913822234, 0.36902694544515946]
actor:  1 policy actor:  1  step number:  30 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  43.327141153471615
UNIT TEST: sample policy line 217 mcts : [0.143 0.551 0.041 0.02  0.041 0.184 0.02 ]
printing an ep nov before normalisation:  56.94679003589915
maxi score, test score, baseline:  0.07061999999999988 0.6815 0.6815
probs:  [0.05359628832860961, 0.05359628832860961, 0.20577149818678836, 0.05359628832860961, 0.2644126913822234, 0.36902694544515946]
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.84214496612549
actor:  1 policy actor:  1  step number:  35 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  31 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.0
using another actor
from probs:  [0.05359628832860961, 0.05359628832860961, 0.20577149818678836, 0.05359628832860961, 0.2644126913822234, 0.36902694544515946]
actor:  1 policy actor:  1  step number:  35 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  56.95060594266195
maxi score, test score, baseline:  0.07061999999999988 0.6815 0.6815
actor:  1 policy actor:  1  step number:  19 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.07061999999999988 0.6815 0.6815
printing an ep nov before normalisation:  47.26977915057761
Printing some Q and Qe and total Qs values:  [[0.152]
 [0.199]
 [0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.152]] [[33.918]
 [43.295]
 [33.918]
 [33.918]
 [33.918]
 [33.918]
 [33.918]] [[0.506]
 [0.716]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]]
maxi score, test score, baseline:  0.07061999999999989 0.6815 0.6815
probs:  [0.05359628832860961, 0.05359628832860961, 0.20577149818678836, 0.05359628832860961, 0.2644126913822234, 0.36902694544515946]
Printing some Q and Qe and total Qs values:  [[0.619]
 [0.702]
 [0.636]
 [0.619]
 [0.619]
 [0.619]
 [0.575]] [[40.296]
 [41.085]
 [37.595]
 [40.296]
 [40.296]
 [40.296]
 [37.278]] [[1.909]
 [2.046]
 [1.738]
 [1.909]
 [1.909]
 [1.909]
 [1.655]]
maxi score, test score, baseline:  0.07061999999999989 0.6815 0.6815
probs:  [0.05359628832860961, 0.05359628832860961, 0.20577149818678836, 0.05359628832860961, 0.2644126913822234, 0.36902694544515946]
printing an ep nov before normalisation:  33.05774668998916
printing an ep nov before normalisation:  1.0063549563314211
actor:  1 policy actor:  1  step number:  28 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.07061999999999989 0.6815 0.6815
probs:  [0.05359628832860961, 0.05359628832860961, 0.20577149818678836, 0.05359628832860961, 0.2644126913822234, 0.36902694544515946]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.65 ]
 [0.65 ]
 [0.821]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]] [[40.815]
 [40.815]
 [44.735]
 [40.815]
 [40.815]
 [40.815]
 [40.815]] [[0.65 ]
 [0.65 ]
 [0.821]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]]
Printing some Q and Qe and total Qs values:  [[0.759]
 [0.033]
 [0.03 ]
 [0.759]
 [0.759]
 [0.759]
 [0.759]] [[41.033]
 [ 1.408]
 [ 1.045]
 [41.033]
 [41.033]
 [41.033]
 [41.033]] [[2.08 ]
 [0.045]
 [0.03 ]
 [2.08 ]
 [2.08 ]
 [2.08 ]
 [2.08 ]]
maxi score, test score, baseline:  0.07061999999999989 0.6815 0.6815
probs:  [0.05359628832860961, 0.05359628832860961, 0.20577149818678836, 0.05359628832860961, 0.2644126913822234, 0.36902694544515946]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3299999999999996  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.65 ]
 [0.688]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.65 ]
 [0.688]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]]
Printing some Q and Qe and total Qs values:  [[0.973]
 [0.995]
 [0.963]
 [0.963]
 [0.963]
 [0.963]
 [0.963]] [[46.884]
 [45.55 ]
 [51.782]
 [51.782]
 [51.782]
 [51.782]
 [51.782]] [[0.973]
 [0.995]
 [0.963]
 [0.963]
 [0.963]
 [0.963]
 [0.963]]
Printing some Q and Qe and total Qs values:  [[0.673]
 [0.77 ]
 [0.696]
 [0.673]
 [0.684]
 [0.673]
 [0.673]] [[ 0.   ]
 [37.139]
 [30.94 ]
 [ 0.   ]
 [30.382]
 [ 0.   ]
 [ 0.   ]] [[0.673]
 [0.77 ]
 [0.696]
 [0.673]
 [0.684]
 [0.673]
 [0.673]]
printing an ep nov before normalisation:  53.543234222264026
printing an ep nov before normalisation:  54.52859230711825
Printing some Q and Qe and total Qs values:  [[0.969]
 [0.957]
 [0.969]
 [0.969]
 [0.969]
 [0.969]
 [0.969]] [[57.957]
 [67.168]
 [57.957]
 [57.957]
 [57.957]
 [57.957]
 [57.957]] [[0.969]
 [0.957]
 [0.969]
 [0.969]
 [0.969]
 [0.969]
 [0.969]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.69 ]
 [0.808]
 [0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]] [[52.502]
 [49.989]
 [40.72 ]
 [40.72 ]
 [40.72 ]
 [40.72 ]
 [40.72 ]] [[0.69 ]
 [0.808]
 [0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]]
actor:  1 policy actor:  1  step number:  35 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.0
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.07061999999999989 0.6815 0.6815
probs:  [0.05359628832860961, 0.05359628832860961, 0.20577149818678836, 0.05359628832860961, 0.2644126913822234, 0.36902694544515946]
Printing some Q and Qe and total Qs values:  [[0.888]
 [0.919]
 [0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]] [[44.49 ]
 [50.352]
 [44.49 ]
 [44.49 ]
 [44.49 ]
 [44.49 ]
 [44.49 ]] [[0.888]
 [0.919]
 [0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]]
printing an ep nov before normalisation:  41.638856498351636
actor:  0 policy actor:  0  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.0
using another actor
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  56.67292628852147
maxi score, test score, baseline:  0.09759999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05335084193236492, 0.05285297908372049, 0.20298202750391647, 0.05285297908372049, 0.27304726191935086, 0.3649139104769269]
printing an ep nov before normalisation:  50.043706288873786
line 256 mcts: sample exp_bonus 46.438327922831256
printing an ep nov before normalisation:  47.89509718135315
maxi score, test score, baseline:  0.09759999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05335084193236492, 0.05285297908372049, 0.20298202750391647, 0.05285297908372049, 0.27304726191935086, 0.3649139104769269]
printing an ep nov before normalisation:  51.32704001412597
maxi score, test score, baseline:  0.09759999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05335084193236492, 0.05285297908372049, 0.20298202750391647, 0.05285297908372049, 0.27304726191935086, 0.3649139104769269]
Printing some Q and Qe and total Qs values:  [[0.191]
 [0.195]
 [0.193]
 [0.191]
 [0.191]
 [0.191]
 [0.146]] [[35.243]
 [41.04 ]
 [43.588]
 [35.243]
 [35.243]
 [35.243]
 [42.653]] [[0.508]
 [0.611]
 [0.652]
 [0.508]
 [0.508]
 [0.508]
 [0.589]]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09469999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05335084193236492, 0.05285297908372049, 0.20298202750391647, 0.05285297908372049, 0.27304726191935086, 0.3649139104769269]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]] [[64.838]
 [64.838]
 [64.838]
 [64.838]
 [64.838]
 [64.838]
 [64.838]] [[1.488]
 [1.488]
 [1.488]
 [1.488]
 [1.488]
 [1.488]
 [1.488]]
printing an ep nov before normalisation:  58.49707791338937
actor:  1 policy actor:  1  step number:  21 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  35.33954268103475
maxi score, test score, baseline:  0.09469999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05335084193236492, 0.05285297908372049, 0.20298202750391647, 0.05285297908372049, 0.27304726191935086, 0.3649139104769269]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09469999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05335084193236492, 0.05285297908372049, 0.20298202750391647, 0.05285297908372049, 0.27304726191935086, 0.3649139104769269]
actor:  1 policy actor:  1  step number:  31 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  48.07593439595737
maxi score, test score, baseline:  0.09469999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05335084193236492, 0.05285297908372049, 0.20298202750391647, 0.05285297908372049, 0.27304726191935086, 0.3649139104769269]
printing an ep nov before normalisation:  46.22813141852795
maxi score, test score, baseline:  0.09469999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05335084193236492, 0.05285297908372049, 0.20298202750391647, 0.05285297908372049, 0.27304726191935086, 0.3649139104769269]
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.499]
 [0.466]
 [0.361]
 [0.432]
 [0.466]
 [0.466]] [[42.597]
 [43.83 ]
 [38.235]
 [45.36 ]
 [45.763]
 [38.235]
 [38.235]] [[0.846]
 [0.914]
 [0.79 ]
 [0.801]
 [0.879]
 [0.79 ]
 [0.79 ]]
Printing some Q and Qe and total Qs values:  [[0.526]
 [0.562]
 [0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]] [[39.465]
 [50.16 ]
 [39.465]
 [39.465]
 [39.465]
 [39.465]
 [39.465]] [[0.904]
 [1.12 ]
 [0.904]
 [0.904]
 [0.904]
 [0.904]
 [0.904]]
actor:  1 policy actor:  1  step number:  35 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  31 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.09469999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05335084193236492, 0.05285297908372049, 0.20298202750391647, 0.05285297908372049, 0.27304726191935086, 0.3649139104769269]
printing an ep nov before normalisation:  69.8166591589521
maxi score, test score, baseline:  0.09469999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05335084193236492, 0.05285297908372049, 0.20298202750391647, 0.05285297908372049, 0.27304726191935086, 0.3649139104769269]
printing an ep nov before normalisation:  52.03740568510854
actor:  0 policy actor:  1  step number:  23 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09461999999999986 0.6759999999999999 0.6759999999999999
probs:  [0.05335084193236492, 0.05285297908372049, 0.20298202750391647, 0.05285297908372049, 0.27304726191935086, 0.3649139104769269]
maxi score, test score, baseline:  0.09461999999999986 0.6759999999999999 0.6759999999999999
actor:  1 policy actor:  1  step number:  25 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.57 ]
 [0.623]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.584]] [[27.434]
 [24.847]
 [25.678]
 [24.979]
 [24.861]
 [24.868]
 [25.321]] [[1.871]
 [1.801]
 [1.812]
 [1.779]
 [1.773]
 [1.774]
 [1.784]]
maxi score, test score, baseline:  0.09461999999999986 0.6759999999999999 0.6759999999999999
probs:  [0.05335084193236492, 0.05285297908372049, 0.20298202750391647, 0.05285297908372049, 0.27304726191935086, 0.3649139104769269]
maxi score, test score, baseline:  0.09461999999999986 0.6759999999999999 0.6759999999999999
probs:  [0.05335084193236492, 0.05285297908372049, 0.20298202750391647, 0.05285297908372049, 0.27304726191935086, 0.3649139104769269]
actor:  1 policy actor:  1  step number:  31 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.337]
 [0.396]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]] [[41.742]
 [54.645]
 [41.742]
 [41.742]
 [41.742]
 [41.742]
 [41.742]] [[1.469]
 [2.114]
 [1.469]
 [1.469]
 [1.469]
 [1.469]
 [1.469]]
maxi score, test score, baseline:  0.09461999999999986 0.6759999999999999 0.6759999999999999
probs:  [0.05335084193236492, 0.05285297908372049, 0.20298202750391647, 0.05285297908372049, 0.27304726191935086, 0.3649139104769269]
printing an ep nov before normalisation:  52.38720980989166
printing an ep nov before normalisation:  67.56521082564498
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8438823
actions average: 
K:  0  action  0 :  tensor([0.6191, 0.0034, 0.0574, 0.0407, 0.1789, 0.0516, 0.0490],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0035, 0.9735, 0.0065, 0.0032, 0.0021, 0.0021, 0.0092],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0717, 0.0113, 0.6543, 0.0523, 0.0738, 0.0689, 0.0676],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1456, 0.0025, 0.2389, 0.1604, 0.1614, 0.1663, 0.1248],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1527, 0.0006, 0.1265, 0.1043, 0.3909, 0.1205, 0.1046],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0988, 0.0007, 0.1600, 0.0675, 0.1200, 0.4324, 0.1206],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1328, 0.0849, 0.1247, 0.1501, 0.1398, 0.1036, 0.2641],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  25 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09461999999999986 0.6759999999999999 0.6759999999999999
probs:  [0.05335084193236492, 0.05285297908372049, 0.20298202750391647, 0.05285297908372049, 0.27304726191935086, 0.3649139104769269]
printing an ep nov before normalisation:  59.3647177732606
Printing some Q and Qe and total Qs values:  [[0.785]
 [0.85 ]
 [0.785]
 [0.785]
 [0.785]
 [0.785]
 [0.785]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.785]
 [0.85 ]
 [0.785]
 [0.785]
 [0.785]
 [0.785]
 [0.785]]
actor:  1 policy actor:  1  step number:  33 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09461999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05335084193236492, 0.05285297908372049, 0.20298202750391647, 0.05285297908372049, 0.27304726191935086, 0.3649139104769269]
Printing some Q and Qe and total Qs values:  [[0.772]
 [0.791]
 [0.772]
 [0.775]
 [0.75 ]
 [0.764]
 [0.772]] [[57.267]
 [56.114]
 [57.267]
 [57.752]
 [56.342]
 [56.437]
 [57.267]] [[1.715]
 [1.705]
 [1.715]
 [1.731]
 [1.67 ]
 [1.686]
 [1.715]]
printing an ep nov before normalisation:  44.1841464877768
printing an ep nov before normalisation:  35.63063384170064
printing an ep nov before normalisation:  29.514715671539307
actor:  1 policy actor:  1  step number:  27 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  1  step number:  21 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.09465999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05335084193236492, 0.05285297908372049, 0.20298202750391647, 0.05285297908372049, 0.27304726191935086, 0.3649139104769269]
actions average: 
K:  4  action  0 :  tensor([0.6476, 0.0024, 0.0472, 0.0651, 0.1003, 0.0546, 0.0829],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0066, 0.9591, 0.0061, 0.0088, 0.0048, 0.0041, 0.0105],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0959, 0.0191, 0.5672, 0.0708, 0.0674, 0.0739, 0.1057],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1560, 0.0133, 0.1225, 0.3107, 0.1256, 0.1261, 0.1459],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1554, 0.0036, 0.1068, 0.1169, 0.3665, 0.1016, 0.1492],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0148, 0.0233, 0.0931, 0.0341, 0.0025, 0.7971, 0.0350],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0937, 0.0070, 0.1023, 0.2878, 0.0806, 0.1205, 0.3081],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  55.9221253147388
maxi score, test score, baseline:  0.09465999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05335084193236492, 0.05285297908372049, 0.20298202750391647, 0.05285297908372049, 0.27304726191935086, 0.3649139104769269]
printing an ep nov before normalisation:  31.144467588717934
actor:  0 policy actor:  1  step number:  32 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09433999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05335084193236492, 0.05285297908372049, 0.20298202750391647, 0.05285297908372049, 0.27304726191935086, 0.3649139104769269]
maxi score, test score, baseline:  0.09433999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05335084193236492, 0.05285297908372049, 0.20298202750391647, 0.05285297908372049, 0.27304726191935086, 0.3649139104769269]
actor:  0 policy actor:  0  step number:  29 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.09443999999999989 0.6759999999999999 0.6759999999999999
actor:  1 policy actor:  1  step number:  42 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09443999999999989 0.6759999999999999 0.6759999999999999
probs:  [0.05335084193236492, 0.05285297908372049, 0.20298202750391647, 0.05285297908372049, 0.27304726191935086, 0.3649139104769269]
printing an ep nov before normalisation:  30.627546310424805
actor:  1 policy actor:  1  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.219]
 [0.245]
 [0.219]
 [0.219]
 [0.219]
 [0.219]
 [0.219]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.219]
 [0.245]
 [0.219]
 [0.219]
 [0.219]
 [0.219]
 [0.219]]
maxi score, test score, baseline:  0.09443999999999989 0.6759999999999999 0.6759999999999999
actor:  1 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.214]
 [0.293]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.214]
 [0.293]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]]
printing an ep nov before normalisation:  41.95203766879582
actions average: 
K:  0  action  0 :  tensor([0.5212, 0.0111, 0.0903, 0.0829, 0.1041, 0.1020, 0.0885],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0006,     0.9945,     0.0005,     0.0007,     0.0001,     0.0001,
            0.0035], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0325, 0.0056, 0.8358, 0.0285, 0.0237, 0.0438, 0.0300],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1453, 0.0157, 0.1374, 0.3120, 0.1087, 0.1222, 0.1588],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1199, 0.0370, 0.0769, 0.0759, 0.5234, 0.0774, 0.0896],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1250, 0.0031, 0.1599, 0.1114, 0.1005, 0.3350, 0.1651],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1017, 0.2211, 0.0973, 0.1182, 0.0910, 0.0895, 0.2813],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  1  step number:  29 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  2.0
from probs:  [0.05643911208082134, 0.05591235776516399, 0.214753541388719, 0.05591235776516399, 0.23090014142656667, 0.3860824895735649]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09451999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05643911208082134, 0.05591235776516399, 0.214753541388719, 0.05591235776516399, 0.23090014142656667, 0.3860824895735649]
Printing some Q and Qe and total Qs values:  [[0.215]
 [0.262]
 [0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.215]
 [0.262]
 [0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]]
actor:  1 policy actor:  1  step number:  35 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  0
maxi score, test score, baseline:  0.09451999999999988 0.6759999999999999 0.6759999999999999
maxi score, test score, baseline:  0.09451999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05643911208082134, 0.05591235776516399, 0.214753541388719, 0.05591235776516399, 0.23090014142656667, 0.3860824895735649]
maxi score, test score, baseline:  0.09451999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05643911208082134, 0.05591235776516399, 0.214753541388719, 0.05591235776516399, 0.23090014142656667, 0.3860824895735649]
maxi score, test score, baseline:  0.09451999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05643911208082134, 0.05591235776516399, 0.214753541388719, 0.05591235776516399, 0.23090014142656667, 0.3860824895735649]
Printing some Q and Qe and total Qs values:  [[0.187]
 [0.365]
 [0.191]
 [0.192]
 [0.187]
 [0.189]
 [0.177]] [[32.012]
 [44.012]
 [32.186]
 [33.3  ]
 [33.535]
 [33.68 ]
 [33.069]] [[0.69 ]
 [1.266]
 [0.699]
 [0.737]
 [0.74 ]
 [0.747]
 [0.714]]
actor:  0 policy actor:  1  step number:  32 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  50.986818928934746
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  39 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8383664
UNIT TEST: sample policy line 217 mcts : [0.    0.449 0.082 0.163 0.102 0.02  0.184]
Printing some Q and Qe and total Qs values:  [[0.363]
 [0.401]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]] [[47.531]
 [45.968]
 [47.531]
 [47.531]
 [47.531]
 [47.531]
 [47.531]] [[1.853]
 [1.817]
 [1.853]
 [1.853]
 [1.853]
 [1.853]
 [1.853]]
printing an ep nov before normalisation:  33.072492079372886
actor:  1 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.09423999999999988 0.6759999999999999 0.6759999999999999
maxi score, test score, baseline:  0.09423999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05643911208082134, 0.05591235776516399, 0.214753541388719, 0.05591235776516399, 0.23090014142656667, 0.3860824895735649]
maxi score, test score, baseline:  0.09423999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05643911208082134, 0.05591235776516399, 0.214753541388719, 0.05591235776516399, 0.23090014142656667, 0.3860824895735649]
actor:  1 policy actor:  1  step number:  36 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09423999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05643911208082134, 0.05591235776516399, 0.214753541388719, 0.05591235776516399, 0.23090014142656667, 0.3860824895735649]
actor:  1 policy actor:  1  step number:  33 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.09]
 [0.09]
 [0.09]
 [0.09]
 [0.09]
 [0.09]
 [0.09]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.09]
 [0.09]
 [0.09]
 [0.09]
 [0.09]
 [0.09]
 [0.09]]
printing an ep nov before normalisation:  43.7308932898177
maxi score, test score, baseline:  0.09423999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05643911208082134, 0.05591235776516399, 0.214753541388719, 0.05591235776516399, 0.23090014142656667, 0.3860824895735649]
maxi score, test score, baseline:  0.09423999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05643911208082134, 0.05591235776516399, 0.214753541388719, 0.05591235776516399, 0.23090014142656667, 0.3860824895735649]
printing an ep nov before normalisation:  38.289555578757636
actor:  1 policy actor:  1  step number:  21 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  23 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.29 ]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]] [[30.669]
 [35.681]
 [28.587]
 [28.587]
 [28.587]
 [28.587]
 [28.587]] [[0.441]
 [0.29 ]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.09423999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05643911208082134, 0.05591235776516399, 0.214753541388719, 0.05591235776516399, 0.23090014142656667, 0.3860824895735649]
maxi score, test score, baseline:  0.09103999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05643911208082134, 0.05591235776516399, 0.214753541388719, 0.05591235776516399, 0.23090014142656667, 0.3860824895735649]
Printing some Q and Qe and total Qs values:  [[0.646]
 [0.632]
 [0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.469]] [[40.322]
 [41.597]
 [37.023]
 [37.023]
 [37.023]
 [37.023]
 [37.023]] [[1.88 ]
 [1.948]
 [1.492]
 [1.492]
 [1.492]
 [1.492]
 [1.492]]
Printing some Q and Qe and total Qs values:  [[0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.724]
 [0.723]
 [0.703]] [[51.009]
 [51.009]
 [51.009]
 [51.009]
 [50.862]
 [51.984]
 [51.009]] [[1.572]
 [1.572]
 [1.572]
 [1.572]
 [1.588]
 [1.62 ]
 [1.572]]
maxi score, test score, baseline:  0.09103999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05643911208082134, 0.05591235776516399, 0.214753541388719, 0.05591235776516399, 0.23090014142656667, 0.3860824895735649]
actor:  1 policy actor:  1  step number:  25 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09103999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05643911208082134, 0.05591235776516399, 0.214753541388719, 0.05591235776516399, 0.23090014142656667, 0.3860824895735649]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09103999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05643911208082134, 0.05591235776516399, 0.214753541388719, 0.05591235776516399, 0.23090014142656667, 0.3860824895735649]
maxi score, test score, baseline:  0.09103999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05643911208082134, 0.05591235776516399, 0.214753541388719, 0.05591235776516399, 0.23090014142656667, 0.3860824895735649]
printing an ep nov before normalisation:  0.00012388052539336059
actor:  1 policy actor:  1  step number:  29 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09103999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05643911208082134, 0.05591235776516399, 0.214753541388719, 0.05591235776516399, 0.23090014142656667, 0.3860824895735649]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  26 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
siam score:  -0.83447695
maxi score, test score, baseline:  0.09103999999999987 0.6759999999999999 0.6759999999999999
maxi score, test score, baseline:  0.09103999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05643911208082134, 0.05591235776516399, 0.214753541388719, 0.05591235776516399, 0.23090014142656667, 0.3860824895735649]
actor:  1 policy actor:  1  step number:  32 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  23 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  0.333
from probs:  [0.05643747157889097, 0.05591245481286915, 0.21475391479735126, 0.05591245481286915, 0.23090054292797216, 0.3860831610700473]
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.671]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.652]
 [0.671]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09103999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05643747157889097, 0.05591245481286915, 0.21475391479735126, 0.05591245481286915, 0.23090054292797216, 0.3860831610700473]
Printing some Q and Qe and total Qs values:  [[0.656]
 [0.743]
 [0.717]
 [0.654]
 [0.654]
 [0.653]
 [0.689]] [[36.255]
 [34.592]
 [29.866]
 [36.404]
 [36.9  ]
 [37.845]
 [37.062]] [[1.602]
 [1.646]
 [1.496]
 [1.604]
 [1.617]
 [1.64 ]
 [1.656]]
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([0.7123, 0.0047, 0.0537, 0.0603, 0.0830, 0.0460, 0.0400],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0046, 0.9696, 0.0062, 0.0037, 0.0017, 0.0023, 0.0119],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0000,     0.9461,     0.0002,     0.0001,     0.0530,
            0.0006], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1164, 0.0065, 0.1377, 0.3490, 0.1080, 0.1310, 0.1514],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1700, 0.0031, 0.0617, 0.0561, 0.5577, 0.0841, 0.0673],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1660, 0.0063, 0.1227, 0.1927, 0.1469, 0.2480, 0.1174],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0557, 0.2707, 0.0300, 0.0285, 0.0236, 0.0314, 0.5599],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  46.99282821160584
printing an ep nov before normalisation:  44.67584594483657
actor:  1 policy actor:  1  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.09103999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05643747157889097, 0.05591245481286915, 0.21475391479735126, 0.05591245481286915, 0.23090054292797216, 0.3860831610700473]
actor:  0 policy actor:  0  step number:  23 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  33 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09407999999999986 0.6759999999999999 0.6759999999999999
probs:  [0.05643747157889097, 0.05591245481286915, 0.21475391479735126, 0.05591245481286915, 0.23090054292797216, 0.3860831610700473]
printing an ep nov before normalisation:  52.807523605709996
maxi score, test score, baseline:  0.09407999999999986 0.6759999999999999 0.6759999999999999
probs:  [0.05643747157889097, 0.05591245481286915, 0.21475391479735126, 0.05591245481286915, 0.23090054292797216, 0.3860831610700473]
Printing some Q and Qe and total Qs values:  [[0.826]
 [0.807]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]] [[48.074]
 [51.258]
 [48.074]
 [48.074]
 [48.074]
 [48.074]
 [48.074]] [[0.826]
 [0.807]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]]
printing an ep nov before normalisation:  36.27589400967147
actor:  1 policy actor:  1  step number:  28 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  24 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  37.591614723205566
from probs:  [0.05643747157889097, 0.05591245481286915, 0.21475391479735126, 0.05591245481286915, 0.23090054292797216, 0.3860831610700473]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
Printing some Q and Qe and total Qs values:  [[0.072]
 [0.076]
 [0.19 ]
 [0.276]
 [0.281]
 [0.127]
 [0.355]] [[35.851]
 [19.454]
 [19.887]
 [16.732]
 [16.895]
 [23.424]
 [31.901]] [[0.072]
 [0.076]
 [0.19 ]
 [0.276]
 [0.281]
 [0.127]
 [0.355]]
maxi score, test score, baseline:  0.09397999999999987 0.6759999999999999 0.6759999999999999
maxi score, test score, baseline:  0.09397999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05643747157889097, 0.05591245481286915, 0.21475391479735126, 0.05591245481286915, 0.23090054292797216, 0.3860831610700473]
printing an ep nov before normalisation:  44.394101337253495
actor:  0 policy actor:  0  step number:  27 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  22 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.09363999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05643747157889097, 0.05591245481286915, 0.21475391479735126, 0.05591245481286915, 0.23090054292797216, 0.3860831610700473]
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.233]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]] [[39.267]
 [47.877]
 [39.267]
 [39.267]
 [39.267]
 [39.267]
 [39.267]] [[1.185]
 [1.445]
 [1.185]
 [1.185]
 [1.185]
 [1.185]
 [1.185]]
maxi score, test score, baseline:  0.09363999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05643747157889097, 0.05591245481286915, 0.21475391479735126, 0.05591245481286915, 0.23090054292797216, 0.3860831610700473]
actor:  1 policy actor:  1  step number:  36 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  50.14839503260538
maxi score, test score, baseline:  0.09363999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05643747157889097, 0.05591245481286915, 0.21475391479735126, 0.05591245481286915, 0.23090054292797216, 0.3860831610700473]
actor:  1 policy actor:  1  step number:  24 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09363999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05643747157889097, 0.05591245481286915, 0.21475391479735126, 0.05591245481286915, 0.23090054292797216, 0.3860831610700473]
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.486]
 [0.525]
 [0.525]] [[42.72 ]
 [42.72 ]
 [42.72 ]
 [42.72 ]
 [45.346]
 [42.72 ]
 [42.72 ]] [[2.119]
 [2.119]
 [2.119]
 [2.119]
 [2.256]
 [2.119]
 [2.119]]
Printing some Q and Qe and total Qs values:  [[0.379]
 [0.452]
 [0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]] [[39.406]
 [33.639]
 [39.406]
 [39.406]
 [39.406]
 [39.406]
 [39.406]] [[1.215]
 [1.098]
 [1.215]
 [1.215]
 [1.215]
 [1.215]
 [1.215]]
maxi score, test score, baseline:  0.09363999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05643747157889097, 0.05591245481286915, 0.21475391479735126, 0.05591245481286915, 0.23090054292797216, 0.3860831610700473]
printing an ep nov before normalisation:  57.90919763018881
printing an ep nov before normalisation:  43.25156182730969
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  21 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  38 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  35 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  45.19197621721318
actor:  1 policy actor:  1  step number:  29 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.09409999999999989 0.6759999999999999 0.6759999999999999
probs:  [0.05643747157889097, 0.05591245481286915, 0.21475391479735126, 0.05591245481286915, 0.23090054292797216, 0.3860831610700473]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
using another actor
from probs:  [0.05643584186408423, 0.05591255122243695, 0.2147542857506342, 0.05591255122243695, 0.23090094178930431, 0.3860838281511033]
maxi score, test score, baseline:  0.09409999999999989 0.6759999999999999 0.6759999999999999
printing an ep nov before normalisation:  38.46813559337043
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 52.82248437504098
actor:  1 policy actor:  1  step number:  37 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  1.0
using another actor
using another actor
siam score:  -0.84222966
maxi score, test score, baseline:  0.09409999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05643584186408423, 0.05591255122243695, 0.2147542857506342, 0.05591255122243695, 0.23090094178930431, 0.3860838281511033]
printing an ep nov before normalisation:  59.06678241603709
printing an ep nov before normalisation:  28.59063351813266
actor:  0 policy actor:  1  step number:  20 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.09429999999999988 0.6759999999999999 0.6759999999999999
printing an ep nov before normalisation:  35.69256544113159
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  28 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09437999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05643584186408423, 0.05591255122243695, 0.2147542857506342, 0.05591255122243695, 0.23090094178930431, 0.3860838281511033]
actor:  1 policy actor:  1  step number:  37 total reward:  0.0799999999999994  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.09437999999999988 0.6759999999999999 0.6759999999999999
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09437999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.056434222830353584, 0.055912647000140835, 0.21475465427270643, 0.055912647000140835, 0.23090133803651766, 0.3860844908601405]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09437999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.056434222830353584, 0.055912647000140835, 0.21475465427270643, 0.055912647000140835, 0.23090133803651766, 0.3860844908601405]
actor:  1 policy actor:  1  step number:  33 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  33 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09437999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.056434222830353584, 0.055912647000140835, 0.21475465427270643, 0.055912647000140835, 0.23090133803651766, 0.3860844908601405]
printing an ep nov before normalisation:  29.702695210774742
maxi score, test score, baseline:  0.09437999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.056434222830353584, 0.055912647000140835, 0.21475465427270643, 0.055912647000140835, 0.23090133803651766, 0.3860844908601405]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.485]
 [0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.326]
 [0.485]
 [0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]]
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[ 0.104]
 [ 0.343]
 [ 0.289]
 [ 0.208]
 [-0.085]
 [ 0.266]
 [ 0.366]] [[31.895]
 [32.01 ]
 [36.98 ]
 [31.802]
 [33.82 ]
 [30.952]
 [32.959]] [[0.949]
 [1.193]
 [1.406]
 [1.047]
 [0.862]
 [1.06 ]
 [1.268]]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09437999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.056434222830353584, 0.055912647000140835, 0.21475465427270643, 0.055912647000140835, 0.23090133803651766, 0.3860844908601405]
Printing some Q and Qe and total Qs values:  [[ 0.]
 [-0.]
 [ 0.]
 [ 0.]
 [ 0.]
 [-0.]
 [ 0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.]
 [-0.]
 [ 0.]
 [ 0.]
 [ 0.]
 [-0.]
 [ 0.]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  47.953109056568145
maxi score, test score, baseline:  0.09437999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.056434222830353584, 0.055912647000140835, 0.21475465427270643, 0.055912647000140835, 0.23090133803651766, 0.3860844908601405]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  0
using explorer policy with actor:  0
printing an ep nov before normalisation:  52.70322449696557
actor:  1 policy actor:  1  step number:  25 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.336]
 [0.38 ]
 [0.336]
 [0.336]
 [0.336]
 [0.263]] [[56.565]
 [56.565]
 [56.26 ]
 [56.565]
 [56.565]
 [56.565]
 [58.802]] [[1.694]
 [1.694]
 [1.728]
 [1.694]
 [1.694]
 [1.694]
 [1.7  ]]
maxi score, test score, baseline:  0.09437999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.056434222830353584, 0.055912647000140835, 0.21475465427270643, 0.055912647000140835, 0.23090133803651766, 0.3860844908601405]
Printing some Q and Qe and total Qs values:  [[0.657]
 [0.592]
 [0.592]
 [0.592]
 [0.631]
 [0.592]
 [0.592]] [[47.452]
 [53.324]
 [53.324]
 [53.324]
 [50.09 ]
 [53.324]
 [53.324]] [[1.45 ]
 [1.525]
 [1.525]
 [1.525]
 [1.487]
 [1.525]
 [1.525]]
printing an ep nov before normalisation:  41.947269558944
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  25 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  0
maxi score, test score, baseline:  0.09753999999999989 0.6759999999999999 0.6759999999999999
probs:  [0.056434222830353584, 0.055912647000140835, 0.21475465427270643, 0.055912647000140835, 0.23090133803651766, 0.3860844908601405]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  29 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  1  step number:  31 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.09741999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.056434222830353584, 0.055912647000140835, 0.21475465427270643, 0.055912647000140835, 0.23090133803651766, 0.3860844908601405]
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.726]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]] [[43.031]
 [41.362]
 [43.031]
 [43.031]
 [43.031]
 [43.031]
 [43.031]] [[1.346]
 [1.44 ]
 [1.346]
 [1.346]
 [1.346]
 [1.346]
 [1.346]]
line 256 mcts: sample exp_bonus 37.28539705276489
Printing some Q and Qe and total Qs values:  [[0.112]
 [0.183]
 [0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.112]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.112]
 [0.183]
 [0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.112]]
printing an ep nov before normalisation:  37.07308657639835
maxi score, test score, baseline:  0.09741999999999988 0.6759999999999999 0.6759999999999999
actor:  1 policy actor:  1  step number:  45 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  40 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  43.31976267989642
maxi score, test score, baseline:  0.09741999999999988 0.6759999999999999 0.6759999999999999
maxi score, test score, baseline:  0.09741999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.056434222830353584, 0.055912647000140835, 0.21475465427270643, 0.055912647000140835, 0.23090133803651766, 0.3860844908601405]
Printing some Q and Qe and total Qs values:  [[0.825]
 [0.831]
 [0.825]
 [0.825]
 [0.825]
 [0.825]
 [0.825]] [[48.762]
 [52.729]
 [48.762]
 [48.762]
 [48.762]
 [48.762]
 [48.762]] [[2.363]
 [2.558]
 [2.363]
 [2.363]
 [2.363]
 [2.363]
 [2.363]]
actor:  1 policy actor:  1  step number:  35 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  25 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.063]
 [0.053]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.063]
 [0.053]
 [0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]]
actor:  1 policy actor:  1  step number:  35 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09741999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.056434222830353584, 0.055912647000140835, 0.21475465427270643, 0.055912647000140835, 0.23090133803651766, 0.3860844908601405]
maxi score, test score, baseline:  0.09741999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.056434222830353584, 0.055912647000140835, 0.21475465427270643, 0.055912647000140835, 0.23090133803651766, 0.3860844908601405]
printing an ep nov before normalisation:  39.902705336280576
maxi score, test score, baseline:  0.09741999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.056434222830353584, 0.055912647000140835, 0.21475465427270643, 0.055912647000140835, 0.23090133803651766, 0.3860844908601405]
printing an ep nov before normalisation:  36.05179339724266
actor:  0 policy actor:  1  step number:  28 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09727999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.056434222830353584, 0.055912647000140835, 0.21475465427270643, 0.055912647000140835, 0.23090133803651766, 0.3860844908601405]
printing an ep nov before normalisation:  48.137498667511
printing an ep nov before normalisation:  51.83763981358421
maxi score, test score, baseline:  0.09727999999999988 0.6759999999999999 0.6759999999999999
maxi score, test score, baseline:  0.09727999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.056434222830353584, 0.055912647000140835, 0.21475465427270643, 0.055912647000140835, 0.23090133803651766, 0.3860844908601405]
maxi score, test score, baseline:  0.09727999999999988 0.6759999999999999 0.6759999999999999
actor:  1 policy actor:  1  step number:  33 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  31 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  56.83134167674176
printing an ep nov before normalisation:  23.808847533331978
printing an ep nov before normalisation:  45.82317213234082
siam score:  -0.8405632
maxi score, test score, baseline:  0.09727999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.056434222830353584, 0.055912647000140835, 0.21475465427270643, 0.055912647000140835, 0.23090133803651766, 0.3860844908601405]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.074]
 [0.149]
 [0.068]
 [0.078]
 [0.081]
 [0.081]
 [0.08 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.074]
 [0.149]
 [0.068]
 [0.078]
 [0.081]
 [0.081]
 [0.08 ]]
maxi score, test score, baseline:  0.09727999999999988 0.6759999999999999 0.6759999999999999
maxi score, test score, baseline:  0.09727999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.056434222830353584, 0.055912647000140835, 0.21475465427270643, 0.055912647000140835, 0.23090133803651766, 0.3860844908601405]
maxi score, test score, baseline:  0.09727999999999988 0.6759999999999999 0.6759999999999999
actor:  1 policy actor:  1  step number:  39 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  32 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  38.97144133940249
maxi score, test score, baseline:  0.09727999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.056434222830353584, 0.055912647000140835, 0.21475465427270643, 0.055912647000140835, 0.23090133803651766, 0.3860844908601405]
maxi score, test score, baseline:  0.09727999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.056434222830353584, 0.055912647000140835, 0.21475465427270643, 0.055912647000140835, 0.23090133803651766, 0.3860844908601405]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  1  step number:  34 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09743999999999989 0.6759999999999999 0.6759999999999999
probs:  [0.056434222830353584, 0.055912647000140835, 0.21475465427270643, 0.055912647000140835, 0.23090133803651766, 0.3860844908601405]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09743999999999989 0.6759999999999999 0.6759999999999999
actor:  1 policy actor:  1  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  39.627668033996564
maxi score, test score, baseline:  0.09743999999999989 0.6759999999999999 0.6759999999999999
probs:  [0.056434222830353584, 0.055912647000140835, 0.21475465427270643, 0.055912647000140835, 0.23090133803651766, 0.3860844908601405]
maxi score, test score, baseline:  0.09743999999999989 0.6759999999999999 0.6759999999999999
probs:  [0.056434222830353584, 0.055912647000140835, 0.21475465427270643, 0.055912647000140835, 0.23090133803651766, 0.3860844908601405]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  32.52768039703369
printing an ep nov before normalisation:  36.56016826629639
actor:  1 policy actor:  1  step number:  33 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  29 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09743999999999989 0.6759999999999999 0.6759999999999999
probs:  [0.056434222830353584, 0.055912647000140835, 0.21475465427270643, 0.055912647000140835, 0.23090133803651766, 0.3860844908601405]
printing an ep nov before normalisation:  52.87827897453707
actor:  1 policy actor:  1  step number:  22 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  24 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
using another actor
printing an ep nov before normalisation:  56.62587755851098
actions average: 
K:  2  action  0 :  tensor([0.7183, 0.0193, 0.0458, 0.0438, 0.0732, 0.0510, 0.0486],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0016,     0.9631,     0.0015,     0.0070,     0.0003,     0.0004,
            0.0261], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0123, 0.0425, 0.8841, 0.0154, 0.0129, 0.0193, 0.0136],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1261, 0.1175, 0.1528, 0.2407, 0.1168, 0.1496, 0.0965],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1068, 0.0065, 0.1210, 0.1142, 0.4594, 0.0852, 0.1069],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1290, 0.0080, 0.1435, 0.1124, 0.1092, 0.3896, 0.1082],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1044, 0.2056, 0.0666, 0.0774, 0.0635, 0.0600, 0.4225],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  72.30774593257469
Printing some Q and Qe and total Qs values:  [[0.296]
 [0.278]
 [0.296]
 [0.296]
 [0.296]
 [0.296]
 [0.296]] [[42.865]
 [51.356]
 [42.865]
 [42.865]
 [42.865]
 [42.865]
 [42.865]] [[1.139]
 [1.425]
 [1.139]
 [1.139]
 [1.139]
 [1.139]
 [1.139]]
actor:  0 policy actor:  1  step number:  23 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.09751999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05643261437303701, 0.0559127421521724, 0.21475502038739083, 0.0559127421521724, 0.2309017316952273, 0.38608514924]
actor:  0 policy actor:  1  step number:  24 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10045999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05643261437303701, 0.0559127421521724, 0.21475502038739083, 0.0559127421521724, 0.2309017316952273, 0.38608514924]
printing an ep nov before normalisation:  25.023098074517705
maxi score, test score, baseline:  0.10045999999999987 0.6759999999999999 0.6759999999999999
maxi score, test score, baseline:  0.10045999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05643261437303701, 0.0559127421521724, 0.21475502038739083, 0.0559127421521724, 0.2309017316952273, 0.38608514924]
actor:  1 policy actor:  1  step number:  37 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  0.19576997742248636
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([0.6250, 0.0217, 0.0637, 0.0708, 0.0879, 0.0622, 0.0686],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0018,     0.9839,     0.0022,     0.0030,     0.0005,     0.0006,
            0.0079], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0262, 0.0027, 0.8295, 0.0329, 0.0330, 0.0444, 0.0313],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0973, 0.1027, 0.1223, 0.2350, 0.1100, 0.1407, 0.1920],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1633, 0.0261, 0.0693, 0.1120, 0.4724, 0.0773, 0.0797],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1120, 0.0301, 0.1534, 0.1488, 0.1283, 0.3063, 0.1211],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0621, 0.2082, 0.0927, 0.1387, 0.0649, 0.0827, 0.3507],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.452]
 [0.349]
 [0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]] [[54.665]
 [60.224]
 [61.63 ]
 [61.63 ]
 [61.63 ]
 [61.63 ]
 [61.63 ]] [[2.121]
 [2.275]
 [2.14 ]
 [2.14 ]
 [2.14 ]
 [2.14 ]
 [2.14 ]]
printing an ep nov before normalisation:  0.0018671175575946108
actor:  1 policy actor:  1  step number:  32 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  51.22555945955029
printing an ep nov before normalisation:  24.826524848636918
Printing some Q and Qe and total Qs values:  [[0.906]
 [0.022]
 [0.906]
 [0.906]
 [0.906]
 [0.906]
 [0.906]] [[31.593]
 [ 0.956]
 [31.593]
 [31.593]
 [31.593]
 [31.593]
 [31.593]] [[2.861]
 [0.071]
 [2.861]
 [2.861]
 [2.861]
 [2.861]
 [2.861]]
actor:  1 policy actor:  1  step number:  31 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10045999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05643261437303701, 0.0559127421521724, 0.21475502038739083, 0.0559127421521724, 0.2309017316952273, 0.38608514924]
maxi score, test score, baseline:  0.10045999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05643261437303701, 0.0559127421521724, 0.21475502038739083, 0.0559127421521724, 0.2309017316952273, 0.38608514924]
actor:  0 policy actor:  0  step number:  27 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  0.050188688085484046
actor:  1 policy actor:  1  step number:  21 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  44 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10337999999999989 0.6759999999999999 0.6759999999999999
printing an ep nov before normalisation:  34.55693006515503
actor:  0 policy actor:  0  step number:  24 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  1  step number:  31 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10627999999999989 0.6759999999999999 0.6759999999999999
probs:  [0.05643261437303701, 0.0559127421521724, 0.21475502038739083, 0.0559127421521724, 0.2309017316952273, 0.38608514924]
printing an ep nov before normalisation:  36.12386728233765
actor:  1 policy actor:  1  step number:  29 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10627999999999989 0.6759999999999999 0.6759999999999999
probs:  [0.05643261437303701, 0.0559127421521724, 0.21475502038739083, 0.0559127421521724, 0.2309017316952273, 0.38608514924]
printing an ep nov before normalisation:  35.32003402709961
maxi score, test score, baseline:  0.10627999999999989 0.6759999999999999 0.6759999999999999
probs:  [0.05643261437303701, 0.0559127421521724, 0.21475502038739083, 0.0559127421521724, 0.2309017316952273, 0.38608514924]
Printing some Q and Qe and total Qs values:  [[0.737]
 [0.904]
 [0.776]
 [0.737]
 [0.737]
 [0.737]
 [0.737]] [[30.229]
 [27.894]
 [33.205]
 [30.229]
 [30.229]
 [30.229]
 [30.229]] [[0.737]
 [0.904]
 [0.776]
 [0.737]
 [0.737]
 [0.737]
 [0.737]]
maxi score, test score, baseline:  0.10627999999999989 0.6759999999999999 0.6759999999999999
probs:  [0.05643261437303701, 0.0559127421521724, 0.21475502038739083, 0.0559127421521724, 0.2309017316952273, 0.38608514924]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10627999999999989 0.6759999999999999 0.6759999999999999
probs:  [0.05643261437303701, 0.0559127421521724, 0.21475502038739083, 0.0559127421521724, 0.2309017316952273, 0.38608514924]
actor:  0 policy actor:  0  step number:  26 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.191]
 [0.262]
 [0.164]
 [0.164]
 [0.168]
 [0.185]
 [0.156]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.191]
 [0.262]
 [0.164]
 [0.164]
 [0.168]
 [0.185]
 [0.156]]
maxi score, test score, baseline:  0.10691999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05643261437303701, 0.0559127421521724, 0.21475502038739083, 0.0559127421521724, 0.2309017316952273, 0.38608514924]
printing an ep nov before normalisation:  47.01719905088466
maxi score, test score, baseline:  0.10691999999999988 0.6759999999999999 0.6759999999999999
actor:  1 policy actor:  1  step number:  28 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  0.02191372455598639
actor:  1 policy actor:  1  step number:  30 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10691999999999988 0.6759999999999999 0.6759999999999999
actor:  1 policy actor:  1  step number:  38 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.10691999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05643261437303701, 0.0559127421521724, 0.21475502038739083, 0.0559127421521724, 0.2309017316952273, 0.38608514924]
printing an ep nov before normalisation:  54.81922285954613
actor:  1 policy actor:  1  step number:  26 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.372]
 [0.359]
 [0.378]
 [0.374]
 [0.364]
 [0.381]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.364]
 [0.372]
 [0.359]
 [0.378]
 [0.374]
 [0.364]
 [0.381]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  1.667
from probs:  [0.056431016388835735, 0.05591283668464245, 0.2147553841182003, 0.05591283668464245, 0.23090212279071523, 0.3860858033329639]
maxi score, test score, baseline:  0.10347999999999989 0.6759999999999999 0.6759999999999999
probs:  [0.056431016388835735, 0.05591283668464245, 0.2147553841182003, 0.05591283668464245, 0.23090212279071523, 0.3860858033329639]
actor:  1 policy actor:  1  step number:  32 total reward:  0.4299999999999998  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  44.79546689042186
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  30 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10035999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.056431016388835735, 0.05591283668464245, 0.2147553841182003, 0.05591283668464245, 0.23090212279071523, 0.3860858033329639]
maxi score, test score, baseline:  0.10035999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.056431016388835735, 0.05591283668464245, 0.2147553841182003, 0.05591283668464245, 0.23090212279071523, 0.3860858033329639]
using explorer policy with actor:  1
using explorer policy with actor:  1
from probs:  [0.056431016388835735, 0.05591283668464245, 0.2147553841182003, 0.05591283668464245, 0.23090212279071523, 0.3860858033329639]
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  41.729390188082164
maxi score, test score, baseline:  0.10035999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.056431016388835735, 0.05591283668464245, 0.2147553841182003, 0.05591283668464245, 0.23090212279071523, 0.3860858033329639]
actor:  1 policy actor:  1  step number:  38 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  41.341071788500635
maxi score, test score, baseline:  0.10035999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.056431016388835735, 0.05591283668464245, 0.2147553841182003, 0.05591283668464245, 0.23090212279071523, 0.3860858033329639]
actor:  1 policy actor:  1  step number:  30 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.10323999999999989 0.6759999999999999 0.6759999999999999
probs:  [0.056431016388835735, 0.05591283668464245, 0.2147553841182003, 0.05591283668464245, 0.23090212279071523, 0.3860858033329639]
maxi score, test score, baseline:  0.10323999999999989 0.6759999999999999 0.6759999999999999
probs:  [0.056431016388835735, 0.05591283668464245, 0.2147553841182003, 0.05591283668464245, 0.23090212279071523, 0.3860858033329639]
actor:  1 policy actor:  1  step number:  24 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  2.0
from probs:  [0.056431016388835735, 0.05591283668464245, 0.2147553841182003, 0.05591283668464245, 0.23090212279071523, 0.3860858033329639]
printing an ep nov before normalisation:  39.203290939331055
maxi score, test score, baseline:  0.10323999999999989 0.6759999999999999 0.6759999999999999
probs:  [0.056431016388835735, 0.05591283668464245, 0.2147553841182003, 0.05591283668464245, 0.23090212279071523, 0.3860858033329639]
actor:  0 policy actor:  1  step number:  30 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.0
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  35.92565059661865
Printing some Q and Qe and total Qs values:  [[0.061]
 [0.191]
 [0.061]
 [0.061]
 [0.061]
 [0.061]
 [0.061]] [[15.152]
 [24.685]
 [15.152]
 [15.152]
 [15.152]
 [15.152]
 [15.152]] [[0.135]
 [0.485]
 [0.135]
 [0.135]
 [0.135]
 [0.135]
 [0.135]]
actor:  0 policy actor:  0  step number:  19 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  36 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  2.0
from probs:  [0.056431016388835735, 0.05591283668464245, 0.2147553841182003, 0.05591283668464245, 0.23090212279071523, 0.3860858033329639]
maxi score, test score, baseline:  0.10341999999999989 0.6759999999999999 0.6759999999999999
probs:  [0.056431016388835735, 0.05591283668464245, 0.2147553841182003, 0.05591283668464245, 0.23090212279071523, 0.3860858033329639]
siam score:  -0.82976156
actor:  1 policy actor:  1  step number:  33 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  29 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.10349999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.056431016388835735, 0.05591283668464245, 0.2147553841182003, 0.05591283668464245, 0.23090212279071523, 0.3860858033329639]
printing an ep nov before normalisation:  56.475142230019635
actions average: 
K:  0  action  0 :  tensor([0.4517, 0.0034, 0.0941, 0.1112, 0.1518, 0.0913, 0.0963],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0015,     0.9808,     0.0007,     0.0036,     0.0004,     0.0006,
            0.0124], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0367, 0.0021, 0.8316, 0.0255, 0.0276, 0.0446, 0.0318],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1599, 0.0111, 0.1432, 0.2388, 0.1444, 0.1513, 0.1513],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2256, 0.0134, 0.0862, 0.1014, 0.3393, 0.1043, 0.1298],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0903, 0.0005, 0.1644, 0.1246, 0.0896, 0.4260, 0.1046],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1285, 0.0839, 0.1110, 0.1111, 0.1007, 0.1274, 0.3374],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.10349999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.056431016388835735, 0.05591283668464245, 0.2147553841182003, 0.05591283668464245, 0.23090212279071523, 0.3860858033329639]
actor:  1 policy actor:  1  step number:  31 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  47.818094564504555
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  34 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.137]
 [0.137]
 [0.025]
 [0.137]
 [0.137]
 [0.137]
 [0.137]] [[35.839]
 [35.839]
 [27.163]
 [35.839]
 [35.839]
 [35.839]
 [35.839]] [[1.371]
 [1.371]
 [0.673]
 [1.371]
 [1.371]
 [1.371]
 [1.371]]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.10349999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.056431016388835735, 0.05591283668464245, 0.2147553841182003, 0.05591283668464245, 0.23090212279071523, 0.3860858033329639]
printing an ep nov before normalisation:  50.669332365291474
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.622]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]] [[45.372]
 [49.892]
 [45.372]
 [45.372]
 [45.372]
 [45.372]
 [45.372]] [[1.943]
 [2.214]
 [1.943]
 [1.943]
 [1.943]
 [1.943]
 [1.943]]
maxi score, test score, baseline:  0.10349999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.056431016388835735, 0.05591283668464245, 0.2147553841182003, 0.05591283668464245, 0.23090212279071523, 0.3860858033329639]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.10349999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.056431016388835735, 0.05591283668464245, 0.2147553841182003, 0.05591283668464245, 0.23090212279071523, 0.3860858033329639]
line 256 mcts: sample exp_bonus 34.343323737460175
actor:  1 policy actor:  1  step number:  37 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  29 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10349999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.056431016388835735, 0.05591283668464245, 0.2147553841182003, 0.05591283668464245, 0.23090212279071523, 0.3860858033329639]
printing an ep nov before normalisation:  29.472887648335206
printing an ep nov before normalisation:  35.547678510337185
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  29 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  26 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  36 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  41.67311113266604
printing an ep nov before normalisation:  43.600585230840196
maxi score, test score, baseline:  0.10637999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.056431016388835735, 0.05591283668464245, 0.2147553841182003, 0.05591283668464245, 0.23090212279071523, 0.3860858033329639]
printing an ep nov before normalisation:  35.718186785540695
actor:  0 policy actor:  1  step number:  22 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  45.391965905413564
printing an ep nov before normalisation:  30.87478259680371
actor:  1 policy actor:  1  step number:  31 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  31 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.10627999999999986 0.6759999999999999 0.6759999999999999
probs:  [0.056431016388835735, 0.05591283668464245, 0.2147553841182003, 0.05591283668464245, 0.23090212279071523, 0.3860858033329639]
maxi score, test score, baseline:  0.10301999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.056431016388835735, 0.05591283668464245, 0.2147553841182003, 0.05591283668464245, 0.23090212279071523, 0.3860858033329639]
actions average: 
K:  2  action  0 :  tensor([0.5111, 0.0274, 0.0743, 0.1171, 0.0894, 0.0950, 0.0858],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0058, 0.9709, 0.0045, 0.0042, 0.0035, 0.0040, 0.0070],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0603, 0.0028, 0.6573, 0.0726, 0.0500, 0.0955, 0.0615],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1555, 0.0373, 0.1886, 0.1630, 0.1153, 0.1852, 0.1550],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0781, 0.0034, 0.0564, 0.1118, 0.6150, 0.0759, 0.0594],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1105, 0.0032, 0.1517, 0.1010, 0.0844, 0.4318, 0.1174],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1245, 0.0093, 0.1194, 0.0902, 0.0813, 0.1049, 0.4704],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.464]
 [0.367]
 [0.429]
 [0.415]
 [0.439]
 [0.425]] [[27.899]
 [27.965]
 [29.315]
 [27.76 ]
 [27.825]
 [27.803]
 [27.792]] [[2.276]
 [2.292]
 [2.367]
 [2.231]
 [2.226]
 [2.247]
 [2.231]]
actor:  1 policy actor:  1  step number:  25 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  32 total reward:  0.46999999999999986  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  33 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
siam score:  -0.825557
maxi score, test score, baseline:  0.10295999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.056431016388835735, 0.05591283668464245, 0.2147553841182003, 0.05591283668464245, 0.23090212279071523, 0.3860858033329639]
maxi score, test score, baseline:  0.10295999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.056431016388835735, 0.05591283668464245, 0.2147553841182003, 0.05591283668464245, 0.23090212279071523, 0.3860858033329639]
actor:  1 policy actor:  1  step number:  27 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  23 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  37.26353089750468
actor:  1 policy actor:  1  step number:  28 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
siam score:  -0.831077
actor:  0 policy actor:  0  step number:  20 total reward:  0.69  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.10019999999999989 0.6759999999999999 0.6759999999999999
printing an ep nov before normalisation:  0.44114678645740923
actor:  0 policy actor:  1  step number:  20 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.10345999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05641385880027231, 0.05589534902353672, 0.2148390767291745, 0.05589534902353672, 0.23099610062714387, 0.38596026579633586]
Printing some Q and Qe and total Qs values:  [[0.057]
 [0.231]
 [0.057]
 [0.148]
 [0.057]
 [0.057]
 [0.057]] [[27.187]
 [35.872]
 [27.187]
 [26.49 ]
 [27.187]
 [27.187]
 [27.187]] [[0.858]
 [1.508]
 [0.858]
 [0.911]
 [0.858]
 [0.858]
 [0.858]]
Printing some Q and Qe and total Qs values:  [[0.642]
 [0.72 ]
 [0.644]
 [0.644]
 [0.64 ]
 [0.644]
 [0.644]] [[36.326]
 [35.122]
 [36.019]
 [35.665]
 [35.654]
 [37.132]
 [36.951]] [[2.075]
 [2.06 ]
 [2.053]
 [2.026]
 [2.021]
 [2.139]
 [2.125]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.73529863357544
maxi score, test score, baseline:  0.10345999999999987 0.6759999999999999 0.6759999999999999
maxi score, test score, baseline:  0.10345999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05641385880027231, 0.05589534902353672, 0.2148390767291745, 0.05589534902353672, 0.23099610062714387, 0.38596026579633586]
Printing some Q and Qe and total Qs values:  [[0.219]
 [0.219]
 [0.219]
 [0.219]
 [0.219]
 [0.219]
 [0.219]] [[31.027]
 [31.027]
 [31.027]
 [31.027]
 [31.027]
 [31.027]
 [31.027]] [[0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]]
printing an ep nov before normalisation:  32.30203804327512
actor:  1 policy actor:  1  step number:  47 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  35 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.10345999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05641385880027231, 0.05589534902353672, 0.2148390767291745, 0.05589534902353672, 0.23099610062714387, 0.38596026579633586]
actor:  1 policy actor:  1  step number:  19 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.667
using another actor
actions average: 
K:  2  action  0 :  tensor([0.6839, 0.0033, 0.0532, 0.0552, 0.0956, 0.0508, 0.0580],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0055,     0.9580,     0.0013,     0.0158,     0.0027,     0.0005,
            0.0163], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0326, 0.0533, 0.7848, 0.0315, 0.0268, 0.0436, 0.0274],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1111, 0.0678, 0.0928, 0.3712, 0.1149, 0.0827, 0.1595],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1653, 0.0099, 0.1038, 0.1467, 0.3401, 0.1274, 0.1068],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0373, 0.0062, 0.2092, 0.0659, 0.0412, 0.5864, 0.0538],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1149, 0.2599, 0.0621, 0.1094, 0.1231, 0.0714, 0.2592],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.697]
 [0.869]
 [0.728]
 [0.745]
 [0.718]
 [0.724]
 [0.722]] [[33.78 ]
 [36.815]
 [33.722]
 [34.455]
 [33.782]
 [34.301]
 [32.965]] [[0.697]
 [0.869]
 [0.728]
 [0.745]
 [0.718]
 [0.724]
 [0.722]]
maxi score, test score, baseline:  0.10345999999999987 0.6759999999999999 0.6759999999999999
maxi score, test score, baseline:  0.10345999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05641385880027231, 0.05589534902353672, 0.2148390767291745, 0.05589534902353672, 0.23099610062714387, 0.38596026579633586]
actor:  0 policy actor:  0  step number:  31 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  26 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  32 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.10313999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05641385880027231, 0.05589534902353672, 0.2148390767291745, 0.05589534902353672, 0.23099610062714387, 0.38596026579633586]
printing an ep nov before normalisation:  46.54160911372905
maxi score, test score, baseline:  0.10313999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05641385880027231, 0.05589534902353672, 0.2148390767291745, 0.05589534902353672, 0.23099610062714387, 0.38596026579633586]
printing an ep nov before normalisation:  44.620046264190776
maxi score, test score, baseline:  0.10313999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05641385880027231, 0.05589534902353672, 0.2148390767291745, 0.05589534902353672, 0.23099610062714387, 0.38596026579633586]
printing an ep nov before normalisation:  34.32077261385905
actor:  0 policy actor:  1  step number:  22 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
from probs:  [0.05641385880027231, 0.05589534902353672, 0.2148390767291745, 0.05589534902353672, 0.23099610062714387, 0.38596026579633586]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10357999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05641385880027231, 0.05589534902353672, 0.2148390767291745, 0.05589534902353672, 0.23099610062714387, 0.38596026579633586]
maxi score, test score, baseline:  0.10357999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05641385880027231, 0.05589534902353672, 0.2148390767291745, 0.05589534902353672, 0.23099610062714387, 0.38596026579633586]
maxi score, test score, baseline:  0.10357999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05641385880027231, 0.05589534902353672, 0.2148390767291745, 0.05589534902353672, 0.23099610062714387, 0.38596026579633586]
actor:  1 policy actor:  1  step number:  35 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  23 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.10685999999999989 0.6759999999999999 0.6759999999999999
probs:  [0.05641385880027231, 0.05589534902353672, 0.2148390767291745, 0.05589534902353672, 0.23099610062714387, 0.38596026579633586]
Printing some Q and Qe and total Qs values:  [[0.645]
 [0.68 ]
 [0.623]
 [0.593]
 [0.63 ]
 [0.206]
 [0.677]] [[47.811]
 [47.111]
 [45.96 ]
 [48.975]
 [46.876]
 [46.088]
 [45.148]] [[1.868]
 [1.871]
 [1.759]
 [1.872]
 [1.809]
 [1.348]
 [1.773]]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  22 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  21 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.10685999999999989 0.6759999999999999 0.6759999999999999
probs:  [0.05639593239416531, 0.055877587772235025, 0.21508895260429825, 0.055877587772235025, 0.2309225665980917, 0.3858373728589746]
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.741]
 [0.643]
 [0.661]
 [0.661]
 [0.639]
 [0.661]] [[35.519]
 [36.29 ]
 [28.791]
 [35.519]
 [35.519]
 [26.874]
 [35.519]] [[0.661]
 [0.741]
 [0.643]
 [0.661]
 [0.661]
 [0.639]
 [0.661]]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  56.042209695304145
Printing some Q and Qe and total Qs values:  [[0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.721]
 [0.765]
 [0.765]] [[50.398]
 [50.398]
 [50.398]
 [50.398]
 [45.072]
 [50.398]
 [50.398]] [[1.627]
 [1.627]
 [1.627]
 [1.627]
 [1.452]
 [1.627]
 [1.627]]
printing an ep nov before normalisation:  48.532995772640945
printing an ep nov before normalisation:  44.98153646773877
maxi score, test score, baseline:  0.10423999999999989 0.6759999999999999 0.6759999999999999
probs:  [0.05639593239416531, 0.055877587772235025, 0.21508895260429825, 0.055877587772235025, 0.2309225665980917, 0.3858373728589746]
printing an ep nov before normalisation:  45.49208493075394
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.37915533763175
actor:  1 policy actor:  1  step number:  27 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.10423999999999989 0.6759999999999999 0.6759999999999999
maxi score, test score, baseline:  0.10423999999999989 0.6759999999999999 0.6759999999999999
probs:  [0.05639434421674933, 0.05587768166169695, 0.21508931465127618, 0.05587768166169695, 0.23092295531340823, 0.3858380224951724]
actor:  1 policy actor:  1  step number:  34 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  39 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.10423999999999989 0.6759999999999999 0.6759999999999999
probs:  [0.05639434421674933, 0.05587768166169695, 0.21508931465127618, 0.05587768166169695, 0.23092295531340823, 0.3858380224951724]
printing an ep nov before normalisation:  42.1995160403551
actor:  0 policy actor:  1  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[ 0.005]
 [ 0.082]
 [-0.01 ]
 [-0.001]
 [ 0.001]
 [-0.   ]
 [-0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.005]
 [ 0.082]
 [-0.01 ]
 [-0.001]
 [ 0.001]
 [-0.   ]
 [-0.001]]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10731999999999986 0.6759999999999999 0.6759999999999999
probs:  [0.05639434421674933, 0.05587768166169695, 0.21508931465127618, 0.05587768166169695, 0.23092295531340823, 0.3858380224951724]
printing an ep nov before normalisation:  43.92097008749467
maxi score, test score, baseline:  0.10731999999999986 0.6759999999999999 0.6759999999999999
probs:  [0.05639434421674933, 0.05587768166169695, 0.21508931465127618, 0.05587768166169695, 0.23092295531340823, 0.3858380224951724]
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.356]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.378]
 [0.356]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]]
maxi score, test score, baseline:  0.10731999999999986 0.6759999999999999 0.6759999999999999
probs:  [0.05639434421674933, 0.05587768166169695, 0.21508931465127618, 0.05587768166169695, 0.23092295531340823, 0.3858380224951724]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10429999999999988 0.6759999999999999 0.6759999999999999
printing an ep nov before normalisation:  26.327364976950108
printing an ep nov before normalisation:  36.064293384552
maxi score, test score, baseline:  0.10429999999999988 0.6759999999999999 0.6759999999999999
maxi score, test score, baseline:  0.10429999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05639434421674933, 0.05587768166169695, 0.21508931465127618, 0.05587768166169695, 0.23092295531340823, 0.3858380224951724]
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.725]
 [0.633]
 [0.591]
 [0.595]
 [0.659]
 [0.724]] [[44.311]
 [40.943]
 [41.631]
 [47.562]
 [47.922]
 [51.107]
 [45.843]] [[0.661]
 [0.725]
 [0.633]
 [0.591]
 [0.595]
 [0.659]
 [0.724]]
printing an ep nov before normalisation:  1.0906794020411326
maxi score, test score, baseline:  0.10429999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05639434421674933, 0.05587768166169695, 0.21508931465127618, 0.05587768166169695, 0.23092295531340823, 0.3858380224951724]
actor:  1 policy actor:  1  step number:  28 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  22 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10419999999999985 0.6759999999999999 0.6759999999999999
probs:  [0.05639434421674933, 0.05587768166169695, 0.21508931465127618, 0.05587768166169695, 0.23092295531340823, 0.3858380224951724]
maxi score, test score, baseline:  0.10419999999999985 0.6759999999999999 0.6759999999999999
probs:  [0.05639434421674933, 0.05587768166169695, 0.21508931465127618, 0.05587768166169695, 0.23092295531340823, 0.3858380224951724]
Printing some Q and Qe and total Qs values:  [[0.842]
 [0.899]
 [0.842]
 [0.842]
 [0.879]
 [0.842]
 [0.842]] [[29.861]
 [38.025]
 [29.861]
 [29.861]
 [38.677]
 [29.861]
 [29.861]] [[0.842]
 [0.899]
 [0.842]
 [0.842]
 [0.879]
 [0.842]
 [0.842]]
actor:  0 policy actor:  1  step number:  21 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  22.82468183831972
actor:  1 policy actor:  1  step number:  34 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  42.916523259726944
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.10731999999999986 0.6759999999999999 0.6759999999999999
probs:  [0.05639434421674933, 0.05587768166169695, 0.21508931465127618, 0.05587768166169695, 0.23092295531340823, 0.3858380224951724]
maxi score, test score, baseline:  0.10731999999999986 0.6759999999999999 0.6759999999999999
probs:  [0.05639434421674933, 0.05587768166169695, 0.21508931465127618, 0.05587768166169695, 0.23092295531340823, 0.3858380224951724]
printing an ep nov before normalisation:  34.979288247752855
maxi score, test score, baseline:  0.10731999999999986 0.6759999999999999 0.6759999999999999
probs:  [0.05639434421674933, 0.05587768166169695, 0.21508931465127618, 0.05587768166169695, 0.23092295531340823, 0.3858380224951724]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.961]
 [0.961]
 [0.961]
 [0.961]
 [0.961]
 [0.961]
 [0.961]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.961]
 [0.961]
 [0.961]
 [0.961]
 [0.961]
 [0.961]
 [0.961]]
actor:  1 policy actor:  1  step number:  20 total reward:  0.69  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  54.66357242815587
printing an ep nov before normalisation:  58.67396677420663
actor:  0 policy actor:  0  step number:  22 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  23 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.11049999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05615622483288321, 0.05564174899887054, 0.2141795351531909, 0.05987076277170183, 0.2299461615623411, 0.38420556668101247]
printing an ep nov before normalisation:  63.57996119986706
maxi score, test score, baseline:  0.11049999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05615622483288321, 0.05564174899887054, 0.2141795351531909, 0.05987076277170183, 0.2299461615623411, 0.38420556668101247]
printing an ep nov before normalisation:  35.63752309452665
printing an ep nov before normalisation:  0.00013903762180689228
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.017]
 [0.017]
 [0.779]
 [0.779]
 [0.016]
 [0.779]] [[ 0.427]
 [ 0.477]
 [ 0.49 ]
 [37.421]
 [37.421]
 [ 0.364]
 [37.421]] [[0.037]
 [0.04 ]
 [0.041]
 [2.779]
 [2.779]
 [0.033]
 [2.779]]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.256]
 [0.325]
 [0.256]
 [0.256]
 [0.256]
 [0.256]
 [0.256]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.256]
 [0.325]
 [0.256]
 [0.256]
 [0.256]
 [0.256]
 [0.256]]
printing an ep nov before normalisation:  64.91074294177052
printing an ep nov before normalisation:  56.34118409760023
maxi score, test score, baseline:  0.11049999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05839261935128703, 0.05785760604205078, 0.1828463453191848, 0.06224587825019861, 0.23912011521126314, 0.39953743582601575]
printing an ep nov before normalisation:  51.59134543917722
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.712]
 [0.673]
 [0.651]
 [0.65 ]
 [0.65 ]
 [0.649]] [[17.092]
 [25.003]
 [28.824]
 [17.668]
 [16.762]
 [16.719]
 [16.944]] [[1.424]
 [1.841]
 [1.975]
 [1.449]
 [1.407]
 [1.405]
 [1.415]]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
using another actor
from probs:  [0.05839261935128703, 0.05785760604205078, 0.1828463453191848, 0.06224587825019861, 0.23912011521126314, 0.39953743582601575]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11049999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.057785360338058894, 0.05723733785079427, 0.18526525528058124, 0.06173231359959729, 0.24290735653604906, 0.3950723763949192]
maxi score, test score, baseline:  0.11049999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.057785360338058894, 0.05723733785079427, 0.18526525528058124, 0.06173231359959729, 0.24290735653604906, 0.3950723763949192]
printing an ep nov before normalisation:  30.405688285827637
maxi score, test score, baseline:  0.10731999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.057785360338058894, 0.05723733785079427, 0.18526525528058124, 0.06173231359959729, 0.24290735653604906, 0.3950723763949192]
printing an ep nov before normalisation:  29.571221475828118
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  40 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  31 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.10731999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05778592292979249, 0.05723789509471318, 0.185267061870249, 0.06172316412237879, 0.24290972561955113, 0.3950762303633155]
maxi score, test score, baseline:  0.10731999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05778592292979249, 0.05723789509471318, 0.185267061870249, 0.06172316412237879, 0.24290972561955113, 0.3950762303633155]
actor:  0 policy actor:  0  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.229]
 [0.301]
 [0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.229]] [[40.011]
 [40.49 ]
 [40.011]
 [40.011]
 [40.011]
 [40.011]
 [40.011]] [[1.422]
 [1.517]
 [1.422]
 [1.422]
 [1.422]
 [1.422]
 [1.422]]
Printing some Q and Qe and total Qs values:  [[0.258]
 [0.307]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]] [[39.559]
 [36.9  ]
 [39.559]
 [39.559]
 [39.559]
 [39.559]
 [39.559]] [[1.482]
 [1.401]
 [1.482]
 [1.482]
 [1.482]
 [1.482]
 [1.482]]
maxi score, test score, baseline:  0.10743999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05778592292979249, 0.05723789509471318, 0.185267061870249, 0.06172316412237879, 0.24290972561955113, 0.3950762303633155]
printing an ep nov before normalisation:  28.329346179962158
siam score:  -0.83844155
actor:  1 policy actor:  1  step number:  33 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  35 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.10743999999999987 0.6759999999999999 0.6759999999999999
actor:  1 policy actor:  1  step number:  35 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10743999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05778592292979249, 0.05723789509471318, 0.185267061870249, 0.06172316412237879, 0.24290972561955113, 0.3950762303633155]
maxi score, test score, baseline:  0.10743999999999987 0.6759999999999999 0.6759999999999999
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  21 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.626]
 [0.748]
 [0.626]
 [0.626]
 [0.626]
 [0.626]] [[49.537]
 [49.537]
 [50.039]
 [49.537]
 [49.537]
 [49.537]
 [49.537]] [[1.531]
 [1.531]
 [1.667]
 [1.531]
 [1.531]
 [1.531]
 [1.531]]
printing an ep nov before normalisation:  40.03251513958325
actor:  1 policy actor:  1  step number:  35 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.10743999999999987 0.6759999999999999 0.6759999999999999
actor:  0 policy actor:  1  step number:  23 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  30.164610432850193
siam score:  -0.83576643
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 29.709506472600527
actor:  0 policy actor:  0  step number:  22 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[ 0.556]
 [ 0.611]
 [-0.094]
 [ 0.496]
 [ 0.533]
 [-0.104]
 [ 0.565]] [[41.59 ]
 [43.12 ]
 [51.651]
 [42.346]
 [43.619]
 [42.033]
 [44.824]] [[1.39 ]
 [1.531]
 [1.306]
 [1.372]
 [1.481]
 [0.754]
 [1.58 ]]
maxi score, test score, baseline:  0.11075999999999989 0.6759999999999999 0.6759999999999999
probs:  [0.057786483096972666, 0.05723844993712592, 0.1852688606742128, 0.06171405407587481, 0.24291208449321766, 0.395080067722596]
printing an ep nov before normalisation:  49.464391846827304
maxi score, test score, baseline:  0.11075999999999989 0.6759999999999999 0.6759999999999999
probs:  [0.057786483096972666, 0.05723844993712592, 0.1852688606742128, 0.06171405407587481, 0.24291208449321766, 0.395080067722596]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  17 total reward:  0.72  reward:  1.0 rdn_beta:  2.0
from probs:  [0.05780596832276969, 0.05725774994268482, 0.18533143146134134, 0.0617348667133776, 0.24265643459329103, 0.39521354896653554]
siam score:  -0.82547015
maxi score, test score, baseline:  0.10745999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05973778649934976, 0.05923100121195105, 0.17762516648347165, 0.06336974772570746, 0.23061767684560924, 0.40941862123391093]
actor:  0 policy actor:  0  step number:  19 total reward:  0.7  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.10755999999999986 0.6759999999999999 0.6759999999999999
probs:  [0.05973778649934976, 0.05923100121195105, 0.17762516648347165, 0.06336974772570746, 0.23061767684560924, 0.40941862123391093]
actor:  1 policy actor:  1  step number:  31 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.349]
 [0.512]
 [0.349]
 [0.349]
 [0.349]
 [0.349]] [[40.613]
 [40.613]
 [53.375]
 [40.613]
 [40.613]
 [40.613]
 [40.613]] [[0.771]
 [0.771]
 [1.151]
 [0.771]
 [0.771]
 [0.771]
 [0.771]]
maxi score, test score, baseline:  0.10755999999999986 0.6759999999999999 0.6759999999999999
probs:  [0.05973778649934976, 0.05923100121195105, 0.17762516648347165, 0.06336974772570746, 0.23061767684560924, 0.40941862123391093]
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.435]
 [0.332]
 [0.332]
 [0.332]
 [0.332]
 [0.332]] [[32.363]
 [49.426]
 [32.363]
 [32.363]
 [32.363]
 [32.363]
 [32.363]] [[0.607]
 [0.986]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
from probs:  [0.05973778649934976, 0.05923100121195105, 0.17762516648347165, 0.06336974772570746, 0.23061767684560924, 0.40941862123391093]
printing an ep nov before normalisation:  53.72004472579115
maxi score, test score, baseline:  0.10755999999999986 0.6759999999999999 0.6759999999999999
probs:  [0.05973778649934976, 0.05923100121195105, 0.17762516648347165, 0.06336974772570746, 0.23061767684560924, 0.40941862123391093]
UNIT TEST: sample policy line 217 mcts : [0.082 0.143 0.061 0.449 0.082 0.102 0.082]
actor:  1 policy actor:  1  step number:  30 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  24 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]]
printing an ep nov before normalisation:  8.734782113606343e-05
Printing some Q and Qe and total Qs values:  [[0.442]
 [0.569]
 [0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]] [[69.437]
 [56.453]
 [64.509]
 [64.509]
 [64.509]
 [64.509]
 [64.509]] [[1.442]
 [1.323]
 [1.421]
 [1.421]
 [1.421]
 [1.421]
 [1.421]]
actor:  0 policy actor:  0  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.10739999999999987 0.6759999999999999 0.6759999999999999
maxi score, test score, baseline:  0.10739999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05973778649934976, 0.05923100121195105, 0.17762516648347165, 0.06336974772570746, 0.23061767684560924, 0.40941862123391093]
maxi score, test score, baseline:  0.10739999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05973778649934976, 0.05923100121195105, 0.17762516648347165, 0.06336974772570746, 0.23061767684560924, 0.40941862123391093]
Printing some Q and Qe and total Qs values:  [[0.93 ]
 [0.965]
 [0.93 ]
 [0.93 ]
 [0.93 ]
 [0.93 ]
 [0.93 ]] [[39.232]
 [44.098]
 [39.232]
 [39.232]
 [39.232]
 [39.232]
 [39.232]] [[0.93 ]
 [0.965]
 [0.93 ]
 [0.93 ]
 [0.93 ]
 [0.93 ]
 [0.93 ]]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  26 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  1  step number:  25 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8211261
actor:  0 policy actor:  1  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  22 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  33.92442214626965
maxi score, test score, baseline:  0.10691999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05973778649934976, 0.05923100121195105, 0.17762516648347165, 0.06336974772570746, 0.23061767684560924, 0.40941862123391093]
Printing some Q and Qe and total Qs values:  [[0.245]
 [0.457]
 [0.373]
 [0.373]
 [0.373]
 [0.373]
 [0.373]] [[34.317]
 [47.163]
 [37.165]
 [37.165]
 [37.165]
 [37.165]
 [37.165]] [[0.577]
 [1.065]
 [0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]]
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.26 ]
 [0.314]
 [0.289]
 [0.296]
 [0.246]
 [0.253]] [[36.57 ]
 [39.718]
 [34.598]
 [33.277]
 [36.458]
 [38.487]
 [37.226]] [[0.842]
 [0.952]
 [0.855]
 [0.79 ]
 [0.892]
 [0.902]
 [0.871]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10691999999999988 0.6759999999999999 0.6759999999999999
actor:  1 policy actor:  1  step number:  21 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  35 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.236]
 [0.295]
 [0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.236]
 [0.295]
 [0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  75.50434279998589
maxi score, test score, baseline:  0.10691999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.059194787954109075, 0.05867634024884577, 0.1797950562570564, 0.06290118515588378, 0.23400705900562271, 0.40542557137848223]
actor:  1 policy actor:  1  step number:  24 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  21.2514591217041
maxi score, test score, baseline:  0.10691999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.059194787954109075, 0.05867634024884577, 0.1797950562570564, 0.06290118515588378, 0.23400705900562271, 0.40542557137848223]
actor:  1 policy actor:  1  step number:  33 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10691999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.059194787954109075, 0.05867634024884577, 0.1797950562570564, 0.06290118515588378, 0.23400705900562271, 0.40542557137848223]
actor:  1 policy actor:  1  step number:  42 total reward:  0.2899999999999996  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10691999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.059194787954109075, 0.05867634024884577, 0.1797950562570564, 0.06290118515588378, 0.23400705900562271, 0.40542557137848223]
printing an ep nov before normalisation:  37.45298188514341
Printing some Q and Qe and total Qs values:  [[0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]] [[28.642]
 [28.642]
 [28.642]
 [28.642]
 [28.642]
 [28.642]
 [28.642]] [[57.922]
 [57.922]
 [57.922]
 [57.922]
 [57.922]
 [57.922]
 [57.922]]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10691999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.059194787954109075, 0.05867634024884577, 0.1797950562570564, 0.06290118515588378, 0.23400705900562271, 0.40542557137848223]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10691999999999988 0.6759999999999999 0.6759999999999999
maxi score, test score, baseline:  0.10691999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.059194787954109075, 0.05867634024884577, 0.1797950562570564, 0.06290118515588378, 0.23400705900562271, 0.40542557137848223]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  43.084452231319055
actor:  1 policy actor:  1  step number:  21 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10691999999999988 0.6759999999999999 0.6759999999999999
printing an ep nov before normalisation:  31.126701734243827
printing an ep nov before normalisation:  49.44387931757312
actor:  1 policy actor:  1  step number:  30 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  28 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.10691999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.059194787954109075, 0.05867634024884577, 0.1797950562570564, 0.06290118515588378, 0.23400705900562271, 0.40542557137848223]
maxi score, test score, baseline:  0.10691999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.059194787954109075, 0.05867634024884577, 0.1797950562570564, 0.06290118515588378, 0.23400705900562271, 0.40542557137848223]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  18 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  35.384483337402344
maxi score, test score, baseline:  0.10691999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.059194787954109075, 0.05867634024884577, 0.1797950562570564, 0.06290118515588378, 0.23400705900562271, 0.40542557137848223]
printing an ep nov before normalisation:  61.09526646969859
Printing some Q and Qe and total Qs values:  [[0.779]
 [0.908]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]] [[41.472]
 [42.411]
 [41.472]
 [41.472]
 [41.472]
 [41.472]
 [41.472]] [[0.779]
 [0.908]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]]
Printing some Q and Qe and total Qs values:  [[0.217]
 [0.224]
 [0.217]
 [0.217]
 [0.217]
 [0.217]
 [0.217]] [[33.924]
 [39.583]
 [33.924]
 [33.924]
 [33.924]
 [33.924]
 [33.924]] [[0.9  ]
 [1.124]
 [0.9  ]
 [0.9  ]
 [0.9  ]
 [0.9  ]
 [0.9  ]]
maxi score, test score, baseline:  0.10691999999999988 0.6759999999999999 0.6759999999999999
maxi score, test score, baseline:  0.10691999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.059194787954109075, 0.05867634024884577, 0.1797950562570564, 0.06290118515588378, 0.23400705900562271, 0.40542557137848223]
maxi score, test score, baseline:  0.10691999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.059194787954109075, 0.05867634024884577, 0.1797950562570564, 0.06290118515588378, 0.23400705900562271, 0.40542557137848223]
printing an ep nov before normalisation:  45.35583345325967
UNIT TEST: sample policy line 217 mcts : [0.469 0.184 0.02  0.02  0.265 0.02  0.02 ]
actor:  0 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  24 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.10669999999999986 0.6759999999999999 0.6759999999999999
probs:  [0.059194787954109075, 0.05867634024884577, 0.1797950562570564, 0.06290118515588378, 0.23400705900562271, 0.40542557137848223]
using explorer policy with actor:  1
siam score:  -0.8187256
maxi score, test score, baseline:  0.10669999999999986 0.6759999999999999 0.6759999999999999
probs:  [0.059194787954109075, 0.05867634024884577, 0.1797950562570564, 0.06290118515588378, 0.23400705900562271, 0.40542557137848223]
maxi score, test score, baseline:  0.10669999999999986 0.6759999999999999 0.6759999999999999
actor:  1 policy actor:  1  step number:  31 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]] [[39.018]
 [39.018]
 [39.018]
 [39.018]
 [39.018]
 [39.018]
 [39.018]] [[0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]]
maxi score, test score, baseline:  0.10669999999999986 0.6759999999999999 0.6759999999999999
probs:  [0.059194787954109075, 0.05867634024884577, 0.1797950562570564, 0.06290118515588378, 0.23400705900562271, 0.40542557137848223]
maxi score, test score, baseline:  0.10669999999999986 0.6759999999999999 0.6759999999999999
probs:  [0.059194787954109075, 0.05867634024884577, 0.1797950562570564, 0.06290118515588378, 0.23400705900562271, 0.40542557137848223]
printing an ep nov before normalisation:  29.77147102355957
maxi score, test score, baseline:  0.10669999999999986 0.6759999999999999 0.6759999999999999
probs:  [0.059194787954109075, 0.05867634024884577, 0.1797950562570564, 0.06290118515588378, 0.23400705900562271, 0.40542557137848223]
printing an ep nov before normalisation:  52.47340190032898
maxi score, test score, baseline:  0.10669999999999986 0.6759999999999999 0.6759999999999999
probs:  [0.059194787954109075, 0.05867634024884577, 0.1797950562570564, 0.06290118515588378, 0.23400705900562271, 0.40542557137848223]
actor:  1 policy actor:  1  step number:  33 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  33 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10669999999999986 0.6759999999999999 0.6759999999999999
probs:  [0.0591932093013673, 0.05867643855264867, 0.17979535793969706, 0.0629012905539166, 0.23400745171939782, 0.4054262519329725]
printing an ep nov before normalisation:  66.78770769185509
printing an ep nov before normalisation:  49.665880351462114
maxi score, test score, baseline:  0.10669999999999986 0.6759999999999999 0.6759999999999999
probs:  [0.0591932093013673, 0.05867643855264867, 0.17979535793969706, 0.0629012905539166, 0.23400745171939782, 0.4054262519329725]
siam score:  -0.81651413
Printing some Q and Qe and total Qs values:  [[0.924]
 [0.96 ]
 [0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.924]] [[36.886]
 [39.876]
 [36.886]
 [36.886]
 [36.886]
 [36.886]
 [36.886]] [[0.924]
 [0.96 ]
 [0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.924]]
maxi score, test score, baseline:  0.10669999999999986 0.6759999999999999 0.6759999999999999
probs:  [0.0591932093013673, 0.05867643855264867, 0.17979535793969706, 0.0629012905539166, 0.23400745171939782, 0.4054262519329725]
actor:  0 policy actor:  0  step number:  27 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
from probs:  [0.0591932093013673, 0.05867643855264867, 0.17979535793969706, 0.0629012905539166, 0.23400745171939782, 0.4054262519329725]
actor:  1 policy actor:  1  step number:  31 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10645999999999986 0.6759999999999999 0.6759999999999999
probs:  [0.0591932093013673, 0.05867643855264867, 0.17979535793969706, 0.0629012905539166, 0.23400745171939782, 0.4054262519329725]
Printing some Q and Qe and total Qs values:  [[0.206]
 [0.317]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.206]
 [0.317]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]]
maxi score, test score, baseline:  0.10645999999999986 0.6759999999999999 0.6759999999999999
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.318]
 [0.241]
 [0.301]
 [0.301]
 [0.222]
 [0.301]] [[37.741]
 [39.121]
 [36.021]
 [37.741]
 [37.741]
 [31.231]
 [37.741]] [[0.775]
 [0.82 ]
 [0.68 ]
 [0.775]
 [0.775]
 [0.564]
 [0.775]]
maxi score, test score, baseline:  0.10321999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.0591932093013673, 0.05867643855264867, 0.17979535793969706, 0.0629012905539166, 0.23400745171939782, 0.4054262519329725]
maxi score, test score, baseline:  0.10321999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.0591932093013673, 0.05867643855264867, 0.17979535793969706, 0.0629012905539166, 0.23400745171939782, 0.4054262519329725]
actor:  1 policy actor:  1  step number:  22 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.276]
 [0.265]
 [0.293]
 [0.293]
 [0.293]
 [0.293]] [[50.954]
 [45.199]
 [50.272]
 [50.954]
 [50.954]
 [50.954]
 [50.954]] [[2.24 ]
 [2.003]
 [2.186]
 [2.24 ]
 [2.24 ]
 [2.24 ]
 [2.24 ]]
siam score:  -0.82378656
maxi score, test score, baseline:  0.10321999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.0591932093013673, 0.05867643855264867, 0.17979535793969706, 0.0629012905539166, 0.23400745171939782, 0.4054262519329725]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  36 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.10321999999999987 0.6759999999999999 0.6759999999999999
maxi score, test score, baseline:  0.09997999999999987 0.6759999999999999 0.6759999999999999
maxi score, test score, baseline:  0.09997999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.0591932093013673, 0.05867643855264867, 0.17979535793969706, 0.0629012905539166, 0.23400745171939782, 0.4054262519329725]
actor:  1 policy actor:  1  step number:  33 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09997999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.0591932093013673, 0.05867643855264867, 0.17979535793969706, 0.0629012905539166, 0.23400745171939782, 0.4054262519329725]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  25 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
siam score:  -0.82177216
actor:  1 policy actor:  1  step number:  39 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  1  step number:  32 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.09963999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.059191640828231334, 0.05867653622256042, 0.17979565767700156, 0.06290139527231309, 0.23400784190084173, 0.40542692809905195]
printing an ep nov before normalisation:  65.7773821279821
actor:  1 policy actor:  1  step number:  26 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  35.77752112837483
printing an ep nov before normalisation:  50.91276869926327
maxi score, test score, baseline:  0.09963999999999987 0.6759999999999999 0.6759999999999999
printing an ep nov before normalisation:  43.03086757659912
actor:  1 policy actor:  1  step number:  34 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  27 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  32 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  24 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.09963999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05918853402945448, 0.05867672968507848, 0.179796251390384, 0.06290160269632177, 0.2340086147640838, 0.4054282674346776]
from probs:  [0.05918853402945448, 0.05867672968507848, 0.179796251390384, 0.06290160269632177, 0.2340086147640838, 0.4054282674346776]
maxi score, test score, baseline:  0.09963999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05918853402945448, 0.05867672968507848, 0.179796251390384, 0.06290160269632177, 0.2340086147640838, 0.4054282674346776]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.839]
 [0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]] [[54.621]
 [67.377]
 [67.377]
 [67.377]
 [67.377]
 [67.377]
 [67.377]] [[0.839]
 [0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]]
maxi score, test score, baseline:  0.09963999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05918853402945448, 0.05867672968507848, 0.179796251390384, 0.06290160269632177, 0.2340086147640838, 0.4054282674346776]
maxi score, test score, baseline:  0.09639999999999989 0.6759999999999999 0.6759999999999999
maxi score, test score, baseline:  0.09639999999999989 0.6759999999999999 0.6759999999999999
probs:  [0.05918853402945448, 0.05867672968507848, 0.179796251390384, 0.06290160269632177, 0.2340086147640838, 0.4054282674346776]
maxi score, test score, baseline:  0.09639999999999989 0.6759999999999999 0.6759999999999999
probs:  [0.05918853402945448, 0.05867672968507848, 0.179796251390384, 0.06290160269632177, 0.2340086147640838, 0.4054282674346776]
Printing some Q and Qe and total Qs values:  [[0.67]
 [0.67]
 [0.67]
 [0.67]
 [0.67]
 [0.67]
 [0.67]] [[51.817]
 [51.817]
 [51.817]
 [51.817]
 [51.817]
 [51.817]
 [51.817]] [[1.821]
 [1.821]
 [1.821]
 [1.821]
 [1.821]
 [1.821]
 [1.821]]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  26.067681312561035
actor:  0 policy actor:  1  step number:  28 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.687]
 [0.689]
 [0.675]
 [0.67 ]
 [0.633]
 [0.633]
 [0.687]] [[29.641]
 [45.138]
 [29.516]
 [29.613]
 [43.891]
 [43.891]
 [29.893]] [[1.147]
 [1.887]
 [1.129]
 [1.129]
 [1.772]
 [1.772]
 [1.159]]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.09597999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05918853402945448, 0.05867672968507848, 0.179796251390384, 0.06290160269632177, 0.2340086147640838, 0.4054282674346776]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.79 ]
 [0.805]
 [0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]] [[47.668]
 [50.497]
 [47.417]
 [47.417]
 [47.417]
 [47.417]
 [47.417]] [[0.79 ]
 [0.805]
 [0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]]
actor:  1 policy actor:  1  step number:  25 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  32 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  30.47719379300136
actor:  1 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09589999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05918853402945448, 0.05867672968507848, 0.179796251390384, 0.06290160269632177, 0.2340086147640838, 0.4054282674346776]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  38 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  29 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.09265999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05918853402945448, 0.05867672968507848, 0.179796251390384, 0.06290160269632177, 0.2340086147640838, 0.4054282674346776]
from probs:  [0.05918853402945448, 0.05867672968507848, 0.179796251390384, 0.06290160269632177, 0.2340086147640838, 0.4054282674346776]
maxi score, test score, baseline:  0.09265999999999987 0.6759999999999999 0.6759999999999999
printing an ep nov before normalisation:  0.0029344312693524444
Printing some Q and Qe and total Qs values:  [[0.24 ]
 [0.362]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.24 ]
 [0.362]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]]
Printing some Q and Qe and total Qs values:  [[0.693]
 [0.735]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.693]
 [0.735]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]]
maxi score, test score, baseline:  0.09265999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05918853402945448, 0.05867672968507848, 0.179796251390384, 0.06290160269632177, 0.2340086147640838, 0.4054282674346776]
maxi score, test score, baseline:  0.09265999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05918853402945448, 0.05867672968507848, 0.179796251390384, 0.06290160269632177, 0.2340086147640838, 0.4054282674346776]
actor:  0 policy actor:  1  step number:  23 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  22 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.09249999999999986 0.6759999999999999 0.6759999999999999
probs:  [0.05918853402945448, 0.05867672968507848, 0.179796251390384, 0.06290160269632177, 0.2340086147640838, 0.4054282674346776]
Printing some Q and Qe and total Qs values:  [[0.697]
 [0.775]
 [0.692]
 [0.692]
 [0.687]
 [0.687]
 [0.687]] [[25.641]
 [31.499]
 [28.043]
 [27.412]
 [26.123]
 [26.123]
 [26.123]] [[0.697]
 [0.775]
 [0.692]
 [0.692]
 [0.687]
 [0.687]
 [0.687]]
printing an ep nov before normalisation:  55.03197493769142
Printing some Q and Qe and total Qs values:  [[ 0.38 ]
 [ 0.328]
 [-0.111]
 [ 0.366]
 [-0.1  ]
 [-0.11 ]
 [-0.11 ]] [[51.448]
 [47.526]
 [56.7  ]
 [53.881]
 [56.368]
 [54.453]
 [57.562]] [[1.972]
 [1.752]
 [1.706]
 [2.062]
 [1.703]
 [1.611]
 [1.745]]
printing an ep nov before normalisation:  34.36537929861885
actor:  0 policy actor:  0  step number:  26 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.178]
 [0.51 ]
 [0.178]
 [0.178]
 [0.172]
 [0.178]
 [0.178]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.178]
 [0.51 ]
 [0.178]
 [0.178]
 [0.172]
 [0.178]
 [0.178]]
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.441]
 [0.501]
 [0.441]
 [0.441]
 [0.441]
 [0.441]] [[44.261]
 [44.261]
 [49.851]
 [44.261]
 [44.261]
 [44.261]
 [44.261]] [[1.401]
 [1.401]
 [1.654]
 [1.401]
 [1.401]
 [1.401]
 [1.401]]
actor:  1 policy actor:  1  step number:  19 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  37.197013489652434
maxi score, test score, baseline:  0.09221999999999987 0.6759999999999999 0.6759999999999999
actor:  1 policy actor:  1  step number:  30 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  0.333
from probs:  [0.05918853402945448, 0.05867672968507848, 0.179796251390384, 0.06290160269632177, 0.2340086147640838, 0.4054282674346776]
maxi score, test score, baseline:  0.08901999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05918699551127654, 0.05867682548967417, 0.17979654540325615, 0.06290170541478897, 0.23400899749377838, 0.40542893068722585]
printing an ep nov before normalisation:  36.61211967468262
actor:  1 policy actor:  1  step number:  23 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]] [[22.039]
 [22.039]
 [22.039]
 [22.039]
 [22.039]
 [22.039]
 [22.039]] [[1.495]
 [1.495]
 [1.495]
 [1.495]
 [1.495]
 [1.495]
 [1.495]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  35 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.08901999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05918699551127654, 0.05867682548967417, 0.17979654540325615, 0.06290170541478897, 0.23400899749377838, 0.40542893068722585]
maxi score, test score, baseline:  0.08901999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05918699551127654, 0.05867682548967417, 0.17979654540325615, 0.06290170541478897, 0.23400899749377838, 0.40542893068722585]
maxi score, test score, baseline:  0.08901999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05918699551127654, 0.05867682548967417, 0.17979654540325615, 0.06290170541478897, 0.23400899749377838, 0.40542893068722585]
printing an ep nov before normalisation:  35.903681666244026
maxi score, test score, baseline:  0.08901999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05918699551127654, 0.05867682548967417, 0.17979654540325615, 0.06290170541478897, 0.23400899749377838, 0.40542893068722585]
actor:  1 policy actor:  1  step number:  30 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.08901999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.059185466787589514, 0.05867692068436015, 0.179796837544388, 0.06290180747933118, 0.23400937778694494, 0.4054295897173863]
using explorer policy with actor:  1
siam score:  -0.8139236
maxi score, test score, baseline:  0.08901999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.059185466787589514, 0.05867692068436015, 0.179796837544388, 0.06290180747933118, 0.23400937778694494, 0.4054295897173863]
printing an ep nov before normalisation:  35.15324941585501
actor:  1 policy actor:  1  step number:  32 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  28 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.256]
 [0.244]
 [0.197]
 [0.256]
 [0.188]
 [0.183]
 [0.254]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.256]
 [0.244]
 [0.197]
 [0.256]
 [0.188]
 [0.183]
 [0.254]]
maxi score, test score, baseline:  0.08901999999999988 0.6759999999999999 0.6759999999999999
maxi score, test score, baseline:  0.08901999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.059185466787589514, 0.05867692068436015, 0.179796837544388, 0.06290180747933118, 0.23400937778694494, 0.4054295897173863]
maxi score, test score, baseline:  0.08901999999999988 0.6759999999999999 0.6759999999999999
actor:  1 policy actor:  1  step number:  30 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  25 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  28 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.637]
 [0.736]
 [0.712]
 [0.656]
 [0.712]
 [0.712]
 [0.662]] [[35.322]
 [42.705]
 [27.73 ]
 [35.683]
 [27.73 ]
 [27.73 ]
 [36.491]] [[0.637]
 [0.736]
 [0.712]
 [0.656]
 [0.712]
 [0.712]
 [0.662]]
printing an ep nov before normalisation:  47.48879673116488
printing an ep nov before normalisation:  46.34937122256232
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.871]
 [0.789]
 [0.789]
 [0.685]
 [0.686]
 [0.666]] [[27.233]
 [32.982]
 [27.477]
 [27.477]
 [27.348]
 [27.164]
 [27.664]] [[0.661]
 [0.871]
 [0.789]
 [0.789]
 [0.685]
 [0.686]
 [0.666]]
maxi score, test score, baseline:  0.08901999999999988 0.6759999999999999 0.6759999999999999
maxi score, test score, baseline:  0.08901999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.059185466787589514, 0.05867692068436015, 0.179796837544388, 0.06290180747933118, 0.23400937778694494, 0.4054295897173863]
printing an ep nov before normalisation:  33.89385223388672
actor:  1 policy actor:  1  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  47.10325134841703
maxi score, test score, baseline:  0.08901999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.059185466787589514, 0.05867692068436015, 0.179796837544388, 0.06290180747933118, 0.23400937778694494, 0.4054295897173863]
printing an ep nov before normalisation:  33.543205276305
actor:  1 policy actor:  1  step number:  30 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  42 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  25 total reward:  0.72  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09515999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.0591860049145198, 0.058677454177048866, 0.1797984747697891, 0.06289327402981243, 0.23401150903641454, 0.40543328307241533]
printing an ep nov before normalisation:  42.63673704752422
actor:  1 policy actor:  1  step number:  26 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.09515999999999987 0.6759999999999999 0.6759999999999999
printing an ep nov before normalisation:  50.79679714644017
actor:  0 policy actor:  0  step number:  26 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.57150402898361
maxi score, test score, baseline:  0.09805999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.0591865407366703, 0.058677985384805925, 0.17980010498300786, 0.06288477712877699, 0.23401363115781282, 0.4054369606089262]
maxi score, test score, baseline:  0.09805999999999987 0.6759999999999999 0.6759999999999999
actor:  1 policy actor:  1  step number:  35 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.09805999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.0591870742688164, 0.05867851432227944, 0.17980172822899718, 0.0628763165419227, 0.2340157442096572, 0.4054406224283272]
maxi score, test score, baseline:  0.09805999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.0591870742688164, 0.05867851432227944, 0.17980172822899718, 0.0628763165419227, 0.2340157442096572, 0.4054406224283272]
maxi score, test score, baseline:  0.09805999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.0591870742688164, 0.05867851432227944, 0.17980172822899718, 0.0628763165419227, 0.2340157442096572, 0.4054406224283272]
actor:  1 policy actor:  1  step number:  37 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  1  action  0 :  tensor([0.7262, 0.0042, 0.0426, 0.0450, 0.1002, 0.0424, 0.0394],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0012,     0.9560,     0.0018,     0.0112,     0.0006,     0.0003,
            0.0289], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0207, 0.0482, 0.8272, 0.0231, 0.0215, 0.0247, 0.0347],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1169, 0.1173, 0.0942, 0.2848, 0.1162, 0.1153, 0.1553],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0780, 0.0174, 0.0873, 0.0756, 0.5896, 0.0628, 0.0893],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1257, 0.0012, 0.1756, 0.1029, 0.1234, 0.3523, 0.1189],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1464, 0.0496, 0.1344, 0.1613, 0.1432, 0.1306, 0.2346],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  31 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.86 ]
 [1.072]
 [0.817]
 [0.637]
 [0.793]
 [0.817]
 [0.692]] [[45.316]
 [48.361]
 [56.976]
 [57.24 ]
 [51.229]
 [56.976]
 [55.198]] [[1.766]
 [2.068]
 [2.069]
 [1.897]
 [1.875]
 [2.069]
 [1.891]]
actor:  0 policy actor:  1  step number:  34 total reward:  0.3299999999999996  reward:  1.0 rdn_beta:  1.667
from probs:  [0.0591870742688164, 0.05867851432227944, 0.17980172822899718, 0.0628763165419227, 0.2340157442096572, 0.4054406224283272]
Printing some Q and Qe and total Qs values:  [[ 0.558]
 [ 0.623]
 [-0.097]
 [ 0.498]
 [ 0.564]
 [ 0.5  ]
 [ 0.505]] [[50.441]
 [56.619]
 [66.839]
 [63.305]
 [62.142]
 [73.77 ]
 [63.933]] [[1.144]
 [1.318]
 [0.78 ]
 [1.312]
 [1.358]
 [1.5  ]
 [1.331]]
maxi score, test score, baseline:  0.0978199999999999 0.6759999999999999 0.6759999999999999
probs:  [0.0591870742688164, 0.05867851432227944, 0.17980172822899718, 0.0628763165419227, 0.2340157442096572, 0.4054406224283272]
Printing some Q and Qe and total Qs values:  [[0.658]
 [0.725]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]] [[41.528]
 [50.062]
 [41.528]
 [41.528]
 [41.528]
 [41.528]
 [41.528]] [[1.294]
 [1.565]
 [1.294]
 [1.294]
 [1.294]
 [1.294]
 [1.294]]
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.707]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.653]
 [0.707]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]]
maxi score, test score, baseline:  0.0978199999999999 0.6759999999999999 0.6759999999999999
probs:  [0.0591870742688164, 0.05867851432227944, 0.17980172822899718, 0.0628763165419227, 0.2340157442096572, 0.4054406224283272]
printing an ep nov before normalisation:  41.036057191986266
maxi score, test score, baseline:  0.0978199999999999 0.6759999999999999 0.6759999999999999
probs:  [0.0591870742688164, 0.05867851432227944, 0.17980172822899718, 0.0628763165419227, 0.2340157442096572, 0.4054406224283272]
maxi score, test score, baseline:  0.09781999999999987 0.6759999999999999 0.6759999999999999
maxi score, test score, baseline:  0.09781999999999987 0.6759999999999999 0.6759999999999999
maxi score, test score, baseline:  0.09781999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.05918760552560683, 0.05867904100399253, 0.17980334455232688, 0.06286789203694568, 0.23401784824996635, 0.40544426863116173]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  33 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.632]
 [0.742]
 [0.629]
 [0.681]
 [0.618]
 [0.681]
 [0.614]] [[31.373]
 [38.988]
 [29.617]
 [31.215]
 [33.022]
 [31.215]
 [32.01 ]] [[0.632]
 [0.742]
 [0.629]
 [0.681]
 [0.618]
 [0.681]
 [0.614]]
maxi score, test score, baseline:  0.09781999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.0591881345215666, 0.058679565444344724, 0.1798049539971874, 0.06285950338351946, 0.2340199433362647, 0.40544789931711706]
printing an ep nov before normalisation:  48.38676690897073
Printing some Q and Qe and total Qs values:  [[0.588]
 [0.588]
 [0.588]
 [0.588]
 [0.591]
 [0.588]
 [0.588]] [[55.707]
 [55.707]
 [55.707]
 [55.707]
 [57.868]
 [55.707]
 [55.707]] [[0.588]
 [0.588]
 [0.588]
 [0.588]
 [0.591]
 [0.588]
 [0.588]]
printing an ep nov before normalisation:  52.11728385322939
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.59626665378346
maxi score, test score, baseline:  0.09781999999999988 0.6759999999999999 0.6759999999999999
maxi score, test score, baseline:  0.09781999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.0591881345215666, 0.058679565444344724, 0.1798049539971874, 0.06285950338351946, 0.2340199433362647, 0.40544789931711706]
maxi score, test score, baseline:  0.09781999999999988 0.6759999999999999 0.6759999999999999
maxi score, test score, baseline:  0.09781999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.0591881345215666, 0.058679565444344724, 0.1798049539971874, 0.06285950338351946, 0.2340199433362647, 0.40544789931711706]
actor:  1 policy actor:  1  step number:  35 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.333
using another actor
maxi score, test score, baseline:  0.09781999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05918866127109687, 0.0586800876576132, 0.17980655660739395, 0.0628511503532736, 0.23402202952558848, 0.40545151458503387]
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]] [[38.588]
 [38.588]
 [38.588]
 [38.588]
 [38.588]
 [38.588]
 [38.588]] [[77.571]
 [77.571]
 [77.571]
 [77.571]
 [77.571]
 [77.571]
 [77.571]]
Printing some Q and Qe and total Qs values:  [[0.446]
 [0.446]
 [0.446]
 [0.446]
 [0.446]
 [0.446]
 [0.446]] [[32.115]
 [32.115]
 [32.115]
 [32.115]
 [32.115]
 [32.115]
 [32.115]] [[1.394]
 [1.394]
 [1.394]
 [1.394]
 [1.394]
 [1.394]
 [1.394]]
maxi score, test score, baseline:  0.09781999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05918866127109687, 0.0586800876576132, 0.17980655660739395, 0.0628511503532736, 0.23402202952558848, 0.40545151458503387]
maxi score, test score, baseline:  0.09781999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05918866127109687, 0.0586800876576132, 0.17980655660739395, 0.0628511503532736, 0.23402202952558848, 0.40545151458503387]
actor:  1 policy actor:  1  step number:  43 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09781999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.05918866127109687, 0.0586800876576132, 0.17980655660739395, 0.0628511503532736, 0.23402202952558848, 0.40545151458503387]
printing an ep nov before normalisation:  29.71811532974243
maxi score, test score, baseline:  0.09781999999999988 0.6759999999999999 0.6759999999999999
actor:  1 policy actor:  1  step number:  22 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  50.081712418213534
printing an ep nov before normalisation:  52.7618077455291
actor:  1 policy actor:  1  step number:  20 total reward:  0.69  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  36.105955185237875
maxi score, test score, baseline:  0.09781999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.0589583264885902, 0.05845173647227421, 0.17910577406821734, 0.06260653071056674, 0.23700698657306024, 0.4038706456872912]
printing an ep nov before normalisation:  29.434661865234375
actor:  1 policy actor:  1  step number:  34 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  39.61822679889162
printing an ep nov before normalisation:  19.216571150901952
line 256 mcts: sample exp_bonus 34.804916821665266
printing an ep nov before normalisation:  45.84720242050481
actor:  1 policy actor:  1  step number:  39 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]]
maxi score, test score, baseline:  0.09781999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.0589583264885902, 0.05845173647227421, 0.17910577406821734, 0.06260653071056674, 0.23700698657306024, 0.4038706456872912]
actor:  1 policy actor:  1  step number:  37 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  39.11111831665039
actor:  1 policy actor:  1  step number:  35 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.685]
 [0.685]
 [0.749]] [[58.151]
 [58.151]
 [58.151]
 [58.151]
 [55.715]
 [54.108]
 [58.151]] [[2.314]
 [2.314]
 [2.314]
 [2.314]
 [2.155]
 [2.093]
 [2.314]]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  22 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]] [[56.414]
 [56.414]
 [56.414]
 [56.414]
 [56.414]
 [56.414]
 [56.414]] [[2.25]
 [2.25]
 [2.25]
 [2.25]
 [2.25]
 [2.25]
 [2.25]]
maxi score, test score, baseline:  0.09781999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.0589583264885902, 0.05845173647227421, 0.17910577406821734, 0.06260653071056674, 0.23700698657306024, 0.4038706456872912]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.09781999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.0589583264885902, 0.05845173647227421, 0.17910577406821734, 0.06260653071056674, 0.23700698657306024, 0.4038706456872912]
printing an ep nov before normalisation:  35.16878199734942
maxi score, test score, baseline:  0.09781999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.0589583264885902, 0.05845173647227421, 0.17910577406821734, 0.06260653071056674, 0.23700698657306024, 0.4038706456872912]
printing an ep nov before normalisation:  28.42090200585337
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  51.05650833735414
actor:  1 policy actor:  1  step number:  26 total reward:  0.61  reward:  1.0 rdn_beta:  0.333
using another actor
printing an ep nov before normalisation:  39.28124445973305
maxi score, test score, baseline:  0.09781999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.058956812943398704, 0.05845183033658291, 0.17910606212658026, 0.06260663126206438, 0.23700736782408713, 0.4038712955072866]
maxi score, test score, baseline:  0.09781999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.058957333364621244, 0.05845234629014737, 0.17910764552650202, 0.06259834389360701, 0.2370094634856754, 0.40387486743944684]
printing an ep nov before normalisation:  49.97345430467785
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.09781999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.058957333364621244, 0.05845234629014737, 0.17910764552650202, 0.06259834389360701, 0.2370094634856754, 0.40387486743944684]
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.557]
 [0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.053]] [[42.373]
 [47.733]
 [29.374]
 [29.374]
 [29.374]
 [29.374]
 [29.374]] [[0.957]
 [1.039]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]]
maxi score, test score, baseline:  0.09781999999999988 0.6759999999999999 0.6759999999999999
maxi score, test score, baseline:  0.09781999999999988 0.6759999999999999 0.6759999999999999
probs:  [0.058957333364621244, 0.05845234629014737, 0.17910764552650202, 0.06259834389360701, 0.2370094634856754, 0.40387486743944684]
actor:  0 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.441]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]] [[34.559]
 [49.4  ]
 [34.559]
 [34.559]
 [34.559]
 [34.559]
 [34.559]] [[0.849]
 [1.207]
 [0.849]
 [0.849]
 [0.849]
 [0.849]
 [0.849]]
actor:  1 policy actor:  1  step number:  25 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.10095999999999987 0.6759999999999999 0.6759999999999999
printing an ep nov before normalisation:  37.19399341829587
maxi score, test score, baseline:  0.10095999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.058957333364621244, 0.05845234629014737, 0.17910764552650202, 0.06259834389360701, 0.2370094634856754, 0.40387486743944684]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.10095999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.058957333364621244, 0.05845234629014737, 0.17910764552650202, 0.06259834389360701, 0.2370094634856754, 0.40387486743944684]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10095999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.058957333364621244, 0.05845234629014737, 0.17910764552650202, 0.06259834389360701, 0.2370094634856754, 0.40387486743944684]
Starting evaluation
actor:  1 policy actor:  1  step number:  22 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8177118
printing an ep nov before normalisation:  60.76102117956225
printing an ep nov before normalisation:  48.48851334658684
printing an ep nov before normalisation:  40.895126981649184
Printing some Q and Qe and total Qs values:  [[0.951]
 [0.974]
 [0.951]
 [0.951]
 [0.951]
 [0.951]
 [0.951]] [[36.85 ]
 [50.274]
 [36.85 ]
 [36.85 ]
 [36.85 ]
 [36.85 ]
 [36.85 ]] [[0.951]
 [0.974]
 [0.951]
 [0.951]
 [0.951]
 [0.951]
 [0.951]]
printing an ep nov before normalisation:  45.48269780378999
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  44.181494998292095
printing an ep nov before normalisation:  39.43551743036655
printing an ep nov before normalisation:  47.07433117338865
printing an ep nov before normalisation:  49.53813970307688
printing an ep nov before normalisation:  34.79304442526484
Printing some Q and Qe and total Qs values:  [[0.916]
 [0.99 ]
 [0.919]
 [0.919]
 [0.919]
 [0.919]
 [0.952]] [[40.062]
 [41.221]
 [40.308]
 [40.308]
 [40.308]
 [40.308]
 [39.957]] [[0.916]
 [0.99 ]
 [0.919]
 [0.919]
 [0.919]
 [0.919]
 [0.952]]
printing an ep nov before normalisation:  56.03684318535893
line 256 mcts: sample exp_bonus 34.289721249652246
maxi score, test score, baseline:  0.09797999999999987 0.6759999999999999 0.6759999999999999
probs:  [0.058957333364621244, 0.05845234629014737, 0.17910764552650202, 0.06259834389360701, 0.2370094634856754, 0.40387486743944684]
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  37.84383373602799
actor:  0 policy actor:  0  step number:  21 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.646]
 [0.676]
 [0.656]
 [0.644]
 [0.642]
 [0.647]
 [0.635]] [[40.384]
 [34.217]
 [39.318]
 [41.145]
 [41.777]
 [41.274]
 [34.765]] [[0.646]
 [0.676]
 [0.656]
 [0.644]
 [0.642]
 [0.647]
 [0.635]]
maxi score, test score, baseline:  0.13273999999999986 0.6775 0.6775
actor:  1 policy actor:  1  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.13273999999999986 0.6775 0.6775
actor:  1 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  45 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.13197999999999988 0.6775 0.6775
probs:  [0.0589982933618304, 0.05878947963038249, 0.1802125763300215, 0.06228822034347224, 0.23375450521057722, 0.4059569251237163]
Printing some Q and Qe and total Qs values:  [[0.633]
 [0.498]
 [0.498]
 [0.32 ]
 [0.523]
 [0.498]
 [0.498]] [[36.153]
 [31.35 ]
 [31.35 ]
 [34.139]
 [36.452]
 [31.35 ]
 [31.35 ]] [[1.137]
 [0.882]
 [0.882]
 [0.773]
 [1.034]
 [0.882]
 [0.882]]
maxi score, test score, baseline:  0.13197999999999988 0.6775 0.6775
probs:  [0.0589982933618304, 0.05878947963038249, 0.1802125763300215, 0.06228822034347224, 0.23375450521057722, 0.4059569251237163]
printing an ep nov before normalisation:  35.87374938793407
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  24 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  31.84700065942313
printing an ep nov before normalisation:  27.58835190873234
actor:  1 policy actor:  1  step number:  35 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  49.51557490920185
actions average: 
K:  1  action  0 :  tensor([0.6790, 0.0012, 0.0552, 0.0562, 0.0951, 0.0655, 0.0478],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0065, 0.9413, 0.0064, 0.0116, 0.0049, 0.0035, 0.0258],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0430,     0.0003,     0.7759,     0.0605,     0.0529,     0.0405,
            0.0269], grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1335, 0.0058, 0.1005, 0.3644, 0.1455, 0.1300, 0.1203],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.3000, 0.0011, 0.0407, 0.0509, 0.5109, 0.0486, 0.0479],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1426, 0.0009, 0.1002, 0.1232, 0.1479, 0.3592, 0.1260],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0983, 0.2551, 0.0688, 0.0778, 0.0948, 0.0744, 0.3308],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  19 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1321599999999999 0.6775 0.6775
probs:  [0.05699203275482235, 0.05676602727292409, 0.18818595494194304, 0.06055282125153854, 0.24613602055547537, 0.39136714322329663]
maxi score, test score, baseline:  0.1321599999999999 0.6775 0.6775
probs:  [0.05699203275482235, 0.05676602727292409, 0.18818595494194304, 0.06055282125153854, 0.24613602055547537, 0.39136714322329663]
printing an ep nov before normalisation:  33.625146231146594
maxi score, test score, baseline:  0.1321599999999999 0.6775 0.6775
probs:  [0.05699203275482235, 0.05676602727292409, 0.18818595494194304, 0.06055282125153854, 0.24613602055547537, 0.39136714322329663]
printing an ep nov before normalisation:  45.95163718628586
actor:  1 policy actor:  1  step number:  33 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1321599999999999 0.6775 0.6775
probs:  [0.05699203275482235, 0.05676602727292409, 0.18818595494194304, 0.06055282125153854, 0.24613602055547537, 0.39136714322329663]
actor:  1 policy actor:  1  step number:  19 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.003]
 [-0.003]
 [ 0.209]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.003]
 [-0.003]
 [ 0.209]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]]
maxi score, test score, baseline:  0.1321599999999999 0.6775 0.6775
probs:  [0.05699203275482235, 0.05676602727292409, 0.18818595494194304, 0.06055282125153854, 0.24613602055547537, 0.39136714322329663]
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.82 ]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.571]
 [0.82 ]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]]
maxi score, test score, baseline:  0.1321599999999999 0.6775 0.6775
probs:  [0.05699203275482235, 0.05676602727292409, 0.18818595494194304, 0.06055282125153854, 0.24613602055547537, 0.39136714322329663]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.63 ]
 [0.729]
 [0.588]
 [0.59 ]
 [0.59 ]
 [0.591]
 [0.591]] [[31.378]
 [33.536]
 [30.703]
 [30.567]
 [29.962]
 [30.03 ]
 [29.966]] [[0.63 ]
 [0.729]
 [0.588]
 [0.59 ]
 [0.59 ]
 [0.591]
 [0.591]]
actor:  1 policy actor:  1  step number:  36 total reward:  0.4099999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1321599999999999 0.6775 0.6775
probs:  [0.05699203275482235, 0.05676602727292409, 0.18818595494194304, 0.06055282125153854, 0.24613602055547537, 0.39136714322329663]
maxi score, test score, baseline:  0.1321599999999999 0.6775 0.6775
probs:  [0.05699203275482235, 0.05676602727292409, 0.18818595494194304, 0.06055282125153854, 0.24613602055547537, 0.39136714322329663]
actor:  0 policy actor:  1  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.249]
 [0.734]
 [0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.249]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.249]
 [0.734]
 [0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.249]]
actor:  1 policy actor:  1  step number:  36 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12893999999999986 0.6775 0.6775
probs:  [0.05699203275482235, 0.05676602727292409, 0.18818595494194304, 0.06055282125153854, 0.24613602055547537, 0.39136714322329663]
printing an ep nov before normalisation:  52.56387881598784
maxi score, test score, baseline:  0.12893999999999986 0.6775 0.6775
actor:  1 policy actor:  1  step number:  26 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  39.994471073150635
printing an ep nov before normalisation:  44.200883049290304
Printing some Q and Qe and total Qs values:  [[0.657]
 [0.804]
 [0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]] [[42.055]
 [59.804]
 [42.055]
 [42.055]
 [42.055]
 [42.055]
 [42.055]] [[1.826]
 [2.804]
 [1.826]
 [1.826]
 [1.826]
 [1.826]
 [1.826]]
Printing some Q and Qe and total Qs values:  [[0.738]
 [0.821]
 [0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]] [[33.857]
 [39.134]
 [33.857]
 [33.857]
 [33.857]
 [33.857]
 [33.857]] [[2.322]
 [2.653]
 [2.322]
 [2.322]
 [2.322]
 [2.322]
 [2.322]]
maxi score, test score, baseline:  0.12893999999999986 0.6775 0.6775
probs:  [0.05699135824547975, 0.05676606780954433, 0.1881860895463058, 0.06055286449866979, 0.2461361966393304, 0.39136742326067]
maxi score, test score, baseline:  0.12893999999999986 0.6775 0.6775
probs:  [0.05699135824547975, 0.05676606780954433, 0.1881860895463058, 0.06055286449866979, 0.2461361966393304, 0.39136742326067]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  35 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.12893999999999986 0.6775 0.6775
printing an ep nov before normalisation:  0.03379526497781171
actor:  1 policy actor:  1  step number:  19 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  37.23169297912948
printing an ep nov before normalisation:  0.2671611489573422
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  56.27814760925412
maxi score, test score, baseline:  0.12893999999999986 0.6775 0.6775
probs:  [0.05699135824547975, 0.05676606780954433, 0.1881860895463058, 0.06055286449866979, 0.2461361966393304, 0.39136742326067]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
siam score:  -0.82137316
siam score:  -0.82090485
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  33 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.677]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.598]
 [0.677]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]]
actor:  0 policy actor:  0  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.13205999999999984 0.6775 0.6775
maxi score, test score, baseline:  0.13205999999999984 0.6775 0.6775
probs:  [0.05699068799075993, 0.05676610809047052, 0.1881862233016206, 0.060552907473009905, 0.246136371612496, 0.391367701531643]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  23 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.0
from probs:  [0.05699068799075993, 0.05676610809047052, 0.1881862233016206, 0.060552907473009905, 0.246136371612496, 0.391367701531643]
actor:  1 policy actor:  1  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  32 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.173]
 [0.298]
 [0.173]
 [0.173]
 [0.173]
 [0.173]
 [0.173]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.173]
 [0.298]
 [0.173]
 [0.173]
 [0.173]
 [0.173]
 [0.173]]
printing an ep nov before normalisation:  41.234068391773704
maxi score, test score, baseline:  0.13189999999999985 0.6775 0.6775
probs:  [0.05699068799075993, 0.05676610809047052, 0.1881862233016206, 0.060552907473009905, 0.246136371612496, 0.391367701531643]
maxi score, test score, baseline:  0.13189999999999985 0.6775 0.6775
probs:  [0.05699068799075993, 0.05676610809047052, 0.1881862233016206, 0.060552907473009905, 0.246136371612496, 0.391367701531643]
maxi score, test score, baseline:  0.13189999999999985 0.6775 0.6775
probs:  [0.05699068799075993, 0.05676610809047052, 0.1881862233016206, 0.060552907473009905, 0.246136371612496, 0.391367701531643]
line 256 mcts: sample exp_bonus 36.671080875289626
maxi score, test score, baseline:  0.13189999999999985 0.6775 0.6775
probs:  [0.05699068799075993, 0.05676610809047052, 0.1881862233016206, 0.060552907473009905, 0.246136371612496, 0.391367701531643]
actor:  0 policy actor:  0  step number:  24 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.6022],
        [0.6685],
        [0.5759],
        [0.2774],
        [0.1400],
        [0.9308],
        [0.4788],
        [0.1977],
        [0.0000],
        [0.5451]], dtype=torch.float64)
-0.06831985059899999 0.5339121529246873
-0.048519850599000006 0.6200256190742692
-0.145559551797 0.4303494854109894
-0.08675157179700001 0.1906811127417437
-0.048519850599000006 0.09144826715097414
-0.08752783059899999 0.8433127466686536
-0.145559551797 0.33326565610752523
-0.048519850599000006 0.14922037176173403
-0.7253729999999999 -0.7253729999999999
-0.145559551797 0.39950446977768694
actor:  0 policy actor:  0  step number:  25 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.13477999999999984 0.6775 0.6775
probs:  [0.05699068799075993, 0.05676610809047052, 0.1881862233016206, 0.060552907473009905, 0.246136371612496, 0.391367701531643]
printing an ep nov before normalisation:  22.69927956777963
actor:  1 policy actor:  1  step number:  39 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.614]
 [0.675]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]] [[36.106]
 [45.023]
 [36.106]
 [36.106]
 [36.106]
 [36.106]
 [36.106]] [[0.91 ]
 [1.131]
 [0.91 ]
 [0.91 ]
 [0.91 ]
 [0.91 ]
 [0.91 ]]
printing an ep nov before normalisation:  39.57644782183813
maxi score, test score, baseline:  0.13807999999999987 0.6775 0.6775
probs:  [0.05699068799075993, 0.05676610809047052, 0.1881862233016206, 0.060552907473009905, 0.246136371612496, 0.391367701531643]
maxi score, test score, baseline:  0.13807999999999987 0.6775 0.6775
printing an ep nov before normalisation:  42.10429939589551
maxi score, test score, baseline:  0.13807999999999987 0.6775 0.6775
probs:  [0.05699068799075993, 0.05676610809047052, 0.1881862233016206, 0.060552907473009905, 0.246136371612496, 0.391367701531643]
printing an ep nov before normalisation:  0.029848991769654276
maxi score, test score, baseline:  0.13807999999999987 0.6775 0.6775
probs:  [0.05699068799075993, 0.05676610809047052, 0.1881862233016206, 0.060552907473009905, 0.246136371612496, 0.391367701531643]
printing an ep nov before normalisation:  24.458286444365758
maxi score, test score, baseline:  0.13807999999999987 0.6775 0.6775
probs:  [0.05699114548922955, 0.05676656378187813, 0.18818773645321507, 0.06054535365610355, 0.2461383510547687, 0.39137084956480506]
actor:  1 policy actor:  1  step number:  36 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
from probs:  [0.05699114548922955, 0.05676656378187813, 0.18818773645321507, 0.06054535365610355, 0.2461383510547687, 0.39137084956480506]
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  32 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.13807999999999987 0.6775 0.6775
probs:  [0.05699114548922955, 0.05676656378187813, 0.18818773645321507, 0.06054535365610355, 0.2461383510547687, 0.39137084956480506]
actor:  1 policy actor:  1  step number:  40 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  56.28190332640759
siam score:  -0.82375526
maxi score, test score, baseline:  0.13807999999999987 0.6775 0.6775
probs:  [0.056990479443967935, 0.056766603810166065, 0.18818786936962892, 0.06054539635522995, 0.24613852493051877, 0.39137112609048835]
printing an ep nov before normalisation:  0.007176279316354339
actor:  0 policy actor:  1  step number:  23 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.875]
 [0.726]
 [0.726]
 [0.726]
 [0.837]
 [0.726]
 [0.726]] [[60.015]
 [70.582]
 [70.582]
 [70.582]
 [62.922]
 [70.582]
 [70.582]] [[2.16 ]
 [2.393]
 [2.393]
 [2.393]
 [2.228]
 [2.393]
 [2.393]]
maxi score, test score, baseline:  0.14119999999999985 0.6775 0.6775
probs:  [0.056990479443967935, 0.056766603810166065, 0.18818786936962892, 0.06054539635522995, 0.24613852493051877, 0.39137112609048835]
printing an ep nov before normalisation:  31.173906326293945
maxi score, test score, baseline:  0.14119999999999985 0.6775 0.6775
probs:  [0.056990479443967935, 0.056766603810166065, 0.18818786936962892, 0.06054539635522995, 0.24613852493051877, 0.39137112609048835]
actor:  1 policy actor:  1  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  39 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.14119999999999985 0.6775 0.6775
probs:  [0.05699027313084081, 0.05676709735662653, 0.1881895082212754, 0.06053791684679336, 0.24614066880839572, 0.3913745356360681]
printing an ep nov before normalisation:  46.27174392709102
actor:  0 policy actor:  1  step number:  20 total reward:  0.69  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.14457999999999988 0.6775 0.6775
probs:  [0.05699027313084081, 0.05676709735662653, 0.1881895082212754, 0.06053791684679336, 0.24614066880839572, 0.3913745356360681]
actor:  1 policy actor:  1  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  41.542181959758416
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  39.77195461496194
actor:  1 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.14457999999999988 0.6775 0.6775
probs:  [0.0569896153912954, 0.056767136886089635, 0.1881896394813106, 0.06053795900822323, 0.2461408405173396, 0.39137480871574154]
maxi score, test score, baseline:  0.14457999999999988 0.6775 0.6775
probs:  [0.0569896153912954, 0.056767136886089635, 0.1881896394813106, 0.06053795900822323, 0.2461408405173396, 0.39137480871574154]
actor:  1 policy actor:  1  step number:  26 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  45.454419333839844
actor:  1 policy actor:  1  step number:  23 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  2.0
from probs:  [0.0569896153912954, 0.056767136886089635, 0.1881896394813106, 0.06053795900822323, 0.2461408405173396, 0.39137480871574154]
actor:  1 policy actor:  1  step number:  43 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
Printing some Q and Qe and total Qs values:  [[0.827]
 [0.894]
 [0.874]
 [0.874]
 [0.874]
 [0.874]
 [0.874]] [[47.062]
 [40.632]
 [57.89 ]
 [57.89 ]
 [48.647]
 [57.89 ]
 [57.89 ]] [[0.827]
 [0.894]
 [0.874]
 [0.874]
 [0.874]
 [0.874]
 [0.874]]
maxi score, test score, baseline:  0.14481999999999984 0.6775 0.6775
probs:  [0.05698896174890643, 0.056767176169317916, 0.18818976992370764, 0.06053800090702351, 0.24614101115668308, 0.3913750800943615]
printing an ep nov before normalisation:  0.004529521100664624
actor:  0 policy actor:  1  step number:  31 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
using another actor
from probs:  [0.05698896174890643, 0.056767176169317916, 0.18818976992370764, 0.06053800090702351, 0.24614101115668308, 0.3913750800943615]
maxi score, test score, baseline:  0.14797999999999986 0.6775 0.6775
probs:  [0.05698896174890643, 0.056767176169317916, 0.18818976992370764, 0.06053800090702351, 0.24614101115668308, 0.3913750800943615]
printing an ep nov before normalisation:  28.212404251098633
actor:  1 policy actor:  1  step number:  31 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.14797999999999986 0.6775 0.6775
probs:  [0.05698896174890643, 0.056767176169317916, 0.18818976992370764, 0.06053800090702351, 0.24614101115668308, 0.3913750800943615]
Printing some Q and Qe and total Qs values:  [[0.392]
 [0.356]
 [0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.392]] [[32.548]
 [43.266]
 [32.548]
 [32.548]
 [32.548]
 [32.548]
 [32.548]] [[1.078]
 [1.461]
 [1.078]
 [1.078]
 [1.078]
 [1.078]
 [1.078]]
actor:  1 policy actor:  1  step number:  37 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.14797999999999986 0.6775 0.6775
probs:  [0.05698896174890643, 0.056767176169317916, 0.18818976992370764, 0.06053800090702351, 0.24614101115668308, 0.3913750800943615]
actor:  1 policy actor:  1  step number:  20 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  7.300320476133493e-05
from probs:  [0.05698896174890643, 0.056767176169317916, 0.18818976992370764, 0.06053800090702351, 0.24614101115668308, 0.3913750800943615]
maxi score, test score, baseline:  0.14797999999999986 0.6775 0.6775
probs:  [0.05698896174890643, 0.056767176169317916, 0.18818976992370764, 0.06053800090702351, 0.24614101115668308, 0.3913750800943615]
maxi score, test score, baseline:  0.14797999999999986 0.6775 0.6775
maxi score, test score, baseline:  0.14797999999999986 0.6775 0.6775
probs:  [0.05698896174890643, 0.056767176169317916, 0.18818976992370764, 0.06053800090702351, 0.24614101115668308, 0.3913750800943615]
maxi score, test score, baseline:  0.14797999999999986 0.6775 0.6775
actor:  1 policy actor:  1  step number:  28 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  34 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  19 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  62.62584874787628
maxi score, test score, baseline:  0.14747999999999983 0.6775 0.6775
probs:  [0.056996474259388086, 0.05677475184554273, 0.18815991572818833, 0.060544502631789406, 0.24609465213147813, 0.3914297034036134]
printing an ep nov before normalisation:  66.06801173485746
line 256 mcts: sample exp_bonus 60.31258995527106
maxi score, test score, baseline:  0.14747999999999983 0.6775 0.6775
probs:  [0.056996474259388086, 0.05677475184554273, 0.18815991572818833, 0.060544502631789406, 0.24609465213147813, 0.3914297034036134]
printing an ep nov before normalisation:  62.249582092747396
actor:  1 policy actor:  1  step number:  19 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  29.611515359499805
actor:  1 policy actor:  1  step number:  29 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.14763999999999983 0.6775 0.6775
printing an ep nov before normalisation:  44.42436677227092
printing an ep nov before normalisation:  54.805934134845934
maxi score, test score, baseline:  0.14763999999999983 0.6775 0.6775
actor:  1 policy actor:  1  step number:  33 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  42 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.14763999999999983 0.6775 0.6775
probs:  [0.05699582486617318, 0.05677479087893185, 0.18816004530306987, 0.06054454426302775, 0.24609482163093993, 0.3914299730578574]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.78949980278588
Printing some Q and Qe and total Qs values:  [[0.73]
 [0.73]
 [0.73]
 [0.73]
 [0.73]
 [0.73]
 [0.73]] [[47.296]
 [47.296]
 [47.296]
 [47.296]
 [47.296]
 [47.296]
 [47.296]] [[0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]]
maxi score, test score, baseline:  0.14763999999999983 0.6775 0.6775
probs:  [0.05699582486617318, 0.05677479087893185, 0.18816004530306987, 0.06054454426302775, 0.24609482163093993, 0.3914299730578574]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  33 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  0.06353078343863672
printing an ep nov before normalisation:  0.11613496244137878
printing an ep nov before normalisation:  33.76236501649458
printing an ep nov before normalisation:  0.23215275043355632
actor:  1 policy actor:  1  step number:  39 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.333
from probs:  [0.0569951794930824, 0.056774829670681405, 0.1881601740758072, 0.06054458563654448, 0.2460949900811009, 0.3914302410427837]
actor:  1 policy actor:  1  step number:  35 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  37 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.14763999999999983 0.6775 0.6775
probs:  [0.056995633042514315, 0.05677528146253137, 0.18816167383990293, 0.060537097547861005, 0.24609695195213477, 0.39143336215505564]
printing an ep nov before normalisation:  44.44518356179844
Printing some Q and Qe and total Qs values:  [[0.81 ]
 [0.81 ]
 [0.81 ]
 [0.781]
 [0.79 ]
 [0.786]
 [0.806]] [[41.962]
 [34.785]
 [34.432]
 [40.864]
 [44.767]
 [41.815]
 [33.233]] [[0.81 ]
 [0.81 ]
 [0.81 ]
 [0.781]
 [0.79 ]
 [0.786]
 [0.806]]
printing an ep nov before normalisation:  41.428088625521355
actor:  0 policy actor:  0  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.15071999999999985 0.6775 0.6775
probs:  [0.056995633042514315, 0.05677528146253137, 0.18816167383990293, 0.060537097547861005, 0.24609695195213477, 0.39143336215505564]
printing an ep nov before normalisation:  0.005421034717443263
actor:  1 policy actor:  1  step number:  27 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  21 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.15091999999999983 0.6775 0.6775
probs:  [0.056995633042514315, 0.05677528146253137, 0.18816167383990293, 0.060537097547861005, 0.24609695195213477, 0.39143336215505564]
Printing some Q and Qe and total Qs values:  [[0.308]
 [0.308]
 [0.481]
 [0.308]
 [0.308]
 [0.308]
 [0.308]] [[43.794]
 [43.794]
 [49.397]
 [43.794]
 [43.794]
 [43.794]
 [43.794]] [[0.737]
 [0.737]
 [0.997]
 [0.737]
 [0.737]
 [0.737]
 [0.737]]
printing an ep nov before normalisation:  64.26920971993363
printing an ep nov before normalisation:  62.08535162999293
siam score:  -0.81959045
actor:  1 policy actor:  1  step number:  27 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  23 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.15091999999999983 0.6775 0.6775
probs:  [0.056995633042514315, 0.05677528146253137, 0.18816167383990293, 0.060537097547861005, 0.24609695195213477, 0.39143336215505564]
maxi score, test score, baseline:  0.15091999999999983 0.6775 0.6775
probs:  [0.056995633042514315, 0.05677528146253137, 0.18816167383990293, 0.060537097547861005, 0.24609695195213477, 0.39143336215505564]
actor:  0 policy actor:  0  step number:  23 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  0.042899521957622255
actor:  1 policy actor:  1  step number:  27 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.136]
 [0.441]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.136]
 [0.441]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]]
maxi score, test score, baseline:  0.15399999999999986 0.6775 0.6775
probs:  [0.056995633042514315, 0.05677528146253137, 0.18816167383990293, 0.060537097547861005, 0.24609695195213477, 0.39143336215505564]
Printing some Q and Qe and total Qs values:  [[0.738]
 [0.826]
 [0.741]
 [0.776]
 [0.776]
 [0.741]
 [0.745]] [[12.519]
 [24.887]
 [13.24 ]
 [15.584]
 [15.584]
 [12.485]
 [12.73 ]] [[1.185]
 [2.159]
 [1.24 ]
 [1.442]
 [1.442]
 [1.185]
 [1.206]]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.184]
 [0.204]
 [0.184]
 [0.186]
 [0.192]
 [0.184]
 [0.181]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.184]
 [0.204]
 [0.184]
 [0.186]
 [0.192]
 [0.184]
 [0.181]]
printing an ep nov before normalisation:  26.10281467437744
printing an ep nov before normalisation:  31.14711581066512
actor:  1 policy actor:  1  step number:  31 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  32 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.07]
 [0.07]
 [0.07]
 [0.07]
 [0.07]
 [0.07]
 [0.07]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.07]
 [0.07]
 [0.07]
 [0.07]
 [0.07]
 [0.07]
 [0.07]]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  49.104893349778926
maxi score, test score, baseline:  0.15399999999999986 0.6775 0.6775
probs:  [0.05699608468542502, 0.05677573135524843, 0.18816316729965415, 0.060529640935787624, 0.24609890557633132, 0.39143647014755345]
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.498]
 [0.498]
 [0.498]
 [0.524]
 [0.498]
 [0.498]] [[39.682]
 [34.489]
 [34.489]
 [34.489]
 [42.252]
 [34.489]
 [34.489]] [[1.774]
 [1.493]
 [1.493]
 [1.493]
 [1.886]
 [1.493]
 [1.493]]
Printing some Q and Qe and total Qs values:  [[0.318]
 [0.279]
 [0.318]
 [0.318]
 [0.318]
 [0.318]
 [0.318]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.318]
 [0.279]
 [0.318]
 [0.318]
 [0.318]
 [0.318]
 [0.318]]
maxi score, test score, baseline:  0.15399999999999986 0.6775 0.6775
probs:  [0.05699608468542502, 0.05677573135524843, 0.18816316729965415, 0.060529640935787624, 0.24609890557633132, 0.39143647014755345]
actor:  1 policy actor:  1  step number:  31 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  54.05212253415654
maxi score, test score, baseline:  0.15399999999999986 0.6775 0.6775
siam score:  -0.8132017
maxi score, test score, baseline:  0.15399999999999986 0.6775 0.6775
probs:  [0.05699608468542502, 0.05677573135524843, 0.18816316729965415, 0.060529640935787624, 0.24609890557633132, 0.39143647014755345]
maxi score, test score, baseline:  0.15399999999999986 0.6775 0.6775
actor:  1 policy actor:  1  step number:  37 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  23 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.15399999999999986 0.6775 0.6775
Printing some Q and Qe and total Qs values:  [[0.311]
 [0.489]
 [0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]] [[28.371]
 [37.667]
 [28.371]
 [28.371]
 [28.371]
 [28.371]
 [28.371]] [[0.655]
 [1.095]
 [0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.655]]
printing an ep nov before normalisation:  36.7454534267952
printing an ep nov before normalisation:  41.917419920650325
printing an ep nov before normalisation:  8.367287591681816e-06
actor:  0 policy actor:  0  step number:  20 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  39 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  24 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1572199999999999 0.6775 0.6775
printing an ep nov before normalisation:  35.859775672243984
actor:  1 policy actor:  1  step number:  21 total reward:  0.7  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  46.253737985757844
siam score:  -0.81024504
actions average: 
K:  3  action  0 :  tensor([0.7326, 0.0165, 0.0452, 0.0465, 0.0636, 0.0446, 0.0510],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0017,     0.9893,     0.0018,     0.0008,     0.0003,     0.0003,
            0.0058], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0122,     0.0001,     0.9212,     0.0160,     0.0153,     0.0197,
            0.0155], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1025, 0.0052, 0.1201, 0.3995, 0.1174, 0.1197, 0.1356],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1651, 0.0020, 0.1362, 0.1127, 0.3001, 0.1609, 0.1231],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1049, 0.0288, 0.2129, 0.1043, 0.0978, 0.3464, 0.1049],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0788, 0.1318, 0.0633, 0.0915, 0.0438, 0.0713, 0.5195],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  40.06911454521267
Printing some Q and Qe and total Qs values:  [[0.929]
 [0.961]
 [0.929]
 [0.929]
 [0.929]
 [0.929]
 [0.929]] [[39.874]
 [45.611]
 [39.874]
 [39.874]
 [39.874]
 [39.874]
 [39.874]] [[0.929]
 [0.961]
 [0.929]
 [0.929]
 [0.929]
 [0.929]
 [0.929]]
maxi score, test score, baseline:  0.1572199999999999 0.6775 0.6775
probs:  [0.07348279397282301, 0.05578474009395773, 0.18487348176670035, 0.05947297299554283, 0.2417956029593396, 0.38459040821163654]
actor:  0 policy actor:  0  step number:  22 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  2.0
from probs:  [0.07348279397282301, 0.05578474009395773, 0.18487348176670035, 0.05947297299554283, 0.2417956029593396, 0.38459040821163654]
actor:  1 policy actor:  1  step number:  38 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  41 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]]
actor:  1 policy actor:  1  step number:  29 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  30 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  26 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  1  action  0 :  tensor([0.5551, 0.0063, 0.0702, 0.0575, 0.1635, 0.0807, 0.0666],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0012,     0.9781,     0.0016,     0.0038,     0.0008,     0.0006,
            0.0139], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0490, 0.0371, 0.7017, 0.0429, 0.0358, 0.0819, 0.0515],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0785, 0.2144, 0.0944, 0.3094, 0.0598, 0.1075, 0.1359],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1476, 0.0527, 0.1189, 0.1058, 0.3254, 0.1275, 0.1221],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0988, 0.0174, 0.1470, 0.1402, 0.1183, 0.3297, 0.1486],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0605, 0.0975, 0.0833, 0.0723, 0.0660, 0.0820, 0.5385],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.339]
 [0.48 ]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]] [[33.598]
 [41.263]
 [33.598]
 [33.598]
 [33.598]
 [33.598]
 [33.598]] [[0.94 ]
 [1.356]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [0.94 ]
 [0.94 ]]
maxi score, test score, baseline:  0.15783999999999987 0.6775 0.6775
probs:  [0.07348336397032894, 0.05578517256036498, 0.18487491737823294, 0.059465669361923504, 0.2417974809111387, 0.38459339581801083]
maxi score, test score, baseline:  0.15783999999999987 0.6775 0.6775
probs:  [0.07348336397032894, 0.05578517256036498, 0.18487491737823294, 0.059465669361923504, 0.2417974809111387, 0.38459339581801083]
actor:  1 policy actor:  1  step number:  32 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.15783999999999987 0.6775 0.6775
probs:  [0.07348336397032894, 0.05578517256036498, 0.18487491737823294, 0.059465669361923504, 0.2417974809111387, 0.38459339581801083]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  40 total reward:  0.2899999999999996  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  30.138261318206787
printing an ep nov before normalisation:  33.02842855453491
maxi score, test score, baseline:  0.15783999999999987 0.6775 0.6775
probs:  [0.07348393158169064, 0.05578560321636558, 0.18487634697995614, 0.05945839630304152, 0.24179935100138755, 0.38459637091755855]
actor:  1 policy actor:  1  step number:  27 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  31 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.15783999999999987 0.6775 0.6775
probs:  [0.07348449682186026, 0.055786032073303994, 0.184877770609529, 0.05945115362730796, 0.24180121327934842, 0.38459933358865034]
maxi score, test score, baseline:  0.15783999999999987 0.6775 0.6775
probs:  [0.07348449682186026, 0.055786032073303994, 0.184877770609529, 0.05945115362730796, 0.24180121327934842, 0.38459933358865034]
actor:  1 policy actor:  1  step number:  36 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  36.5084193622646
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  3  action  0 :  tensor([0.5569, 0.0111, 0.0671, 0.0621, 0.1665, 0.0618, 0.0744],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0076, 0.9532, 0.0088, 0.0102, 0.0053, 0.0030, 0.0118],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0672, 0.0009, 0.6941, 0.0632, 0.0639, 0.0521, 0.0586],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0721, 0.1201, 0.0810, 0.3873, 0.0887, 0.0872, 0.1637],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1145, 0.0030, 0.0825, 0.0701, 0.5735, 0.0753, 0.0811],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1194, 0.0396, 0.1424, 0.1898, 0.1588, 0.1630, 0.1869],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0938, 0.1717, 0.1609, 0.1092, 0.0730, 0.0703, 0.3212],
       grad_fn=<DivBackward0>)
siam score:  -0.81354433
actor:  1 policy actor:  1  step number:  41 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.15783999999999987 0.6775 0.6775
probs:  [0.07348449682186026, 0.055786032073303994, 0.184877770609529, 0.05945115362730796, 0.24180121327934842, 0.38459933358865034]
printing an ep nov before normalisation:  42.11650163509692
printing an ep nov before normalisation:  42.112756035667275
maxi score, test score, baseline:  0.15783999999999987 0.6775 0.6775
probs:  [0.07348449682186026, 0.055786032073303994, 0.184877770609529, 0.05945115362730796, 0.24180121327934842, 0.38459933358865034]
actor:  1 policy actor:  1  step number:  28 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  2.0
from probs:  [0.07348449682186026, 0.055786032073303994, 0.184877770609529, 0.05945115362730796, 0.24180121327934842, 0.38459933358865034]
maxi score, test score, baseline:  0.15783999999999987 0.6775 0.6775
probs:  [0.07343403492915372, 0.05578906533765963, 0.18488783980615886, 0.05945438665306833, 0.24181438499432675, 0.38462028827963274]
maxi score, test score, baseline:  0.15783999999999987 0.6775 0.6775
actor:  0 policy actor:  1  step number:  31 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.16055999999999987 0.6775 0.6775
probs:  [0.07343403492915372, 0.05578906533765963, 0.18488783980615886, 0.05945438665306833, 0.24181438499432675, 0.38462028827963274]
actor:  0 policy actor:  1  step number:  20 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  37.90735541364261
Printing some Q and Qe and total Qs values:  [[-0.091]
 [-0.087]
 [-0.091]
 [-0.066]
 [-0.091]
 [-0.091]
 [-0.091]] [[35.78 ]
 [37.286]
 [35.78 ]
 [37.228]
 [35.78 ]
 [35.78 ]
 [35.78 ]] [[1.226]
 [1.324]
 [1.226]
 [1.34 ]
 [1.226]
 [1.226]
 [1.226]]
siam score:  -0.81600916
actor:  1 policy actor:  1  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  33.5607529012471
maxi score, test score, baseline:  0.16079999999999986 0.6775 0.6775
probs:  [0.07343403492915372, 0.05578906533765963, 0.18488783980615886, 0.05945438665306833, 0.24181438499432675, 0.38462028827963274]
maxi score, test score, baseline:  0.15785999999999986 0.6775 0.6775
actor:  1 policy actor:  1  step number:  19 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  46 total reward:  0.0699999999999994  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.15785999999999986 0.6775 0.6775
printing an ep nov before normalisation:  36.034256776438724
maxi score, test score, baseline:  0.15785999999999986 0.6775 0.6775
probs:  [0.07337362438115932, 0.05578428983188603, 0.18487198708084263, 0.059449296647261014, 0.2419335043913239, 0.38458729766752714]
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.69138075881186
actor:  1 policy actor:  1  step number:  31 total reward:  0.5199999999999999  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 34.83115579917729
maxi score, test score, baseline:  0.15785999999999986 0.6775 0.6775
probs:  [0.07332377443736175, 0.055787285859405775, 0.18488193266641256, 0.059452489983882785, 0.2419465219375484, 0.38460799511538873]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.299]
 [0.405]
 [0.323]
 [0.323]
 [0.321]
 [0.357]
 [0.325]] [[30.827]
 [31.77 ]
 [30.904]
 [30.904]
 [31.26 ]
 [31.345]
 [31.689]] [[0.755]
 [0.888]
 [0.781]
 [0.781]
 [0.789]
 [0.827]
 [0.805]]
maxi score, test score, baseline:  0.15785999999999986 0.6775 0.6775
actor:  1 policy actor:  1  step number:  35 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.15785999999999986 0.6775 0.6775
actor:  1 policy actor:  1  step number:  35 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  32.69141435623169
maxi score, test score, baseline:  0.15785999999999986 0.6775 0.6775
maxi score, test score, baseline:  0.15785999999999986 0.6775 0.6775
probs:  [0.07332377443736175, 0.055787285859405775, 0.18488193266641256, 0.059452489983882785, 0.2419465219375484, 0.38460799511538873]
maxi score, test score, baseline:  0.15785999999999986 0.6775 0.6775
probs:  [0.07332377443736175, 0.055787285859405775, 0.18488193266641256, 0.059452489983882785, 0.2419465219375484, 0.38460799511538873]
actor:  1 policy actor:  1  step number:  34 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.754]
 [0.78 ]
 [0.754]
 [0.744]
 [0.697]
 [0.707]
 [0.754]] [[46.666]
 [39.892]
 [46.666]
 [43.409]
 [49.336]
 [49.06 ]
 [46.666]] [[2.289]
 [1.973]
 [2.289]
 [2.114]
 [2.366]
 [2.363]
 [2.289]]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
siam score:  -0.81338257
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]] [[40.003]
 [40.618]
 [40.618]
 [40.618]
 [40.618]
 [40.618]
 [40.618]] [[0.864]
 [0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]]
actor:  1 policy actor:  1  step number:  32 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  0  action  0 :  tensor([0.6008, 0.0007, 0.0665, 0.0715, 0.0986, 0.0719, 0.0900],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0041,     0.9752,     0.0044,     0.0066,     0.0002,     0.0003,
            0.0092], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0426,     0.0006,     0.7599,     0.0468,     0.0452,     0.0630,
            0.0418], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1469, 0.0070, 0.1276, 0.3412, 0.1383, 0.1197, 0.1192],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1871, 0.0041, 0.1327, 0.1383, 0.2948, 0.1244, 0.1186],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1190, 0.0023, 0.1492, 0.1144, 0.1117, 0.4002, 0.1032],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0928, 0.0195, 0.1406, 0.1201, 0.1132, 0.1107, 0.4031],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  36 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.446]
 [0.517]
 [0.446]
 [0.446]
 [0.446]
 [0.322]
 [0.446]] [[34.633]
 [37.787]
 [34.633]
 [34.633]
 [34.633]
 [28.098]
 [34.633]] [[1.545]
 [1.8  ]
 [1.545]
 [1.545]
 [1.545]
 [1.042]
 [1.545]]
maxi score, test score, baseline:  0.15785999999999986 0.6775 0.6775
probs:  [0.07327422313790656, 0.0557902639381256, 0.1848918186693101, 0.05945566418965189, 0.2419594614973956, 0.38462856856761013]
maxi score, test score, baseline:  0.15785999999999986 0.6775 0.6775
probs:  [0.07327422313790656, 0.0557902639381256, 0.1848918186693101, 0.05945566418965189, 0.2419594614973956, 0.38462856856761013]
actor:  1 policy actor:  1  step number:  33 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  21.835743642447174
actor:  1 policy actor:  1  step number:  41 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.15785999999999986 0.6775 0.6775
probs:  [0.07327422313790656, 0.0557902639381256, 0.1848918186693101, 0.05945566418965189, 0.2419594614973956, 0.38462856856761013]
printing an ep nov before normalisation:  47.0074643577099
printing an ep nov before normalisation:  53.67134002002223
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  0 policy actor:  0  step number:  19 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  0.00019492301703394332
actor:  1 policy actor:  1  step number:  25 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.667
using another actor
line 256 mcts: sample exp_bonus 31.30591906989197
siam score:  -0.81802523
maxi score, test score, baseline:  0.16121999999999986 0.6775 0.6775
actor:  1 policy actor:  1  step number:  59 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.16121999999999986 0.6775 0.6775
probs:  [0.07327422313790656, 0.0557902639381256, 0.1848918186693101, 0.05945566418965189, 0.2419594614973956, 0.38462856856761013]
printing an ep nov before normalisation:  23.717806339263916
actor:  0 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.16433999999999985 0.6775 0.6775
probs:  [0.07327422313790656, 0.0557902639381256, 0.1848918186693101, 0.05945566418965189, 0.2419594614973956, 0.38462856856761013]
maxi score, test score, baseline:  0.16433999999999985 0.6775 0.6775
probs:  [0.07327422313790656, 0.0557902639381256, 0.1848918186693101, 0.05945566418965189, 0.2419594614973956, 0.38462856856761013]
actor:  0 policy actor:  1  step number:  37 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.16407999999999986 0.6775 0.6775
probs:  [0.07327422313790656, 0.0557902639381256, 0.1848918186693101, 0.05945566418965189, 0.2419594614973956, 0.38462856856761013]
actor:  1 policy actor:  1  step number:  36 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.16407999999999986 0.6775 0.6775
probs:  [0.07327422313790656, 0.0557902639381256, 0.1848918186693101, 0.05945566418965189, 0.2419594614973956, 0.38462856856761013]
maxi score, test score, baseline:  0.16407999999999986 0.6775 0.6775
probs:  [0.07327422313790656, 0.0557902639381256, 0.1848918186693101, 0.05945566418965189, 0.2419594614973956, 0.38462856856761013]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  35 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.16407999999999986 0.6775 0.6775
probs:  [0.07327422313790656, 0.0557902639381256, 0.1848918186693101, 0.05945566418965189, 0.2419594614973956, 0.38462856856761013]
maxi score, test score, baseline:  0.16407999999999986 0.6775 0.6775
actor:  1 policy actor:  1  step number:  33 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  43.585006264958
maxi score, test score, baseline:  0.16407999999999986 0.6775 0.6775
probs:  [0.07322496780710155, 0.05579322422885703, 0.18490164562336395, 0.059458819435971255, 0.24197232376958122, 0.384649019135125]
maxi score, test score, baseline:  0.16407999999999986 0.6775 0.6775
probs:  [0.07322496780710155, 0.05579322422885703, 0.18490164562336395, 0.059458819435971255, 0.24197232376958122, 0.384649019135125]
actor:  1 policy actor:  1  step number:  26 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.371]
 [0.364]
 [0.371]
 [0.371]
 [0.371]
 [0.266]] [[41.248]
 [41.248]
 [40.526]
 [41.248]
 [41.248]
 [41.248]
 [40.746]] [[1.821]
 [1.821]
 [1.779]
 [1.821]
 [1.821]
 [1.821]
 [1.692]]
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.672]
 [0.621]
 [0.621]
 [0.542]
 [0.621]
 [0.621]] [[32.157]
 [33.915]
 [32.157]
 [32.157]
 [32.251]
 [32.157]
 [32.157]] [[1.823]
 [1.999]
 [1.823]
 [1.823]
 [1.75 ]
 [1.823]
 [1.823]]
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.671]
 [0.582]
 [0.582]
 [0.628]
 [0.622]
 [0.605]] [[41.385]
 [43.26 ]
 [41.385]
 [41.385]
 [42.564]
 [43.021]
 [44.824]] [[1.824]
 [2.014]
 [1.824]
 [1.824]
 [1.933]
 [1.952]
 [2.031]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  30 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  2.0
from probs:  [0.07322496780710155, 0.05579322422885703, 0.18490164562336395, 0.059458819435971255, 0.24197232376958122, 0.384649019135125]
actor:  1 policy actor:  1  step number:  19 total reward:  0.7  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  32.433330784692515
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.16407999999999986 0.6775 0.6775
probs:  [0.07344977821424926, 0.05640444351832737, 0.18265092281360817, 0.0599887835979398, 0.2384565171283148, 0.3890495547275607]
printing an ep nov before normalisation:  43.68598261750492
maxi score, test score, baseline:  0.16407999999999986 0.6775 0.6775
probs:  [0.07344977821424926, 0.05640444351832737, 0.18265092281360817, 0.0599887835979398, 0.2384565171283148, 0.3890495547275607]
maxi score, test score, baseline:  0.16407999999999986 0.6775 0.6775
probs:  [0.07344977821424926, 0.05640444351832737, 0.18265092281360817, 0.0599887835979398, 0.2384565171283148, 0.3890495547275607]
maxi score, test score, baseline:  0.16407999999999986 0.6775 0.6775
maxi score, test score, baseline:  0.16407999999999986 0.6775 0.6775
probs:  [0.07345032843111292, 0.056404865815500166, 0.18265229254953105, 0.05998173411873046, 0.2384583056670655, 0.38905247341806]
Printing some Q and Qe and total Qs values:  [[1.011]
 [1.001]
 [1.011]
 [1.001]
 [1.001]
 [1.001]
 [1.001]] [[37.064]
 [36.603]
 [36.353]
 [36.603]
 [36.603]
 [36.603]
 [36.603]] [[1.011]
 [1.001]
 [1.011]
 [1.001]
 [1.001]
 [1.001]
 [1.001]]
printing an ep nov before normalisation:  15.824584656739397
actor:  0 policy actor:  0  step number:  37 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  58 total reward:  0.24999999999999956  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  26.172856489817303
maxi score, test score, baseline:  0.16405999999999984 0.6775 0.6775
probs:  [0.07340246311153721, 0.05640777488546073, 0.1826617282215744, 0.05998482810173604, 0.2384706263376459, 0.3890725793420457]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.16405999999999984 0.6775 0.6775
probs:  [0.07340246311153721, 0.05640777488546073, 0.1826617282215744, 0.05998482810173604, 0.2384706263376459, 0.3890725793420457]
actor:  0 policy actor:  1  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.16131999999999985 0.6775 0.6775
probs:  [0.07340246311153721, 0.05640777488546073, 0.1826617282215744, 0.05998482810173604, 0.2384706263376459, 0.3890725793420457]
actor:  1 policy actor:  1  step number:  32 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.16131999999999985 0.6775 0.6775
siam score:  -0.823903
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  34 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  33.91287370308714
maxi score, test score, baseline:  0.16131999999999985 0.6775 0.6775
probs:  [0.07340246311153721, 0.05640777488546073, 0.1826617282215744, 0.05998482810173604, 0.2384706263376459, 0.3890725793420457]
actor:  1 policy actor:  1  step number:  24 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  1  step number:  27 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
using another actor
maxi score, test score, baseline:  0.16435999999999984 0.6775 0.6775
probs:  [0.07340246311153721, 0.05640777488546073, 0.1826617282215744, 0.05998482810173604, 0.2384706263376459, 0.3890725793420457]
maxi score, test score, baseline:  0.16435999999999987 0.6775 0.6775
probs:  [0.07340246311153721, 0.05640777488546073, 0.1826617282215744, 0.05998482810173604, 0.2384706263376459, 0.3890725793420457]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
from probs:  [0.07340246311153721, 0.05640777488546073, 0.1826617282215744, 0.05998482810173604, 0.2384706263376459, 0.3890725793420457]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  19 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  31.86272382736206
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.16435999999999987 0.6775 0.6775
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.543]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]] [[41.287]
 [43.568]
 [41.287]
 [41.287]
 [41.287]
 [41.287]
 [41.287]] [[1.95 ]
 [2.067]
 [1.95 ]
 [1.95 ]
 [1.95 ]
 [1.95 ]
 [1.95 ]]
maxi score, test score, baseline:  0.16173999999999986 0.6775 0.6775
probs:  [0.07339298167156659, 0.0564004926511082, 0.18263810803504582, 0.05996961619148145, 0.23857655298849148, 0.3890222484623065]
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.615]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]] [[30.176]
 [48.095]
 [30.176]
 [30.176]
 [30.176]
 [30.176]
 [30.176]] [[0.573]
 [0.615]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]]
line 256 mcts: sample exp_bonus 44.80943297528282
maxi score, test score, baseline:  0.16173999999999986 0.6775 0.6775
probs:  [0.07339298167156659, 0.0564004926511082, 0.18263810803504582, 0.05996961619148145, 0.23857655298849148, 0.3890222484623065]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  20 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.16197999999999987 0.6775 0.6775
probs:  [0.07339298167156659, 0.0564004926511082, 0.18263810803504582, 0.05996961619148145, 0.23857655298849148, 0.3890222484623065]
maxi score, test score, baseline:  0.16197999999999987 0.6775 0.6775
probs:  [0.07339298167156659, 0.0564004926511082, 0.18263810803504582, 0.05996961619148145, 0.23857655298849148, 0.3890222484623065]
printing an ep nov before normalisation:  31.66839361190796
printing an ep nov before normalisation:  52.01651434075186
maxi score, test score, baseline:  0.16197999999999987 0.6775 0.6775
probs:  [0.07339298167156659, 0.0564004926511082, 0.18263810803504582, 0.05996961619148145, 0.23857655298849148, 0.3890222484623065]
printing an ep nov before normalisation:  32.78755528353565
maxi score, test score, baseline:  0.16197999999999987 0.6775 0.6775
probs:  [0.07339298167156659, 0.0564004926511082, 0.18263810803504582, 0.05996961619148145, 0.23857655298849148, 0.3890222484623065]
actor:  1 policy actor:  1  step number:  31 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  39 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  2.0
from probs:  [0.07339298167156659, 0.0564004926511082, 0.18263810803504582, 0.05996961619148145, 0.23857655298849148, 0.3890222484623065]
maxi score, test score, baseline:  0.16197999999999987 0.6775 0.6775
probs:  [0.07339298167156659, 0.0564004926511082, 0.18263810803504582, 0.05996961619148145, 0.23857655298849148, 0.3890222484623065]
actor:  1 policy actor:  1  step number:  25 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.16197999999999987 0.6775 0.6775
printing an ep nov before normalisation:  53.2265958758308
maxi score, test score, baseline:  0.16197999999999987 0.6775 0.6775
probs:  [0.07339352684707023, 0.05640091137398068, 0.1826394661775985, 0.05996262574524559, 0.23857832740693688, 0.3890251424491681]
actor:  1 policy actor:  1  step number:  33 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  48.02699952525145
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.59160038337291
maxi score, test score, baseline:  0.16197999999999987 0.6775 0.6775
probs:  [0.07339352684707023, 0.05640091137398068, 0.1826394661775985, 0.05996262574524559, 0.23857832740693688, 0.3890251424491681]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[ 0.351]
 [ 0.11 ]
 [ 0.44 ]
 [ 0.351]
 [ 0.351]
 [-0.034]
 [ 0.351]] [[31.854]
 [35.003]
 [37.849]
 [31.854]
 [31.854]
 [28.239]
 [31.854]] [[1.052]
 [0.937]
 [1.381]
 [1.052]
 [1.052]
 [0.522]
 [1.052]]
maxi score, test score, baseline:  0.16197999999999987 0.6775 0.6775
probs:  [0.07339352684707023, 0.05640091137398068, 0.1826394661775985, 0.05996262574524559, 0.23857832740693688, 0.3890251424491681]
printing an ep nov before normalisation:  45.28358432244465
actor:  1 policy actor:  1  step number:  30 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  57 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  24 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.919]
 [0.951]
 [0.919]
 [0.919]
 [0.919]
 [0.919]
 [0.919]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.919]
 [0.951]
 [0.919]
 [0.919]
 [0.919]
 [0.919]
 [0.919]]
printing an ep nov before normalisation:  29.33539663042341
printing an ep nov before normalisation:  43.00821355636638
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.80150079650987
actor:  0 policy actor:  0  step number:  25 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  22 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.285]
 [0.282]
 [0.297]
 [0.323]
 [0.269]
 [0.321]
 [0.343]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.285]
 [0.282]
 [0.297]
 [0.323]
 [0.269]
 [0.321]
 [0.343]]
printing an ep nov before normalisation:  37.68234928027571
printing an ep nov before normalisation:  21.37479389368538
printing an ep nov before normalisation:  41.231644982477306
actor:  1 policy actor:  1  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.667
using another actor
maxi score, test score, baseline:  0.16155999999999984 0.6775 0.6775
probs:  [0.07339406976379519, 0.056401328361994955, 0.1826408186930766, 0.059955664261920324, 0.23858009447358824, 0.38902802444562457]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  3  action  0 :  tensor([0.7707, 0.0027, 0.0384, 0.0402, 0.0627, 0.0351, 0.0502],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0020,     0.9760,     0.0029,     0.0015,     0.0007,     0.0008,
            0.0161], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1004, 0.0471, 0.4850, 0.1098, 0.0940, 0.0822, 0.0815],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1064, 0.0433, 0.1295, 0.3404, 0.1219, 0.1294, 0.1291],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1346, 0.0054, 0.1049, 0.0791, 0.5153, 0.0709, 0.0898],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0722, 0.0896, 0.1178, 0.1020, 0.0935, 0.4296, 0.0955],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1272, 0.2183, 0.1055, 0.1012, 0.1036, 0.0939, 0.2503],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  32.00643095861447
Printing some Q and Qe and total Qs values:  [[0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.821]] [[42.595]
 [42.595]
 [42.595]
 [42.595]
 [42.595]
 [42.595]
 [42.595]] [[0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.821]]
printing an ep nov before normalisation:  46.854524765975434
using explorer policy with actor:  1
maxi score, test score, baseline:  0.16155999999999984 0.6775 0.6775
probs:  [0.07339406976379519, 0.056401328361994955, 0.1826408186930766, 0.059955664261920324, 0.23858009447358824, 0.38902802444562457]
actor:  0 policy actor:  1  step number:  31 total reward:  0.4999999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.16217999999999985 0.6775 0.6775
probs:  [0.07339406976379519, 0.056401328361994955, 0.1826408186930766, 0.059955664261920324, 0.23858009447358824, 0.38902802444562457]
actor:  1 policy actor:  1  step number:  31 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.16217999999999985 0.6775 0.6775
probs:  [0.07339406976379519, 0.056401328361994955, 0.1826408186930766, 0.059955664261920324, 0.23858009447358824, 0.38902802444562457]
printing an ep nov before normalisation:  44.293315865834174
actor:  1 policy actor:  1  step number:  29 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  23 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.16217999999999985 0.6775 0.6775
probs:  [0.07339406976379519, 0.056401328361994955, 0.1826408186930766, 0.059955664261920324, 0.23858009447358824, 0.38902802444562457]
actor:  0 policy actor:  0  step number:  48 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  53.7088826534233
Printing some Q and Qe and total Qs values:  [[0.69 ]
 [0.671]
 [0.692]
 [0.697]
 [0.692]
 [0.691]
 [0.692]] [[51.223]
 [45.035]
 [51.778]
 [51.096]
 [51.49 ]
 [52.358]
 [51.165]] [[2.269]
 [1.963]
 [2.297]
 [2.27 ]
 [2.284]
 [2.323]
 [2.269]]
printing an ep nov before normalisation:  39.818890220732136
actor:  1 policy actor:  1  step number:  22 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  2.0
from probs:  [0.07339406976379519, 0.056401328361994955, 0.1826408186930766, 0.059955664261920324, 0.23858009447358824, 0.38902802444562457]
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]] [[30.314]
 [30.314]
 [30.314]
 [30.314]
 [30.314]
 [30.314]
 [30.314]] [[1.105]
 [1.105]
 [1.105]
 [1.105]
 [1.105]
 [1.105]
 [1.105]]
maxi score, test score, baseline:  0.16479999999999986 0.6775 0.6775
probs:  [0.07339406976379519, 0.056401328361994955, 0.1826408186930766, 0.059955664261920324, 0.23858009447358824, 0.38902802444562457]
maxi score, test score, baseline:  0.16479999999999986 0.6775 0.6775
probs:  [0.07339406976379519, 0.056401328361994955, 0.1826408186930766, 0.059955664261920324, 0.23858009447358824, 0.38902802444562457]
printing an ep nov before normalisation:  0.0002249198587378487
actor:  1 policy actor:  1  step number:  35 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.16479999999999986 0.6775 0.6775
probs:  [0.07339406976379519, 0.056401328361994955, 0.1826408186930766, 0.059955664261920324, 0.23858009447358824, 0.38902802444562457]
actor:  1 policy actor:  1  step number:  35 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  30.479438304901123
Printing some Q and Qe and total Qs values:  [[0.405]
 [0.494]
 [0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]] [[49.287]
 [47.066]
 [49.287]
 [49.287]
 [49.287]
 [49.287]
 [49.287]] [[1.709]
 [1.72 ]
 [1.709]
 [1.709]
 [1.709]
 [1.709]
 [1.709]]
actor:  0 policy actor:  1  step number:  25 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.16783999999999988 0.6775 0.6775
probs:  [0.07339406976379519, 0.056401328361994955, 0.1826408186930766, 0.059955664261920324, 0.23858009447358824, 0.38902802444562457]
maxi score, test score, baseline:  0.16783999999999988 0.6775 0.6775
printing an ep nov before normalisation:  29.888594150543213
actor:  1 policy actor:  1  step number:  35 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.16783999999999988 0.6775 0.6775
probs:  [0.07334649377584128, 0.056404219490051664, 0.18265019616991557, 0.05995873801636641, 0.23859234618298616, 0.389048006364839]
printing an ep nov before normalisation:  58.02488596319288
maxi score, test score, baseline:  0.16783999999999988 0.6775 0.6775
probs:  [0.07334649377584128, 0.056404219490051664, 0.18265019616991557, 0.05995873801636641, 0.23859234618298616, 0.389048006364839]
printing an ep nov before normalisation:  18.2460958475701
maxi score, test score, baseline:  0.16783999999999988 0.6775 0.6775
probs:  [0.07334649377584128, 0.056404219490051664, 0.18265019616991557, 0.05995873801636641, 0.23859234618298616, 0.389048006364839]
actor:  0 policy actor:  1  step number:  45 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.16749999999999984 0.6775 0.6775
probs:  [0.07334649377584128, 0.056404219490051664, 0.18265019616991557, 0.05995873801636641, 0.23859234618298616, 0.389048006364839]
printing an ep nov before normalisation:  32.315406799316406
actor:  1 policy actor:  1  step number:  29 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  33 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.16749999999999984 0.6775 0.6775
probs:  [0.07334649377584128, 0.056404219490051664, 0.18265019616991557, 0.05995873801636641, 0.23859234618298616, 0.389048006364839]
maxi score, test score, baseline:  0.16749999999999984 0.6775 0.6775
probs:  [0.07334649377584128, 0.056404219490051664, 0.18265019616991557, 0.05995873801636641, 0.23859234618298616, 0.389048006364839]
maxi score, test score, baseline:  0.16749999999999984 0.6775 0.6775
probs:  [0.07334649377584128, 0.056404219490051664, 0.18265019616991557, 0.05995873801636641, 0.23859234618298616, 0.389048006364839]
printing an ep nov before normalisation:  27.77116298675537
printing an ep nov before normalisation:  50.44323565833756
maxi score, test score, baseline:  0.16749999999999984 0.6775 0.6775
probs:  [0.07334649377584128, 0.056404219490051664, 0.18265019616991557, 0.05995873801636641, 0.23859234618298616, 0.389048006364839]
actor:  1 policy actor:  1  step number:  32 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
using another actor
maxi score, test score, baseline:  0.16749999999999984 0.6775 0.6775
probs:  [0.07334649377584128, 0.056404219490051664, 0.18265019616991557, 0.05995873801636641, 0.23859234618298616, 0.389048006364839]
maxi score, test score, baseline:  0.16749999999999984 0.6775 0.6775
probs:  [0.07334649377584128, 0.056404219490051664, 0.18265019616991557, 0.05995873801636641, 0.23859234618298616, 0.389048006364839]
maxi score, test score, baseline:  0.16749999999999984 0.6775 0.6775
Printing some Q and Qe and total Qs values:  [[0.674]
 [0.766]
 [0.674]
 [0.674]
 [0.692]
 [0.674]
 [0.695]] [[36.063]
 [38.176]
 [36.063]
 [36.063]
 [31.07 ]
 [36.063]
 [32.641]] [[0.674]
 [0.766]
 [0.674]
 [0.674]
 [0.692]
 [0.674]
 [0.695]]
printing an ep nov before normalisation:  48.7657415074562
Printing some Q and Qe and total Qs values:  [[0.871]
 [0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]] [[49.31]
 [57.32]
 [57.32]
 [57.32]
 [57.32]
 [57.32]
 [57.32]] [[0.871]
 [0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]]
Printing some Q and Qe and total Qs values:  [[0.73 ]
 [0.747]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]] [[38.414]
 [47.427]
 [38.414]
 [38.414]
 [38.414]
 [38.414]
 [38.414]] [[0.73 ]
 [0.747]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]]
actor:  1 policy actor:  1  step number:  42 total reward:  0.24999999999999956  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  42 total reward:  0.34999999999999976  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  52.70699809037578
siam score:  -0.8137924
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.16455999999999985 0.6775 0.6775
probs:  [0.07334649377584128, 0.056404219490051664, 0.18265019616991557, 0.05995873801636641, 0.23859234618298616, 0.389048006364839]
actor:  1 policy actor:  1  step number:  23 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  2  action  0 :  tensor([0.6897, 0.0012, 0.0337, 0.0544, 0.0867, 0.0386, 0.0956],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0042, 0.9392, 0.0160, 0.0104, 0.0010, 0.0016, 0.0274],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0529, 0.0296, 0.7715, 0.0213, 0.0156, 0.0759, 0.0333],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1554, 0.0051, 0.1658, 0.1857, 0.1641, 0.1630, 0.1608],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1488, 0.0032, 0.0965, 0.1228, 0.4125, 0.1159, 0.1003],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([    0.0840,     0.0005,     0.0663,     0.0993,     0.0945,     0.5878,
            0.0675], grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0961, 0.1725, 0.0661, 0.1126, 0.0509, 0.0367, 0.4652],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.337]
 [0.383]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]] [[38.831]
 [40.737]
 [38.831]
 [38.831]
 [38.831]
 [38.831]
 [38.831]] [[1.33 ]
 [1.459]
 [1.33 ]
 [1.33 ]
 [1.33 ]
 [1.33 ]
 [1.33 ]]
maxi score, test score, baseline:  0.16455999999999985 0.6775 0.6775
probs:  [0.07329919954503404, 0.05640709349610989, 0.18265951811094253, 0.05996179356725406, 0.2386045253346333, 0.3890678699460262]
siam score:  -0.81370664
maxi score, test score, baseline:  0.16455999999999985 0.6775 0.6775
probs:  [0.07329919954503404, 0.05640709349610989, 0.18265951811094253, 0.05996179356725406, 0.2386045253346333, 0.3890678699460262]
maxi score, test score, baseline:  0.16455999999999985 0.6775 0.6775
probs:  [0.07329919954503404, 0.05640709349610989, 0.18265951811094253, 0.05996179356725406, 0.2386045253346333, 0.3890678699460262]
siam score:  -0.814285
actor:  1 policy actor:  1  step number:  27 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  30.030282195404855
actor:  0 policy actor:  1  step number:  21 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.0
from probs:  [0.070227979110389, 0.04834995154725137, 0.21235991920038377, 0.05296773410044633, 0.2850360587129851, 0.3310583573285445]
actor:  1 policy actor:  1  step number:  19 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.815]
 [0.805]
 [0.805]
 [0.805]
 [0.805]
 [0.805]
 [0.805]] [[60.538]
 [67.559]
 [67.559]
 [67.559]
 [67.559]
 [67.559]
 [67.559]] [[2.269]
 [2.472]
 [2.472]
 [2.472]
 [2.472]
 [2.472]
 [2.472]]
maxi score, test score, baseline:  0.16475999999999988 0.6775 0.6775
probs:  [0.070227979110389, 0.04834995154725137, 0.21235991920038377, 0.05296773410044633, 0.2850360587129851, 0.3310583573285445]
maxi score, test score, baseline:  0.16475999999999988 0.6775 0.6775
probs:  [0.070227979110389, 0.04834995154725137, 0.21235991920038377, 0.05296773410044633, 0.2850360587129851, 0.3310583573285445]
printing an ep nov before normalisation:  49.51109051038973
maxi score, test score, baseline:  0.16475999999999988 0.6775 0.6775
probs:  [0.070227979110389, 0.04834995154725137, 0.21235991920038377, 0.05296773410044633, 0.2850360587129851, 0.3310583573285445]
actor:  0 policy actor:  1  step number:  23 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.16491999999999987 0.6775 0.6775
probs:  [0.070227979110389, 0.04834995154725137, 0.21235991920038377, 0.05296773410044633, 0.2850360587129851, 0.3310583573285445]
maxi score, test score, baseline:  0.16491999999999987 0.6775 0.6775
using explorer policy with actor:  1
maxi score, test score, baseline:  0.16491999999999987 0.6775 0.6775
probs:  [0.070227979110389, 0.04834995154725137, 0.21235991920038377, 0.05296773410044633, 0.2850360587129851, 0.3310583573285445]
actor:  1 policy actor:  1  step number:  38 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  24 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  2.0
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.16491999999999987 0.6775 0.6775
probs:  [0.070227979110389, 0.04834995154725137, 0.21235991920038377, 0.05296773410044633, 0.2850360587129851, 0.3310583573285445]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  21 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.16491999999999987 0.6775 0.6775
probs:  [0.070227979110389, 0.04834995154725137, 0.21235991920038377, 0.05296773410044633, 0.2850360587129851, 0.3310583573285445]
Printing some Q and Qe and total Qs values:  [[0.185]
 [0.287]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]] [[41.065]
 [46.191]
 [41.065]
 [41.065]
 [41.065]
 [41.065]
 [41.065]] [[1.17 ]
 [1.497]
 [1.17 ]
 [1.17 ]
 [1.17 ]
 [1.17 ]
 [1.17 ]]
maxi score, test score, baseline:  0.16491999999999987 0.6775 0.6775
probs:  [0.070227979110389, 0.04834995154725137, 0.21235991920038377, 0.05296773410044633, 0.2850360587129851, 0.3310583573285445]
actor:  0 policy actor:  0  step number:  22 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  25 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.16799999999999984 0.6775 0.6775
probs:  [0.070227979110389, 0.04834995154725137, 0.21235991920038377, 0.05296773410044633, 0.2850360587129851, 0.3310583573285445]
maxi score, test score, baseline:  0.16799999999999984 0.6775 0.6775
probs:  [0.070227979110389, 0.04834995154725137, 0.21235991920038377, 0.05296773410044633, 0.2850360587129851, 0.3310583573285445]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  31 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.124]
 [0.321]
 [0.124]
 [0.124]
 [0.124]
 [0.124]
 [0.124]] [[17.13 ]
 [22.405]
 [17.13 ]
 [17.13 ]
 [17.13 ]
 [17.13 ]
 [17.13 ]] [[0.775]
 [1.244]
 [0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.775]]
maxi score, test score, baseline:  0.16799999999999984 0.6775 0.6775
probs:  [0.070227979110389, 0.04834995154725137, 0.21235991920038377, 0.05296773410044633, 0.2850360587129851, 0.3310583573285445]
maxi score, test score, baseline:  0.16799999999999984 0.6775 0.6775
probs:  [0.070227979110389, 0.04834995154725137, 0.21235991920038377, 0.05296773410044633, 0.2850360587129851, 0.3310583573285445]
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.598]
 [0.552]
 [0.584]
 [0.519]
 [0.487]
 [0.487]] [[26.902]
 [26.109]
 [35.816]
 [31.426]
 [30.769]
 [26.902]
 [26.902]] [[1.778]
 [1.85 ]
 [2.271]
 [2.092]
 [1.995]
 [1.778]
 [1.778]]
actor:  1 policy actor:  1  step number:  34 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  24.848247449323328
printing an ep nov before normalisation:  40.93605338495942
Printing some Q and Qe and total Qs values:  [[0.698]
 [0.736]
 [0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]] [[29.219]
 [35.181]
 [29.219]
 [29.219]
 [29.219]
 [29.219]
 [29.219]] [[1.506]
 [1.771]
 [1.506]
 [1.506]
 [1.506]
 [1.506]
 [1.506]]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.16493999999999984 0.6775 0.6775
probs:  [0.07022797911038901, 0.04834995154725139, 0.21235991920038375, 0.052967734100446334, 0.285036058712985, 0.3310583573285446]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.435622948429206
printing an ep nov before normalisation:  29.405760765075684
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  22 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.16529999999999986 0.6775 0.6775
printing an ep nov before normalisation:  59.900452889700766
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.28910169153056
actor:  1 policy actor:  1  step number:  22 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.16529999999999986 0.6775 0.6775
probs:  [0.07022797911038901, 0.04834995154725139, 0.21235991920038375, 0.052967734100446334, 0.285036058712985, 0.3310583573285446]
printing an ep nov before normalisation:  44.59291278648733
maxi score, test score, baseline:  0.16529999999999986 0.6775 0.6775
probs:  [0.07022797911038901, 0.04834995154725139, 0.21235991920038375, 0.052967734100446334, 0.285036058712985, 0.3310583573285446]
actor:  1 policy actor:  1  step number:  38 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  35 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.16529999999999986 0.6775 0.6775
probs:  [0.07022865119415739, 0.048350413859721456, 0.212361954076183, 0.052958660135634175, 0.2850387904234556, 0.3310615303108484]
Printing some Q and Qe and total Qs values:  [[0.645]
 [0.467]
 [0.525]
 [0.341]
 [0.525]
 [0.525]
 [0.525]] [[55.043]
 [54.009]
 [57.067]
 [59.943]
 [57.067]
 [57.067]
 [57.067]] [[2.261]
 [2.039]
 [2.229]
 [2.169]
 [2.229]
 [2.229]
 [2.229]]
actor:  0 policy actor:  0  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.16597999999999985 0.6775 0.6775
probs:  [0.07022865119415739, 0.048350413859721456, 0.212361954076183, 0.052958660135634175, 0.2850387904234556, 0.3310615303108484]
actor:  1 policy actor:  1  step number:  27 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  48.94282937767144
actor:  0 policy actor:  0  step number:  27 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.16573999999999986 0.6775 0.6775
probs:  [0.07022865119415742, 0.04835041385972148, 0.21236195407618297, 0.052958660135634196, 0.2850387904234555, 0.33106153031084856]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.257]
 [0.31 ]
 [0.257]
 [0.257]
 [0.257]
 [0.257]
 [0.257]] [[26.518]
 [30.437]
 [26.518]
 [26.518]
 [26.518]
 [26.518]
 [26.518]] [[0.667]
 [0.836]
 [0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]]
Printing some Q and Qe and total Qs values:  [[0.31 ]
 [0.31 ]
 [0.402]
 [0.31 ]
 [0.31 ]
 [0.31 ]
 [0.31 ]] [[49.318]
 [49.318]
 [49.308]
 [49.318]
 [49.318]
 [49.318]
 [49.318]] [[1.955]
 [1.955]
 [2.047]
 [1.955]
 [1.955]
 [1.955]
 [1.955]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.16573999999999986 0.6775 0.6775
probs:  [0.07022865119415742, 0.04835041385972148, 0.21236195407618297, 0.052958660135634196, 0.2850387904234555, 0.33106153031084856]
maxi score, test score, baseline:  0.16573999999999986 0.6775 0.6775
probs:  [0.07022865119415742, 0.04835041385972148, 0.21236195407618297, 0.052958660135634196, 0.2850387904234555, 0.33106153031084856]
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.64 ]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]] [[25.277]
 [33.253]
 [25.277]
 [25.277]
 [25.277]
 [25.277]
 [25.277]] [[0.941]
 [1.247]
 [0.941]
 [0.941]
 [0.941]
 [0.941]
 [0.941]]
actor:  1 policy actor:  1  step number:  26 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.16573999999999986 0.6775 0.6775
actor:  1 policy actor:  1  step number:  36 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  2.0
using another actor
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.182]
 [0.182]
 [0.241]
 [0.182]
 [0.182]
 [0.182]
 [0.182]] [[28.567]
 [28.567]
 [36.254]
 [28.567]
 [28.567]
 [28.567]
 [28.567]] [[0.763]
 [0.763]
 [1.121]
 [0.763]
 [0.763]
 [0.763]
 [0.763]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.699]
 [0.725]
 [0.689]
 [0.7  ]
 [0.7  ]
 [0.69 ]
 [0.69 ]] [[20.545]
 [20.519]
 [21.893]
 [21.027]
 [19.67 ]
 [20.028]
 [19.947]] [[1.296]
 [1.321]
 [1.325]
 [1.311]
 [1.272]
 [1.272]
 [1.269]]
maxi score, test score, baseline:  0.16573999999999986 0.6775 0.6775
probs:  [0.07022865119415742, 0.04835041385972148, 0.21236195407618297, 0.052958660135634196, 0.2850387904234555, 0.33106153031084856]
actor:  1 policy actor:  1  step number:  52 total reward:  0.12999999999999956  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.16573999999999986 0.6775 0.6775
probs:  [0.07022865119415742, 0.04835041385972148, 0.21236195407618297, 0.052958660135634196, 0.2850387904234555, 0.33106153031084856]
printing an ep nov before normalisation:  34.85483601077611
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  30 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  32 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.433]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]] [[37.698]
 [42.464]
 [37.698]
 [37.698]
 [37.698]
 [37.698]
 [37.698]] [[1.466]
 [1.707]
 [1.466]
 [1.466]
 [1.466]
 [1.466]
 [1.466]]
maxi score, test score, baseline:  0.1685999999999999 0.6775 0.6775
probs:  [0.07016773867955996, 0.04835357499805754, 0.21237586787759302, 0.052962123392464554, 0.28505746894772266, 0.33108322610460217]
printing an ep nov before normalisation:  39.29440975189209
printing an ep nov before normalisation:  41.742527817531325
Printing some Q and Qe and total Qs values:  [[0.689]
 [0.718]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]] [[30.781]
 [45.108]
 [30.781]
 [30.781]
 [30.781]
 [30.781]
 [30.781]] [[1.34 ]
 [1.947]
 [1.34 ]
 [1.34 ]
 [1.34 ]
 [1.34 ]
 [1.34 ]]
printing an ep nov before normalisation:  30.480469144962832
actor:  1 policy actor:  1  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  35.90086517659925
actor:  1 policy actor:  1  step number:  29 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1685999999999999 0.6775 0.6775
actor:  1 policy actor:  1  step number:  36 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]] [[50.245]
 [50.245]
 [50.245]
 [50.245]
 [50.245]
 [50.245]
 [50.245]] [[1.759]
 [1.759]
 [1.759]
 [1.759]
 [1.759]
 [1.759]
 [1.759]]
printing an ep nov before normalisation:  39.4314473996799
actor:  1 policy actor:  1  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
from probs:  [0.07016840745539435, 0.04835403546537207, 0.2123778946319103, 0.052953086268702064, 0.2850601897555438, 0.3310863864230774]
Printing some Q and Qe and total Qs values:  [[0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]] [[40.179]
 [40.179]
 [40.179]
 [40.179]
 [40.179]
 [40.179]
 [40.179]] [[0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]]
actor:  1 policy actor:  1  step number:  28 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1685999999999999 0.6775 0.6775
probs:  [0.07016840745539435, 0.04835403546537207, 0.2123778946319103, 0.052953086268702064, 0.2850601897555438, 0.3310863864230774]
printing an ep nov before normalisation:  26.81026193830702
actor:  0 policy actor:  0  step number:  31 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  2.0
using another actor
actions average: 
K:  3  action  0 :  tensor([0.7324, 0.0522, 0.0560, 0.0317, 0.0429, 0.0317, 0.0532],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0068, 0.9537, 0.0078, 0.0066, 0.0055, 0.0023, 0.0173],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0256, 0.0012, 0.9121, 0.0198, 0.0118, 0.0123, 0.0173],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0895, 0.0378, 0.0893, 0.5659, 0.1035, 0.0565, 0.0575],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1579, 0.0530, 0.1176, 0.1174, 0.3370, 0.0893, 0.1279],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0956, 0.1869, 0.1032, 0.1002, 0.1573, 0.2243, 0.1325],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1081, 0.0093, 0.1125, 0.0989, 0.1077, 0.1049, 0.4585],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  1  step number:  24 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.16781999999999986 0.6775 0.6775
probs:  [0.07016840745539436, 0.04835403546537209, 0.21237789463191023, 0.052953086268702085, 0.28506018975554376, 0.3310863864230775]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.16781999999999986 0.6775 0.6775
actor:  1 policy actor:  1  step number:  24 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  21 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  42.99328859689871
siam score:  -0.81966025
maxi score, test score, baseline:  0.16821999999999987 0.6775 0.6775
probs:  [0.07016840745539436, 0.04835403546537209, 0.21237789463191023, 0.052953086268702085, 0.28506018975554376, 0.3310863864230775]
maxi score, test score, baseline:  0.16821999999999987 0.6775 0.6775
probs:  [0.07016840745539436, 0.04835403546537209, 0.21237789463191023, 0.052953086268702085, 0.28506018975554376, 0.3310863864230775]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  31 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  44.546894598229485
maxi score, test score, baseline:  0.16821999999999987 0.6775 0.6775
probs:  [0.07016840745539436, 0.04835403546537209, 0.21237789463191023, 0.052953086268702085, 0.28506018975554376, 0.3310863864230775]
Printing some Q and Qe and total Qs values:  [[0.939]
 [0.941]
 [0.941]
 [0.941]
 [0.941]
 [0.941]
 [0.941]] [[43.261]
 [41.907]
 [41.907]
 [41.907]
 [41.907]
 [41.907]
 [41.907]] [[0.939]
 [0.941]
 [0.941]
 [0.941]
 [0.941]
 [0.941]
 [0.941]]
printing an ep nov before normalisation:  38.24131004134146
actor:  1 policy actor:  1  step number:  21 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  37 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.483]
 [0.65 ]
 [0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.483]] [[28.362]
 [35.605]
 [28.362]
 [28.362]
 [28.362]
 [28.362]
 [28.362]] [[0.483]
 [0.65 ]
 [0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.483]]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.16831999999999986 0.6775 0.6775
actor:  0 policy actor:  0  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  1.5769001038279384e-05
actor:  0 policy actor:  1  step number:  24 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  36 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.17477999999999982 0.6775 0.6775
actor:  0 policy actor:  0  step number:  20 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  33.16675553243661
siam score:  -0.80967295
printing an ep nov before normalisation:  45.916982841844636
printing an ep nov before normalisation:  33.40719699859619
maxi score, test score, baseline:  0.17467999999999986 0.6775 0.6775
probs:  [0.07016840745539436, 0.04835403546537209, 0.21237789463191023, 0.052953086268702085, 0.28506018975554376, 0.3310863864230775]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  24 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17467999999999986 0.6775 0.6775
probs:  [0.07016907348038588, 0.048354494038668976, 0.21237991304967815, 0.052944086316897634, 0.2850628993719996, 0.3310895337423696]
Printing some Q and Qe and total Qs values:  [[0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]]
maxi score, test score, baseline:  0.17467999999999986 0.6775 0.6775
probs:  [0.07016907348038588, 0.048354494038668976, 0.21237991304967815, 0.052944086316897634, 0.2850628993719996, 0.3310895337423696]
printing an ep nov before normalisation:  40.646539005773214
maxi score, test score, baseline:  0.17467999999999986 0.6775 0.6775
actor:  0 policy actor:  1  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17455999999999985 0.6775 0.6775
probs:  [0.07016907348038588, 0.04835449403866896, 0.21237991304967827, 0.05294408631689763, 0.2850628993719997, 0.33108953374236955]
Printing some Q and Qe and total Qs values:  [[0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]] [[37.261]
 [37.261]
 [37.261]
 [37.261]
 [37.261]
 [37.261]
 [37.261]] [[62.35]
 [62.35]
 [62.35]
 [62.35]
 [62.35]
 [62.35]
 [62.35]]
maxi score, test score, baseline:  0.17455999999999985 0.6775 0.6775
probs:  [0.07016907348038588, 0.04835449403866896, 0.21237991304967827, 0.05294408631689763, 0.2850628993719997, 0.33108953374236955]
Printing some Q and Qe and total Qs values:  [[0.57 ]
 [0.603]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]] [[31.831]
 [35.615]
 [31.831]
 [31.831]
 [31.831]
 [31.831]
 [31.831]] [[0.57 ]
 [0.603]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]]
Printing some Q and Qe and total Qs values:  [[ 0.486]
 [ 0.515]
 [ 0.517]
 [-0.05 ]
 [ 0.517]
 [ 0.517]
 [ 0.517]] [[53.498]
 [47.195]
 [52.913]
 [51.439]
 [52.913]
 [52.913]
 [52.913]] [[1.928]
 [1.72 ]
 [1.937]
 [1.314]
 [1.937]
 [1.937]
 [1.937]]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.16783999999999988 0.6775 0.6775
probs:  [0.07016907348038588, 0.04835449403866896, 0.21237991304967827, 0.05294408631689763, 0.2850628993719997, 0.33108953374236955]
maxi score, test score, baseline:  0.16783999999999988 0.6775 0.6775
maxi score, test score, baseline:  0.16783999999999988 0.6775 0.6775
probs:  [0.07016907348038588, 0.04835449403866896, 0.21237991304967827, 0.05294408631689763, 0.2850628993719997, 0.33108953374236955]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.67 ]
 [0.754]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [0.67 ]] [[18.289]
 [24.043]
 [18.289]
 [18.289]
 [18.289]
 [18.289]
 [18.289]] [[1.632]
 [2.204]
 [1.632]
 [1.632]
 [1.632]
 [1.632]
 [1.632]]
maxi score, test score, baseline:  0.16783999999999988 0.6775 0.6775
probs:  [0.07016907348038588, 0.04835449403866896, 0.21237991304967827, 0.05294408631689763, 0.2850628993719997, 0.33108953374236955]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  47 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.667
from probs:  [0.07016907348038588, 0.04835449403866896, 0.21237991304967827, 0.05294408631689763, 0.2850628993719997, 0.33108953374236955]
maxi score, test score, baseline:  0.16783999999999988 0.6775 0.6775
probs:  [0.07016907348038588, 0.04835449403866896, 0.21237991304967827, 0.05294408631689763, 0.2850628993719997, 0.33108953374236955]
actor:  0 policy actor:  0  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  1  step number:  27 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.563]
 [0.622]
 [0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.563]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.563]
 [0.622]
 [0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.563]]
printing an ep nov before normalisation:  42.92199849565473
printing an ep nov before normalisation:  38.34781714264731
actor:  0 policy actor:  1  step number:  30 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  22 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.16661999999999985 0.6775 0.6775
probs:  [0.07016907348038588, 0.048354494038668976, 0.21237991304967815, 0.052944086316897634, 0.2850628993719996, 0.3310895337423696]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.16661999999999985 0.6775 0.6775
probs:  [0.07016907348038588, 0.048354494038668976, 0.21237991304967815, 0.052944086316897634, 0.2850628993719996, 0.3310895337423696]
actions average: 
K:  2  action  0 :  tensor([    0.8514,     0.0004,     0.0153,     0.0185,     0.0509,     0.0208,
            0.0428], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0038, 0.9427, 0.0180, 0.0086, 0.0009, 0.0012, 0.0247],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0660, 0.0023, 0.6804, 0.0451, 0.0623, 0.0834, 0.0605],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1460, 0.0512, 0.1298, 0.2142, 0.1188, 0.1249, 0.2151],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1319, 0.0058, 0.0910, 0.1201, 0.4599, 0.1031, 0.0882],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0921, 0.0040, 0.1048, 0.1224, 0.0947, 0.4699, 0.1121],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0994, 0.1012, 0.0923, 0.0665, 0.0628, 0.0763, 0.5016],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.16661999999999985 0.6775 0.6775
maxi score, test score, baseline:  0.16661999999999985 0.6775 0.6775
probs:  [0.07016907348038588, 0.048354494038668976, 0.21237991304967815, 0.052944086316897634, 0.2850628993719996, 0.3310895337423696]
using explorer policy with actor:  1
printing an ep nov before normalisation:  27.080166339874268
actor:  0 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  28 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.16639999999999985 0.6775 0.6775
probs:  [0.07016907348038588, 0.048354494038668976, 0.21237991304967815, 0.052944086316897634, 0.2850628993719996, 0.3310895337423696]
actor:  1 policy actor:  1  step number:  33 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  41.113213381707816
Printing some Q and Qe and total Qs values:  [[0.6  ]
 [0.666]
 [0.607]
 [0.606]
 [0.614]
 [0.61 ]
 [0.606]] [[33.695]
 [29.473]
 [30.755]
 [32.456]
 [33.961]
 [35.561]
 [35.303]] [[1.335]
 [1.246]
 [1.234]
 [1.296]
 [1.358]
 [1.413]
 [1.4  ]]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  32 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  34 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.643]
 [0.581]
 [0.381]
 [0.469]
 [0.526]
 [0.381]
 [0.53 ]] [[59.414]
 [59.466]
 [69.545]
 [62.701]
 [60.347]
 [69.545]
 [60.998]] [[2.219]
 [2.159]
 [2.336]
 [2.168]
 [2.137]
 [2.336]
 [2.166]]
maxi score, test score, baseline:  0.16639999999999985 0.6775 0.6775
printing an ep nov before normalisation:  53.914104503899374
Printing some Q and Qe and total Qs values:  [[0.319]
 [0.707]
 [0.678]
 [0.691]
 [0.648]
 [0.633]
 [0.702]] [[35.233]
 [38.925]
 [29.414]
 [27.977]
 [29.862]
 [27.504]
 [28.517]] [[1.332]
 [1.934]
 [1.353]
 [1.283]
 [1.349]
 [1.197]
 [1.325]]
maxi score, test score, baseline:  0.16639999999999985 0.6775 0.6775
actor:  1 policy actor:  1  step number:  31 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  29 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.16639999999999985 0.6775 0.6775
probs:  [0.07010917840006829, 0.04835809358562206, 0.21239575651369208, 0.052938564677424686, 0.2850841683637251, 0.33111423845946775]
printing an ep nov before normalisation:  50.593406073358686
actor:  1 policy actor:  1  step number:  38 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  2.217253329550033
actor:  0 policy actor:  0  step number:  24 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  67.6752495684537
printing an ep nov before normalisation:  59.64324674228198
printing an ep nov before normalisation:  37.97842502593994
line 256 mcts: sample exp_bonus 0.0
actor:  1 policy actor:  1  step number:  25 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1660999999999999 0.6775 0.6775
probs:  [0.07010917840006829, 0.04835809358562206, 0.21239575651369208, 0.052938564677424686, 0.2850841683637251, 0.33111423845946775]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.927]
 [0.876]
 [0.876]
 [0.876]
 [0.876]
 [0.876]
 [0.876]] [[56.409]
 [59.383]
 [59.383]
 [59.383]
 [59.383]
 [59.383]
 [59.383]] [[0.927]
 [0.876]
 [0.876]
 [0.876]
 [0.876]
 [0.876]
 [0.876]]
actor:  1 policy actor:  1  step number:  24 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.748]
 [0.748]
 [0.748]
 [0.748]
 [0.748]
 [0.748]
 [0.748]] [[31.211]
 [31.211]
 [31.211]
 [31.211]
 [31.211]
 [31.211]
 [31.211]] [[0.748]
 [0.748]
 [0.748]
 [0.748]
 [0.748]
 [0.748]
 [0.748]]
maxi score, test score, baseline:  0.1660999999999999 0.6775 0.6775
probs:  [0.07004897267119775, 0.04836121814020547, 0.21240950929112412, 0.05294198600694762, 0.28510263072202946, 0.3311356831684956]
printing an ep nov before normalisation:  40.37609824021432
using explorer policy with actor:  0
printing an ep nov before normalisation:  33.25327623745578
actor:  0 policy actor:  1  step number:  27 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.16589999999999985 0.6775 0.6775
probs:  [0.07004897267119775, 0.04836121814020547, 0.21240950929112412, 0.05294198600694762, 0.28510263072202946, 0.3311356831684956]
maxi score, test score, baseline:  0.16589999999999985 0.6775 0.6775
probs:  [0.07004897267119775, 0.04836121814020547, 0.21240950929112412, 0.05294198600694762, 0.28510263072202946, 0.3311356831684956]
printing an ep nov before normalisation:  48.798678770935766
maxi score, test score, baseline:  0.16589999999999985 0.6775 0.6775
probs:  [0.07004897267119775, 0.04836121814020547, 0.21240950929112412, 0.05294198600694762, 0.28510263072202946, 0.3311356831684956]
maxi score, test score, baseline:  0.16589999999999985 0.6775 0.6775
probs:  [0.07004897267119775, 0.04836121814020547, 0.21240950929112412, 0.05294198600694762, 0.28510263072202946, 0.3311356831684956]
printing an ep nov before normalisation:  38.145694732666016
printing an ep nov before normalisation:  33.74399185180664
actor:  0 policy actor:  0  step number:  45 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.16529999999999986 0.6775 0.6775
maxi score, test score, baseline:  0.16529999999999986 0.6775 0.6775
maxi score, test score, baseline:  0.16529999999999986 0.6775 0.6775
probs:  [0.07004897267119775, 0.04836121814020547, 0.21240950929112412, 0.05294198600694762, 0.28510263072202946, 0.3311356831684956]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  35.96887069914717
actor:  1 policy actor:  1  step number:  46 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  42.4694106326841
actor:  1 policy actor:  1  step number:  37 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.667
using another actor
printing an ep nov before normalisation:  31.984000431952516
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  0
printing an ep nov before normalisation:  47.42750274431619
printing an ep nov before normalisation:  45.26762208036926
maxi score, test score, baseline:  0.16529999999999986 0.6775 0.6775
probs:  [0.07004897267119775, 0.04836121814020547, 0.21240950929112412, 0.05294198600694762, 0.28510263072202946, 0.3311356831684956]
maxi score, test score, baseline:  0.16529999999999986 0.6775 0.6775
probs:  [0.07004897267119775, 0.04836121814020547, 0.21240950929112412, 0.05294198600694762, 0.28510263072202946, 0.3311356831684956]
Printing some Q and Qe and total Qs values:  [[0.69 ]
 [0.767]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]] [[33.926]
 [40.105]
 [33.926]
 [33.926]
 [33.926]
 [33.926]
 [33.926]] [[0.69 ]
 [0.767]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]]
actor:  0 policy actor:  0  step number:  40 total reward:  0.32999999999999974  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  29 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.085]
 [0.085]
 [0.231]
 [0.085]
 [0.085]
 [0.085]
 [0.085]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.085]
 [0.085]
 [0.231]
 [0.085]
 [0.085]
 [0.085]
 [0.085]]
actor:  0 policy actor:  1  step number:  30 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.318]
 [0.493]
 [0.452]
 [0.312]
 [0.359]
 [0.379]
 [0.321]] [[22.856]
 [32.636]
 [31.059]
 [23.234]
 [23.01 ]
 [23.826]
 [23.089]] [[0.798]
 [1.444]
 [1.327]
 [0.81 ]
 [0.846]
 [0.906]
 [0.813]]
maxi score, test score, baseline:  0.16077999999999987 0.6775 0.6775
probs:  [0.07004897267119774, 0.04836121814020544, 0.21240950929112412, 0.0529419860069476, 0.28510263072202957, 0.33113568316849545]
printing an ep nov before normalisation:  0.007809442270456657
siam score:  -0.80254656
maxi score, test score, baseline:  0.16077999999999987 0.6775 0.6775
probs:  [0.07004897267119774, 0.04836121814020544, 0.21240950929112412, 0.0529419860069476, 0.28510263072202957, 0.33113568316849545]
actor:  1 policy actor:  1  step number:  41 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.16077999999999987 0.6775 0.6775
actor:  1 policy actor:  1  step number:  52 total reward:  0.2699999999999996  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  44.29319338654553
actor:  0 policy actor:  1  step number:  19 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  21 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.16073999999999986 0.6775 0.6775
probs:  [0.06998911651354986, 0.048364324552755085, 0.21242318221610182, 0.05294538747127904, 0.28512098588274165, 0.3311570033635725]
maxi score, test score, baseline:  0.16073999999999986 0.6775 0.6775
probs:  [0.06998911651354986, 0.048364324552755085, 0.21242318221610182, 0.05294538747127904, 0.28512098588274165, 0.3311570033635725]
printing an ep nov before normalisation:  51.61396769754445
Printing some Q and Qe and total Qs values:  [[0.918]
 [0.918]
 [0.918]
 [0.918]
 [0.918]
 [0.918]
 [0.918]] [[45.219]
 [45.219]
 [45.219]
 [45.219]
 [45.219]
 [45.219]
 [45.219]] [[0.918]
 [0.918]
 [0.918]
 [0.918]
 [0.918]
 [0.918]
 [0.918]]
actor:  1 policy actor:  1  step number:  21 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  0 policy actor:  0  step number:  22 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  34 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.16067999999999985 0.6775 0.6775
probs:  [0.06998911651354986, 0.048364324552755085, 0.21242318221610182, 0.05294538747127904, 0.28512098588274165, 0.3311570033635725]
maxi score, test score, baseline:  0.16067999999999985 0.6775 0.6775
probs:  [0.06998911651354986, 0.048364324552755085, 0.21242318221610182, 0.05294538747127904, 0.28512098588274165, 0.3311570033635725]
maxi score, test score, baseline:  0.16067999999999985 0.6775 0.6775
probs:  [0.06998911651354986, 0.048364324552755085, 0.21242318221610182, 0.05294538747127904, 0.28512098588274165, 0.3311570033635725]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.16067999999999985 0.6775 0.6775
probs:  [0.06998911651354986, 0.048364324552755085, 0.21242318221610182, 0.05294538747127904, 0.28512098588274165, 0.3311570033635725]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  12.947200798310307
maxi score, test score, baseline:  0.15731999999999985 0.6775 0.6775
probs:  [0.06998977551202507, 0.04836477954974985, 0.21242518489271842, 0.052936459540268925, 0.2851236743675545, 0.3311601261376834]
printing an ep nov before normalisation:  53.9732873673526
maxi score, test score, baseline:  0.15731999999999985 0.6775 0.6775
probs:  [0.06998977551202507, 0.04836477954974985, 0.21242518489271842, 0.052936459540268925, 0.2851236743675545, 0.3311601261376834]
printing an ep nov before normalisation:  53.55884742583037
maxi score, test score, baseline:  0.15731999999999985 0.6775 0.6775
probs:  [0.06998977551202507, 0.04836477954974985, 0.21242518489271842, 0.052936459540268925, 0.2851236743675545, 0.3311601261376834]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.15731999999999985 0.6775 0.6775
probs:  [0.06998977551202507, 0.04836477954974985, 0.21242518489271842, 0.052936459540268925, 0.2851236743675545, 0.3311601261376834]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.487]
 [0.425]
 [0.425]
 [0.432]
 [0.425]
 [0.451]] [[36.979]
 [39.241]
 [33.855]
 [33.855]
 [40.251]
 [33.855]
 [37.865]] [[1.665]
 [1.806]
 [1.412]
 [1.412]
 [1.814]
 [1.412]
 [1.686]]
maxi score, test score, baseline:  0.15731999999999985 0.6775 0.6775
probs:  [0.06993026537061846, 0.048367868036087586, 0.2124387789151847, 0.0529398407721306, 0.2851419236059219, 0.3311813233000568]
printing an ep nov before normalisation:  50.96117655012268
printing an ep nov before normalisation:  54.547597335454824
maxi score, test score, baseline:  0.15731999999999985 0.6775 0.6775
probs:  [0.06993026537061846, 0.048367868036087565, 0.21243877891518478, 0.05293984077213058, 0.285141923605922, 0.3311813233000566]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 35.91036315647603
maxi score, test score, baseline:  0.15731999999999985 0.6775 0.6775
probs:  [0.06993026537061846, 0.048367868036087586, 0.2124387789151847, 0.0529398407721306, 0.2851419236059219, 0.3311813233000568]
maxi score, test score, baseline:  0.15731999999999985 0.6775 0.6775
probs:  [0.06993026537061846, 0.048367868036087586, 0.2124387789151847, 0.0529398407721306, 0.2851419236059219, 0.3311813233000568]
printing an ep nov before normalisation:  28.65843750511034
maxi score, test score, baseline:  0.15731999999999985 0.6775 0.6775
probs:  [0.06993026537061846, 0.048367868036087586, 0.2124387789151847, 0.0529398407721306, 0.2851419236059219, 0.3311813233000568]
Printing some Q and Qe and total Qs values:  [[0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.561]
 [0.579]] [[46.402]
 [46.402]
 [46.402]
 [46.402]
 [46.402]
 [52.018]
 [46.402]] [[1.894]
 [1.894]
 [1.894]
 [1.894]
 [1.894]
 [2.131]
 [1.894]]
printing an ep nov before normalisation:  27.879220645501487
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  30 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.15731999999999985 0.6775 0.6775
maxi score, test score, baseline:  0.15731999999999985 0.6775 0.6775
probs:  [0.06993026537061846, 0.048367868036087586, 0.2124387789151847, 0.0529398407721306, 0.2851419236059219, 0.3311813233000568]
maxi score, test score, baseline:  0.15731999999999985 0.6775 0.6775
Printing some Q and Qe and total Qs values:  [[0.338]
 [0.338]
 [0.535]
 [0.338]
 [0.338]
 [0.338]
 [0.338]] [[34.426]
 [34.426]
 [48.749]
 [34.426]
 [34.426]
 [34.426]
 [34.426]] [[0.724]
 [0.724]
 [1.228]
 [0.724]
 [0.724]
 [0.724]
 [0.724]]
actor:  1 policy actor:  1  step number:  22 total reward:  0.69  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  47.93440243879705
actor:  1 policy actor:  1  step number:  24 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
using another actor
maxi score, test score, baseline:  0.15731999999999985 0.6775 0.6775
probs:  [0.07592879375371445, 0.04805655348499191, 0.21106852289363667, 0.05259901793400181, 0.2833024291646394, 0.32904468276901566]
printing an ep nov before normalisation:  44.25062102420231
maxi score, test score, baseline:  0.15395999999999985 0.6775 0.6775
probs:  [0.07592879375371445, 0.04805655348499191, 0.21106852289363667, 0.05259901793400181, 0.2833024291646394, 0.32904468276901566]
actor:  1 policy actor:  1  step number:  34 total reward:  0.44999999999999984  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  34.11907911300659
printing an ep nov before normalisation:  55.576700488421906
actor:  1 policy actor:  1  step number:  31 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  23 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  24 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.15395999999999985 0.6775 0.6775
probs:  [0.07592879375371445, 0.04805655348499191, 0.21106852289363667, 0.05259901793400181, 0.2833024291646394, 0.32904468276901566]
actor:  1 policy actor:  1  step number:  19 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
from probs:  [0.07592879375371445, 0.04805655348499191, 0.21106852289363667, 0.05259901793400181, 0.2833024291646394, 0.32904468276901566]
maxi score, test score, baseline:  0.15395999999999985 0.6775 0.6775
probs:  [0.07593142109209863, 0.048069026371799245, 0.21102341367642555, 0.052609886247468055, 0.2832318041505231, 0.32913444846168544]
maxi score, test score, baseline:  0.15395999999999985 0.6775 0.6775
probs:  [0.07593142109209863, 0.048069026371799245, 0.21102341367642555, 0.052609886247468055, 0.2832318041505231, 0.32913444846168544]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  43.944124828625014
actor:  0 policy actor:  0  step number:  25 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]] [[58.486]
 [58.486]
 [58.486]
 [58.486]
 [58.486]
 [58.486]
 [58.486]] [[2.21]
 [2.21]
 [2.21]
 [2.21]
 [2.21]
 [2.21]
 [2.21]]
printing an ep nov before normalisation:  57.72445346978313
actor:  1 policy actor:  1  step number:  22 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.15347999999999987 0.6775 0.6775
probs:  [0.07593142109209863, 0.048069026371799245, 0.21102341367642555, 0.052609886247468055, 0.2832318041505231, 0.32913444846168544]
maxi score, test score, baseline:  0.15347999999999987 0.6775 0.6775
probs:  [0.07593142109209863, 0.048069026371799245, 0.21102341367642555, 0.052609886247468055, 0.2832318041505231, 0.32913444846168544]
printing an ep nov before normalisation:  60.169637323899735
maxi score, test score, baseline:  0.15347999999999987 0.6775 0.6775
probs:  [0.07593142109209863, 0.048069026371799245, 0.21102341367642555, 0.052609886247468055, 0.2832318041505231, 0.32913444846168544]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  29 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.852]
 [0.815]
 [0.815]] [[38.634]
 [38.634]
 [38.634]
 [38.634]
 [50.325]
 [38.634]
 [38.634]] [[1.836]
 [1.836]
 [1.836]
 [1.836]
 [2.494]
 [1.836]
 [1.836]]
maxi score, test score, baseline:  0.15347999999999987 0.6775 0.6775
actor:  1 policy actor:  1  step number:  24 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.15347999999999987 0.6775 0.6775
probs:  [0.07593142109209863, 0.048069026371799245, 0.21102341367642555, 0.052609886247468055, 0.2832318041505231, 0.32913444846168544]
actor:  1 policy actor:  1  step number:  33 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8190287
maxi score, test score, baseline:  0.15347999999999987 0.6775 0.6775
probs:  [0.07593142109209863, 0.048069026371799245, 0.21102341367642555, 0.052609886247468055, 0.2832318041505231, 0.32913444846168544]
maxi score, test score, baseline:  0.15347999999999987 0.6775 0.6775
maxi score, test score, baseline:  0.15015999999999985 0.6775 0.6775
probs:  [0.07593142109209863, 0.048069026371799245, 0.21102341367642555, 0.052609886247468055, 0.2832318041505231, 0.32913444846168544]
maxi score, test score, baseline:  0.15015999999999985 0.6775 0.6775
probs:  [0.07593142109209863, 0.048069026371799245, 0.21102341367642555, 0.052609886247468055, 0.2832318041505231, 0.32913444846168544]
maxi score, test score, baseline:  0.15015999999999985 0.6775 0.6775
probs:  [0.07593142109209863, 0.048069026371799245, 0.21102341367642555, 0.052609886247468055, 0.2832318041505231, 0.32913444846168544]
using explorer policy with actor:  1
printing an ep nov before normalisation:  52.47951423444273
maxi score, test score, baseline:  0.15015999999999985 0.6775 0.6775
probs:  [0.07593142109209863, 0.048069026371799245, 0.21102341367642555, 0.052609886247468055, 0.2832318041505231, 0.32913444846168544]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  35 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  39 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.277]
 [0.331]
 [0.297]
 [0.303]
 [0.322]
 [0.247]
 [0.249]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.277]
 [0.331]
 [0.297]
 [0.303]
 [0.322]
 [0.247]
 [0.249]]
printing an ep nov before normalisation:  54.84214025038666
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.455]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]] [[34.837]
 [41.697]
 [35.328]
 [35.328]
 [35.328]
 [35.328]
 [35.328]] [[1.265]
 [1.78 ]
 [1.192]
 [1.192]
 [1.192]
 [1.192]
 [1.192]]
Printing some Q and Qe and total Qs values:  [[0.921]
 [0.915]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]] [[32.569]
 [42.057]
 [32.791]
 [32.791]
 [32.791]
 [32.791]
 [32.791]] [[0.921]
 [0.915]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]]
maxi score, test score, baseline:  0.15015999999999988 0.6775 0.6775
probs:  [0.07585568315708817, 0.04807295809029897, 0.21104071099608895, 0.052614190409588736, 0.2832550240388492, 0.32916143330808595]
actor:  0 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  35 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.15325999999999984 0.6775 0.6775
actor:  1 policy actor:  1  step number:  29 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.15325999999999984 0.6775 0.6775
probs:  [0.07585568315708817, 0.04807295809029897, 0.21104071099608895, 0.052614190409588736, 0.2832550240388492, 0.32916143330808595]
actor:  1 policy actor:  1  step number:  31 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.036]] [[1.852]
 [1.852]
 [1.852]
 [1.852]
 [1.852]
 [1.852]
 [1.852]] [[0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]]
actor:  1 policy actor:  1  step number:  33 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.15325999999999984 0.6775 0.6775
probs:  [0.07585568315708817, 0.04807295809029897, 0.21104071099608895, 0.052614190409588736, 0.2832550240388492, 0.32916143330808595]
Printing some Q and Qe and total Qs values:  [[0.221]
 [0.346]
 [0.221]
 [0.221]
 [0.221]
 [0.221]
 [0.221]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.221]
 [0.346]
 [0.221]
 [0.221]
 [0.221]
 [0.221]
 [0.221]]
printing an ep nov before normalisation:  33.71435240225586
maxi score, test score, baseline:  0.15325999999999984 0.6775 0.6775
probs:  [0.07585568315708817, 0.04807295809029897, 0.21104071099608895, 0.052614190409588736, 0.2832550240388492, 0.32916143330808595]
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.623]
 [0.546]
 [0.549]
 [0.535]
 [0.54 ]
 [0.524]] [[36.232]
 [42.457]
 [36.232]
 [34.901]
 [34.102]
 [34.977]
 [35.668]] [[1.048]
 [1.302]
 [1.048]
 [1.013]
 [0.976]
 [1.006]
 [1.01 ]]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  50.310594678691
actor:  1 policy actor:  1  step number:  28 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.15325999999999984 0.6775 0.6775
probs:  [0.07585568315708817, 0.04807295809029897, 0.21104071099608895, 0.052614190409588736, 0.2832550240388492, 0.32916143330808595]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.15325999999999984 0.6775 0.6775
probs:  [0.07585568315708817, 0.04807295809029897, 0.21104071099608895, 0.052614190409588736, 0.2832550240388492, 0.32916143330808595]
maxi score, test score, baseline:  0.15325999999999984 0.6775 0.6775
probs:  [0.07585568315708817, 0.04807295809029897, 0.21104071099608895, 0.052614190409588736, 0.2832550240388492, 0.32916143330808595]
actor:  1 policy actor:  1  step number:  31 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.15325999999999984 0.6775 0.6775
probs:  [0.07585568315708817, 0.04807295809029897, 0.21104071099608895, 0.052614190409588736, 0.2832550240388492, 0.32916143330808595]
maxi score, test score, baseline:  0.15325999999999984 0.6775 0.6775
probs:  [0.07585568315708817, 0.04807295809029897, 0.21104071099608895, 0.052614190409588736, 0.2832550240388492, 0.32916143330808595]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.75922341953485
Printing some Q and Qe and total Qs values:  [[ 0.153]
 [-0.065]
 [ 0.327]
 [ 0.153]
 [ 0.153]
 [ 0.153]
 [ 0.153]] [[32.783]
 [37.872]
 [47.321]
 [32.783]
 [32.783]
 [32.783]
 [32.783]] [[0.881]
 [0.86 ]
 [1.617]
 [0.881]
 [0.881]
 [0.881]
 [0.881]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.15325999999999984 0.6775 0.6775
probs:  [0.07585568315708817, 0.04807295809029897, 0.21104071099608895, 0.052614190409588736, 0.2832550240388492, 0.32916143330808595]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.15325999999999984 0.6775 0.6775
probs:  [0.07585568315708817, 0.04807295809029897, 0.21104071099608895, 0.052614190409588736, 0.2832550240388492, 0.32916143330808595]
Starting evaluation
actor:  1 policy actor:  1  step number:  44 total reward:  0.3899999999999998  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[ 0.868]
 [ 0.942]
 [ 0.868]
 [ 0.868]
 [ 0.868]
 [-0.007]
 [ 0.868]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.868]
 [ 0.942]
 [ 0.868]
 [ 0.868]
 [ 0.868]
 [-0.007]
 [ 0.868]]
Printing some Q and Qe and total Qs values:  [[ 0.883]
 [ 0.842]
 [ 0.816]
 [ 0.86 ]
 [ 0.857]
 [-0.007]
 [ 0.846]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.883]
 [ 0.842]
 [ 0.816]
 [ 0.86 ]
 [ 0.857]
 [-0.007]
 [ 0.846]]
maxi score, test score, baseline:  0.15325999999999984 0.6775 0.6775
probs:  [0.07585568315708817, 0.04807295809029897, 0.21104071099608895, 0.052614190409588736, 0.2832550240388492, 0.32916143330808595]
maxi score, test score, baseline:  0.15325999999999984 0.6775 0.6775
probs:  [0.07585568315708817, 0.04807295809029897, 0.21104071099608895, 0.052614190409588736, 0.2832550240388492, 0.32916143330808595]
printing an ep nov before normalisation:  57.26678863085202
maxi score, test score, baseline:  0.15325999999999984 0.6775 0.6775
probs:  [0.07585568315708817, 0.04807295809029897, 0.21104071099608895, 0.052614190409588736, 0.2832550240388492, 0.32916143330808595]
printing an ep nov before normalisation:  43.183501415194605
printing an ep nov before normalisation:  43.91549318960209
printing an ep nov before normalisation:  65.31216285900784
printing an ep nov before normalisation:  61.414833861658806
Printing some Q and Qe and total Qs values:  [[0.78 ]
 [0.881]
 [0.78 ]
 [0.78 ]
 [0.78 ]
 [0.78 ]
 [0.78 ]] [[32.678]
 [40.688]
 [32.678]
 [32.678]
 [32.678]
 [32.678]
 [32.678]] [[0.78 ]
 [0.881]
 [0.78 ]
 [0.78 ]
 [0.78 ]
 [0.78 ]
 [0.78 ]]
Printing some Q and Qe and total Qs values:  [[0.922]
 [0.979]
 [0.922]
 [0.922]
 [0.922]
 [0.922]
 [0.922]] [[44.545]
 [50.432]
 [44.545]
 [44.545]
 [44.545]
 [44.545]
 [44.545]] [[0.922]
 [0.979]
 [0.922]
 [0.922]
 [0.922]
 [0.922]
 [0.922]]
printing an ep nov before normalisation:  54.21653865989915
printing an ep nov before normalisation:  32.67070954219415
printing an ep nov before normalisation:  31.319127074403347
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  40 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  19 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.19023999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.07496855654536556, 0.04820004589012875, 0.2120429839307867, 0.052479493678893624, 0.28240994392145014, 0.32989897603337537]
maxi score, test score, baseline:  0.19023999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.07496855654536556, 0.04820004589012875, 0.2120429839307867, 0.052479493678893624, 0.28240994392145014, 0.32989897603337537]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  42 total reward:  0.14999999999999947  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  60.201649990522974
actor:  1 policy actor:  1  step number:  21 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  44.287533318478886
printing an ep nov before normalisation:  43.43488162027604
siam score:  -0.8177762
actor:  1 policy actor:  1  step number:  25 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.396]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]] [[33.423]
 [41.385]
 [33.423]
 [33.423]
 [33.423]
 [33.423]
 [33.423]] [[1.255]
 [1.556]
 [1.255]
 [1.255]
 [1.255]
 [1.255]
 [1.255]]
maxi score, test score, baseline:  0.19023999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.07520870246516384, 0.048404779256543504, 0.21294558158008436, 0.05270245470771474, 0.27943491848515406, 0.3313035635053394]
maxi score, test score, baseline:  0.19023999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.07520870246516384, 0.048404779256543504, 0.21294558158008436, 0.05270245470771474, 0.27943491848515406, 0.3313035635053394]
actor:  1 policy actor:  1  step number:  38 total reward:  0.46999999999999986  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  30 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  39.66928720474243
Printing some Q and Qe and total Qs values:  [[0.724]
 [0.741]
 [0.741]
 [0.734]
 [0.73 ]
 [0.729]
 [0.734]] [[50.315]
 [47.424]
 [47.424]
 [48.562]
 [49.138]
 [49.958]
 [49.851]] [[2.291]
 [2.169]
 [2.169]
 [2.216]
 [2.24 ]
 [2.278]
 [2.278]]
maxi score, test score, baseline:  0.19017999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.0752093655267673, 0.04840520558768162, 0.2129474611246345, 0.05269409412210868, 0.27943738525910145, 0.3313064883797065]
maxi score, test score, baseline:  0.19017999999999985 0.6779999999999999 0.6779999999999999
actor:  1 policy actor:  1  step number:  32 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  52.94090440379861
actor:  0 policy actor:  1  step number:  32 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  19 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  47.270130055595104
maxi score, test score, baseline:  0.19093999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.0752100258825455, 0.04840563017904547, 0.21294933299912977, 0.05268576765442274, 0.279439841966625, 0.33130940131823144]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  18 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.19093999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.0752100258825455, 0.04840563017904547, 0.21294933299912977, 0.05268576765442274, 0.279439841966625, 0.33130940131823144]
actor:  1 policy actor:  1  step number:  40 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.19093999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.0752100258825455, 0.04840563017904547, 0.21294933299912977, 0.05268576765442274, 0.279439841966625, 0.33130940131823144]
printing an ep nov before normalisation:  48.13769828669485
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  22 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.19093999999999986 0.6779999999999999 0.6779999999999999
printing an ep nov before normalisation:  47.629065367595466
printing an ep nov before normalisation:  44.44089899188843
maxi score, test score, baseline:  0.19093999999999986 0.6779999999999999 0.6779999999999999
printing an ep nov before normalisation:  27.42448329925537
maxi score, test score, baseline:  0.19093999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.0752100258825455, 0.04840563017904547, 0.21294933299912977, 0.05268576765442274, 0.279439841966625, 0.33130940131823144]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.19093999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.0752100258825455, 0.04840563017904547, 0.21294933299912977, 0.05268576765442274, 0.279439841966625, 0.33130940131823144]
printing an ep nov before normalisation:  38.599419553816695
printing an ep nov before normalisation:  38.346428871154785
maxi score, test score, baseline:  0.19093999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.0752100258825455, 0.04840563017904547, 0.21294933299912977, 0.05268576765442274, 0.279439841966625, 0.33130940131823144]
actor:  0 policy actor:  1  step number:  24 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.257]
 [0.272]
 [0.257]
 [0.257]
 [0.257]
 [0.257]
 [0.257]] [[30.385]
 [37.491]
 [30.385]
 [30.385]
 [30.385]
 [30.385]
 [30.385]] [[0.861]
 [1.164]
 [0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]]
actor:  1 policy actor:  1  step number:  25 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.19399999999999984 0.6779999999999999 0.6779999999999999
probs:  [0.0752100258825455, 0.04840563017904547, 0.21294933299912977, 0.05268576765442274, 0.279439841966625, 0.33130940131823144]
printing an ep nov before normalisation:  54.227691905215025
maxi score, test score, baseline:  0.19399999999999984 0.6779999999999999 0.6779999999999999
probs:  [0.0752100258825455, 0.04840563017904547, 0.21294933299912977, 0.05268576765442274, 0.279439841966625, 0.33130940131823144]
siam score:  -0.80482394
maxi score, test score, baseline:  0.19399999999999984 0.6779999999999999 0.6779999999999999
printing an ep nov before normalisation:  30.902239945628587
actor:  1 policy actor:  1  step number:  26 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  33 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  60.71644139854179
printing an ep nov before normalisation:  49.48830547817939
siam score:  -0.8041472
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([0.8041, 0.0013, 0.0259, 0.0225, 0.0738, 0.0280, 0.0444],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0050,     0.9739,     0.0073,     0.0051,     0.0005,     0.0006,
            0.0076], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0933, 0.0064, 0.5541, 0.0734, 0.0665, 0.1045, 0.1018],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0998, 0.0302, 0.2392, 0.2493, 0.0902, 0.1072, 0.1841],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2216, 0.0094, 0.1261, 0.1016, 0.2678, 0.1488, 0.1248],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1453, 0.0490, 0.0985, 0.1075, 0.1234, 0.3476, 0.1286],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1118, 0.0747, 0.1350, 0.1787, 0.1322, 0.0789, 0.2888],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.19399999999999984 0.6779999999999999 0.6779999999999999
probs:  [0.0752100258825455, 0.04840563017904547, 0.21294933299912977, 0.05268576765442274, 0.279439841966625, 0.33130940131823144]
actor:  1 policy actor:  1  step number:  27 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.19399999999999984 0.6779999999999999 0.6779999999999999
probs:  [0.0752100258825455, 0.04840563017904547, 0.21294933299912977, 0.05268576765442274, 0.279439841966625, 0.33130940131823144]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.19399999999999984 0.6779999999999999 0.6779999999999999
probs:  [0.0752100258825455, 0.04840563017904547, 0.21294933299912977, 0.05268576765442274, 0.279439841966625, 0.33130940131823144]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  30 total reward:  0.5499999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1970999999999999 0.6779999999999999 0.6779999999999999
probs:  [0.0752100258825455, 0.04840563017904547, 0.21294933299912977, 0.05268576765442274, 0.279439841966625, 0.33130940131823144]
actor:  0 policy actor:  0  step number:  23 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.19753999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.0752100258825455, 0.04840563017904547, 0.21294933299912977, 0.05268576765442274, 0.279439841966625, 0.33130940131823144]
maxi score, test score, baseline:  0.19753999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.0752100258825455, 0.04840563017904547, 0.21294933299912977, 0.05268576765442274, 0.279439841966625, 0.33130940131823144]
siam score:  -0.80973625
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19753999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.0752100258825455, 0.04840563017904547, 0.21294933299912977, 0.05268576765442274, 0.279439841966625, 0.33130940131823144]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  32 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.19753999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.0752100258825455, 0.04840563017904547, 0.21294933299912977, 0.05268576765442274, 0.279439841966625, 0.33130940131823144]
actor:  1 policy actor:  1  step number:  35 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  34 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  58.00692765471138
actor:  1 policy actor:  1  step number:  20 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.19753999999999985 0.6779999999999999 0.6779999999999999
printing an ep nov before normalisation:  48.421207687643346
Printing some Q and Qe and total Qs values:  [[0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]] [[41.824]
 [41.824]
 [41.824]
 [41.824]
 [41.824]
 [41.824]
 [41.824]] [[2.458]
 [2.458]
 [2.458]
 [2.458]
 [2.458]
 [2.458]
 [2.458]]
printing an ep nov before normalisation:  19.965428113937378
maxi score, test score, baseline:  0.19753999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.0752100258825455, 0.04840563017904547, 0.21294933299912977, 0.05268576765442274, 0.279439841966625, 0.33130940131823144]
actor:  1 policy actor:  1  step number:  21 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  0.0991545394356308
actor:  1 policy actor:  1  step number:  47 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  27 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
siam score:  -0.811721
maxi score, test score, baseline:  0.19753999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.0752100258825455, 0.04840563017904547, 0.21294933299912977, 0.05268576765442274, 0.279439841966625, 0.33130940131823144]
maxi score, test score, baseline:  0.19753999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.0752100258825455, 0.04840563017904547, 0.21294933299912977, 0.05268576765442274, 0.279439841966625, 0.33130940131823144]
siam score:  -0.8126341
printing an ep nov before normalisation:  34.84378305295878
printing an ep nov before normalisation:  36.62571580712181
actor:  0 policy actor:  0  step number:  25 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19725999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.0752100258825455, 0.04840563017904547, 0.21294933299912977, 0.05268576765442274, 0.279439841966625, 0.33130940131823144]
maxi score, test score, baseline:  0.19725999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.0752100258825455, 0.04840563017904547, 0.21294933299912977, 0.05268576765442274, 0.279439841966625, 0.33130940131823144]
actor:  1 policy actor:  1  step number:  26 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.19725999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.07513774501795184, 0.04840940583215963, 0.21296597853026655, 0.052689878080890776, 0.2794616880902212, 0.33133530444851006]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 34.604623867697484
printing an ep nov before normalisation:  50.023086600523996
printing an ep nov before normalisation:  60.05994685682809
actor:  1 policy actor:  1  step number:  35 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  29 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.224]
 [0.285]
 [0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.324]] [[19.429]
 [30.539]
 [19.429]
 [19.429]
 [19.429]
 [19.429]
 [27.879]] [[0.756]
 [1.247]
 [0.756]
 [0.756]
 [0.756]
 [0.756]
 [1.182]]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  51.52821623894807
maxi score, test score, baseline:  0.19725999999999985 0.6779999999999999 0.6779999999999999
printing an ep nov before normalisation:  46.77108122762205
maxi score, test score, baseline:  0.19725999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.07513774501795184, 0.04840940583215963, 0.21296597853026655, 0.052689878080890776, 0.2794616880902212, 0.33133530444851006]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  53.10256746226722
actor:  1 policy actor:  1  step number:  39 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19725999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.07513774501795184, 0.04840940583215963, 0.21296597853026655, 0.052689878080890776, 0.2794616880902212, 0.33133530444851006]
actor:  0 policy actor:  0  step number:  21 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  33 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  49.787397709694034
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19767999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.07513774501795184, 0.04840940583215963, 0.21296597853026655, 0.052689878080890776, 0.2794616880902212, 0.33133530444851006]
actor:  1 policy actor:  1  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.19767999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.07506587318151067, 0.04841316011933653, 0.21298252986647526, 0.0526939652469819, 0.2794834105894025, 0.3313610609962931]
actor:  1 policy actor:  1  step number:  42 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.19767999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.07506587318151067, 0.04841316011933653, 0.21298252986647526, 0.0526939652469819, 0.2794834105894025, 0.3313610609962931]
actor:  1 policy actor:  1  step number:  27 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.19767999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.07499440691106124, 0.04841689322142516, 0.21299898780505544, 0.05269802934958032, 0.2795050105105705, 0.3313866722023074]
printing an ep nov before normalisation:  19.88718271255493
maxi score, test score, baseline:  0.19767999999999986 0.6779999999999999 0.6779999999999999
Printing some Q and Qe and total Qs values:  [[0.72 ]
 [0.768]
 [0.743]
 [0.715]
 [0.72 ]
 [0.72 ]
 [0.72 ]] [[13.764]
 [23.759]
 [24.641]
 [23.53 ]
 [13.764]
 [13.764]
 [13.764]] [[1.268]
 [1.713]
 [1.723]
 [1.651]
 [1.268]
 [1.268]
 [1.268]]
actor:  1 policy actor:  1  step number:  33 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  37 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.19767999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.07492334278340594, 0.04842060531723912, 0.21301535313433387, 0.05270207058335464, 0.2795264888883503, 0.33141213929331614]
maxi score, test score, baseline:  0.19767999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.07492334278340594, 0.04842060531723912, 0.21301535313433387, 0.05270207058335464, 0.2795264888883503, 0.33141213929331614]
actor:  0 policy actor:  1  step number:  24 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  24.048399925231934
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.224]
 [0.367]
 [0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.224]] [[25.045]
 [37.295]
 [25.045]
 [25.045]
 [25.045]
 [25.045]
 [25.045]] [[0.539]
 [1.014]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8215825
actor:  1 policy actor:  1  step number:  39 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  34 total reward:  0.1899999999999995  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.206]
 [0.26 ]
 [0.215]
 [0.218]
 [0.212]
 [0.205]
 [0.209]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.206]
 [0.26 ]
 [0.215]
 [0.218]
 [0.212]
 [0.205]
 [0.209]]
printing an ep nov before normalisation:  46.50801860190092
printing an ep nov before normalisation:  47.38377456022191
printing an ep nov before normalisation:  24.866214296652075
actor:  1 policy actor:  1  step number:  27 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.0
from probs:  [0.07485333220300784, 0.048424719772899714, 0.21303349232714075, 0.05269779355401966, 0.2795502953409492, 0.3314403668019828]
printing an ep nov before normalisation:  33.1195330619812
Printing some Q and Qe and total Qs values:  [[ 0.621]
 [ 0.718]
 [ 0.192]
 [ 0.674]
 [ 0.704]
 [-0.006]
 [ 0.672]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.621]
 [ 0.718]
 [ 0.192]
 [ 0.674]
 [ 0.704]
 [-0.006]
 [ 0.672]]
siam score:  -0.82048273
maxi score, test score, baseline:  0.19769999999999985 0.6779999999999999 0.6779999999999999
actor:  0 policy actor:  0  step number:  27 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  30 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
using another actor
maxi score, test score, baseline:  0.20045999999999983 0.6779999999999999 0.6779999999999999
probs:  [0.07485333220300784, 0.048424719772899714, 0.21303349232714075, 0.05269779355401966, 0.2795502953409492, 0.3314403668019828]
actor:  0 policy actor:  1  step number:  22 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.20359999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.07485333220300784, 0.048424719772899714, 0.21303349232714075, 0.05269779355401966, 0.2795502953409492, 0.3314403668019828]
actor:  1 policy actor:  1  step number:  18 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.20359999999999986 0.6779999999999999 0.6779999999999999
maxi score, test score, baseline:  0.20359999999999986 0.6779999999999999 0.6779999999999999
actor:  0 policy actor:  1  step number:  29 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  29.62640921274821
using explorer policy with actor:  0
printing an ep nov before normalisation:  44.30880843314649
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  27 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  25 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  1  step number:  26 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  38.28420215715971
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]] [[39.431]
 [39.431]
 [39.431]
 [39.431]
 [39.431]
 [39.431]
 [39.431]] [[1.756]
 [1.756]
 [1.756]
 [1.756]
 [1.756]
 [1.756]
 [1.756]]
siam score:  -0.80997926
maxi score, test score, baseline:  0.20471999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07486201404698706, 0.048430330835750775, 0.21305822953645884, 0.052703901111885776, 0.27946666258075015, 0.3314788618881674]
printing an ep nov before normalisation:  39.332076088323426
actor:  0 policy actor:  0  step number:  30 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  50.32295761260698
actor:  1 policy actor:  1  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  21 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  22 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.218]
 [0.187]
 [0.151]
 [0.163]
 [0.148]
 [0.16 ]
 [0.154]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.218]
 [0.187]
 [0.151]
 [0.163]
 [0.148]
 [0.16 ]
 [0.154]]
actor:  1 policy actor:  1  step number:  31 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  24 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.20891999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.07479173601691441, 0.04843400236487861, 0.21307441602143778, 0.052707897516346434, 0.2794878974118127, 0.33150405066861005]
maxi score, test score, baseline:  0.20891999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.07479173601691441, 0.04843400236487861, 0.21307441602143778, 0.052707897516346434, 0.2794878974118127, 0.33150405066861005]
printing an ep nov before normalisation:  34.284126540336864
actor:  1 policy actor:  1  step number:  29 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  32.8377103805542
printing an ep nov before normalisation:  54.07473678058523
actor:  1 policy actor:  1  step number:  33 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  32.78515815734863
actor:  1 policy actor:  1  step number:  28 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.0
from probs:  [0.07479238773261739, 0.04843442399630741, 0.21307627484661695, 0.05269963414469934, 0.2794903359794638, 0.33150694330029506]
maxi score, test score, baseline:  0.20891999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.07479238773261739, 0.04843442399630741, 0.21307627484661695, 0.05269963414469934, 0.2794903359794638, 0.33150694330029506]
actor:  1 policy actor:  1  step number:  20 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.977]
 [0.88 ]
 [0.977]
 [0.977]
 [0.977]
 [0.977]
 [0.977]] [[65.669]
 [64.329]
 [65.669]
 [65.669]
 [65.669]
 [65.669]
 [65.669]] [[0.977]
 [0.88 ]
 [0.977]
 [0.977]
 [0.977]
 [0.977]
 [0.977]]
printing an ep nov before normalisation:  35.85926281242423
siam score:  -0.8080722
maxi score, test score, baseline:  0.20891999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.07479238773261739, 0.04843442399630741, 0.21307627484661695, 0.05269963414469934, 0.2794903359794638, 0.33150694330029506]
actor:  1 policy actor:  1  step number:  29 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  64.50960146494432
printing an ep nov before normalisation:  41.084152439454876
maxi score, test score, baseline:  0.20891999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.07479238773261739, 0.04843442399630741, 0.21307627484661695, 0.05269963414469934, 0.2794903359794638, 0.33150694330029506]
using explorer policy with actor:  0
printing an ep nov before normalisation:  58.15576669720252
printing an ep nov before normalisation:  44.80106552460129
actor:  0 policy actor:  1  step number:  24 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  42 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20849999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.07479238773261739, 0.04843442399630741, 0.21307627484661695, 0.05269963414469934, 0.2794903359794638, 0.33150694330029506]
maxi score, test score, baseline:  0.20849999999999985 0.6779999999999999 0.6779999999999999
printing an ep nov before normalisation:  53.815500922435085
actor:  1 policy actor:  1  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.20849999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.07479238773261739, 0.04843442399630741, 0.21307627484661695, 0.05269963414469934, 0.2794903359794638, 0.33150694330029506]
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.55 ]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]] [[46.056]
 [44.739]
 [41.776]
 [41.776]
 [41.776]
 [41.776]
 [41.776]] [[1.541]
 [1.424]
 [1.286]
 [1.286]
 [1.286]
 [1.286]
 [1.286]]
actor:  1 policy actor:  1  step number:  20 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20849999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.07479238773261739, 0.04843442399630741, 0.21307627484661695, 0.05269963414469934, 0.2794903359794638, 0.33150694330029506]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  42.25671148622201
UNIT TEST: sample policy line 217 mcts : [0.041 0.469 0.061 0.041 0.041 0.306 0.041]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.759]
 [0.805]
 [0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.701]] [[37.424]
 [41.524]
 [37.424]
 [37.424]
 [37.424]
 [37.424]
 [27.859]] [[1.736]
 [1.972]
 [1.736]
 [1.736]
 [1.736]
 [1.736]
 [1.234]]
actor:  0 policy actor:  0  step number:  24 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.808]
 [0.903]
 [0.808]
 [0.808]
 [0.806]
 [0.808]
 [0.808]] [[23.174]
 [20.861]
 [23.174]
 [23.174]
 [23.689]
 [23.174]
 [23.901]] [[1.946]
 [1.928]
 [1.946]
 [1.946]
 [1.97 ]
 [1.946]
 [1.982]]
maxi score, test score, baseline:  0.20897999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.07472250128698432, 0.04843807510236778, 0.21309237129344977, 0.05270360765963369, 0.2795114526906988, 0.3315319919668657]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  20 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.20897999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.07472250128698432, 0.04843807510236778, 0.21309237129344977, 0.05270360765963369, 0.2795114526906988, 0.3315319919668657]
maxi score, test score, baseline:  0.20897999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.07472250128698432, 0.04843807510236778, 0.21309237129344977, 0.05270360765963369, 0.2795114526906988, 0.3315319919668657]
printing an ep nov before normalisation:  58.19016228855989
printing an ep nov before normalisation:  58.87864285906056
maxi score, test score, baseline:  0.20897999999999986 0.6779999999999999 0.6779999999999999
maxi score, test score, baseline:  0.20897999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.07472250128698432, 0.04843807510236778, 0.21309237129344977, 0.05270360765963369, 0.2795114526906988, 0.3315319919668657]
actor:  1 policy actor:  1  step number:  27 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  31 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  33 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.659]
 [0.659]
 [0.659]
 [0.691]
 [0.692]
 [0.691]
 [0.659]] [[54.948]
 [54.948]
 [54.948]
 [56.289]
 [56.988]
 [57.674]
 [54.948]] [[2.135]
 [2.135]
 [2.135]
 [2.22 ]
 [2.248]
 [2.275]
 [2.135]]
printing an ep nov before normalisation:  39.60936821352837
printing an ep nov before normalisation:  47.84460637908889
actor:  1 policy actor:  1  step number:  26 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.21185999999999985 0.6779999999999999 0.6779999999999999
actor:  1 policy actor:  1  step number:  26 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.21185999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.07465300371658427, 0.04844170589226086, 0.21310837817342249, 0.05270755906439294, 0.2795324519003791, 0.3315569012529604]
printing an ep nov before normalisation:  45.24651527404785
actor:  1 policy actor:  1  step number:  19 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  1  action  0 :  tensor([0.7801, 0.0122, 0.0434, 0.0339, 0.0534, 0.0294, 0.0476],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0028,     0.9717,     0.0049,     0.0086,     0.0005,     0.0006,
            0.0110], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0415, 0.0101, 0.7994, 0.0400, 0.0332, 0.0349, 0.0410],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0792, 0.1078, 0.0970, 0.3733, 0.1040, 0.0874, 0.1513],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2600, 0.0023, 0.1668, 0.0868, 0.2633, 0.1195, 0.1012],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0843, 0.0008, 0.1393, 0.0917, 0.1213, 0.4480, 0.1147],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0833, 0.1130, 0.0476, 0.0623, 0.0321, 0.0320, 0.6297],
       grad_fn=<DivBackward0>)
siam score:  -0.8115749
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.596]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]] [[36.554]
 [41.35 ]
 [36.554]
 [36.554]
 [36.554]
 [36.554]
 [36.554]] [[0.668]
 [0.851]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]]
siam score:  -0.8121468
maxi score, test score, baseline:  0.21185999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.07465365167484414, 0.04844212594019116, 0.21311023001751178, 0.052699328036876214, 0.2795348813096326, 0.3315597830209441]
actor:  1 policy actor:  1  step number:  27 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.21185999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.07458453919035228, 0.04844573664580957, 0.2131261483529572, 0.05270325693445917, 0.2795557643589435, 0.33158455451747826]
siam score:  -0.8139351
maxi score, test score, baseline:  0.21185999999999985 0.6779999999999999 0.6779999999999999
printing an ep nov before normalisation:  32.47720618705831
actor:  1 policy actor:  1  step number:  33 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  2.0
from probs:  [0.07458453919035228, 0.04844573664580957, 0.2131261483529572, 0.05270325693445917, 0.2795557643589435, 0.33158455451747826]
Printing some Q and Qe and total Qs values:  [[0.274]
 [0.544]
 [0.466]
 [0.518]
 [0.283]
 [0.288]
 [0.294]] [[27.868]
 [38.408]
 [36.748]
 [36.539]
 [27.536]
 [27.294]
 [26.985]] [[0.64 ]
 [1.171]
 [1.052]
 [1.098]
 [0.641]
 [0.64 ]
 [0.638]]
actor:  1 policy actor:  1  step number:  33 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21185999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.07458453919035228, 0.04844573664580957, 0.2131261483529572, 0.05270325693445917, 0.2795557643589435, 0.33158455451747826]
printing an ep nov before normalisation:  42.64357015757922
actor:  1 policy actor:  1  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  39.004724944089915
maxi score, test score, baseline:  0.21185999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.07458453919035228, 0.04844573664580957, 0.2131261483529572, 0.05270325693445917, 0.2795557643589435, 0.33158455451747826]
printing an ep nov before normalisation:  36.294941868843694
printing an ep nov before normalisation:  40.316156991997744
actor:  0 policy actor:  0  step number:  21 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  29 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.21149999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.07458453919035228, 0.04844573664580957, 0.2131261483529572, 0.05270325693445917, 0.2795557643589435, 0.33158455451747826]
maxi score, test score, baseline:  0.21149999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.074515809146337, 0.048449327371246596, 0.21314197860278644, 0.05270716409111507, 0.27957653184992387, 0.3316091889385911]
Printing some Q and Qe and total Qs values:  [[0.719]
 [0.764]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]] [[34.713]
 [40.215]
 [34.713]
 [34.713]
 [34.713]
 [34.713]
 [34.713]] [[0.719]
 [0.764]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]]
printing an ep nov before normalisation:  51.95684634961925
maxi score, test score, baseline:  0.21149999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.074515809146337, 0.048449327371246596, 0.21314197860278644, 0.05270716409111507, 0.27957653184992387, 0.3316091889385911]
actor:  1 policy actor:  1  step number:  25 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.333
UNIT TEST: sample policy line 217 mcts : [0.082 0.469 0.061 0.082 0.184 0.061 0.061]
maxi score, test score, baseline:  0.21149999999999985 0.6779999999999999 0.6779999999999999
actor:  0 policy actor:  1  step number:  19 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.311]
 [0.343]
 [0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]] [[40.943]
 [41.159]
 [40.943]
 [40.943]
 [40.943]
 [40.943]
 [40.943]] [[0.566]
 [0.6  ]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]]
printing an ep nov before normalisation:  54.965202046340885
printing an ep nov before normalisation:  51.67090878593809
maxi score, test score, baseline:  0.21197999999999984 0.6779999999999999 0.6779999999999999
probs:  [0.07444810207053809, 0.04845331681616188, 0.21315956666703617, 0.052702850249996984, 0.2795996053939051, 0.3316365588023618]
maxi score, test score, baseline:  0.21197999999999984 0.6779999999999999 0.6779999999999999
probs:  [0.07444810207053809, 0.04845331681616188, 0.21315956666703617, 0.052702850249996984, 0.2795996053939051, 0.3316365588023618]
maxi score, test score, baseline:  0.21197999999999984 0.6779999999999999 0.6779999999999999
probs:  [0.07438012690386206, 0.04845686813709594, 0.21317522319622395, 0.05270671389354952, 0.2796201449831218, 0.3316609228861468]
maxi score, test score, baseline:  0.21197999999999984 0.6779999999999999 0.6779999999999999
probs:  [0.07431252478765538, 0.048460399968236804, 0.2131907938017451, 0.05271055633326944, 0.2796405718502407, 0.33168515325885267]
actor:  1 policy actor:  1  step number:  24 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
from probs:  [0.07431252478765538, 0.048460399968236804, 0.2131907938017451, 0.05271055633326944, 0.2796405718502407, 0.33168515325885267]
Printing some Q and Qe and total Qs values:  [[0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]] [[49.14]
 [49.14]
 [49.14]
 [49.14]
 [49.14]
 [49.14]
 [49.14]] [[0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]]
printing an ep nov before normalisation:  52.79167448609651
maxi score, test score, baseline:  0.21197999999999984 0.6779999999999999 0.6779999999999999
probs:  [0.07431252478765538, 0.048460399968236804, 0.2131907938017451, 0.05271055633326944, 0.2796405718502407, 0.33168515325885267]
printing an ep nov before normalisation:  28.093154430389404
actor:  0 policy actor:  1  step number:  19 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  25 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  29 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.21241999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.07431252478765538, 0.048460399968236804, 0.2131907938017451, 0.05271055633326944, 0.2796405718502407, 0.33168515325885267]
printing an ep nov before normalisation:  51.813532780917456
printing an ep nov before normalisation:  47.058757536244364
maxi score, test score, baseline:  0.21241999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.07431252478765538, 0.048460399968236804, 0.2131907938017451, 0.05271055633326944, 0.2796405718502407, 0.33168515325885267]
Printing some Q and Qe and total Qs values:  [[0.541]
 [0.529]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]] [[46.222]
 [45.748]
 [40.722]
 [40.722]
 [40.722]
 [40.722]
 [40.722]] [[1.237]
 [1.214]
 [1.075]
 [1.075]
 [1.075]
 [1.075]
 [1.075]]
actions average: 
K:  4  action  0 :  tensor([0.6876, 0.0316, 0.0741, 0.0628, 0.0622, 0.0390, 0.0427],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0108,     0.9382,     0.0168,     0.0098,     0.0007,     0.0008,
            0.0228], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0745, 0.1011, 0.6874, 0.0237, 0.0363, 0.0402, 0.0368],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0791, 0.1727, 0.0832, 0.3761, 0.0638, 0.1210, 0.1040],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.3118, 0.0125, 0.1049, 0.0897, 0.3194, 0.0853, 0.0764],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1301, 0.0057, 0.1150, 0.0964, 0.1070, 0.4310, 0.1149],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1391, 0.1233, 0.1409, 0.1377, 0.0763, 0.1039, 0.2789],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.21241999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.07431252478765538, 0.048460399968236804, 0.2131907938017451, 0.05271055633326944, 0.2796405718502407, 0.33168515325885267]
printing an ep nov before normalisation:  0.01720624556114103
actor:  1 policy actor:  1  step number:  32 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  33 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  0.667
siam score:  -0.80565065
actor:  0 policy actor:  0  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  36.0037917980386
printing an ep nov before normalisation:  25.08437461709637
maxi score, test score, baseline:  0.21575999999999984 0.6779999999999999 0.6779999999999999
probs:  [0.07431252478765538, 0.048460399968236804, 0.2131907938017451, 0.05271055633326944, 0.2796405718502407, 0.33168515325885267]
actor:  0 policy actor:  1  step number:  31 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  37 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  36 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.704]
 [0.772]
 [0.705]
 [0.704]
 [0.695]
 [0.703]
 [0.702]] [[35.758]
 [33.123]
 [37.12 ]
 [37.473]
 [36.366]
 [37.371]
 [37.59 ]] [[2.202]
 [2.159]
 [2.26 ]
 [2.274]
 [2.218]
 [2.268]
 [2.276]]
maxi score, test score, baseline:  0.21603999999999984 0.6779999999999999 0.6779999999999999
probs:  [0.0743131648050122, 0.04846081693402763, 0.2131926320577619, 0.05270238888810855, 0.2796429834334852, 0.3316880138816045]
actor:  1 policy actor:  1  step number:  43 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21319999999999983 0.6779999999999999 0.6779999999999999
probs:  [0.0743131648050122, 0.04846081693402763, 0.2131926320577619, 0.05270238888810855, 0.2796429834334852, 0.3316880138816045]
actor:  1 policy actor:  1  step number:  37 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.664]
 [0.735]
 [0.637]
 [0.637]
 [0.637]
 [0.637]
 [0.637]] [[41.354]
 [42.234]
 [37.974]
 [37.974]
 [37.974]
 [37.974]
 [37.974]] [[1.705]
 [1.815]
 [1.527]
 [1.527]
 [1.527]
 [1.527]
 [1.527]]
Printing some Q and Qe and total Qs values:  [[0.761]
 [0.659]
 [0.659]
 [0.659]
 [0.743]
 [0.659]
 [0.659]] [[45.784]
 [52.029]
 [52.029]
 [52.029]
 [46.178]
 [52.029]
 [52.029]] [[1.242]
 [1.241]
 [1.241]
 [1.241]
 [1.23 ]
 [1.241]
 [1.241]]
maxi score, test score, baseline:  0.21319999999999983 0.6779999999999999 0.6779999999999999
probs:  [0.0743131648050122, 0.04846081693402763, 0.2131926320577619, 0.05270238888810855, 0.2796429834334852, 0.3316880138816045]
actor:  1 policy actor:  1  step number:  26 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  48.28307934620337
actor:  1 policy actor:  1  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  44.70275938501442
printing an ep nov before normalisation:  48.46810684425954
printing an ep nov before normalisation:  40.65097032197968
maxi score, test score, baseline:  0.21319999999999983 0.6779999999999999 0.6779999999999999
probs:  [0.0743131648050122, 0.04846081693402763, 0.2131926320577619, 0.05270238888810855, 0.2796429834334852, 0.3316880138816045]
actor:  1 policy actor:  1  step number:  33 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  30 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.896]
 [0.896]
 [0.965]
 [0.901]
 [0.918]
 [0.926]
 [0.973]] [[31.778]
 [31.778]
 [30.846]
 [32.642]
 [39.631]
 [32.939]
 [26.175]] [[0.896]
 [0.896]
 [0.965]
 [0.901]
 [0.918]
 [0.926]
 [0.973]]
actor:  0 policy actor:  1  step number:  22 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  48.56314333826031
maxi score, test score, baseline:  0.21329999999999982 0.6779999999999999 0.6779999999999999
probs:  [0.0743131648050122, 0.04846081693402763, 0.2131926320577619, 0.05270238888810855, 0.2796429834334852, 0.3316880138816045]
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.113886157871033
actor:  1 policy actor:  1  step number:  25 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  32 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.494]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.441]
 [0.494]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]]
siam score:  -0.80915725
actor:  1 policy actor:  1  step number:  43 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21329999999999982 0.6779999999999999 0.6779999999999999
probs:  [0.0743131648050122, 0.04846081693402763, 0.2131926320577619, 0.05270238888810855, 0.2796429834334852, 0.3316880138816045]
printing an ep nov before normalisation:  44.821608072948166
printing an ep nov before normalisation:  27.49910593032837
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.21329999999999982 0.6779999999999999 0.6779999999999999
probs:  [0.0743131648050122, 0.04846081693402763, 0.2131926320577619, 0.05270238888810855, 0.2796429834334852, 0.3316880138816045]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
using another actor
from probs:  [0.0743131648050122, 0.04846081693402763, 0.2131926320577619, 0.05270238888810855, 0.2796429834334852, 0.3316880138816045]
actor:  1 policy actor:  1  step number:  35 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21329999999999982 0.6779999999999999 0.6779999999999999
probs:  [0.0743131648050122, 0.04846081693402763, 0.2131926320577619, 0.05270238888810855, 0.2796429834334852, 0.3316880138816045]
maxi score, test score, baseline:  0.21329999999999982 0.6779999999999999 0.6779999999999999
maxi score, test score, baseline:  0.21329999999999982 0.6779999999999999 0.6779999999999999
probs:  [0.0743131648050122, 0.04846081693402763, 0.2131926320577619, 0.05270238888810855, 0.2796429834334852, 0.3316880138816045]
printing an ep nov before normalisation:  36.967413370771474
actor:  1 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  31 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.644]
 [0.763]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]] [[30.687]
 [36.637]
 [30.687]
 [30.687]
 [30.687]
 [30.687]
 [30.687]] [[0.644]
 [0.763]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]]
actor:  1 policy actor:  1  step number:  27 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.691]
 [0.832]
 [0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.691]] [[38.3  ]
 [35.554]
 [38.3  ]
 [38.3  ]
 [38.3  ]
 [38.3  ]
 [38.3  ]] [[0.691]
 [0.832]
 [0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.691]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.21293999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.07431380224218294, 0.0484612322188494, 0.21319446290297503, 0.05269425436944407, 0.27964538529459704, 0.3316908629719515]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.21293999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.07431380224218294, 0.0484612322188494, 0.21319446290297503, 0.05269425436944407, 0.27964538529459704, 0.3316908629719515]
maxi score, test score, baseline:  0.21037999999999984 0.6779999999999999 0.6779999999999999
probs:  [0.07431380224218294, 0.0484612322188494, 0.21319446290297503, 0.05269425436944407, 0.27964538529459704, 0.3316908629719515]
maxi score, test score, baseline:  0.21037999999999984 0.6779999999999999 0.6779999999999999
probs:  [0.07431380224218294, 0.0484612322188494, 0.21319446290297503, 0.05269425436944407, 0.27964538529459704, 0.3316908629719515]
from probs:  [0.07431380224218294, 0.0484612322188494, 0.21319446290297503, 0.05269425436944407, 0.27964538529459704, 0.3316908629719515]
Printing some Q and Qe and total Qs values:  [[0.655]
 [0.725]
 [0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.655]] [[30.642]
 [33.86 ]
 [30.642]
 [30.642]
 [30.642]
 [30.642]
 [30.642]] [[1.691]
 [1.966]
 [1.691]
 [1.691]
 [1.691]
 [1.691]
 [1.691]]
maxi score, test score, baseline:  0.21037999999999984 0.6779999999999999 0.6779999999999999
probs:  [0.07431380224218294, 0.0484612322188494, 0.21319446290297503, 0.05269425436944407, 0.27964538529459704, 0.3316908629719515]
actions average: 
K:  2  action  0 :  tensor([0.8301, 0.0045, 0.0325, 0.0362, 0.0311, 0.0295, 0.0361],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0041,     0.9779,     0.0058,     0.0038,     0.0005,     0.0036,
            0.0042], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0413, 0.0034, 0.7474, 0.0481, 0.0316, 0.0899, 0.0384],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0634, 0.0407, 0.0609, 0.5582, 0.0639, 0.1226, 0.0903],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2431, 0.0006, 0.0479, 0.0627, 0.5420, 0.0572, 0.0463],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1630, 0.0004, 0.1595, 0.1579, 0.1814, 0.1728, 0.1650],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1112, 0.0334, 0.0930, 0.0975, 0.0791, 0.0448, 0.5409],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.21037999999999984 0.6779999999999999 0.6779999999999999
probs:  [0.07431380224218294, 0.0484612322188494, 0.21319446290297503, 0.05269425436944407, 0.27964538529459704, 0.3316908629719515]
actor:  1 policy actor:  1  step number:  41 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  38 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.21319999999999983 0.6779999999999999 0.6779999999999999
probs:  [0.07431443711473887, 0.04846164583284673, 0.2131962863821094, 0.05268615257856459, 0.27964777749224906, 0.3316937005994914]
printing an ep nov before normalisation:  27.46279239654541
printing an ep nov before normalisation:  48.52235422586595
actor:  0 policy actor:  1  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.0
using another actor
maxi score, test score, baseline:  0.21297999999999984 0.6779999999999999 0.6779999999999999
probs:  [0.07431443711473887, 0.04846164583284673, 0.2131962863821094, 0.05268615257856459, 0.27964777749224906, 0.3316937005994914]
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.716]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]] [[48.031]
 [44.055]
 [52.788]
 [52.788]
 [52.788]
 [52.788]
 [52.788]] [[0.675]
 [0.716]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]]
maxi score, test score, baseline:  0.21297999999999984 0.6779999999999999 0.6779999999999999
probs:  [0.07431443711473887, 0.04846164583284673, 0.2131962863821094, 0.05268615257856459, 0.27964777749224906, 0.3316937005994914]
actor:  1 policy actor:  1  step number:  27 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  25.9691795605203
actor:  1 policy actor:  1  step number:  34 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  30.190206246817223
actor:  0 policy actor:  1  step number:  36 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  42.503965782631255
maxi score, test score, baseline:  0.21285999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.07431506943812652, 0.04846205778608282, 0.2131981025395291, 0.05267808331835537, 0.2796501600846427, 0.3316965268332635]
Printing some Q and Qe and total Qs values:  [[0.361]
 [0.361]
 [0.361]
 [0.361]
 [0.361]
 [0.361]
 [0.361]] [[36.084]
 [36.084]
 [36.084]
 [36.084]
 [36.084]
 [36.084]
 [36.084]] [[2.028]
 [2.028]
 [2.028]
 [2.028]
 [2.028]
 [2.028]
 [2.028]]
printing an ep nov before normalisation:  49.9023586350548
actor:  1 policy actor:  1  step number:  31 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  50.48200551823174
maxi score, test score, baseline:  0.21285999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.07424783518827287, 0.04846557052843154, 0.21321358898925663, 0.0526819024991405, 0.2796704765489125, 0.33172062624598586]
actor:  0 policy actor:  0  step number:  25 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  50.17091956977251
actor:  1 policy actor:  1  step number:  24 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.21293999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.07424783518827287, 0.04846557052843154, 0.21321358898925663, 0.0526819024991405, 0.2796704765489125, 0.33172062624598586]
maxi score, test score, baseline:  0.21293999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.07424783518827287, 0.04846557052843154, 0.21321358898925663, 0.0526819024991405, 0.2796704765489125, 0.33172062624598586]
actor:  0 policy actor:  0  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21289999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07424783518827287, 0.04846557052843154, 0.21321358898925663, 0.0526819024991405, 0.2796704765489125, 0.33172062624598586]
actor:  1 policy actor:  1  step number:  42 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.21289999999999987 0.6779999999999999 0.6779999999999999
maxi score, test score, baseline:  0.21289999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07424783518827287, 0.04846557052843154, 0.21321358898925663, 0.0526819024991405, 0.2796704765489125, 0.33172062624598586]
maxi score, test score, baseline:  0.21289999999999987 0.6779999999999999 0.6779999999999999
actor:  0 policy actor:  1  step number:  23 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  31.76005069836665
printing an ep nov before normalisation:  33.98513606783165
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.488]
 [0.522]
 [0.488]
 [0.488]
 [0.488]
 [0.488]] [[34.836]
 [34.836]
 [40.107]
 [34.836]
 [34.836]
 [34.836]
 [34.836]] [[1.618]
 [1.618]
 [1.964]
 [1.618]
 [1.618]
 [1.618]
 [1.618]]
printing an ep nov before normalisation:  15.398792025986122
actor:  0 policy actor:  1  step number:  69 total reward:  0.11999999999999944  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.21833999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07424783518827287, 0.04846557052843154, 0.21321358898925663, 0.0526819024991405, 0.2796704765489125, 0.33172062624598586]
actor:  0 policy actor:  1  step number:  41 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.22109999999999988 0.6779999999999999 0.6779999999999999
probs:  [0.07424783518827287, 0.04846557052843154, 0.21321358898925663, 0.0526819024991405, 0.2796704765489125, 0.33172062624598586]
maxi score, test score, baseline:  0.22109999999999988 0.6779999999999999 0.6779999999999999
probs:  [0.07424783518827287, 0.04846557052843154, 0.21321358898925663, 0.0526819024991405, 0.2796704765489125, 0.33172062624598586]
maxi score, test score, baseline:  0.22109999999999988 0.6779999999999999 0.6779999999999999
probs:  [0.07424783518827287, 0.04846557052843154, 0.21321358898925663, 0.0526819024991405, 0.2796704765489125, 0.33172062624598586]
actor:  1 policy actor:  1  step number:  34 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  2.0
siam score:  -0.80655265
maxi score, test score, baseline:  0.22109999999999988 0.6779999999999999 0.6779999999999999
probs:  [0.07424783518827287, 0.04846557052843154, 0.21321358898925663, 0.0526819024991405, 0.2796704765489125, 0.33172062624598586]
printing an ep nov before normalisation:  63.03439123608292
actor:  1 policy actor:  1  step number:  29 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  26 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
from probs:  [0.07424783518827287, 0.04846557052843154, 0.21321358898925663, 0.0526819024991405, 0.2796704765489125, 0.33172062624598586]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.22127999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07424783518827287, 0.04846557052843154, 0.21321358898925663, 0.0526819024991405, 0.2796704765489125, 0.33172062624598586]
siam score:  -0.80315644
maxi score, test score, baseline:  0.22127999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07424783518827287, 0.04846557052843154, 0.21321358898925663, 0.0526819024991405, 0.2796704765489125, 0.33172062624598586]
actor:  1 policy actor:  1  step number:  22 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  51.972190648992154
printing an ep nov before normalisation:  48.143193288627344
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.22127999999999987 0.6779999999999999 0.6779999999999999
printing an ep nov before normalisation:  54.012009673000435
printing an ep nov before normalisation:  26.30318627199262
actor:  1 policy actor:  1  step number:  41 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.22127999999999987 0.6779999999999999 0.6779999999999999
maxi score, test score, baseline:  0.22127999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07424783518827287, 0.04846557052843154, 0.21321358898925663, 0.0526819024991405, 0.2796704765489125, 0.33172062624598586]
actor:  1 policy actor:  1  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  39.376998937741334
maxi score, test score, baseline:  0.22127999999999987 0.6779999999999999 0.6779999999999999
actor:  1 policy actor:  1  step number:  20 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.21833999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07424783518827287, 0.04846557052843154, 0.21321358898925663, 0.0526819024991405, 0.2796704765489125, 0.33172062624598586]
maxi score, test score, baseline:  0.21833999999999987 0.6779999999999999 0.6779999999999999
actor:  0 policy actor:  1  step number:  40 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]]
maxi score, test score, baseline:  0.21797999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07424783518827287, 0.04846557052843154, 0.21321358898925663, 0.0526819024991405, 0.2796704765489125, 0.33172062624598586]
printing an ep nov before normalisation:  49.93552155077357
maxi score, test score, baseline:  0.21797999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07424783518827287, 0.04846557052843154, 0.21321358898925663, 0.0526819024991405, 0.2796704765489125, 0.33172062624598586]
printing an ep nov before normalisation:  56.30594616764418
actor:  0 policy actor:  0  step number:  27 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.21787999999999988 0.6779999999999999 0.6779999999999999
probs:  [0.07424783518827287, 0.04846557052843154, 0.21321358898925663, 0.0526819024991405, 0.2796704765489125, 0.33172062624598586]
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.520043403505866
maxi score, test score, baseline:  0.21787999999999988 0.6779999999999999 0.6779999999999999
maxi score, test score, baseline:  0.21787999999999988 0.6779999999999999 0.6779999999999999
probs:  [0.07424783518827287, 0.04846557052843154, 0.21321358898925663, 0.0526819024991405, 0.2796704765489125, 0.33172062624598586]
printing an ep nov before normalisation:  45.79665907847135
maxi score, test score, baseline:  0.21787999999999988 0.6779999999999999 0.6779999999999999
probs:  [0.07424783518827287, 0.04846557052843154, 0.21321358898925663, 0.0526819024991405, 0.2796704765489125, 0.33172062624598586]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21787999999999988 0.6779999999999999 0.6779999999999999
probs:  [0.07424783518827287, 0.04846557052843154, 0.21321358898925663, 0.0526819024991405, 0.2796704765489125, 0.33172062624598586]
actor:  1 policy actor:  1  step number:  36 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  45.9509203819772
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.8071643
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  31 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  18 total reward:  0.69  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2212799999999999 0.6779999999999999 0.6779999999999999
probs:  [0.07424783518827287, 0.04846557052843154, 0.21321358898925663, 0.0526819024991405, 0.2796704765489125, 0.33172062624598586]
Printing some Q and Qe and total Qs values:  [[0.189]
 [0.269]
 [0.153]
 [0.178]
 [0.242]
 [0.196]
 [0.233]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.189]
 [0.269]
 [0.153]
 [0.178]
 [0.242]
 [0.196]
 [0.233]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.0781],
        [0.8784],
        [0.0000],
        [0.0000],
        [0.8525],
        [0.4363],
        [0.0000],
        [0.4024],
        [0.0000]], dtype=torch.float64)
-0.6761732835329999 -0.6761732835329999
-0.048519850599000006 0.02954562660859679
-0.08713775079899999 0.7912894751800578
-0.5673377555999999 -0.5673377555999999
-0.9702 -0.9702
-0.106157551797 0.74635268362051
-0.145559551797 0.29070586216438743
-0.96089103 -0.96089103
-0.125759551797 0.276618127247751
-0.9602999999999999 -0.9602999999999999
maxi score, test score, baseline:  0.2212799999999999 0.6779999999999999 0.6779999999999999
probs:  [0.07424783518827287, 0.04846557052843154, 0.21321358898925663, 0.0526819024991405, 0.2796704765489125, 0.33172062624598586]
maxi score, test score, baseline:  0.2212799999999999 0.6779999999999999 0.6779999999999999
probs:  [0.07424783518827287, 0.04846557052843154, 0.21321358898925663, 0.0526819024991405, 0.2796704765489125, 0.33172062624598586]
actor:  1 policy actor:  1  step number:  20 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.667
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.2212799999999999 0.6779999999999999 0.6779999999999999
probs:  [0.07424783518827287, 0.04846557052843154, 0.21321358898925663, 0.0526819024991405, 0.2796704765489125, 0.33172062624598586]
maxi score, test score, baseline:  0.2212799999999999 0.6779999999999999 0.6779999999999999
probs:  [0.07424783518827287, 0.04846557052843154, 0.21321358898925663, 0.0526819024991405, 0.2796704765489125, 0.33172062624598586]
printing an ep nov before normalisation:  44.04955063543204
actor:  0 policy actor:  0  step number:  23 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.22435999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07424783518827287, 0.04846557052843154, 0.21321358898925663, 0.0526819024991405, 0.2796704765489125, 0.33172062624598586]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  25 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  0.024274903005334636
actor:  1 policy actor:  1  step number:  32 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  19 total reward:  0.7  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.22435999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07449792286948859, 0.04942080499005625, 0.20966295337588448, 0.053513568440923365, 0.274302240312904, 0.3386025100107434]
maxi score, test score, baseline:  0.22435999999999987 0.6779999999999999 0.6779999999999999
maxi score, test score, baseline:  0.22435999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07449792286948859, 0.04942080499005625, 0.20966295337588448, 0.053513568440923365, 0.274302240312904, 0.3386025100107434]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  41.95915209456056
actor:  1 policy actor:  1  step number:  35 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  31.18544164591289
actor:  1 policy actor:  1  step number:  37 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  46 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  45.613768691729625
printing an ep nov before normalisation:  29.834890365600586
actor:  1 policy actor:  1  step number:  27 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.22435999999999987 0.6779999999999999 0.6779999999999999
maxi score, test score, baseline:  0.22435999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07443351328653564, 0.04942467561084351, 0.20967940803128457, 0.05350954142610438, 0.2743237711797277, 0.3386290904655042]
printing an ep nov before normalisation:  46.688692383855404
actor:  1 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  31 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  42 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.535]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]] [[44.982]
 [44.47 ]
 [44.982]
 [44.982]
 [44.982]
 [44.982]
 [44.982]] [[1.876]
 [1.932]
 [1.876]
 [1.876]
 [1.876]
 [1.876]
 [1.876]]
maxi score, test score, baseline:  0.22435999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07443351328653564, 0.04942467561084351, 0.20967940803128457, 0.05350954142610438, 0.2743237711797277, 0.3386290904655042]
actor:  1 policy actor:  1  step number:  22 total reward:  0.71  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  23 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.22435999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07234269616252306, 0.04803760955567291, 0.20378275857642347, 0.052007526858698125, 0.29472562415412573, 0.3291037846925566]
actor:  1 policy actor:  1  step number:  35 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  2.0
from probs:  [0.07234269616252306, 0.04803760955567291, 0.20378275857642347, 0.052007526858698125, 0.29472562415412573, 0.3291037846925566]
printing an ep nov before normalisation:  40.419805745769274
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.22123999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.07234269616252306, 0.04803760955567291, 0.20378275857642347, 0.052007526858698125, 0.29472562415412573, 0.3291037846925566]
actor:  1 policy actor:  1  step number:  25 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  0
maxi score, test score, baseline:  0.22123999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.07234269616252306, 0.04803760955567291, 0.20378275857642347, 0.052007526858698125, 0.29472562415412573, 0.3291037846925566]
maxi score, test score, baseline:  0.22123999999999985 0.6779999999999999 0.6779999999999999
actor:  1 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  40 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  32 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.22113999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.07331607130413467, 0.048683354992885224, 0.2065279302761584, 0.05270678631351151, 0.28522758805138704, 0.33353826906192324]
printing an ep nov before normalisation:  41.09125043628968
printing an ep nov before normalisation:  52.434837096262214
printing an ep nov before normalisation:  49.04744471232599
actor:  1 policy actor:  1  step number:  24 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  50.55924180580096
maxi score, test score, baseline:  0.22113999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.07331607130413467, 0.048683354992885224, 0.2065279302761584, 0.05270678631351151, 0.28522758805138704, 0.33353826906192324]
siam score:  -0.80023277
printing an ep nov before normalisation:  50.901189992577535
Printing some Q and Qe and total Qs values:  [[0.889]
 [0.895]
 [0.889]
 [0.889]
 [0.889]
 [0.889]
 [0.889]] [[21.852]
 [31.427]
 [21.852]
 [21.852]
 [21.852]
 [21.852]
 [21.852]] [[0.889]
 [0.895]
 [0.889]
 [0.889]
 [0.889]
 [0.889]
 [0.889]]
Printing some Q and Qe and total Qs values:  [[0.725]
 [0.713]
 [0.725]
 [0.725]
 [0.725]
 [0.725]
 [0.725]] [[41.14 ]
 [43.167]
 [41.14 ]
 [41.14 ]
 [41.14 ]
 [41.14 ]
 [41.14 ]] [[2.35]
 [2.46]
 [2.35]
 [2.35]
 [2.35]
 [2.35]
 [2.35]]
maxi score, test score, baseline:  0.22113999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.07331607130413467, 0.048683354992885224, 0.2065279302761584, 0.05270678631351151, 0.28522758805138704, 0.33353826906192324]
siam score:  -0.7996922
actor:  0 policy actor:  1  step number:  37 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.22067999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.07331607130413467, 0.048683354992885224, 0.2065279302761584, 0.05270678631351151, 0.28522758805138704, 0.33353826906192324]
printing an ep nov before normalisation:  26.80777072906494
actor:  1 policy actor:  1  step number:  41 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  32 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.22063999999999984 0.6779999999999999 0.6779999999999999
actor:  1 policy actor:  1  step number:  28 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.22063999999999984 0.6779999999999999 0.6779999999999999
probs:  [0.07331607130413467, 0.048683354992885224, 0.2065279302761584, 0.05270678631351151, 0.28522758805138704, 0.33353826906192324]
actor:  0 policy actor:  1  step number:  33 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  31 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.22055999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07331607130413467, 0.048683354992885224, 0.2065279302761584, 0.05270678631351151, 0.28522758805138704, 0.33353826906192324]
actions average: 
K:  3  action  0 :  tensor([0.7086, 0.0012, 0.0385, 0.0492, 0.1029, 0.0453, 0.0542],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0012, 0.9816, 0.0029, 0.0027, 0.0015, 0.0016, 0.0085],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0697, 0.0197, 0.5632, 0.0735, 0.0745, 0.1343, 0.0651],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0697, 0.0061, 0.0636, 0.5578, 0.1163, 0.0941, 0.0923],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1229, 0.0046, 0.0990, 0.1822, 0.3880, 0.0966, 0.1068],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0702, 0.0188, 0.1876, 0.1673, 0.0974, 0.3989, 0.0597],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2172, 0.0589, 0.0364, 0.0827, 0.0560, 0.0381, 0.5107],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.22055999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07331607130413467, 0.048683354992885224, 0.2065279302761584, 0.05270678631351151, 0.28522758805138704, 0.33353826906192324]
actor:  0 policy actor:  0  step number:  25 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  47.869258294288485
printing an ep nov before normalisation:  47.62976991849128
UNIT TEST: sample policy line 217 mcts : [0.245 0.245 0.082 0.184 0.02  0.204 0.02 ]
maxi score, test score, baseline:  0.21997999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07331607130413467, 0.048683354992885224, 0.2065279302761584, 0.05270678631351151, 0.28522758805138704, 0.33353826906192324]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
from probs:  [0.07331607130413467, 0.048683354992885224, 0.2065279302761584, 0.05270678631351151, 0.28522758805138704, 0.33353826906192324]
Printing some Q and Qe and total Qs values:  [[0.255]
 [0.308]
 [0.255]
 [0.255]
 [0.255]
 [0.255]
 [0.255]] [[27.504]
 [33.044]
 [27.504]
 [27.504]
 [27.504]
 [27.504]
 [27.504]] [[1.079]
 [1.449]
 [1.079]
 [1.079]
 [1.079]
 [1.079]
 [1.079]]
using another actor
from probs:  [0.07331607130413467, 0.048683354992885224, 0.2065279302761584, 0.05270678631351151, 0.28522758805138704, 0.33353826906192324]
Printing some Q and Qe and total Qs values:  [[0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.176]] [[26.609]
 [26.609]
 [26.609]
 [26.609]
 [26.609]
 [26.609]
 [26.609]] [[0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.176]]
maxi score, test score, baseline:  0.21997999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07331607130413467, 0.048683354992885224, 0.2065279302761584, 0.05270678631351151, 0.28522758805138704, 0.33353826906192324]
actor:  0 policy actor:  1  step number:  24 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  1  step number:  31 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
from probs:  [0.07331607130413467, 0.048683354992885224, 0.2065279302761584, 0.05270678631351151, 0.28522758805138704, 0.33353826906192324]
maxi score, test score, baseline:  0.21999999999999986 0.6779999999999999 0.6779999999999999
maxi score, test score, baseline:  0.21999999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.07331607130413467, 0.048683354992885224, 0.2065279302761584, 0.05270678631351151, 0.28522758805138704, 0.33353826906192324]
printing an ep nov before normalisation:  54.05601513087176
maxi score, test score, baseline:  0.21999999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.07331607130413467, 0.048683354992885224, 0.2065279302761584, 0.05270678631351151, 0.28522758805138704, 0.33353826906192324]
printing an ep nov before normalisation:  50.88507356525508
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  21 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.21711999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07331607130413467, 0.048683354992885224, 0.2065279302761584, 0.05270678631351151, 0.28522758805138704, 0.33353826906192324]
maxi score, test score, baseline:  0.21711999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07331607130413467, 0.048683354992885224, 0.2065279302761584, 0.05270678631351151, 0.28522758805138704, 0.33353826906192324]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.21711999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07325229992416876, 0.04868669845852095, 0.20654214390696904, 0.05271040685780494, 0.28524722143309944, 0.3335612294194368]
printing an ep nov before normalisation:  51.525180499230615
maxi score, test score, baseline:  0.21711999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07325229992416876, 0.04868669845852095, 0.20654214390696904, 0.05271040685780494, 0.28524722143309944, 0.3335612294194368]
printing an ep nov before normalisation:  46.81473762793857
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.587]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]] [[43.519]
 [42.421]
 [40.328]
 [40.328]
 [40.328]
 [40.328]
 [40.328]] [[1.834]
 [1.757]
 [1.503]
 [1.503]
 [1.503]
 [1.503]
 [1.503]]
actor:  1 policy actor:  1  step number:  33 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21711999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07325229992416876, 0.04868669845852095, 0.20654214390696904, 0.05271040685780494, 0.28524722143309944, 0.3335612294194368]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  28.65109920501709
printing an ep nov before normalisation:  29.495151788703083
actor:  1 policy actor:  1  step number:  41 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.21711999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07325229992416876, 0.04868669845852095, 0.20654214390696904, 0.05271040685780494, 0.28524722143309944, 0.3335612294194368]
maxi score, test score, baseline:  0.21711999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07325229992416876, 0.04868669845852095, 0.20654214390696904, 0.05271040685780494, 0.28524722143309944, 0.3335612294194368]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.21711999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07325229992416876, 0.04868669845852095, 0.20654214390696904, 0.05271040685780494, 0.28524722143309944, 0.3335612294194368]
from probs:  [0.07325229992416876, 0.04868669845852095, 0.20654214390696904, 0.05271040685780494, 0.28524722143309944, 0.3335612294194368]
maxi score, test score, baseline:  0.21431999999999984 0.6779999999999999 0.6779999999999999
probs:  [0.07325229992416876, 0.04868669845852095, 0.20654214390696904, 0.05271040685780494, 0.28524722143309944, 0.3335612294194368]
actor:  1 policy actor:  1  step number:  39 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  47.712291969284784
actor:  0 policy actor:  0  step number:  31 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  39.794956449298226
actor:  1 policy actor:  1  step number:  31 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.21409999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07325288999946533, 0.04868709028794338, 0.20654380963928334, 0.052702767549835466, 0.2852495223200997, 0.3335639202033728]
actor:  1 policy actor:  1  step number:  51 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.252]
 [0.309]
 [0.252]
 [0.252]
 [0.252]
 [0.252]
 [0.246]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.252]
 [0.309]
 [0.252]
 [0.252]
 [0.252]
 [0.252]
 [0.246]]
maxi score, test score, baseline:  0.21409999999999987 0.6779999999999999 0.6779999999999999
maxi score, test score, baseline:  0.21409999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07325288999946533, 0.04868709028794338, 0.20654380963928334, 0.052702767549835466, 0.2852495223200997, 0.3335639202033728]
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.416]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]] [[29.552]
 [40.648]
 [29.552]
 [29.552]
 [29.552]
 [29.552]
 [29.552]] [[1.097]
 [1.657]
 [1.097]
 [1.097]
 [1.097]
 [1.097]
 [1.097]]
maxi score, test score, baseline:  0.21409999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07325288999946533, 0.04868709028794338, 0.20654380963928334, 0.052702767549835466, 0.2852495223200997, 0.3335639202033728]
actor:  1 policy actor:  1  step number:  28 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21409999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07325288999946533, 0.04868709028794338, 0.20654380963928334, 0.052702767549835466, 0.2852495223200997, 0.3335639202033728]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.21409999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07325288999946533, 0.04868709028794338, 0.20654380963928334, 0.052702767549835466, 0.2852495223200997, 0.3335639202033728]
maxi score, test score, baseline:  0.21409999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07325288999946533, 0.04868709028794338, 0.20654380963928334, 0.052702767549835466, 0.2852495223200997, 0.3335639202033728]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.21409999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07325288999946533, 0.04868709028794338, 0.20654380963928334, 0.052702767549835466, 0.2852495223200997, 0.3335639202033728]
maxi score, test score, baseline:  0.21409999999999987 0.6779999999999999 0.6779999999999999
actor:  1 policy actor:  1  step number:  27 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21409999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07325288999946533, 0.04868709028794338, 0.20654380963928334, 0.052702767549835466, 0.2852495223200997, 0.3335639202033728]
printing an ep nov before normalisation:  29.724394046417324
maxi score, test score, baseline:  0.21409999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07325288999946533, 0.04868709028794338, 0.20654380963928334, 0.052702767549835466, 0.2852495223200997, 0.3335639202033728]
actor:  1 policy actor:  1  step number:  31 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.21409999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.0731894647096207, 0.04869041563739646, 0.20655794625517615, 0.052706367924368204, 0.28526904932059927, 0.3335867561528392]
actor:  1 policy actor:  1  step number:  19 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  27.46873126556673
maxi score, test score, baseline:  0.21409999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07319005196449746, 0.04869080595924689, 0.20655960557855, 0.052698758558890606, 0.285271341354888, 0.3335894365839271]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  44.24712638931131
printing an ep nov before normalisation:  37.4515279433767
printing an ep nov before normalisation:  0.002621307875187995
actor:  1 policy actor:  1  step number:  31 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  34 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.21409999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07319005196449746, 0.04869080595924689, 0.20655960557855, 0.052698758558890606, 0.285271341354888, 0.3335894365839271]
actor:  1 policy actor:  1  step number:  32 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.21409999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07306422890611301, 0.04869740284304552, 0.20658765003332558, 0.05270589995083825, 0.2853100793446994, 0.33363473892197826]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  38 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  35 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.21409999999999987 0.6779999999999999 0.6779999999999999
probs:  [0.07306422890611301, 0.04869740284304552, 0.20658765003332558, 0.05270589995083825, 0.2853100793446994, 0.33363473892197826]
Printing some Q and Qe and total Qs values:  [[0.238]
 [0.447]
 [0.238]
 [0.25 ]
 [0.238]
 [0.25 ]
 [0.238]] [[29.641]
 [37.311]
 [29.641]
 [30.299]
 [29.641]
 [30.272]
 [29.641]] [[0.817]
 [1.399]
 [0.817]
 [0.86 ]
 [0.817]
 [0.859]
 [0.817]]
maxi score, test score, baseline:  0.21129999999999988 0.6779999999999999 0.6779999999999999
probs:  [0.07306422890611301, 0.04869740284304552, 0.20658765003332558, 0.05270589995083825, 0.2853100793446994, 0.33363473892197826]
maxi score, test score, baseline:  0.20813999999999983 0.6779999999999999 0.6779999999999999
probs:  [0.072980933344089, 0.04841710040317895, 0.20758389729920373, 0.05245800638925562, 0.28694280097021185, 0.33161726159406074]
actor:  1 policy actor:  1  step number:  31 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.20813999999999983 0.6779999999999999 0.6779999999999999
probs:  [0.072980933344089, 0.04841710040317895, 0.20758389729920373, 0.05245800638925562, 0.28694280097021185, 0.33161726159406074]
from probs:  [0.072980933344089, 0.04841710040317895, 0.20758389729920373, 0.05245800638925562, 0.28694280097021185, 0.33161726159406074]
maxi score, test score, baseline:  0.20813999999999983 0.6779999999999999 0.6779999999999999
probs:  [0.072980933344089, 0.04841710040317895, 0.20758389729920373, 0.05245800638925562, 0.28694280097021185, 0.33161726159406074]
maxi score, test score, baseline:  0.20813999999999983 0.6779999999999999 0.6779999999999999
probs:  [0.072980933344089, 0.04841710040317895, 0.20758389729920373, 0.05245800638925562, 0.28694280097021185, 0.33161726159406074]
maxi score, test score, baseline:  0.20813999999999983 0.6779999999999999 0.6779999999999999
probs:  [0.072980933344089, 0.04841710040317895, 0.20758389729920373, 0.05245800638925562, 0.28694280097021185, 0.33161726159406074]
printing an ep nov before normalisation:  33.03104229261389
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.227403125984246
actor:  0 policy actor:  0  step number:  28 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  0
maxi score, test score, baseline:  0.20789999999999986 0.6779999999999999 0.6779999999999999
maxi score, test score, baseline:  0.20789999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.072980933344089, 0.04841710040317895, 0.20758389729920373, 0.05245800638925562, 0.28694280097021185, 0.33161726159406074]
maxi score, test score, baseline:  0.20789999999999986 0.6779999999999999 0.6779999999999999
printing an ep nov before normalisation:  0.0013016810424915093
actor:  1 policy actor:  1  step number:  33 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  35 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.21077999999999988 0.6779999999999999 0.6779999999999999
probs:  [0.072980933344089, 0.04841710040317895, 0.20758389729920373, 0.05245800638925562, 0.28694280097021185, 0.33161726159406074]
Printing some Q and Qe and total Qs values:  [[0.518]
 [0.528]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]] [[56.302]
 [44.552]
 [56.302]
 [56.302]
 [56.302]
 [56.302]
 [56.302]] [[1.518]
 [1.249]
 [1.518]
 [1.518]
 [1.518]
 [1.518]
 [1.518]]
printing an ep nov before normalisation:  36.89123741096337
actor:  1 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
using another actor
from probs:  [0.072980933344089, 0.04841710040317895, 0.20758389729920373, 0.05245800638925562, 0.28694280097021185, 0.33161726159406074]
maxi score, test score, baseline:  0.21077999999999988 0.6779999999999999 0.6779999999999999
probs:  [0.072980933344089, 0.04841710040317895, 0.20758389729920373, 0.05245800638925562, 0.28694280097021185, 0.33161726159406074]
siam score:  -0.79157454
using explorer policy with actor:  1
maxi score, test score, baseline:  0.21077999999999988 0.6779999999999999 0.6779999999999999
probs:  [0.07398478274155199, 0.04908245748495158, 0.19667349577190968, 0.053179047599315805, 0.29089506840228835, 0.33618514799998256]
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.504]
 [0.581]
 [0.504]
 [0.504]
 [0.504]
 [0.504]] [[37.254]
 [37.254]
 [44.519]
 [37.254]
 [37.254]
 [37.254]
 [37.254]] [[1.416]
 [1.416]
 [1.814]
 [1.416]
 [1.416]
 [1.416]
 [1.416]]
Printing some Q and Qe and total Qs values:  [[ 0.589]
 [ 0.564]
 [-0.109]
 [ 0.618]
 [ 0.59 ]
 [-0.09 ]
 [ 0.58 ]] [[39.129]
 [41.7  ]
 [44.861]
 [41.67 ]
 [41.966]
 [38.459]
 [43.353]] [[1.674]
 [1.768]
 [1.24 ]
 [1.82 ]
 [1.806]
 [0.964]
 [1.86 ]]
maxi score, test score, baseline:  0.21077999999999988 0.6779999999999999 0.6779999999999999
probs:  [0.07398478274155199, 0.04908245748495158, 0.19667349577190968, 0.053179047599315805, 0.29089506840228835, 0.33618514799998256]
actor:  0 policy actor:  1  step number:  19 total reward:  0.7  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.21417999999999987 0.6779999999999999 0.6779999999999999
actor:  1 policy actor:  1  step number:  30 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  0.0042353363625124985
actor:  0 policy actor:  0  step number:  28 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  22 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 42.03186423782347
actor:  0 policy actor:  1  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[ 0.47 ]
 [-0.097]
 [ 0.535]
 [ 0.47 ]
 [-0.056]
 [ 0.47 ]
 [ 0.549]] [[30.59 ]
 [33.089]
 [34.455]
 [30.59 ]
 [30.863]
 [30.59 ]
 [33.933]] [[1.351]
 [0.936]
 [1.651]
 [1.351]
 [0.841]
 [1.351]
 [1.633]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.21355999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.07398478274155199, 0.04908245748495158, 0.19667349577190968, 0.053179047599315805, 0.29089506840228835, 0.33618514799998256]
maxi score, test score, baseline:  0.21355999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.07398478274155199, 0.04908245748495158, 0.19667349577190968, 0.053179047599315805, 0.29089506840228835, 0.33618514799998256]
printing an ep nov before normalisation:  48.56019299960403
maxi score, test score, baseline:  0.21019999999999983 0.6779999999999999 0.6779999999999999
probs:  [0.07398478274155199, 0.04908245748495158, 0.19667349577190968, 0.053179047599315805, 0.29089506840228835, 0.33618514799998256]
printing an ep nov before normalisation:  49.000331616433066
maxi score, test score, baseline:  0.21019999999999983 0.6779999999999999 0.6779999999999999
printing an ep nov before normalisation:  55.28214082240832
printing an ep nov before normalisation:  49.01343042976199
maxi score, test score, baseline:  0.21019999999999983 0.6779999999999999 0.6779999999999999
probs:  [0.07398478274155199, 0.04908245748495158, 0.19667349577190968, 0.053179047599315805, 0.29089506840228835, 0.33618514799998256]
actor:  1 policy actor:  1  step number:  36 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  25 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.21019999999999983 0.6779999999999999 0.6779999999999999
probs:  [0.07398478274155199, 0.04908245748495158, 0.19667349577190968, 0.053179047599315805, 0.29089506840228835, 0.33618514799998256]
Printing some Q and Qe and total Qs values:  [[0.623]
 [0.588]
 [0.588]
 [0.588]
 [0.554]
 [0.588]
 [0.588]] [[46.95 ]
 [45.335]
 [45.335]
 [45.335]
 [48.006]
 [45.335]
 [45.335]] [[1.432]
 [1.355]
 [1.355]
 [1.355]
 [1.391]
 [1.355]
 [1.355]]
maxi score, test score, baseline:  0.21019999999999983 0.6779999999999999 0.6779999999999999
probs:  [0.07398478274155199, 0.04908245748495158, 0.19667349577190968, 0.053179047599315805, 0.29089506840228835, 0.33618514799998256]
maxi score, test score, baseline:  0.21019999999999983 0.6779999999999999 0.6779999999999999
probs:  [0.07398478274155199, 0.04908245748495158, 0.19667349577190968, 0.053179047599315805, 0.29089506840228835, 0.33618514799998256]
line 256 mcts: sample exp_bonus 40.47035879105292
Printing some Q and Qe and total Qs values:  [[0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.867]] [[42.317]
 [42.317]
 [42.317]
 [42.317]
 [42.317]
 [42.317]
 [42.317]] [[1.591]
 [1.591]
 [1.591]
 [1.591]
 [1.591]
 [1.591]
 [1.591]]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  22 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.667804479177626
actor:  1 policy actor:  1  step number:  33 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.21019999999999983 0.6779999999999999 0.6779999999999999
probs:  [0.07398478274155199, 0.04908245748495158, 0.19667349577190968, 0.053179047599315805, 0.29089506840228835, 0.33618514799998256]
maxi score, test score, baseline:  0.21019999999999983 0.6779999999999999 0.6779999999999999
probs:  [0.0739210717847882, 0.0490858276750544, 0.19668702771223912, 0.053182699842624574, 0.2909150875663556, 0.3362082854189381]
printing an ep nov before normalisation:  45.056120457973904
actor:  1 policy actor:  1  step number:  33 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.21019999999999983 0.6779999999999999 0.6779999999999999
probs:  [0.0739210717847882, 0.0490858276750544, 0.19668702771223912, 0.053182699842624574, 0.2909150875663556, 0.3362082854189381]
actor:  1 policy actor:  1  step number:  27 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.21019999999999983 0.6779999999999999 0.6779999999999999
probs:  [0.0739210717847882, 0.0490858276750544, 0.19668702771223912, 0.053182699842624574, 0.2909150875663556, 0.3362082854189381]
actor:  1 policy actor:  1  step number:  36 total reward:  0.2899999999999996  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  2  action  0 :  tensor([0.6965, 0.0381, 0.0350, 0.0408, 0.0965, 0.0419, 0.0512],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0037, 0.9599, 0.0054, 0.0060, 0.0020, 0.0027, 0.0204],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0501, 0.0187, 0.6406, 0.0513, 0.0442, 0.1274, 0.0676],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1171, 0.1993, 0.1021, 0.2005, 0.1093, 0.1117, 0.1599],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2095, 0.0177, 0.1537, 0.1381, 0.1700, 0.1538, 0.1572],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0941, 0.0050, 0.0986, 0.1324, 0.1029, 0.4402, 0.1267],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1397, 0.0923, 0.1218, 0.1823, 0.1322, 0.1399, 0.1918],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.913]
 [0.913]
 [0.913]
 [0.913]
 [0.913]
 [0.913]
 [0.913]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.913]
 [0.913]
 [0.913]
 [0.913]
 [0.913]
 [0.913]
 [0.913]]
maxi score, test score, baseline:  0.21019999999999983 0.6779999999999999 0.6779999999999999
probs:  [0.0738577031512642, 0.04908917975689996, 0.19670048694454645, 0.0531863324621853, 0.2909349991661129, 0.33623129851899125]
Printing some Q and Qe and total Qs values:  [[0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.932]
 [0.934]
 [0.934]] [[33.685]
 [33.685]
 [33.685]
 [33.685]
 [36.887]
 [36.744]
 [33.685]] [[0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.932]
 [0.934]
 [0.934]]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.20997999999999983 0.6779999999999999 0.6779999999999999
probs:  [0.0738577031512642, 0.04908917975689996, 0.19670048694454645, 0.0531863324621853, 0.2909349991661129, 0.33623129851899125]
actor:  1 policy actor:  1  step number:  51 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  44.46972927623604
printing an ep nov before normalisation:  29.79929208755493
actor:  1 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  32 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  28 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20997999999999983 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.731]
 [0.724]
 [0.724]
 [0.697]
 [0.693]
 [0.689]
 [0.724]] [[45.797]
 [40.716]
 [40.716]
 [51.605]
 [51.856]
 [51.288]
 [40.716]] [[1.964]
 [1.719]
 [1.719]
 [2.2  ]
 [2.208]
 [2.178]
 [1.719]]
maxi score, test score, baseline:  0.20997999999999983 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.347]
 [0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.354]] [[33.215]
 [41.619]
 [33.215]
 [33.215]
 [33.215]
 [33.215]
 [33.215]] [[0.698]
 [0.887]
 [0.698]
 [0.698]
 [0.698]
 [0.698]
 [0.698]]
actions average: 
K:  0  action  0 :  tensor([0.6544, 0.0040, 0.0367, 0.0420, 0.1672, 0.0487, 0.0471],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0046,     0.9733,     0.0057,     0.0034,     0.0005,     0.0010,
            0.0116], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0227, 0.0013, 0.8764, 0.0212, 0.0208, 0.0363, 0.0212],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1250, 0.0165, 0.1220, 0.3659, 0.1120, 0.1629, 0.0957],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1402, 0.0025, 0.0659, 0.0665, 0.5953, 0.0823, 0.0473],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1225, 0.0074, 0.1226, 0.0903, 0.1064, 0.4660, 0.0847],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1248, 0.0822, 0.0960, 0.1045, 0.0904, 0.1313, 0.3708],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  13.769738674163818
Printing some Q and Qe and total Qs values:  [[0.339]
 [0.497]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]] [[33.558]
 [40.066]
 [33.558]
 [33.558]
 [33.558]
 [33.558]
 [33.558]] [[0.783]
 [1.107]
 [0.783]
 [0.783]
 [0.783]
 [0.783]
 [0.783]]
actor:  1 policy actor:  1  step number:  34 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.20997999999999983 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.685]
 [0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.628]] [[35.728]
 [43.572]
 [35.728]
 [35.728]
 [35.728]
 [35.728]
 [35.728]] [[1.182]
 [1.455]
 [1.182]
 [1.182]
 [1.182]
 [1.182]
 [1.182]]
line 256 mcts: sample exp_bonus 32.33366987974376
maxi score, test score, baseline:  0.20997999999999983 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
using another actor
from probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
maxi score, test score, baseline:  0.20997999999999983 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
Printing some Q and Qe and total Qs values:  [[ 0.521]
 [ 0.583]
 [-0.103]
 [ 0.36 ]
 [ 0.183]
 [-0.102]
 [-0.084]] [[32.934]
 [33.551]
 [28.702]
 [28.184]
 [30.495]
 [29.061]
 [30.423]] [[1.934]
 [2.049]
 [0.948]
 [1.367]
 [1.387]
 [0.98 ]
 [1.114]]
actor:  1 policy actor:  1  step number:  36 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.702]
 [0.768]
 [0.677]
 [0.677]
 [0.676]
 [0.676]
 [0.675]] [[36.902]
 [32.976]
 [36.169]
 [36.508]
 [36.305]
 [35.997]
 [35.82 ]] [[2.076]
 [1.857]
 [1.997]
 [2.021]
 [2.006]
 [1.984]
 [1.97 ]]
maxi score, test score, baseline:  0.20997999999999983 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
actor:  1 policy actor:  1  step number:  31 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20997999999999983 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  48.47237506280864
actor:  0 policy actor:  1  step number:  26 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.0
siam score:  -0.80029386
maxi score, test score, baseline:  0.20611999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
actor:  1 policy actor:  1  step number:  22 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  27 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  25 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.20611999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
printing an ep nov before normalisation:  23.958438634872437
printing an ep nov before normalisation:  35.80863042031509
actor:  0 policy actor:  0  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2.0
from probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
printing an ep nov before normalisation:  26.865382194519043
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  31 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.088]
 [0.12 ]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.088]
 [0.12 ]
 [0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]]
UNIT TEST: sample policy line 217 mcts : [0.143 0.204 0.102 0.245 0.082 0.082 0.143]
actor:  0 policy actor:  0  step number:  33 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[ 0.285]
 [ 0.257]
 [-0.201]
 [ 0.236]
 [ 0.258]
 [ 0.223]
 [ 0.243]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.285]
 [ 0.257]
 [-0.201]
 [ 0.236]
 [ 0.258]
 [ 0.223]
 [ 0.243]]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.454588704843964
from probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
Printing some Q and Qe and total Qs values:  [[0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]] [[21.501]
 [21.501]
 [21.501]
 [21.501]
 [21.501]
 [21.501]
 [21.501]] [[0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]]
actor:  1 policy actor:  1  step number:  33 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.19851999999999986 0.6779999999999999 0.6779999999999999
maxi score, test score, baseline:  0.19851999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
printing an ep nov before normalisation:  50.78876512821909
maxi score, test score, baseline:  0.19851999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
maxi score, test score, baseline:  0.19851999999999986 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
actor:  0 policy actor:  0  step number:  26 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  33 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  2  action  0 :  tensor([0.6937, 0.0486, 0.0526, 0.0419, 0.0758, 0.0419, 0.0455],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0018,     0.9608,     0.0175,     0.0062,     0.0001,     0.0005,
            0.0132], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0551, 0.0991, 0.6704, 0.0410, 0.0473, 0.0567, 0.0304],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1582, 0.0967, 0.0974, 0.3086, 0.1211, 0.1258, 0.0922],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0980, 0.0914, 0.0796, 0.0918, 0.4501, 0.1057, 0.0835],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0638, 0.0025, 0.1479, 0.0606, 0.0659, 0.6014, 0.0579],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1057, 0.1764, 0.0689, 0.0619, 0.0619, 0.0703, 0.4550],
       grad_fn=<DivBackward0>)
siam score:  -0.80737704
actor:  1 policy actor:  1  step number:  30 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.0
using another actor
Printing some Q and Qe and total Qs values:  [[0.563]
 [0.548]
 [0.563]
 [0.563]
 [0.503]
 [0.563]
 [0.563]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.563]
 [0.548]
 [0.563]
 [0.563]
 [0.503]
 [0.563]
 [0.563]]
maxi score, test score, baseline:  0.19833999999999982 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19497999999999988 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
from probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
printing an ep nov before normalisation:  50.84450035035866
actor:  0 policy actor:  0  step number:  24 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  1  action  0 :  tensor([0.5147, 0.0039, 0.0952, 0.0768, 0.1478, 0.0800, 0.0817],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0046,     0.9742,     0.0030,     0.0050,     0.0003,     0.0003,
            0.0125], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0237, 0.0025, 0.8552, 0.0286, 0.0270, 0.0362, 0.0267],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0979, 0.0068, 0.1577, 0.3720, 0.1021, 0.1282, 0.1354],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0773, 0.0102, 0.0777, 0.0932, 0.5706, 0.0872, 0.0838],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0457, 0.0027, 0.0901, 0.0969, 0.0815, 0.5701, 0.1130],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1055, 0.0154, 0.1619, 0.1300, 0.1024, 0.1239, 0.3608],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  25 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.19135999999999984 0.6779999999999999 0.6779999999999999
maxi score, test score, baseline:  0.19135999999999984 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
Printing some Q and Qe and total Qs values:  [[0.208]
 [0.323]
 [0.212]
 [0.239]
 [0.208]
 [0.194]
 [0.22 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.208]
 [0.323]
 [0.212]
 [0.239]
 [0.208]
 [0.194]
 [0.22 ]]
maxi score, test score, baseline:  0.19135999999999984 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
printing an ep nov before normalisation:  52.757936037510824
maxi score, test score, baseline:  0.19135999999999984 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
printing an ep nov before normalisation:  52.78339471275704
printing an ep nov before normalisation:  53.32870805897615
printing an ep nov before normalisation:  41.50257842273887
actor:  1 policy actor:  1  step number:  24 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  21 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  29 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.19135999999999984 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
maxi score, test score, baseline:  0.19135999999999984 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
printing an ep nov before normalisation:  64.48907286939385
printing an ep nov before normalisation:  50.9256860484334
maxi score, test score, baseline:  0.19135999999999984 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
actor:  1 policy actor:  1  step number:  21 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  50.63651856834669
maxi score, test score, baseline:  0.19135999999999984 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
actor:  0 policy actor:  1  step number:  24 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8010312
maxi score, test score, baseline:  0.19117999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
printing an ep nov before normalisation:  53.425209413639074
actions average: 
K:  4  action  0 :  tensor([0.6474, 0.0116, 0.1163, 0.1004, 0.0400, 0.0445, 0.0398],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0054,     0.9645,     0.0035,     0.0017,     0.0003,     0.0013,
            0.0233], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1295, 0.0038, 0.4541, 0.1154, 0.1006, 0.1089, 0.0878],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0715, 0.0292, 0.0888, 0.3291, 0.1545, 0.1365, 0.1904],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1011, 0.0066, 0.0694, 0.1264, 0.5107, 0.1167, 0.0691],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([    0.0287,     0.0004,     0.1064,     0.1072,     0.1135,     0.5772,
            0.0667], grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1153, 0.1036, 0.1171, 0.1239, 0.0843, 0.1605, 0.2953],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  61.00865671659363
printing an ep nov before normalisation:  62.733792766712014
maxi score, test score, baseline:  0.19117999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
from probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
actor:  0 policy actor:  1  step number:  22 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  56.42245587045579
printing an ep nov before normalisation:  48.88667478510852
actor:  1 policy actor:  1  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.604]
 [0.701]
 [0.62 ]
 [0.604]
 [0.604]
 [0.604]
 [0.645]] [[36.55 ]
 [38.951]
 [33.451]
 [36.55 ]
 [36.55 ]
 [36.55 ]
 [39.369]] [[0.991]
 [1.137]
 [0.944]
 [0.991]
 [0.991]
 [0.991]
 [1.089]]
actor:  0 policy actor:  1  step number:  23 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
from probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
siam score:  -0.796727
maxi score, test score, baseline:  0.19411999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
Printing some Q and Qe and total Qs values:  [[0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]] [[34.88]
 [34.88]
 [34.88]
 [34.88]
 [34.88]
 [34.88]
 [34.88]] [[0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]]
Printing some Q and Qe and total Qs values:  [[0.44 ]
 [0.463]
 [0.421]
 [0.423]
 [0.415]
 [0.416]
 [0.771]] [[30.093]
 [22.741]
 [30.019]
 [33.354]
 [33.789]
 [32.473]
 [31.35 ]] [[0.967]
 [0.862]
 [0.947]
 [1.008]
 [1.008]
 [0.985]
 [1.321]]
siam score:  -0.7963878
actor:  1 policy actor:  1  step number:  23 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  39 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.19411999999999985 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
maxi score, test score, baseline:  0.19411999999999985 0.6779999999999999 0.6779999999999999
actor:  0 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.522]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]] [[32.648]
 [38.534]
 [32.648]
 [32.648]
 [32.648]
 [32.648]
 [32.648]] [[1.447]
 [1.798]
 [1.447]
 [1.447]
 [1.447]
 [1.447]
 [1.447]]
actor:  1 policy actor:  1  step number:  28 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.19389999999999982 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
actor:  1 policy actor:  1  step number:  30 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.738]
 [0.892]
 [0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]] [[44.794]
 [44.678]
 [44.794]
 [44.794]
 [44.794]
 [44.794]
 [44.794]] [[0.738]
 [0.892]
 [0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]]
maxi score, test score, baseline:  0.19389999999999982 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
actor:  1 policy actor:  1  step number:  36 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  33 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.19341999999999984 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
maxi score, test score, baseline:  0.19341999999999984 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
printing an ep nov before normalisation:  41.272631859335554
printing an ep nov before normalisation:  29.637508392333984
maxi score, test score, baseline:  0.19341999999999984 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
actor:  0 policy actor:  0  step number:  26 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  28 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  40.47042323468661
maxi score, test score, baseline:  0.1927799999999999 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
maxi score, test score, baseline:  0.1927799999999999 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
actions average: 
K:  0  action  0 :  tensor([0.7493, 0.0084, 0.0408, 0.0431, 0.0665, 0.0443, 0.0476],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0021,     0.9718,     0.0048,     0.0026,     0.0003,     0.0004,
            0.0180], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0511, 0.0103, 0.7302, 0.0317, 0.0594, 0.0724, 0.0449],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1476, 0.0032, 0.1979, 0.1416, 0.1541, 0.2074, 0.1483],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0993,     0.0003,     0.0570,     0.0558,     0.6323,     0.0851,
            0.0702], grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([    0.0563,     0.0004,     0.0528,     0.0276,     0.1099,     0.7363,
            0.0167], grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0862, 0.3245, 0.0631, 0.1378, 0.1036, 0.0942, 0.1905],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  31 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1927799999999999 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
actor:  1 policy actor:  1  step number:  42 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  40.98361421751976
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1927799999999999 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
maxi score, test score, baseline:  0.1927799999999999 0.6779999999999999 0.6779999999999999
probs:  [0.073794674089384, 0.04909251387604252, 0.19671387405325924, 0.053189945615733945, 0.2909548040661625, 0.3362541882994178]
actor:  1 policy actor:  1  step number:  39 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  29.684068567654574
Printing some Q and Qe and total Qs values:  [[0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.641]] [[36.484]
 [36.484]
 [36.484]
 [36.484]
 [36.484]
 [36.484]
 [36.484]] [[1.806]
 [1.806]
 [1.806]
 [1.806]
 [1.806]
 [1.806]
 [1.806]]
actor:  1 policy actor:  1  step number:  32 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  37 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  42.59500386603214
actor:  1 policy actor:  1  step number:  22 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  38.012929199742494
actor:  1 policy actor:  1  step number:  24 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1927799999999999 0.6779999999999999 0.6779999999999999
probs:  [0.07373198187696144, 0.04909583017648066, 0.1967271896165582, 0.05319353945932005, 0.2909745031218661, 0.3362769557488135]
Printing some Q and Qe and total Qs values:  [[0.44 ]
 [0.432]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]] [[35.578]
 [40.426]
 [35.578]
 [35.578]
 [35.578]
 [35.578]
 [35.578]] [[1.689]
 [1.946]
 [1.689]
 [1.689]
 [1.689]
 [1.689]
 [1.689]]
printing an ep nov before normalisation:  44.555507363112675
maxi score, test score, baseline:  0.1927799999999999 0.6779999999999999 0.6779999999999999
maxi score, test score, baseline:  0.1927799999999999 0.6779999999999999 0.6779999999999999
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.421]
 [0.703]
 [0.421]
 [0.421]
 [0.421]
 [0.421]] [[31.226]
 [31.226]
 [39.27 ]
 [31.226]
 [31.226]
 [31.226]
 [31.226]] [[1.231]
 [1.231]
 [1.909]
 [1.231]
 [1.231]
 [1.231]
 [1.231]]
printing an ep nov before normalisation:  30.572902675413562
actor:  1 policy actor:  1  step number:  32 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  29 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.48563027606394
actions average: 
K:  4  action  0 :  tensor([0.6208, 0.0188, 0.0744, 0.0702, 0.0754, 0.0691, 0.0714],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0141, 0.9413, 0.0043, 0.0076, 0.0048, 0.0025, 0.0255],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0413, 0.0845, 0.6867, 0.0400, 0.0379, 0.0773, 0.0324],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0868, 0.0212, 0.1414, 0.3029, 0.1698, 0.1478, 0.1301],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1469, 0.0052, 0.0471, 0.0387, 0.6503, 0.0506, 0.0612],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0853, 0.0225, 0.3185, 0.1111, 0.1082, 0.2569, 0.0974],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1009, 0.0128, 0.1069, 0.2267, 0.1564, 0.1349, 0.2614],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1927799999999999 0.6779999999999999 0.6779999999999999
probs:  [0.07366962382083117, 0.049099128800677745, 0.19674043420646045, 0.05319711414733016, 0.29099409717946606, 0.33629960184523433]
maxi score, test score, baseline:  0.1927799999999999 0.6779999999999999 0.6779999999999999
probs:  [0.07366962382083117, 0.049099128800677745, 0.19674043420646045, 0.05319711414733016, 0.29099409717946606, 0.33629960184523433]
printing an ep nov before normalisation:  49.432566349076964
actor:  1 policy actor:  1  step number:  23 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  23 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[ 0.097]
 [ 0.21 ]
 [ 0.266]
 [ 0.188]
 [-0.067]
 [ 0.232]
 [-0.027]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.097]
 [ 0.21 ]
 [ 0.266]
 [ 0.188]
 [-0.067]
 [ 0.232]
 [-0.027]]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[ 0.429]
 [ 0.615]
 [ 0.429]
 [ 0.325]
 [ 0.429]
 [-0.103]
 [-0.104]] [[40.675]
 [37.147]
 [40.675]
 [32.405]
 [40.675]
 [30.276]
 [33.499]] [[2.349]
 [2.247]
 [2.349]
 [1.57 ]
 [2.349]
 [0.968]
 [1.23 ]]
Printing some Q and Qe and total Qs values:  [[0.756]
 [0.846]
 [0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]] [[36.892]
 [43.861]
 [36.892]
 [36.892]
 [36.892]
 [36.892]
 [36.892]] [[0.756]
 [0.846]
 [0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]]
Printing some Q and Qe and total Qs values:  [[0.709]
 [0.759]
 [0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.709]] [[39.633]
 [45.821]
 [39.633]
 [39.633]
 [39.633]
 [39.633]
 [39.633]] [[0.709]
 [0.759]
 [0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.709]]
printing an ep nov before normalisation:  36.285814415357265
printing an ep nov before normalisation:  29.491400718688965
printing an ep nov before normalisation:  43.369940004668464
printing an ep nov before normalisation:  41.15848732511628
maxi score, test score, baseline:  0.19273999999999983 0.6779999999999999 0.6779999999999999
probs:  [0.07366962382083117, 0.049099128800677745, 0.19674043420646045, 0.05319711414733016, 0.29099409717946606, 0.33629960184523433]
printing an ep nov before normalisation:  42.33005974149049
Printing some Q and Qe and total Qs values:  [[0.673]
 [0.673]
 [0.673]
 [0.673]
 [0.673]
 [0.689]
 [0.673]] [[37.951]
 [37.951]
 [37.951]
 [37.951]
 [37.951]
 [44.062]
 [37.951]] [[1.974]
 [1.974]
 [1.974]
 [1.974]
 [1.974]
 [2.356]
 [1.974]]
actor:  0 policy actor:  0  step number:  16 total reward:  0.73  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.72  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.72  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.21153999999999984 0.6779999999999999 0.6779999999999999
probs:  [0.07366962382083117, 0.049099128800677745, 0.19674043420646045, 0.05319711414733016, 0.29099409717946606, 0.33629960184523433]
actor:  0 policy actor:  0  step number:  19 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  19 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  48.750383557842376
actor:  1 policy actor:  1  step number:  40 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  1.667
from probs:  [0.06618217448399248, 0.04986978303740746, 0.20845460566794938, 0.05160082699127168, 0.2838433362301179, 0.340049273589261]
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.438]
 [0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.354]] [[34.2  ]
 [40.568]
 [34.2  ]
 [34.2  ]
 [34.2  ]
 [34.2  ]
 [34.2  ]] [[0.765]
 [0.995]
 [0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]]
maxi score, test score, baseline:  0.21817999999999987 0.683 0.683
probs:  [0.06618217448399248, 0.04986978303740746, 0.20845460566794938, 0.05160082699127168, 0.2838433362301179, 0.340049273589261]
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.718]
 [0.628]
 [0.589]
 [0.592]
 [0.625]
 [0.638]] [[48.469]
 [48.233]
 [48.188]
 [48.923]
 [48.469]
 [48.888]
 [49.056]] [[1.683]
 [1.798]
 [1.706]
 [1.701]
 [1.683]
 [1.735]
 [1.755]]
printing an ep nov before normalisation:  43.05020955605076
actor:  1 policy actor:  1  step number:  33 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.21817999999999987 0.683 0.683
probs:  [0.06618217448399248, 0.04986978303740746, 0.20845460566794938, 0.05160082699127168, 0.2838433362301179, 0.340049273589261]
actor:  0 policy actor:  1  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  47.7516303916859
actor:  1 policy actor:  1  step number:  37 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.201]
 [0.352]
 [0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.201]
 [0.352]
 [0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.114]]
printing an ep nov before normalisation:  23.71713433946882
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.21811999999999987 0.683 0.683
probs:  [0.06618217448399248, 0.04986978303740746, 0.20845460566794938, 0.05160082699127168, 0.2838433362301179, 0.340049273589261]
actor:  1 policy actor:  1  step number:  29 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.358]
 [0.332]
 [0.354]
 [0.354]
 [0.354]
 [0.354]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.354]
 [0.358]
 [0.332]
 [0.354]
 [0.354]
 [0.354]
 [0.354]]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  38.71675946605398
Printing some Q and Qe and total Qs values:  [[ 0.73 ]
 [ 0.741]
 [-0.114]
 [ 0.627]
 [ 0.674]
 [-0.112]
 [ 0.75 ]] [[47.351]
 [48.113]
 [50.947]
 [51.531]
 [49.561]
 [48.526]
 [49.694]] [[2.174]
 [2.226]
 [1.524]
 [2.297]
 [2.237]
 [1.395]
 [2.321]]
maxi score, test score, baseline:  0.21811999999999987 0.683 0.683
probs:  [0.06618217448399248, 0.04986978303740746, 0.20845460566794938, 0.05160082699127168, 0.2838433362301179, 0.340049273589261]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.21811999999999987 0.683 0.683
actor:  1 policy actor:  1  step number:  23 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  29 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.667
using another actor
from probs:  [0.06618217448399248, 0.04986978303740746, 0.20845460566794938, 0.05160082699127168, 0.2838433362301179, 0.340049273589261]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  31 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[ 0.775]
 [-0.008]
 [-0.081]
 [ 0.643]
 [ 0.666]
 [ 0.709]
 [ 0.356]] [[33.883]
 [30.398]
 [27.859]
 [28.135]
 [31.052]
 [28.042]
 [28.133]] [[1.178]
 [0.325]
 [0.201]
 [0.931]
 [1.012]
 [0.995]
 [0.644]]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]] [[44.32 ]
 [44.021]
 [44.021]
 [44.021]
 [44.021]
 [44.021]
 [44.021]] [[2.243]
 [2.118]
 [2.118]
 [2.118]
 [2.118]
 [2.118]
 [2.118]]
Printing some Q and Qe and total Qs values:  [[0.291]
 [0.239]
 [0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.291]
 [0.239]
 [0.291]
 [0.291]
 [0.291]
 [0.291]
 [0.291]]
actor:  1 policy actor:  1  step number:  26 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.21221999999999985 0.683 0.683
actor:  1 policy actor:  1  step number:  32 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  40.762207859213795
printing an ep nov before normalisation:  40.824637385257844
maxi score, test score, baseline:  0.21221999999999985 0.683 0.683
probs:  [0.06618217448399248, 0.04986978303740746, 0.20845460566794938, 0.05160082699127168, 0.2838433362301179, 0.340049273589261]
printing an ep nov before normalisation:  42.35261602234306
actor:  0 policy actor:  0  step number:  21 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  35 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.21273999999999987 0.683 0.683
Printing some Q and Qe and total Qs values:  [[0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]] [[34.389]
 [34.389]
 [34.389]
 [34.389]
 [34.389]
 [34.389]
 [34.389]] [[2.572]
 [2.572]
 [2.572]
 [2.572]
 [2.572]
 [2.572]
 [2.572]]
printing an ep nov before normalisation:  21.50219202041626
maxi score, test score, baseline:  0.21273999999999987 0.683 0.683
probs:  [0.06618217448399248, 0.04986978303740746, 0.20845460566794938, 0.05160082699127168, 0.2838433362301179, 0.340049273589261]
actor:  1 policy actor:  1  step number:  44 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  39.55740506406925
Printing some Q and Qe and total Qs values:  [[0.796]
 [0.824]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]] [[ 0.   ]
 [23.261]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.796]
 [0.824]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]]
actor:  0 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  29.58694815325699
maxi score, test score, baseline:  0.21287999999999985 0.683 0.683
siam score:  -0.78928536
maxi score, test score, baseline:  0.21287999999999985 0.683 0.683
maxi score, test score, baseline:  0.21287999999999985 0.683 0.683
probs:  [0.06618217448399248, 0.04986978303740746, 0.20845460566794938, 0.05160082699127168, 0.2838433362301179, 0.340049273589261]
actor:  1 policy actor:  1  step number:  33 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21287999999999985 0.683 0.683
probs:  [0.06618217448399248, 0.04986978303740746, 0.20845460566794938, 0.05160082699127168, 0.2838433362301179, 0.340049273589261]
Printing some Q and Qe and total Qs values:  [[1.011]
 [1.016]
 [0.995]
 [0.987]
 [0.969]
 [0.978]
 [0.995]] [[34.492]
 [33.432]
 [36.487]
 [37.706]
 [40.99 ]
 [38.497]
 [36.487]] [[1.011]
 [1.016]
 [0.995]
 [0.987]
 [0.969]
 [0.978]
 [0.995]]
actor:  0 policy actor:  1  step number:  24 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.21279999999999982 0.683 0.683
probs:  [0.06618217448399248, 0.04986978303740746, 0.20845460566794938, 0.05160082699127168, 0.2838433362301179, 0.340049273589261]
actor:  0 policy actor:  0  step number:  37 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.53986602838401
maxi score, test score, baseline:  0.21547999999999984 0.683 0.683
probs:  [0.06618217448399248, 0.04986978303740746, 0.20845460566794938, 0.05160082699127168, 0.2838433362301179, 0.340049273589261]
maxi score, test score, baseline:  0.21547999999999984 0.683 0.683
probs:  [0.06618217448399248, 0.04986978303740746, 0.20845460566794938, 0.05160082699127168, 0.2838433362301179, 0.340049273589261]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  1  action  0 :  tensor([0.5969, 0.0018, 0.0603, 0.0672, 0.1317, 0.0783, 0.0637],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0016, 0.9749, 0.0029, 0.0063, 0.0017, 0.0013, 0.0112],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0492, 0.0008, 0.7984, 0.0345, 0.0380, 0.0478, 0.0312],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0701, 0.2693, 0.0865, 0.1561, 0.0716, 0.0703, 0.2761],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2645, 0.0019, 0.0933, 0.0930, 0.3388, 0.1285, 0.0800],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1318, 0.0875, 0.1845, 0.0753, 0.0849, 0.3715, 0.0644],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0780, 0.1945, 0.0746, 0.0989, 0.0609, 0.0709, 0.4222],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]] [[39.079]
 [39.079]
 [39.079]
 [39.079]
 [39.079]
 [39.079]
 [39.079]] [[0.846]
 [0.846]
 [0.846]
 [0.846]
 [0.846]
 [0.846]
 [0.846]]
printing an ep nov before normalisation:  45.12707805616696
maxi score, test score, baseline:  0.21547999999999984 0.683 0.683
probs:  [0.06618217448399248, 0.04986978303740746, 0.20845460566794938, 0.05160082699127168, 0.2838433362301179, 0.340049273589261]
line 256 mcts: sample exp_bonus 39.12656198451496
actor:  1 policy actor:  1  step number:  27 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  0.333
from probs:  [0.06618217448399248, 0.04986978303740746, 0.20845460566794938, 0.05160082699127168, 0.2838433362301179, 0.340049273589261]
maxi score, test score, baseline:  0.21547999999999987 0.683 0.683
maxi score, test score, baseline:  0.21547999999999987 0.683 0.683
probs:  [0.06614066279211167, 0.04987199559150556, 0.20846387294246668, 0.051603116551674086, 0.28385595720777057, 0.34006439491447155]
printing an ep nov before normalisation:  57.59171567578549
printing an ep nov before normalisation:  63.800872590199674
actor:  1 policy actor:  1  step number:  25 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  29 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.21547999999999987 0.683 0.683
probs:  [0.06614066279211167, 0.04987199559150556, 0.20846387294246668, 0.051603116551674086, 0.28385595720777057, 0.34006439491447155]
printing an ep nov before normalisation:  25.67058310613741
maxi score, test score, baseline:  0.21547999999999987 0.683 0.683
probs:  [0.06614066279211167, 0.04987199559150556, 0.20846387294246668, 0.051603116551674086, 0.28385595720777057, 0.34006439491447155]
siam score:  -0.7875371
maxi score, test score, baseline:  0.21547999999999987 0.683 0.683
probs:  [0.06614066279211167, 0.04987199559150556, 0.20846387294246668, 0.051603116551674086, 0.28385595720777057, 0.34006439491447155]
printing an ep nov before normalisation:  0.0037033132400665636
actor:  1 policy actor:  1  step number:  22 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  22 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21547999999999987 0.683 0.683
actor:  1 policy actor:  1  step number:  21 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 34.6175802619886
maxi score, test score, baseline:  0.21547999999999987 0.683 0.683
probs:  [0.06614066279211167, 0.04987199559150556, 0.20846387294246668, 0.051603116551674086, 0.28385595720777057, 0.34006439491447155]
actor:  1 policy actor:  1  step number:  31 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
from probs:  [0.06614066279211167, 0.04987199559150556, 0.20846387294246668, 0.051603116551674086, 0.28385595720777057, 0.34006439491447155]
printing an ep nov before normalisation:  45.1067877101397
actor:  1 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  38 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.312]
 [0.327]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.312]
 [0.327]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]]
actions average: 
K:  4  action  0 :  tensor([0.5433, 0.0723, 0.0622, 0.0630, 0.0902, 0.0995, 0.0695],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0039,     0.9596,     0.0023,     0.0016,     0.0001,     0.0003,
            0.0322], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0403, 0.0736, 0.6991, 0.0550, 0.0266, 0.0671, 0.0382],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1744, 0.1442, 0.0948, 0.1403, 0.0796, 0.0760, 0.2908],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.3152,     0.0000,     0.0000,     0.0025,     0.6805,     0.0000,
            0.0017], grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0836, 0.1766, 0.3071, 0.0789, 0.0459, 0.2160, 0.0918],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1334, 0.2966, 0.1313, 0.1024, 0.1005, 0.1096, 0.1263],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  53.86943839716683
actor:  1 policy actor:  1  step number:  19 total reward:  0.7  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  22 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.21215999999999988 0.683 0.683
maxi score, test score, baseline:  0.21215999999999988 0.683 0.683
probs:  [0.06648551622500411, 0.05054253744954521, 0.20595950574749677, 0.052239002463267584, 0.2798422881818532, 0.34493114993283314]
printing an ep nov before normalisation:  42.79339940381548
printing an ep nov before normalisation:  45.90032675925796
maxi score, test score, baseline:  0.21215999999999988 0.683 0.683
probs:  [0.06648551622500411, 0.05054253744954521, 0.20595950574749677, 0.052239002463267584, 0.2798422881818532, 0.34493114993283314]
Printing some Q and Qe and total Qs values:  [[0.784]
 [0.815]
 [0.748]
 [0.749]
 [0.785]
 [0.785]
 [0.766]] [[19.204]
 [17.894]
 [22.357]
 [22.299]
 [18.937]
 [18.825]
 [20.53 ]] [[1.416]
 [1.403]
 [1.484]
 [1.483]
 [1.408]
 [1.404]
 [1.441]]
printing an ep nov before normalisation:  0.0019574116259946095
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  36 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.0
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  41.67296943176125
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.21215999999999988 0.683 0.683
maxi score, test score, baseline:  0.21215999999999988 0.683 0.683
probs:  [0.06648551622500411, 0.05054253744954521, 0.20595950574749677, 0.052239002463267584, 0.2798422881818532, 0.34493114993283314]
maxi score, test score, baseline:  0.21215999999999988 0.683 0.683
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21215999999999988 0.683 0.683
probs:  [0.06648551622500411, 0.05054253744954521, 0.20595950574749677, 0.052239002463267584, 0.2798422881818532, 0.34493114993283314]
actor:  0 policy actor:  0  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
using another actor
actor:  1 policy actor:  1  step number:  37 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2.0
using another actor
from probs:  [0.06648551622500411, 0.05054253744954521, 0.20595950574749677, 0.052239002463267584, 0.2798422881818532, 0.34493114993283314]
printing an ep nov before normalisation:  0.14249154654066842
maxi score, test score, baseline:  0.21207999999999985 0.683 0.683
probs:  [0.06614066279211167, 0.04987199559150556, 0.20846387294246668, 0.051603116551674086, 0.28385595720777057, 0.34006439491447155]
printing an ep nov before normalisation:  49.724008205429946
printing an ep nov before normalisation:  60.72384255524362
actor:  0 policy actor:  0  step number:  27 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  32.56662368774414
using explorer policy with actor:  0
printing an ep nov before normalisation:  49.718484709735876
actor:  0 policy actor:  1  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.21519999999999984 0.683 0.683
probs:  [0.06614066279211167, 0.04987199559150556, 0.20846387294246668, 0.051603116551674086, 0.28385595720777057, 0.34006439491447155]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.359]
 [0.453]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]] [[35.77 ]
 [42.181]
 [35.77 ]
 [35.77 ]
 [35.77 ]
 [35.77 ]
 [35.77 ]] [[1.3]
 [1.7]
 [1.3]
 [1.3]
 [1.3]
 [1.3]
 [1.3]]
maxi score, test score, baseline:  0.21519999999999984 0.683 0.683
actor:  0 policy actor:  1  step number:  27 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 44.18069369965106
actor:  0 policy actor:  1  step number:  33 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  25 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  24 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.677]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]] [[47.328]
 [50.186]
 [50.186]
 [50.186]
 [50.186]
 [50.186]
 [50.186]] [[1.746]
 [1.755]
 [1.755]
 [1.755]
 [1.755]
 [1.755]
 [1.755]]
maxi score, test score, baseline:  0.2210799999999999 0.683 0.683
printing an ep nov before normalisation:  54.86577846480845
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2210799999999999 0.683 0.683
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  31 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.22123999999999985 0.683 0.683
probs:  [0.06614066279211167, 0.04987199559150556, 0.20846387294246668, 0.051603116551674086, 0.28385595720777057, 0.34006439491447155]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.22123999999999985 0.683 0.683
maxi score, test score, baseline:  0.22123999999999985 0.683 0.683
probs:  [0.06609937304380976, 0.04987419631611291, 0.20847309066920644, 0.051605393870869334, 0.2838685107069651, 0.3400794353930365]
maxi score, test score, baseline:  0.22123999999999985 0.683 0.683
probs:  [0.06609937304380976, 0.04987419631611291, 0.20847309066920644, 0.051605393870869334, 0.2838685107069651, 0.3400794353930365]
actor:  1 policy actor:  1  step number:  31 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  21 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.22431999999999988 0.683 0.683
probs:  [0.06609937304380976, 0.04987419631611291, 0.20847309066920644, 0.051605393870869334, 0.2838685107069651, 0.3400794353930365]
maxi score, test score, baseline:  0.22431999999999988 0.683 0.683
probs:  [0.06609937304380976, 0.04987419631611291, 0.20847309066920644, 0.051605393870869334, 0.2838685107069651, 0.3400794353930365]
maxi score, test score, baseline:  0.22431999999999988 0.683 0.683
probs:  [0.06609937304380976, 0.04987419631611291, 0.20847309066920644, 0.051605393870869334, 0.2838685107069651, 0.3400794353930365]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.22431999999999988 0.683 0.683
probs:  [0.06609937304380976, 0.04987419631611291, 0.20847309066920644, 0.051605393870869334, 0.2838685107069651, 0.3400794353930365]
maxi score, test score, baseline:  0.22431999999999988 0.683 0.683
probs:  [0.06609937304380976, 0.04987419631611291, 0.20847309066920644, 0.051605393870869334, 0.2838685107069651, 0.3400794353930365]
printing an ep nov before normalisation:  27.973379029168022
maxi score, test score, baseline:  0.22431999999999988 0.683 0.683
actor:  1 policy actor:  1  step number:  36 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  35.008912119499136
printing an ep nov before normalisation:  46.03467993371193
actor:  1 policy actor:  1  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.22431999999999988 0.683 0.683
Printing some Q and Qe and total Qs values:  [[0.182]
 [0.336]
 [0.186]
 [0.26 ]
 [0.184]
 [0.271]
 [0.191]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.182]
 [0.336]
 [0.186]
 [0.26 ]
 [0.184]
 [0.271]
 [0.191]]
maxi score, test score, baseline:  0.22431999999999988 0.683 0.683
probs:  [0.06609937304380976, 0.04987419631611291, 0.20847309066920644, 0.051605393870869334, 0.2838685107069651, 0.3400794353930365]
printing an ep nov before normalisation:  39.80218045577104
actor:  0 policy actor:  1  step number:  21 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  44 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.22449999999999987 0.683 0.683
probs:  [0.06609937304380976, 0.04987419631611291, 0.20847309066920644, 0.051605393870869334, 0.2838685107069651, 0.3400794353930365]
printing an ep nov before normalisation:  31.59634152561096
maxi score, test score, baseline:  0.22449999999999987 0.683 0.683
probs:  [0.06609937304380976, 0.04987419631611291, 0.20847309066920644, 0.051605393870869334, 0.2838685107069651, 0.3400794353930365]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.274]
 [0.274]
 [0.383]
 [0.274]
 [0.274]
 [0.274]
 [0.274]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.274]
 [0.274]
 [0.383]
 [0.274]
 [0.274]
 [0.274]
 [0.274]]
printing an ep nov before normalisation:  24.694852828979492
printing an ep nov before normalisation:  40.69107754906348
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  26 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.22449999999999987 0.683 0.683
probs:  [0.06609937304380976, 0.04987419631611291, 0.20847309066920644, 0.051605393870869334, 0.2838685107069651, 0.3400794353930365]
maxi score, test score, baseline:  0.22449999999999987 0.683 0.683
probs:  [0.06609937304380976, 0.04987419631611291, 0.20847309066920644, 0.051605393870869334, 0.2838685107069651, 0.3400794353930365]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
Printing some Q and Qe and total Qs values:  [[0.692]
 [0.705]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]] [[32.47 ]
 [44.498]
 [32.47 ]
 [32.47 ]
 [32.47 ]
 [32.47 ]
 [32.47 ]] [[1.51 ]
 [2.114]
 [1.51 ]
 [1.51 ]
 [1.51 ]
 [1.51 ]
 [1.51 ]]
printing an ep nov before normalisation:  48.852624323102425
actor:  1 policy actor:  1  step number:  21 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.713]
 [0.793]
 [0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]] [[46.676]
 [50.096]
 [46.676]
 [46.676]
 [46.676]
 [46.676]
 [46.676]] [[0.976]
 [1.083]
 [0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.976]]
printing an ep nov before normalisation:  46.13695095275945
maxi score, test score, baseline:  0.22449999999999987 0.683 0.683
probs:  [0.06609937304380976, 0.04987419631611291, 0.20847309066920644, 0.051605393870869334, 0.2838685107069651, 0.3400794353930365]
actor:  0 policy actor:  1  step number:  30 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.929]
 [0.975]
 [0.929]
 [0.929]
 [0.929]
 [0.929]
 [0.929]] [[25.656]
 [31.005]
 [25.656]
 [25.656]
 [25.656]
 [25.656]
 [25.656]] [[0.929]
 [0.975]
 [0.929]
 [0.929]
 [0.929]
 [0.929]
 [0.929]]
actor:  0 policy actor:  0  step number:  24 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  21 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  54.372711499415345
maxi score, test score, baseline:  0.23065999999999984 0.683 0.683
probs:  [0.06605830346389, 0.04987638530584667, 0.20848225924447233, 0.051607659046767174, 0.28388099726742305, 0.3400943956716008]
maxi score, test score, baseline:  0.23065999999999984 0.683 0.683
maxi score, test score, baseline:  0.23065999999999984 0.683 0.683
probs:  [0.06605830346389, 0.04987638530584667, 0.20848225924447233, 0.051607659046767174, 0.28388099726742305, 0.3400943956716008]
maxi score, test score, baseline:  0.23065999999999984 0.683 0.683
actor:  1 policy actor:  1  step number:  41 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  29 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  36.448789847493344
maxi score, test score, baseline:  0.23065999999999984 0.683 0.683
probs:  [0.06605830346389, 0.04987638530584667, 0.20848225924447233, 0.051607659046767174, 0.28388099726742305, 0.3400943956716008]
actor:  1 policy actor:  1  step number:  35 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.669]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]] [[49.19 ]
 [48.223]
 [48.223]
 [48.223]
 [48.223]
 [48.223]
 [48.223]] [[2.343]
 [2.18 ]
 [2.18 ]
 [2.18 ]
 [2.18 ]
 [2.18 ]
 [2.18 ]]
maxi score, test score, baseline:  0.23065999999999984 0.683 0.683
actor:  1 policy actor:  1  step number:  28 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  26 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.23065999999999984 0.683 0.683
probs:  [0.06601745229603644, 0.04987856265431769, 0.208491379060353, 0.05160991217623703, 0.2838934174231236, 0.34010927638993227]
Printing some Q and Qe and total Qs values:  [[0.235]
 [0.368]
 [0.237]
 [0.235]
 [0.234]
 [0.231]
 [0.231]] [[19.394]
 [35.719]
 [19.121]
 [19.032]
 [19.085]
 [19.081]
 [19.451]] [[0.481]
 [1.153]
 [0.474]
 [0.469]
 [0.469]
 [0.466]
 [0.479]]
printing an ep nov before normalisation:  42.224543677728434
printing an ep nov before normalisation:  32.372145652770996
Printing some Q and Qe and total Qs values:  [[0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]] [[51.186]
 [51.186]
 [51.186]
 [51.186]
 [51.186]
 [51.186]
 [51.186]] [[2.244]
 [2.244]
 [2.244]
 [2.244]
 [2.244]
 [2.244]
 [2.244]]
actor:  1 policy actor:  1  step number:  35 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.432]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.504]
 [0.432]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]]
actor:  1 policy actor:  1  step number:  30 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.23065999999999984 0.683 0.683
probs:  [0.06601745229603644, 0.04987856265431769, 0.208491379060353, 0.05160991217623703, 0.2838934174231236, 0.34010927638993227]
maxi score, test score, baseline:  0.23065999999999984 0.683 0.683
actor:  1 policy actor:  1  step number:  32 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  0  action  0 :  tensor([0.6057, 0.0238, 0.0767, 0.0745, 0.0819, 0.0674, 0.0700],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0016,     0.9655,     0.0013,     0.0048,     0.0002,     0.0003,
            0.0264], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0421, 0.0160, 0.7499, 0.0383, 0.0413, 0.0761, 0.0364],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1511, 0.0925, 0.0941, 0.2590, 0.1185, 0.1440, 0.1409],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0802, 0.0220, 0.0543, 0.0606, 0.6587, 0.0705, 0.0536],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0804, 0.0032, 0.2071, 0.1044, 0.1005, 0.4154, 0.0891],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1232, 0.1572, 0.1245, 0.1443, 0.1346, 0.1408, 0.1753],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.23065999999999984 0.683 0.683
probs:  [0.06601745229603644, 0.04987856265431769, 0.208491379060353, 0.05160991217623703, 0.2838934174231236, 0.34010927638993227]
Printing some Q and Qe and total Qs values:  [[ 0.481]
 [ 0.481]
 [ 0.562]
 [ 0.209]
 [ 0.211]
 [-0.081]
 [ 0.381]] [[31.998]
 [31.998]
 [41.774]
 [30.188]
 [31.053]
 [29.868]
 [35.526]] [[0.602]
 [0.602]
 [0.76 ]
 [0.316]
 [0.325]
 [0.024]
 [0.53 ]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  34 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  2.0
from probs:  [0.06601745229603644, 0.04987856265431769, 0.208491379060353, 0.05160991217623703, 0.2838934174231236, 0.34010927638993227]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
from probs:  [0.06601745229603644, 0.04987856265431769, 0.208491379060353, 0.05160991217623703, 0.2838934174231236, 0.34010927638993227]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.303]
 [0.317]
 [0.303]
 [0.303]
 [0.303]
 [0.303]
 [0.303]] [[30.658]
 [42.098]
 [30.658]
 [30.658]
 [38.838]
 [30.658]
 [30.658]] [[1.119]
 [1.512]
 [1.119]
 [1.119]
 [1.389]
 [1.119]
 [1.119]]
maxi score, test score, baseline:  0.23065999999999984 0.683 0.683
probs:  [0.07155623351088784, 0.05409620315742582, 0.2261569504428274, 0.055974344528997105, 0.2232822442618503, 0.3689340240980115]
maxi score, test score, baseline:  0.23065999999999984 0.683 0.683
probs:  [0.07155623351088784, 0.05409620315742582, 0.2261569504428274, 0.055974344528997105, 0.2232822442618503, 0.3689340240980115]
printing an ep nov before normalisation:  51.50099488725252
maxi score, test score, baseline:  0.22781999999999986 0.683 0.683
probs:  [0.07155623351088784, 0.05409620315742582, 0.2261569504428274, 0.055974344528997105, 0.2232822442618503, 0.3689340240980115]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  40.22717444898731
actor:  1 policy actor:  1  step number:  46 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.667
using another actor
printing an ep nov before normalisation:  47.03324317932129
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  31 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  22 total reward:  0.69  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.553]
 [0.529]
 [0.326]
 [0.529]
 [0.529]
 [0.529]] [[48.193]
 [43.576]
 [45.583]
 [46.987]
 [45.583]
 [45.583]
 [45.583]] [[1.212]
 [1.243]
 [1.267]
 [1.097]
 [1.267]
 [1.267]
 [1.267]]
Printing some Q and Qe and total Qs values:  [[0.248]
 [0.293]
 [0.249]
 [0.236]
 [0.24 ]
 [0.24 ]
 [0.263]] [[26.786]
 [34.454]
 [26.769]
 [26.792]
 [27.768]
 [28.228]
 [32.446]] [[0.601]
 [0.851]
 [0.601]
 [0.588]
 [0.619]
 [0.631]
 [0.767]]
maxi score, test score, baseline:  0.22475999999999985 0.683 0.683
probs:  [0.07142223556143491, 0.05399496227563105, 0.2276075211480181, 0.05586958003339802, 0.2228635904957092, 0.3682421104858089]
maxi score, test score, baseline:  0.22475999999999985 0.683 0.683
printing an ep nov before normalisation:  23.29557180404663
actor:  1 policy actor:  1  step number:  23 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.22475999999999985 0.683 0.683
probs:  [0.07142223556143491, 0.05399496227563105, 0.2276075211480181, 0.05586958003339802, 0.2228635904957092, 0.3682421104858089]
actor:  1 policy actor:  1  step number:  29 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.22475999999999985 0.683 0.683
printing an ep nov before normalisation:  33.51075749489686
printing an ep nov before normalisation:  47.750174924307
printing an ep nov before normalisation:  36.002536301780424
printing an ep nov before normalisation:  31.409934588841033
printing an ep nov before normalisation:  32.33170509338379
printing an ep nov before normalisation:  34.334655132833866
printing an ep nov before normalisation:  39.43060490221704
printing an ep nov before normalisation:  54.343966434617336
maxi score, test score, baseline:  0.22475999999999985 0.683 0.683
probs:  [0.07137872839700941, 0.053997487700796765, 0.22761818677737503, 0.05587219335411353, 0.22287403369551134, 0.36825937007519394]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  33 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
using another actor
maxi score, test score, baseline:  0.22475999999999985 0.683 0.683
probs:  [0.07137872839700941, 0.053997487700796765, 0.22761818677737503, 0.05587219335411353, 0.22287403369551134, 0.36825937007519394]
actor:  1 policy actor:  1  step number:  34 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.36439060354116
Printing some Q and Qe and total Qs values:  [[0.275]
 [0.357]
 [0.275]
 [0.275]
 [0.275]
 [0.275]
 [0.275]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.275]
 [0.357]
 [0.275]
 [0.275]
 [0.275]
 [0.275]
 [0.275]]
actor:  1 policy actor:  1  step number:  20 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.22475999999999985 0.683 0.683
probs:  [0.07137872839700941, 0.053997487700796765, 0.22761818677737503, 0.05587219335411353, 0.22287403369551134, 0.36825937007519394]
Printing some Q and Qe and total Qs values:  [[0.335]
 [0.372]
 [0.335]
 [0.335]
 [0.335]
 [0.335]
 [0.335]] [[39.212]
 [39.894]
 [39.212]
 [39.212]
 [39.212]
 [39.212]
 [39.212]] [[1.734]
 [1.816]
 [1.734]
 [1.734]
 [1.734]
 [1.734]
 [1.734]]
actor:  0 policy actor:  1  step number:  32 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.22437999999999986 0.683 0.683
probs:  [0.07137872839700941, 0.053997487700796765, 0.22761818677737503, 0.05587219335411353, 0.22287403369551134, 0.36825937007519394]
maxi score, test score, baseline:  0.22437999999999986 0.683 0.683
probs:  [0.07137872839700941, 0.053997487700796765, 0.22761818677737503, 0.05587219335411353, 0.22287403369551134, 0.36825937007519394]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  36 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  21 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  42.93189357835504
Printing some Q and Qe and total Qs values:  [[ 0.677]
 [ 0.696]
 [ 0.654]
 [ 0.667]
 [ 0.669]
 [-0.005]
 [ 0.713]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.677]
 [ 0.696]
 [ 0.654]
 [ 0.667]
 [ 0.669]
 [-0.005]
 [ 0.713]]
maxi score, test score, baseline:  0.22479999999999986 0.683 0.683
probs:  [0.07137872839700941, 0.053997487700796765, 0.22761818677737503, 0.05587219335411353, 0.22287403369551134, 0.36825937007519394]
maxi score, test score, baseline:  0.22479999999999986 0.683 0.683
probs:  [0.07137872839700941, 0.053997487700796765, 0.22761818677737503, 0.05587219335411353, 0.22287403369551134, 0.36825937007519394]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  26 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7888033
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.572]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]] [[36.749]
 [43.024]
 [36.749]
 [36.749]
 [36.749]
 [36.749]
 [36.749]] [[1.629]
 [2.03 ]
 [1.629]
 [1.629]
 [1.629]
 [1.629]
 [1.629]]
using another actor
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.22161999999999984 0.683 0.683
probs:  [0.07133545046767606, 0.05399999981973855, 0.22762879621054996, 0.055874792905492306, 0.2228844218710918, 0.36827653872545135]
printing an ep nov before normalisation:  44.500336916676055
maxi score, test score, baseline:  0.21861999999999987 0.683 0.683
probs:  [0.07133545046767606, 0.05399999981973855, 0.22762879621054996, 0.055874792905492306, 0.2228844218710918, 0.36827653872545135]
maxi score, test score, baseline:  0.21861999999999987 0.683 0.683
probs:  [0.07133545046767606, 0.05399999981973855, 0.22762879621054996, 0.055874792905492306, 0.2228844218710918, 0.36827653872545135]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  57 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  31 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  37.32692241668701
line 256 mcts: sample exp_bonus 41.170322546075816
printing an ep nov before normalisation:  38.31680896291349
maxi score, test score, baseline:  0.21861999999999987 0.683 0.683
probs:  [0.07133545046767606, 0.05399999981973855, 0.22762879621054996, 0.055874792905492306, 0.2228844218710918, 0.36827653872545135]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  32 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.26 ]
 [0.394]
 [0.27 ]
 [0.334]
 [0.271]
 [0.268]
 [0.261]] [[25.105]
 [35.316]
 [32.229]
 [32.102]
 [29.551]
 [32.262]
 [26.218]] [[0.409]
 [0.714]
 [0.538]
 [0.6  ]
 [0.494]
 [0.537]
 [0.428]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.33887071983281
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.21861999999999987 0.683 0.683
probs:  [0.07133545046767606, 0.05399999981973855, 0.22762879621054996, 0.055874792905492306, 0.2228844218710918, 0.36827653872545135]
Printing some Q and Qe and total Qs values:  [[0.707]
 [0.754]
 [0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.707]
 [0.754]
 [0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]]
actor:  1 policy actor:  1  step number:  27 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.5  ]
 [1.5  ]
 [0.188]
 [1.5  ]
 [1.5  ]
 [0.187]
 [1.5  ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.5  ]
 [1.5  ]
 [0.188]
 [1.5  ]
 [1.5  ]
 [0.187]
 [1.5  ]]
printing an ep nov before normalisation:  50.31026456026784
actor:  0 policy actor:  0  step number:  20 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.0
using another actor
maxi score, test score, baseline:  0.21833999999999987 0.683 0.683
probs:  [0.07133545046767606, 0.05399999981973855, 0.22762879621054996, 0.055874792905492306, 0.2228844218710918, 0.36827653872545135]
actor:  1 policy actor:  1  step number:  31 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
using another actor
actor:  1 policy actor:  1  step number:  24 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 33.206987894875354
Printing some Q and Qe and total Qs values:  [[0.772]
 [0.83 ]
 [0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.772]] [[29.927]
 [33.947]
 [29.927]
 [29.927]
 [29.927]
 [29.927]
 [29.927]] [[0.772]
 [0.83 ]
 [0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.772]]
maxi score, test score, baseline:  0.21833999999999987 0.683 0.683
probs:  [0.07133545046767606, 0.05399999981973855, 0.22762879621054996, 0.055874792905492306, 0.2228844218710918, 0.36827653872545135]
actor:  0 policy actor:  1  step number:  31 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  49.852938859579844
printing an ep nov before normalisation:  0.013053513838485742
actor:  0 policy actor:  1  step number:  28 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.22415999999999986 0.683 0.683
probs:  [0.07133545046767606, 0.05399999981973855, 0.22762879621054996, 0.055874792905492306, 0.2228844218710918, 0.36827653872545135]
actor:  1 policy actor:  1  step number:  27 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.22415999999999986 0.683 0.683
probs:  [0.07133545046767606, 0.05399999981973855, 0.22762879621054996, 0.055874792905492306, 0.2228844218710918, 0.36827653872545135]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.22415999999999986 0.683 0.683
probs:  [0.07133545046767606, 0.05399999981973855, 0.22762879621054996, 0.055874792905492306, 0.2228844218710918, 0.36827653872545135]
printing an ep nov before normalisation:  44.32477026743759
siam score:  -0.79360616
printing an ep nov before normalisation:  51.430782115216395
from probs:  [0.07133545046767606, 0.05399999981973855, 0.22762879621054996, 0.055874792905492306, 0.2228844218710918, 0.36827653872545135]
maxi score, test score, baseline:  0.22415999999999986 0.683 0.683
probs:  [0.07129239996646886, 0.05400249873734403, 0.22763934989051446, 0.05587737879607128, 0.22289475545618342, 0.36829361715341796]
maxi score, test score, baseline:  0.22415999999999986 0.683 0.683
probs:  [0.07129239996646886, 0.05400249873734403, 0.22763934989051446, 0.05587737879607128, 0.22289475545618342, 0.36829361715341796]
actor:  1 policy actor:  1  step number:  50 total reward:  0.14999999999999947  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  16.14224511180405
actor:  1 policy actor:  1  step number:  33 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.609]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]] [[45.808]
 [47.053]
 [45.808]
 [45.808]
 [45.808]
 [45.808]
 [45.808]] [[2.383]
 [2.43 ]
 [2.383]
 [2.383]
 [2.383]
 [2.383]
 [2.383]]
maxi score, test score, baseline:  0.22453999999999985 0.683 0.683
probs:  [0.07129239996646886, 0.05400249873734403, 0.22763934989051446, 0.05587737879607128, 0.22289475545618342, 0.36829361715341796]
maxi score, test score, baseline:  0.22453999999999985 0.683 0.683
printing an ep nov before normalisation:  41.37193827091059
Printing some Q and Qe and total Qs values:  [[0.96 ]
 [1.001]
 [0.96 ]
 [0.96 ]
 [0.961]
 [0.96 ]
 [0.96 ]] [[15.165]
 [25.71 ]
 [15.165]
 [15.165]
 [15.433]
 [15.165]
 [15.165]] [[0.96 ]
 [1.001]
 [0.96 ]
 [0.96 ]
 [0.961]
 [0.96 ]
 [0.96 ]]
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.633]
 [0.601]
 [0.559]
 [0.613]
 [0.601]
 [0.556]] [[26.062]
 [35.651]
 [33.762]
 [26.284]
 [32.791]
 [33.762]
 [26.477]] [[0.869]
 [1.157]
 [1.082]
 [0.872]
 [1.072]
 [1.082]
 [0.873]]
maxi score, test score, baseline:  0.22453999999999985 0.683 0.683
printing an ep nov before normalisation:  0.04603621689902582
actor:  0 policy actor:  0  step number:  45 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  37 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 18.66629978075327
Printing some Q and Qe and total Qs values:  [[0.618]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]] [[37.097]
 [45.263]
 [45.263]
 [45.263]
 [45.263]
 [45.263]
 [45.263]] [[1.301]
 [1.404]
 [1.404]
 [1.404]
 [1.404]
 [1.404]
 [1.404]]
actor:  1 policy actor:  1  step number:  37 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.22725999999999985 0.683 0.683
probs:  [0.07129239996646886, 0.05400249873734403, 0.22763934989051446, 0.05587737879607128, 0.22289475545618342, 0.36829361715341796]
maxi score, test score, baseline:  0.22725999999999985 0.683 0.683
probs:  [0.07129239996646886, 0.05400249873734403, 0.22763934989051446, 0.05587737879607128, 0.22289475545618342, 0.36829361715341796]
printing an ep nov before normalisation:  31.99848692529945
actions average: 
K:  2  action  0 :  tensor([0.4245, 0.0997, 0.0862, 0.0765, 0.1353, 0.0724, 0.1054],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0012,     0.9509,     0.0091,     0.0098,     0.0004,     0.0007,
            0.0279], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0329, 0.0128, 0.7590, 0.0468, 0.0260, 0.0528, 0.0697],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0989, 0.0968, 0.1149, 0.3058, 0.1016, 0.1091, 0.1729],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.1251,     0.0006,     0.0639,     0.0609,     0.6158,     0.0530,
            0.0807], grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0782, 0.0058, 0.1381, 0.1151, 0.1376, 0.4214, 0.1037],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0728, 0.0259, 0.1998, 0.1431, 0.0745, 0.0948, 0.3891],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  29 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  34.551634080045275
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.541]
 [0.447]
 [0.447]
 [0.447]
 [0.447]
 [0.447]] [[32.994]
 [37.313]
 [32.166]
 [32.166]
 [32.166]
 [32.166]
 [32.166]] [[1.821]
 [2.208]
 [1.717]
 [1.717]
 [1.717]
 [1.717]
 [1.717]]
printing an ep nov before normalisation:  43.70408370113968
actor:  1 policy actor:  1  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.22725999999999985 0.683 0.683
probs:  [0.07129239996646886, 0.05400249873734403, 0.22763934989051446, 0.05587737879607128, 0.22289475545618342, 0.36829361715341796]
from probs:  [0.07129239996646886, 0.05400249873734403, 0.22763934989051446, 0.05587737879607128, 0.22289475545618342, 0.36829361715341796]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  41 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2239199999999998 0.683 0.683
probs:  [0.0712495751053633, 0.054004984557401, 0.2276498482555965, 0.05587995113325217, 0.22290503487997337, 0.3683106060684137]
siam score:  -0.7845088
UNIT TEST: sample policy line 217 mcts : [0.    0.061 0.898 0.02  0.    0.    0.02 ]
printing an ep nov before normalisation:  46.654380185209725
maxi score, test score, baseline:  0.2239199999999998 0.683 0.683
probs:  [0.0712495751053633, 0.054004984557401, 0.2276498482555965, 0.05587995113325217, 0.22290503487997337, 0.3683106060684137]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  45.0458768098256
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.467]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]] [[29.48 ]
 [34.705]
 [29.48 ]
 [29.48 ]
 [29.48 ]
 [29.48 ]
 [29.48 ]] [[1.206]
 [1.446]
 [1.206]
 [1.206]
 [1.206]
 [1.206]
 [1.206]]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2239199999999998 0.683 0.683
printing an ep nov before normalisation:  45.948921523444724
maxi score, test score, baseline:  0.2239199999999998 0.683 0.683
probs:  [0.0712495751053633, 0.054004984557401, 0.2276498482555965, 0.05587995113325217, 0.22290503487997337, 0.3683106060684137]
actor:  1 policy actor:  1  step number:  25 total reward:  0.7  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2239199999999998 0.683 0.683
probs:  [0.07047112288158364, 0.053415294695174455, 0.23609683904975395, 0.05526973755272055, 0.2204665349647234, 0.3642804708560441]
Printing some Q and Qe and total Qs values:  [[0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]]
Printing some Q and Qe and total Qs values:  [[0.685]
 [0.674]
 [0.685]
 [0.685]
 [0.685]
 [0.685]
 [0.685]] [[32.292]
 [41.01 ]
 [32.292]
 [32.292]
 [32.292]
 [32.292]
 [32.292]] [[1.675]
 [2.174]
 [1.675]
 [1.675]
 [1.675]
 [1.675]
 [1.675]]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2239199999999998 0.683 0.683
printing an ep nov before normalisation:  38.34376166563118
maxi score, test score, baseline:  0.2239199999999998 0.683 0.683
probs:  [0.07047112288158364, 0.053415294695174455, 0.23609683904975395, 0.05526973755272055, 0.2204665349647234, 0.3642804708560441]
actor:  1 policy actor:  1  step number:  35 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 31.73642284437905
maxi score, test score, baseline:  0.2239199999999998 0.683 0.683
probs:  [0.07047112288158364, 0.053415294695174455, 0.23609683904975395, 0.05526973755272055, 0.2204665349647234, 0.3642804708560441]
Printing some Q and Qe and total Qs values:  [[0.281]
 [0.379]
 [0.291]
 [0.273]
 [0.228]
 [0.291]
 [0.275]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.281]
 [0.379]
 [0.291]
 [0.273]
 [0.228]
 [0.291]
 [0.275]]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  39 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  54.41807099072953
maxi score, test score, baseline:  0.2239199999999998 0.683 0.683
probs:  [0.07047112288158364, 0.053415294695174455, 0.23609683904975395, 0.05526973755272055, 0.2204665349647234, 0.3642804708560441]
maxi score, test score, baseline:  0.2239199999999998 0.683 0.683
siam score:  -0.78942317
actor:  1 policy actor:  1  step number:  33 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  33.43519950146359
printing an ep nov before normalisation:  32.68898248672485
actor:  1 policy actor:  1  step number:  32 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  35 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 34.398311573066486
maxi score, test score, baseline:  0.2239199999999998 0.683 0.683
probs:  [0.07042895288903629, 0.053417713679490636, 0.2361075517022348, 0.055272240727991494, 0.22047653800772618, 0.3642970029935206]
printing an ep nov before normalisation:  27.115654945373535
maxi score, test score, baseline:  0.2239199999999998 0.683 0.683
probs:  [0.07042895288903629, 0.053417713679490636, 0.2361075517022348, 0.055272240727991494, 0.22047653800772618, 0.3642970029935206]
Printing some Q and Qe and total Qs values:  [[0.644]
 [0.641]
 [0.65 ]
 [0.65 ]
 [0.616]
 [0.65 ]
 [0.65 ]] [[33.317]
 [31.59 ]
 [27.015]
 [27.015]
 [29.44 ]
 [27.015]
 [27.015]] [[1.741]
 [1.645]
 [1.41 ]
 [1.41 ]
 [1.505]
 [1.41 ]
 [1.41 ]]
actor:  1 policy actor:  1  step number:  33 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  32 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  35.752819164702196
maxi score, test score, baseline:  0.2239199999999998 0.683 0.683
maxi score, test score, baseline:  0.2239199999999998 0.683 0.683
probs:  [0.07042895288903629, 0.053417713679490636, 0.2361075517022348, 0.055272240727991494, 0.22047653800772618, 0.3642970029935206]
actor:  1 policy actor:  1  step number:  23 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  48.53814377963774
printing an ep nov before normalisation:  29.776628749587008
maxi score, test score, baseline:  0.22409999999999988 0.683 0.683
actor:  1 policy actor:  1  step number:  22 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.775]
 [0.774]
 [0.721]
 [0.684]
 [0.667]
 [0.673]
 [0.721]] [[38.505]
 [38.164]
 [43.451]
 [37.969]
 [40.664]
 [40.671]
 [43.451]] [[1.596]
 [1.583]
 [1.721]
 [1.486]
 [1.566]
 [1.573]
 [1.721]]
Printing some Q and Qe and total Qs values:  [[0.605]
 [0.511]
 [0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.605]
 [0.511]
 [0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]]
printing an ep nov before normalisation:  35.78344724326499
actor:  1 policy actor:  1  step number:  33 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.22409999999999988 0.683 0.683
probs:  [0.07042895288903629, 0.053417713679490636, 0.2361075517022348, 0.055272240727991494, 0.22047653800772618, 0.3642970029935206]
printing an ep nov before normalisation:  33.80019426345825
actor:  1 policy actor:  1  step number:  30 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  31 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.767]
 [0.87 ]
 [0.767]
 [0.767]
 [0.767]
 [0.767]
 [0.767]] [[32.696]
 [32.475]
 [32.696]
 [32.696]
 [32.696]
 [32.696]
 [32.696]] [[0.767]
 [0.87 ]
 [0.767]
 [0.767]
 [0.767]
 [0.767]
 [0.767]]
printing an ep nov before normalisation:  50.72364104661374
actor:  0 policy actor:  1  step number:  28 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  20 total reward:  0.71  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.32 ]
 [0.421]
 [0.32 ]
 [0.32 ]
 [0.32 ]
 [0.32 ]
 [0.32 ]] [[24.929]
 [38.113]
 [24.929]
 [24.929]
 [24.929]
 [24.929]
 [24.929]] [[0.49 ]
 [0.772]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]]
maxi score, test score, baseline:  0.22389999999999985 0.683 0.683
probs:  [0.10320635425107617, 0.05153751368346107, 0.22778094577211067, 0.05332660176557072, 0.21270148908004258, 0.3514470954477389]
maxi score, test score, baseline:  0.22389999999999985 0.683 0.683
probs:  [0.10320635425107617, 0.05153751368346107, 0.22778094577211067, 0.05332660176557072, 0.21270148908004258, 0.3514470954477389]
printing an ep nov before normalisation:  31.650524139404297
maxi score, test score, baseline:  0.22389999999999985 0.683 0.683
probs:  [0.10320635425107617, 0.05153751368346107, 0.22778094577211067, 0.05332660176557072, 0.21270148908004258, 0.3514470954477389]
printing an ep nov before normalisation:  41.24627257052562
maxi score, test score, baseline:  0.22389999999999985 0.683 0.683
probs:  [0.10320635425107617, 0.05153751368346107, 0.22778094577211067, 0.05332660176557072, 0.21270148908004258, 0.3514470954477389]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  1  step number:  31 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.22677999999999982 0.683 0.683
probs:  [0.10320635425107617, 0.05153751368346107, 0.22778094577211067, 0.05332660176557072, 0.21270148908004258, 0.3514470954477389]
maxi score, test score, baseline:  0.22677999999999982 0.683 0.683
probs:  [0.10320635425107617, 0.05153751368346107, 0.22778094577211067, 0.05332660176557072, 0.21270148908004258, 0.3514470954477389]
printing an ep nov before normalisation:  31.27046184811344
line 256 mcts: sample exp_bonus 36.12572904421959
printing an ep nov before normalisation:  43.882575658261004
maxi score, test score, baseline:  0.22677999999999982 0.683 0.683
probs:  [0.10320635425107617, 0.05153751368346107, 0.22778094577211067, 0.05332660176557072, 0.21270148908004258, 0.3514470954477389]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  60.383661037940676
actor:  0 policy actor:  0  step number:  18 total reward:  0.71  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.22723999999999986 0.683 0.683
probs:  [0.10320635425107617, 0.05153751368346107, 0.22778094577211067, 0.05332660176557072, 0.21270148908004258, 0.3514470954477389]
maxi score, test score, baseline:  0.22723999999999986 0.683 0.683
probs:  [0.10320635425107617, 0.05153751368346107, 0.22778094577211067, 0.05332660176557072, 0.21270148908004258, 0.3514470954477389]
actor:  0 policy actor:  1  step number:  32 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.58089810145957
line 256 mcts: sample exp_bonus 36.2947676034559
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  18 total reward:  0.73  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  36 total reward:  0.2899999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.22709999999999986 0.683 0.683
probs:  [0.09550776574491182, 0.04769809925857198, 0.21077781260223752, 0.04935355936916648, 0.2714554933905577, 0.3252072696345546]
printing an ep nov before normalisation:  54.023048556189465
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.53361588660458
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.557383458134584
actor:  1 policy actor:  1  step number:  34 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.85]
 [0.87]
 [0.87]
 [0.85]
 [0.85]
 [0.85]
 [0.85]] [[38.694]
 [23.846]
 [43.129]
 [38.694]
 [38.694]
 [38.694]
 [38.694]] [[1.932]
 [1.537]
 [2.076]
 [1.932]
 [1.932]
 [1.932]
 [1.932]]
maxi score, test score, baseline:  0.22709999999999986 0.683 0.683
probs:  [0.09550776574491182, 0.04769809925857198, 0.21077781260223752, 0.04935355936916648, 0.2714554933905577, 0.3252072696345546]
actor:  1 policy actor:  1  step number:  31 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.22709999999999986 0.683 0.683
probs:  [0.09550776574491182, 0.04769809925857198, 0.21077781260223752, 0.04935355936916648, 0.2714554933905577, 0.3252072696345546]
actor:  1 policy actor:  1  step number:  34 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.328]
 [0.385]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]] [[31.623]
 [41.138]
 [31.623]
 [31.623]
 [31.623]
 [31.623]
 [31.623]] [[0.826]
 [1.183]
 [0.826]
 [0.826]
 [0.826]
 [0.826]
 [0.826]]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.0
line 256 mcts: sample exp_bonus 0.0
using explorer policy with actor:  1
siam score:  -0.79731995
actor:  1 policy actor:  1  step number:  29 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  32 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  20 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.22709999999999986 0.683 0.683
probs:  [0.09550776574491182, 0.04769809925857198, 0.21077781260223752, 0.04935355936916648, 0.2714554933905577, 0.3252072696345546]
using another actor
maxi score, test score, baseline:  0.22377999999999984 0.683 0.683
probs:  [0.09550776574491182, 0.04769809925857198, 0.21077781260223752, 0.04935355936916648, 0.2714554933905577, 0.3252072696345546]
siam score:  -0.79167557
Printing some Q and Qe and total Qs values:  [[0.965]
 [0.87 ]
 [0.965]
 [0.965]
 [0.965]
 [0.965]
 [0.965]] [[51.66 ]
 [47.558]
 [51.66 ]
 [51.66 ]
 [51.66 ]
 [51.66 ]
 [51.66 ]] [[0.965]
 [0.87 ]
 [0.965]
 [0.965]
 [0.965]
 [0.965]
 [0.965]]
siam score:  -0.79172784
actor:  1 policy actor:  1  step number:  35 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.205459439651214
printing an ep nov before normalisation:  36.53434250804078
printing an ep nov before normalisation:  40.51539799022659
maxi score, test score, baseline:  0.22053999999999985 0.683 0.683
probs:  [0.09550776574491182, 0.04769809925857198, 0.21077781260223752, 0.04935355936916648, 0.2714554933905577, 0.3252072696345546]
actor:  1 policy actor:  1  step number:  22 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.667
from probs:  [0.09550776574491182, 0.04769809925857198, 0.21077781260223752, 0.04935355936916648, 0.2714554933905577, 0.3252072696345546]
maxi score, test score, baseline:  0.22053999999999985 0.683 0.683
probs:  [0.09550776574491182, 0.04769809925857198, 0.21077781260223752, 0.04935355936916648, 0.2714554933905577, 0.3252072696345546]
printing an ep nov before normalisation:  38.24317351956575
actor:  1 policy actor:  1  step number:  21 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.667
from probs:  [0.09550776574491182, 0.04769809925857198, 0.21077781260223752, 0.04935355936916648, 0.2714554933905577, 0.3252072696345546]
printing an ep nov before normalisation:  31.24484468362925
actor:  1 policy actor:  1  step number:  38 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  52.25783283915123
maxi score, test score, baseline:  0.22053999999999985 0.683 0.683
probs:  [0.09550776574491182, 0.04769809925857198, 0.21077781260223752, 0.04935355936916648, 0.2714554933905577, 0.3252072696345546]
printing an ep nov before normalisation:  45.443661035676605
actor:  0 policy actor:  0  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.22055999999999987 0.683 0.683
probs:  [0.09550776574491182, 0.04769809925857198, 0.21077781260223752, 0.04935355936916648, 0.2714554933905577, 0.3252072696345546]
actor:  0 policy actor:  1  step number:  30 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.22081999999999985 0.683 0.683
probs:  [0.09550776574491182, 0.04769809925857198, 0.21077781260223752, 0.04935355936916648, 0.2714554933905577, 0.3252072696345546]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
from probs:  [0.09550776574491182, 0.04769809925857198, 0.21077781260223752, 0.04935355936916648, 0.2714554933905577, 0.3252072696345546]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  50.913164245004374
printing an ep nov before normalisation:  40.55134797640277
maxi score, test score, baseline:  0.22081999999999985 0.683 0.683
probs:  [0.09550776574491182, 0.04769809925857198, 0.21077781260223752, 0.04935355936916648, 0.2714554933905577, 0.3252072696345546]
printing an ep nov before normalisation:  38.620008950628325
printing an ep nov before normalisation:  40.66098300962322
actor:  0 policy actor:  1  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.22397999999999985 0.683 0.683
probs:  [0.09550776574491182, 0.04769809925857198, 0.21077781260223752, 0.04935355936916648, 0.2714554933905577, 0.3252072696345546]
Printing some Q and Qe and total Qs values:  [[0.206]
 [0.406]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.206]
 [0.406]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]]
printing an ep nov before normalisation:  41.945121210098584
printing an ep nov before normalisation:  39.73119219177131
actor:  1 policy actor:  1  step number:  32 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  48.101328966294034
maxi score, test score, baseline:  0.22397999999999985 0.683 0.683
probs:  [0.09539334334121005, 0.047704120867198926, 0.2108044797452049, 0.049359790555415534, 0.27148984219494565, 0.32524842329602494]
printing an ep nov before normalisation:  49.10320744036174
printing an ep nov before normalisation:  43.4691242579645
maxi score, test score, baseline:  0.22397999999999985 0.683 0.683
probs:  [0.09539334334121005, 0.047704120867198926, 0.2108044797452049, 0.049359790555415534, 0.27148984219494565, 0.32524842329602494]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.396]
 [0.396]
 [0.396]
 [0.396]
 [0.396]
 [0.396]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.396]
 [0.396]
 [0.396]
 [0.396]
 [0.396]
 [0.396]
 [0.396]]
printing an ep nov before normalisation:  41.36479973442143
maxi score, test score, baseline:  0.22397999999999985 0.683 0.683
probs:  [0.09539334334121005, 0.047704120867198926, 0.2108044797452049, 0.049359790555415534, 0.27148984219494565, 0.32524842329602494]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  48.84313543502608
line 256 mcts: sample exp_bonus 35.81278176685939
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  28 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  50.08930074591003
printing an ep nov before normalisation:  44.25574082270831
actor:  1 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  34 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  23 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  4  action  0 :  tensor([    0.6827,     0.0001,     0.0604,     0.0516,     0.0656,     0.0681,
            0.0715], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0093, 0.9281, 0.0203, 0.0121, 0.0025, 0.0025, 0.0253],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0892, 0.0058, 0.4272, 0.1203, 0.1228, 0.1262, 0.1086],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1174, 0.0058, 0.1377, 0.3501, 0.1260, 0.1205, 0.1424],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1640, 0.0008, 0.0981, 0.0854, 0.4165, 0.1008, 0.1343],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([    0.0227,     0.0000,     0.3303,     0.0917,     0.0431,     0.4209,
            0.0913], grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([    0.0006,     0.1414,     0.0041,     0.4510,     0.0000,     0.0050,
            0.3978], grad_fn=<DivBackward0>)
siam score:  -0.78576815
maxi score, test score, baseline:  0.22397999999999987 0.683 0.683
probs:  [0.09539334334121005, 0.047704120867198926, 0.2108044797452049, 0.049359790555415534, 0.27148984219494565, 0.32524842329602494]
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.439]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.524]
 [0.439]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]]
actor:  0 policy actor:  0  step number:  28 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  45.81732968587069
maxi score, test score, baseline:  0.22345999999999988 0.683 0.683
probs:  [0.09539334334121005, 0.047704120867198926, 0.2108044797452049, 0.049359790555415534, 0.27148984219494565, 0.32524842329602494]
printing an ep nov before normalisation:  27.64299402418695
actor:  1 policy actor:  1  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  31 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.22345999999999988 0.683 0.683
probs:  [0.09539334334121005, 0.047704120867198926, 0.2108044797452049, 0.049359790555415534, 0.27148984219494565, 0.32524842329602494]
printing an ep nov before normalisation:  49.60289809302962
maxi score, test score, baseline:  0.22345999999999988 0.683 0.683
probs:  [0.09539334334121005, 0.047704120867198926, 0.2108044797452049, 0.049359790555415534, 0.27148984219494565, 0.32524842329602494]
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.364]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]] [[25.697]
 [26.713]
 [25.697]
 [25.697]
 [25.697]
 [25.697]
 [25.697]] [[2.189]
 [2.364]
 [2.189]
 [2.189]
 [2.189]
 [2.189]
 [2.189]]
maxi score, test score, baseline:  0.22345999999999988 0.683 0.683
probs:  [0.09539334334121005, 0.047704120867198926, 0.2108044797452049, 0.049359790555415534, 0.27148984219494565, 0.32524842329602494]
printing an ep nov before normalisation:  51.72881812889148
printing an ep nov before normalisation:  38.54891269289266
printing an ep nov before normalisation:  48.80544891147681
printing an ep nov before normalisation:  0.12512232747781127
actor:  1 policy actor:  1  step number:  25 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  48.56603045094095
maxi score, test score, baseline:  0.22345999999999988 0.683 0.683
probs:  [0.09539334334121005, 0.047704120867198926, 0.2108044797452049, 0.049359790555415534, 0.27148984219494565, 0.32524842329602494]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.22345999999999988 0.683 0.683
probs:  [0.09539334334121005, 0.047704120867198926, 0.2108044797452049, 0.049359790555415534, 0.27148984219494565, 0.32524842329602494]
maxi score, test score, baseline:  0.22345999999999988 0.683 0.683
probs:  [0.09539334334121005, 0.047704120867198926, 0.2108044797452049, 0.049359790555415534, 0.27148984219494565, 0.32524842329602494]
actor:  1 policy actor:  1  step number:  32 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.22345999999999988 0.683 0.683
probs:  [0.09539334334121005, 0.047704120867198926, 0.2108044797452049, 0.049359790555415534, 0.27148984219494565, 0.32524842329602494]
maxi score, test score, baseline:  0.22345999999999988 0.683 0.683
probs:  [0.09539334334121005, 0.047704120867198926, 0.2108044797452049, 0.049359790555415534, 0.27148984219494565, 0.32524842329602494]
actor:  1 policy actor:  1  step number:  35 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.22345999999999988 0.683 0.683
actor:  1 policy actor:  1  step number:  26 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.22345999999999988 0.683 0.683
probs:  [0.09539334334121005, 0.047704120867198926, 0.2108044797452049, 0.049359790555415534, 0.27148984219494565, 0.32524842329602494]
printing an ep nov before normalisation:  43.545442028574236
printing an ep nov before normalisation:  34.21999532249647
maxi score, test score, baseline:  0.22345999999999988 0.683 0.683
probs:  [0.09539334334121005, 0.047704120867198926, 0.2108044797452049, 0.049359790555415534, 0.27148984219494565, 0.32524842329602494]
actor:  1 policy actor:  1  step number:  31 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  28.08037757873535
maxi score, test score, baseline:  0.22345999999999988 0.683 0.683
maxi score, test score, baseline:  0.22345999999999988 0.683 0.683
probs:  [0.09539334334121005, 0.047704120867198926, 0.2108044797452049, 0.049359790555415534, 0.27148984219494565, 0.32524842329602494]
printing an ep nov before normalisation:  27.600145316382978
actor:  0 policy actor:  0  step number:  33 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  52.846186907342705
printing an ep nov before normalisation:  46.61747455875758
printing an ep nov before normalisation:  54.92174259922741
actor:  0 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.469]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.469]]
actor:  1 policy actor:  1  step number:  20 total reward:  0.69  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  39 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  49.827301330608705
maxi score, test score, baseline:  0.22321999999999986 0.683 0.683
probs:  [0.09725170337833688, 0.04760632256486075, 0.21037137266833847, 0.04925858845569767, 0.2709319755243244, 0.32458003740844177]
printing an ep nov before normalisation:  52.7109239246687
printing an ep nov before normalisation:  45.728249061975035
actor:  1 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  33 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.265]
 [0.365]
 [0.25 ]
 [0.25 ]
 [0.358]
 [0.286]
 [0.25 ]] [[34.704]
 [37.103]
 [32.816]
 [32.816]
 [36.161]
 [36.347]
 [32.816]] [[0.876]
 [1.06 ]
 [0.795]
 [0.795]
 [1.02 ]
 [0.955]
 [0.795]]
Printing some Q and Qe and total Qs values:  [[0.178]
 [0.198]
 [0.178]
 [0.178]
 [0.178]
 [0.178]
 [0.178]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.178]
 [0.198]
 [0.178]
 [0.178]
 [0.178]
 [0.178]
 [0.178]]
maxi score, test score, baseline:  0.22321999999999986 0.683 0.683
probs:  [0.09725170337833688, 0.04760632256486075, 0.21037137266833847, 0.04925858845569767, 0.2709319755243244, 0.32458003740844177]
maxi score, test score, baseline:  0.22321999999999986 0.683 0.683
probs:  [0.09725170337833688, 0.04760632256486075, 0.21037137266833847, 0.04925858845569767, 0.2709319755243244, 0.32458003740844177]
maxi score, test score, baseline:  0.21731999999999987 0.683 0.683
probs:  [0.09725170337833688, 0.04760632256486075, 0.21037137266833847, 0.04925858845569767, 0.2709319755243244, 0.32458003740844177]
maxi score, test score, baseline:  0.21731999999999987 0.683 0.683
actor:  1 policy actor:  1  step number:  24 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  35.331120842132414
printing an ep nov before normalisation:  0.008468240463344046
actor:  0 policy actor:  1  step number:  31 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.21653999999999984 0.683 0.683
probs:  [0.09725170337833688, 0.04760632256486075, 0.21037137266833847, 0.04925858845569767, 0.2709319755243244, 0.32458003740844177]
printing an ep nov before normalisation:  37.65251874923706
printing an ep nov before normalisation:  37.606801652948484
printing an ep nov before normalisation:  47.712432488266366
siam score:  -0.78568655
maxi score, test score, baseline:  0.21653999999999984 0.683 0.683
actor:  1 policy actor:  1  step number:  41 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  21 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  28 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.21653999999999984 0.683 0.683
probs:  [0.09701636619922434, 0.04761870745053113, 0.21042622005834194, 0.0492714043881287, 0.2710026220976267, 0.3246646798061471]
maxi score, test score, baseline:  0.21325999999999987 0.683 0.683
probs:  [0.09701636619922434, 0.04761870745053113, 0.21042622005834194, 0.0492714043881287, 0.2710026220976267, 0.3246646798061471]
actor:  1 policy actor:  1  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.21325999999999987 0.683 0.683
probs:  [0.09701636619922434, 0.04761870745053113, 0.21042622005834194, 0.0492714043881287, 0.2710026220976267, 0.3246646798061471]
printing an ep nov before normalisation:  40.51081770801402
maxi score, test score, baseline:  0.21325999999999987 0.683 0.683
probs:  [0.09701636619922434, 0.04761870745053113, 0.21042622005834194, 0.0492714043881287, 0.2710026220976267, 0.3246646798061471]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  39 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.21281999999999984 0.683 0.683
actor:  1 policy actor:  1  step number:  24 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.21281999999999984 0.683 0.683
probs:  [0.09701636619922434, 0.04761870745053113, 0.21042622005834194, 0.0492714043881287, 0.2710026220976267, 0.3246646798061471]
using explorer policy with actor:  1
siam score:  -0.7968401
maxi score, test score, baseline:  0.21281999999999984 0.683 0.683
probs:  [0.09701636619922434, 0.04761870745053113, 0.21042622005834194, 0.0492714043881287, 0.2710026220976267, 0.3246646798061471]
maxi score, test score, baseline:  0.21281999999999984 0.683 0.683
probs:  [0.09701636619922434, 0.04761870745053113, 0.21042622005834194, 0.0492714043881287, 0.2710026220976267, 0.3246646798061471]
maxi score, test score, baseline:  0.21281999999999984 0.683 0.683
probs:  [0.09701636619922434, 0.04761870745053113, 0.21042622005834194, 0.0492714043881287, 0.2710026220976267, 0.3246646798061471]
printing an ep nov before normalisation:  0.00015569942206639098
printing an ep nov before normalisation:  24.1017179595934
from probs:  [0.09701636619922434, 0.04761870745053113, 0.21042622005834194, 0.0492714043881287, 0.2710026220976267, 0.3246646798061471]
maxi score, test score, baseline:  0.21281999999999984 0.683 0.683
actor:  1 policy actor:  1  step number:  46 total reward:  0.2699999999999996  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  45.833789165357096
maxi score, test score, baseline:  0.21281999999999984 0.683 0.683
probs:  [0.09701636619922434, 0.04761870745053113, 0.21042622005834194, 0.0492714043881287, 0.2710026220976267, 0.3246646798061471]
maxi score, test score, baseline:  0.21281999999999984 0.683 0.683
probs:  [0.09701636619922434, 0.04761870745053113, 0.21042622005834194, 0.0492714043881287, 0.2710026220976267, 0.3246646798061471]
printing an ep nov before normalisation:  46.19337208335599
maxi score, test score, baseline:  0.21281999999999984 0.683 0.683
probs:  [0.09701636619922434, 0.04761870745053113, 0.21042622005834194, 0.0492714043881287, 0.2710026220976267, 0.3246646798061471]
printing an ep nov before normalisation:  27.676515579223633
printing an ep nov before normalisation:  47.41050982911107
maxi score, test score, baseline:  0.21281999999999984 0.683 0.683
probs:  [0.09701636619922434, 0.04761870745053113, 0.21042622005834194, 0.0492714043881287, 0.2710026220976267, 0.3246646798061471]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20945999999999984 0.683 0.683
probs:  [0.09701636619922434, 0.04761870745053113, 0.21042622005834194, 0.0492714043881287, 0.2710026220976267, 0.3246646798061471]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.20945999999999984 0.683 0.683
maxi score, test score, baseline:  0.20945999999999984 0.683 0.683
probs:  [0.09701636619922434, 0.04761870745053113, 0.21042622005834194, 0.0492714043881287, 0.2710026220976267, 0.3246646798061471]
maxi score, test score, baseline:  0.20609999999999984 0.683 0.683
probs:  [0.09701636619922434, 0.04761870745053113, 0.21042622005834194, 0.0492714043881287, 0.2710026220976267, 0.3246646798061471]
printing an ep nov before normalisation:  26.06948913964997
actor:  0 policy actor:  1  step number:  36 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  37 total reward:  0.2799999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.20535999999999988 0.683 0.683
probs:  [0.09689957613739579, 0.04762485365984936, 0.21045343900476524, 0.049277764511703716, 0.2710376816564055, 0.32470668502988026]
printing an ep nov before normalisation:  16.643389633723668
maxi score, test score, baseline:  0.20199999999999985 0.683 0.683
probs:  [0.09689957613739579, 0.04762485365984936, 0.21045343900476524, 0.049277764511703716, 0.2710376816564055, 0.32470668502988026]
maxi score, test score, baseline:  0.20199999999999985 0.683 0.683
probs:  [0.09689957613739579, 0.04762485365984936, 0.21045343900476524, 0.049277764511703716, 0.2710376816564055, 0.32470668502988026]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  35 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  49.37802561086577
Printing some Q and Qe and total Qs values:  [[0.665]
 [0.786]
 [0.665]
 [0.665]
 [0.665]
 [0.665]
 [0.665]] [[50.182]
 [40.824]
 [50.182]
 [50.182]
 [50.182]
 [50.182]
 [50.182]] [[0.665]
 [0.786]
 [0.665]
 [0.665]
 [0.665]
 [0.665]
 [0.665]]
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.389]
 [0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.342]] [[28.846]
 [33.341]
 [28.846]
 [28.846]
 [28.846]
 [28.846]
 [28.846]] [[0.494]
 [0.586]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]]
maxi score, test score, baseline:  0.19863999999999987 0.683 0.683
probs:  [0.0967833659447996, 0.04763096935289228, 0.21048052280758708, 0.04928409305690721, 0.27107256714250116, 0.32474848169531256]
maxi score, test score, baseline:  0.19863999999999987 0.683 0.683
probs:  [0.0967833659447996, 0.04763096935289228, 0.21048052280758708, 0.04928409305690721, 0.27107256714250116, 0.32474848169531256]
printing an ep nov before normalisation:  30.664741400536293
actor:  1 policy actor:  1  step number:  33 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  26.98907229847118
actor:  1 policy actor:  1  step number:  29 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.014]
 [ 0.236]
 [-0.159]
 [ 0.185]
 [ 0.248]
 [ 0.174]
 [ 0.191]] [[29.342]
 [38.053]
 [30.758]
 [30.318]
 [34.014]
 [34.474]
 [29.529]] [[0.561]
 [1.16 ]
 [0.473]
 [0.799]
 [1.011]
 [0.955]
 [0.773]]
maxi score, test score, baseline:  0.19527999999999984 0.683 0.683
probs:  [0.09666773131350491, 0.04763705475636966, 0.21050747247080848, 0.04929039025833922, 0.27110727984912425, 0.32479007135185345]
maxi score, test score, baseline:  0.19527999999999984 0.683 0.683
maxi score, test score, baseline:  0.19527999999999984 0.683 0.683
printing an ep nov before normalisation:  47.253845154311996
actor:  1 policy actor:  1  step number:  23 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  34 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.19191999999999984 0.683 0.683
probs:  [0.09666773131350491, 0.04763705475636966, 0.21050747247080848, 0.04929039025833922, 0.27110727984912425, 0.32479007135185345]
printing an ep nov before normalisation:  35.31986713409424
maxi score, test score, baseline:  0.19191999999999984 0.683 0.683
probs:  [0.09666773131350491, 0.04763705475636966, 0.21050747247080848, 0.04929039025833922, 0.27110727984912425, 0.32479007135185345]
maxi score, test score, baseline:  0.19191999999999984 0.683 0.683
probs:  [0.09666773131350491, 0.04763705475636966, 0.21050747247080848, 0.04929039025833922, 0.27110727984912425, 0.32479007135185345]
siam score:  -0.78811294
maxi score, test score, baseline:  0.19191999999999984 0.683 0.683
probs:  [0.09666773131350491, 0.04763705475636966, 0.21050747247080848, 0.04929039025833922, 0.27110727984912425, 0.32479007135185345]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.19191999999999984 0.683 0.683
probs:  [0.09666773131350491, 0.04763705475636966, 0.21050747247080848, 0.04929039025833922, 0.27110727984912425, 0.32479007135185345]
maxi score, test score, baseline:  0.19191999999999984 0.683 0.683
probs:  [0.09666773131350491, 0.04763705475636966, 0.21050747247080848, 0.04929039025833922, 0.27110727984912425, 0.32479007135185345]
printing an ep nov before normalisation:  38.689418813952976
actor:  1 policy actor:  1  step number:  32 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.19191999999999984 0.683 0.683
probs:  [0.09666773131350491, 0.04763705475636966, 0.21050747247080848, 0.04929039025833922, 0.27110727984912425, 0.32479007135185345]
actor:  1 policy actor:  1  step number:  34 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.248]
 [0.326]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]] [[31.967]
 [45.557]
 [31.967]
 [31.967]
 [31.967]
 [31.967]
 [31.967]] [[1.004]
 [1.696]
 [1.004]
 [1.004]
 [1.004]
 [1.004]
 [1.004]]
actor:  1 policy actor:  1  step number:  22 total reward:  0.69  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.19191999999999984 0.683 0.683
probs:  [0.0965082635714116, 0.04755858200481773, 0.21015995003712837, 0.04920918632086959, 0.2723102554068428, 0.32425376265893]
Printing some Q and Qe and total Qs values:  [[0.257]
 [0.257]
 [0.257]
 [0.257]
 [0.206]
 [0.257]
 [0.257]] [[ 0.  ]
 [ 0.  ]
 [ 0.  ]
 [ 0.  ]
 [17.22]
 [ 0.  ]
 [ 0.  ]] [[0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.652]
 [0.011]
 [0.011]]
maxi score, test score, baseline:  0.19191999999999984 0.683 0.683
probs:  [0.0965082635714116, 0.04755858200481773, 0.21015995003712837, 0.04920918632086959, 0.2723102554068428, 0.32425376265893]
maxi score, test score, baseline:  0.19191999999999984 0.683 0.683
probs:  [0.0965082635714116, 0.04755858200481773, 0.21015995003712837, 0.04920918632086959, 0.2723102554068428, 0.32425376265893]
maxi score, test score, baseline:  0.19191999999999984 0.683 0.683
probs:  [0.09639337004236675, 0.04756461735252508, 0.2101866780246384, 0.049215431724377944, 0.2723448926382799, 0.324295010217812]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.19191999999999984 0.683 0.683
probs:  [0.09639337004236675, 0.04756461735252508, 0.2101866780246384, 0.049215431724377944, 0.2723448926382799, 0.324295010217812]
printing an ep nov before normalisation:  35.03973181539832
actor:  1 policy actor:  1  step number:  32 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.19191999999999984 0.683 0.683
probs:  [0.09639337004236675, 0.04756461735252508, 0.2101866780246384, 0.049215431724377944, 0.2723448926382799, 0.324295010217812]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  0.667
from probs:  [0.09639337004236675, 0.04756461735252508, 0.2101866780246384, 0.049215431724377944, 0.2723448926382799, 0.324295010217812]
maxi score, test score, baseline:  0.19191999999999984 0.683 0.683
probs:  [0.09639337004236675, 0.04756461735252508, 0.2101866780246384, 0.049215431724377944, 0.2723448926382799, 0.324295010217812]
actions average: 
K:  2  action  0 :  tensor([0.6738, 0.0111, 0.0645, 0.0761, 0.0667, 0.0555, 0.0524],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0025,     0.9484,     0.0084,     0.0103,     0.0008,     0.0010,
            0.0285], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0168, 0.0203, 0.8349, 0.0444, 0.0189, 0.0366, 0.0281],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0977, 0.0271, 0.1399, 0.3284, 0.1281, 0.1149, 0.1640],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2448, 0.0113, 0.0673, 0.0810, 0.4643, 0.0625, 0.0689],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0745, 0.0116, 0.0890, 0.1063, 0.0872, 0.5581, 0.0733],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0895, 0.0267, 0.0620, 0.0649, 0.0584, 0.0512, 0.6472],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.54730798570268
Printing some Q and Qe and total Qs values:  [[-0.055]
 [-0.055]
 [ 0.311]
 [-0.055]
 [-0.055]
 [-0.055]
 [-0.055]] [[37.   ]
 [37.   ]
 [39.971]
 [37.   ]
 [37.   ]
 [37.   ]
 [37.   ]] [[0.882]
 [0.882]
 [1.38 ]
 [0.882]
 [0.882]
 [0.882]
 [0.882]]
actor:  0 policy actor:  0  step number:  21 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.107]
 [0.197]
 [0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.107]
 [0.197]
 [0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]]
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  34 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  45.97459136798525
actor:  1 policy actor:  1  step number:  30 total reward:  0.71  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  28.78450870513916
printing an ep nov before normalisation:  29.90987984108309
actor:  1 policy actor:  1  step number:  33 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  43.968431707877684
actor:  1 policy actor:  1  step number:  37 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  34.80531125912337
Printing some Q and Qe and total Qs values:  [[0.751]
 [0.739]
 [0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]] [[37.268]
 [39.664]
 [37.268]
 [37.268]
 [37.268]
 [37.268]
 [37.268]] [[0.751]
 [0.739]
 [0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]]
maxi score, test score, baseline:  0.18831999999999985 0.683 0.683
probs:  [0.09578322234761409, 0.049241519539105195, 0.2042466589491762, 0.05081501277115245, 0.2634934959517824, 0.33642009044116966]
printing an ep nov before normalisation:  21.101844310760498
actor:  1 policy actor:  1  step number:  34 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  33 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18495999999999985 0.683 0.683
probs:  [0.09578322234761409, 0.049241519539105195, 0.2042466589491762, 0.05081501277115245, 0.2634934959517824, 0.33642009044116966]
actions average: 
K:  4  action  0 :  tensor([0.5708, 0.0714, 0.0516, 0.0562, 0.1425, 0.0555, 0.0521],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0042,     0.9760,     0.0017,     0.0030,     0.0009,     0.0014,
            0.0129], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0455, 0.0172, 0.6784, 0.0351, 0.0358, 0.1398, 0.0481],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1608, 0.0016, 0.1145, 0.3144, 0.1646, 0.1226, 0.1214],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1403, 0.0064, 0.0753, 0.1042, 0.5019, 0.0995, 0.0723],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0972, 0.0060, 0.2025, 0.0944, 0.0870, 0.4339, 0.0789],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0809, 0.1294, 0.0533, 0.1763, 0.0714, 0.0738, 0.4150],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  53.695632171567524
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.752975331374834
Printing some Q and Qe and total Qs values:  [[0.6  ]
 [0.173]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]] [[34.078]
 [ 4.331]
 [34.078]
 [34.078]
 [34.078]
 [34.078]
 [34.078]] [[2.532]
 [0.36 ]
 [2.532]
 [2.532]
 [2.532]
 [2.532]
 [2.532]]
siam score:  -0.7858702
maxi score, test score, baseline:  0.18495999999999985 0.683 0.683
probs:  [0.09578322234761409, 0.049241519539105195, 0.2042466589491762, 0.05081501277115245, 0.2634934959517824, 0.33642009044116966]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
Printing some Q and Qe and total Qs values:  [[0.637]
 [0.713]
 [0.68 ]
 [0.629]
 [0.675]
 [0.681]
 [0.683]] [[18.334]
 [15.128]
 [15.893]
 [18.521]
 [21.119]
 [15.798]
 [15.897]] [[1.976]
 [1.817]
 [1.841]
 [1.981]
 [2.217]
 [1.834]
 [1.844]]
maxi score, test score, baseline:  0.18495999999999985 0.683 0.683
probs:  [0.09578322234761409, 0.049241519539105195, 0.2042466589491762, 0.05081501277115245, 0.2634934959517824, 0.33642009044116966]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.18495999999999985 0.683 0.683
probs:  [0.09578322234761409, 0.049241519539105195, 0.2042466589491762, 0.05081501277115245, 0.2634934959517824, 0.33642009044116966]
actions average: 
K:  3  action  0 :  tensor([0.5536, 0.0027, 0.0736, 0.0778, 0.1036, 0.0865, 0.1022],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0057,     0.9691,     0.0051,     0.0035,     0.0004,     0.0005,
            0.0158], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0438, 0.0039, 0.7390, 0.0492, 0.0349, 0.0800, 0.0493],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1359, 0.0078, 0.1893, 0.2739, 0.1079, 0.1417, 0.1434],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2891, 0.0229, 0.0775, 0.0967, 0.3330, 0.0856, 0.0952],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0999, 0.0234, 0.1958, 0.1243, 0.0770, 0.3817, 0.0978],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1246, 0.1401, 0.1578, 0.1231, 0.1006, 0.1443, 0.2095],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
siam score:  -0.78690153
maxi score, test score, baseline:  0.18463999999999983 0.683 0.683
probs:  [0.09578322234761409, 0.049241519539105195, 0.2042466589491762, 0.05081501277115245, 0.2634934959517824, 0.33642009044116966]
maxi score, test score, baseline:  0.18463999999999983 0.683 0.683
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  30 total reward:  0.71  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  36 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]] [[36.715]
 [36.715]
 [36.715]
 [36.715]
 [36.715]
 [36.715]
 [36.715]] [[1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  52.91356417037591
printing an ep nov before normalisation:  44.87887645315996
actor:  1 policy actor:  1  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  52.12429356291487
actor:  1 policy actor:  1  step number:  33 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  46.5658572077701
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.18463999999999983 0.683 0.683
probs:  [0.09522767374522367, 0.050768364062343925, 0.19883818273280288, 0.0522714552400633, 0.25543416687346887, 0.3474601573460973]
printing an ep nov before normalisation:  38.675091465546785
actor:  0 policy actor:  1  step number:  20 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.18449999999999986 0.683 0.683
printing an ep nov before normalisation:  41.46398795957493
printing an ep nov before normalisation:  42.53996158272561
actor:  1 policy actor:  1  step number:  31 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  30.40064504502763
actor:  0 policy actor:  0  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18427999999999986 0.683 0.683
probs:  [0.09522767374522367, 0.050768364062343925, 0.19883818273280288, 0.0522714552400633, 0.25543416687346887, 0.3474601573460973]
siam score:  -0.79486036
maxi score, test score, baseline:  0.18427999999999986 0.683 0.683
probs:  [0.09522767374522367, 0.050768364062343925, 0.19883818273280288, 0.0522714552400633, 0.25543416687346887, 0.3474601573460973]
maxi score, test score, baseline:  0.18427999999999986 0.683 0.683
probs:  [0.09522767374522367, 0.050768364062343925, 0.19883818273280288, 0.0522714552400633, 0.25543416687346887, 0.3474601573460973]
actor:  0 policy actor:  0  step number:  22 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18417999999999987 0.683 0.683
maxi score, test score, baseline:  0.18417999999999987 0.683 0.683
probs:  [0.09522767374522367, 0.050768364062343925, 0.19883818273280288, 0.0522714552400633, 0.25543416687346887, 0.3474601573460973]
actor:  1 policy actor:  1  step number:  42 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  35 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  25 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.18081999999999984 0.683 0.683
probs:  [0.09512344387049933, 0.050774201533718755, 0.19886109043196568, 0.05227746599529209, 0.2554635992401829, 0.34750019892834133]
printing an ep nov before normalisation:  51.8028334349573
printing an ep nov before normalisation:  58.59961845259487
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.504]
 [0.538]
 [0.504]
 [0.504]
 [0.504]
 [0.45 ]] [[33.138]
 [33.138]
 [38.594]
 [33.138]
 [33.138]
 [33.138]
 [37.059]] [[0.945]
 [0.945]
 [1.117]
 [0.945]
 [0.945]
 [0.945]
 [0.99 ]]
maxi score, test score, baseline:  0.18081999999999984 0.683 0.683
probs:  [0.09512344387049933, 0.050774201533718755, 0.19886109043196568, 0.05227746599529209, 0.2554635992401829, 0.34750019892834133]
maxi score, test score, baseline:  0.18081999999999984 0.683 0.683
probs:  [0.09512344387049933, 0.050774201533718755, 0.19886109043196568, 0.05227746599529209, 0.2554635992401829, 0.34750019892834133]
actor:  1 policy actor:  1  step number:  33 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  3  action  0 :  tensor([0.5965, 0.0372, 0.0655, 0.0733, 0.0749, 0.0583, 0.0942],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0009,     0.9547,     0.0057,     0.0096,     0.0001,     0.0007,
            0.0283], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0256, 0.0033, 0.7969, 0.0283, 0.0258, 0.0824, 0.0377],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1335, 0.0144, 0.1596, 0.1778, 0.1845, 0.2085, 0.1218],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1072, 0.0052, 0.0970, 0.1146, 0.4560, 0.1246, 0.0953],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1431, 0.0024, 0.1296, 0.1382, 0.1454, 0.3061, 0.1352],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1010, 0.0022, 0.0927, 0.1313, 0.1230, 0.1290, 0.4207],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  36 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  18.438188885573602
maxi score, test score, baseline:  0.18081999999999984 0.683 0.683
probs:  [0.09512344387049933, 0.050774201533718755, 0.19886109043196568, 0.05227746599529209, 0.2554635992401829, 0.34750019892834133]
printing an ep nov before normalisation:  0.02040590857177449
printing an ep nov before normalisation:  43.97615547826343
maxi score, test score, baseline:  0.18081999999999984 0.683 0.683
probs:  [0.09512344387049933, 0.050774201533718755, 0.19886109043196568, 0.05227746599529209, 0.2554635992401829, 0.34750019892834133]
printing an ep nov before normalisation:  0.011266881953986285
actor:  1 policy actor:  1  step number:  63 total reward:  0.13999999999999946  reward:  1.0 rdn_beta:  1.667
from probs:  [0.09512344387049933, 0.050774201533718755, 0.19886109043196568, 0.05227746599529209, 0.2554635992401829, 0.34750019892834133]
actions average: 
K:  2  action  0 :  tensor([0.8053, 0.0162, 0.0397, 0.0178, 0.0363, 0.0235, 0.0612],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0008,     0.9713,     0.0013,     0.0106,     0.0013,     0.0006,
            0.0142], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0358, 0.0042, 0.7647, 0.0453, 0.0487, 0.0562, 0.0451],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0726, 0.0666, 0.0403, 0.5654, 0.0979, 0.0521, 0.1052],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1049, 0.0013, 0.0942, 0.1046, 0.5077, 0.1041, 0.0831],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1039, 0.0017, 0.1750, 0.0775, 0.1106, 0.4274, 0.1038],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0547, 0.2851, 0.0526, 0.1704, 0.0620, 0.0627, 0.3125],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.17745999999999984 0.683 0.683
probs:  [0.09512344387049933, 0.050774201533718755, 0.19886109043196568, 0.05227746599529209, 0.2554635992401829, 0.34750019892834133]
actor:  1 policy actor:  1  step number:  43 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  2.0
siam score:  -0.7888107
actor:  1 policy actor:  1  step number:  45 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  31 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  34 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  46.955614121564416
printing an ep nov before normalisation:  20.48122241688841
maxi score, test score, baseline:  0.17745999999999984 0.683 0.683
probs:  [0.09505488790648298, 0.05068169130583636, 0.19923241807401207, 0.052189664293539, 0.2560122173048558, 0.34682912111527375]
printing an ep nov before normalisation:  39.93806761086336
maxi score, test score, baseline:  0.17745999999999984 0.683 0.683
probs:  [0.09505488790648298, 0.05068169130583636, 0.19923241807401207, 0.052189664293539, 0.2560122173048558, 0.34682912111527375]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.79067148146437
actor:  1 policy actor:  1  step number:  33 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17745999999999984 0.683 0.683
probs:  [0.09505488790648298, 0.05068169130583636, 0.19923241807401207, 0.052189664293539, 0.2560122173048558, 0.34682912111527375]
Printing some Q and Qe and total Qs values:  [[0.847]
 [0.896]
 [0.847]
 [0.847]
 [0.847]
 [0.847]
 [0.847]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.847]
 [0.896]
 [0.847]
 [0.847]
 [0.847]
 [0.847]
 [0.847]]
actor:  1 policy actor:  1  step number:  28 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  26.589516758049193
maxi score, test score, baseline:  0.17745999999999984 0.683 0.683
probs:  [0.09505488790648298, 0.05068169130583636, 0.19923241807401207, 0.052189664293539, 0.2560122173048558, 0.34682912111527375]
from probs:  [0.09505488790648298, 0.05068169130583636, 0.19923241807401207, 0.052189664293539, 0.2560122173048558, 0.34682912111527375]
printing an ep nov before normalisation:  48.43613419434307
actor:  0 policy actor:  0  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  45 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17719999999999986 0.683 0.683
probs:  [0.09505488790648298, 0.05068169130583636, 0.19923241807401207, 0.052189664293539, 0.2560122173048558, 0.34682912111527375]
maxi score, test score, baseline:  0.17719999999999986 0.683 0.683
probs:  [0.09505488790648298, 0.05068169130583636, 0.19923241807401207, 0.052189664293539, 0.2560122173048558, 0.34682912111527375]
printing an ep nov before normalisation:  41.471388699187536
printing an ep nov before normalisation:  41.89646401759418
Printing some Q and Qe and total Qs values:  [[0.35 ]
 [0.431]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]] [[42.636]
 [45.606]
 [42.636]
 [42.636]
 [42.636]
 [42.636]
 [42.636]] [[1.619]
 [1.855]
 [1.619]
 [1.619]
 [1.619]
 [1.619]
 [1.619]]
maxi score, test score, baseline:  0.17383999999999988 0.683 0.683
probs:  [0.09505488790648298, 0.05068169130583636, 0.19923241807401207, 0.052189664293539, 0.2560122173048558, 0.34682912111527375]
actor:  0 policy actor:  1  step number:  32 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17339999999999986 0.683 0.683
probs:  [0.09505488790648298, 0.05068169130583636, 0.19923241807401207, 0.052189664293539, 0.2560122173048558, 0.34682912111527375]
Printing some Q and Qe and total Qs values:  [[-0.125]
 [ 0.615]
 [ 0.637]
 [ 0.53 ]
 [ 0.615]
 [ 0.615]
 [ 0.603]] [[53.15 ]
 [52.618]
 [45.446]
 [41.727]
 [52.618]
 [52.618]
 [49.799]] [[1.875]
 [2.586]
 [2.218]
 [1.909]
 [2.586]
 [2.586]
 [2.421]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
line 256 mcts: sample exp_bonus 42.7832582936864
actor:  1 policy actor:  1  step number:  35 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  33.17716538422934
actor:  1 policy actor:  1  step number:  33 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
siam score:  -0.789066
actor:  1 policy actor:  1  step number:  34 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17339999999999986 0.683 0.683
probs:  [0.09505488790648298, 0.05068169130583636, 0.19923241807401207, 0.052189664293539, 0.2560122173048558, 0.34682912111527375]
maxi score, test score, baseline:  0.17339999999999986 0.683 0.683
probs:  [0.09505488790648298, 0.05068169130583636, 0.19923241807401207, 0.052189664293539, 0.2560122173048558, 0.34682912111527375]
maxi score, test score, baseline:  0.17339999999999986 0.683 0.683
probs:  [0.09505488790648298, 0.05068169130583636, 0.19923241807401207, 0.052189664293539, 0.2560122173048558, 0.34682912111527375]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  22.68356742035
actor:  1 policy actor:  1  step number:  38 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.17339999999999986 0.683 0.683
probs:  [0.09505488790648298, 0.05068169130583636, 0.19923241807401207, 0.052189664293539, 0.2560122173048558, 0.34682912111527375]
actions average: 
K:  0  action  0 :  tensor([    0.9508,     0.0001,     0.0037,     0.0173,     0.0259,     0.0003,
            0.0019], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0033,     0.9422,     0.0019,     0.0231,     0.0002,     0.0006,
            0.0287], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0382, 0.0008, 0.7878, 0.0469, 0.0426, 0.0467, 0.0371],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0775, 0.0089, 0.0811, 0.5581, 0.0844, 0.0885, 0.1015],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1615, 0.0010, 0.0640, 0.0740, 0.5642, 0.0671, 0.0682],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0795, 0.0007, 0.1572, 0.0929, 0.0861, 0.5009, 0.0828],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0995, 0.0470, 0.0728, 0.1017, 0.0705, 0.0687, 0.5398],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  32 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17339999999999986 0.683 0.683
probs:  [0.09505488790648298, 0.05068169130583636, 0.19923241807401207, 0.052189664293539, 0.2560122173048558, 0.34682912111527375]
maxi score, test score, baseline:  0.17339999999999986 0.683 0.683
probs:  [0.09505488790648298, 0.05068169130583636, 0.19923241807401207, 0.052189664293539, 0.2560122173048558, 0.34682912111527375]
printing an ep nov before normalisation:  22.797529830028502
actor:  1 policy actor:  1  step number:  30 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.05571254439574
maxi score, test score, baseline:  0.17339999999999986 0.683 0.683
probs:  [0.09505488790648298, 0.05068169130583636, 0.19923241807401207, 0.052189664293539, 0.2560122173048558, 0.34682912111527375]
printing an ep nov before normalisation:  42.814184824538714
actor:  1 policy actor:  1  step number:  20 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  38 total reward:  0.2899999999999996  reward:  1.0 rdn_beta:  1.0
siam score:  -0.7822077
actor:  1 policy actor:  1  step number:  36 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.678]
 [0.728]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]] [[29.11 ]
 [39.786]
 [29.11 ]
 [29.11 ]
 [29.11 ]
 [29.11 ]
 [29.11 ]] [[0.678]
 [0.728]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]]
printing an ep nov before normalisation:  31.320000145822867
maxi score, test score, baseline:  0.17339999999999986 0.683 0.683
probs:  [0.09505488790648298, 0.05068169130583636, 0.19923241807401207, 0.052189664293539, 0.2560122173048558, 0.34682912111527375]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.564]
 [0.635]
 [0.564]
 [0.591]
 [0.564]
 [0.608]
 [0.592]] [[30.123]
 [23.406]
 [30.123]
 [32.729]
 [30.123]
 [24.702]
 [33.251]] [[1.872]
 [1.651]
 [1.872]
 [2.012]
 [1.872]
 [1.68 ]
 [2.036]]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.398]
 [0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]] [[38.076]
 [40.936]
 [38.076]
 [38.076]
 [38.076]
 [38.076]
 [38.076]] [[0.503]
 [0.606]
 [0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.503]]
maxi score, test score, baseline:  0.17339999999999986 0.683 0.683
probs:  [0.09505488790648298, 0.05068169130583636, 0.19923241807401207, 0.052189664293539, 0.2560122173048558, 0.34682912111527375]
maxi score, test score, baseline:  0.17339999999999986 0.683 0.683
actor:  1 policy actor:  1  step number:  36 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  21.51172876950585
printing an ep nov before normalisation:  27.520304284922414
maxi score, test score, baseline:  0.17339999999999986 0.683 0.683
maxi score, test score, baseline:  0.17339999999999986 0.683 0.683
probs:  [0.09495137644695868, 0.050687477512823445, 0.19925520866007862, 0.05219562311593394, 0.2560415073894518, 0.3468688068747535]
printing an ep nov before normalisation:  53.06948271170078
actor:  0 policy actor:  0  step number:  27 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  43 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  28 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  2  action  0 :  tensor([0.4755, 0.0081, 0.0826, 0.1132, 0.1397, 0.1129, 0.0680],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0054, 0.9428, 0.0171, 0.0146, 0.0022, 0.0030, 0.0148],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0222, 0.0052, 0.8831, 0.0243, 0.0198, 0.0279, 0.0174],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0995, 0.0775, 0.0779, 0.4265, 0.1134, 0.1144, 0.0909],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1861, 0.0051, 0.0739, 0.1017, 0.4199, 0.1058, 0.1074],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0739, 0.0112, 0.3388, 0.0152, 0.0484, 0.4950, 0.0175],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0994, 0.1695, 0.0632, 0.0896, 0.0696, 0.0696, 0.4393],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.17647999999999986 0.683 0.683
probs:  [0.09495137644695868, 0.050687477512823445, 0.19925520866007862, 0.05219562311593394, 0.2560415073894518, 0.3468688068747535]
maxi score, test score, baseline:  0.17647999999999986 0.683 0.683
probs:  [0.09495137644695868, 0.050687477512823445, 0.19925520866007862, 0.05219562311593394, 0.2560415073894518, 0.3468688068747535]
maxi score, test score, baseline:  0.17647999999999986 0.683 0.683
probs:  [0.09495137644695868, 0.050687477512823445, 0.19925520866007862, 0.05219562311593394, 0.2560415073894518, 0.3468688068747535]
Printing some Q and Qe and total Qs values:  [[0.27 ]
 [0.222]
 [0.189]
 [0.249]
 [0.241]
 [0.24 ]
 [0.196]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.27 ]
 [0.222]
 [0.189]
 [0.249]
 [0.241]
 [0.24 ]
 [0.196]]
printing an ep nov before normalisation:  43.70049476045875
maxi score, test score, baseline:  0.17647999999999986 0.683 0.683
probs:  [0.09495137644695868, 0.050687477512823445, 0.19925520866007862, 0.05219562311593394, 0.2560415073894518, 0.3468688068747535]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.17647999999999986 0.683 0.683
actor:  1 policy actor:  1  step number:  39 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.0
using another actor
actor:  1 policy actor:  1  step number:  29 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17377999999999982 0.683 0.683
probs:  [0.09495137644695868, 0.050687477512823445, 0.19925520866007862, 0.05219562311593394, 0.2560415073894518, 0.3468688068747535]
actor:  1 policy actor:  1  step number:  26 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
from probs:  [0.09495137644695868, 0.050687477512823445, 0.19925520866007862, 0.05219562311593394, 0.2560415073894518, 0.3468688068747535]
maxi score, test score, baseline:  0.17377999999999982 0.683 0.683
maxi score, test score, baseline:  0.17377999999999982 0.683 0.683
probs:  [0.09495137644695868, 0.050687477512823445, 0.19925520866007862, 0.05219562311593394, 0.2560415073894518, 0.3468688068747535]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  28 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17377999999999982 0.683 0.683
probs:  [0.09495137644695868, 0.050687477512823445, 0.19925520866007862, 0.05219562311593394, 0.2560415073894518, 0.3468688068747535]
printing an ep nov before normalisation:  23.655203410557338
actor:  1 policy actor:  1  step number:  30 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17377999999999982 0.683 0.683
maxi score, test score, baseline:  0.17377999999999982 0.683 0.683
probs:  [0.09495137644695868, 0.050687477512823445, 0.19925520866007862, 0.05219562311593394, 0.2560415073894518, 0.3468688068747535]
Printing some Q and Qe and total Qs values:  [[0.664]
 [0.664]
 [0.646]
 [0.646]
 [0.582]
 [0.646]
 [0.646]] [[44.317]
 [44.393]
 [45.998]
 [45.998]
 [46.51 ]
 [45.998]
 [45.998]] [[1.529]
 [1.531]
 [1.562]
 [1.562]
 [1.514]
 [1.562]
 [1.562]]
actor:  0 policy actor:  0  step number:  33 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17383999999999983 0.683 0.683
probs:  [0.09495137644695868, 0.050687477512823445, 0.19925520866007862, 0.05219562311593394, 0.2560415073894518, 0.3468688068747535]
actor:  1 policy actor:  1  step number:  27 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.0
siam score:  -0.78058153
printing an ep nov before normalisation:  55.15942642332498
maxi score, test score, baseline:  0.17383999999999983 0.683 0.683
probs:  [0.09495137644695868, 0.050687477512823445, 0.19925520866007862, 0.05219562311593394, 0.2560415073894518, 0.3468688068747535]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  21 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17383999999999983 0.683 0.683
maxi score, test score, baseline:  0.17383999999999983 0.683 0.683
probs:  [0.09495137644695868, 0.050687477512823445, 0.19925520866007862, 0.05219562311593394, 0.2560415073894518, 0.3468688068747535]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  24.862310886383057
printing an ep nov before normalisation:  47.152671621293095
printing an ep nov before normalisation:  36.58085346221924
actor:  1 policy actor:  1  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17383999999999983 0.683 0.683
probs:  [0.09495137644695868, 0.050687477512823445, 0.19925520866007862, 0.05219562311593394, 0.2560415073894518, 0.3468688068747535]
Printing some Q and Qe and total Qs values:  [[0.622]
 [0.53 ]
 [0.538]
 [0.499]
 [0.373]
 [0.538]
 [0.48 ]] [[43.915]
 [42.083]
 [51.33 ]
 [48.518]
 [52.184]
 [51.33 ]
 [45.203]] [[0.889]
 [0.783]
 [0.863]
 [0.802]
 [0.704]
 [0.863]
 [0.757]]
maxi score, test score, baseline:  0.17383999999999983 0.683 0.683
probs:  [0.09495137644695868, 0.050687477512823445, 0.19925520866007862, 0.05219562311593394, 0.2560415073894518, 0.3468688068747535]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]] [[41.742]
 [41.742]
 [41.742]
 [41.742]
 [41.742]
 [41.742]
 [41.742]] [[0.823]
 [0.823]
 [0.823]
 [0.823]
 [0.823]
 [0.823]
 [0.823]]
maxi score, test score, baseline:  0.17383999999999983 0.683 0.683
probs:  [0.09495137644695868, 0.050687477512823445, 0.19925520866007862, 0.05219562311593394, 0.2560415073894518, 0.3468688068747535]
actor:  1 policy actor:  1  step number:  40 total reward:  0.22999999999999954  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  25 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  29.872152041988276
actor:  1 policy actor:  1  step number:  32 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.17383999999999983 0.683 0.683
maxi score, test score, baseline:  0.17383999999999983 0.683 0.683
maxi score, test score, baseline:  0.17383999999999983 0.683 0.683
probs:  [0.09484837366219155, 0.050693235285302375, 0.19927788724892395, 0.05220155265555653, 0.25607065353706987, 0.34690829761095576]
maxi score, test score, baseline:  0.17383999999999983 0.683 0.683
actor:  1 policy actor:  1  step number:  32 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  27.733981618682115
actor:  1 policy actor:  1  step number:  32 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  41 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.17383999999999983 0.683 0.683
probs:  [0.09474587581178713, 0.050698964832358286, 0.19930045466408747, 0.05220745312772889, 0.25609965680610913, 0.34694759475792897]
printing an ep nov before normalisation:  52.51285080359207
printing an ep nov before normalisation:  44.561152063141776
Starting evaluation
actor:  0 policy actor:  0  step number:  24 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.956]
 [0.962]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.956]
 [0.962]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.943]
 [0.952]
 [0.943]
 [0.943]
 [0.943]
 [0.943]
 [0.943]] [[50.714]
 [48.339]
 [50.714]
 [50.714]
 [50.714]
 [50.714]
 [50.714]] [[0.943]
 [0.952]
 [0.943]
 [0.943]
 [0.943]
 [0.943]
 [0.943]]
actor:  1 policy actor:  1  step number:  20 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.667
using another actor
from probs:  [0.09474587581178713, 0.050698964832358286, 0.19930045466408747, 0.05220745312772889, 0.25609965680610913, 0.34694759475792897]
maxi score, test score, baseline:  0.17395999999999984 0.683 0.683
probs:  [0.09474587581178713, 0.050698964832358286, 0.19930045466408747, 0.05220745312772889, 0.25609965680610913, 0.34694759475792897]
Printing some Q and Qe and total Qs values:  [[0.328]
 [0.454]
 [0.352]
 [0.312]
 [0.313]
 [0.29 ]
 [0.319]] [[29.2  ]
 [31.726]
 [35.287]
 [34.842]
 [34.985]
 [31.615]
 [33.099]] [[0.461]
 [0.614]
 [0.55 ]
 [0.505]
 [0.507]
 [0.449]
 [0.493]]
printing an ep nov before normalisation:  47.08214282989502
maxi score, test score, baseline:  0.17395999999999984 0.683 0.683
probs:  [0.09474587581178713, 0.050698964832358286, 0.19930045466408747, 0.05220745312772889, 0.25609965680610913, 0.34694759475792897]
printing an ep nov before normalisation:  32.32550832873131
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.72  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.72  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  0.71  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  19 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.19585999999999987 0.683 0.683
probs:  [0.09474587581178713, 0.050698964832358286, 0.19930045466408747, 0.05220745312772889, 0.25609965680610913, 0.34694759475792897]
printing an ep nov before normalisation:  42.516144211521215
actor:  0 policy actor:  0  step number:  21 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.19603999999999988 0.682 0.682
probs:  [0.09687161303814897, 0.05049845562630443, 0.19754878830386605, 0.052376193314126324, 0.2568148840757486, 0.3458900656418056]
maxi score, test score, baseline:  0.19603999999999988 0.682 0.682
probs:  [0.09687161303814897, 0.05049845562630443, 0.19754878830386605, 0.052376193314126324, 0.2568148840757486, 0.3458900656418056]
actor:  0 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.19619999999999985 0.682 0.682
probs:  [0.09687161303814897, 0.05049845562630443, 0.19754878830386605, 0.052376193314126324, 0.2568148840757486, 0.3458900656418056]
printing an ep nov before normalisation:  31.45961819212508
printing an ep nov before normalisation:  49.06034975070914
actor:  1 policy actor:  1  step number:  29 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.19619999999999985 0.682 0.682
maxi score, test score, baseline:  0.19619999999999985 0.682 0.682
probs:  [0.09687161303814897, 0.05049845562630443, 0.19754878830386605, 0.052376193314126324, 0.2568148840757486, 0.3458900656418056]
printing an ep nov before normalisation:  58.95872881197371
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.645]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.56 ]] [[34.84 ]
 [47.431]
 [34.84 ]
 [34.84 ]
 [34.84 ]
 [34.84 ]
 [34.84 ]] [[1.387]
 [1.946]
 [1.387]
 [1.387]
 [1.387]
 [1.387]
 [1.387]]
siam score:  -0.7809838
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  31 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.19903999999999988 0.682 0.682
probs:  [0.09687161303814897, 0.05049845562630443, 0.19754878830386605, 0.052376193314126324, 0.2568148840757486, 0.3458900656418056]
printing an ep nov before normalisation:  0.003682032748599795
actor:  1 policy actor:  1  step number:  35 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.232]
 [0.289]
 [0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]] [[25.014]
 [34.717]
 [25.014]
 [25.014]
 [25.014]
 [25.014]
 [25.014]] [[0.86 ]
 [1.379]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]]
maxi score, test score, baseline:  0.19903999999999988 0.682 0.682
probs:  [0.09687161303814897, 0.05049845562630443, 0.19754878830386605, 0.052376193314126324, 0.2568148840757486, 0.3458900656418056]
printing an ep nov before normalisation:  45.262589622299785
maxi score, test score, baseline:  0.19903999999999988 0.682 0.682
probs:  [0.09687161303814897, 0.05049845562630443, 0.19754878830386605, 0.052376193314126324, 0.2568148840757486, 0.3458900656418056]
line 256 mcts: sample exp_bonus 33.927616542424616
maxi score, test score, baseline:  0.19903999999999988 0.682 0.682
probs:  [0.09687161303814897, 0.05049845562630443, 0.19754878830386605, 0.052376193314126324, 0.2568148840757486, 0.3458900656418056]
siam score:  -0.7780557
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.417]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]] [[45.276]
 [42.518]
 [45.276]
 [45.276]
 [45.276]
 [45.276]
 [45.276]] [[2.037]
 [1.837]
 [2.037]
 [2.037]
 [2.037]
 [2.037]
 [2.037]]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  28.68151200593179
actor:  1 policy actor:  1  step number:  34 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.904]
 [0.966]
 [0.904]
 [0.904]
 [0.904]
 [0.904]
 [0.904]] [[47.385]
 [47.486]
 [47.385]
 [47.385]
 [47.385]
 [47.385]
 [47.385]] [[0.904]
 [0.966]
 [0.904]
 [0.904]
 [0.904]
 [0.904]
 [0.904]]
printing an ep nov before normalisation:  54.28915828628187
maxi score, test score, baseline:  0.19903999999999988 0.682 0.682
probs:  [0.09687161303814897, 0.05049845562630443, 0.19754878830386605, 0.052376193314126324, 0.2568148840757486, 0.3458900656418056]
printing an ep nov before normalisation:  2.5843287403404247e-05
maxi score, test score, baseline:  0.19903999999999988 0.682 0.682
probs:  [0.09687161303814897, 0.05049845562630443, 0.19754878830386605, 0.052376193314126324, 0.2568148840757486, 0.3458900656418056]
actor:  1 policy actor:  1  step number:  35 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  20 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2.0
from probs:  [0.09687161303814897, 0.05049845562630443, 0.19754878830386605, 0.052376193314126324, 0.2568148840757486, 0.3458900656418056]
maxi score, test score, baseline:  0.19965999999999987 0.682 0.682
Printing some Q and Qe and total Qs values:  [[0.745]
 [0.772]
 [0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.745]
 [0.772]
 [0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]]
printing an ep nov before normalisation:  49.79031361748605
printing an ep nov before normalisation:  49.10191354693097
maxi score, test score, baseline:  0.19965999999999987 0.682 0.682
probs:  [0.09687161303814897, 0.05049845562630443, 0.19754878830386605, 0.052376193314126324, 0.2568148840757486, 0.3458900656418056]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  45.60726160474613
siam score:  -0.77675265
actor:  0 policy actor:  0  step number:  19 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  39.424468234659386
maxi score, test score, baseline:  0.19999999999999984 0.682 0.682
probs:  [0.09687161303814897, 0.05049845562630443, 0.19754878830386605, 0.052376193314126324, 0.2568148840757486, 0.3458900656418056]
actions average: 
K:  0  action  0 :  tensor([0.5799, 0.0029, 0.0677, 0.0754, 0.1467, 0.0675, 0.0600],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0034,     0.9652,     0.0037,     0.0072,     0.0009,     0.0008,
            0.0189], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0083,     0.0004,     0.9132,     0.0111,     0.0066,     0.0505,
            0.0098], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1033, 0.0034, 0.1150, 0.4550, 0.1013, 0.1097, 0.1124],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1054, 0.0049, 0.0697, 0.0703, 0.6348, 0.0643, 0.0507],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1055, 0.0039, 0.0930, 0.0982, 0.0958, 0.5431, 0.0604],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1312, 0.0831, 0.1025, 0.1258, 0.1070, 0.0926, 0.3578],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.19999999999999984 0.682 0.682
printing an ep nov before normalisation:  27.829794883728027
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.774477909485576
actor:  1 policy actor:  1  step number:  24 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  31.856577812385
Printing some Q and Qe and total Qs values:  [[0.406]
 [0.356]
 [0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]] [[30.716]
 [33.387]
 [30.716]
 [30.716]
 [30.716]
 [30.716]
 [30.716]] [[1.622]
 [1.698]
 [1.622]
 [1.622]
 [1.622]
 [1.622]
 [1.622]]
maxi score, test score, baseline:  0.19999999999999984 0.682 0.682
probs:  [0.09687161303814897, 0.05049845562630443, 0.19754878830386605, 0.052376193314126324, 0.2568148840757486, 0.3458900656418056]
actor:  1 policy actor:  1  step number:  51 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  46.936135650691256
maxi score, test score, baseline:  0.19999999999999984 0.682 0.682
maxi score, test score, baseline:  0.19999999999999984 0.682 0.682
probs:  [0.09687161303814897, 0.05049845562630443, 0.19754878830386605, 0.052376193314126324, 0.2568148840757486, 0.3458900656418056]
printing an ep nov before normalisation:  46.95931217025679
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  36 total reward:  0.3299999999999996  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  0  action  0 :  tensor([0.6895, 0.0049, 0.0829, 0.0393, 0.1027, 0.0407, 0.0399],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0033,     0.9876,     0.0005,     0.0003,     0.0002,     0.0001,
            0.0081], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0322, 0.0106, 0.8198, 0.0301, 0.0240, 0.0579, 0.0254],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1059, 0.0164, 0.0829, 0.4963, 0.0995, 0.0900, 0.1090],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1497, 0.0012, 0.0572, 0.0670, 0.5943, 0.0682, 0.0624],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0703, 0.0275, 0.1112, 0.0839, 0.0797, 0.5715, 0.0558],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1155, 0.0763, 0.0640, 0.1325, 0.0633, 0.0615, 0.4868],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.19999999999999984 0.682 0.682
probs:  [0.09687161303814897, 0.05049845562630443, 0.19754878830386605, 0.052376193314126324, 0.2568148840757486, 0.3458900656418056]
actor:  1 policy actor:  1  step number:  40 total reward:  0.3299999999999996  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  34 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.644]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]] [[48.93 ]
 [50.821]
 [50.821]
 [50.821]
 [50.821]
 [50.821]
 [50.821]] [[1.518]
 [1.345]
 [1.345]
 [1.345]
 [1.345]
 [1.345]
 [1.345]]
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]] [[48.203]
 [49.358]
 [49.358]
 [49.358]
 [49.358]
 [49.358]
 [49.358]] [[1.46 ]
 [1.344]
 [1.344]
 [1.344]
 [1.344]
 [1.344]
 [1.344]]
Printing some Q and Qe and total Qs values:  [[0.185]
 [0.288]
 [0.213]
 [0.213]
 [0.213]
 [0.213]
 [0.213]] [[50.661]
 [48.742]
 [60.352]
 [60.352]
 [60.352]
 [60.352]
 [60.352]] [[1.754]
 [1.772]
 [2.213]
 [2.213]
 [2.213]
 [2.213]
 [2.213]]
maxi score, test score, baseline:  0.19999999999999984 0.682 0.682
probs:  [0.11198175073342526, 0.04865462973409811, 0.24946636501793953, 0.0512188657792229, 0.3304000651921932, 0.208278323543121]
printing an ep nov before normalisation:  44.39566736251309
maxi score, test score, baseline:  0.19999999999999984 0.682 0.682
probs:  [0.11198175073342526, 0.04865462973409811, 0.24946636501793953, 0.0512188657792229, 0.3304000651921932, 0.208278323543121]
printing an ep nov before normalisation:  0.0005606822014669888
maxi score, test score, baseline:  0.19999999999999984 0.682 0.682
probs:  [0.11198175073342526, 0.04865462973409811, 0.24946636501793953, 0.0512188657792229, 0.3304000651921932, 0.208278323543121]
printing an ep nov before normalisation:  12.914110605871047
actor:  1 policy actor:  1  step number:  36 total reward:  0.3299999999999996  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  34 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.19999999999999984 0.682 0.682
probs:  [0.11198175073342526, 0.04865462973409811, 0.24946636501793953, 0.0512188657792229, 0.3304000651921932, 0.208278323543121]
Printing some Q and Qe and total Qs values:  [[0.842]
 [0.753]
 [0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.819]] [[35.337]
 [35.97 ]
 [38.157]
 [38.157]
 [38.157]
 [38.157]
 [38.157]] [[0.842]
 [0.753]
 [0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.819]]
maxi score, test score, baseline:  0.19999999999999984 0.682 0.682
probs:  [0.11198175073342526, 0.04865462973409811, 0.24946636501793953, 0.0512188657792229, 0.3304000651921932, 0.208278323543121]
actor:  1 policy actor:  1  step number:  35 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.19999999999999984 0.682 0.682
probs:  [0.11198175073342526, 0.04865462973409811, 0.24946636501793953, 0.0512188657792229, 0.3304000651921932, 0.208278323543121]
actor:  0 policy actor:  0  step number:  33 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20029999999999987 0.682 0.682
probs:  [0.11198175073342526, 0.04865462973409811, 0.24946636501793953, 0.0512188657792229, 0.3304000651921932, 0.208278323543121]
maxi score, test score, baseline:  0.20029999999999987 0.682 0.682
probs:  [0.11198175073342526, 0.04865462973409811, 0.24946636501793953, 0.0512188657792229, 0.3304000651921932, 0.208278323543121]
maxi score, test score, baseline:  0.20029999999999987 0.682 0.682
probs:  [0.11198175073342526, 0.04865462973409811, 0.24946636501793953, 0.0512188657792229, 0.3304000651921932, 0.208278323543121]
Printing some Q and Qe and total Qs values:  [[0.624]
 [0.629]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]] [[40.807]
 [43.985]
 [40.807]
 [40.807]
 [40.807]
 [40.807]
 [40.807]] [[1.736]
 [1.916]
 [1.736]
 [1.736]
 [1.736]
 [1.736]
 [1.736]]
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.692]
 [0.663]
 [0.66 ]
 [0.662]
 [0.663]
 [0.663]] [[52.044]
 [46.279]
 [53.161]
 [51.544]
 [51.896]
 [52.559]
 [52.399]] [[2.385]
 [2.106]
 [2.457]
 [2.365]
 [2.386]
 [2.424]
 [2.415]]
maxi score, test score, baseline:  0.20029999999999987 0.682 0.682
actor:  1 policy actor:  1  step number:  47 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.643]
 [0.65 ]
 [0.669]
 [0.658]
 [0.643]
 [0.652]
 [0.643]] [[40.458]
 [41.847]
 [35.986]
 [34.599]
 [40.458]
 [34.417]
 [40.458]] [[2.045]
 [2.149]
 [1.758]
 [1.65 ]
 [2.045]
 [1.632]
 [2.045]]
maxi score, test score, baseline:  0.20029999999999987 0.682 0.682
probs:  [0.11198175073342526, 0.04865462973409811, 0.24946636501793953, 0.0512188657792229, 0.3304000651921932, 0.208278323543121]
maxi score, test score, baseline:  0.20029999999999987 0.682 0.682
probs:  [0.11198175073342526, 0.04865462973409811, 0.24946636501793953, 0.0512188657792229, 0.3304000651921932, 0.208278323543121]
using another actor
from probs:  [0.11198175073342526, 0.04865462973409811, 0.24946636501793953, 0.0512188657792229, 0.3304000651921932, 0.208278323543121]
actor:  1 policy actor:  1  step number:  38 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  18.570697573019853
siam score:  -0.7732166
maxi score, test score, baseline:  0.20029999999999987 0.682 0.682
probs:  [0.11198175073342526, 0.04865462973409811, 0.24946636501793953, 0.0512188657792229, 0.3304000651921932, 0.208278323543121]
maxi score, test score, baseline:  0.20029999999999987 0.682 0.682
probs:  [0.11198175073342526, 0.04865462973409811, 0.24946636501793953, 0.0512188657792229, 0.3304000651921932, 0.208278323543121]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20029999999999987 0.682 0.682
probs:  [0.11198175073342526, 0.04865462973409811, 0.24946636501793953, 0.0512188657792229, 0.3304000651921932, 0.208278323543121]
actor:  0 policy actor:  1  step number:  28 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  48.818011001461834
actor:  1 policy actor:  1  step number:  23 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20025999999999988 0.682 0.682
probs:  [0.11198175073342526, 0.04865462973409811, 0.24946636501793953, 0.0512188657792229, 0.3304000651921932, 0.208278323543121]
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.06765120087751
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  47 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.667
using another actor
actor:  0 policy actor:  1  step number:  24 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[ 0.273]
 [ 0.226]
 [-0.208]
 [ 0.235]
 [ 0.236]
 [ 0.122]
 [-0.065]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.273]
 [ 0.226]
 [-0.208]
 [ 0.235]
 [ 0.236]
 [ 0.122]
 [-0.065]]
printing an ep nov before normalisation:  57.215743060481756
printing an ep nov before normalisation:  27.97465312321652
printing an ep nov before normalisation:  21.080510640413014
using another actor
printing an ep nov before normalisation:  46.53495692275309
actor:  1 policy actor:  1  step number:  44 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  1.333
from probs:  [0.11198175073342526, 0.04865462973409811, 0.24946636501793953, 0.0512188657792229, 0.3304000651921932, 0.208278323543121]
printing an ep nov before normalisation:  42.3451398718018
actor:  1 policy actor:  1  step number:  24 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.19677999999999984 0.682 0.682
probs:  [0.11198175073342526, 0.04865462973409811, 0.24946636501793953, 0.0512188657792229, 0.3304000651921932, 0.208278323543121]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  0 policy actor:  1  step number:  22 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  34 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  30 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  28 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  53.41055319721581
printing an ep nov before normalisation:  51.18412592609888
maxi score, test score, baseline:  0.19951999999999986 0.682 0.682
probs:  [0.11198175073342526, 0.04865462973409811, 0.24946636501793953, 0.0512188657792229, 0.3304000651921932, 0.208278323543121]
printing an ep nov before normalisation:  42.37526639889102
actor:  1 policy actor:  1  step number:  22 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  26 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  49.25784597515218
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19951999999999986 0.682 0.682
probs:  [0.11198175073342526, 0.04865462973409811, 0.24946636501793953, 0.0512188657792229, 0.3304000651921932, 0.208278323543121]
printing an ep nov before normalisation:  57.399087662626016
maxi score, test score, baseline:  0.19951999999999986 0.682 0.682
probs:  [0.11198175073342526, 0.04865462973409811, 0.24946636501793953, 0.0512188657792229, 0.3304000651921932, 0.208278323543121]
printing an ep nov before normalisation:  27.179806789914633
actions average: 
K:  2  action  0 :  tensor([    0.9265,     0.0003,     0.0238,     0.0072,     0.0376,     0.0007,
            0.0038], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0018,     0.9675,     0.0102,     0.0016,     0.0004,     0.0031,
            0.0155], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0291, 0.0579, 0.7331, 0.0371, 0.0436, 0.0679, 0.0313],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1107, 0.0839, 0.1256, 0.2492, 0.1666, 0.1332, 0.1309],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1221, 0.0028, 0.0661, 0.0678, 0.6230, 0.0663, 0.0520],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1187, 0.0006, 0.1225, 0.1001, 0.1043, 0.4493, 0.1045],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0876, 0.1421, 0.0881, 0.0671, 0.0735, 0.0758, 0.4659],
       grad_fn=<DivBackward0>)
siam score:  -0.77456313
actor:  1 policy actor:  1  step number:  32 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.064]
 [0.121]
 [0.094]
 [0.069]
 [0.07 ]
 [0.067]
 [0.106]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.064]
 [0.121]
 [0.094]
 [0.069]
 [0.07 ]
 [0.067]
 [0.106]]
actor:  1 policy actor:  1  step number:  37 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.126]
 [0.215]
 [0.126]
 [0.126]
 [0.126]
 [0.126]
 [0.126]] [[34.517]
 [44.858]
 [34.517]
 [34.517]
 [34.517]
 [34.517]
 [34.517]] [[0.126]
 [0.215]
 [0.126]
 [0.126]
 [0.126]
 [0.126]
 [0.126]]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  31.59281015396118
maxi score, test score, baseline:  0.19951999999999986 0.682 0.682
probs:  [0.11212931826113087, 0.04847562047075977, 0.2503229381571821, 0.051053080233714666, 0.3290965521624975, 0.20892249071471516]
maxi score, test score, baseline:  0.19951999999999986 0.682 0.682
probs:  [0.11212931826113087, 0.04847562047075977, 0.2503229381571821, 0.051053080233714666, 0.3290965521624975, 0.20892249071471516]
printing an ep nov before normalisation:  46.435583586328974
actor:  0 policy actor:  0  step number:  27 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  31 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.59 ]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.578]] [[49.761]
 [44.974]
 [49.761]
 [49.761]
 [49.761]
 [49.761]
 [49.294]] [[1.499]
 [1.397]
 [1.499]
 [1.499]
 [1.499]
 [1.499]
 [1.485]]
printing an ep nov before normalisation:  43.37124479837776
actor:  1 policy actor:  1  step number:  31 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  0.333
from probs:  [0.11212931826113087, 0.04847562047075977, 0.2503229381571821, 0.051053080233714666, 0.3290965521624975, 0.20892249071471516]
printing an ep nov before normalisation:  45.87918933078916
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.20211999999999986 0.682 0.682
probs:  [0.11198511362909824, 0.04848347795651509, 0.2503636035430321, 0.051061356655145794, 0.3291500212699329, 0.20895642694627586]
maxi score, test score, baseline:  0.20211999999999986 0.682 0.682
probs:  [0.11198511362909824, 0.04848347795651509, 0.2503636035430321, 0.051061356655145794, 0.3291500212699329, 0.20895642694627586]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.20211999999999986 0.682 0.682
probs:  [0.11184159633506663, 0.048491297990293475, 0.2504040751010931, 0.0510695936277816, 0.3292032355218278, 0.2089902014239373]
actor:  1 policy actor:  1  step number:  27 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.57149819001725
maxi score, test score, baseline:  0.20211999999999986 0.682 0.682
probs:  [0.11184159633506663, 0.048491297990293475, 0.2504040751010931, 0.0510695936277816, 0.3292032355218278, 0.2089902014239373]
actor:  1 policy actor:  1  step number:  32 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  35 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
using another actor
from probs:  [0.11184159633506663, 0.048491297990293475, 0.2504040751010931, 0.0510695936277816, 0.3292032355218278, 0.2089902014239373]
actor:  0 policy actor:  1  step number:  34 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.087]
 [0.139]
 [0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.087]
 [0.139]
 [0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]]
maxi score, test score, baseline:  0.20199999999999985 0.682 0.682
probs:  [0.11184159633506663, 0.048491297990293475, 0.2504040751010931, 0.0510695936277816, 0.3292032355218278, 0.2089902014239373]
maxi score, test score, baseline:  0.20199999999999985 0.682 0.682
probs:  [0.11184159633506663, 0.048491297990293475, 0.2504040751010931, 0.0510695936277816, 0.3292032355218278, 0.2089902014239373]
maxi score, test score, baseline:  0.20199999999999985 0.682 0.682
probs:  [0.11184159633506663, 0.048491297990293475, 0.2504040751010931, 0.0510695936277816, 0.3292032355218278, 0.2089902014239373]
actor:  0 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.682]
 [0.764]
 [0.68 ]
 [0.688]
 [0.681]
 [0.703]
 [0.683]] [[27.277]
 [27.021]
 [27.423]
 [27.639]
 [27.349]
 [25.388]
 [27.463]] [[1.616]
 [1.689]
 [1.618]
 [1.634]
 [1.617]
 [1.571]
 [1.623]]
maxi score, test score, baseline:  0.20501999999999984 0.682 0.682
probs:  [0.11184159633506663, 0.048491297990293475, 0.2504040751010931, 0.0510695936277816, 0.3292032355218278, 0.2089902014239373]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.333
from probs:  [0.11184159633506663, 0.048491297990293475, 0.2504040751010931, 0.0510695936277816, 0.3292032355218278, 0.2089902014239373]
maxi score, test score, baseline:  0.20501999999999984 0.682 0.682
probs:  [0.11184159633506663, 0.048491297990293475, 0.2504040751010931, 0.0510695936277816, 0.3292032355218278, 0.2089902014239373]
printing an ep nov before normalisation:  43.91719528282833
printing an ep nov before normalisation:  0.0027547161153052002
printing an ep nov before normalisation:  48.74386437290561
printing an ep nov before normalisation:  49.47638457681686
printing an ep nov before normalisation:  39.7875639455168
maxi score, test score, baseline:  0.20501999999999984 0.682 0.682
probs:  [0.11184159633506663, 0.048491297990293475, 0.2504040751010931, 0.0510695936277816, 0.3292032355218278, 0.2089902014239373]
maxi score, test score, baseline:  0.20501999999999984 0.682 0.682
probs:  [0.11184159633506663, 0.048491297990293475, 0.2504040751010931, 0.0510695936277816, 0.3292032355218278, 0.2089902014239373]
actor:  0 policy actor:  1  step number:  27 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  39 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]] [[32.327]
 [32.327]
 [32.327]
 [32.327]
 [32.327]
 [32.327]
 [32.327]] [[1.372]
 [1.372]
 [1.372]
 [1.372]
 [1.372]
 [1.372]
 [1.372]]
printing an ep nov before normalisation:  24.3155680192863
maxi score, test score, baseline:  0.20515999999999984 0.682 0.682
probs:  [0.11169876147652068, 0.048499080839225356, 0.2504443542138633, 0.051077791432995366, 0.32925619673596823, 0.20902381530142705]
actor:  1 policy actor:  1  step number:  30 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20515999999999984 0.682 0.682
probs:  [0.11155660419745828, 0.048506826767906706, 0.2504844422507244, 0.051085950349490955, 0.32930890671289303, 0.20905726972152655]
maxi score, test score, baseline:  0.20515999999999984 0.682 0.682
maxi score, test score, baseline:  0.20515999999999984 0.682 0.682
probs:  [0.11155660419745828, 0.048506826767906706, 0.2504844422507244, 0.051085950349490955, 0.32930890671289303, 0.20905726972152655]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
Printing some Q and Qe and total Qs values:  [[0.912]
 [0.87 ]
 [0.856]
 [0.856]
 [0.856]
 [0.856]
 [0.856]] [[48.773]
 [51.922]
 [55.638]
 [55.638]
 [55.638]
 [55.638]
 [55.638]] [[0.912]
 [0.87 ]
 [0.856]
 [0.856]
 [0.856]
 [0.856]
 [0.856]]
printing an ep nov before normalisation:  49.39038413435331
actor:  0 policy actor:  1  step number:  21 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  29 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  37.39248293835735
printing an ep nov before normalisation:  47.03642746820203
maxi score, test score, baseline:  0.20817999999999987 0.682 0.682
maxi score, test score, baseline:  0.20817999999999987 0.682 0.682
probs:  [0.11141511968783903, 0.048514536038429194, 0.25052434056809725, 0.05109407065333309, 0.32936136723610004, 0.20909056581620153]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  21 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  49.32350361855313
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20853999999999986 0.682 0.682
probs:  [0.11141511968783903, 0.048514536038429194, 0.25052434056809725, 0.05109407065333309, 0.32936136723610004, 0.20909056581620153]
actor:  1 policy actor:  1  step number:  30 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
Printing some Q and Qe and total Qs values:  [[ 0.659]
 [ 0.635]
 [-0.108]
 [-0.024]
 [ 0.592]
 [ 0.193]
 [ 0.604]] [[38.591]
 [39.968]
 [30.046]
 [29.815]
 [30.805]
 [30.266]
 [28.837]] [[1.961]
 [2.017]
 [0.699]
 [0.77 ]
 [1.443]
 [1.013]
 [1.34 ]]
printing an ep nov before normalisation:  51.23117174003664
printing an ep nov before normalisation:  53.01363535682597
Printing some Q and Qe and total Qs values:  [[0.914]
 [0.888]
 [0.828]
 [0.478]
 [0.828]
 [0.828]
 [0.828]] [[47.19 ]
 [46.626]
 [37.922]
 [43.496]
 [37.922]
 [37.922]
 [37.922]] [[0.914]
 [0.888]
 [0.828]
 [0.478]
 [0.828]
 [0.828]
 [0.828]]
siam score:  -0.77989554
actor:  1 policy actor:  1  step number:  35 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  0
maxi score, test score, baseline:  0.20853999999999986 0.682 0.682
probs:  [0.11141511968783903, 0.048514536038429194, 0.25052434056809725, 0.05109407065333309, 0.32936136723610004, 0.20909056581620153]
actor:  0 policy actor:  1  step number:  32 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20815999999999985 0.682 0.682
maxi score, test score, baseline:  0.20815999999999985 0.682 0.682
probs:  [0.11141511968783903, 0.048514536038429194, 0.25052434056809725, 0.05109407065333309, 0.32936136723610004, 0.20909056581620153]
Printing some Q and Qe and total Qs values:  [[0.774]
 [0.774]
 [0.774]
 [0.774]
 [0.691]
 [0.774]
 [0.774]] [[56.139]
 [56.139]
 [56.139]
 [56.139]
 [56.018]
 [56.139]
 [56.139]] [[2.521]
 [2.521]
 [2.521]
 [2.521]
 [2.433]
 [2.521]
 [2.521]]
siam score:  -0.7780732
actor:  1 policy actor:  1  step number:  33 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.20815999999999985 0.682 0.682
probs:  [0.11127430318304408, 0.04852220891040937, 0.25056405050959385, 0.05110215261798073, 0.32941358007224397, 0.209123704706728]
actor:  1 policy actor:  1  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.403]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]] [[38.609]
 [50.342]
 [38.609]
 [38.609]
 [38.609]
 [38.609]
 [38.609]] [[0.938]
 [1.146]
 [0.938]
 [0.938]
 [0.938]
 [0.938]
 [0.938]]
maxi score, test score, baseline:  0.20815999999999985 0.682 0.682
probs:  [0.11127430318304408, 0.04852220891040937, 0.25056405050959385, 0.05110215261798073, 0.32941358007224397, 0.209123704706728]
printing an ep nov before normalisation:  61.24943840987213
printing an ep nov before normalisation:  59.56432926134346
actor:  0 policy actor:  0  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.20803999999999984 0.682 0.682
probs:  [0.11127430318304408, 0.04852220891040937, 0.25056405050959385, 0.05110215261798073, 0.32941358007224397, 0.209123704706728]
actor:  1 policy actor:  1  step number:  28 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20803999999999984 0.682 0.682
probs:  [0.11127430318304408, 0.04852220891040937, 0.25056405050959385, 0.05110215261798073, 0.32941358007224397, 0.209123704706728]
printing an ep nov before normalisation:  64.68110768510631
line 256 mcts: sample exp_bonus 60.2217144881477
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  24 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  61.69779026345863
actor:  1 policy actor:  1  step number:  25 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  19 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  35 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  56.899373555082704
maxi score, test score, baseline:  0.20475999999999983 0.682 0.682
probs:  [0.11127430318304408, 0.04852220891040937, 0.25056405050959385, 0.05110215261798073, 0.32941358007224397, 0.209123704706728]
siam score:  -0.7789999
maxi score, test score, baseline:  0.20475999999999983 0.682 0.682
probs:  [0.11127430318304408, 0.04852220891040937, 0.25056405050959385, 0.05110215261798073, 0.32941358007224397, 0.209123704706728]
Printing some Q and Qe and total Qs values:  [[0.209]
 [0.401]
 [0.209]
 [0.209]
 [0.209]
 [0.209]
 [0.209]] [[23.151]
 [27.999]
 [23.151]
 [23.151]
 [23.151]
 [23.151]
 [23.151]] [[0.699]
 [1.087]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]]
maxi score, test score, baseline:  0.20475999999999983 0.682 0.682
probs:  [0.11127430318304408, 0.04852220891040937, 0.25056405050959385, 0.05110215261798073, 0.32941358007224397, 0.209123704706728]
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.594]
 [0.558]
 [0.358]
 [0.558]
 [0.558]
 [0.558]] [[42.776]
 [50.23 ]
 [42.776]
 [36.585]
 [42.776]
 [42.776]
 [42.776]] [[0.919]
 [1.06 ]
 [0.919]
 [0.63 ]
 [0.919]
 [0.919]
 [0.919]]
printing an ep nov before normalisation:  52.925485885999066
maxi score, test score, baseline:  0.20475999999999983 0.682 0.682
probs:  [0.11127430318304408, 0.04852220891040937, 0.25056405050959385, 0.05110215261798073, 0.32941358007224397, 0.209123704706728]
maxi score, test score, baseline:  0.20475999999999983 0.682 0.682
probs:  [0.11127430318304408, 0.04852220891040937, 0.25056405050959385, 0.05110215261798073, 0.32941358007224397, 0.209123704706728]
maxi score, test score, baseline:  0.20475999999999983 0.682 0.682
probs:  [0.11127430318304408, 0.04852220891040937, 0.25056405050959385, 0.05110215261798073, 0.32941358007224397, 0.209123704706728]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  23 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  20 total reward:  0.75  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.237]
 [0.237]
 [0.237]
 [0.237]
 [0.237]
 [0.237]
 [0.237]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.237]
 [0.237]
 [0.237]
 [0.237]
 [0.237]
 [0.237]
 [0.237]]
Printing some Q and Qe and total Qs values:  [[0.183]
 [0.185]
 [0.183]
 [0.182]
 [0.206]
 [0.191]
 [0.182]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.183]
 [0.185]
 [0.183]
 [0.182]
 [0.206]
 [0.191]
 [0.182]]
maxi score, test score, baseline:  0.20513999999999985 0.682 0.682
probs:  [0.11113414996333945, 0.048529845641018324, 0.25060357340616873, 0.05111019651431602, 0.3294655469713388, 0.20915668750381874]
actor:  1 policy actor:  1  step number:  35 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.0
siam score:  -0.7754456
printing an ep nov before normalisation:  32.04050372791289
actor:  1 policy actor:  1  step number:  25 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20513999999999985 0.682 0.682
probs:  [0.11113414996333945, 0.048529845641018324, 0.25060357340616873, 0.05111019651431602, 0.3294655469713388, 0.20915668750381874]
printing an ep nov before normalisation:  50.38962573544896
Printing some Q and Qe and total Qs values:  [[0.707]
 [0.77 ]
 [0.739]
 [0.73 ]
 [0.731]
 [0.708]
 [0.731]] [[20.91 ]
 [26.85 ]
 [29.054]
 [26.326]
 [26.454]
 [21.651]
 [25.94 ]] [[0.889]
 [1.003]
 [0.991]
 [0.958]
 [0.961]
 [0.896]
 [0.956]]
actor:  1 policy actor:  1  step number:  44 total reward:  0.16999999999999948  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  0.00015617413509971811
actor:  1 policy actor:  1  step number:  40 total reward:  0.2899999999999996  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  31 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.196]
 [0.196]
 [0.316]
 [0.196]
 [0.196]
 [0.196]
 [0.196]] [[39.788]
 [39.788]
 [35.635]
 [39.788]
 [39.788]
 [39.788]
 [39.788]] [[2.111]
 [2.111]
 [1.996]
 [2.111]
 [2.111]
 [2.111]
 [2.111]]
printing an ep nov before normalisation:  54.255827119932235
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.351]
 [0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.401]] [[37.219]
 [40.45 ]
 [37.219]
 [37.219]
 [37.219]
 [37.219]
 [37.219]] [[1.748]
 [1.891]
 [1.748]
 [1.748]
 [1.748]
 [1.748]
 [1.748]]
printing an ep nov before normalisation:  46.6315901486159
printing an ep nov before normalisation:  48.950230568972984
maxi score, test score, baseline:  0.20229999999999984 0.682 0.682
probs:  [0.11099465535334811, 0.04853744648500993, 0.25064291057626736, 0.05111820261067641, 0.32951726966694933, 0.20918951530774896]
printing an ep nov before normalisation:  47.25484656970005
actor:  1 policy actor:  1  step number:  24 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.20229999999999984 0.682 0.682
printing an ep nov before normalisation:  32.74374374018933
actor:  1 policy actor:  1  step number:  37 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  27 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.20229999999999984 0.682 0.682
probs:  [0.11099465535334811, 0.04853744648500993, 0.25064291057626736, 0.05111820261067641, 0.32951726966694933, 0.20918951530774896]
maxi score, test score, baseline:  0.20229999999999984 0.682 0.682
actor:  1 policy actor:  1  step number:  22 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
siam score:  -0.7713382
Printing some Q and Qe and total Qs values:  [[0.64 ]
 [0.659]
 [0.64 ]
 [0.648]
 [0.652]
 [0.64 ]
 [0.64 ]] [[47.232]
 [42.663]
 [47.232]
 [46.267]
 [45.99 ]
 [47.232]
 [47.232]] [[2.171]
 [1.934]
 [2.171]
 [2.125]
 [2.114]
 [2.171]
 [2.171]]
actor:  1 policy actor:  1  step number:  24 total reward:  0.73  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20229999999999984 0.682 0.682
probs:  [0.10666394966829121, 0.05399694529520204, 0.2244222581451238, 0.05617316636910369, 0.36927697337670107, 0.18946670714557828]
maxi score, test score, baseline:  0.20229999999999984 0.682 0.682
probs:  [0.10666394966829121, 0.05399694529520204, 0.2244222581451238, 0.05617316636910369, 0.36927697337670107, 0.18946670714557828]
maxi score, test score, baseline:  0.20229999999999984 0.682 0.682
probs:  [0.10666394966829121, 0.05399694529520204, 0.2244222581451238, 0.05617316636910369, 0.36927697337670107, 0.18946670714557828]
actor:  1 policy actor:  1  step number:  35 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  29 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.228]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.228]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20229999999999984 0.682 0.682
probs:  [0.10654630451135362, 0.05400404400605097, 0.22445181724396632, 0.05618055188458758, 0.36932562290907367, 0.18949165944496782]
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.515]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]] [[44.464]
 [49.364]
 [44.464]
 [44.464]
 [44.464]
 [44.464]
 [44.464]] [[1.978]
 [2.182]
 [1.978]
 [1.978]
 [1.978]
 [1.978]
 [1.978]]
printing an ep nov before normalisation:  63.18755174266199
maxi score, test score, baseline:  0.20229999999999984 0.682 0.682
probs:  [0.10654630451135362, 0.05400404400605097, 0.22445181724396632, 0.05618055188458758, 0.36932562290907367, 0.18949165944496782]
actor:  1 policy actor:  1  step number:  21 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.20229999999999984 0.682 0.682
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  30 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.171]
 [0.344]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.171]
 [0.344]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20229999999999984 0.682 0.682
probs:  [0.10654630451135362, 0.05400404400605097, 0.22445181724396632, 0.05618055188458758, 0.36932562290907367, 0.18949165944496782]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  25 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.20201999999999984 0.682 0.682
probs:  [0.10654630451135362, 0.05400404400605097, 0.22445181724396632, 0.05618055188458758, 0.36932562290907367, 0.18949165944496782]
maxi score, test score, baseline:  0.20201999999999984 0.682 0.682
probs:  [0.10654630451135362, 0.05400404400605097, 0.22445181724396632, 0.05618055188458758, 0.36932562290907367, 0.18949165944496782]
actor:  1 policy actor:  1  step number:  35 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.368]
 [0.466]
 [0.466]
 [0.4  ]
 [0.466]
 [0.466]] [[27.762]
 [28.864]
 [26.589]
 [26.589]
 [28.087]
 [26.589]
 [26.589]] [[1.422]
 [1.453]
 [1.399]
 [1.399]
 [1.433]
 [1.399]
 [1.399]]
maxi score, test score, baseline:  0.20201999999999984 0.682 0.682
probs:  [0.10654630451135362, 0.05400404400605097, 0.22445181724396632, 0.05618055188458758, 0.36932562290907367, 0.18949165944496782]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.268]
 [0.268]
 [0.268]
 [0.268]
 [0.268]
 [0.268]] [[24.292]
 [24.292]
 [24.292]
 [24.292]
 [24.292]
 [24.292]
 [24.292]] [[0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]]
maxi score, test score, baseline:  0.20201999999999984 0.682 0.682
probs:  [0.10654630451135362, 0.05400404400605097, 0.22445181724396632, 0.05618055188458758, 0.36932562290907367, 0.18949165944496782]
maxi score, test score, baseline:  0.20201999999999984 0.682 0.682
probs:  [0.10654630451135362, 0.05400404400605097, 0.22445181724396632, 0.05618055188458758, 0.36932562290907367, 0.18949165944496782]
actor:  1 policy actor:  1  step number:  30 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
using another actor
actor:  1 policy actor:  1  step number:  33 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.186]
 [0.363]
 [0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.186]
 [0.363]
 [0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]]
actor:  1 policy actor:  1  step number:  36 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20201999999999984 0.682 0.682
printing an ep nov before normalisation:  29.243911449712726
actor:  0 policy actor:  1  step number:  23 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  33.84829382056261
maxi score, test score, baseline:  0.20521999999999982 0.682 0.682
probs:  [0.10654630451135362, 0.05400404400605097, 0.22445181724396632, 0.05618055188458758, 0.36932562290907367, 0.18949165944496782]
actor:  1 policy actor:  1  step number:  35 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20521999999999982 0.682 0.682
probs:  [0.10654630451135362, 0.05400404400605097, 0.22445181724396632, 0.05618055188458758, 0.36932562290907367, 0.18949165944496782]
siam score:  -0.7771625
maxi score, test score, baseline:  0.20521999999999982 0.682 0.682
probs:  [0.10654630451135362, 0.05400404400605097, 0.22445181724396632, 0.05618055188458758, 0.36932562290907367, 0.18949165944496782]
actor:  1 policy actor:  1  step number:  36 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  26 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20521999999999982 0.682 0.682
probs:  [0.10654630451135362, 0.05400404400605097, 0.22445181724396632, 0.05618055188458758, 0.36932562290907367, 0.18949165944496782]
maxi score, test score, baseline:  0.20521999999999982 0.682 0.682
probs:  [0.10654630451135362, 0.05400404400605097, 0.22445181724396632, 0.05618055188458758, 0.36932562290907367, 0.18949165944496782]
actor:  1 policy actor:  1  step number:  19 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  50.3285975164429
printing an ep nov before normalisation:  46.672841389773865
maxi score, test score, baseline:  0.20521999999999982 0.682 0.682
maxi score, test score, baseline:  0.20521999999999982 0.682 0.682
actor:  0 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
from probs:  [0.10654630451135362, 0.05400404400605097, 0.22445181724396632, 0.05618055188458758, 0.36932562290907367, 0.18949165944496782]
maxi score, test score, baseline:  0.20831999999999984 0.682 0.682
probs:  [0.10654630451135362, 0.05400404400605097, 0.22445181724396632, 0.05618055188458758, 0.36932562290907367, 0.18949165944496782]
printing an ep nov before normalisation:  45.59695200120595
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.63913694149375
siam score:  -0.78197664
printing an ep nov before normalisation:  45.83944946252517
printing an ep nov before normalisation:  47.443440680084024
maxi score, test score, baseline:  0.20831999999999984 0.682 0.682
probs:  [0.10654630451135362, 0.05400404400605097, 0.22445181724396632, 0.05618055188458758, 0.36932562290907367, 0.18949165944496782]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.164]
 [0.164]
 [0.164]
 [0.164]
 [0.164]
 [0.164]
 [0.164]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.164]
 [0.164]
 [0.164]
 [0.164]
 [0.164]
 [0.164]
 [0.164]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.20831999999999984 0.682 0.682
probs:  [0.10654630451135362, 0.05400404400605097, 0.22445181724396632, 0.05618055188458758, 0.36932562290907367, 0.18949165944496782]
maxi score, test score, baseline:  0.20531999999999984 0.682 0.682
probs:  [0.10654630451135362, 0.05400404400605097, 0.22445181724396632, 0.05618055188458758, 0.36932562290907367, 0.18949165944496782]
actor:  0 policy actor:  0  step number:  34 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  32.69267559051514
siam score:  -0.78519285
siam score:  -0.78550047
using another actor
printing an ep nov before normalisation:  32.29721716013818
maxi score, test score, baseline:  0.20519999999999985 0.682 0.682
probs:  [0.10642921533190872, 0.05401110916920815, 0.22448123664990427, 0.05618790249697427, 0.3693740425293467, 0.18951649382265795]
actor:  1 policy actor:  1  step number:  34 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20205999999999985 0.682 0.682
probs:  [0.10642921533190872, 0.05401110916920815, 0.22448123664990427, 0.05618790249697427, 0.3693740425293467, 0.18951649382265795]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.642]
 [0.599]
 [0.599]
 [0.613]
 [0.599]
 [0.631]] [[45.95 ]
 [46.629]
 [57.371]
 [57.371]
 [48.957]
 [57.371]
 [48.049]] [[1.872]
 [1.845]
 [2.207]
 [2.207]
 [1.903]
 [2.207]
 [1.888]]
siam score:  -0.78559065
maxi score, test score, baseline:  0.20205999999999985 0.682 0.682
probs:  [0.1063126781980184, 0.05401814102192678, 0.2245105173508625, 0.05619521845310233, 0.3694222338634853, 0.18954121111260458]
printing an ep nov before normalisation:  0.011778709165355394
actor:  1 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.20205999999999985 0.682 0.682
probs:  [0.1063126781980184, 0.05401814102192678, 0.2245105173508625, 0.05619521845310233, 0.3694222338634853, 0.18954121111260458]
line 256 mcts: sample exp_bonus 32.78266513347626
printing an ep nov before normalisation:  37.36186629491982
Printing some Q and Qe and total Qs values:  [[-0.112]
 [-0.093]
 [-0.093]
 [-0.105]
 [-0.102]
 [-0.108]
 [-0.093]] [[40.866]
 [39.237]
 [39.237]
 [35.935]
 [37.719]
 [38.59 ]
 [39.237]] [[1.555]
 [1.508]
 [1.508]
 [1.361]
 [1.437]
 [1.466]
 [1.508]]
maxi score, test score, baseline:  0.20205999999999985 0.682 0.682
probs:  [0.1063126781980184, 0.05401814102192678, 0.2245105173508625, 0.05619521845310233, 0.3694222338634853, 0.18954121111260458]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20205999999999985 0.682 0.682
probs:  [0.1063126781980184, 0.05401814102192678, 0.2245105173508625, 0.05619521845310233, 0.3694222338634853, 0.18954121111260458]
maxi score, test score, baseline:  0.20205999999999985 0.682 0.682
maxi score, test score, baseline:  0.20205999999999985 0.682 0.682
probs:  [0.1063126781980184, 0.05401814102192678, 0.2245105173508625, 0.05619521845310233, 0.3694222338634853, 0.18954121111260458]
printing an ep nov before normalisation:  37.870023748513006
actor:  1 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.19951999999999984 0.682 0.682
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19951999999999984 0.682 0.682
probs:  [0.1063126781980184, 0.05401814102192678, 0.2245105173508625, 0.05619521845310233, 0.3694222338634853, 0.18954121111260458]
printing an ep nov before normalisation:  23.15637647519888
maxi score, test score, baseline:  0.19951999999999984 0.682 0.682
maxi score, test score, baseline:  0.19951999999999984 0.682 0.682
probs:  [0.1063126781980184, 0.05401814102192678, 0.2245105173508625, 0.05619521845310233, 0.3694222338634853, 0.18954121111260458]
actor:  1 policy actor:  1  step number:  29 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.19951999999999984 0.682 0.682
probs:  [0.1063126781980184, 0.05401814102192678, 0.2245105173508625, 0.05619521845310233, 0.3694222338634853, 0.18954121111260458]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19951999999999984 0.682 0.682
probs:  [0.1063126781980184, 0.05401814102192678, 0.2245105173508625, 0.05619521845310233, 0.3694222338634853, 0.18954121111260458]
Printing some Q and Qe and total Qs values:  [[0.816]
 [0.817]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]] [[40.08 ]
 [43.407]
 [38.958]
 [38.958]
 [38.958]
 [38.958]
 [38.958]] [[1.683]
 [1.817]
 [1.619]
 [1.619]
 [1.619]
 [1.619]
 [1.619]]
actor:  1 policy actor:  1  step number:  43 total reward:  0.21999999999999964  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.19951999999999984 0.682 0.682
probs:  [0.1063126781980184, 0.05401814102192678, 0.2245105173508625, 0.05619521845310233, 0.3694222338634853, 0.18954121111260458]
printing an ep nov before normalisation:  24.26011309544685
actor:  1 policy actor:  1  step number:  46 total reward:  0.1899999999999995  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19951999999999984 0.682 0.682
probs:  [0.1063126781980184, 0.05401814102192678, 0.2245105173508625, 0.05619521845310233, 0.3694222338634853, 0.18954121111260458]
from probs:  [0.1063126781980184, 0.05401814102192678, 0.2245105173508625, 0.05619521845310233, 0.3694222338634853, 0.18954121111260458]
maxi score, test score, baseline:  0.19649999999999984 0.682 0.682
probs:  [0.1063126781980184, 0.05401814102192678, 0.2245105173508625, 0.05619521845310233, 0.3694222338634853, 0.18954121111260458]
actor:  1 policy actor:  1  step number:  31 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  43.712442280752526
actor:  1 policy actor:  1  step number:  38 total reward:  0.14999999999999947  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.409]
 [0.422]
 [0.391]
 [0.4  ]
 [0.407]
 [0.426]] [[30.283]
 [28.335]
 [29.126]
 [28.982]
 [29.88 ]
 [29.857]
 [28.025]] [[1.226]
 [1.124]
 [1.181]
 [1.142]
 [1.2  ]
 [1.206]
 [1.124]]
actor:  0 policy actor:  0  step number:  29 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  2  action  0 :  tensor([0.6706, 0.0118, 0.0679, 0.0543, 0.0861, 0.0579, 0.0513],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0027, 0.9217, 0.0067, 0.0032, 0.0185, 0.0037, 0.0435],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0476, 0.0011, 0.6389, 0.0559, 0.0941, 0.0754, 0.0869],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1303, 0.0484, 0.1648, 0.3402, 0.0923, 0.0800, 0.1439],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0810, 0.0015, 0.0883, 0.1246, 0.5187, 0.1082, 0.0777],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1001, 0.0015, 0.1235, 0.0885, 0.0971, 0.4592, 0.1302],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1136, 0.0511, 0.1234, 0.1051, 0.1074, 0.0732, 0.4262],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  22 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.19667999999999985 0.682 0.682
probs:  [0.10619668921473419, 0.054025139799228176, 0.2245396603254718, 0.05620249999748825, 0.3694701985221592, 0.18956581214091853]
actor:  1 policy actor:  1  step number:  27 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.19667999999999985 0.682 0.682
probs:  [0.10619668921473419, 0.054025139799228176, 0.2245396603254718, 0.05620249999748825, 0.3694701985221592, 0.18956581214091853]
Printing some Q and Qe and total Qs values:  [[0.62 ]
 [0.621]
 [0.62 ]
 [0.62 ]
 [0.654]
 [0.62 ]
 [0.62 ]] [[35.614]
 [41.389]
 [35.614]
 [35.614]
 [42.413]
 [35.614]
 [35.614]] [[1.755]
 [2.107]
 [1.755]
 [1.755]
 [2.202]
 [1.755]
 [1.755]]
actor:  1 policy actor:  1  step number:  42 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.19667999999999985 0.682 0.682
actor:  1 policy actor:  1  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  26.76337380457639
actor:  1 policy actor:  1  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.19667999999999985 0.682 0.682
maxi score, test score, baseline:  0.19667999999999985 0.682 0.682
probs:  [0.10619668921473419, 0.054025139799228176, 0.2245396603254718, 0.05620249999748825, 0.3694701985221592, 0.18956581214091853]
actor:  0 policy actor:  1  step number:  34 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.19643999999999984 0.682 0.682
maxi score, test score, baseline:  0.19643999999999984 0.682 0.682
probs:  [0.10608124452366238, 0.05403210573392784, 0.22456866654317809, 0.05620974737235393, 0.36951793810092054, 0.18959029772595715]
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.023657526745068935
maxi score, test score, baseline:  0.19643999999999984 0.682 0.682
probs:  [0.10608124452366238, 0.05403210573392784, 0.22456866654317809, 0.05620974737235393, 0.36951793810092054, 0.18959029772595715]
actor:  1 policy actor:  1  step number:  47 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.020474401882566
actor:  1 policy actor:  1  step number:  33 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.19643999999999984 0.682 0.682
probs:  [0.10608124452366238, 0.05403210573392784, 0.22456866654317809, 0.05620974737235393, 0.36951793810092054, 0.18959029772595715]
Printing some Q and Qe and total Qs values:  [[0.655]
 [0.598]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]] [[54.768]
 [58.852]
 [62.383]
 [62.383]
 [62.383]
 [62.383]
 [62.383]] [[2.304]
 [2.435]
 [2.594]
 [2.594]
 [2.594]
 [2.594]
 [2.594]]
printing an ep nov before normalisation:  60.29588607887691
maxi score, test score, baseline:  0.19643999999999984 0.682 0.682
printing an ep nov before normalisation:  61.954331387449876
actor:  1 policy actor:  1  step number:  21 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  25 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.19643999999999984 0.682 0.682
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19643999999999984 0.682 0.682
actor:  1 policy actor:  1  step number:  25 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.19643999999999984 0.682 0.682
probs:  [0.10608124452366238, 0.05403210573392784, 0.22456866654317809, 0.05620974737235393, 0.36951793810092054, 0.18959029772595715]
maxi score, test score, baseline:  0.19643999999999984 0.682 0.682
maxi score, test score, baseline:  0.19643999999999984 0.682 0.682
printing an ep nov before normalisation:  47.780037344425274
actor:  1 policy actor:  1  step number:  20 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.19643999999999984 0.682 0.682
probs:  [0.10608124452366238, 0.05403210573392784, 0.22456866654317809, 0.05620974737235393, 0.36951793810092054, 0.18959029772595715]
Printing some Q and Qe and total Qs values:  [[0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]] [[59.772]
 [59.772]
 [59.772]
 [59.772]
 [59.772]
 [59.772]
 [59.772]] [[2.314]
 [2.314]
 [2.314]
 [2.314]
 [2.314]
 [2.314]
 [2.314]]
maxi score, test score, baseline:  0.19643999999999984 0.682 0.682
probs:  [0.10608124452366238, 0.05403210573392784, 0.22456866654317809, 0.05620974737235393, 0.36951793810092054, 0.18959029772595715]
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]
 [0.371]] [[28.926]
 [28.926]
 [28.926]
 [28.926]
 [28.926]
 [28.926]
 [28.926]] [[1.661]
 [1.661]
 [1.661]
 [1.661]
 [1.661]
 [1.661]
 [1.661]]
printing an ep nov before normalisation:  41.06469927499743
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]] [[42.552]
 [42.552]
 [42.552]
 [42.552]
 [42.552]
 [42.552]
 [42.552]] [[2.184]
 [2.184]
 [2.184]
 [2.184]
 [2.184]
 [2.184]
 [2.184]]
actor:  0 policy actor:  1  step number:  36 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  0.10057897568202634
actor:  1 policy actor:  1  step number:  34 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.293]
 [0.363]
 [0.326]
 [0.311]
 [0.307]
 [0.32 ]] [[18.596]
 [33.59 ]
 [26.351]
 [18.793]
 [18.744]
 [18.426]
 [18.462]] [[0.579]
 [1.012]
 [0.855]
 [0.581]
 [0.564]
 [0.55 ]
 [0.564]]
actor:  1 policy actor:  1  step number:  35 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.19617999999999985 0.682 0.682
probs:  [0.10608124452366238, 0.05403210573392784, 0.22456866654317809, 0.05620974737235393, 0.36951793810092054, 0.18959029772595715]
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.564]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]] [[44.148]
 [49.24 ]
 [44.148]
 [44.148]
 [44.148]
 [44.148]
 [44.148]] [[1.946]
 [2.169]
 [1.946]
 [1.946]
 [1.946]
 [1.946]
 [1.946]]
printing an ep nov before normalisation:  59.24196683623496
actor:  1 policy actor:  1  step number:  30 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  43.92196572147266
printing an ep nov before normalisation:  34.68021140704376
siam score:  -0.7869739
maxi score, test score, baseline:  0.19617999999999985 0.682 0.682
probs:  [0.10608124452366238, 0.05403210573392784, 0.22456866654317809, 0.05620974737235393, 0.36951793810092054, 0.18959029772595715]
maxi score, test score, baseline:  0.19617999999999985 0.682 0.682
probs:  [0.10608124452366238, 0.05403210573392784, 0.22456866654317809, 0.05620974737235393, 0.36951793810092054, 0.18959029772595715]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
using another actor
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.644]
 [0.573]
 [0.568]
 [0.538]
 [0.556]
 [0.568]] [[15.447]
 [18.337]
 [15.747]
 [16.513]
 [22.722]
 [19.79 ]
 [16.362]] [[1.383]
 [1.612]
 [1.404]
 [1.439]
 [1.737]
 [1.6  ]
 [1.431]]
Printing some Q and Qe and total Qs values:  [[0.204]
 [0.329]
 [0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.204]
 [0.329]
 [0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]]
maxi score, test score, baseline:  0.19617999999999985 0.682 0.682
printing an ep nov before normalisation:  35.77971445003823
actor:  0 policy actor:  1  step number:  24 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.19607999999999984 0.682 0.682
probs:  [0.10608124452366238, 0.05403210573392784, 0.22456866654317809, 0.05620974737235393, 0.36951793810092054, 0.18959029772595715]
actor:  1 policy actor:  1  step number:  48 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  33.94267248619978
actor:  0 policy actor:  1  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  24 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.19905999999999985 0.682 0.682
probs:  [0.10608124452366238, 0.05403210573392784, 0.22456866654317809, 0.05620974737235393, 0.36951793810092054, 0.18959029772595715]
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.617]
 [0.53 ]
 [0.53 ]
 [0.53 ]
 [0.53 ]
 [0.53 ]] [[45.281]
 [49.943]
 [45.281]
 [45.281]
 [45.281]
 [45.281]
 [45.281]] [[1.242]
 [1.436]
 [1.242]
 [1.242]
 [1.242]
 [1.242]
 [1.242]]
maxi score, test score, baseline:  0.19905999999999985 0.682 0.682
probs:  [0.10608124452366238, 0.05403210573392784, 0.22456866654317809, 0.05620974737235393, 0.36951793810092054, 0.18959029772595715]
Printing some Q and Qe and total Qs values:  [[0.95 ]
 [0.944]
 [0.916]
 [0.878]
 [0.87 ]
 [0.901]
 [0.949]] [[48.059]
 [46.857]
 [46.781]
 [50.96 ]
 [47.772]
 [52.992]
 [50.281]] [[0.95 ]
 [0.944]
 [0.916]
 [0.878]
 [0.87 ]
 [0.901]
 [0.949]]
siam score:  -0.7970491
actor:  1 policy actor:  1  step number:  23 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  29 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  25 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 33.92566121177664
printing an ep nov before normalisation:  32.36729841820353
printing an ep nov before normalisation:  46.470326324353856
maxi score, test score, baseline:  0.19885999999999987 0.682 0.682
probs:  [0.10608124452366238, 0.05403210573392784, 0.22456866654317809, 0.05620974737235393, 0.36951793810092054, 0.18959029772595715]
maxi score, test score, baseline:  0.19885999999999987 0.682 0.682
actor:  1 policy actor:  1  step number:  27 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  27 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
from probs:  [0.10608124452366238, 0.05403210573392784, 0.22456866654317809, 0.05620974737235393, 0.36951793810092054, 0.18959029772595715]
maxi score, test score, baseline:  0.19885999999999987 0.682 0.682
probs:  [0.10608124452366238, 0.05403210573392784, 0.22456866654317809, 0.05620974737235393, 0.36951793810092054, 0.18959029772595715]
actor:  1 policy actor:  1  step number:  34 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
from probs:  [0.10608124452366238, 0.05403210573392784, 0.22456866654317809, 0.05620974737235393, 0.36951793810092054, 0.18959029772595715]
siam score:  -0.80205864
maxi score, test score, baseline:  0.19885999999999987 0.682 0.682
printing an ep nov before normalisation:  49.80533749273675
actor:  0 policy actor:  1  step number:  22 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20207999999999984 0.682 0.682
probs:  [0.10608124452366238, 0.05403210573392784, 0.22456866654317809, 0.05620974737235393, 0.36951793810092054, 0.18959029772595715]
maxi score, test score, baseline:  0.20207999999999984 0.682 0.682
probs:  [0.10608124452366238, 0.05403210573392784, 0.22456866654317809, 0.05620974737235393, 0.36951793810092054, 0.18959029772595715]
Printing some Q and Qe and total Qs values:  [[0.94 ]
 [1.001]
 [0.94 ]
 [0.94 ]
 [0.939]
 [0.939]
 [0.939]] [[34.384]
 [38.07 ]
 [34.673]
 [34.763]
 [34.517]
 [34.517]
 [34.517]] [[0.94 ]
 [1.001]
 [0.94 ]
 [0.94 ]
 [0.939]
 [0.939]
 [0.939]]
actor:  0 policy actor:  1  step number:  26 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  1  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  25 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.20173999999999986 0.682 0.682
UNIT TEST: sample policy line 217 mcts : [0.02  0.898 0.    0.    0.    0.061 0.02 ]
printing an ep nov before normalisation:  42.46019671488492
actor:  1 policy actor:  1  step number:  35 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.20173999999999986 0.682 0.682
probs:  [0.10585197276479463, 0.05404593999591017, 0.22462627254038656, 0.05622414057109891, 0.36961274832639435, 0.18963892580141545]
maxi score, test score, baseline:  0.20173999999999986 0.682 0.682
probs:  [0.10585197276479463, 0.05404593999591017, 0.22462627254038656, 0.05622414057109891, 0.36961274832639435, 0.18963892580141545]
maxi score, test score, baseline:  0.20173999999999986 0.682 0.682
probs:  [0.10585197276479463, 0.05404593999591017, 0.22462627254038656, 0.05622414057109891, 0.36961274832639435, 0.18963892580141545]
printing an ep nov before normalisation:  50.98140927717686
printing an ep nov before normalisation:  46.659476370588514
maxi score, test score, baseline:  0.20173999999999986 0.682 0.682
maxi score, test score, baseline:  0.20173999999999986 0.682 0.682
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.20173999999999986 0.682 0.682
actor:  1 policy actor:  1  step number:  28 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.20173999999999986 0.682 0.682
probs:  [0.10585197276479463, 0.05404593999591017, 0.22462627254038656, 0.05622414057109891, 0.36961274832639435, 0.18963892580141545]
Printing some Q and Qe and total Qs values:  [[0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]] [[38.762]
 [38.762]
 [38.762]
 [38.762]
 [38.762]
 [38.762]
 [38.762]] [[1.858]
 [1.858]
 [1.858]
 [1.858]
 [1.858]
 [1.858]
 [1.858]]
printing an ep nov before normalisation:  25.19563778976463
actions average: 
K:  3  action  0 :  tensor([0.8163, 0.0102, 0.0383, 0.0294, 0.0412, 0.0329, 0.0317],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0021,     0.9851,     0.0015,     0.0011,     0.0004,     0.0005,
            0.0093], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0273, 0.0482, 0.7759, 0.0213, 0.0205, 0.0692, 0.0376],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1261, 0.0099, 0.1241, 0.4074, 0.1097, 0.1210, 0.1019],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1651, 0.0006, 0.0566, 0.0635, 0.5834, 0.0688, 0.0619],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0963, 0.0065, 0.2534, 0.0742, 0.0860, 0.3992, 0.0845],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1336, 0.0348, 0.0913, 0.1490, 0.1024, 0.0829, 0.4058],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.20173999999999986 0.682 0.682
actor:  1 policy actor:  1  step number:  52 total reward:  0.12999999999999945  reward:  1.0 rdn_beta:  1.333
from probs:  [0.10585197276479463, 0.05404593999591017, 0.22462627254038656, 0.05622414057109891, 0.36961274832639435, 0.18963892580141545]
printing an ep nov before normalisation:  36.540366671002445
maxi score, test score, baseline:  0.20173999999999986 0.682 0.682
probs:  [0.10585197276479463, 0.05404593999591017, 0.22462627254038656, 0.05622414057109891, 0.36961274832639435, 0.18963892580141545]
maxi score, test score, baseline:  0.20173999999999986 0.682 0.682
probs:  [0.10585197276479463, 0.05404593999591017, 0.22462627254038656, 0.05622414057109891, 0.36961274832639435, 0.18963892580141545]
maxi score, test score, baseline:  0.20173999999999986 0.682 0.682
probs:  [0.10585197276479463, 0.05404593999591017, 0.22462627254038656, 0.05622414057109891, 0.36961274832639435, 0.18963892580141545]
actions average: 
K:  2  action  0 :  tensor([0.4735, 0.0123, 0.0799, 0.0925, 0.1767, 0.0984, 0.0666],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0027,     0.9718,     0.0035,     0.0014,     0.0003,     0.0006,
            0.0197], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0386, 0.0704, 0.7568, 0.0299, 0.0294, 0.0377, 0.0372],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1319, 0.1097, 0.1481, 0.2003, 0.1445, 0.1497, 0.1158],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2396, 0.0151, 0.1027, 0.0838, 0.4005, 0.0972, 0.0611],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0386, 0.0163, 0.1064, 0.0487, 0.0276, 0.7028, 0.0596],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1222, 0.1550, 0.1350, 0.1163, 0.1271, 0.1496, 0.1946],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  39.023346205192894
maxi score, test score, baseline:  0.20173999999999986 0.682 0.682
printing an ep nov before normalisation:  39.850004854641746
actor:  0 policy actor:  1  step number:  33 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20445999999999984 0.682 0.682
probs:  [0.10585197276479463, 0.05404593999591017, 0.22462627254038656, 0.05622414057109891, 0.36961274832639435, 0.18963892580141545]
maxi score, test score, baseline:  0.20445999999999984 0.682 0.682
probs:  [0.10585197276479463, 0.05404593999591017, 0.22462627254038656, 0.05622414057109891, 0.36961274832639435, 0.18963892580141545]
maxi score, test score, baseline:  0.20445999999999984 0.682 0.682
probs:  [0.10585197276479463, 0.05404593999591017, 0.22462627254038656, 0.05622414057109891, 0.36961274832639435, 0.18963892580141545]
printing an ep nov before normalisation:  44.20693919517055
actor:  1 policy actor:  1  step number:  20 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  43.03819110724957
printing an ep nov before normalisation:  20.752053724441886
printing an ep nov before normalisation:  28.97221326828003
printing an ep nov before normalisation:  41.63749058300876
printing an ep nov before normalisation:  21.84902856936704
maxi score, test score, baseline:  0.20445999999999984 0.682 0.682
probs:  [0.10585197276479463, 0.05404593999591017, 0.22462627254038656, 0.05622414057109891, 0.36961274832639435, 0.18963892580141545]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  37 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 40.77649283810034
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.372]
 [0.612]
 [0.372]
 [0.372]
 [0.372]
 [0.479]] [[39.714]
 [39.714]
 [40.505]
 [39.714]
 [39.714]
 [39.714]
 [42.55 ]] [[1.291]
 [1.291]
 [1.566]
 [1.291]
 [1.291]
 [1.291]
 [1.523]]
maxi score, test score, baseline:  0.20445999999999984 0.682 0.682
probs:  [0.1057381381591665, 0.05405280877802568, 0.22465487421381752, 0.0562312868681875, 0.3696598220902087, 0.18966306989059414]
actor:  1 policy actor:  1  step number:  33 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  29.03546928812442
actor:  1 policy actor:  1  step number:  37 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  29.393112214553867
actor:  1 policy actor:  1  step number:  37 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  2  action  0 :  tensor([0.7252, 0.0097, 0.0430, 0.0488, 0.0659, 0.0473, 0.0601],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0030,     0.9606,     0.0037,     0.0045,     0.0008,     0.0008,
            0.0266], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0294, 0.0598, 0.7305, 0.0314, 0.0269, 0.0789, 0.0433],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1128, 0.1515, 0.1118, 0.2748, 0.1047, 0.1233, 0.1211],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1142, 0.0031, 0.1118, 0.1380, 0.3913, 0.1311, 0.1105],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1415, 0.0217, 0.1892, 0.1257, 0.1294, 0.2560, 0.1367],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0456, 0.3951, 0.2166, 0.0684, 0.0379, 0.0480, 0.1884],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  23 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.20757999999999985 0.682 0.682
probs:  [0.1056248327692585, 0.05405964562725535, 0.22468334291841077, 0.05623839994222618, 0.3697066770086585, 0.1896871017341907]
maxi score, test score, baseline:  0.20757999999999985 0.682 0.682
probs:  [0.1056248327692585, 0.05405964562725535, 0.22468334291841077, 0.05623839994222618, 0.3697066770086585, 0.1896871017341907]
printing an ep nov before normalisation:  42.52678782056677
actions average: 
K:  0  action  0 :  tensor([0.7732, 0.0066, 0.0219, 0.0227, 0.1248, 0.0212, 0.0297],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0003,     0.9763,     0.0004,     0.0085,     0.0001,     0.0002,
            0.0142], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0340, 0.0073, 0.7723, 0.0413, 0.0397, 0.0743, 0.0310],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1440, 0.1254, 0.0758, 0.2795, 0.1413, 0.1077, 0.1263],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1129, 0.0044, 0.1099, 0.1152, 0.4285, 0.1241, 0.1049],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0667, 0.0039, 0.1625, 0.0551, 0.0517, 0.6017, 0.0583],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1297, 0.2390, 0.0744, 0.0659, 0.0861, 0.0589, 0.3459],
       grad_fn=<DivBackward0>)
siam score:  -0.7892981
maxi score, test score, baseline:  0.20757999999999985 0.682 0.682
printing an ep nov before normalisation:  55.61280641511761
actor:  1 policy actor:  1  step number:  32 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.20757999999999985 0.682 0.682
probs:  [0.1056248327692585, 0.05405964562725535, 0.22468334291841077, 0.05623839994222618, 0.3697066770086585, 0.1896871017341907]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  27 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.0
using another actor
maxi score, test score, baseline:  0.20757999999999985 0.682 0.682
probs:  [0.1056248327692585, 0.05405964562725535, 0.22468334291841077, 0.05623839994222618, 0.3697066770086585, 0.1896871017341907]
printing an ep nov before normalisation:  37.03118931775626
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.30779363715292
maxi score, test score, baseline:  0.20757999999999985 0.682 0.682
probs:  [0.1056248327692585, 0.05405964562725535, 0.22468334291841077, 0.05623839994222618, 0.3697066770086585, 0.1896871017341907]
printing an ep nov before normalisation:  55.731790530333754
printing an ep nov before normalisation:  65.71929451726906
maxi score, test score, baseline:  0.20757999999999985 0.682 0.682
probs:  [0.1056248327692585, 0.05405964562725535, 0.22468334291841077, 0.05623839994222618, 0.3697066770086585, 0.1896871017341907]
actor:  0 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  33 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  32 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  32.50144766506554
maxi score, test score, baseline:  0.21059999999999984 0.682 0.682
probs:  [0.1056248327692585, 0.05405964562725535, 0.22468334291841077, 0.05623839994222618, 0.3697066770086585, 0.1896871017341907]
printing an ep nov before normalisation:  31.172287464141846
printing an ep nov before normalisation:  48.40066883653235
maxi score, test score, baseline:  0.21059999999999984 0.682 0.682
probs:  [0.1056248327692585, 0.05405964562725535, 0.22468334291841077, 0.05623839994222618, 0.3697066770086585, 0.1896871017341907]
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  31 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  34 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.20995999999999984 0.682 0.682
probs:  [0.1056248327692585, 0.05405964562725535, 0.22468334291841077, 0.05623839994222618, 0.3697066770086585, 0.1896871017341907]
maxi score, test score, baseline:  0.20995999999999984 0.682 0.682
probs:  [0.1056248327692585, 0.05405964562725535, 0.22468334291841077, 0.05623839994222618, 0.3697066770086585, 0.1896871017341907]
printing an ep nov before normalisation:  47.91235136977342
actions average: 
K:  1  action  0 :  tensor([0.6189, 0.0085, 0.0802, 0.0662, 0.1011, 0.0688, 0.0563],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0012,     0.9840,     0.0018,     0.0039,     0.0002,     0.0004,
            0.0085], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0662, 0.0098, 0.6104, 0.0718, 0.0894, 0.0866, 0.0656],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1072, 0.0732, 0.1024, 0.3765, 0.1169, 0.1315, 0.0923],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0886, 0.0062, 0.0818, 0.0784, 0.6113, 0.0720, 0.0617],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0919, 0.0045, 0.1183, 0.1266, 0.1295, 0.4490, 0.0803],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0882, 0.1343, 0.1055, 0.0777, 0.0674, 0.0744, 0.4525],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  55.74458550966238
maxi score, test score, baseline:  0.20995999999999984 0.682 0.682
probs:  [0.1056248327692585, 0.05405964562725535, 0.22468334291841077, 0.05623839994222618, 0.3697066770086585, 0.1896871017341907]
maxi score, test score, baseline:  0.20995999999999984 0.682 0.682
probs:  [0.1056248327692585, 0.05405964562725535, 0.22468334291841077, 0.05623839994222618, 0.3697066770086585, 0.1896871017341907]
actor:  1 policy actor:  1  step number:  35 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20995999999999984 0.682 0.682
maxi score, test score, baseline:  0.20995999999999984 0.682 0.682
probs:  [0.1056248327692585, 0.05405964562725535, 0.22468334291841077, 0.05623839994222618, 0.3697066770086585, 0.1896871017341907]
actor:  1 policy actor:  1  step number:  30 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.20995999999999984 0.682 0.682
printing an ep nov before normalisation:  57.417149500683486
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.20995999999999984 0.682 0.682
probs:  [0.1056248327692585, 0.05405964562725535, 0.22468334291841077, 0.05623839994222618, 0.3697066770086585, 0.1896871017341907]
actor:  1 policy actor:  1  step number:  23 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  26.353561878204346
maxi score, test score, baseline:  0.20995999999999984 0.682 0.682
probs:  [0.1056248327692585, 0.05405964562725535, 0.22468334291841077, 0.05623839994222618, 0.3697066770086585, 0.1896871017341907]
Printing some Q and Qe and total Qs values:  [[0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]] [[33.081]
 [33.081]
 [33.081]
 [33.081]
 [33.081]
 [33.081]
 [33.081]] [[1.667]
 [1.667]
 [1.667]
 [1.667]
 [1.667]
 [1.667]
 [1.667]]
actor:  1 policy actor:  1  step number:  37 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.20995999999999984 0.682 0.682
probs:  [0.10551205291315316, 0.05406645076576603, 0.2247116795792715, 0.05624548002435841, 0.36975331460431793, 0.1897110221131329]
maxi score, test score, baseline:  0.20995999999999984 0.682 0.682
probs:  [0.10551205291315316, 0.05406645076576603, 0.2247116795792715, 0.05624548002435841, 0.36975331460431793, 0.1897110221131329]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  51.249112790641604
maxi score, test score, baseline:  0.20995999999999984 0.682 0.682
probs:  [0.10551205291315316, 0.05406645076576603, 0.2247116795792715, 0.05624548002435841, 0.36975331460431793, 0.1897110221131329]
Printing some Q and Qe and total Qs values:  [[0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.704]] [[45.715]
 [45.715]
 [45.715]
 [45.715]
 [45.715]
 [45.715]
 [45.715]] [[0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.704]]
maxi score, test score, baseline:  0.20995999999999984 0.682 0.682
probs:  [0.10551205291315316, 0.05406645076576603, 0.2247116795792715, 0.05624548002435841, 0.36975331460431793, 0.1897110221131329]
printing an ep nov before normalisation:  43.655981910537655
actions average: 
K:  0  action  0 :  tensor([0.5656, 0.0052, 0.0819, 0.0923, 0.0944, 0.0929, 0.0677],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0011,     0.9818,     0.0045,     0.0056,     0.0001,     0.0005,
            0.0065], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0326, 0.0074, 0.8193, 0.0367, 0.0292, 0.0395, 0.0354],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1150, 0.0699, 0.1006, 0.3763, 0.1103, 0.1125, 0.1154],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.1196,     0.0004,     0.0441,     0.0726,     0.6522,     0.0570,
            0.0542], grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([    0.0488,     0.0003,     0.2084,     0.0662,     0.0579,     0.5631,
            0.0553], grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1252, 0.0565, 0.0967, 0.1470, 0.1144, 0.1087, 0.3515],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  24 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.20957999999999985 0.682 0.682
probs:  [0.10551205291315316, 0.05406645076576603, 0.2247116795792715, 0.05624548002435841, 0.36975331460431793, 0.1897110221131329]
maxi score, test score, baseline:  0.20957999999999985 0.682 0.682
probs:  [0.10539979494300863, 0.05407322441366861, 0.22473988511294368, 0.056252527343587724, 0.369799736385671, 0.18973483180112047]
maxi score, test score, baseline:  0.20957999999999985 0.682 0.682
probs:  [0.10539979494300863, 0.05407322441366861, 0.22473988511294368, 0.056252527343587724, 0.369799736385671, 0.18973483180112047]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20957999999999985 0.682 0.682
maxi score, test score, baseline:  0.20957999999999985 0.682 0.682
probs:  [0.10539979494300863, 0.05407322441366861, 0.22473988511294368, 0.056252527343587724, 0.369799736385671, 0.18973483180112047]
printing an ep nov before normalisation:  51.16321886060295
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.20957999999999985 0.682 0.682
probs:  [0.10539979494300863, 0.05407322441366861, 0.22473988511294368, 0.056252527343587724, 0.369799736385671, 0.18973483180112047]
printing an ep nov before normalisation:  33.54936774362498
maxi score, test score, baseline:  0.20957999999999985 0.682 0.682
probs:  [0.10539979494300863, 0.05407322441366861, 0.22473988511294368, 0.056252527343587724, 0.369799736385671, 0.18973483180112047]
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.588]
 [0.525]
 [0.536]
 [0.536]
 [0.536]
 [0.534]] [[31.996]
 [48.802]
 [41.738]
 [31.996]
 [31.996]
 [31.996]
 [43.953]] [[0.641]
 [0.882]
 [0.739]
 [0.641]
 [0.641]
 [0.641]
 [0.774]]
printing an ep nov before normalisation:  45.64735302063085
actor:  0 policy actor:  1  step number:  30 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  44.71683025972063
actor:  1 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  42 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  26.992044096702728
maxi score, test score, baseline:  0.20911999999999986 0.682 0.682
maxi score, test score, baseline:  0.20911999999999986 0.682 0.682
probs:  [0.10578294022846012, 0.053418807367730727, 0.22796015515424956, 0.05564758754696798, 0.36503013617734137, 0.19216037352525028]
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]] [[50.856]
 [50.856]
 [50.856]
 [50.856]
 [50.856]
 [50.856]
 [50.856]] [[2.37]
 [2.37]
 [2.37]
 [2.37]
 [2.37]
 [2.37]
 [2.37]]
printing an ep nov before normalisation:  50.43042274112302
actor:  1 policy actor:  1  step number:  33 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.533]
 [0.649]
 [0.533]
 [0.533]
 [0.533]
 [0.533]] [[39.18 ]
 [39.18 ]
 [44.223]
 [39.18 ]
 [39.18 ]
 [39.18 ]
 [39.18 ]] [[1.775]
 [1.775]
 [2.148]
 [1.775]
 [1.775]
 [1.775]
 [1.775]]
actor:  1 policy actor:  1  step number:  33 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.20911999999999986 0.682 0.682
probs:  [0.10578294022846012, 0.053418807367730727, 0.22796015515424956, 0.05564758754696798, 0.36503013617734137, 0.19216037352525028]
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]]
actor:  0 policy actor:  1  step number:  30 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.278]
 [0.409]
 [0.278]
 [0.278]
 [0.352]
 [0.278]
 [0.278]] [[32.669]
 [28.479]
 [32.669]
 [32.669]
 [21.773]
 [32.669]
 [32.669]] [[2.412]
 [2.076]
 [2.412]
 [2.412]
 [1.272]
 [2.412]
 [2.412]]
maxi score, test score, baseline:  0.20533999999999983 0.682 0.682
probs:  [0.10578294022846012, 0.053418807367730727, 0.22796015515424956, 0.05564758754696798, 0.36503013617734137, 0.19216037352525028]
Printing some Q and Qe and total Qs values:  [[0.67 ]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]] [[46.463]
 [50.68 ]
 [50.68 ]
 [50.68 ]
 [50.68 ]
 [50.68 ]
 [50.68 ]] [[1.606]
 [1.766]
 [1.766]
 [1.766]
 [1.766]
 [1.766]
 [1.766]]
maxi score, test score, baseline:  0.20533999999999983 0.682 0.682
probs:  [0.10578294022846012, 0.053418807367730727, 0.22796015515424956, 0.05564758754696798, 0.36503013617734137, 0.19216037352525028]
printing an ep nov before normalisation:  52.23274861875069
Printing some Q and Qe and total Qs values:  [[0.684]
 [0.667]
 [0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.684]] [[34.591]
 [44.184]
 [34.591]
 [34.591]
 [34.591]
 [34.591]
 [34.591]] [[1.657]
 [2.153]
 [1.657]
 [1.657]
 [1.657]
 [1.657]
 [1.657]]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20533999999999983 0.682 0.682
printing an ep nov before normalisation:  37.164382160829426
maxi score, test score, baseline:  0.20533999999999983 0.682 0.682
probs:  [0.10578294022846012, 0.053418807367730727, 0.22796015515424956, 0.05564758754696798, 0.36503013617734137, 0.19216037352525028]
actor:  1 policy actor:  1  step number:  31 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.20533999999999983 0.682 0.682
probs:  [0.10578294022846012, 0.053418807367730727, 0.22796015515424956, 0.05564758754696798, 0.36503013617734137, 0.19216037352525028]
actor:  1 policy actor:  1  step number:  48 total reward:  0.2699999999999996  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  43.40743349545491
printing an ep nov before normalisation:  41.76825305501214
siam score:  -0.7904167
maxi score, test score, baseline:  0.20533999999999983 0.682 0.682
probs:  [0.10578294022846012, 0.053418807367730727, 0.22796015515424956, 0.05564758754696798, 0.36503013617734137, 0.19216037352525028]
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.548]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]] [[38.685]
 [38.58 ]
 [36.039]
 [36.039]
 [36.039]
 [36.039]
 [36.039]] [[1.045]
 [1.115]
 [0.972]
 [0.972]
 [0.972]
 [0.972]
 [0.972]]
actor:  0 policy actor:  0  step number:  23 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2050999999999998 0.682 0.682
probs:  [0.10578294022846012, 0.053418807367730727, 0.22796015515424956, 0.05564758754696798, 0.36503013617734137, 0.19216037352525028]
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.726]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]] [[42.723]
 [46.902]
 [42.723]
 [42.723]
 [42.723]
 [42.723]
 [42.723]] [[1.395]
 [1.569]
 [1.395]
 [1.395]
 [1.395]
 [1.395]
 [1.395]]
printing an ep nov before normalisation:  31.840929816327314
siam score:  -0.7905793
actor:  1 policy actor:  1  step number:  32 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  49.16294286030329
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.375]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.242]
 [0.375]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]]
maxi score, test score, baseline:  0.2050999999999998 0.682 0.682
probs:  [0.10578294022846012, 0.053418807367730727, 0.22796015515424956, 0.05564758754696798, 0.36503013617734137, 0.19216037352525028]
actor:  1 policy actor:  1  step number:  31 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  56.8494809008347
printing an ep nov before normalisation:  48.92231458265957
actor:  1 policy actor:  1  step number:  20 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2050999999999998 0.682 0.682
probs:  [0.10578294022846012, 0.053418807367730727, 0.22796015515424956, 0.05564758754696798, 0.36503013617734137, 0.19216037352525028]
printing an ep nov before normalisation:  45.578659188412686
actor:  1 policy actor:  1  step number:  24 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2050999999999998 0.682 0.682
probs:  [0.10578294022846012, 0.053418807367730727, 0.22796015515424956, 0.05564758754696798, 0.36503013617734137, 0.19216037352525028]
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.462]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]] [[38.296]
 [43.124]
 [38.296]
 [38.296]
 [38.296]
 [38.296]
 [38.296]] [[1.379]
 [1.604]
 [1.379]
 [1.379]
 [1.379]
 [1.379]
 [1.379]]
maxi score, test score, baseline:  0.2050999999999998 0.682 0.682
printing an ep nov before normalisation:  55.21596831075102
maxi score, test score, baseline:  0.2050999999999998 0.682 0.682
probs:  [0.10578294022846012, 0.053418807367730727, 0.22796015515424956, 0.05564758754696798, 0.36503013617734137, 0.19216037352525028]
siam score:  -0.78583926
maxi score, test score, baseline:  0.2050999999999998 0.682 0.682
actor:  1 policy actor:  1  step number:  35 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2050999999999998 0.682 0.682
actor:  1 policy actor:  1  step number:  35 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2050999999999998 0.682 0.682
probs:  [0.10578294022846012, 0.053418807367730727, 0.22796015515424956, 0.05564758754696798, 0.36503013617734137, 0.19216037352525028]
siam score:  -0.7869931
maxi score, test score, baseline:  0.2050999999999998 0.682 0.682
maxi score, test score, baseline:  0.2050999999999998 0.682 0.682
probs:  [0.10578294022846012, 0.053418807367730727, 0.22796015515424956, 0.05564758754696798, 0.36503013617734137, 0.19216037352525028]
actor:  1 policy actor:  1  step number:  57 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  29 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.244]
 [0.331]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.244]
 [0.331]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]]
maxi score, test score, baseline:  0.2050999999999998 0.682 0.682
probs:  [0.10578294022846012, 0.053418807367730727, 0.22796015515424956, 0.05564758754696798, 0.36503013617734137, 0.19216037352525028]
printing an ep nov before normalisation:  31.80450893058155
actor:  1 policy actor:  1  step number:  36 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 48.56315694269884
Printing some Q and Qe and total Qs values:  [[0.254]
 [0.254]
 [0.254]
 [0.254]
 [0.254]
 [0.254]
 [0.254]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.254]
 [0.254]
 [0.254]
 [0.254]
 [0.254]
 [0.254]
 [0.254]]
printing an ep nov before normalisation:  63.27254127328245
using another actor
printing an ep nov before normalisation:  0.09816039599854776
actor:  1 policy actor:  1  step number:  28 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]]
maxi score, test score, baseline:  0.2050999999999998 0.682 0.682
probs:  [0.10578294022846012, 0.053418807367730727, 0.22796015515424956, 0.05564758754696798, 0.36503013617734137, 0.19216037352525028]
actor:  1 policy actor:  1  step number:  35 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2050999999999998 0.682 0.682
probs:  [0.10578294022846012, 0.053418807367730727, 0.22796015515424956, 0.05564758754696798, 0.36503013617734137, 0.19216037352525028]
maxi score, test score, baseline:  0.2050999999999998 0.682 0.682
probs:  [0.10578294022846012, 0.053418807367730727, 0.22796015515424956, 0.05564758754696798, 0.36503013617734137, 0.19216037352525028]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  0 policy actor:  1  step number:  29 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20473999999999984 0.682 0.682
probs:  [0.10578294022846012, 0.053418807367730727, 0.22796015515424956, 0.05564758754696798, 0.36503013617734137, 0.19216037352525028]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  27 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20473999999999984 0.682 0.682
probs:  [0.10578294022846012, 0.053418807367730727, 0.22796015515424956, 0.05564758754696798, 0.36503013617734137, 0.19216037352525028]
actor:  1 policy actor:  1  step number:  20 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.0
line 256 mcts: sample exp_bonus 40.66029966118907
printing an ep nov before normalisation:  0.01456309300053249
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  4.229981698244956e-06
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]] [[39.881]
 [39.881]
 [39.881]
 [39.881]
 [39.881]
 [39.881]
 [39.881]] [[1.75]
 [1.75]
 [1.75]
 [1.75]
 [1.75]
 [1.75]
 [1.75]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.284428258208095
maxi score, test score, baseline:  0.20137999999999984 0.682 0.682
probs:  [0.10578294022846012, 0.053418807367730727, 0.22796015515424956, 0.05564758754696798, 0.36503013617734137, 0.19216037352525028]
maxi score, test score, baseline:  0.20137999999999984 0.682 0.682
maxi score, test score, baseline:  0.20137999999999984 0.682 0.682
probs:  [0.10578294022846012, 0.053418807367730727, 0.22796015515424956, 0.05564758754696798, 0.36503013617734137, 0.19216037352525028]
printing an ep nov before normalisation:  37.19909664828322
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  32 total reward:  0.4899999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.20137999999999984 0.682 0.682
probs:  [0.10578294022846012, 0.053418807367730727, 0.22796015515424956, 0.05564758754696798, 0.36503013617734137, 0.19216037352525028]
actor:  1 policy actor:  1  step number:  33 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.20137999999999984 0.682 0.682
probs:  [0.10566926690664016, 0.053425586107226586, 0.22798913821833408, 0.055654649821334744, 0.36507655663599553, 0.19218480231046892]
maxi score, test score, baseline:  0.20137999999999984 0.682 0.682
probs:  [0.10566926690664016, 0.053425586107226586, 0.22798913821833408, 0.055654649821334744, 0.36507655663599553, 0.19218480231046892]
printing an ep nov before normalisation:  36.116237545468664
printing an ep nov before normalisation:  45.67832609410176
maxi score, test score, baseline:  0.20137999999999984 0.682 0.682
probs:  [0.10566926690664016, 0.053425586107226586, 0.22798913821833408, 0.055654649821334744, 0.36507655663599553, 0.19218480231046892]
printing an ep nov before normalisation:  20.0050687789917
actor:  1 policy actor:  1  step number:  49 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  39 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.667
from probs:  [0.10566926690664016, 0.053425586107226586, 0.22798913821833408, 0.055654649821334744, 0.36507655663599553, 0.19218480231046892]
Printing some Q and Qe and total Qs values:  [[0.949]
 [0.893]
 [0.862]
 [0.862]
 [0.862]
 [0.862]
 [0.862]] [[49.5  ]
 [52.069]
 [46.91 ]
 [46.91 ]
 [46.91 ]
 [46.91 ]
 [46.91 ]] [[0.949]
 [0.893]
 [0.862]
 [0.862]
 [0.862]
 [0.862]
 [0.862]]
printing an ep nov before normalisation:  48.11470864623699
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.20137999999999984 0.682 0.682
probs:  [0.10566926690664016, 0.053425586107226586, 0.22798913821833408, 0.055654649821334744, 0.36507655663599553, 0.19218480231046892]
printing an ep nov before normalisation:  31.773264064453677
actor:  0 policy actor:  0  step number:  24 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  37.63910398370703
actor:  0 policy actor:  1  step number:  31 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  47.93778981995825
actor:  0 policy actor:  0  step number:  25 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]] [[48.163]
 [48.163]
 [48.163]
 [48.163]
 [48.163]
 [48.163]
 [48.163]] [[2.671]
 [2.671]
 [2.671]
 [2.671]
 [2.671]
 [2.671]
 [2.671]]
printing an ep nov before normalisation:  34.883763684546444
actor:  1 policy actor:  1  step number:  40 total reward:  0.3299999999999996  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.20019999999999985 0.682 0.682
probs:  [0.10566926690664016, 0.053425586107226586, 0.22798913821833408, 0.055654649821334744, 0.36507655663599553, 0.19218480231046892]
actor:  1 policy actor:  1  step number:  37 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 36.80408267909511
Printing some Q and Qe and total Qs values:  [[0.649]
 [0.649]
 [0.644]
 [0.404]
 [0.649]
 [0.649]
 [0.394]] [[53.865]
 [53.865]
 [45.213]
 [37.082]
 [53.865]
 [53.865]
 [55.182]] [[2.264]
 [2.264]
 [1.915]
 [1.352]
 [2.264]
 [2.264]
 [2.061]]
printing an ep nov before normalisation:  32.701013119187046
Printing some Q and Qe and total Qs values:  [[0.895]
 [0.955]
 [0.921]
 [0.91 ]
 [0.895]
 [0.91 ]
 [0.918]] [[30.152]
 [37.788]
 [38.056]
 [37.632]
 [30.152]
 [38.086]
 [33.882]] [[0.895]
 [0.955]
 [0.921]
 [0.91 ]
 [0.895]
 [0.91 ]
 [0.918]]
actor:  0 policy actor:  1  step number:  23 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  33 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.20011999999999985 0.682 0.682
from probs:  [0.10555611534510247, 0.05343233373232392, 0.2280179882502223, 0.055661679679878756, 0.36512276402485244, 0.19220911896762014]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20011999999999985 0.682 0.682
printing an ep nov before normalisation:  24.18846607208252
Printing some Q and Qe and total Qs values:  [[0.766]
 [0.85 ]
 [0.777]
 [0.788]
 [0.762]
 [0.788]
 [0.783]] [[29.976]
 [36.988]
 [30.189]
 [33.722]
 [30.776]
 [33.722]
 [35.394]] [[0.766]
 [0.85 ]
 [0.777]
 [0.788]
 [0.762]
 [0.788]
 [0.783]]
printing an ep nov before normalisation:  46.6810706719585
printing an ep nov before normalisation:  48.631391568298845
actor:  1 policy actor:  1  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.20011999999999985 0.682 0.682
probs:  [0.10555611534510247, 0.05343233373232392, 0.2280179882502223, 0.055661679679878756, 0.36512276402485244, 0.19220911896762014]
printing an ep nov before normalisation:  51.119262778732605
actor:  0 policy actor:  1  step number:  38 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  52.029197187016024
Printing some Q and Qe and total Qs values:  [[0.37 ]
 [0.486]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]] [[31.06 ]
 [30.495]
 [31.06 ]
 [31.06 ]
 [31.06 ]
 [31.06 ]
 [31.06 ]] [[1.509]
 [1.599]
 [1.509]
 [1.509]
 [1.509]
 [1.509]
 [1.509]]
actor:  1 policy actor:  1  step number:  30 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.305]
 [0.261]
 [0.324]
 [0.265]
 [0.261]
 [0.319]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.268]
 [0.305]
 [0.261]
 [0.324]
 [0.265]
 [0.261]
 [0.319]]
maxi score, test score, baseline:  0.19949999999999984 0.682 0.682
probs:  [0.10555611534510247, 0.05343233373232392, 0.2280179882502223, 0.055661679679878756, 0.36512276402485244, 0.19220911896762014]
actor:  1 policy actor:  1  step number:  35 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.482]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.42 ]
 [0.482]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]]
printing an ep nov before normalisation:  41.878907599510306
maxi score, test score, baseline:  0.19949999999999984 0.682 0.682
probs:  [0.10555611534510247, 0.05343233373232392, 0.2280179882502223, 0.055661679679878756, 0.36512276402485244, 0.19220911896762014]
printing an ep nov before normalisation:  25.232606111422783
maxi score, test score, baseline:  0.19949999999999984 0.682 0.682
probs:  [0.10555611534510247, 0.05343233373232392, 0.2280179882502223, 0.055661679679878756, 0.36512276402485244, 0.19220911896762014]
actor:  1 policy actor:  1  step number:  40 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  33.975991520025055
maxi score, test score, baseline:  0.19949999999999984 0.682 0.682
probs:  [0.10555611534510247, 0.05343233373232392, 0.2280179882502223, 0.055661679679878756, 0.36512276402485244, 0.19220911896762014]
maxi score, test score, baseline:  0.19949999999999984 0.682 0.682
probs:  [0.10555611534510247, 0.05343233373232392, 0.2280179882502223, 0.055661679679878756, 0.36512276402485244, 0.19220911896762014]
actor:  1 policy actor:  1  step number:  30 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
siam score:  -0.77792096
actor:  0 policy actor:  1  step number:  26 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  0.03705419302846735
actor:  1 policy actor:  1  step number:  27 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.19913999999999984 0.682 0.682
probs:  [0.10555611534510247, 0.05343233373232392, 0.2280179882502223, 0.055661679679878756, 0.36512276402485244, 0.19220911896762014]
printing an ep nov before normalisation:  59.8949430969133
printing an ep nov before normalisation:  50.99512127069629
printing an ep nov before normalisation:  51.263160269755105
printing an ep nov before normalisation:  46.48710993751821
printing an ep nov before normalisation:  40.03523972722851
maxi score, test score, baseline:  0.19913999999999984 0.682 0.682
probs:  [0.10555611534510247, 0.05343233373232392, 0.2280179882502223, 0.055661679679878756, 0.36512276402485244, 0.19220911896762014]
actor:  1 policy actor:  1  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  23.774396547670204
actor:  1 policy actor:  1  step number:  35 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.19913999999999984 0.682 0.682
Printing some Q and Qe and total Qs values:  [[0.58 ]
 [0.617]
 [0.591]
 [0.592]
 [0.582]
 [0.592]
 [0.593]] [[18.114]
 [14.287]
 [15.994]
 [15.709]
 [15.926]
 [16.419]
 [15.719]] [[1.777]
 [1.561]
 [1.648]
 [1.63 ]
 [1.635]
 [1.677]
 [1.632]]
maxi score, test score, baseline:  0.19913999999999984 0.682 0.682
probs:  [0.10555611534510247, 0.05343233373232392, 0.2280179882502223, 0.055661679679878756, 0.36512276402485244, 0.19220911896762014]
actor:  1 policy actor:  1  step number:  44 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  27.681455462191064
actor:  1 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  36 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  35.92487803880645
maxi score, test score, baseline:  0.19913999999999984 0.682 0.682
probs:  [0.10555611534510247, 0.05343233373232392, 0.2280179882502223, 0.055661679679878756, 0.36512276402485244, 0.19220911896762014]
printing an ep nov before normalisation:  36.207184625937764
siam score:  -0.7753273
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19913999999999984 0.682 0.682
probs:  [0.10555611534510247, 0.05343233373232392, 0.2280179882502223, 0.055661679679878756, 0.36512276402485244, 0.19220911896762014]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  24 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.19913999999999984 0.682 0.682
probs:  [0.10544348195975516, 0.053439050456754775, 0.22804670616374265, 0.0556686773452716, 0.36516875980753805, 0.19223332426693782]
maxi score, test score, baseline:  0.19913999999999984 0.682 0.682
actions average: 
K:  1  action  0 :  tensor([0.5285, 0.0130, 0.0713, 0.0811, 0.0991, 0.1012, 0.1058],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0011,     0.9734,     0.0009,     0.0054,     0.0017,     0.0007,
            0.0167], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0566, 0.0102, 0.6921, 0.0544, 0.0555, 0.0859, 0.0453],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0926, 0.1319, 0.0667, 0.4014, 0.0968, 0.1314, 0.0792],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1281, 0.0133, 0.0581, 0.0581, 0.5994, 0.0713, 0.0716],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1861, 0.0014, 0.1846, 0.1578, 0.1777, 0.1716, 0.1208],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1726, 0.0070, 0.1361, 0.1547, 0.1469, 0.1780, 0.2046],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.19579999999999984 0.682 0.682
probs:  [0.10544348195975516, 0.053439050456754775, 0.22804670616374265, 0.0556686773452716, 0.36516875980753805, 0.19223332426693782]
actor:  1 policy actor:  1  step number:  24 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  31 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19579999999999984 0.682 0.682
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.578]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]] [[46.519]
 [41.26 ]
 [39.517]
 [39.517]
 [39.517]
 [39.517]
 [39.517]] [[1.342]
 [1.318]
 [1.239]
 [1.239]
 [1.239]
 [1.239]
 [1.239]]
Printing some Q and Qe and total Qs values:  [[0.958]
 [1.012]
 [0.958]
 [0.958]
 [0.936]
 [0.958]
 [0.928]] [[53.344]
 [41.608]
 [53.344]
 [53.344]
 [42.106]
 [53.344]
 [46.449]] [[0.958]
 [1.012]
 [0.958]
 [0.958]
 [0.936]
 [0.958]
 [0.928]]
maxi score, test score, baseline:  0.19579999999999984 0.682 0.682
probs:  [0.10533136319925755, 0.05344573649229805, 0.22807529286437364, 0.0556756430381505, 0.3652145454343035, 0.1922574189716168]
Printing some Q and Qe and total Qs values:  [[0.717]
 [0.767]
 [0.69 ]
 [0.712]
 [0.697]
 [0.717]
 [0.717]] [[34.739]
 [45.254]
 [35.438]
 [42.269]
 [34.494]
 [34.739]
 [34.739]] [[1.044]
 [1.419]
 [1.038]
 [1.271]
 [1.016]
 [1.044]
 [1.044]]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  32 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  34 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.19547999999999985 0.682 0.682
probs:  [0.10533136319925755, 0.05344573649229805, 0.22807529286437364, 0.0556756430381505, 0.3652145454343035, 0.1922574189716168]
siam score:  -0.77467614
actor:  1 policy actor:  1  step number:  21 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  32 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.19213999999999984 0.682 0.682
probs:  [0.10533136319925755, 0.05344573649229805, 0.22807529286437364, 0.0556756430381505, 0.3652145454343035, 0.1922574189716168]
Printing some Q and Qe and total Qs values:  [[0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]] [[48.1]
 [48.1]
 [48.1]
 [48.1]
 [48.1]
 [48.1]
 [48.1]] [[2.72]
 [2.72]
 [2.72]
 [2.72]
 [2.72]
 [2.72]
 [2.72]]
printing an ep nov before normalisation:  42.67354453283154
Printing some Q and Qe and total Qs values:  [[0.812]
 [0.812]
 [0.812]
 [0.812]
 [0.827]
 [0.812]
 [0.812]] [[36.671]
 [36.671]
 [36.671]
 [36.671]
 [33.597]
 [36.671]
 [36.671]] [[2.812]
 [2.812]
 [2.812]
 [2.812]
 [2.556]
 [2.812]
 [2.812]]
maxi score, test score, baseline:  0.19213999999999984 0.682 0.682
probs:  [0.10533136319925755, 0.05344573649229805, 0.22807529286437364, 0.0556756430381505, 0.3652145454343035, 0.1922574189716168]
actor:  1 policy actor:  1  step number:  31 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.19213999999999984 0.682 0.682
probs:  [0.10533136319925755, 0.05344573649229805, 0.22807529286437364, 0.0556756430381505, 0.3652145454343035, 0.1922574189716168]
printing an ep nov before normalisation:  30.760349835270215
maxi score, test score, baseline:  0.19213999999999984 0.682 0.682
probs:  [0.10533136319925755, 0.05344573649229805, 0.22807529286437364, 0.0556756430381505, 0.3652145454343035, 0.1922574189716168]
actor:  1 policy actor:  1  step number:  38 total reward:  0.2899999999999996  reward:  1.0 rdn_beta:  1.667
using another actor
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
Printing some Q and Qe and total Qs values:  [[0.135]
 [0.205]
 [0.135]
 [0.16 ]
 [0.086]
 [0.135]
 [0.172]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.135]
 [0.205]
 [0.135]
 [0.16 ]
 [0.086]
 [0.135]
 [0.172]]
maxi score, test score, baseline:  0.19213999999999984 0.682 0.682
probs:  [0.10533136319925755, 0.05344573649229805, 0.22807529286437364, 0.0556756430381505, 0.3652145454343035, 0.1922574189716168]
Printing some Q and Qe and total Qs values:  [[0.151]
 [0.25 ]
 [0.151]
 [0.163]
 [0.171]
 [0.151]
 [0.151]] [[31.219]
 [35.716]
 [31.219]
 [29.809]
 [31.261]
 [31.219]
 [31.219]] [[0.151]
 [0.25 ]
 [0.151]
 [0.163]
 [0.171]
 [0.151]
 [0.151]]
printing an ep nov before normalisation:  22.7915895626723
maxi score, test score, baseline:  0.19213999999999984 0.682 0.682
probs:  [0.10533136319925755, 0.05344573649229805, 0.22807529286437364, 0.0556756430381505, 0.3652145454343035, 0.1922574189716168]
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.514]
 [0.744]
 [0.514]
 [0.514]
 [0.514]
 [0.368]] [[35.494]
 [35.494]
 [45.602]
 [35.494]
 [35.494]
 [35.494]
 [44.426]] [[0.514]
 [0.514]
 [0.744]
 [0.514]
 [0.514]
 [0.514]
 [0.368]]
maxi score, test score, baseline:  0.19213999999999984 0.682 0.682
maxi score, test score, baseline:  0.19213999999999984 0.682 0.682
probs:  [0.10533136319925755, 0.05344573649229805, 0.22807529286437364, 0.0556756430381505, 0.3652145454343035, 0.1922574189716168]
actor:  0 policy actor:  0  step number:  32 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.19177999999999987 0.682 0.682
probs:  [0.10533136319925755, 0.05344573649229805, 0.22807529286437364, 0.0556756430381505, 0.3652145454343035, 0.1922574189716168]
maxi score, test score, baseline:  0.19177999999999987 0.682 0.682
probs:  [0.10533136319925755, 0.05344573649229805, 0.22807529286437364, 0.0556756430381505, 0.3652145454343035, 0.1922574189716168]
actor:  1 policy actor:  1  step number:  37 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  29 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
siam score:  -0.7833549
printing an ep nov before normalisation:  41.07282110782017
maxi score, test score, baseline:  0.19159999999999985 0.682 0.682
probs:  [0.10533136319925755, 0.05344573649229805, 0.22807529286437364, 0.0556756430381505, 0.3652145454343035, 0.1922574189716168]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19159999999999985 0.682 0.682
actor:  1 policy actor:  1  step number:  26 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.19159999999999985 0.682 0.682
probs:  [0.10521975554464771, 0.0534523920488018, 0.228103749249338, 0.0556825769771405, 0.3652601223421771, 0.1922814038378948]
maxi score, test score, baseline:  0.19159999999999985 0.682 0.682
probs:  [0.10521975554464771, 0.0534523920488018, 0.228103749249338, 0.0556825769771405, 0.3652601223421771, 0.1922814038378948]
maxi score, test score, baseline:  0.19159999999999985 0.682 0.682
actor:  1 policy actor:  1  step number:  28 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.451]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]] [[36.881]
 [45.834]
 [36.881]
 [36.881]
 [36.881]
 [36.881]
 [36.881]] [[1.316]
 [1.609]
 [1.316]
 [1.316]
 [1.316]
 [1.316]
 [1.316]]
printing an ep nov before normalisation:  31.10342502593994
actor:  1 policy actor:  1  step number:  26 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.19159999999999985 0.682 0.682
probs:  [0.10510865550897315, 0.053459017334205244, 0.22813207620769735, 0.05568947937887886, 0.36530549195511636, 0.192305279615129]
printing an ep nov before normalisation:  25.971182651376395
maxi score, test score, baseline:  0.19159999999999985 0.682 0.682
printing an ep nov before normalisation:  31.422339593634934
actor:  0 policy actor:  0  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.529]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]] [[20.904]
 [30.284]
 [20.904]
 [20.904]
 [20.904]
 [20.904]
 [20.904]] [[0.553]
 [0.689]
 [0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.553]]
maxi score, test score, baseline:  0.19133999999999984 0.682 0.682
maxi score, test score, baseline:  0.19133999999999984 0.682 0.682
probs:  [0.10510865550897315, 0.053459017334205244, 0.22813207620769735, 0.05568947937887886, 0.36530549195511636, 0.192305279615129]
maxi score, test score, baseline:  0.19133999999999984 0.682 0.682
probs:  [0.10510865550897315, 0.053459017334205244, 0.22813207620769735, 0.05568947937887886, 0.36530549195511636, 0.192305279615129]
actor:  1 policy actor:  1  step number:  24 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.781]
 [0.781]
 [0.781]
 [0.781]
 [0.781]
 [0.781]
 [0.781]] [[46.647]
 [46.647]
 [46.647]
 [46.647]
 [46.647]
 [46.647]
 [46.647]] [[1.017]
 [1.017]
 [1.017]
 [1.017]
 [1.017]
 [1.017]
 [1.017]]
maxi score, test score, baseline:  0.19133999999999984 0.682 0.682
probs:  [0.10510865550897315, 0.053459017334205244, 0.22813207620769735, 0.05568947937887886, 0.36530549195511636, 0.192305279615129]
printing an ep nov before normalisation:  26.211712219406376
actor:  1 policy actor:  1  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.19133999999999984 0.682 0.682
maxi score, test score, baseline:  0.19133999999999984 0.682 0.682
probs:  [0.10510865550897315, 0.053459017334205244, 0.22813207620769735, 0.05568947937887886, 0.36530549195511636, 0.192305279615129]
actor:  1 policy actor:  1  step number:  46 total reward:  0.1899999999999995  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.19133999999999984 0.682 0.682
probs:  [0.10499805963692879, 0.05346561255456041, 0.22816027462044403, 0.05569635045803529, 0.3653506556841546, 0.19232904704587686]
siam score:  -0.7724001
printing an ep nov before normalisation:  21.045248746846948
maxi score, test score, baseline:  0.19133999999999984 0.682 0.682
probs:  [0.10499805963692879, 0.05346561255456041, 0.22816027462044403, 0.05569635045803529, 0.3653506556841546, 0.19232904704587686]
printing an ep nov before normalisation:  29.75179823282897
line 256 mcts: sample exp_bonus 37.1295408399912
actor:  1 policy actor:  1  step number:  33 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.19133999999999984 0.682 0.682
actor:  1 policy actor:  1  step number:  28 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.18799999999999986 0.682 0.682
printing an ep nov before normalisation:  50.70977696148712
Printing some Q and Qe and total Qs values:  [[0.66]
 [0.66]
 [0.66]
 [0.66]
 [0.66]
 [0.66]
 [0.66]] [[46.738]
 [46.738]
 [46.738]
 [46.738]
 [46.738]
 [46.738]
 [46.738]] [[1.844]
 [1.844]
 [1.844]
 [1.844]
 [1.844]
 [1.844]
 [1.844]]
Printing some Q and Qe and total Qs values:  [[0.756]
 [0.747]
 [0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]] [[42.475]
 [46.589]
 [42.475]
 [42.475]
 [42.475]
 [42.475]
 [42.475]] [[1.84 ]
 [2.005]
 [1.84 ]
 [1.84 ]
 [1.84 ]
 [1.84 ]
 [1.84 ]]
maxi score, test score, baseline:  0.18799999999999986 0.682 0.682
probs:  [0.10477836671860266, 0.053478713615025375, 0.2282162892932701, 0.05570999949758857, 0.3654403710709181, 0.19237625980459527]
Printing some Q and Qe and total Qs values:  [[0.696]
 [0.765]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]] [[45.339]
 [50.109]
 [45.339]
 [45.339]
 [45.339]
 [45.339]
 [45.339]] [[1.74 ]
 [2.006]
 [1.74 ]
 [1.74 ]
 [1.74 ]
 [1.74 ]
 [1.74 ]]
maxi score, test score, baseline:  0.18799999999999986 0.682 0.682
probs:  [0.10477836671860266, 0.053478713615025375, 0.2282162892932701, 0.05570999949758857, 0.3654403710709181, 0.19237625980459527]
maxi score, test score, baseline:  0.18799999999999986 0.682 0.682
probs:  [0.10477836671860266, 0.053478713615025375, 0.2282162892932701, 0.05570999949758857, 0.3654403710709181, 0.19237625980459527]
maxi score, test score, baseline:  0.18799999999999986 0.682 0.682
probs:  [0.10477836671860266, 0.053478713615025375, 0.2282162892932701, 0.05570999949758857, 0.3654403710709181, 0.19237625980459527]
maxi score, test score, baseline:  0.18799999999999986 0.682 0.682
probs:  [0.10477836671860266, 0.053478713615025375, 0.2282162892932701, 0.05570999949758857, 0.3654403710709181, 0.19237625980459527]
printing an ep nov before normalisation:  0.4020322223524886
actor:  1 policy actor:  1  step number:  41 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18799999999999986 0.682 0.682
probs:  [0.10477836671860266, 0.053478713615025375, 0.2282162892932701, 0.05570999949758857, 0.3654403710709181, 0.19237625980459527]
actor:  1 policy actor:  1  step number:  43 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.18799999999999986 0.682 0.682
probs:  [0.10477836671860266, 0.053478713615025375, 0.2282162892932701, 0.05570999949758857, 0.3654403710709181, 0.19237625980459527]
printing an ep nov before normalisation:  52.69608248933211
maxi score, test score, baseline:  0.18799999999999986 0.682 0.682
maxi score, test score, baseline:  0.18799999999999986 0.682 0.682
probs:  [0.10477836671860266, 0.053478713615025375, 0.2282162892932701, 0.05570999949758857, 0.3654403710709181, 0.19237625980459527]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.18799999999999986 0.682 0.682
probs:  [0.10477836671860266, 0.053478713615025375, 0.2282162892932701, 0.05570999949758857, 0.3654403710709181, 0.19237625980459527]
maxi score, test score, baseline:  0.18799999999999986 0.682 0.682
printing an ep nov before normalisation:  36.16865839128305
printing an ep nov before normalisation:  32.40041732788086
actor:  1 policy actor:  1  step number:  35 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  0.00026180454881341575
actor:  1 policy actor:  1  step number:  40 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  26 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.18797999999999984 0.682 0.682
probs:  [0.10477836671860266, 0.053478713615025375, 0.2282162892932701, 0.05570999949758857, 0.3654403710709181, 0.19237625980459527]
maxi score, test score, baseline:  0.18797999999999984 0.682 0.682
maxi score, test score, baseline:  0.18797999999999984 0.682 0.682
probs:  [0.10477836671860266, 0.053478713615025375, 0.2282162892932701, 0.05570999949758857, 0.3654403710709181, 0.19237625980459527]
maxi score, test score, baseline:  0.18797999999999984 0.682 0.682
probs:  [0.10477836671860266, 0.053478713615025375, 0.2282162892932701, 0.05570999949758857, 0.3654403710709181, 0.19237625980459527]
printing an ep nov before normalisation:  49.87791752349159
maxi score, test score, baseline:  0.18797999999999984 0.682 0.682
maxi score, test score, baseline:  0.18797999999999984 0.682 0.682
probs:  [0.10477836671860266, 0.053478713615025375, 0.2282162892932701, 0.05570999949758857, 0.3654403710709181, 0.19237625980459527]
maxi score, test score, baseline:  0.18797999999999984 0.682 0.682
probs:  [0.10477836671860266, 0.053478713615025375, 0.2282162892932701, 0.05570999949758857, 0.3654403710709181, 0.19237625980459527]
maxi score, test score, baseline:  0.18797999999999984 0.682 0.682
probs:  [0.10477836671860266, 0.053478713615025375, 0.2282162892932701, 0.05570999949758857, 0.3654403710709181, 0.19237625980459527]
actor:  1 policy actor:  1  step number:  23 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18797999999999984 0.682 0.682
probs:  [0.10477836671860266, 0.053478713615025375, 0.2282162892932701, 0.05570999949758857, 0.3654403710709181, 0.19237625980459527]
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.382]
 [0.311]
 [0.311]
 [0.324]
 [0.312]
 [0.305]] [[31.748]
 [35.849]
 [30.752]
 [30.76 ]
 [30.89 ]
 [30.489]
 [30.536]] [[0.71 ]
 [0.885]
 [0.691]
 [0.691]
 [0.707]
 [0.686]
 [0.68 ]]
actor:  1 policy actor:  1  step number:  34 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([0.7449, 0.0029, 0.0171, 0.0198, 0.1409, 0.0192, 0.0553],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0054,     0.9102,     0.0153,     0.0167,     0.0006,     0.0062,
            0.0457], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0556, 0.0088, 0.6778, 0.0482, 0.0509, 0.1051, 0.0536],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1611, 0.0022, 0.1643, 0.1483, 0.1644, 0.1985, 0.1612],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1983, 0.0011, 0.0795, 0.0674, 0.4858, 0.0884, 0.0795],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0580, 0.0021, 0.1360, 0.0634, 0.0797, 0.5814, 0.0795],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0521, 0.1158, 0.0822, 0.0440, 0.0416, 0.0758, 0.5885],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  36 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  23 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.18797999999999984 0.682 0.682
probs:  [0.10477836671860266, 0.053478713615025375, 0.2282162892932701, 0.05570999949758857, 0.3654403710709181, 0.19237625980459527]
printing an ep nov before normalisation:  49.92768031207466
maxi score, test score, baseline:  0.18797999999999984 0.682 0.682
probs:  [0.10477836671860266, 0.053478713615025375, 0.2282162892932701, 0.05570999949758857, 0.3654403710709181, 0.19237625980459527]
printing an ep nov before normalisation:  0.9062268271532048
maxi score, test score, baseline:  0.18797999999999984 0.682 0.682
maxi score, test score, baseline:  0.18797999999999984 0.682 0.682
probs:  [0.10477836671860266, 0.053478713615025375, 0.2282162892932701, 0.05570999949758857, 0.3654403710709181, 0.19237625980459527]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  35 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.18797999999999984 0.682 0.682
probs:  [0.10477836671860266, 0.053478713615025375, 0.2282162892932701, 0.05570999949758857, 0.3654403710709181, 0.19237625980459527]
actions average: 
K:  2  action  0 :  tensor([0.8118, 0.0340, 0.0327, 0.0324, 0.0251, 0.0311, 0.0330],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0082, 0.8727, 0.0192, 0.0371, 0.0025, 0.0050, 0.0553],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0219, 0.0049, 0.8724, 0.0265, 0.0209, 0.0310, 0.0224],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0596, 0.0090, 0.0318, 0.5773, 0.0345, 0.0501, 0.2377],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1079, 0.0030, 0.1009, 0.1434, 0.4429, 0.1142, 0.0877],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0945, 0.0764, 0.1274, 0.1244, 0.1011, 0.3680, 0.1083],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0585, 0.3827, 0.0802, 0.0952, 0.0437, 0.0591, 0.2807],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  29 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.18797999999999984 0.682 0.682
probs:  [0.10477836671860266, 0.053478713615025375, 0.2282162892932701, 0.05570999949758857, 0.3654403710709181, 0.19237625980459527]
maxi score, test score, baseline:  0.18797999999999984 0.682 0.682
probs:  [0.10477836671860266, 0.053478713615025375, 0.2282162892932701, 0.05570999949758857, 0.3654403710709181, 0.19237625980459527]
printing an ep nov before normalisation:  33.8094262015214
maxi score, test score, baseline:  0.18797999999999984 0.682 0.682
probs:  [0.10477836671860266, 0.053478713615025375, 0.2282162892932701, 0.05570999949758857, 0.3654403710709181, 0.19237625980459527]
printing an ep nov before normalisation:  0.000472843660190847
line 256 mcts: sample exp_bonus 33.120983040507845
maxi score, test score, baseline:  0.18797999999999984 0.682 0.682
probs:  [0.10477836671860266, 0.053478713615025375, 0.2282162892932701, 0.05570999949758857, 0.3654403710709181, 0.19237625980459527]
actor:  1 policy actor:  1  step number:  38 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  1.667
using another actor
maxi score, test score, baseline:  0.18797999999999984 0.682 0.682
probs:  [0.10477836671860266, 0.053478713615025375, 0.2282162892932701, 0.05570999949758857, 0.3654403710709181, 0.19237625980459527]
printing an ep nov before normalisation:  28.71927724200116
printing an ep nov before normalisation:  38.84451301105239
printing an ep nov before normalisation:  20.22505283355713
printing an ep nov before normalisation:  41.24764702265122
printing an ep nov before normalisation:  43.16591327770844
Printing some Q and Qe and total Qs values:  [[0.274]
 [0.361]
 [0.274]
 [0.274]
 [0.274]
 [0.274]
 [0.274]] [[25.451]
 [31.777]
 [25.451]
 [25.451]
 [25.451]
 [25.451]
 [25.451]] [[0.647]
 [0.911]
 [0.647]
 [0.647]
 [0.647]
 [0.647]
 [0.647]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  30 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  48.88454662298779
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.068]
 [0.149]
 [0.068]
 [0.068]
 [0.068]
 [0.068]
 [0.068]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.068]
 [0.149]
 [0.068]
 [0.068]
 [0.068]
 [0.068]
 [0.068]]
maxi score, test score, baseline:  0.18797999999999984 0.682 0.682
Printing some Q and Qe and total Qs values:  [[0.796]
 [0.867]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]] [[28.226]
 [38.124]
 [28.226]
 [28.226]
 [28.226]
 [28.226]
 [28.226]] [[0.796]
 [0.867]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]]
Printing some Q and Qe and total Qs values:  [[0.233]
 [0.235]
 [0.233]
 [0.233]
 [0.233]
 [0.233]
 [0.233]] [[29.817]
 [31.351]
 [29.817]
 [29.817]
 [29.817]
 [29.817]
 [29.817]] [[1.158]
 [1.25 ]
 [1.158]
 [1.158]
 [1.158]
 [1.158]
 [1.158]]
maxi score, test score, baseline:  0.18797999999999984 0.682 0.682
probs:  [0.10477836671860266, 0.053478713615025375, 0.2282162892932701, 0.05570999949758857, 0.3654403710709181, 0.19237625980459527]
Printing some Q and Qe and total Qs values:  [[0.758]
 [0.877]
 [0.758]
 [0.758]
 [0.758]
 [0.758]
 [0.758]] [[26.471]
 [42.659]
 [26.471]
 [26.471]
 [26.471]
 [26.471]
 [26.471]] [[0.758]
 [0.877]
 [0.758]
 [0.758]
 [0.758]
 [0.758]
 [0.758]]
printing an ep nov before normalisation:  55.333449668890964
Printing some Q and Qe and total Qs values:  [[0.925]
 [0.915]
 [0.883]
 [0.883]
 [0.863]
 [0.883]
 [0.848]] [[48.483]
 [50.194]
 [59.997]
 [59.997]
 [54.152]
 [59.997]
 [62.52 ]] [[0.925]
 [0.915]
 [0.883]
 [0.883]
 [0.863]
 [0.883]
 [0.848]]
Printing some Q and Qe and total Qs values:  [[0.925]
 [0.899]
 [0.859]
 [0.859]
 [0.871]
 [0.859]
 [0.859]] [[49.648]
 [50.931]
 [62.208]
 [62.208]
 [57.502]
 [62.208]
 [62.208]] [[0.925]
 [0.899]
 [0.859]
 [0.859]
 [0.871]
 [0.859]
 [0.859]]
printing an ep nov before normalisation:  40.910968304757596
printing an ep nov before normalisation:  44.066723546582736
printing an ep nov before normalisation:  55.222163250538436
maxi score, test score, baseline:  0.18797999999999984 0.682 0.682
probs:  [0.10477836671860266, 0.053478713615025375, 0.2282162892932701, 0.05570999949758857, 0.3654403710709181, 0.19237625980459527]
printing an ep nov before normalisation:  46.75462239140259
actor:  1 policy actor:  1  step number:  33 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  43.61651239679554
actor:  1 policy actor:  1  step number:  29 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  30.700032547625824
maxi score, test score, baseline:  0.18797999999999984 0.682 0.682
probs:  [0.10477836671860266, 0.053478713615025375, 0.2282162892932701, 0.05570999949758857, 0.3654403710709181, 0.19237625980459527]
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  0.6024757304730599
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  54.86802935327779
maxi score, test score, baseline:  0.20935999999999988 0.6789999999999999 0.6789999999999999
probs:  [0.11116042764620807, 0.052479597322866486, 0.22040166019878388, 0.056135288199180206, 0.35961756029184483, 0.20020546634111647]
printing an ep nov before normalisation:  36.49399275014263
printing an ep nov before normalisation:  40.28730448111311
printing an ep nov before normalisation:  14.812303767095646
actor:  1 policy actor:  1  step number:  27 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.20935999999999988 0.6789999999999999 0.6789999999999999
probs:  [0.11116042764620807, 0.052479597322866486, 0.22040166019878388, 0.056135288199180206, 0.35961756029184483, 0.20020546634111647]
actor:  1 policy actor:  1  step number:  49 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
Printing some Q and Qe and total Qs values:  [[0.392]
 [0.362]
 [0.284]
 [0.282]
 [0.281]
 [0.392]
 [0.392]] [[30.734]
 [30.785]
 [29.432]
 [29.61 ]
 [29.414]
 [30.734]
 [30.734]] [[1.151]
 [1.124]
 [0.984]
 [0.99 ]
 [0.981]
 [1.151]
 [1.151]]
actor:  1 policy actor:  1  step number:  38 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.20935999999999988 0.6789999999999999 0.6789999999999999
probs:  [0.11116042764620807, 0.052479597322866486, 0.22040166019878388, 0.056135288199180206, 0.35961756029184483, 0.20020546634111647]
Printing some Q and Qe and total Qs values:  [[0.694]
 [0.62 ]
 [0.578]
 [0.578]
 [0.676]
 [0.578]
 [0.609]] [[39.008]
 [36.938]
 [32.911]
 [32.911]
 [39.834]
 [32.911]
 [36.022]] [[1.378]
 [1.243]
 [1.084]
 [1.084]
 [1.384]
 [1.084]
 [1.205]]
maxi score, test score, baseline:  0.20935999999999988 0.6789999999999999 0.6789999999999999
probs:  [0.11116042764620807, 0.052479597322866486, 0.22040166019878388, 0.056135288199180206, 0.35961756029184483, 0.20020546634111647]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  23 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20935999999999988 0.6789999999999999 0.6789999999999999
probs:  [0.11116042764620807, 0.052479597322866486, 0.22040166019878388, 0.056135288199180206, 0.35961756029184483, 0.20020546634111647]
maxi score, test score, baseline:  0.20935999999999988 0.6789999999999999 0.6789999999999999
probs:  [0.11116042764620807, 0.052479597322866486, 0.22040166019878388, 0.056135288199180206, 0.35961756029184483, 0.20020546634111647]
actor:  0 policy actor:  0  step number:  26 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.20931999999999987 0.6789999999999999 0.6789999999999999
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.477]] [[31.251]
 [31.251]
 [31.251]
 [31.251]
 [31.251]
 [31.251]
 [31.251]] [[1.282]
 [1.282]
 [1.282]
 [1.282]
 [1.282]
 [1.282]
 [1.282]]
actor:  1 policy actor:  1  step number:  22 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.239]
 [0.239]
 [0.358]
 [0.239]
 [0.239]
 [0.239]
 [0.239]] [[39.171]
 [39.171]
 [38.291]
 [39.171]
 [39.171]
 [39.171]
 [39.171]] [[0.671]
 [0.671]
 [0.771]
 [0.671]
 [0.671]
 [0.671]
 [0.671]]
printing an ep nov before normalisation:  60.903990795578814
maxi score, test score, baseline:  0.20931999999999987 0.6789999999999999 0.6789999999999999
probs:  [0.11116042764620807, 0.052479597322866486, 0.22040166019878388, 0.056135288199180206, 0.35961756029184483, 0.20020546634111647]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  2  action  0 :  tensor([0.5285, 0.0134, 0.0916, 0.1008, 0.0880, 0.0866, 0.0911],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0011,     0.9768,     0.0028,     0.0014,     0.0004,     0.0008,
            0.0168], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0490, 0.0222, 0.6892, 0.0595, 0.0284, 0.0720, 0.0797],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1154, 0.0843, 0.1010, 0.3407, 0.1203, 0.1105, 0.1276],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1868, 0.0066, 0.0845, 0.1252, 0.3908, 0.1024, 0.1037],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0948, 0.0037, 0.1023, 0.0987, 0.0816, 0.5285, 0.0904],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1426, 0.0089, 0.1623, 0.1337, 0.1086, 0.1452, 0.2988],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  1  step number:  29 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20879999999999987 0.6789999999999999 0.6789999999999999
probs:  [0.11116042764620807, 0.052479597322866486, 0.22040166019878388, 0.056135288199180206, 0.35961756029184483, 0.20020546634111647]
maxi score, test score, baseline:  0.20879999999999987 0.6789999999999999 0.6789999999999999
probs:  [0.11116042764620807, 0.052479597322866486, 0.22040166019878388, 0.056135288199180206, 0.35961756029184483, 0.20020546634111647]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.20879999999999987 0.6789999999999999 0.6789999999999999
actor:  1 policy actor:  1  step number:  27 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20879999999999987 0.6789999999999999 0.6789999999999999
probs:  [0.11116042764620807, 0.052479597322866486, 0.22040166019878388, 0.056135288199180206, 0.35961756029184483, 0.20020546634111647]
actor:  1 policy actor:  1  step number:  30 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]] [[43.287]
 [43.287]
 [43.287]
 [43.287]
 [43.287]
 [43.287]
 [43.287]] [[1.777]
 [1.777]
 [1.777]
 [1.777]
 [1.777]
 [1.777]
 [1.777]]
printing an ep nov before normalisation:  31.752187358246122
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  41 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20879999999999987 0.6789999999999999 0.6789999999999999
probs:  [0.11103651380526747, 0.052486900445831985, 0.22043239102954332, 0.056143101347190416, 0.3596677138796367, 0.2002333794925302]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.715]
 [0.715]
 [0.716]
 [0.715]
 [0.715]
 [0.715]
 [0.715]] [[33.395]
 [33.395]
 [34.822]
 [33.395]
 [33.395]
 [33.395]
 [33.395]] [[2.572]
 [2.572]
 [2.716]
 [2.572]
 [2.572]
 [2.572]
 [2.572]]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  37.1393898576128
Printing some Q and Qe and total Qs values:  [[0.72 ]
 [0.807]
 [0.721]
 [0.72 ]
 [0.719]
 [0.719]
 [0.72 ]] [[23.248]
 [27.41 ]
 [24.134]
 [23.248]
 [24.297]
 [24.288]
 [23.248]] [[1.998]
 [2.314]
 [2.048]
 [1.998]
 [2.055]
 [2.055]
 [1.998]]
actor:  1 policy actor:  1  step number:  30 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20879999999999987 0.6789999999999999 0.6789999999999999
probs:  [0.11103651380526747, 0.052486900445831985, 0.22043239102954332, 0.056143101347190416, 0.3596677138796367, 0.2002333794925302]
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.24295143127615
siam score:  -0.77690095
Printing some Q and Qe and total Qs values:  [[0.715]
 [0.738]
 [0.688]
 [0.715]
 [0.658]
 [0.715]
 [0.715]] [[51.379]
 [51.958]
 [53.393]
 [51.379]
 [51.951]
 [51.379]
 [51.379]] [[0.981]
 [1.009]
 [0.968]
 [0.981]
 [0.928]
 [0.981]
 [0.981]]
actor:  1 policy actor:  1  step number:  31 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.20879999999999987 0.6789999999999999 0.6789999999999999
probs:  [0.11103651380526747, 0.052486900445831985, 0.22043239102954332, 0.056143101347190416, 0.3596677138796367, 0.2002333794925302]
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.491]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]] [[30.458]
 [41.791]
 [30.458]
 [30.458]
 [30.458]
 [30.458]
 [30.458]] [[1.156]
 [1.656]
 [1.156]
 [1.156]
 [1.156]
 [1.156]
 [1.156]]
maxi score, test score, baseline:  0.20879999999999987 0.6789999999999999 0.6789999999999999
probs:  [0.11103651380526747, 0.052486900445831985, 0.22043239102954332, 0.056143101347190416, 0.3596677138796367, 0.2002333794925302]
maxi score, test score, baseline:  0.20879999999999987 0.6789999999999999 0.6789999999999999
probs:  [0.11103651380526747, 0.052486900445831985, 0.22043239102954332, 0.056143101347190416, 0.3596677138796367, 0.2002333794925302]
actor:  1 policy actor:  1  step number:  36 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  38.90741821775942
actor:  1 policy actor:  1  step number:  46 total reward:  0.20999999999999963  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  29 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.963]
 [0.96 ]
 [0.963]
 [0.963]
 [0.963]
 [0.963]
 [0.963]] [[32.701]
 [41.86 ]
 [32.701]
 [32.701]
 [32.701]
 [32.701]
 [32.701]] [[0.963]
 [0.96 ]
 [0.963]
 [0.963]
 [0.963]
 [0.963]
 [0.963]]
maxi score, test score, baseline:  0.20585999999999988 0.6789999999999999 0.6789999999999999
probs:  [0.11091315289858972, 0.05249417098045315, 0.220462984731721, 0.05615087963099832, 0.35971764366969206, 0.2002611680885457]
actor:  1 policy actor:  1  step number:  38 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.0
from probs:  [0.11091315289858972, 0.05249417098045315, 0.220462984731721, 0.05615087963099832, 0.35971764366969206, 0.2002611680885457]
printing an ep nov before normalisation:  56.70773064219514
maxi score, test score, baseline:  0.20585999999999988 0.6789999999999999 0.6789999999999999
probs:  [0.11079034123341867, 0.052501409144370396, 0.22049344222112635, 0.05615862328344341, 0.3597673511566384, 0.20028883296100272]
maxi score, test score, baseline:  0.20585999999999988 0.6789999999999999 0.6789999999999999
probs:  [0.11079034123341867, 0.052501409144370396, 0.22049344222112635, 0.05615862328344341, 0.3597673511566384, 0.20028883296100272]
siam score:  -0.7805266
maxi score, test score, baseline:  0.20585999999999988 0.6789999999999999 0.6789999999999999
Printing some Q and Qe and total Qs values:  [[0.835]
 [0.217]
 [0.882]
 [0.735]
 [0.835]
 [0.184]
 [0.795]] [[31.585]
 [42.758]
 [40.731]
 [39.942]
 [31.585]
 [37.873]
 [45.503]] [[0.835]
 [0.217]
 [0.882]
 [0.735]
 [0.835]
 [0.184]
 [0.795]]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20293999999999984 0.6789999999999999 0.6789999999999999
probs:  [0.11079034123341867, 0.052501409144370396, 0.22049344222112635, 0.05615862328344341, 0.3597673511566384, 0.20028883296100272]
actor:  1 policy actor:  1  step number:  28 total reward:  0.73  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  46.8689478157572
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.502]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]] [[44.546]
 [41.292]
 [37.625]
 [37.625]
 [37.625]
 [37.625]
 [37.625]] [[1.72 ]
 [1.585]
 [1.452]
 [1.452]
 [1.452]
 [1.452]
 [1.452]]
maxi score, test score, baseline:  0.20293999999999984 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  0.20293999999999984 0.6789999999999999 0.6789999999999999
probs:  [0.10278047061418144, 0.04871075937464205, 0.2768694923165721, 0.05210324754270017, 0.33373538005164183, 0.18580065010026242]
actor:  1 policy actor:  1  step number:  33 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.322]
 [0.404]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.322]
 [0.404]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  48.31641348252499
maxi score, test score, baseline:  0.20293999999999984 0.6789999999999999 0.6789999999999999
probs:  [0.10278047061418144, 0.04871075937464205, 0.2768694923165721, 0.05210324754270017, 0.33373538005164183, 0.18580065010026242]
siam score:  -0.7773776
maxi score, test score, baseline:  0.20293999999999984 0.6789999999999999 0.6789999999999999
probs:  [0.10278047061418144, 0.04871075937464205, 0.2768694923165721, 0.05210324754270017, 0.33373538005164183, 0.18580065010026242]
printing an ep nov before normalisation:  43.725733203296315
printing an ep nov before normalisation:  48.763383848626674
actor:  1 policy actor:  1  step number:  34 total reward:  0.3299999999999996  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  30 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.749]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]] [[48.215]
 [49.994]
 [49.994]
 [49.994]
 [49.994]
 [49.994]
 [49.994]] [[2.183]
 [2.022]
 [2.022]
 [2.022]
 [2.022]
 [2.022]
 [2.022]]
actor:  1 policy actor:  1  step number:  28 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 37.61025104390631
actor:  1 policy actor:  1  step number:  29 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  26 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
from probs:  [0.10255210580563427, 0.04872313275808044, 0.2769399809507261, 0.05211648504077435, 0.3338203532303067, 0.18584794221447806]
printing an ep nov before normalisation:  43.71071476262153
maxi score, test score, baseline:  0.20293999999999984 0.6789999999999999 0.6789999999999999
probs:  [0.10255210580563427, 0.04872313275808044, 0.2769399809507261, 0.05211648504077435, 0.3338203532303067, 0.18584794221447806]
Printing some Q and Qe and total Qs values:  [[0.877]
 [0.964]
 [0.877]
 [0.877]
 [0.877]
 [0.877]
 [0.877]] [[24.829]
 [32.113]
 [24.829]
 [24.829]
 [24.829]
 [24.829]
 [24.829]] [[0.877]
 [0.964]
 [0.877]
 [0.877]
 [0.877]
 [0.877]
 [0.877]]
maxi score, test score, baseline:  0.20293999999999984 0.6789999999999999 0.6789999999999999
actor:  1 policy actor:  1  step number:  35 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  25.294353085347684
actor:  0 policy actor:  0  step number:  29 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  49.09279623614657
siam score:  -0.7751167
printing an ep nov before normalisation:  39.0315452609827
maxi score, test score, baseline:  0.20259999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.10255210580563427, 0.04872313275808044, 0.2769399809507261, 0.05211648504077435, 0.3338203532303067, 0.18584794221447806]
maxi score, test score, baseline:  0.20259999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.10255210580563427, 0.04872313275808044, 0.2769399809507261, 0.05211648504077435, 0.3338203532303067, 0.18584794221447806]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  44.128284870740906
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.603]
 [0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]] [[29.529]
 [33.325]
 [29.529]
 [29.529]
 [29.529]
 [29.529]
 [29.529]] [[0.99 ]
 [1.132]
 [0.99 ]
 [0.99 ]
 [0.99 ]
 [0.99 ]
 [0.99 ]]
maxi score, test score, baseline:  0.20259999999999986 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  0.20259999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.10255210580563427, 0.04872313275808044, 0.2769399809507261, 0.05211648504077435, 0.3338203532303067, 0.18584794221447806]
actor:  1 policy actor:  1  step number:  32 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20259999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.10255210580563427, 0.04872313275808044, 0.2769399809507261, 0.05211648504077435, 0.3338203532303067, 0.18584794221447806]
actor:  0 policy actor:  0  step number:  33 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  36.08174020616666
actor:  1 policy actor:  1  step number:  32 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  48.62394319937685
siam score:  -0.77862275
maxi score, test score, baseline:  0.20243999999999984 0.6789999999999999 0.6789999999999999
probs:  [0.10255210580563427, 0.04872313275808044, 0.2769399809507261, 0.05211648504077435, 0.3338203532303067, 0.18584794221447806]
maxi score, test score, baseline:  0.20243999999999984 0.6789999999999999 0.6789999999999999
probs:  [0.10255210580563427, 0.04872313275808044, 0.2769399809507261, 0.05211648504077435, 0.3338203532303067, 0.18584794221447806]
printing an ep nov before normalisation:  47.194026989564854
maxi score, test score, baseline:  0.20243999999999984 0.6789999999999999 0.6789999999999999
probs:  [0.10255210580563427, 0.04872313275808044, 0.2769399809507261, 0.05211648504077435, 0.3338203532303067, 0.18584794221447806]
Printing some Q and Qe and total Qs values:  [[0.736]
 [0.794]
 [0.767]
 [0.767]
 [0.736]
 [0.736]
 [0.767]] [[13.054]
 [23.249]
 [14.184]
 [14.269]
 [13.054]
 [13.054]
 [14.368]] [[1.779]
 [2.651]
 [1.899]
 [1.906]
 [1.779]
 [1.779]
 [1.914]]
actor:  1 policy actor:  1  step number:  36 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.20243999999999984 0.6789999999999999 0.6789999999999999
probs:  [0.10255210580563427, 0.04872313275808044, 0.2769399809507261, 0.05211648504077435, 0.3338203532303067, 0.18584794221447806]
actor:  1 policy actor:  1  step number:  31 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.097]
 [-0.05 ]
 [ 0.607]
 [ 0.462]
 [ 0.462]
 [ 0.462]
 [ 0.462]] [[42.624]
 [42.786]
 [44.217]
 [39.663]
 [39.663]
 [39.663]
 [39.663]] [[0.936]
 [0.991]
 [1.714]
 [1.357]
 [1.357]
 [1.357]
 [1.357]]
maxi score, test score, baseline:  0.20243999999999984 0.6789999999999999 0.6789999999999999
probs:  [0.10255210580563427, 0.04872313275808044, 0.2769399809507261, 0.05211648504077435, 0.3338203532303067, 0.18584794221447806]
printing an ep nov before normalisation:  55.02699174388278
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  32 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  44.656189259098205
siam score:  -0.7763411
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20243999999999984 0.6789999999999999 0.6789999999999999
probs:  [0.10255210580563427, 0.04872313275808044, 0.2769399809507261, 0.05211648504077435, 0.3338203532303067, 0.18584794221447806]
actor:  1 policy actor:  1  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.906]
 [0.789]
 [0.906]
 [0.906]
 [0.906]
 [0.906]
 [0.906]] [[62.883]
 [60.12 ]
 [62.883]
 [62.883]
 [62.883]
 [62.883]
 [62.883]] [[0.906]
 [0.789]
 [0.906]
 [0.906]
 [0.906]
 [0.906]
 [0.906]]
actor:  1 policy actor:  1  step number:  27 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.20243999999999984 0.6789999999999999 0.6789999999999999
probs:  [0.1024386842804384, 0.04872927822344507, 0.27697499040969453, 0.052123059684349715, 0.33386255670108794, 0.18587143070098439]
actor:  0 policy actor:  0  step number:  23 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.109]
 [0.142]
 [0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.109]
 [0.142]
 [0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]]
printing an ep nov before normalisation:  41.42186978813841
actor:  0 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  1  action  0 :  tensor([0.6551, 0.0305, 0.0537, 0.0506, 0.1008, 0.0582, 0.0512],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0022,     0.9652,     0.0033,     0.0030,     0.0006,     0.0009,
            0.0248], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0178, 0.0041, 0.8207, 0.0429, 0.0254, 0.0534, 0.0356],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1012, 0.1028, 0.1230, 0.2749, 0.1551, 0.1184, 0.1246],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2046, 0.0018, 0.1147, 0.1074, 0.3348, 0.1337, 0.1029],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0348, 0.0095, 0.0717, 0.0289, 0.0278, 0.7862, 0.0412],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0896, 0.1829, 0.0397, 0.0739, 0.0426, 0.0449, 0.5265],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  50.5615740818711
printing an ep nov before normalisation:  44.38555851993499
maxi score, test score, baseline:  0.20563999999999988 0.6789999999999999 0.6789999999999999
probs:  [0.1024386842804384, 0.04872927822344507, 0.27697499040969453, 0.052123059684349715, 0.33386255670108794, 0.18587143070098439]
printing an ep nov before normalisation:  56.52480642132606
maxi score, test score, baseline:  0.20563999999999988 0.6789999999999999 0.6789999999999999
probs:  [0.1024386842804384, 0.04872927822344507, 0.27697499040969453, 0.052123059684349715, 0.33386255670108794, 0.18587143070098439]
printing an ep nov before normalisation:  34.16102408810568
actor:  1 policy actor:  1  step number:  36 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.20563999999999988 0.6789999999999999 0.6789999999999999
probs:  [0.10232576551103235, 0.04873539644822821, 0.2770098446846435, 0.05212960518495499, 0.3339045730994481, 0.18589481507169278]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  25 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  45.918611765393
maxi score, test score, baseline:  0.20879999999999987 0.6789999999999999 0.6789999999999999
probs:  [0.10232576551103235, 0.04873539644822821, 0.2770098446846435, 0.05212960518495499, 0.3339045730994481, 0.18589481507169278]
actor:  0 policy actor:  0  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.016361772668965
maxi score, test score, baseline:  0.21199999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.10232576551103235, 0.04873539644822821, 0.2770098446846435, 0.05212960518495499, 0.3339045730994481, 0.18589481507169278]
printing an ep nov before normalisation:  36.21062618953532
maxi score, test score, baseline:  0.21199999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.10232576551103235, 0.04873539644822821, 0.2770098446846435, 0.05212960518495499, 0.3339045730994481, 0.18589481507169278]
maxi score, test score, baseline:  0.21199999999999986 0.6789999999999999 0.6789999999999999
printing an ep nov before normalisation:  15.227117326063228
actor:  1 policy actor:  1  step number:  40 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  1  step number:  23 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  32.146300280464395
printing an ep nov before normalisation:  49.61742928156876
maxi score, test score, baseline:  0.21239999999999987 0.6789999999999999 0.6789999999999999
probs:  [0.10232576551103235, 0.04873539644822821, 0.2770098446846435, 0.05212960518495499, 0.3339045730994481, 0.18589481507169278]
actor:  1 policy actor:  1  step number:  30 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  36.59261703491211
actor:  0 policy actor:  0  step number:  22 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
siam score:  -0.7809287
maxi score, test score, baseline:  0.21271999999999988 0.6789999999999999 0.6789999999999999
probs:  [0.10232576551103235, 0.04873539644822821, 0.2770098446846435, 0.05212960518495499, 0.3339045730994481, 0.18589481507169278]
printing an ep nov before normalisation:  44.97468199726521
maxi score, test score, baseline:  0.21271999999999988 0.6789999999999999 0.6789999999999999
probs:  [0.10232576551103235, 0.04873539644822821, 0.2770098446846435, 0.05212960518495499, 0.3339045730994481, 0.18589481507169278]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
using another actor
Sims:  50 1 epoch:  171997 pick best:  False frame count:  171997
actor:  0 policy actor:  1  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.302]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]] [[30.826]
 [30.501]
 [30.826]
 [30.826]
 [30.826]
 [30.826]
 [30.826]] [[0.649]
 [0.617]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]]
maxi score, test score, baseline:  0.21267999999999987 0.6789999999999999 0.6789999999999999
probs:  [0.10232576551103235, 0.04873539644822821, 0.2770098446846435, 0.05212960518495499, 0.3339045730994481, 0.18589481507169278]
actor:  1 policy actor:  1  step number:  39 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  1  step number:  21 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.21287999999999987 0.6789999999999999 0.6789999999999999
probs:  [0.10232576551103235, 0.04873539644822821, 0.2770098446846435, 0.05212960518495499, 0.3339045730994481, 0.18589481507169278]
maxi score, test score, baseline:  0.21287999999999987 0.6789999999999999 0.6789999999999999
probs:  [0.10232576551103235, 0.04873539644822821, 0.2770098446846435, 0.05212960518495499, 0.3339045730994481, 0.18589481507169278]
printing an ep nov before normalisation:  44.32194709777832
actor:  1 policy actor:  1  step number:  27 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.333
using another actor
actor:  1 policy actor:  1  step number:  30 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21287999999999987 0.6789999999999999 0.6789999999999999
probs:  [0.10221334616201208, 0.048741487613150565, 0.27704454480510143, 0.05213612173593154, 0.3339464036664714, 0.18591809601733303]
actor:  1 policy actor:  1  step number:  35 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21287999999999987 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  0.21287999999999987 0.6789999999999999 0.6789999999999999
probs:  [0.10221334616201208, 0.048741487613150565, 0.27704454480510143, 0.05213612173593154, 0.3339464036664714, 0.18591809601733303]
maxi score, test score, baseline:  0.20999999999999985 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  0.20999999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.10221334616201208, 0.048741487613150565, 0.27704454480510143, 0.05213612173593154, 0.3339464036664714, 0.18591809601733303]
actions average: 
K:  1  action  0 :  tensor([0.3889, 0.0064, 0.1214, 0.0956, 0.2085, 0.0918, 0.0874],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0016, 0.9846, 0.0023, 0.0025, 0.0017, 0.0014, 0.0060],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0552, 0.0148, 0.6478, 0.0598, 0.0591, 0.1073, 0.0560],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1702, 0.0111, 0.2007, 0.1587, 0.1622, 0.1484, 0.1486],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1390, 0.0185, 0.1793, 0.1047, 0.3605, 0.1039, 0.0941],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0760, 0.0088, 0.1133, 0.1110, 0.0859, 0.5066, 0.0984],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0915, 0.2546, 0.1083, 0.1053, 0.0975, 0.0914, 0.2514],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  19 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20999999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.10221334616201208, 0.048741487613150565, 0.27704454480510143, 0.05213612173593154, 0.3339464036664714, 0.18591809601733303]
printing an ep nov before normalisation:  31.786370537442515
actor:  1 policy actor:  1  step number:  24 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.20999999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.10210142292741241, 0.04874755189733767, 0.2770790917915098, 0.052142609528914856, 0.3339880496322875, 0.1859412742225378]
actor:  1 policy actor:  1  step number:  44 total reward:  0.1899999999999995  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.20999999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.10198999253038221, 0.04875358947833763, 0.27711348665532365, 0.05214906875385187, 0.3340295122161934, 0.18596435036591125]
actions average: 
K:  4  action  0 :  tensor([0.5264, 0.0247, 0.0793, 0.0940, 0.0944, 0.0985, 0.0827],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0014,     0.9646,     0.0024,     0.0094,     0.0003,     0.0023,
            0.0196], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1064, 0.0059, 0.4538, 0.1176, 0.1191, 0.1104, 0.0869],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0797, 0.0725, 0.1982, 0.4257, 0.0622, 0.0783, 0.0835],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1422, 0.0221, 0.0481, 0.0484, 0.6518, 0.0488, 0.0386],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1320, 0.0149, 0.2305, 0.1193, 0.1320, 0.2599, 0.1114],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1370, 0.0978, 0.1231, 0.1109, 0.1860, 0.1292, 0.2162],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  45.688309786102806
maxi score, test score, baseline:  0.20999999999999985 0.6789999999999999 0.6789999999999999
Printing some Q and Qe and total Qs values:  [[0.672]
 [0.753]
 [0.686]
 [0.687]
 [0.716]
 [0.715]
 [0.684]] [[39.609]
 [38.437]
 [38.935]
 [38.845]
 [38.282]
 [38.326]
 [38.608]] [[2.16 ]
 [2.156]
 [2.125]
 [2.119]
 [2.107]
 [2.109]
 [2.099]]
actor:  1 policy actor:  1  step number:  25 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  38 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  51.579929147027265
maxi score, test score, baseline:  0.20999999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.10198895553146724, 0.04875551682924385, 0.2771026982970241, 0.05215080703510248, 0.33404338486412716, 0.18595863744303517]
printing an ep nov before normalisation:  25.534067153930664
printing an ep nov before normalisation:  48.16110614362583
actor:  1 policy actor:  1  step number:  40 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.272]
 [0.247]
 [0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.236]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.272]
 [0.247]
 [0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.236]]
maxi score, test score, baseline:  0.20999999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.10198895553146724, 0.04875551682924385, 0.2771026982970241, 0.05215080703510248, 0.33404338486412716, 0.18595863744303517]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20999999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.10187802077400519, 0.048761527786560926, 0.2771369388000722, 0.052157237737036274, 0.33408466469082715, 0.18598161021149812]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  42 total reward:  0.46999999999999986  reward:  1.0 rdn_beta:  2.0
siam score:  -0.7782852
maxi score, test score, baseline:  0.20999999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.10187802077400519, 0.048761527786560926, 0.2771369388000722, 0.052157237737036274, 0.33408466469082715, 0.18598161021149812]
printing an ep nov before normalisation:  42.080405334318094
maxi score, test score, baseline:  0.20999999999999985 0.6789999999999999 0.6789999999999999
Printing some Q and Qe and total Qs values:  [[0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.38]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.38]]
printing an ep nov before normalisation:  31.379144925581354
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  38 total reward:  0.3299999999999996  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  33 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  45.21749368311196
maxi score, test score, baseline:  0.20677999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.10187802077400519, 0.048761527786560926, 0.2771369388000722, 0.052157237737036274, 0.33408466469082715, 0.18598161021149812]
actor:  1 policy actor:  1  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  22.09808349609375
maxi score, test score, baseline:  0.20677999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.10176757236147371, 0.04876751239146437, 0.27717102919060854, 0.052163640246369485, 0.3341257635441796, 0.1860044822659044]
line 256 mcts: sample exp_bonus 27.99421866734822
printing an ep nov before normalisation:  49.8241619553644
printing an ep nov before normalisation:  46.34548573351116
actor:  1 policy actor:  1  step number:  39 total reward:  0.13999999999999946  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.20677999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.1016576071026183, 0.048773470816871015, 0.2772049704536279, 0.052170014748094004, 0.3341666826116787, 0.1860272542671101]
actor:  1 policy actor:  1  step number:  31 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20677999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.1016576071026183, 0.048773470816871015, 0.2772049704536279, 0.052170014748094004, 0.3341666826116787, 0.1860272542671101]
maxi score, test score, baseline:  0.20677999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.1016576071026183, 0.048773470816871015, 0.2772049704536279, 0.052170014748094004, 0.3341666826116787, 0.1860272542671101]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  35 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  28 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.436218517006104
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20677999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.1016576071026183, 0.048773470816871015, 0.2772049704536279, 0.052170014748094004, 0.3341666826116787, 0.1860272542671101]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  1.6310640284405054
actor:  1 policy actor:  1  step number:  25 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  0
maxi score, test score, baseline:  0.20677999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.1016576071026183, 0.048773470816871015, 0.2772049704536279, 0.052170014748094004, 0.3341666826116787, 0.1860272542671101]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  22 total reward:  0.7899999999999999  reward:  1.0 rdn_beta:  2.0
from probs:  [0.1016576071026183, 0.048773470816871015, 0.2772049704536279, 0.052170014748094004, 0.3341666826116787, 0.1860272542671101]
printing an ep nov before normalisation:  34.86103296279907
actor:  1 policy actor:  1  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  31 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.21035999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.1016576071026183, 0.048773470816871015, 0.2772049704536279, 0.052170014748094004, 0.3341666826116787, 0.1860272542671101]
printing an ep nov before normalisation:  35.70198310469283
actor:  0 policy actor:  0  step number:  23 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  32.22158432006836
maxi score, test score, baseline:  0.21061999999999986 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  0.21061999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.1016576071026183, 0.048773470816871015, 0.2772049704536279, 0.052170014748094004, 0.3341666826116787, 0.1860272542671101]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.00794437387413
actor:  1 policy actor:  1  step number:  21 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  34 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.21061999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.10154812183404341, 0.04877940323418826, 0.27723876356552635, 0.05217636142558674, 0.3342074230704526, 0.18604992687020264]
Printing some Q and Qe and total Qs values:  [[0.984]
 [0.984]
 [0.86 ]
 [0.984]
 [0.984]
 [0.984]
 [0.984]] [[40.117]
 [40.117]
 [42.634]
 [40.117]
 [40.117]
 [40.117]
 [40.117]] [[0.984]
 [0.984]
 [0.86 ]
 [0.984]
 [0.984]
 [0.984]
 [0.984]]
actor:  1 policy actor:  1  step number:  28 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  43.4031392845139
printing an ep nov before normalisation:  32.233986854456504
using explorer policy with actor:  1
maxi score, test score, baseline:  0.21061999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.10154812183404341, 0.04877940323418826, 0.27723876356552635, 0.05217636142558674, 0.3342074230704526, 0.18604992687020264]
actor:  1 policy actor:  1  step number:  24 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  33 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.571]
 [0.455]
 [0.403]
 [0.418]
 [0.403]
 [0.397]] [[31.616]
 [33.191]
 [28.857]
 [31.616]
 [27.404]
 [31.616]
 [27.254]] [[1.216]
 [1.458]
 [1.138]
 [1.216]
 [1.033]
 [1.216]
 [1.004]]
maxi score, test score, baseline:  0.21353999999999984 0.6789999999999999 0.6789999999999999
probs:  [0.10154812183404341, 0.04877940323418826, 0.27723876356552635, 0.05217636142558674, 0.3342074230704526, 0.18604992687020264]
Printing some Q and Qe and total Qs values:  [[0.778]
 [0.776]
 [0.777]
 [0.777]
 [0.777]
 [0.777]
 [0.778]] [[4.141]
 [3.564]
 [4.106]
 [4.825]
 [4.148]
 [2.604]
 [4.025]] [[1.045]
 [1.006]
 [1.042]
 [1.089]
 [1.045]
 [0.943]
 [1.037]]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.21353999999999984 0.6789999999999999 0.6789999999999999
probs:  [0.10154812183404341, 0.04877940323418826, 0.27723876356552635, 0.05217636142558674, 0.3342074230704526, 0.18604992687020264]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21353999999999984 0.6789999999999999 0.6789999999999999
probs:  [0.10154812183404341, 0.04877940323418826, 0.27723876356552635, 0.05217636142558674, 0.3342074230704526, 0.18604992687020264]
printing an ep nov before normalisation:  42.03078638496288
using explorer policy with actor:  1
maxi score, test score, baseline:  0.21353999999999984 0.6789999999999999 0.6789999999999999
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.794075965708785
actor:  1 policy actor:  1  step number:  31 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  47 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  35.40667533874512
actor:  1 policy actor:  1  step number:  21 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.21353999999999984 0.6789999999999999 0.6789999999999999
probs:  [0.10143911341990927, 0.0487853098133304, 0.27727240949419485, 0.05218268046062673, 0.3342479860873759, 0.18607250072456288]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2104599999999999 0.6789999999999999 0.6789999999999999
printing an ep nov before normalisation:  47.698221002658954
printing an ep nov before normalisation:  0.029788237259253947
actor:  1 policy actor:  1  step number:  29 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  39 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2104599999999999 0.6789999999999999 0.6789999999999999
probs:  [0.10143911341990927, 0.0487853098133304, 0.27727240949419485, 0.05218268046062673, 0.3342479860873759, 0.18607250072456288]
Printing some Q and Qe and total Qs values:  [[0.387]
 [0.387]
 [0.435]
 [0.387]
 [0.387]
 [0.387]
 [0.387]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.387]
 [0.387]
 [0.435]
 [0.387]
 [0.387]
 [0.387]
 [0.387]]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.0
from probs:  [0.10143911341990927, 0.0487853098133304, 0.27727240949419485, 0.05218268046062673, 0.3342479860873759, 0.18607250072456288]
maxi score, test score, baseline:  0.2104599999999999 0.6789999999999999 0.6789999999999999
probs:  [0.10143911341990927, 0.0487853098133304, 0.27727240949419485, 0.05218268046062673, 0.3342479860873759, 0.18607250072456288]
printing an ep nov before normalisation:  29.38030242393034
maxi score, test score, baseline:  0.2104599999999999 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  0.2104599999999999 0.6789999999999999 0.6789999999999999
probs:  [0.10143911341990927, 0.0487853098133304, 0.27727240949419485, 0.05218268046062673, 0.3342479860873759, 0.18607250072456288]
printing an ep nov before normalisation:  49.3747335642773
Printing some Q and Qe and total Qs values:  [[0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]] [[30.55]
 [30.55]
 [30.55]
 [30.55]
 [30.55]
 [30.55]
 [30.55]] [[2.24]
 [2.24]
 [2.24]
 [2.24]
 [2.24]
 [2.24]
 [2.24]]
maxi score, test score, baseline:  0.2104599999999999 0.6789999999999999 0.6789999999999999
actor:  0 policy actor:  1  step number:  27 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  45.278111378116705
maxi score, test score, baseline:  0.21007999999999988 0.6789999999999999 0.6789999999999999
actor:  1 policy actor:  1  step number:  36 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  33 total reward:  0.5199999999999999  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  1  action  0 :  tensor([0.7518, 0.0010, 0.0385, 0.0760, 0.0626, 0.0376, 0.0324],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0029, 0.9574, 0.0032, 0.0064, 0.0023, 0.0023, 0.0256],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0443, 0.0085, 0.7181, 0.0581, 0.0533, 0.0639, 0.0538],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0995, 0.0011, 0.0930, 0.4814, 0.1550, 0.1000, 0.0698],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2222, 0.0019, 0.1225, 0.1159, 0.3204, 0.1023, 0.1149],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1002, 0.0079, 0.1470, 0.1766, 0.1300, 0.3159, 0.1224],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0910, 0.0653, 0.1135, 0.1152, 0.0889, 0.0906, 0.4355],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.59003829956055
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  21 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  30 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  34 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  34.74014731943715
maxi score, test score, baseline:  0.20967999999999987 0.6789999999999999 0.6789999999999999
probs:  [0.10156178027707118, 0.048550588878404105, 0.2785885451827656, 0.05197101918542819, 0.33255844961736586, 0.18676961685896512]
printing an ep nov before normalisation:  23.890936374664307
Printing some Q and Qe and total Qs values:  [[0.436]
 [0.436]
 [0.616]
 [0.436]
 [0.436]
 [0.436]
 [0.436]] [[46.549]
 [46.549]
 [41.045]
 [46.549]
 [46.549]
 [46.549]
 [46.549]] [[0.718]
 [0.718]
 [0.833]
 [0.718]
 [0.718]
 [0.718]
 [0.718]]
actor:  1 policy actor:  1  step number:  35 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  2.0
from probs:  [0.10145252376811036, 0.048556481144427704, 0.2786224324649266, 0.05197732770885199, 0.3325989048953911, 0.18679233001829232]
maxi score, test score, baseline:  0.20967999999999987 0.6789999999999999 0.6789999999999999
probs:  [0.10134374087555637, 0.04856234786804754, 0.27865617284899474, 0.051983608885436294, 0.332639184803696, 0.18681494471826915]
printing an ep nov before normalisation:  41.54250749178684
maxi score, test score, baseline:  0.20967999999999987 0.6789999999999999 0.6789999999999999
probs:  [0.10134374087555637, 0.04856234786804754, 0.27865617284899474, 0.051983608885436294, 0.332639184803696, 0.18681494471826915]
actor:  1 policy actor:  1  step number:  46 total reward:  0.08999999999999941  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  25.9641170501709
printing an ep nov before normalisation:  39.036800797156054
actor:  0 policy actor:  0  step number:  25 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20971999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.10123542852644749, 0.04856818921499021, 0.2786897672880874, 0.0519898628926158, 0.33267929048013034, 0.18683746159772865]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 32.30227285401587
maxi score, test score, baseline:  0.20971999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.10123542852644749, 0.04856818921499021, 0.2786897672880874, 0.0519898628926158, 0.33267929048013034, 0.18683746159772865]
printing an ep nov before normalisation:  34.339127382193766
maxi score, test score, baseline:  0.20971999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.10123542852644749, 0.04856818921499021, 0.2786897672880874, 0.0519898628926158, 0.33267929048013034, 0.18683746159772865]
printing an ep nov before normalisation:  39.46381821981049
actor:  0 policy actor:  1  step number:  32 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20625999999999983 0.6789999999999999 0.6789999999999999
probs:  [0.10123542852644749, 0.04856818921499021, 0.2786897672880874, 0.0519898628926158, 0.33267929048013034, 0.18683746159772865]
actor:  1 policy actor:  1  step number:  42 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20297999999999988 0.6789999999999999 0.6789999999999999
probs:  [0.10123542852644749, 0.04856818921499021, 0.2786897672880874, 0.0519898628926158, 0.33267929048013034, 0.18683746159772865]
printing an ep nov before normalisation:  20.287450154577108
maxi score, test score, baseline:  0.20297999999999988 0.6789999999999999 0.6789999999999999
probs:  [0.10123542852644749, 0.04856818921499021, 0.2786897672880874, 0.0519898628926158, 0.33267929048013034, 0.18683746159772865]
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.383]
 [0.455]
 [0.455]
 [0.455]
 [0.423]
 [0.432]] [[28.629]
 [24.635]
 [30.885]
 [30.885]
 [30.885]
 [28.646]
 [28.693]] [[0.688]
 [0.603]
 [0.788]
 [0.788]
 [0.788]
 [0.716]
 [0.725]]
printing an ep nov before normalisation:  31.011234885044573
maxi score, test score, baseline:  0.20297999999999988 0.6789999999999999 0.6789999999999999
probs:  [0.10123542852644749, 0.04856818921499021, 0.2786897672880874, 0.0519898628926158, 0.33267929048013034, 0.18683746159772865]
siam score:  -0.777486
printing an ep nov before normalisation:  38.30955463160542
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  33 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  29.059484004974365
Printing some Q and Qe and total Qs values:  [[0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  27 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  33.077476254991545
actor:  1 policy actor:  1  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.20297999999999988 0.6789999999999999 0.6789999999999999
probs:  [0.10102020329906847, 0.04857979643461274, 0.27875652210276813, 0.05200229010085497, 0.33275898363978224, 0.18688220442291348]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20297999999999988 0.6789999999999999 0.6789999999999999
probs:  [0.10102020329906847, 0.04857979643461274, 0.27875652210276813, 0.05200229010085497, 0.33275898363978224, 0.18688220442291348]
from probs:  [0.10102020329906847, 0.04857979643461274, 0.27875652210276813, 0.05200229010085497, 0.33275898363978224, 0.18688220442291348]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20297999999999988 0.6789999999999999 0.6789999999999999
probs:  [0.1093319270138723, 0.047016224435205506, 0.32053859458657685, 0.05108322372325773, 0.26066703129689234, 0.21136299894419525]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20611999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.1093319270138723, 0.047016224435205506, 0.32053859458657685, 0.05108322372325773, 0.26066703129689234, 0.21136299894419525]
printing an ep nov before normalisation:  32.02953974638372
printing an ep nov before normalisation:  36.94370269873741
actor:  1 policy actor:  1  step number:  32 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20611999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.1093319270138723, 0.047016224435205506, 0.32053859458657685, 0.05108322372325773, 0.26066703129689234, 0.21136299894419525]
Printing some Q and Qe and total Qs values:  [[0.954]
 [0.955]
 [0.954]
 [0.954]
 [0.954]
 [0.954]
 [0.954]] [[37.508]
 [39.836]
 [37.508]
 [37.508]
 [37.508]
 [37.508]
 [37.508]] [[0.954]
 [0.955]
 [0.954]
 [0.954]
 [0.954]
 [0.954]
 [0.954]]
actor:  1 policy actor:  1  step number:  24 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  1  step number:  27 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20903999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.1093319270138723, 0.047016224435205506, 0.32053859458657685, 0.05108322372325773, 0.26066703129689234, 0.21136299894419525]
printing an ep nov before normalisation:  41.07425845860131
printing an ep nov before normalisation:  45.19778234218394
maxi score, test score, baseline:  0.20903999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.1093319270138723, 0.047016224435205506, 0.32053859458657685, 0.05108322372325773, 0.26066703129689234, 0.21136299894419525]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20903999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.1093319270138723, 0.047016224435205506, 0.32053859458657685, 0.05108322372325773, 0.26066703129689234, 0.21136299894419525]
actor:  1 policy actor:  1  step number:  30 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20903999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.1093319270138723, 0.047016224435205506, 0.32053859458657685, 0.05108322372325773, 0.26066703129689234, 0.21136299894419525]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  28 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20903999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.1093319270138723, 0.047016224435205506, 0.32053859458657685, 0.05108322372325773, 0.26066703129689234, 0.21136299894419525]
Printing some Q and Qe and total Qs values:  [[0.357]
 [0.357]
 [0.447]
 [0.357]
 [0.357]
 [0.357]
 [0.357]] [[30.534]
 [30.534]
 [35.968]
 [30.534]
 [30.534]
 [30.534]
 [30.534]] [[0.98 ]
 [0.98 ]
 [1.256]
 [0.98 ]
 [0.98 ]
 [0.98 ]
 [0.98 ]]
maxi score, test score, baseline:  0.20903999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.1093319270138723, 0.047016224435205506, 0.32053859458657685, 0.05108322372325773, 0.26066703129689234, 0.21136299894419525]
actor:  1 policy actor:  1  step number:  36 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  41.88533607123402
line 256 mcts: sample exp_bonus 32.74000587732659
actor:  1 policy actor:  1  step number:  30 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  40 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20903999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.1093319270138723, 0.047016224435205506, 0.32053859458657685, 0.05108322372325773, 0.26066703129689234, 0.21136299894419525]
printing an ep nov before normalisation:  30.430173873901367
actions average: 
K:  3  action  0 :  tensor([0.5786, 0.0506, 0.0582, 0.0635, 0.1370, 0.0506, 0.0615],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0012, 0.9751, 0.0055, 0.0055, 0.0012, 0.0039, 0.0076],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0541, 0.0636, 0.6209, 0.0474, 0.0525, 0.0909, 0.0706],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1634, 0.0066, 0.1655, 0.1580, 0.1753, 0.1605, 0.1707],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1075, 0.0046, 0.0741, 0.1020, 0.5506, 0.0724, 0.0889],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0650, 0.0117, 0.1755, 0.0731, 0.0824, 0.4968, 0.0956],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1012, 0.2649, 0.1074, 0.1343, 0.0995, 0.1000, 0.1927],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.20903999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.1093319270138723, 0.047016224435205506, 0.32053859458657685, 0.05108322372325773, 0.26066703129689234, 0.21136299894419525]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20903999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.1093319270138723, 0.047016224435205506, 0.32053859458657685, 0.05108322372325773, 0.26066703129689234, 0.21136299894419525]
actor:  1 policy actor:  1  step number:  30 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
from probs:  [0.1093319270138723, 0.047016224435205506, 0.32053859458657685, 0.05108322372325773, 0.26066703129689234, 0.21136299894419525]
maxi score, test score, baseline:  0.20903999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.1093319270138723, 0.047016224435205506, 0.32053859458657685, 0.05108322372325773, 0.26066703129689234, 0.21136299894419525]
actor:  0 policy actor:  0  step number:  37 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  36.25158071517944
actor:  0 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  2.0
using another actor
maxi score, test score, baseline:  0.21211999999999986 0.6789999999999999 0.6789999999999999
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.577]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]] [[37.968]
 [40.709]
 [37.968]
 [37.968]
 [37.968]
 [37.968]
 [37.968]] [[1.137]
 [1.311]
 [1.137]
 [1.137]
 [1.137]
 [1.137]
 [1.137]]
actor:  0 policy actor:  0  step number:  26 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21513999999999983 0.6789999999999999 0.6789999999999999
probs:  [0.1093319270138723, 0.047016224435205506, 0.32053859458657685, 0.05108322372325773, 0.26066703129689234, 0.21136299894419525]
printing an ep nov before normalisation:  0.6897766310771658
printing an ep nov before normalisation:  42.478299140930176
maxi score, test score, baseline:  0.21513999999999983 0.6789999999999999 0.6789999999999999
probs:  [0.1093319270138723, 0.047016224435205506, 0.32053859458657685, 0.05108322372325773, 0.26066703129689234, 0.21136299894419525]
actor:  0 policy actor:  1  step number:  31 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  55.52612919071196
actor:  1 policy actor:  1  step number:  34 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.76 ]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]] [[32.3  ]
 [39.845]
 [32.3  ]
 [32.3  ]
 [32.3  ]
 [32.3  ]
 [32.3  ]] [[1.553]
 [2.14 ]
 [1.553]
 [1.553]
 [1.553]
 [1.553]
 [1.553]]
maxi score, test score, baseline:  0.21523999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.1093319270138723, 0.047016224435205506, 0.32053859458657685, 0.05108322372325773, 0.26066703129689234, 0.21136299894419525]
printing an ep nov before normalisation:  49.27107885512432
actor:  1 policy actor:  1  step number:  27 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  37.50819554796492
maxi score, test score, baseline:  0.21523999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.1093319270138723, 0.047016224435205506, 0.32053859458657685, 0.05108322372325773, 0.26066703129689234, 0.21136299894419525]
actor:  1 policy actor:  1  step number:  24 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  27 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.21523999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.1093319270138723, 0.047016224435205506, 0.32053859458657685, 0.05108322372325773, 0.26066703129689234, 0.21136299894419525]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.21523999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.1093319270138723, 0.047016224435205506, 0.32053859458657685, 0.05108322372325773, 0.26066703129689234, 0.21136299894419525]
siam score:  -0.77514887
Printing some Q and Qe and total Qs values:  [[0.912]
 [0.8  ]
 [0.8  ]
 [0.8  ]
 [0.8  ]
 [0.8  ]
 [0.8  ]] [[40.275]
 [47.243]
 [47.243]
 [47.243]
 [47.243]
 [47.243]
 [47.243]] [[0.912]
 [0.8  ]
 [0.8  ]
 [0.8  ]
 [0.8  ]
 [0.8  ]
 [0.8  ]]
maxi score, test score, baseline:  0.21523999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.1093319270138723, 0.047016224435205506, 0.32053859458657685, 0.05108322372325773, 0.26066703129689234, 0.21136299894419525]
maxi score, test score, baseline:  0.21523999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.1093319270138723, 0.047016224435205506, 0.32053859458657685, 0.05108322372325773, 0.26066703129689234, 0.21136299894419525]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  1  step number:  33 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  47 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.21487999999999985 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  0.21487999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.1093319270138723, 0.047016224435205506, 0.32053859458657685, 0.05108322372325773, 0.26066703129689234, 0.21136299894419525]
Printing some Q and Qe and total Qs values:  [[-0.055]
 [-0.055]
 [-0.055]
 [-0.055]
 [-0.055]
 [-0.055]
 [-0.055]] [[60.51]
 [60.51]
 [60.51]
 [60.51]
 [60.51]
 [60.51]
 [60.51]] [[1.284]
 [1.284]
 [1.284]
 [1.284]
 [1.284]
 [1.284]
 [1.284]]
actor:  1 policy actor:  1  step number:  27 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.0
from probs:  [0.1093319270138723, 0.047016224435205506, 0.32053859458657685, 0.05108322372325773, 0.26066703129689234, 0.21136299894419525]
maxi score, test score, baseline:  0.21487999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.1093319270138723, 0.047016224435205506, 0.32053859458657685, 0.05108322372325773, 0.26066703129689234, 0.21136299894419525]
maxi score, test score, baseline:  0.21487999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.1093319270138723, 0.047016224435205506, 0.32053859458657685, 0.05108322372325773, 0.26066703129689234, 0.21136299894419525]
printing an ep nov before normalisation:  37.74335957676412
actor:  1 policy actor:  1  step number:  27 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.21487999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.10920604647957388, 0.047022855495810846, 0.32058391223940147, 0.051090430014119045, 0.26070388080594775, 0.21139287496514703]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  27 total reward:  0.74  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.541]
 [0.69 ]
 [0.646]
 [0.644]
 [0.61 ]
 [0.61 ]
 [0.61 ]] [[42.078]
 [41.232]
 [40.802]
 [40.722]
 [37.798]
 [37.798]
 [37.798]] [[1.753]
 [1.853]
 [1.784]
 [1.777]
 [1.575]
 [1.575]
 [1.575]]
maxi score, test score, baseline:  0.21517999999999987 0.6789999999999999 0.6789999999999999
probs:  [0.10920604647957388, 0.047022855495810846, 0.32058391223940147, 0.051090430014119045, 0.26070388080594775, 0.21139287496514703]
siam score:  -0.77892953
maxi score, test score, baseline:  0.21517999999999987 0.6789999999999999 0.6789999999999999
actor:  1 policy actor:  1  step number:  34 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  38.670819290527334
maxi score, test score, baseline:  0.21517999999999987 0.6789999999999999 0.6789999999999999
printing an ep nov before normalisation:  0.01120053938620913
actor:  0 policy actor:  0  step number:  31 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  28 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  34 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
siam score:  -0.7759848
actor:  1 policy actor:  1  step number:  38 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  24 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  32.73823383407024
printing an ep nov before normalisation:  40.00192642211914
actor:  0 policy actor:  0  step number:  22 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.667
from probs:  [0.10883159666796278, 0.04704258054232652, 0.3207187161362663, 0.05111186616602682, 0.26081349498736744, 0.21148174550005017]
actor:  0 policy actor:  0  step number:  34 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  24 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.21769999999999984 0.6789999999999999 0.6789999999999999
probs:  [0.10883159666796278, 0.04704258054232652, 0.3207187161362663, 0.05111186616602682, 0.26081349498736744, 0.21148174550005017]
Printing some Q and Qe and total Qs values:  [[0.79]
 [0.79]
 [0.79]
 [0.79]
 [0.79]
 [0.79]
 [0.79]] [[30.416]
 [30.416]
 [30.416]
 [30.416]
 [30.416]
 [30.416]
 [30.416]] [[2.451]
 [2.451]
 [2.451]
 [2.451]
 [2.451]
 [2.451]
 [2.451]]
actor:  1 policy actor:  1  step number:  39 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.21769999999999984 0.6789999999999999 0.6789999999999999
printing an ep nov before normalisation:  31.80382490158081
printing an ep nov before normalisation:  23.053181171417236
using explorer policy with actor:  1
siam score:  -0.77153105
line 256 mcts: sample exp_bonus 45.647675275443056
actor:  1 policy actor:  1  step number:  51 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  1.667
using another actor
from probs:  [0.10883159666796278, 0.04704258054232652, 0.3207187161362663, 0.05111186616602682, 0.26081349498736744, 0.21148174550005017]
actor:  1 policy actor:  1  step number:  29 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.0
siam score:  -0.7692462
actor:  1 policy actor:  1  step number:  30 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21769999999999984 0.6789999999999999 0.6789999999999999
probs:  [0.10883159666796278, 0.04704258054232652, 0.3207187161362663, 0.05111186616602682, 0.26081349498736744, 0.21148174550005017]
maxi score, test score, baseline:  0.21769999999999984 0.6789999999999999 0.6789999999999999
probs:  [0.10883159666796278, 0.04704258054232652, 0.3207187161362663, 0.05111186616602682, 0.26081349498736744, 0.21148174550005017]
maxi score, test score, baseline:  0.21769999999999984 0.6789999999999999 0.6789999999999999
probs:  [0.10883159666796278, 0.04704258054232652, 0.3207187161362663, 0.05111186616602682, 0.26081349498736744, 0.21148174550005017]
printing an ep nov before normalisation:  36.20350643530915
actor:  0 policy actor:  1  step number:  30 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  25 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.361]
 [0.391]
 [0.361]
 [0.361]
 [0.361]
 [0.361]
 [0.361]] [[35.627]
 [38.037]
 [35.627]
 [35.627]
 [35.627]
 [35.627]
 [35.627]] [[1.183]
 [1.323]
 [1.183]
 [1.183]
 [1.183]
 [1.183]
 [1.183]]
Printing some Q and Qe and total Qs values:  [[0.738]
 [0.89 ]
 [0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]] [[28.566]
 [32.128]
 [28.566]
 [28.566]
 [28.566]
 [28.566]
 [28.566]] [[2.22 ]
 [2.557]
 [2.22 ]
 [2.22 ]
 [2.22 ]
 [2.22 ]
 [2.22 ]]
maxi score, test score, baseline:  0.22059999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.10883159666796278, 0.04704258054232652, 0.3207187161362663, 0.05111186616602682, 0.26081349498736744, 0.21148174550005017]
maxi score, test score, baseline:  0.22059999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.10883159666796278, 0.04704258054232652, 0.3207187161362663, 0.05111186616602682, 0.26081349498736744, 0.21148174550005017]
printing an ep nov before normalisation:  47.03794754125001
maxi score, test score, baseline:  0.22059999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.10883159666796278, 0.04704258054232652, 0.3207187161362663, 0.05111186616602682, 0.26081349498736744, 0.21148174550005017]
printing an ep nov before normalisation:  39.11588060730174
printing an ep nov before normalisation:  0.011152648468737425
siam score:  -0.76734054
actor:  1 policy actor:  1  step number:  59 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  31 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.22059999999999985 0.6789999999999999 0.6789999999999999
printing an ep nov before normalisation:  47.43719832141215
maxi score, test score, baseline:  0.22059999999999985 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  0.22059999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.10883159666796278, 0.04704258054232652, 0.3207187161362663, 0.05111186616602682, 0.26081349498736744, 0.21148174550005017]
maxi score, test score, baseline:  0.22059999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.10883159666796278, 0.04704258054232652, 0.3207187161362663, 0.05111186616602682, 0.26081349498736744, 0.21148174550005017]
actor:  1 policy actor:  1  step number:  38 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  23 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.22059999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.10883159666796278, 0.04704258054232652, 0.3207187161362663, 0.05111186616602682, 0.26081349498736744, 0.21148174550005017]
maxi score, test score, baseline:  0.22059999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.10883159666796278, 0.04704258054232652, 0.3207187161362663, 0.05111186616602682, 0.26081349498736744, 0.21148174550005017]
maxi score, test score, baseline:  0.22059999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.10883159666796278, 0.04704258054232652, 0.3207187161362663, 0.05111186616602682, 0.26081349498736744, 0.21148174550005017]
actor:  1 policy actor:  1  step number:  39 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  24 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  45.57153372045494
siam score:  -0.775114
actor:  1 policy actor:  1  step number:  26 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  24 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  46.81995699120332
printing an ep nov before normalisation:  39.57787211126191
actor:  1 policy actor:  1  step number:  32 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  47.184594084956906
maxi score, test score, baseline:  0.21721999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.10870783277792104, 0.04704910010359301, 0.32076327178605274, 0.051118951285221474, 0.260849724882738, 0.21151111916447374]
maxi score, test score, baseline:  0.21721999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.10870783277792104, 0.04704910010359301, 0.32076327178605274, 0.051118951285221474, 0.260849724882738, 0.21151111916447374]
printing an ep nov before normalisation:  44.41183075852949
actor:  1 policy actor:  1  step number:  35 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  0.007059860881071245
actor:  0 policy actor:  0  step number:  28 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.978]
 [0.959]
 [0.978]
 [0.978]
 [0.978]
 [0.978]
 [0.978]] [[33.402]
 [37.892]
 [33.402]
 [33.402]
 [33.402]
 [33.402]
 [33.402]] [[0.978]
 [0.959]
 [0.978]
 [0.978]
 [0.978]
 [0.978]
 [0.978]]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  36.94162078753187
actor:  1 policy actor:  1  step number:  28 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  34 total reward:  0.1899999999999995  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.21621999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.12788479079771187, 0.04799392414984646, 0.18475348940560507, 0.05326720700736962, 0.3250141686897187, 0.2610864199497483]
maxi score, test score, baseline:  0.21621999999999986 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  0.21621999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.12788479079771187, 0.04799392414984646, 0.18475348940560507, 0.05326720700736962, 0.3250141686897187, 0.2610864199497483]
maxi score, test score, baseline:  0.21621999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.12788479079771187, 0.04799392414984646, 0.18475348940560507, 0.05326720700736962, 0.3250141686897187, 0.2610864199497483]
actor:  1 policy actor:  1  step number:  37 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.21621999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.12788479079771187, 0.04799392414984646, 0.18475348940560507, 0.05326720700736962, 0.3250141686897187, 0.2610864199497483]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  22 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  0.12802897897898902
actor:  1 policy actor:  1  step number:  18 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  33.563175201416016
printing an ep nov before normalisation:  23.32837056984653
actor:  1 policy actor:  1  step number:  32 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  33.402390480041504
using explorer policy with actor:  1
maxi score, test score, baseline:  0.21621999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.12788479079771187, 0.04799392414984646, 0.18475348940560507, 0.05326720700736962, 0.3250141686897187, 0.2610864199497483]
siam score:  -0.7645595
maxi score, test score, baseline:  0.21621999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.12788479079771187, 0.04799392414984646, 0.18475348940560507, 0.05326720700736962, 0.3250141686897187, 0.2610864199497483]
actor:  1 policy actor:  1  step number:  30 total reward:  0.71  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  37 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.007182347086946
Printing some Q and Qe and total Qs values:  [[0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]] [[27.117]
 [27.117]
 [27.117]
 [27.117]
 [27.117]
 [27.117]
 [27.117]] [[1.755]
 [1.755]
 [1.755]
 [1.755]
 [1.755]
 [1.755]
 [1.755]]
actor:  1 policy actor:  1  step number:  31 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  41.297925978569914
maxi score, test score, baseline:  0.21621999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.1227859763285608, 0.04608371022629173, 0.17738493408850786, 0.05114652606421534, 0.31204753567918136, 0.290551317613243]
actor:  1 policy actor:  1  step number:  32 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  23 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.21607999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.1227859763285608, 0.04608371022629173, 0.17738493408850786, 0.05114652606421534, 0.31204753567918136, 0.290551317613243]
actor:  1 policy actor:  1  step number:  24 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.21607999999999986 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  0.21607999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.1227859763285608, 0.04608371022629173, 0.17738493408850786, 0.05114652606421534, 0.31204753567918136, 0.290551317613243]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  40.27333927192938
actions average: 
K:  2  action  0 :  tensor([0.6269, 0.0338, 0.0556, 0.0460, 0.1204, 0.0538, 0.0635],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0055, 0.9433, 0.0114, 0.0195, 0.0018, 0.0038, 0.0147],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0399, 0.0086, 0.7253, 0.0516, 0.0462, 0.0808, 0.0475],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1153, 0.0977, 0.0894, 0.3578, 0.1020, 0.1106, 0.1272],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0998, 0.0014, 0.0942, 0.0924, 0.5187, 0.1067, 0.0867],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1186, 0.0011, 0.1166, 0.1616, 0.1484, 0.3269, 0.1268],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0272, 0.1521, 0.0833, 0.0279, 0.0065, 0.0390, 0.6641],
       grad_fn=<DivBackward0>)
UNIT TEST: sample policy line 217 mcts : [0.082 0.878 0.    0.    0.02  0.02  0.   ]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.20999999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.1227859763285608, 0.04608371022629173, 0.17738493408850786, 0.05114652606421534, 0.31204753567918136, 0.290551317613243]
actor:  1 policy actor:  1  step number:  40 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  31 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  37.719199601635026
maxi score, test score, baseline:  0.21259999999999987 0.6789999999999999 0.6789999999999999
probs:  [0.1227859763285608, 0.04608371022629173, 0.17738493408850786, 0.05114652606421534, 0.31204753567918136, 0.290551317613243]
printing an ep nov before normalisation:  32.55066394763623
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  19 total reward:  0.7  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[ 0.419]
 [-0.085]
 [ 0.531]
 [ 0.419]
 [-0.07 ]
 [ 0.419]
 [ 0.518]] [[33.53 ]
 [48.413]
 [40.824]
 [33.53 ]
 [41.362]
 [33.53 ]
 [38.404]] [[0.616]
 [0.374]
 [0.856]
 [0.616]
 [0.265]
 [0.616]
 [0.801]]
maxi score, test score, baseline:  0.21599999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.1227859763285608, 0.04608371022629173, 0.17738493408850786, 0.05114652606421534, 0.31204753567918136, 0.290551317613243]
Printing some Q and Qe and total Qs values:  [[0.237]
 [0.485]
 [0.237]
 [0.237]
 [0.237]
 [0.237]
 [0.237]] [[34.464]
 [40.79 ]
 [34.464]
 [34.464]
 [34.464]
 [34.464]
 [34.464]] [[0.479]
 [0.869]
 [0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]]
printing an ep nov before normalisation:  1.046227837183551
actor:  1 policy actor:  1  step number:  31 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.36 ]
 [0.386]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.36 ]
 [0.386]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]
 [0.36 ]]
maxi score, test score, baseline:  0.21599999999999986 0.6789999999999999 0.6789999999999999
printing an ep nov before normalisation:  33.739359636141806
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.5  ]
 [0.403]
 [0.396]
 [0.361]
 [0.388]
 [0.407]] [[35.25 ]
 [36.65 ]
 [33.346]
 [34.818]
 [35.755]
 [33.71 ]
 [32.746]] [[0.466]
 [0.663]
 [0.54 ]
 [0.545]
 [0.517]
 [0.528]
 [0.539]]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.629]
 [0.626]
 [0.514]
 [0.519]
 [0.514]
 [0.514]
 [0.475]] [[45.846]
 [43.429]
 [62.494]
 [48.03 ]
 [62.494]
 [62.494]
 [46.675]] [[0.822]
 [0.799]
 [0.847]
 [0.73 ]
 [0.847]
 [0.847]
 [0.675]]
printing an ep nov before normalisation:  34.822843181004295
actor:  1 policy actor:  1  step number:  33 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21599999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.12248481250530024, 0.0460994978671532, 0.17744583412885398, 0.05116405318445017, 0.3121547030192518, 0.29065109929499067]
UNIT TEST: sample policy line 217 mcts : [0.143 0.    0.122 0.204 0.163 0.143 0.224]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  21.246278389742823
printing an ep nov before normalisation:  44.718071224060985
actor:  1 policy actor:  1  step number:  36 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  29 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  61 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21605999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.12248481250530024, 0.0460994978671532, 0.17744583412885398, 0.05116405318445017, 0.3121547030192518, 0.29065109929499067]
maxi score, test score, baseline:  0.21605999999999986 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  0.21605999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.12248481250530024, 0.0460994978671532, 0.17744583412885398, 0.05116405318445017, 0.3121547030192518, 0.29065109929499067]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  40.10816667950765
actor:  1 policy actor:  1  step number:  22 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  29 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.21605999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.12248481250530024, 0.0460994978671532, 0.17744583412885398, 0.05116405318445017, 0.3121547030192518, 0.29065109929499067]
actor:  1 policy actor:  1  step number:  33 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  38.338581025001794
actor:  1 policy actor:  1  step number:  22 total reward:  0.69  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.21605999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.1218728201428622, 0.04586957843718364, 0.17655893134937178, 0.05590671900534468, 0.3105939977842803, 0.2891979532809575]
actor:  0 policy actor:  1  step number:  36 total reward:  0.10999999999999943  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  42.221055030822754
actor:  1 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.21827999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.1218728201428622, 0.04586957843718364, 0.17655893134937178, 0.05590671900534468, 0.3105939977842803, 0.2891979532809575]
printing an ep nov before normalisation:  37.945424458789276
printing an ep nov before normalisation:  44.90670569431443
printing an ep nov before normalisation:  39.359826596378646
maxi score, test score, baseline:  0.21827999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.1218728201428622, 0.04586957843718364, 0.17655893134937178, 0.05590671900534468, 0.3105939977842803, 0.2891979532809575]
actor:  1 policy actor:  1  step number:  42 total reward:  0.3699999999999998  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.53391125075208
printing an ep nov before normalisation:  38.49904249970583
Printing some Q and Qe and total Qs values:  [[0.57 ]
 [0.671]
 [0.625]
 [0.625]
 [0.664]
 [0.625]
 [0.625]] [[47.163]
 [49.766]
 [49.368]
 [49.368]
 [47.416]
 [49.368]
 [49.368]] [[0.765]
 [0.889]
 [0.839]
 [0.839]
 [0.861]
 [0.839]
 [0.839]]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.21827999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.12172381440656745, 0.045877345139521714, 0.17658889101781708, 0.05591619016114808, 0.31064671856998816, 0.28924704070495744]
actor:  0 policy actor:  1  step number:  26 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
using another actor
maxi score, test score, baseline:  0.21839999999999982 0.6789999999999999 0.6789999999999999
printing an ep nov before normalisation:  38.46000405192959
maxi score, test score, baseline:  0.21839999999999982 0.6789999999999999 0.6789999999999999
probs:  [0.12172381440656745, 0.045877345139521714, 0.17658889101781708, 0.05591619016114808, 0.31064671856998816, 0.28924704070495744]
Printing some Q and Qe and total Qs values:  [[-0.094]
 [ 0.   ]
 [-0.094]
 [-0.094]
 [ 0.   ]
 [ 0.   ]
 [-0.094]] [[31.028]
 [ 0.001]
 [31.028]
 [31.028]
 [ 0.001]
 [ 0.001]
 [31.028]] [[1.603]
 [0.   ]
 [1.603]
 [1.603]
 [0.   ]
 [0.   ]
 [1.603]]
maxi score, test score, baseline:  0.21839999999999982 0.6789999999999999 0.6789999999999999
probs:  [0.12172381440656745, 0.045877345139521714, 0.17658889101781708, 0.05591619016114808, 0.31064671856998816, 0.28924704070495744]
Printing some Q and Qe and total Qs values:  [[0.735]
 [0.852]
 [0.735]
 [0.767]
 [0.735]
 [0.784]
 [0.775]] [[31.047]
 [35.91 ]
 [31.047]
 [40.141]
 [31.047]
 [39.797]
 [39.191]] [[0.735]
 [0.852]
 [0.735]
 [0.767]
 [0.735]
 [0.784]
 [0.775]]
maxi score, test score, baseline:  0.21839999999999982 0.6789999999999999 0.6789999999999999
probs:  [0.12172381440656745, 0.045877345139521714, 0.17658889101781708, 0.05591619016114808, 0.31064671856998816, 0.28924704070495744]
maxi score, test score, baseline:  0.21839999999999982 0.6789999999999999 0.6789999999999999
probs:  [0.12172381440656745, 0.045877345139521714, 0.17658889101781708, 0.05591619016114808, 0.31064671856998816, 0.28924704070495744]
actor:  0 policy actor:  1  step number:  27 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.21851999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.12172381440656745, 0.045877345139521714, 0.17658889101781708, 0.05591619016114808, 0.31064671856998816, 0.28924704070495744]
printing an ep nov before normalisation:  43.15781223284632
actor:  1 policy actor:  1  step number:  25 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  0  action  0 :  tensor([0.6529, 0.0061, 0.0518, 0.0744, 0.0820, 0.0576, 0.0753],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0016,     0.9681,     0.0012,     0.0101,     0.0003,     0.0004,
            0.0181], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0482, 0.0012, 0.6734, 0.0594, 0.0616, 0.0872, 0.0690],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0796, 0.0024, 0.0943, 0.4784, 0.1021, 0.0792, 0.1639],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1627, 0.0020, 0.1411, 0.1381, 0.2687, 0.1352, 0.1522],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1196, 0.0090, 0.1544, 0.0938, 0.1128, 0.3725, 0.1380],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0759, 0.1177, 0.1009, 0.1366, 0.1228, 0.1049, 0.3410],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.21851999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.12157542211534404, 0.045885079866948036, 0.1766187273446289, 0.05592562232493657, 0.3106992223083062, 0.2892959260398362]
actor:  1 policy actor:  1  step number:  33 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.21851999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.12157542211534404, 0.045885079866948036, 0.1766187273446289, 0.05592562232493657, 0.3106992223083062, 0.2892959260398362]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.21851999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.12157542211534404, 0.045885079866948036, 0.1766187273446289, 0.05592562232493657, 0.3106992223083062, 0.2892959260398362]
printing an ep nov before normalisation:  23.217566814875884
printing an ep nov before normalisation:  31.16506647316391
maxi score, test score, baseline:  0.21851999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.12157542211534404, 0.045885079866948036, 0.1766187273446289, 0.05592562232493657, 0.3106992223083062, 0.2892959260398362]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  21 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.0
siam score:  -0.77333426
actor:  0 policy actor:  1  step number:  32 total reward:  0.3299999999999996  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  36 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.22117999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.12157542211534404, 0.045885079866948036, 0.1766187273446289, 0.05592562232493657, 0.3106992223083062, 0.2892959260398362]
siam score:  -0.7756967
maxi score, test score, baseline:  0.22117999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.12157542211534404, 0.045885079866948036, 0.1766187273446289, 0.05592562232493657, 0.3106992223083062, 0.2892959260398362]
maxi score, test score, baseline:  0.21797999999999984 0.6789999999999999 0.6789999999999999
probs:  [0.12157542211534404, 0.045885079866948036, 0.1766187273446289, 0.05592562232493657, 0.3106992223083062, 0.2892959260398362]
actor:  1 policy actor:  1  step number:  38 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.21797999999999984 0.6789999999999999 0.6789999999999999
probs:  [0.12157542211534404, 0.045885079866948036, 0.1766187273446289, 0.05592562232493657, 0.3106992223083062, 0.2892959260398362]
printing an ep nov before normalisation:  36.4343275114086
printing an ep nov before normalisation:  41.99248250088738
maxi score, test score, baseline:  0.21797999999999984 0.6789999999999999 0.6789999999999999
probs:  [0.12157542211534404, 0.045885079866948036, 0.1766187273446289, 0.05592562232493657, 0.3106992223083062, 0.2892959260398362]
maxi score, test score, baseline:  0.21797999999999984 0.6789999999999999 0.6789999999999999
probs:  [0.12157542211534404, 0.045885079866948036, 0.1766187273446289, 0.05592562232493657, 0.3106992223083062, 0.2892959260398362]
maxi score, test score, baseline:  0.21797999999999984 0.6789999999999999 0.6789999999999999
probs:  [0.12157542211534404, 0.045885079866948036, 0.1766187273446289, 0.05592562232493657, 0.3106992223083062, 0.2892959260398362]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]] [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]] [[1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]]
actor:  1 policy actor:  1  step number:  37 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.21797999999999984 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  0.21797999999999984 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  0.21797999999999984 0.6789999999999999 0.6789999999999999
probs:  [0.12157542211534404, 0.045885079866948036, 0.1766187273446289, 0.05592562232493657, 0.3106992223083062, 0.2892959260398362]
printing an ep nov before normalisation:  50.38916230218936
actor:  0 policy actor:  0  step number:  44 total reward:  0.16999999999999948  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.21725999999999984 0.6789999999999999 0.6789999999999999
probs:  [0.12157542211534404, 0.045885079866948036, 0.1766187273446289, 0.05592562232493657, 0.3106992223083062, 0.2892959260398362]
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.48 ]
 [0.481]
 [0.479]
 [0.479]
 [0.479]
 [0.481]] [[0.204]
 [0.206]
 [0.195]
 [0.204]
 [0.204]
 [0.204]
 [0.205]] [[0.479]
 [0.48 ]
 [0.481]
 [0.479]
 [0.479]
 [0.479]
 [0.481]]
maxi score, test score, baseline:  0.21435999999999983 0.6789999999999999 0.6789999999999999
probs:  [0.12157542211534404, 0.045885079866948036, 0.1766187273446289, 0.05592562232493657, 0.3106992223083062, 0.2892959260398362]
actor:  1 policy actor:  1  step number:  22 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  49.56199904484236
printing an ep nov before normalisation:  34.37799073811476
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.333
from probs:  [0.12157542211534404, 0.045885079866948036, 0.1766187273446289, 0.05592562232493657, 0.3106992223083062, 0.2892959260398362]
maxi score, test score, baseline:  0.21435999999999983 0.6789999999999999 0.6789999999999999
probs:  [0.12157542211534404, 0.045885079866948036, 0.1766187273446289, 0.05592562232493657, 0.3106992223083062, 0.2892959260398362]
using explorer policy with actor:  0
printing an ep nov before normalisation:  33.61289825960894
maxi score, test score, baseline:  0.21435999999999983 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  0.21435999999999983 0.6789999999999999 0.6789999999999999
probs:  [0.12157542211534404, 0.045885079866948036, 0.1766187273446289, 0.05592562232493657, 0.3106992223083062, 0.2892959260398362]
actor:  0 policy actor:  1  step number:  29 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2173599999999998 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  0.2173599999999998 0.6789999999999999 0.6789999999999999
probs:  [0.12157542211534404, 0.045885079866948036, 0.1766187273446289, 0.05592562232493657, 0.3106992223083062, 0.2892959260398362]
printing an ep nov before normalisation:  32.48557684341321
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  56 total reward:  0.1899999999999995  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.011]
 [1.011]
 [1.009]
 [0.992]
 [0.992]
 [0.992]
 [1.012]] [[38.947]
 [33.182]
 [35.459]
 [39.233]
 [39.233]
 [39.233]
 [35.019]] [[1.011]
 [1.011]
 [1.009]
 [0.992]
 [0.992]
 [0.992]
 [1.012]]
maxi score, test score, baseline:  0.21465999999999988 0.6789999999999999 0.6789999999999999
probs:  [0.12157542211534404, 0.045885079866948036, 0.1766187273446289, 0.05592562232493657, 0.3106992223083062, 0.2892959260398362]
actor:  0 policy actor:  0  step number:  27 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  33 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7884063
actor:  1 policy actor:  1  step number:  22 total reward:  0.7699999999999999  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  46.50454409462202
actions average: 
K:  0  action  0 :  tensor([0.5215, 0.0008, 0.0595, 0.1103, 0.1649, 0.0776, 0.0655],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0010,     0.9635,     0.0010,     0.0095,     0.0002,     0.0003,
            0.0245], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0643, 0.0014, 0.6822, 0.0621, 0.0550, 0.0718, 0.0632],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1512, 0.0885, 0.1709, 0.1782, 0.1197, 0.1207, 0.1707],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1330, 0.0103, 0.1410, 0.1346, 0.3149, 0.1491, 0.1171],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0932, 0.0164, 0.1102, 0.0924, 0.0763, 0.5260, 0.0854],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1171, 0.0981, 0.1448, 0.1175, 0.0878, 0.0967, 0.3380],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.21421999999999983 0.6789999999999999 0.6789999999999999
probs:  [0.09088214822610087, 0.2884916892316056, 0.13179973186445912, 0.04235333603302719, 0.23116760398804573, 0.21530549065676133]
printing an ep nov before normalisation:  60.242757975557545
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.779]
 [0.738]
 [0.737]
 [0.716]
 [0.663]
 [0.733]] [[47.289]
 [47.29 ]
 [43.801]
 [43.777]
 [44.75 ]
 [47.145]
 [43.558]] [[2.232]
 [2.345]
 [2.189]
 [2.187]
 [2.198]
 [2.225]
 [2.176]]
actor:  1 policy actor:  1  step number:  22 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.21421999999999983 0.6789999999999999 0.6789999999999999
probs:  [0.09088214822610087, 0.2884916892316056, 0.13179973186445912, 0.04235333603302719, 0.23116760398804573, 0.21530549065676133]
printing an ep nov before normalisation:  36.860899261691465
maxi score, test score, baseline:  0.21421999999999983 0.6789999999999999 0.6789999999999999
probs:  [0.09088214822610087, 0.2884916892316056, 0.13179973186445912, 0.04235333603302719, 0.23116760398804573, 0.21530549065676133]
printing an ep nov before normalisation:  46.72001838684082
line 256 mcts: sample exp_bonus 0.0015849287720669735
actor:  1 policy actor:  1  step number:  32 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  27 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.0
siam score:  -0.78660494
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]] [[29.849]
 [29.849]
 [29.849]
 [29.849]
 [29.849]
 [29.849]
 [29.849]] [[30.503]
 [30.503]
 [30.503]
 [30.503]
 [30.503]
 [30.503]
 [30.503]]
printing an ep nov before normalisation:  50.797881756217315
actor:  1 policy actor:  1  step number:  27 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.601]
 [0.601]
 [0.718]
 [0.601]
 [0.601]
 [0.601]
 [0.52 ]] [[40.265]
 [40.265]
 [44.605]
 [40.265]
 [40.265]
 [40.265]
 [42.252]] [[1.015]
 [1.015]
 [1.217]
 [1.015]
 [1.015]
 [1.015]
 [0.973]]
actor:  1 policy actor:  1  step number:  22 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.644]
 [0.727]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]] [[30.114]
 [37.652]
 [30.114]
 [30.114]
 [30.114]
 [30.114]
 [30.114]] [[0.849]
 [1.029]
 [0.849]
 [0.849]
 [0.849]
 [0.849]
 [0.849]]
actor:  1 policy actor:  1  step number:  27 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.21421999999999983 0.6789999999999999 0.6789999999999999
probs:  [0.0906569302538054, 0.2885631773546684, 0.1318323738449987, 0.0423638029925957, 0.23122488066322824, 0.21535883489070345]
actor:  1 policy actor:  1  step number:  41 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.21421999999999983 0.6789999999999999 0.6789999999999999
probs:  [0.0906569302538054, 0.2885631773546684, 0.1318323738449987, 0.0423638029925957, 0.23122488066322824, 0.21535883489070345]
printing an ep nov before normalisation:  48.70184174949935
actor:  1 policy actor:  1  step number:  30 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21421999999999983 0.6789999999999999 0.6789999999999999
probs:  [0.0906569302538054, 0.2885631773546684, 0.1318323738449987, 0.0423638029925957, 0.23122488066322824, 0.21535883489070345]
maxi score, test score, baseline:  0.21421999999999983 0.6789999999999999 0.6789999999999999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.62]
 [0.62]
 [0.62]
 [0.62]
 [0.62]
 [0.62]
 [0.62]] [[38.902]
 [38.902]
 [38.902]
 [38.902]
 [38.902]
 [38.902]
 [38.902]] [[2.08]
 [2.08]
 [2.08]
 [2.08]
 [2.08]
 [2.08]
 [2.08]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
Printing some Q and Qe and total Qs values:  [[0.916]
 [0.942]
 [0.875]
 [0.875]
 [0.882]
 [0.875]
 [0.93 ]] [[45.687]
 [42.678]
 [53.651]
 [53.651]
 [50.957]
 [53.651]
 [50.748]] [[0.916]
 [0.942]
 [0.875]
 [0.875]
 [0.882]
 [0.875]
 [0.93 ]]
printing an ep nov before normalisation:  42.62287139892578
printing an ep nov before normalisation:  12.589573860168457
actor:  0 policy actor:  0  step number:  21 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  25.43020841749896
maxi score, test score, baseline:  0.21443999999999985 0.6789999999999999 0.6789999999999999
probs:  [0.0906569302538054, 0.2885631773546684, 0.1318323738449987, 0.0423638029925957, 0.23122488066322824, 0.21535883489070345]
actor:  0 policy actor:  0  step number:  37 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.365]
 [0.339]
 [0.377]
 [0.35 ]
 [0.313]
 [0.377]
 [0.355]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.365]
 [0.339]
 [0.377]
 [0.35 ]
 [0.313]
 [0.377]
 [0.355]]
maxi score, test score, baseline:  0.21457999999999983 0.6789999999999999 0.6789999999999999
probs:  [0.0906569302538054, 0.2885631773546684, 0.1318323738449987, 0.0423638029925957, 0.23122488066322824, 0.21535883489070345]
printing an ep nov before normalisation:  38.72148239974167
Printing some Q and Qe and total Qs values:  [[0.648]
 [0.708]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]] [[39.869]
 [41.936]
 [39.869]
 [39.869]
 [39.869]
 [39.869]
 [39.869]] [[2.158]
 [2.364]
 [2.158]
 [2.158]
 [2.158]
 [2.158]
 [2.158]]
actor:  1 policy actor:  1  step number:  32 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.21457999999999983 0.6789999999999999 0.6789999999999999
probs:  [0.09054502551054519, 0.2885986978771913, 0.13184859276578287, 0.04236900374283462, 0.23125333990014335, 0.21538534020350258]
printing an ep nov before normalisation:  26.492423127605573
maxi score, test score, baseline:  0.21457999999999983 0.6789999999999999 0.6789999999999999
probs:  [0.09054502551054519, 0.2885986978771913, 0.13184859276578287, 0.04236900374283462, 0.23125333990014335, 0.21538534020350258]
actor:  1 policy actor:  1  step number:  38 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21457999999999983 0.6789999999999999 0.6789999999999999
probs:  [0.09054502551054519, 0.2885986978771913, 0.13184859276578287, 0.04236900374283462, 0.23125333990014335, 0.21538534020350258]
actor:  1 policy actor:  1  step number:  32 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.467]
 [0.546]
 [0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]] [[39.063]
 [55.007]
 [39.063]
 [39.063]
 [39.063]
 [39.063]
 [39.063]] [[0.824]
 [1.152]
 [0.824]
 [0.824]
 [0.824]
 [0.824]
 [0.824]]
maxi score, test score, baseline:  0.21457999999999983 0.6789999999999999 0.6789999999999999
probs:  [0.09054502551054519, 0.2885986978771913, 0.13184859276578287, 0.04236900374283462, 0.23125333990014335, 0.21538534020350258]
siam score:  -0.7743585
actor:  1 policy actor:  1  step number:  32 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  0.667
using another actor
from probs:  [0.09054502551054519, 0.2885986978771913, 0.13184859276578287, 0.04236900374283462, 0.23125333990014335, 0.21538534020350258]
actor:  1 policy actor:  1  step number:  22 total reward:  0.77  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  35.09738058529081
actions average: 
K:  2  action  0 :  tensor([0.4871, 0.1016, 0.1210, 0.0682, 0.0842, 0.0659, 0.0721],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0020,     0.9738,     0.0068,     0.0040,     0.0001,     0.0008,
            0.0125], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0164, 0.0010, 0.9327, 0.0142, 0.0094, 0.0170, 0.0092],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0823, 0.0018, 0.1342, 0.5072, 0.0894, 0.1009, 0.0843],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2689, 0.0008, 0.1455, 0.0897, 0.2875, 0.1039, 0.1038],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1047, 0.0005, 0.1350, 0.0779, 0.0744, 0.5160, 0.0913],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0920, 0.2417, 0.0921, 0.0704, 0.0675, 0.0859, 0.3503],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  30 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  0.667
using another actor
maxi score, test score, baseline:  0.21457999999999983 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  0.21457999999999983 0.6789999999999999 0.6789999999999999
probs:  [0.08812826778977768, 0.38777154053143975, 0.11552607581866017, 0.05617176902465876, 0.18146401677947302, 0.17093833005599046]
printing an ep nov before normalisation:  38.48265853669742
printing an ep nov before normalisation:  0.00010000239569762925
printing an ep nov before normalisation:  46.56163959165125
maxi score, test score, baseline:  0.21457999999999983 0.6789999999999999 0.6789999999999999
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  21 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.21781999999999985 0.6789999999999999 0.6789999999999999
actor:  0 policy actor:  1  step number:  31 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.22061999999999984 0.6789999999999999 0.6789999999999999
probs:  [0.08812826778977768, 0.38777154053143975, 0.11552607581866017, 0.05617176902465876, 0.18146401677947302, 0.17093833005599046]
printing an ep nov before normalisation:  0.10299740332413876
printing an ep nov before normalisation:  0.11029111844209183
actor:  1 policy actor:  1  step number:  40 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  38.68907122975371
maxi score, test score, baseline:  0.22061999999999984 0.6789999999999999 0.6789999999999999
probs:  [0.08812826778977768, 0.38777154053143975, 0.11552607581866017, 0.05617176902465876, 0.18146401677947302, 0.17093833005599046]
maxi score, test score, baseline:  0.22061999999999984 0.6789999999999999 0.6789999999999999
probs:  [0.08812826778977768, 0.38777154053143975, 0.11552607581866017, 0.05617176902465876, 0.18146401677947302, 0.17093833005599046]
actor:  0 policy actor:  1  step number:  29 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.22345999999999983 0.6789999999999999 0.6789999999999999
probs:  [0.08812826778977768, 0.38777154053143975, 0.11552607581866017, 0.05617176902465876, 0.18146401677947302, 0.17093833005599046]
maxi score, test score, baseline:  0.22345999999999983 0.6789999999999999 0.6789999999999999
probs:  [0.08812826778977768, 0.38777154053143975, 0.11552607581866017, 0.05617176902465876, 0.18146401677947302, 0.17093833005599046]
actor:  1 policy actor:  1  step number:  34 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  30 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.0
using another actor
from probs:  [0.08812826778977768, 0.38777154053143975, 0.11552607581866017, 0.05617176902465876, 0.18146401677947302, 0.17093833005599046]
maxi score, test score, baseline:  0.22345999999999983 0.6789999999999999 0.6789999999999999
Printing some Q and Qe and total Qs values:  [[0.848]
 [0.819]
 [0.848]
 [0.848]
 [0.848]
 [0.848]
 [0.848]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.848]
 [0.819]
 [0.848]
 [0.848]
 [0.848]
 [0.848]
 [0.848]]
maxi score, test score, baseline:  0.22345999999999983 0.6789999999999999 0.6789999999999999
probs:  [0.08812826778977768, 0.38777154053143975, 0.11552607581866017, 0.05617176902465876, 0.18146401677947302, 0.17093833005599046]
actor:  1 policy actor:  1  step number:  36 total reward:  0.3299999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.22345999999999983 0.6789999999999999 0.6789999999999999
actor:  1 policy actor:  1  step number:  27 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.22345999999999983 0.6789999999999999 0.6789999999999999
probs:  [0.08812826778977768, 0.38777154053143975, 0.11552607581866017, 0.05617176902465876, 0.18146401677947302, 0.17093833005599046]
maxi score, test score, baseline:  0.22345999999999983 0.6789999999999999 0.6789999999999999
probs:  [0.08812826778977768, 0.38777154053143975, 0.11552607581866017, 0.05617176902465876, 0.18146401677947302, 0.17093833005599046]
UNIT TEST: sample policy line 217 mcts : [0.    0.776 0.    0.184 0.    0.    0.041]
actor:  0 policy actor:  1  step number:  41 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  27 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2232799999999998 0.6789999999999999 0.6789999999999999
probs:  [0.08812826778977768, 0.38777154053143975, 0.11552607581866017, 0.05617176902465876, 0.18146401677947302, 0.17093833005599046]
actor:  1 policy actor:  1  step number:  36 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.2232799999999998 0.6789999999999999 0.6789999999999999
probs:  [0.08812826778977768, 0.38777154053143975, 0.11552607581866017, 0.05617176902465876, 0.18146401677947302, 0.17093833005599046]
actor:  0 policy actor:  0  step number:  25 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.22643999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.08812826778977768, 0.38777154053143975, 0.11552607581866017, 0.05617176902465876, 0.18146401677947302, 0.17093833005599046]
maxi score, test score, baseline:  0.22643999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.08812826778977768, 0.38777154053143975, 0.11552607581866017, 0.05617176902465876, 0.18146401677947302, 0.17093833005599046]
maxi score, test score, baseline:  0.22643999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.08812826778977768, 0.38777154053143975, 0.11552607581866017, 0.05617176902465876, 0.18146401677947302, 0.17093833005599046]
maxi score, test score, baseline:  0.22643999999999986 0.6789999999999999 0.6789999999999999
maxi score, test score, baseline:  0.22643999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.08812826778977768, 0.38777154053143975, 0.11552607581866017, 0.05617176902465876, 0.18146401677947302, 0.17093833005599046]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.22643999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.08812826778977768, 0.38777154053143975, 0.11552607581866017, 0.05617176902465876, 0.18146401677947302, 0.17093833005599046]
printing an ep nov before normalisation:  28.846335912543957
maxi score, test score, baseline:  0.22643999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.08812826778977768, 0.38777154053143975, 0.11552607581866017, 0.05617176902465876, 0.18146401677947302, 0.17093833005599046]
actor:  1 policy actor:  1  step number:  37 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  31 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.22643999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.08812826778977768, 0.38777154053143975, 0.11552607581866017, 0.05617176902465876, 0.18146401677947302, 0.17093833005599046]
maxi score, test score, baseline:  0.22643999999999986 0.6789999999999999 0.6789999999999999
printing an ep nov before normalisation:  46.642131314284846
Printing some Q and Qe and total Qs values:  [[0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]] [[29.042]
 [29.042]
 [29.042]
 [29.042]
 [29.042]
 [29.042]
 [29.042]] [[1.022]
 [1.022]
 [1.022]
 [1.022]
 [1.022]
 [1.022]
 [1.022]]
actor:  0 policy actor:  0  step number:  22 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.339]
 [0.383]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.339]
 [0.383]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]]
actor:  1 policy actor:  1  step number:  26 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.22629999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.08812826778977768, 0.38777154053143975, 0.11552607581866017, 0.05617176902465876, 0.18146401677947302, 0.17093833005599046]
maxi score, test score, baseline:  0.22629999999999986 0.6789999999999999 0.6789999999999999
printing an ep nov before normalisation:  36.80808391292771
actor:  1 policy actor:  1  step number:  48 total reward:  0.20999999999999952  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  47.165373641866125
maxi score, test score, baseline:  0.22629999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.08812826778977768, 0.38777154053143975, 0.11552607581866017, 0.05617176902465876, 0.18146401677947302, 0.17093833005599046]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.22629999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.08812826778977768, 0.38777154053143975, 0.11552607581866017, 0.05617176902465876, 0.18146401677947302, 0.17093833005599046]
printing an ep nov before normalisation:  28.946892970955933
printing an ep nov before normalisation:  30.65447644704338
maxi score, test score, baseline:  0.22629999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.08812826778977768, 0.38777154053143975, 0.11552607581866017, 0.05617176902465876, 0.18146401677947302, 0.17093833005599046]
actor:  1 policy actor:  1  step number:  35 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  0.0002589230598459835
actor:  1 policy actor:  1  step number:  34 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.231]
 [0.347]
 [0.226]
 [0.229]
 [0.228]
 [0.229]
 [0.225]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.231]
 [0.347]
 [0.226]
 [0.229]
 [0.228]
 [0.229]
 [0.225]]
printing an ep nov before normalisation:  43.1922608746991
actor:  1 policy actor:  1  step number:  38 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.22629999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.08812826778977768, 0.38777154053143975, 0.11552607581866017, 0.05617176902465876, 0.18146401677947302, 0.17093833005599046]
maxi score, test score, baseline:  0.22629999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.08812826778977768, 0.38777154053143975, 0.11552607581866017, 0.05617176902465876, 0.18146401677947302, 0.17093833005599046]
maxi score, test score, baseline:  0.22629999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.08812826778977768, 0.38777154053143975, 0.11552607581866017, 0.05617176902465876, 0.18146401677947302, 0.17093833005599046]
maxi score, test score, baseline:  0.22629999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.08812826778977768, 0.38777154053143975, 0.11552607581866017, 0.05617176902465876, 0.18146401677947302, 0.17093833005599046]
maxi score, test score, baseline:  0.22629999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.08812826778977768, 0.38777154053143975, 0.11552607581866017, 0.05617176902465876, 0.18146401677947302, 0.17093833005599046]
Printing some Q and Qe and total Qs values:  [[0.243]
 [0.259]
 [0.244]
 [0.243]
 [0.243]
 [0.243]
 [0.243]] [[26.345]
 [24.27 ]
 [25.937]
 [26.345]
 [26.345]
 [26.345]
 [26.345]] [[0.837]
 [0.763]
 [0.821]
 [0.837]
 [0.837]
 [0.837]
 [0.837]]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.22629999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.08812826778977768, 0.38777154053143975, 0.11552607581866017, 0.05617176902465876, 0.18146401677947302, 0.17093833005599046]
maxi score, test score, baseline:  0.22629999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.08812826778977768, 0.38777154053143975, 0.11552607581866017, 0.05617176902465876, 0.18146401677947302, 0.17093833005599046]
actor:  1 policy actor:  1  step number:  49 total reward:  0.09999999999999942  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7702484
printing an ep nov before normalisation:  40.440219680400425
Printing some Q and Qe and total Qs values:  [[0.867]
 [0.867]
 [0.833]
 [0.867]
 [0.867]
 [0.867]
 [0.867]] [[34.897]
 [34.897]
 [33.88 ]
 [34.897]
 [34.897]
 [34.897]
 [34.897]] [[0.867]
 [0.867]
 [0.833]
 [0.867]
 [0.867]
 [0.867]
 [0.867]]
maxi score, test score, baseline:  0.22629999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.08812826778977768, 0.38777154053143975, 0.11552607581866017, 0.05617176902465876, 0.18146401677947302, 0.17093833005599046]
printing an ep nov before normalisation:  35.95359922155665
actions average: 
K:  2  action  0 :  tensor([0.5224, 0.0384, 0.0859, 0.0697, 0.0957, 0.0948, 0.0932],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0021,     0.9673,     0.0078,     0.0039,     0.0003,     0.0029,
            0.0156], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0726, 0.0040, 0.4834, 0.0552, 0.0572, 0.2000, 0.1278],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0519, 0.0840, 0.0745, 0.4754, 0.0739, 0.0876, 0.1528],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0949, 0.0141, 0.0457, 0.0296, 0.7154, 0.0407, 0.0594],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1087, 0.0011, 0.1777, 0.0980, 0.1241, 0.3545, 0.1359],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0946, 0.1602, 0.1241, 0.0893, 0.0917, 0.0951, 0.3451],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  33.70586395263672
actions average: 
K:  3  action  0 :  tensor([0.5451, 0.0258, 0.0825, 0.0572, 0.1213, 0.0740, 0.0942],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0036,     0.9796,     0.0006,     0.0036,     0.0001,     0.0001,
            0.0125], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0538, 0.0329, 0.6863, 0.0434, 0.0521, 0.0688, 0.0627],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1086, 0.0543, 0.1201, 0.2730, 0.1541, 0.1622, 0.1276],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1218, 0.0015, 0.1499, 0.0944, 0.3918, 0.1210, 0.1197],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0853, 0.0071, 0.1024, 0.0884, 0.1002, 0.4157, 0.2010],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0577, 0.2850, 0.0532, 0.0841, 0.0593, 0.0625, 0.3982],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  36 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  29 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  34.256307511848185
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  41.504345648427154
maxi score, test score, baseline:  0.21935999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.08805415357178648, 0.3878030696512543, 0.11553546146201545, 0.05617632700009808, 0.18147876558009876, 0.17095222273474678]
maxi score, test score, baseline:  0.21935999999999986 0.6789999999999999 0.6789999999999999
probs:  [0.08805415357178648, 0.3878030696512543, 0.11553546146201545, 0.05617632700009808, 0.18147876558009876, 0.17095222273474678]
Printing some Q and Qe and total Qs values:  [[ 0.581]
 [ 0.383]
 [-0.069]
 [ 0.416]
 [ 0.537]
 [-0.054]
 [ 0.379]] [[34.082]
 [36.898]
 [35.795]
 [36.879]
 [36.721]
 [41.14 ]
 [38.334]] [[1.456]
 [1.43 ]
 [0.91 ]
 [1.461]
 [1.573]
 [1.25 ]
 [1.513]]
Printing some Q and Qe and total Qs values:  [[0.846]
 [0.933]
 [0.846]
 [0.846]
 [0.846]
 [0.846]
 [0.846]] [[36.781]
 [40.596]
 [36.781]
 [36.781]
 [36.781]
 [36.781]
 [36.781]] [[0.846]
 [0.933]
 [0.846]
 [0.846]
 [0.846]
 [0.846]
 [0.846]]
actor:  0 policy actor:  1  step number:  21 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.21911999999999987 0.6789999999999999 0.6789999999999999
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.155]
 [0.147]
 [0.164]
 [0.155]
 [0.155]
 [0.155]
 [0.166]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.155]
 [0.147]
 [0.164]
 [0.155]
 [0.155]
 [0.155]
 [0.166]]
Printing some Q and Qe and total Qs values:  [[0.05 ]
 [0.058]
 [0.051]
 [0.051]
 [0.052]
 [0.052]
 [0.053]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.05 ]
 [0.058]
 [0.051]
 [0.051]
 [0.052]
 [0.052]
 [0.053]]
maxi score, test score, baseline:  0.21911999999999987 0.6789999999999999 0.6789999999999999
Printing some Q and Qe and total Qs values:  [[0.443]
 [0.443]
 [0.707]
 [0.443]
 [0.443]
 [0.443]
 [0.443]] [[33.874]
 [33.874]
 [39.364]
 [33.874]
 [33.874]
 [33.874]
 [33.874]] [[0.443]
 [0.443]
 [0.707]
 [0.443]
 [0.443]
 [0.443]
 [0.443]]
Printing some Q and Qe and total Qs values:  [[0.443]
 [0.443]
 [0.665]
 [0.443]
 [0.443]
 [0.443]
 [0.443]] [[33.35 ]
 [33.35 ]
 [40.677]
 [33.35 ]
 [33.35 ]
 [33.35 ]
 [33.35 ]] [[0.443]
 [0.443]
 [0.665]
 [0.443]
 [0.443]
 [0.443]
 [0.443]]
maxi score, test score, baseline:  0.21911999999999987 0.6789999999999999 0.6789999999999999
actor:  1 policy actor:  1  step number:  32 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.757]
 [0.668]
 [0.668]
 [0.681]
 [0.668]
 [0.668]] [[40.806]
 [38.351]
 [48.814]
 [48.814]
 [43.484]
 [48.814]
 [48.814]] [[1.185]
 [1.203]
 [1.298]
 [1.298]
 [1.218]
 [1.298]
 [1.298]]
printing an ep nov before normalisation:  41.029181403727556
maxi score, test score, baseline:  0.21911999999999987 0.6789999999999999 0.6789999999999999
probs:  [0.08805415357178648, 0.3878030696512543, 0.11553546146201545, 0.05617632700009808, 0.18147876558009876, 0.17095222273474678]
printing an ep nov before normalisation:  40.149328430755375
printing an ep nov before normalisation:  27.128283977508545
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 0.2199460666908546
actor:  0 policy actor:  0  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  40.1459120940917
actor:  0 policy actor:  0  step number:  19 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  19 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  21 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.22403999999999985 0.6729999999999999 0.6729999999999999
Printing some Q and Qe and total Qs values:  [[ 0.171]
 [ 0.425]
 [ 0.22 ]
 [-0.012]
 [ 0.359]
 [ 0.093]
 [ 0.109]] [[34.802]
 [35.793]
 [34.9  ]
 [37.577]
 [37.656]
 [38.554]
 [38.401]] [[0.46 ]
 [0.733]
 [0.511]
 [0.33 ]
 [0.703]
 [0.454]
 [0.467]]
using another actor
from probs:  [0.09270787721887953, 0.36167355527636486, 0.11506767238524776, 0.05233638690839405, 0.18861411035960174, 0.18960039785151214]
using another actor
from probs:  [0.09270787721887953, 0.36167355527636486, 0.11506767238524776, 0.05233638690839405, 0.18861411035960174, 0.18960039785151214]
actor:  1 policy actor:  1  step number:  36 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.22403999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.09270787721887953, 0.36167355527636486, 0.11506767238524776, 0.05233638690839405, 0.18861411035960174, 0.18960039785151214]
actor:  0 policy actor:  1  step number:  23 total reward:  0.7  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.22129999999999986 0.6729999999999999 0.6729999999999999
probs:  [0.09270787721887953, 0.36167355527636486, 0.11506767238524776, 0.05233638690839405, 0.18861411035960174, 0.18960039785151214]
using explorer policy with actor:  1
siam score:  -0.78702754
maxi score, test score, baseline:  0.22129999999999986 0.6729999999999999 0.6729999999999999
probs:  [0.09270787721887953, 0.36167355527636486, 0.11506767238524776, 0.05233638690839405, 0.18861411035960174, 0.18960039785151214]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  50.962718274306546
printing an ep nov before normalisation:  26.152910977233645
actor:  1 policy actor:  1  step number:  39 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.22129999999999986 0.6729999999999999 0.6729999999999999
probs:  [0.0926060956813377, 0.3617141432707154, 0.11508057539567848, 0.052342247448428844, 0.18863526999363295, 0.1896216682102066]
actor:  1 policy actor:  1  step number:  51 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.152]
 [0.269]
 [0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.152]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.152]
 [0.269]
 [0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.152]]
Printing some Q and Qe and total Qs values:  [[0.16]
 [0.16]
 [0.16]
 [0.16]
 [0.16]
 [0.16]
 [0.16]] [[19.569]
 [19.569]
 [19.569]
 [19.569]
 [19.569]
 [19.569]
 [19.569]] [[0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]]
line 256 mcts: sample exp_bonus 30.17598376531597
maxi score, test score, baseline:  0.22129999999999986 0.6729999999999999 0.6729999999999999
probs:  [0.0926060956813377, 0.3617141432707154, 0.11508057539567848, 0.052342247448428844, 0.18863526999363295, 0.1896216682102066]
printing an ep nov before normalisation:  42.77673261188753
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  24 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  23 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.237]
 [0.363]
 [0.228]
 [0.267]
 [0.237]
 [0.274]
 [0.271]] [[26.831]
 [28.329]
 [29.801]
 [26.74 ]
 [26.831]
 [27.019]
 [26.526]] [[0.564]
 [0.743]
 [0.658]
 [0.591]
 [0.564]
 [0.608]
 [0.588]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  20.587470531463623
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]] [[55.245]
 [55.245]
 [55.245]
 [55.245]
 [55.245]
 [55.245]
 [55.245]] [[1.978]
 [1.978]
 [1.978]
 [1.978]
 [1.978]
 [1.978]
 [1.978]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  33 total reward:  0.5599999999999999  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  44.97916993457102
maxi score, test score, baseline:  0.22161999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.0926060956813377, 0.3617141432707154, 0.11508057539567848, 0.052342247448428844, 0.18863526999363295, 0.1896216682102066]
printing an ep nov before normalisation:  45.3755299379741
actor:  1 policy actor:  1  step number:  44 total reward:  0.2899999999999996  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.692]
 [0.647]
 [0.625]
 [0.625]
 [0.625]
 [0.601]] [[41.902]
 [41.221]
 [40.412]
 [34.192]
 [34.192]
 [34.192]
 [41.197]] [[1.615]
 [1.666]
 [1.583]
 [1.261]
 [1.261]
 [1.261]
 [1.575]]
maxi score, test score, baseline:  0.22161999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.0926060956813377, 0.3617141432707154, 0.11508057539567848, 0.052342247448428844, 0.18863526999363295, 0.1896216682102066]
actions average: 
K:  0  action  0 :  tensor([0.7628, 0.0008, 0.0510, 0.0354, 0.0573, 0.0414, 0.0513],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0014,     0.9754,     0.0011,     0.0133,     0.0001,     0.0001,
            0.0086], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0569, 0.0026, 0.7396, 0.0484, 0.0549, 0.0537, 0.0439],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1175, 0.0156, 0.1357, 0.3835, 0.1272, 0.1162, 0.1043],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0976, 0.0182, 0.1034, 0.0834, 0.5566, 0.0724, 0.0683],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([    0.0581,     0.0004,     0.1538,     0.0681,     0.0609,     0.6112,
            0.0475], grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1367, 0.0470, 0.1354, 0.1370, 0.1254, 0.1115, 0.3070],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  26 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  37.18620212640994
maxi score, test score, baseline:  0.22161999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.0926060956813377, 0.3617141432707154, 0.11508057539567848, 0.052342247448428844, 0.18863526999363295, 0.1896216682102066]
using explorer policy with actor:  1
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  39.07408194061944
maxi score, test score, baseline:  0.22161999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.0926060956813377, 0.3617141432707154, 0.11508057539567848, 0.052342247448428844, 0.18863526999363295, 0.1896216682102066]
printing an ep nov before normalisation:  40.97439825896866
actor:  0 policy actor:  0  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  39 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.22133999999999981 0.6729999999999999 0.6729999999999999
probs:  [0.09250473784881588, 0.36175456230183983, 0.11509342469233722, 0.052348083591698874, 0.18865634154250271, 0.1896428500228054]
Printing some Q and Qe and total Qs values:  [[0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.833]
 [0.815]
 [0.815]] [[50.393]
 [50.393]
 [50.393]
 [50.393]
 [50.278]
 [50.393]
 [50.393]] [[2.475]
 [2.475]
 [2.475]
 [2.475]
 [2.487]
 [2.475]
 [2.475]]
maxi score, test score, baseline:  0.22133999999999981 0.6729999999999999 0.6729999999999999
probs:  [0.09250473784881588, 0.36175456230183983, 0.11509342469233722, 0.052348083591698874, 0.18865634154250271, 0.1896428500228054]
Printing some Q and Qe and total Qs values:  [[0.832]
 [0.825]
 [0.825]
 [0.825]
 [0.825]
 [0.825]
 [0.825]] [[23.894]
 [16.185]
 [16.185]
 [16.185]
 [16.185]
 [16.185]
 [16.185]] [[1.745]
 [1.291]
 [1.291]
 [1.291]
 [1.291]
 [1.291]
 [1.291]]
maxi score, test score, baseline:  0.22133999999999981 0.6729999999999999 0.6729999999999999
probs:  [0.09250473784881588, 0.36175456230183983, 0.11509342469233722, 0.052348083591698874, 0.18865634154250271, 0.1896428500228054]
Printing some Q and Qe and total Qs values:  [[0.831]
 [0.831]
 [0.818]
 [0.788]
 [0.785]
 [0.831]
 [0.788]] [[14.375]
 [14.375]
 [25.912]
 [23.132]
 [23.281]
 [14.375]
 [22.487]] [[1.397]
 [1.397]
 [1.837]
 [1.698]
 [1.701]
 [1.397]
 [1.672]]
actor:  1 policy actor:  1  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  45 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.22133999999999981 0.6729999999999999 0.6729999999999999
probs:  [0.09250473784881588, 0.36175456230183983, 0.11509342469233722, 0.052348083591698874, 0.18865634154250271, 0.1896428500228054]
printing an ep nov before normalisation:  50.91629756827194
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.436]
 [0.436]
 [0.435]
 [0.436]
 [0.436]
 [0.436]
 [0.483]] [[34.737]
 [34.737]
 [44.168]
 [34.737]
 [34.737]
 [34.737]
 [43.841]] [[0.833]
 [0.833]
 [1.171]
 [0.833]
 [0.833]
 [0.833]
 [1.207]]
maxi score, test score, baseline:  0.22133999999999981 0.6729999999999999 0.6729999999999999
probs:  [0.0924038010810553, 0.36179481342260883, 0.115106220609934, 0.05235389549022912, 0.1886773255551015, 0.18966394384107135]
printing an ep nov before normalisation:  29.907585245256676
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]] [[39.588]
 [39.588]
 [39.588]
 [39.588]
 [39.588]
 [39.588]
 [39.588]] [[0.847]
 [0.847]
 [0.847]
 [0.847]
 [0.847]
 [0.847]
 [0.847]]
maxi score, test score, baseline:  0.22133999999999981 0.6729999999999999 0.6729999999999999
probs:  [0.0924038010810553, 0.36179481342260883, 0.115106220609934, 0.05235389549022912, 0.1886773255551015, 0.18966394384107135]
actor:  1 policy actor:  1  step number:  36 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.333
from probs:  [0.0924038010810553, 0.36179481342260883, 0.115106220609934, 0.05235389549022912, 0.1886773255551015, 0.18966394384107135]
maxi score, test score, baseline:  0.22133999999999981 0.6729999999999999 0.6729999999999999
probs:  [0.0924038010810553, 0.36179481342260883, 0.115106220609934, 0.05235389549022912, 0.1886773255551015, 0.18966394384107135]
maxi score, test score, baseline:  0.22133999999999981 0.6729999999999999 0.6729999999999999
maxi score, test score, baseline:  0.22133999999999981 0.6729999999999999 0.6729999999999999
probs:  [0.0924038010810553, 0.36179481342260883, 0.115106220609934, 0.05235389549022912, 0.1886773255551015, 0.18966394384107135]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  2.0
from probs:  [0.0924038010810553, 0.36179481342260883, 0.115106220609934, 0.05235389549022912, 0.1886773255551015, 0.18966394384107135]
maxi score, test score, baseline:  0.22133999999999981 0.6729999999999999 0.6729999999999999
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.2182199999999999 0.6729999999999999 0.6729999999999999
probs:  [0.0924038010810553, 0.36179481342260883, 0.115106220609934, 0.05235389549022912, 0.1886773255551015, 0.18966394384107135]
actor:  1 policy actor:  1  step number:  34 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  23 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.21783999999999984 0.6729999999999999 0.6729999999999999
printing an ep nov before normalisation:  24.085323015848797
maxi score, test score, baseline:  0.21783999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.0924038010810553, 0.36179481342260883, 0.115106220609934, 0.05235389549022912, 0.1886773255551015, 0.18966394384107135]
printing an ep nov before normalisation:  33.11138485173652
actor:  1 policy actor:  1  step number:  33 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  13.367247581481934
Printing some Q and Qe and total Qs values:  [[-0.087]
 [-0.097]
 [-0.077]
 [-0.079]
 [-0.092]
 [-0.074]
 [-0.09 ]] [[12.75 ]
 [18.29 ]
 [17.318]
 [12.946]
 [15.918]
 [13.27 ]
 [15.984]] [[0.16 ]
 [0.258]
 [0.259]
 [0.172]
 [0.217]
 [0.183]
 [0.22 ]]
printing an ep nov before normalisation:  30.62428288349668
actor:  1 policy actor:  1  step number:  31 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  33.62740156795176
maxi score, test score, baseline:  0.21783999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.0924038010810553, 0.36179481342260883, 0.115106220609934, 0.05235389549022912, 0.1886773255551015, 0.18966394384107135]
actor:  0 policy actor:  0  step number:  18 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  28.139662742614746
maxi score, test score, baseline:  0.21779999999999985 0.6729999999999999 0.6729999999999999
actor:  1 policy actor:  1  step number:  32 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.21779999999999985 0.6729999999999999 0.6729999999999999
maxi score, test score, baseline:  0.21779999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.0924038010810553, 0.36179481342260883, 0.115106220609934, 0.05235389549022912, 0.1886773255551015, 0.18966394384107135]
printing an ep nov before normalisation:  26.475697994827797
printing an ep nov before normalisation:  32.466607687746226
maxi score, test score, baseline:  0.21779999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.0924038010810553, 0.36179481342260883, 0.115106220609934, 0.05235389549022912, 0.1886773255551015, 0.18966394384107135]
maxi score, test score, baseline:  0.21779999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.0924038010810553, 0.36179481342260883, 0.115106220609934, 0.05235389549022912, 0.1886773255551015, 0.18966394384107135]
printing an ep nov before normalisation:  33.72459036448892
maxi score, test score, baseline:  0.21779999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.0924038010810553, 0.36179481342260883, 0.115106220609934, 0.05235389549022912, 0.1886773255551015, 0.18966394384107135]
printing an ep nov before normalisation:  44.64148998260498
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.584]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]] [[53.743]
 [39.849]
 [53.743]
 [53.743]
 [53.743]
 [53.743]
 [53.743]] [[0.963]
 [0.933]
 [0.963]
 [0.963]
 [0.963]
 [0.963]
 [0.963]]
printing an ep nov before normalisation:  40.54810482824292
actor:  1 policy actor:  1  step number:  42 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  1.9552490190288079
actions average: 
K:  1  action  0 :  tensor([0.3384, 0.0059, 0.1528, 0.1116, 0.1502, 0.1160, 0.1252],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0010,     0.9773,     0.0021,     0.0044,     0.0001,     0.0002,
            0.0149], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0675, 0.0024, 0.6159, 0.0776, 0.0617, 0.0975, 0.0774],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0962, 0.0345, 0.1070, 0.4101, 0.0919, 0.1121, 0.1482],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1546, 0.0044, 0.1615, 0.1963, 0.1633, 0.1710, 0.1491],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0962, 0.0021, 0.2083, 0.1999, 0.1097, 0.2484, 0.1353],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1398, 0.1018, 0.1205, 0.1060, 0.1078, 0.1323, 0.2918],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.21463999999999986 0.6729999999999999 0.6729999999999999
probs:  [0.0924038010810553, 0.36179481342260883, 0.115106220609934, 0.05235389549022912, 0.1886773255551015, 0.18966394384107135]
printing an ep nov before normalisation:  35.4538228720952
printing an ep nov before normalisation:  32.154481410980225
printing an ep nov before normalisation:  31.7199444770813
printing an ep nov before normalisation:  0.09812323675078005
actor:  1 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.21463999999999986 0.6729999999999999 0.6729999999999999
actor:  1 policy actor:  1  step number:  24 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  35.490574535327596
maxi score, test score, baseline:  0.21463999999999986 0.6729999999999999 0.6729999999999999
actor:  1 policy actor:  1  step number:  36 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  23 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  34.227023124694824
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.55411518005137
printing an ep nov before normalisation:  28.355450559606307
actor:  1 policy actor:  1  step number:  25 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  36.71536508560468
maxi score, test score, baseline:  0.21779999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.0924038010810553, 0.36179481342260883, 0.115106220609934, 0.05235389549022912, 0.1886773255551015, 0.18966394384107135]
actor:  1 policy actor:  1  step number:  30 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  2.0
siam score:  -0.77263635
actor:  1 policy actor:  1  step number:  32 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
siam score:  -0.77363485
printing an ep nov before normalisation:  38.753318969009
using another actor
Printing some Q and Qe and total Qs values:  [[0.775]
 [0.012]
 [0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.775]] [[25.662]
 [ 0.265]
 [25.662]
 [25.662]
 [25.662]
 [25.662]
 [25.662]] [[2.254]
 [0.024]
 [2.254]
 [2.254]
 [2.254]
 [2.254]
 [2.254]]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.21779999999999985 0.6729999999999999 0.6729999999999999
printing an ep nov before normalisation:  36.398820877075195
actor:  1 policy actor:  1  step number:  37 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  0
printing an ep nov before normalisation:  42.412438121808115
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.447]
 [0.563]
 [0.447]
 [0.447]
 [0.447]
 [0.521]] [[33.135]
 [33.135]
 [39.141]
 [33.135]
 [33.135]
 [33.135]
 [36.843]] [[0.595]
 [0.595]
 [0.768]
 [0.595]
 [0.595]
 [0.595]
 [0.704]]
printing an ep nov before normalisation:  37.80199188267511
printing an ep nov before normalisation:  36.823210828563056
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]] [[38.056]
 [38.056]
 [38.056]
 [38.056]
 [38.056]
 [38.056]
 [38.056]] [[0.808]
 [0.808]
 [0.808]
 [0.808]
 [0.808]
 [0.808]
 [0.808]]
maxi score, test score, baseline:  0.21509999999999987 0.6729999999999999 0.6729999999999999
probs:  [0.0924038010810553, 0.36179481342260883, 0.115106220609934, 0.05235389549022912, 0.1886773255551015, 0.18966394384107135]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.21509999999999987 0.6729999999999999 0.6729999999999999
printing an ep nov before normalisation:  48.34678956959646
maxi score, test score, baseline:  0.21509999999999987 0.6729999999999999 0.6729999999999999
probs:  [0.0924038010810553, 0.36179481342260883, 0.115106220609934, 0.05235389549022912, 0.1886773255551015, 0.18966394384107135]
maxi score, test score, baseline:  0.21509999999999987 0.6729999999999999 0.6729999999999999
probs:  [0.09220318028801279, 0.3618748161010051, 0.11513165363293361, 0.052365447154881055, 0.18871903314433966, 0.18970586967882783]
printing an ep nov before normalisation:  45.740804671836806
actor:  0 policy actor:  1  step number:  21 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.655]
 [0.76 ]
 [0.655]
 [0.647]
 [0.654]
 [0.654]
 [0.653]] [[19.197]
 [19.353]
 [19.26 ]
 [18.933]
 [19.18 ]
 [19.373]
 [19.147]] [[2.3  ]
 [2.418]
 [2.305]
 [2.269]
 [2.297]
 [2.314]
 [2.294]]
actor:  1 policy actor:  1  step number:  32 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.21515999999999988 0.6729999999999999 0.6729999999999999
probs:  [0.09220318028801279, 0.3618748161010051, 0.11513165363293361, 0.052365447154881055, 0.18871903314433966, 0.18970586967882783]
actor:  1 policy actor:  1  step number:  31 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 31.18053789936772
maxi score, test score, baseline:  0.21515999999999988 0.6729999999999999 0.6729999999999999
probs:  [0.09220318028801279, 0.3618748161010051, 0.11513165363293361, 0.052365447154881055, 0.18871903314433966, 0.18970586967882783]
Printing some Q and Qe and total Qs values:  [[0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.673]] [[40.42 ]
 [40.42 ]
 [40.42 ]
 [40.42 ]
 [40.42 ]
 [40.42 ]
 [41.254]] [[1.49 ]
 [1.49 ]
 [1.49 ]
 [1.49 ]
 [1.49 ]
 [1.49 ]
 [1.463]]
maxi score, test score, baseline:  0.21515999999999988 0.6729999999999999 0.6729999999999999
maxi score, test score, baseline:  0.21515999999999988 0.6729999999999999 0.6729999999999999
probs:  [0.09220318028801279, 0.3618748161010051, 0.11513165363293361, 0.052365447154881055, 0.18871903314433966, 0.18970586967882783]
actor:  1 policy actor:  1  step number:  35 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  36.41751538512122
maxi score, test score, baseline:  0.21515999999999988 0.6729999999999999 0.6729999999999999
probs:  [0.09220318028801279, 0.3618748161010051, 0.11513165363293361, 0.052365447154881055, 0.18871903314433966, 0.18970586967882783]
actor:  1 policy actor:  1  step number:  34 total reward:  0.2899999999999996  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  22 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21537999999999988 0.6729999999999999 0.6729999999999999
probs:  [0.09220318028801279, 0.3618748161010051, 0.11513165363293361, 0.052365447154881055, 0.18871903314433966, 0.18970586967882783]
printing an ep nov before normalisation:  28.556467202930325
actor:  0 policy actor:  1  step number:  33 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21513999999999983 0.6729999999999999 0.6729999999999999
probs:  [0.09220318028801279, 0.3618748161010051, 0.11513165363293361, 0.052365447154881055, 0.18871903314433966, 0.18970586967882783]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.66 ]
 [0.696]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [0.66 ]] [[47.399]
 [42.6  ]
 [47.399]
 [47.399]
 [47.399]
 [47.399]
 [47.399]] [[1.602]
 [1.502]
 [1.602]
 [1.602]
 [1.602]
 [1.602]
 [1.602]]
maxi score, test score, baseline:  0.21513999999999983 0.6729999999999999 0.6729999999999999
maxi score, test score, baseline:  0.21513999999999983 0.6729999999999999 0.6729999999999999
probs:  [0.09220318028801279, 0.3618748161010051, 0.11513165363293361, 0.052365447154881055, 0.18871903314433966, 0.18970586967882783]
actor:  1 policy actor:  1  step number:  31 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.21513999999999983 0.6729999999999999 0.6729999999999999
probs:  [0.09220318028801279, 0.3618748161010051, 0.11513165363293361, 0.052365447154881055, 0.18871903314433966, 0.18970586967882783]
printing an ep nov before normalisation:  46.40186193192609
maxi score, test score, baseline:  0.21513999999999983 0.6729999999999999 0.6729999999999999
probs:  [0.09220318028801279, 0.3618748161010051, 0.11513165363293361, 0.052365447154881055, 0.18871903314433966, 0.18970586967882783]
using explorer policy with actor:  1
from probs:  [0.09220318028801279, 0.3618748161010051, 0.11513165363293361, 0.052365447154881055, 0.18871903314433966, 0.18970586967882783]
maxi score, test score, baseline:  0.21513999999999983 0.6729999999999999 0.6729999999999999
printing an ep nov before normalisation:  30.675030286711156
printing an ep nov before normalisation:  44.443804997043735
maxi score, test score, baseline:  0.21513999999999983 0.6729999999999999 0.6729999999999999
probs:  [0.09220318028801279, 0.3618748161010051, 0.11513165363293361, 0.052365447154881055, 0.18871903314433966, 0.18970586967882783]
printing an ep nov before normalisation:  27.50514974612362
maxi score, test score, baseline:  0.21513999999999983 0.6729999999999999 0.6729999999999999
actor:  0 policy actor:  1  step number:  31 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  54 total reward:  0.16999999999999948  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  22 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
using another actor
maxi score, test score, baseline:  0.21509999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.09220318028801279, 0.3618748161010051, 0.11513165363293361, 0.052365447154881055, 0.18871903314433966, 0.18970586967882783]
maxi score, test score, baseline:  0.21509999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.09220318028801279, 0.3618748161010051, 0.11513165363293361, 0.052365447154881055, 0.18871903314433966, 0.18970586967882783]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.21509999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.09220318028801279, 0.3618748161010051, 0.11513165363293361, 0.052365447154881055, 0.18871903314433966, 0.18970586967882783]
printing an ep nov before normalisation:  50.52412021740531
actor:  0 policy actor:  1  step number:  18 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  3  action  0 :  tensor([0.5123, 0.0866, 0.0728, 0.0730, 0.0813, 0.0618, 0.1122],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0040,     0.9631,     0.0011,     0.0025,     0.0001,     0.0002,
            0.0290], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0664, 0.0517, 0.6881, 0.0393, 0.0512, 0.0598, 0.0436],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1519, 0.0412, 0.1355, 0.2795, 0.1175, 0.0945, 0.1799],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.1487,     0.0002,     0.0762,     0.0713,     0.5749,     0.0654,
            0.0633], grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([    0.0673,     0.0004,     0.1206,     0.0541,     0.0475,     0.6019,
            0.1081], grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1213, 0.1414, 0.1337, 0.1344, 0.1299, 0.1301, 0.2090],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  20 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  31.971035115240753
maxi score, test score, baseline:  0.21839999999999982 0.6729999999999999 0.6729999999999999
printing an ep nov before normalisation:  36.00282204177148
actor:  1 policy actor:  1  step number:  32 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  30.95219132745114
from probs:  [0.09220318028801279, 0.3618748161010051, 0.11513165363293361, 0.052365447154881055, 0.18871903314433966, 0.18970586967882783]
Printing some Q and Qe and total Qs values:  [[0.087]
 [0.211]
 [0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.087]
 [0.211]
 [0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]]
Printing some Q and Qe and total Qs values:  [[0.294]
 [0.361]
 [0.294]
 [0.294]
 [0.294]
 [0.294]
 [0.294]] [[22.487]
 [27.388]
 [22.487]
 [22.487]
 [22.487]
 [22.487]
 [22.487]] [[0.631]
 [0.842]
 [0.631]
 [0.631]
 [0.631]
 [0.631]
 [0.631]]
siam score:  -0.77668935
maxi score, test score, baseline:  0.21839999999999982 0.6729999999999999 0.6729999999999999
probs:  [0.09220318028801279, 0.3618748161010051, 0.11513165363293361, 0.052365447154881055, 0.18871903314433966, 0.18970586967882783]
Printing some Q and Qe and total Qs values:  [[0.152]
 [0.181]
 [0.105]
 [0.116]
 [0.057]
 [0.105]
 [0.203]] [[27.363]
 [24.319]
 [25.313]
 [23.515]
 [20.526]
 [25.313]
 [23.214]] [[0.152]
 [0.181]
 [0.105]
 [0.116]
 [0.057]
 [0.105]
 [0.203]]
siam score:  -0.7766212
maxi score, test score, baseline:  0.21839999999999982 0.6729999999999999 0.6729999999999999
probs:  [0.09220318028801279, 0.3618748161010051, 0.11513165363293361, 0.052365447154881055, 0.18871903314433966, 0.18970586967882783]
printing an ep nov before normalisation:  44.08067794948568
printing an ep nov before normalisation:  33.05354595184326
printing an ep nov before normalisation:  40.14399480538801
Printing some Q and Qe and total Qs values:  [[0.91 ]
 [0.962]
 [0.91 ]
 [0.91 ]
 [0.91 ]
 [0.91 ]
 [0.91 ]] [[42.802]
 [41.678]
 [42.802]
 [42.802]
 [42.802]
 [42.802]
 [42.802]] [[0.91 ]
 [0.962]
 [0.91 ]
 [0.91 ]
 [0.91 ]
 [0.91 ]
 [0.91 ]]
maxi score, test score, baseline:  0.21839999999999982 0.6729999999999999 0.6729999999999999
probs:  [0.09220318028801279, 0.3618748161010051, 0.11513165363293361, 0.052365447154881055, 0.18871903314433966, 0.18970586967882783]
actor:  0 policy actor:  0  step number:  39 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  14.231177568435669
actor:  0 policy actor:  1  step number:  32 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.21761999999999987 0.6729999999999999 0.6729999999999999
probs:  [0.09220318028801279, 0.3618748161010051, 0.11513165363293361, 0.052365447154881055, 0.18871903314433966, 0.18970586967882783]
Printing some Q and Qe and total Qs values:  [[0.63 ]
 [0.683]
 [0.644]
 [0.635]
 [0.634]
 [0.627]
 [0.636]] [[31.335]
 [22.692]
 [31.845]
 [32.075]
 [31.187]
 [30.994]
 [31.205]] [[0.896]
 [0.876]
 [0.915]
 [0.908]
 [0.899]
 [0.891]
 [0.901]]
actor:  1 policy actor:  1  step number:  19 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  21 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.21761999999999987 0.6729999999999999 0.6729999999999999
maxi score, test score, baseline:  0.21761999999999987 0.6729999999999999 0.6729999999999999
probs:  [0.09220318028801279, 0.3618748161010051, 0.11513165363293361, 0.052365447154881055, 0.18871903314433966, 0.18970586967882783]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  0.333
from probs:  [0.09220318028801279, 0.3618748161010051, 0.11513165363293361, 0.052365447154881055, 0.18871903314433966, 0.18970586967882783]
maxi score, test score, baseline:  0.21761999999999987 0.6729999999999999 0.6729999999999999
probs:  [0.0921034910907664, 0.36191456972108577, 0.1151442913939949, 0.0523711872188024, 0.18873975779619231, 0.18972670277915815]
printing an ep nov before normalisation:  23.375904498048335
Printing some Q and Qe and total Qs values:  [[0.58 ]
 [0.602]
 [0.584]
 [0.583]
 [0.583]
 [0.576]
 [0.588]] [[47.361]
 [39.56 ]
 [46.105]
 [46.863]
 [46.907]
 [48.378]
 [45.894]] [[2.05 ]
 [1.828]
 [2.014]
 [2.038]
 [2.038]
 [2.078]
 [2.012]]
printing an ep nov before normalisation:  36.39974594116211
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.691823468590762
actor:  1 policy actor:  1  step number:  29 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  24.214108835136976
siam score:  -0.7669432
maxi score, test score, baseline:  0.2148999999999999 0.6729999999999999 0.6729999999999999
maxi score, test score, baseline:  0.2148999999999999 0.6729999999999999 0.6729999999999999
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  30 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
from probs:  [0.0921034910907664, 0.36191456972108577, 0.1151442913939949, 0.0523711872188024, 0.18873975779619231, 0.18972670277915815]
actor:  1 policy actor:  1  step number:  35 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  27 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2148999999999999 0.6729999999999999 0.6729999999999999
probs:  [0.0921034910907664, 0.36191456972108577, 0.1151442913939949, 0.0523711872188024, 0.18873975779619231, 0.18972670277915815]
Printing some Q and Qe and total Qs values:  [[0.217]
 [0.322]
 [0.217]
 [0.217]
 [0.217]
 [0.217]
 [0.217]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.217]
 [0.322]
 [0.217]
 [0.217]
 [0.217]
 [0.217]
 [0.217]]
printing an ep nov before normalisation:  43.03593227599758
maxi score, test score, baseline:  0.21167999999999987 0.6729999999999999 0.6729999999999999
probs:  [0.09200421261390793, 0.36195415955589455, 0.11515687708736705, 0.052376903633608814, 0.18876039706229342, 0.18974745004692822]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.21167999999999987 0.6729999999999999 0.6729999999999999
probs:  [0.09190534232439776, 0.36199358661554626, 0.1151694110341677, 0.05238259654515175, 0.1887809514692429, 0.1897681120114936]
Printing some Q and Qe and total Qs values:  [[0.831]
 [0.009]
 [0.831]
 [0.831]
 [0.008]
 [0.831]
 [0.831]] [[39.031]
 [ 0.425]
 [39.031]
 [39.031]
 [ 0.453]
 [39.031]
 [39.031]] [[1.164]
 [0.012]
 [1.164]
 [1.164]
 [0.012]
 [1.164]
 [1.164]]
actor:  1 policy actor:  1  step number:  22 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  3.0083143267283674
actor:  1 policy actor:  1  step number:  36 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  30 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21167999999999987 0.6729999999999999 0.6729999999999999
printing an ep nov before normalisation:  26.972963548593516
actor:  0 policy actor:  1  step number:  27 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.21123999999999987 0.6729999999999999 0.6729999999999999
printing an ep nov before normalisation:  43.923914144036786
printing an ep nov before normalisation:  0.05499646376847522
from probs:  [0.09170881627898493, 0.3620719564084736, 0.11519432495937562, 0.052393912435880485, 0.18882180779052632, 0.18980918212675904]
Printing some Q and Qe and total Qs values:  [[0.31 ]
 [0.538]
 [0.31 ]
 [0.31 ]
 [0.31 ]
 [0.31 ]
 [0.31 ]] [[37.333]
 [35.829]
 [37.333]
 [37.333]
 [37.333]
 [37.333]
 [37.333]] [[1.536]
 [1.654]
 [1.536]
 [1.536]
 [1.536]
 [1.536]
 [1.536]]
printing an ep nov before normalisation:  54.57243423696027
maxi score, test score, baseline:  0.21123999999999987 0.6729999999999999 0.6729999999999999
probs:  [0.09161115556008831, 0.3621109011208702, 0.11520670556694973, 0.052399535700833486, 0.18884211073663015, 0.1898295913146282]
actor:  1 policy actor:  1  step number:  31 total reward:  0.7  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.21123999999999987 0.6729999999999999 0.6729999999999999
probs:  [0.09048880031344696, 0.3576697474208666, 0.11379485328323467, 0.05175827319264532, 0.19878614517204948, 0.18750218061775695]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.124]
 [0.124]
 [0.124]
 [0.124]
 [0.124]
 [0.124]
 [0.124]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.124]
 [0.124]
 [0.124]
 [0.124]
 [0.124]
 [0.124]
 [0.124]]
printing an ep nov before normalisation:  37.21386687610592
printing an ep nov before normalisation:  44.29405668598594
printing an ep nov before normalisation:  18.56138825416565
maxi score, test score, baseline:  0.20811999999999986 0.6729999999999999 0.6729999999999999
probs:  [0.09048880031344696, 0.3576697474208666, 0.11379485328323467, 0.05175827319264532, 0.19878614517204948, 0.18750218061775695]
maxi score, test score, baseline:  0.20811999999999986 0.6729999999999999 0.6729999999999999
probs:  [0.09048880031344696, 0.3576697474208666, 0.11379485328323467, 0.05175827319264532, 0.19878614517204948, 0.18750218061775695]
printing an ep nov before normalisation:  54.64833922261617
maxi score, test score, baseline:  0.20811999999999986 0.6729999999999999 0.6729999999999999
probs:  [0.09048880031344696, 0.3576697474208666, 0.11379485328323467, 0.05175827319264532, 0.19878614517204948, 0.18750218061775695]
printing an ep nov before normalisation:  46.799151399128576
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  33 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.20839999999999986 0.6729999999999999 0.6729999999999999
probs:  [0.09048880031344696, 0.3576697474208666, 0.11379485328323467, 0.05175827319264532, 0.19878614517204948, 0.18750218061775695]
printing an ep nov before normalisation:  44.15437327764224
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  36 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  0.667
siam score:  -0.76698226
actor:  1 policy actor:  1  step number:  33 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  29 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20839999999999986 0.6729999999999999 0.6729999999999999
maxi score, test score, baseline:  0.20839999999999986 0.6729999999999999 0.6729999999999999
probs:  [0.09048880031344696, 0.3576697474208666, 0.11379485328323467, 0.05175827319264532, 0.19878614517204948, 0.18750218061775695]
printing an ep nov before normalisation:  0.007717159941336149
actor:  1 policy actor:  1  step number:  31 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  37 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.20839999999999986 0.6729999999999999 0.6729999999999999
probs:  [0.09397772022412995, 0.3714753848219615, 0.11818369501435055, 0.053751682622937134, 0.20645677460941797, 0.15615474270720295]
actor:  0 policy actor:  1  step number:  25 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  21 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  1  action  0 :  tensor([0.4471, 0.0220, 0.0882, 0.0952, 0.1067, 0.0891, 0.1516],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0058, 0.9295, 0.0080, 0.0089, 0.0036, 0.0044, 0.0397],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0360, 0.0044, 0.7488, 0.0444, 0.0465, 0.0816, 0.0381],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0905, 0.0029, 0.1098, 0.4202, 0.1396, 0.1103, 0.1266],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2403, 0.0065, 0.0956, 0.1311, 0.3335, 0.0988, 0.0942],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1174, 0.0027, 0.1399, 0.1401, 0.1450, 0.3386, 0.1163],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0549, 0.1289, 0.0999, 0.0956, 0.0449, 0.0562, 0.5196],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  38.12815076339793
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.55 ]
 [0.37 ]
 [0.37 ]
 [0.371]
 [0.371]
 [0.363]] [[29.539]
 [34.333]
 [31.674]
 [31.707]
 [31.785]
 [32.169]
 [31.918]] [[1.26 ]
 [1.614]
 [1.291]
 [1.293]
 [1.298]
 [1.319]
 [1.297]]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21167999999999987 0.6729999999999999 0.6729999999999999
probs:  [0.09397772022412995, 0.3714753848219615, 0.11818369501435055, 0.053751682622937134, 0.20645677460941797, 0.15615474270720295]
printing an ep nov before normalisation:  38.4587149127941
maxi score, test score, baseline:  0.21167999999999987 0.6729999999999999 0.6729999999999999
probs:  [0.09397772022412995, 0.3714753848219615, 0.11818369501435055, 0.053751682622937134, 0.20645677460941797, 0.15615474270720295]
Printing some Q and Qe and total Qs values:  [[0.938]
 [0.934]
 [0.927]
 [0.841]
 [0.809]
 [0.805]
 [0.924]] [[35.429]
 [38.258]
 [37.538]
 [35.528]
 [36.948]
 [37.574]
 [35.65 ]] [[2.253]
 [2.43 ]
 [2.377]
 [2.162]
 [2.221]
 [2.257]
 [2.253]]
printing an ep nov before normalisation:  15.366251244618546
actor:  1 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
using another actor
actor:  1 policy actor:  1  step number:  37 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  37.656959640005105
using explorer policy with actor:  0
maxi score, test score, baseline:  0.21167999999999987 0.6729999999999999 0.6729999999999999
probs:  [0.09397772022412994, 0.37147538482196146, 0.11818369501435054, 0.05375168262293713, 0.20645677460941794, 0.1561547427072031]
using another actor
maxi score, test score, baseline:  0.21167999999999987 0.6729999999999999 0.6729999999999999
probs:  [0.09397772022412995, 0.3714753848219615, 0.11818369501435055, 0.053751682622937134, 0.20645677460941797, 0.15615474270720295]
printing an ep nov before normalisation:  42.36491117189576
Printing some Q and Qe and total Qs values:  [[0.19 ]
 [0.402]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.19 ]
 [0.402]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]]
maxi score, test score, baseline:  0.2083399999999999 0.6729999999999999 0.6729999999999999
probs:  [0.09397772022412995, 0.3714753848219615, 0.11818369501435055, 0.053751682622937134, 0.20645677460941797, 0.15615474270720295]
maxi score, test score, baseline:  0.2083399999999999 0.6729999999999999 0.6729999999999999
probs:  [0.09397772022412995, 0.3714753848219615, 0.11818369501435055, 0.053751682622937134, 0.20645677460941797, 0.15615474270720295]
actor:  0 policy actor:  0  step number:  25 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  30.223233699798584
printing an ep nov before normalisation:  23.568693338484355
Printing some Q and Qe and total Qs values:  [[0.38 ]
 [0.488]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]] [[33.066]
 [35.181]
 [33.066]
 [33.066]
 [33.066]
 [33.066]
 [33.066]] [[1.118]
 [1.309]
 [1.118]
 [1.118]
 [1.118]
 [1.118]
 [1.118]]
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.488]
 [0.52 ]
 [0.488]
 [0.488]
 [0.488]
 [0.402]] [[37.027]
 [37.027]
 [37.888]
 [37.027]
 [37.027]
 [37.027]
 [35.879]] [[1.38 ]
 [1.38 ]
 [1.444]
 [1.38 ]
 [1.38 ]
 [1.38 ]
 [1.251]]
maxi score, test score, baseline:  0.20857999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.09397772022412995, 0.3714753848219615, 0.11818369501435055, 0.053751682622937134, 0.20645677460941797, 0.15615474270720295]
actions average: 
K:  1  action  0 :  tensor([0.4896, 0.0068, 0.1017, 0.0967, 0.1115, 0.1150, 0.0786],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0013,     0.9693,     0.0016,     0.0035,     0.0003,     0.0005,
            0.0235], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0439, 0.0058, 0.6381, 0.0654, 0.0463, 0.1461, 0.0544],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1061, 0.0215, 0.1646, 0.3998, 0.0994, 0.1344, 0.0742],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0835, 0.0037, 0.1064, 0.0881, 0.5650, 0.0845, 0.0686],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1037, 0.0021, 0.1289, 0.1068, 0.0996, 0.4401, 0.1189],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0263, 0.1300, 0.0284, 0.0385, 0.0254, 0.0256, 0.7257],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.20857999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.09397772022412995, 0.3714753848219615, 0.11818369501435055, 0.053751682622937134, 0.20645677460941797, 0.15615474270720295]
printing an ep nov before normalisation:  15.698464682264875
actor:  1 policy actor:  1  step number:  32 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.20535999999999988 0.6729999999999999 0.6729999999999999
probs:  [0.09397772022412995, 0.3714753848219615, 0.11818369501435055, 0.053751682622937134, 0.20645677460941797, 0.15615474270720295]
actions average: 
K:  2  action  0 :  tensor([0.5149, 0.0553, 0.0851, 0.0779, 0.1072, 0.0805, 0.0791],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0021,     0.9641,     0.0045,     0.0052,     0.0002,     0.0004,
            0.0234], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0121, 0.0010, 0.8953, 0.0125, 0.0088, 0.0541, 0.0161],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0777, 0.1791, 0.0872, 0.3011, 0.0778, 0.0736, 0.2035],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1497, 0.0024, 0.0826, 0.0899, 0.5079, 0.0834, 0.0840],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1337, 0.0096, 0.1709, 0.1490, 0.1541, 0.2685, 0.1142],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0928, 0.0221, 0.1121, 0.0992, 0.0955, 0.0898, 0.4884],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.20535999999999988 0.6729999999999999 0.6729999999999999
probs:  [0.09397772022412995, 0.3714753848219615, 0.11818369501435055, 0.053751682622937134, 0.20645677460941797, 0.15615474270720295]
maxi score, test score, baseline:  0.20535999999999988 0.6729999999999999 0.6729999999999999
probs:  [0.09397772022412995, 0.3714753848219615, 0.11818369501435055, 0.053751682622937134, 0.20645677460941797, 0.15615474270720295]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  0
printing an ep nov before normalisation:  36.847046750412595
maxi score, test score, baseline:  0.20535999999999988 0.6729999999999999 0.6729999999999999
probs:  [0.09397772022412995, 0.3714753848219615, 0.11818369501435055, 0.053751682622937134, 0.20645677460941797, 0.15615474270720295]
maxi score, test score, baseline:  0.20535999999999988 0.6729999999999999 0.6729999999999999
probs:  [0.09397772022412995, 0.3714753848219615, 0.11818369501435055, 0.053751682622937134, 0.20645677460941797, 0.15615474270720295]
printing an ep nov before normalisation:  43.05241107940674
actor:  0 policy actor:  1  step number:  37 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  30 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.322]
 [0.351]
 [0.322]
 [0.325]
 [0.326]
 [0.322]
 [0.328]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.322]
 [0.351]
 [0.322]
 [0.325]
 [0.326]
 [0.322]
 [0.328]]
printing an ep nov before normalisation:  31.151212509095014
printing an ep nov before normalisation:  11.582428027246927
actor:  1 policy actor:  1  step number:  20 total reward:  0.7899999999999999  reward:  1.0 rdn_beta:  0.667
from probs:  [0.09397772022412995, 0.3714753848219615, 0.11818369501435055, 0.053751682622937134, 0.20645677460941797, 0.15615474270720295]
maxi score, test score, baseline:  0.20815999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.09054417402325761, 0.43997481406812544, 0.1069175403042227, 0.06333454279029843, 0.1666270739001494, 0.13260185491394655]
printing an ep nov before normalisation:  33.14299585225557
siam score:  -0.7599756
actor:  1 policy actor:  1  step number:  32 total reward:  0.69  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20815999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.09023628577243321, 0.43847696385146734, 0.10655389368279718, 0.06311931506365538, 0.1694627995006543, 0.13215074212899247]
printing an ep nov before normalisation:  44.02964335142088
maxi score, test score, baseline:  0.20815999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.09023628577243321, 0.43847696385146734, 0.10655389368279718, 0.06311931506365538, 0.1694627995006543, 0.13215074212899247]
line 256 mcts: sample exp_bonus 32.40576058377953
actor:  0 policy actor:  1  step number:  37 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  38 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20831999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.09023628577243321, 0.43847696385146734, 0.10655389368279718, 0.06311931506365538, 0.1694627995006543, 0.13215074212899247]
siam score:  -0.75873065
maxi score, test score, baseline:  0.20831999999999984 0.6729999999999999 0.6729999999999999
actor:  1 policy actor:  1  step number:  26 total reward:  0.69  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20831999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.08993048738915295, 0.4369892806630909, 0.10958387596309953, 0.0629055482483634, 0.16888811635281212, 0.1317026913834811]
actor:  0 policy actor:  0  step number:  24 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.20827999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08993048738915295, 0.4369892806630909, 0.10958387596309953, 0.0629055482483634, 0.16888811635281212, 0.1317026913834811]
maxi score, test score, baseline:  0.20827999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08993048738915295, 0.4369892806630909, 0.10958387596309953, 0.0629055482483634, 0.16888811635281212, 0.1317026913834811]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20531999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.08993048738915295, 0.4369892806630909, 0.10958387596309953, 0.0629055482483634, 0.16888811635281212, 0.1317026913834811]
printing an ep nov before normalisation:  43.36674656230557
maxi score, test score, baseline:  0.20531999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.08993048738915295, 0.4369892806630909, 0.10958387596309953, 0.0629055482483634, 0.16888811635281212, 0.1317026913834811]
printing an ep nov before normalisation:  44.203587841390785
actor:  1 policy actor:  1  step number:  30 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  0.333
from probs:  [0.08993048738915295, 0.4369892806630909, 0.10958387596309953, 0.0629055482483634, 0.16888811635281212, 0.1317026913834811]
printing an ep nov before normalisation:  23.48274742330344
maxi score, test score, baseline:  0.20531999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.08986333150345631, 0.43702154077549493, 0.1095919584658247, 0.06291018373899976, 0.16890057824193674, 0.13171240727428757]
maxi score, test score, baseline:  0.20531999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.08986333150345631, 0.43702154077549493, 0.1095919584658247, 0.06291018373899976, 0.16890057824193674, 0.13171240727428757]
actor:  1 policy actor:  1  step number:  49 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.20531999999999984 0.6729999999999999 0.6729999999999999
actor:  1 policy actor:  1  step number:  40 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  44.726417334126864
actor:  0 policy actor:  0  step number:  21 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  22.489746707303954
printing an ep nov before normalisation:  32.31583734968694
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20563999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08979645336776654, 0.4370536674633102, 0.10960000754012937, 0.06291480005771183, 0.16891298858993795, 0.13172208298114418]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.20563999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08979645336776654, 0.4370536674633102, 0.10960000754012937, 0.06291480005771183, 0.16891298858993795, 0.13172208298114418]
actor:  1 policy actor:  1  step number:  40 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  30.807613611040985
maxi score, test score, baseline:  0.20563999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08972985126252028, 0.4370856615525742, 0.1096080233929705, 0.06291939732319388, 0.1689253477159094, 0.13173171875283182]
maxi score, test score, baseline:  0.20563999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08972985126252028, 0.4370856615525742, 0.1096080233929705, 0.06291939732319388, 0.1689253477159094, 0.13173171875283182]
printing an ep nov before normalisation:  33.75494956970215
actor:  1 policy actor:  1  step number:  35 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  35 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.20563999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08972985126252028, 0.4370856615525742, 0.1096080233929705, 0.06291939732319388, 0.1689253477159094, 0.13173171875283182]
maxi score, test score, baseline:  0.20301999999999984 0.6729999999999999 0.6729999999999999
maxi score, test score, baseline:  0.20301999999999984 0.6729999999999999 0.6729999999999999
maxi score, test score, baseline:  0.20301999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.08972985126252028, 0.4370856615525742, 0.1096080233929705, 0.06291939732319388, 0.1689253477159094, 0.13173171875283182]
actor:  1 policy actor:  1  step number:  46 total reward:  0.12999999999999945  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.20301999999999984 0.6729999999999999 0.6729999999999999
maxi score, test score, baseline:  0.20301999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.08966352348231943, 0.4371175238625205, 0.10961600622960001, 0.06292397565316242, 0.16893765593631582, 0.13174131483608173]
printing an ep nov before normalisation:  39.98976201342216
printing an ep nov before normalisation:  25.457420314842558
printing an ep nov before normalisation:  42.37095447255044
maxi score, test score, baseline:  0.20301999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.08966352348231943, 0.4371175238625205, 0.10961600622960001, 0.06292397565316242, 0.16893765593631582, 0.13174131483608173]
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.462]
 [0.405]
 [0.479]
 [0.435]
 [0.479]
 [0.379]] [[16.74 ]
 [21.93 ]
 [17.557]
 [15.315]
 [16.901]
 [15.315]
 [17.666]] [[0.879]
 [1.007]
 [0.84 ]
 [0.859]
 [0.854]
 [0.859]
 [0.817]]
line 256 mcts: sample exp_bonus 17.018555340616366
actor:  1 policy actor:  1  step number:  38 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.20301999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.08966352348231943, 0.4371175238625205, 0.10961600622960001, 0.06292397565316242, 0.16893765593631582, 0.13174131483608173]
actor:  1 policy actor:  1  step number:  30 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  20.250266805905504
printing an ep nov before normalisation:  33.31226103948993
maxi score, test score, baseline:  0.19997999999999988 0.6729999999999999 0.6729999999999999
probs:  [0.08966352348231943, 0.4371175238625205, 0.10961600622960001, 0.06292397565316242, 0.16893765593631582, 0.13174131483608173]
actor:  1 policy actor:  1  step number:  32 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  34.79695430552222
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.455]
 [0.396]
 [0.396]
 [0.396]
 [0.396]
 [0.396]] [[25.171]
 [30.621]
 [25.171]
 [25.171]
 [25.171]
 [25.171]
 [25.171]] [[0.838]
 [1.066]
 [0.838]
 [0.838]
 [0.838]
 [0.838]
 [0.838]]
actor:  1 policy actor:  1  step number:  36 total reward:  0.4299999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.19997999999999988 0.6729999999999999 0.6729999999999999
probs:  [0.08966352348231943, 0.4371175238625205, 0.10961600622960001, 0.06292397565316242, 0.16893765593631582, 0.13174131483608173]
actor:  1 policy actor:  1  step number:  38 total reward:  0.2699999999999996  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  42.23340413338253
printing an ep nov before normalisation:  1.5640027092103992
maxi score, test score, baseline:  0.19717999999999986 0.6729999999999999 0.6729999999999999
probs:  [0.08966352348231943, 0.4371175238625205, 0.10961600622960001, 0.06292397565316242, 0.16893765593631582, 0.13174131483608173]
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  22 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
siam score:  -0.7708754
maxi score, test score, baseline:  0.20039999999999986 0.6729999999999999 0.6729999999999999
probs:  [0.08959746833578587, 0.4371492552056475, 0.10962395625358268, 0.06292853516436629, 0.1689499135650206, 0.13175087147559703]
maxi score, test score, baseline:  0.20039999999999986 0.6729999999999999 0.6729999999999999
probs:  [0.08959746833578587, 0.4371492552056475, 0.10962395625358268, 0.06292853516436629, 0.1689499135650206, 0.13175087147559703]
actor:  1 policy actor:  1  step number:  29 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20039999999999986 0.6729999999999999 0.6729999999999999
probs:  [0.08959746833578587, 0.4371492552056475, 0.10962395625358268, 0.06292853516436629, 0.1689499135650206, 0.13175087147559703]
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.47 ]
 [0.441]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.47 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.47 ]
 [0.47 ]
 [0.441]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.47 ]]
actor:  0 policy actor:  1  step number:  25 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.20003999999999986 0.6729999999999999 0.6729999999999999
probs:  [0.08959746833578587, 0.4371492552056475, 0.10962395625358268, 0.06292853516436629, 0.1689499135650206, 0.13175087147559703]
maxi score, test score, baseline:  0.20003999999999986 0.6729999999999999 0.6729999999999999
probs:  [0.08959746833578587, 0.4371492552056475, 0.10962395625358268, 0.06292853516436629, 0.1689499135650206, 0.13175087147559703]
actor:  1 policy actor:  1  step number:  22 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  39 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.20003999999999986 0.6729999999999999 0.6729999999999999
probs:  [0.08959746833578587, 0.4371492552056475, 0.10962395625358268, 0.06292853516436629, 0.1689499135650206, 0.13175087147559703]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  0.667
from probs:  [0.08959746833578587, 0.4371492552056475, 0.10962395625358268, 0.06292853516436629, 0.1689499135650206, 0.13175087147559703]
maxi score, test score, baseline:  0.20003999999999986 0.6729999999999999 0.6729999999999999
probs:  [0.08959746833578587, 0.4371492552056475, 0.10962395625358268, 0.06292853516436629, 0.1689499135650206, 0.13175087147559703]
actions average: 
K:  4  action  0 :  tensor([0.6308, 0.0135, 0.0912, 0.0451, 0.1025, 0.0248, 0.0920],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0054, 0.9483, 0.0071, 0.0105, 0.0019, 0.0021, 0.0247],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0770, 0.0017, 0.4901, 0.1152, 0.1045, 0.1460, 0.0654],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0912, 0.0294, 0.0958, 0.3765, 0.0952, 0.1519, 0.1600],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1168, 0.0122, 0.0924, 0.1279, 0.3852, 0.1463, 0.1191],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1126, 0.0032, 0.2288, 0.1188, 0.1116, 0.3139, 0.1111],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0616, 0.0021, 0.1354, 0.0895, 0.0768, 0.0820, 0.5527],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  1  step number:  22 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  26.894628804262066
printing an ep nov before normalisation:  26.136797666314916
printing an ep nov before normalisation:  54.65457971963662
actor:  0 policy actor:  0  step number:  22 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  0.667
from probs:  [0.08959746833578587, 0.4371492552056475, 0.10962395625358268, 0.06292853516436629, 0.1689499135650206, 0.13175087147559703]
actor:  0 policy actor:  0  step number:  29 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.903]
 [0.894]
 [0.03 ]
 [0.842]
 [0.837]
 [0.317]
 [0.828]] [[32.715]
 [34.805]
 [32.875]
 [32.454]
 [34.34 ]
 [33.503]
 [32.853]] [[0.903]
 [0.894]
 [0.03 ]
 [0.842]
 [0.837]
 [0.317]
 [0.828]]
maxi score, test score, baseline:  0.20005999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08959746833578587, 0.4371492552056475, 0.10962395625358268, 0.06292853516436629, 0.1689499135650206, 0.13175087147559703]
printing an ep nov before normalisation:  35.89597005863538
maxi score, test score, baseline:  0.20005999999999985 0.6729999999999999 0.6729999999999999
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]] [[41.25]
 [41.25]
 [41.25]
 [41.25]
 [41.25]
 [41.25]
 [41.25]] [[1.853]
 [1.853]
 [1.853]
 [1.853]
 [1.853]
 [1.853]
 [1.853]]
maxi score, test score, baseline:  0.20005999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08959746833578587, 0.4371492552056475, 0.10962395625358268, 0.06292853516436629, 0.1689499135650206, 0.13175087147559703]
Printing some Q and Qe and total Qs values:  [[0.688]
 [0.714]
 [0.683]
 [0.684]
 [0.68 ]
 [0.683]
 [0.68 ]] [[18.875]
 [23.868]
 [21.276]
 [18.366]
 [21.321]
 [18.1  ]
 [21.593]] [[0.688]
 [0.714]
 [0.683]
 [0.684]
 [0.68 ]
 [0.683]
 [0.68 ]]
maxi score, test score, baseline:  0.20005999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08959746833578587, 0.4371492552056475, 0.10962395625358268, 0.06292853516436629, 0.1689499135650206, 0.13175087147559703]
actor:  0 policy actor:  1  step number:  41 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  42 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]] [[23.026]
 [23.026]
 [23.026]
 [23.026]
 [23.026]
 [23.026]
 [23.026]] [[0.926]
 [0.926]
 [0.926]
 [0.926]
 [0.926]
 [0.926]
 [0.926]]
printing an ep nov before normalisation:  34.77733850479126
using explorer policy with actor:  1
actions average: 
K:  4  action  0 :  tensor([0.5418, 0.0021, 0.1175, 0.0816, 0.0814, 0.0770, 0.0985],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0089, 0.9089, 0.0040, 0.0057, 0.0011, 0.0015, 0.0699],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0649, 0.0718, 0.5261, 0.0543, 0.0826, 0.1227, 0.0776],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0373, 0.1328, 0.0587, 0.5752, 0.0637, 0.0517, 0.0806],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1438, 0.0027, 0.0996, 0.1111, 0.4274, 0.1231, 0.0923],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0682, 0.0011, 0.2006, 0.0804, 0.0835, 0.4781, 0.0882],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0918, 0.0077, 0.1515, 0.1056, 0.1209, 0.1259, 0.3965],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.19971999999999984 0.6729999999999999 0.6729999999999999
printing an ep nov before normalisation:  31.049168088069614
printing an ep nov before normalisation:  49.27407320888559
Printing some Q and Qe and total Qs values:  [[0.541]
 [0.677]
 [0.541]
 [0.556]
 [0.541]
 [0.568]
 [0.541]] [[41.917]
 [39.983]
 [41.917]
 [40.935]
 [41.917]
 [41.684]
 [41.917]] [[1.08 ]
 [1.169]
 [1.08 ]
 [1.072]
 [1.08 ]
 [1.102]
 [1.08 ]]
maxi score, test score, baseline:  0.19971999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.08959746833578587, 0.4371492552056475, 0.10962395625358268, 0.06292853516436629, 0.1689499135650206, 0.13175087147559703]
printing an ep nov before normalisation:  0.26956974406715517
actor:  1 policy actor:  1  step number:  28 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  22.285287380218506
actor:  1 policy actor:  1  step number:  38 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  41 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  24.120450318005716
printing an ep nov before normalisation:  43.97519903819134
maxi score, test score, baseline:  0.19971999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.08953168414541715, 0.437180856387788, 0.1096318736668134, 0.06293307597259645, 0.16896212091331214, 0.13176038891407305]
actor:  1 policy actor:  1  step number:  22 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.19971999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.08953168414541715, 0.437180856387788, 0.1096318736668134, 0.06293307597259645, 0.16896212091331214, 0.13176038891407305]
printing an ep nov before normalisation:  40.59833561191829
Printing some Q and Qe and total Qs values:  [[ 0.772]
 [ 0.675]
 [ 0.675]
 [-0.062]
 [ 0.673]
 [ 0.675]
 [ 0.675]] [[41.908]
 [51.836]
 [51.836]
 [49.374]
 [46.311]
 [51.836]
 [51.836]] [[2.128]
 [2.544]
 [2.544]
 [1.68 ]
 [2.256]
 [2.544]
 [2.544]]
maxi score, test score, baseline:  0.19971999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.08953168414541715, 0.437180856387788, 0.1096318736668134, 0.06293307597259645, 0.16896212091331214, 0.13176038891407296]
maxi score, test score, baseline:  0.19971999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.08953168414541715, 0.437180856387788, 0.1096318736668134, 0.06293307597259645, 0.16896212091331214, 0.13176038891407296]
actor:  1 policy actor:  1  step number:  37 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.19657999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.08953168414541715, 0.437180856387788, 0.1096318736668134, 0.06293307597259645, 0.16896212091331214, 0.13176038891407296]
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.684]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.451]
 [0.684]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]]
maxi score, test score, baseline:  0.19657999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.08953168414541715, 0.437180856387788, 0.1096318736668134, 0.06293307597259645, 0.16896212091331214, 0.13176038891407296]
printing an ep nov before normalisation:  44.34212864443925
actor:  1 policy actor:  1  step number:  24 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  50.962550529895964
printing an ep nov before normalisation:  33.19609442521103
actor:  1 policy actor:  1  step number:  32 total reward:  0.69  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.19365999999999986 0.6729999999999999 0.6729999999999999
probs:  [0.08953168414541715, 0.437180856387788, 0.1096318736668134, 0.06293307597259645, 0.16896212091331214, 0.13176038891407296]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
Printing some Q and Qe and total Qs values:  [[0.312]
 [0.416]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]] [[29.094]
 [29.45 ]
 [29.094]
 [29.094]
 [29.094]
 [29.094]
 [29.094]] [[0.467]
 [0.575]
 [0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]]
printing an ep nov before normalisation:  40.75240615745077
maxi score, test score, baseline:  0.19365999999999986 0.6729999999999999 0.6729999999999999
probs:  [0.0894661692474451, 0.43721232820817707, 0.1096397586695336, 0.06293759819269584, 0.16897427828993028, 0.13176986739221808]
actor:  1 policy actor:  1  step number:  37 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.0
from probs:  [0.0894661692474451, 0.43721232820817707, 0.1096397586695336, 0.06293759819269584, 0.16897427828993028, 0.13176986739221808]
Printing some Q and Qe and total Qs values:  [[0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]] [[44.163]
 [44.163]
 [44.163]
 [44.163]
 [44.163]
 [44.163]
 [44.163]] [[0.943]
 [0.943]
 [0.943]
 [0.943]
 [0.943]
 [0.943]
 [0.943]]
printing an ep nov before normalisation:  31.675271401373774
printing an ep nov before normalisation:  31.219101990665994
actor:  1 policy actor:  1  step number:  24 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.19365999999999986 0.6729999999999999 0.6729999999999999
probs:  [0.08940092199169505, 0.4372436714595201, 0.10964761146034917, 0.06294210193856918, 0.16898638600109223, 0.13177930714877437]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.0
siam score:  -0.7545893
maxi score, test score, baseline:  0.19365999999999986 0.6729999999999999 0.6729999999999999
probs:  [0.08940092199169505, 0.4372436714595201, 0.10964761146034917, 0.06294210193856918, 0.16898638600109223, 0.13177930714877437]
printing an ep nov before normalisation:  0.14156640988614658
Printing some Q and Qe and total Qs values:  [[0.929]
 [0.988]
 [0.921]
 [0.892]
 [0.915]
 [0.921]
 [0.915]] [[42.509]
 [35.574]
 [44.471]
 [41.341]
 [42.264]
 [42.341]
 [42.606]] [[0.929]
 [0.988]
 [0.921]
 [0.892]
 [0.915]
 [0.921]
 [0.915]]
actor:  0 policy actor:  0  step number:  25 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.19383999999999982 0.6729999999999999 0.6729999999999999
probs:  [0.08940092199169505, 0.4372436714595201, 0.10964761146034917, 0.06294210193856918, 0.16898638600109223, 0.13177930714877437]
Printing some Q and Qe and total Qs values:  [[0.774]
 [0.774]
 [0.774]
 [0.774]
 [0.774]
 [0.774]
 [0.774]] [[31.912]
 [31.912]
 [31.912]
 [31.912]
 [31.912]
 [31.912]
 [31.912]] [[1.721]
 [1.721]
 [1.721]
 [1.721]
 [1.721]
 [1.721]
 [1.721]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  37.89226332290233
maxi score, test score, baseline:  0.19383999999999982 0.6729999999999999 0.6729999999999999
probs:  [0.08940092199169505, 0.4372436714595201, 0.10964761146034917, 0.06294210193856918, 0.16898638600109223, 0.13177930714877437]
actor:  1 policy actor:  1  step number:  31 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  24 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  0.667
from probs:  [0.08940092199169505, 0.4372436714595201, 0.10964761146034917, 0.06294210193856918, 0.16898638600109223, 0.13177930714877437]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  0.667
using another actor
actions average: 
K:  2  action  0 :  tensor([0.6636, 0.0007, 0.1066, 0.0618, 0.0513, 0.0600, 0.0560],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0021,     0.9901,     0.0025,     0.0005,     0.0001,     0.0004,
            0.0041], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0475, 0.0067, 0.7441, 0.0465, 0.0387, 0.0642, 0.0523],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1301, 0.0467, 0.0963, 0.3838, 0.1269, 0.1090, 0.1073],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.1000,     0.0004,     0.0537,     0.0619,     0.6505,     0.0589,
            0.0747], grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0990, 0.1258, 0.1085, 0.1104, 0.0746, 0.3898, 0.0919],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1217, 0.0434, 0.1147, 0.1197, 0.0955, 0.1090, 0.3960],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19119999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.08940092199169505, 0.4372436714595201, 0.10964761146034917, 0.06294210193856918, 0.16898638600109223, 0.13177930714877437]
maxi score, test score, baseline:  0.19119999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.08940092199169505, 0.4372436714595201, 0.10964761146034917, 0.06294210193856918, 0.16898638600109223, 0.13177930714877437]
Printing some Q and Qe and total Qs values:  [[0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.815]] [[33.822]
 [29.359]
 [33.822]
 [33.822]
 [33.822]
 [33.822]
 [33.822]] [[0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.815]]
printing an ep nov before normalisation:  40.23178577423096
siam score:  -0.75320315
maxi score, test score, baseline:  0.19119999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.08940092199169505, 0.4372436714595201, 0.10964761146034917, 0.06294210193856918, 0.16898638600109223, 0.13177930714877437]
maxi score, test score, baseline:  0.19119999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.08940092199169505, 0.4372436714595201, 0.10964761146034917, 0.06294210193856918, 0.16898638600109223, 0.13177930714877437]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  32.29256996037492
maxi score, test score, baseline:  0.18801999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08940092199169505, 0.4372436714595201, 0.10964761146034917, 0.06294210193856918, 0.16898638600109223, 0.13177930714877437]
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.537]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]] [[24.94 ]
 [39.827]
 [24.94 ]
 [24.94 ]
 [24.94 ]
 [24.94 ]
 [24.94 ]] [[0.741]
 [1.54 ]
 [0.741]
 [0.741]
 [0.741]
 [0.741]
 [0.741]]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  0.667
from probs:  [0.08940092199169505, 0.4372436714595201, 0.10964761146034917, 0.06294210193856918, 0.16898638600109223, 0.13177930714877437]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  32 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  47.60423457976897
maxi score, test score, baseline:  0.18801999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08940092199169505, 0.4372436714595201, 0.10964761146034917, 0.06294210193856918, 0.16898638600109223, 0.13177930714877437]
actor:  0 policy actor:  1  step number:  33 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.656357850453745
maxi score, test score, baseline:  0.18749999999999986 0.6729999999999999 0.6729999999999999
probs:  [0.08940092199169505, 0.4372436714595201, 0.10964761146034917, 0.06294210193856918, 0.16898638600109223, 0.13177930714877437]
printing an ep nov before normalisation:  39.77633849534824
actor:  1 policy actor:  1  step number:  22 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  26 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  38.366432586108125
maxi score, test score, baseline:  0.18749999999999986 0.6729999999999999 0.6729999999999999
actor:  1 policy actor:  1  step number:  29 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18749999999999986 0.6729999999999999 0.6729999999999999
probs:  [0.08933594074144753, 0.43727488692805855, 0.10965543223624633, 0.06294658732319237, 0.16899844435051853, 0.13178870842053658]
actor:  1 policy actor:  1  step number:  26 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.0
from probs:  [0.08933594074144753, 0.43727488692805855, 0.10965543223624633, 0.06294658732319237, 0.16899844435051853, 0.13178870842053658]
maxi score, test score, baseline:  0.18749999999999986 0.6729999999999999 0.6729999999999999
printing an ep nov before normalisation:  24.51882186196844
line 256 mcts: sample exp_bonus 38.03118072371787
actor:  0 policy actor:  1  step number:  31 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.18753999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08927122387330119, 0.43730597539363664, 0.10966322119260853, 0.06295105445862205, 0.16901045363945819, 0.13179807144237332]
actor:  0 policy actor:  0  step number:  33 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  28 total reward:  0.69  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  14.661997272528883
Printing some Q and Qe and total Qs values:  [[0.241]
 [0.281]
 [0.207]
 [0.206]
 [0.207]
 [0.241]
 [0.241]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.241]
 [0.281]
 [0.207]
 [0.206]
 [0.207]
 [0.241]
 [0.241]]
maxi score, test score, baseline:  0.18697999999999984 0.6729999999999999 0.6729999999999999
maxi score, test score, baseline:  0.18697999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.08923140250330894, 0.4382896837216413, 0.10948587292453442, 0.06308874052941206, 0.16843285812437045, 0.1314714421967328]
actor:  1 policy actor:  1  step number:  27 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  28.25641632080078
actor:  0 policy actor:  1  step number:  30 total reward:  0.5099999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18677999999999986 0.6729999999999999 0.6729999999999999
probs:  [0.08923140250330894, 0.4382896837216413, 0.10948587292453442, 0.06308874052941206, 0.16843285812437045, 0.1314714421967328]
printing an ep nov before normalisation:  50.51355908993381
printing an ep nov before normalisation:  37.32113142468065
actor:  1 policy actor:  1  step number:  30 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  22 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18677999999999986 0.6729999999999999 0.6729999999999999
maxi score, test score, baseline:  0.18677999999999986 0.6729999999999999 0.6729999999999999
probs:  [0.08923140250330894, 0.4382896837216413, 0.10948587292453442, 0.06308874052941206, 0.16843285812437045, 0.1314714421967328]
printing an ep nov before normalisation:  39.90424228304712
actor:  1 policy actor:  1  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18677999999999986 0.6729999999999999 0.6729999999999999
probs:  [0.08923140250330894, 0.4382896837216413, 0.10948587292453442, 0.06308874052941206, 0.16843285812437045, 0.1314714421967328]
Printing some Q and Qe and total Qs values:  [[0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.558]
 [0.633]] [[32.079]
 [32.079]
 [32.079]
 [32.079]
 [32.079]
 [38.082]
 [32.079]] [[1.573]
 [1.573]
 [1.573]
 [1.573]
 [1.573]
 [1.88 ]
 [1.573]]
printing an ep nov before normalisation:  40.033707617902984
maxi score, test score, baseline:  0.18677999999999986 0.6729999999999999 0.6729999999999999
probs:  [0.08923140250330894, 0.4382896837216413, 0.10948587292453442, 0.06308874052941206, 0.16843285812437045, 0.1314714421967328]
actor:  1 policy actor:  1  step number:  35 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  30 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18649999999999986 0.6729999999999999 0.6729999999999999
Printing some Q and Qe and total Qs values:  [[0.63 ]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]] [[42.726]
 [47.511]
 [47.511]
 [47.511]
 [47.511]
 [47.511]
 [47.511]] [[1.111]
 [1.1  ]
 [1.1  ]
 [1.1  ]
 [1.1  ]
 [1.1  ]
 [1.1  ]]
actor:  1 policy actor:  1  step number:  34 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18649999999999986 0.6729999999999999 0.6729999999999999
probs:  [0.08923140250330894, 0.4382896837216413, 0.10948587292453442, 0.06308874052941206, 0.16843285812437045, 0.1314714421967328]
from probs:  [0.08923140250330894, 0.4382896837216413, 0.10948587292453442, 0.06308874052941206, 0.16843285812437045, 0.1314714421967328]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.18649999999999986 0.6729999999999999 0.6729999999999999
probs:  [0.08923140250330894, 0.4382896837216413, 0.10948587292453442, 0.06308874052941206, 0.16843285812437045, 0.1314714421967328]
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.45 ]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]] [[26.193]
 [34.526]
 [26.193]
 [26.193]
 [26.193]
 [26.193]
 [26.193]] [[0.84 ]
 [1.316]
 [0.84 ]
 [0.84 ]
 [0.84 ]
 [0.84 ]
 [0.84 ]]
actor:  0 policy actor:  0  step number:  25 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  0.667
from probs:  [0.08923140250330894, 0.4382896837216413, 0.10948587292453442, 0.06308874052941206, 0.16843285812437045, 0.1314714421967328]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  42 total reward:  0.2899999999999996  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  45.01878122427714
printing an ep nov before normalisation:  43.44440935570263
maxi score, test score, baseline:  0.18621999999999986 0.6729999999999999 0.6729999999999999
probs:  [0.08923140250330894, 0.4382896837216413, 0.10948587292453442, 0.06308874052941206, 0.16843285812437045, 0.1314714421967328]
actor:  0 policy actor:  0  step number:  21 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  57.35416840439773
Printing some Q and Qe and total Qs values:  [[0.714]
 [0.714]
 [0.714]
 [0.714]
 [0.714]
 [0.714]
 [0.714]] [[52.696]
 [52.696]
 [52.696]
 [52.696]
 [52.696]
 [52.696]
 [52.696]] [[1.234]
 [1.234]
 [1.234]
 [1.234]
 [1.234]
 [1.234]
 [1.234]]
maxi score, test score, baseline:  0.18941999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.08923140250330894, 0.4382896837216413, 0.10948587292453442, 0.06308874052941206, 0.16843285812437045, 0.1314714421967328]
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.622]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]] [[58.886]
 [44.736]
 [45.75 ]
 [45.75 ]
 [45.75 ]
 [45.75 ]
 [45.75 ]] [[0.794]
 [0.8  ]
 [0.68 ]
 [0.68 ]
 [0.68 ]
 [0.68 ]
 [0.68 ]]
printing an ep nov before normalisation:  50.77104563680856
actor:  1 policy actor:  1  step number:  32 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
from probs:  [0.08923140250330894, 0.4382896837216413, 0.10948587292453442, 0.06308874052941206, 0.16843285812437045, 0.1314714421967328]
actor:  1 policy actor:  1  step number:  37 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.18941999999999984 0.6729999999999999 0.6729999999999999
siam score:  -0.75860614
printing an ep nov before normalisation:  20.896639823913574
printing an ep nov before normalisation:  20.515894889831543
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  31 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.316]
 [0.316]
 [0.316]
 [0.316]
 [0.316]
 [0.316]
 [0.316]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.316]
 [0.316]
 [0.316]
 [0.316]
 [0.316]
 [0.316]
 [0.316]]
actor:  0 policy actor:  0  step number:  20 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18959999999999988 0.6729999999999999 0.6729999999999999
maxi score, test score, baseline:  0.18959999999999988 0.6729999999999999 0.6729999999999999
probs:  [0.08910361763221851, 0.4383512040104023, 0.10950122681527891, 0.06309757993215298, 0.1684564885918798, 0.13148988301806736]
line 256 mcts: sample exp_bonus 35.56785384096647
Printing some Q and Qe and total Qs values:  [[0.9]
 [0.9]
 [0.9]
 [0.9]
 [0.9]
 [0.9]
 [0.9]] [[45.059]
 [45.059]
 [45.059]
 [45.059]
 [45.059]
 [45.059]
 [45.059]] [[1.567]
 [1.567]
 [1.567]
 [1.567]
 [1.567]
 [1.567]
 [1.567]]
actor:  1 policy actor:  1  step number:  37 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.667
from probs:  [0.08910361763221851, 0.4383512040104023, 0.10950122681527891, 0.06309757993215298, 0.1684564885918798, 0.13148988301806736]
maxi score, test score, baseline:  0.18959999999999988 0.6729999999999999 0.6729999999999999
probs:  [0.08910361763221851, 0.4383512040104023, 0.10950122681527891, 0.06309757993215298, 0.1684564885918798, 0.13148988301806736]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18959999999999988 0.6729999999999999 0.6729999999999999
probs:  [0.08910361763221851, 0.4383512040104023, 0.10950122681527891, 0.06309757993215298, 0.1684564885918798, 0.13148988301806736]
actor:  1 policy actor:  1  step number:  39 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18959999999999988 0.6729999999999999 0.6729999999999999
probs:  [0.08910361763221851, 0.4383512040104023, 0.10950122681527891, 0.06309757993215298, 0.1684564885918798, 0.13148988301806736]
actor:  0 policy actor:  1  step number:  21 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  2  action  0 :  tensor([0.5824, 0.0762, 0.0735, 0.0634, 0.0864, 0.0598, 0.0582],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0061, 0.9394, 0.0067, 0.0078, 0.0032, 0.0037, 0.0332],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0594, 0.0062, 0.6789, 0.0581, 0.0593, 0.0699, 0.0681],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0599, 0.1441, 0.0716, 0.4671, 0.0605, 0.0611, 0.1357],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2402, 0.0072, 0.0643, 0.0595, 0.5405, 0.0440, 0.0444],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0884, 0.0634, 0.2403, 0.0846, 0.0892, 0.3508, 0.0833],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([    0.0002,     0.0005,     0.0386,     0.0001,     0.0000,     0.0022,
            0.9584], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.18995999999999982 0.6729999999999999 0.6729999999999999
probs:  [0.08910361763221851, 0.4383512040104023, 0.10950122681527891, 0.06309757993215298, 0.1684564885918798, 0.13148988301806736]
Printing some Q and Qe and total Qs values:  [[0.603]
 [0.697]
 [0.626]
 [0.626]
 [0.611]
 [0.584]
 [0.585]] [[40.795]
 [41.226]
 [40.302]
 [41.042]
 [40.605]
 [39.697]
 [39.849]] [[1.284]
 [1.391]
 [1.292]
 [1.315]
 [1.286]
 [1.231]
 [1.237]]
maxi score, test score, baseline:  0.18995999999999982 0.6729999999999999 0.6729999999999999
probs:  [0.08910361763221851, 0.4383512040104023, 0.10950122681527891, 0.06309757993215298, 0.1684564885918798, 0.13148988301806736]
maxi score, test score, baseline:  0.18995999999999982 0.6729999999999999 0.6729999999999999
probs:  [0.08910361763221851, 0.4383512040104023, 0.10950122681527891, 0.06309757993215298, 0.1684564885918798, 0.13148988301806736]
actions average: 
K:  3  action  0 :  tensor([0.7111, 0.1110, 0.0290, 0.0262, 0.0784, 0.0148, 0.0294],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0022, 0.9623, 0.0051, 0.0038, 0.0187, 0.0036, 0.0042],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0244, 0.0071, 0.8032, 0.0300, 0.0222, 0.0904, 0.0227],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1024, 0.0091, 0.1508, 0.3096, 0.1819, 0.1308, 0.1153],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1391, 0.0324, 0.1047, 0.1041, 0.4380, 0.0935, 0.0883],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1119, 0.0130, 0.1377, 0.1201, 0.1300, 0.3943, 0.0929],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0654, 0.2146, 0.1022, 0.0952, 0.0483, 0.0433, 0.4310],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  31 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.0
using another actor
maxi score, test score, baseline:  0.18995999999999982 0.6729999999999999 0.6729999999999999
maxi score, test score, baseline:  0.18995999999999982 0.6729999999999999 0.6729999999999999
probs:  [0.08910361763221851, 0.4383512040104023, 0.10950122681527891, 0.06309757993215298, 0.1684564885918798, 0.13148988301806736]
Printing some Q and Qe and total Qs values:  [[0.954]
 [0.954]
 [0.954]
 [0.954]
 [0.954]
 [0.954]
 [0.954]] [[34.701]
 [34.701]
 [34.701]
 [34.701]
 [34.701]
 [34.701]
 [34.701]] [[1.287]
 [1.287]
 [1.287]
 [1.287]
 [1.287]
 [1.287]
 [1.287]]
actor:  1 policy actor:  1  step number:  38 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.18995999999999982 0.6729999999999999 0.6729999999999999
Printing some Q and Qe and total Qs values:  [[0.9]
 [0.9]
 [0.9]
 [0.9]
 [0.9]
 [0.9]
 [0.9]] [[45.269]
 [45.269]
 [45.269]
 [45.269]
 [45.269]
 [45.269]
 [45.269]] [[1.567]
 [1.567]
 [1.567]
 [1.567]
 [1.567]
 [1.567]
 [1.567]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  23 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  38.724205350485626
UNIT TEST: sample policy line 217 mcts : [0.51  0.347 0.02  0.    0.102 0.    0.02 ]
maxi score, test score, baseline:  0.18713999999999983 0.6729999999999999 0.6729999999999999
probs:  [0.08904011285029148, 0.4383817775242192, 0.10950885718243539, 0.06310197281793546, 0.16846823213924358, 0.131499047485875]
printing an ep nov before normalisation:  42.86462302467925
actor:  1 policy actor:  1  step number:  25 total reward:  0.74  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  34.51083737563615
maxi score, test score, baseline:  0.18713999999999983 0.6729999999999999 0.6729999999999999
probs:  [0.0885020366504206, 0.45211331797960563, 0.10702950614949341, 0.06502389404850842, 0.1603971189092667, 0.1269341262627053]
siam score:  -0.75738794
actor:  1 policy actor:  1  step number:  27 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18713999999999983 0.6729999999999999 0.6729999999999999
using explorer policy with actor:  1
siam score:  -0.75356597
maxi score, test score, baseline:  0.18713999999999983 0.6729999999999999 0.6729999999999999
probs:  [0.0885020366504206, 0.45211331797960563, 0.10702950614949341, 0.06502389404850842, 0.1603971189092667, 0.1269341262627053]
actor:  1 policy actor:  1  step number:  30 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  22 total reward:  0.69  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.18713999999999983 0.6729999999999999 0.6729999999999999
printing an ep nov before normalisation:  40.18270922823088
maxi score, test score, baseline:  0.18713999999999983 0.6729999999999999 0.6729999999999999
probs:  [0.08823296848753646, 0.4507371146229403, 0.10974642284418382, 0.06482631288092372, 0.1599091423133505, 0.12654803885106528]
printing an ep nov before normalisation:  32.579034267484225
printing an ep nov before normalisation:  32.586338913602795
maxi score, test score, baseline:  0.18713999999999983 0.6729999999999999 0.6729999999999999
probs:  [0.08823296848753646, 0.4507371146229403, 0.10974642284418382, 0.06482631288092372, 0.1599091423133505, 0.12654803885106528]
actor:  1 policy actor:  1  step number:  34 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  36.4355642952878
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  24 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  23 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  32.56172503503406
maxi score, test score, baseline:  0.18713999999999983 0.6729999999999999 0.6729999999999999
probs:  [0.08823296848753646, 0.4507371146229403, 0.10974642284418382, 0.06482631288092372, 0.1599091423133505, 0.12654803885106528]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18713999999999983 0.6729999999999999 0.6729999999999999
probs:  [0.08823296848753646, 0.4507371146229403, 0.10974642284418382, 0.06482631288092372, 0.1599091423133505, 0.12654803885106528]
actor:  1 policy actor:  1  step number:  36 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  34 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  35 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18713999999999983 0.6729999999999999 0.6729999999999999
probs:  [0.08823296848753648, 0.45073711462294036, 0.10974642284418383, 0.06482631288092373, 0.15990914231335052, 0.12654803885106516]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18713999999999983 0.6729999999999999 0.6729999999999999
probs:  [0.08823296848753648, 0.45073711462294036, 0.10974642284418383, 0.06482631288092373, 0.15990914231335052, 0.12654803885106516]
maxi score, test score, baseline:  0.18713999999999983 0.6729999999999999 0.6729999999999999
probs:  [0.08823296848753648, 0.45073711462294036, 0.10974642284418383, 0.06482631288092373, 0.15990914231335052, 0.12654803885106516]
actor:  1 policy actor:  1  step number:  26 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  33 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.263]
 [0.263]
 [0.325]
 [0.263]
 [0.263]
 [0.263]
 [0.263]] [[26.947]
 [26.947]
 [29.512]
 [26.947]
 [26.947]
 [26.947]
 [26.947]] [[0.555]
 [0.555]
 [0.654]
 [0.555]
 [0.555]
 [0.555]
 [0.555]]
maxi score, test score, baseline:  0.18713999999999983 0.6729999999999999 0.6729999999999999
printing an ep nov before normalisation:  19.980320085763406
maxi score, test score, baseline:  0.18713999999999983 0.6729999999999999 0.6729999999999999
probs:  [0.08817584269091791, 0.4507653673731215, 0.10975329556394749, 0.06483036912111616, 0.1599191602229798, 0.12655596502791708]
printing an ep nov before normalisation:  21.647126671633075
actor:  1 policy actor:  1  step number:  21 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  36 total reward:  0.24999999999999956  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  24 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  29.33807134628296
maxi score, test score, baseline:  0.18713999999999983 0.6729999999999999 0.6729999999999999
maxi score, test score, baseline:  0.18713999999999983 0.6729999999999999 0.6729999999999999
probs:  [0.08811894779917126, 0.4507935059244985, 0.10976014050388881, 0.06483440896581519, 0.15992913763979724, 0.1265638591668289]
actor:  1 policy actor:  1  step number:  27 total reward:  0.7199999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18713999999999983 0.6729999999999999 0.6729999999999999
maxi score, test score, baseline:  0.18713999999999983 0.6729999999999999 0.6729999999999999
probs:  [0.08790763692742946, 0.4566794842252675, 0.10858655354972611, 0.06565844559618464, 0.15652478518755727, 0.12464309451383492]
actor:  0 policy actor:  0  step number:  20 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.19 ]
 [-0.001]
 [-0.19 ]
 [-0.19 ]
 [-0.19 ]
 [-0.19 ]
 [-0.19 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.19 ]
 [-0.001]
 [-0.19 ]
 [-0.19 ]
 [-0.19 ]
 [-0.19 ]
 [-0.19 ]]
actor:  1 policy actor:  1  step number:  19 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18725999999999984 0.6729999999999999 0.6729999999999999
actor:  1 policy actor:  1  step number:  24 total reward:  0.69  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18725999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.0877845310939135, 0.4572138427556237, 0.10839398360954347, 0.0657325619873609, 0.15652430554279553, 0.12435077501076289]
maxi score, test score, baseline:  0.18725999999999984 0.6729999999999999 0.6729999999999999
Printing some Q and Qe and total Qs values:  [[0.813]
 [0.879]
 [0.845]
 [0.846]
 [0.846]
 [0.845]
 [0.843]] [[20.458]
 [14.836]
 [17.495]
 [17.387]
 [17.475]
 [17.777]
 [17.375]] [[2.005]
 [1.743]
 [1.865]
 [1.859]
 [1.864]
 [1.881]
 [1.855]]
maxi score, test score, baseline:  0.18725999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.0877845310939135, 0.4572138427556237, 0.10839398360954347, 0.0657325619873609, 0.15652430554279553, 0.12435077501076289]
maxi score, test score, baseline:  0.18725999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.0877845310939135, 0.4572138427556237, 0.10839398360954347, 0.0657325619873609, 0.15652430554279553, 0.12435077501076289]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  30.530677181140877
maxi score, test score, baseline:  0.18725999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.0877845310939135, 0.4572138427556237, 0.10839398360954347, 0.0657325619873609, 0.15652430554279553, 0.12435077501076289]
maxi score, test score, baseline:  0.18725999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.0877845310939135, 0.4572138427556237, 0.10839398360954347, 0.0657325619873609, 0.15652430554279553, 0.12435077501076289]
maxi score, test score, baseline:  0.18725999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.0877845310939135, 0.4572138427556237, 0.10839398360954347, 0.0657325619873609, 0.15652430554279553, 0.12435077501076289]
printing an ep nov before normalisation:  35.77854173560554
actor:  1 policy actor:  1  step number:  24 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  38 total reward:  0.4299999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18725999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.08773092550257867, 0.4572407223503575, 0.10840035011288668, 0.06573641969535418, 0.1565335024517668, 0.12435807988705627]
maxi score, test score, baseline:  0.18725999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.08773092550257867, 0.4572407223503575, 0.10840035011288668, 0.06573641969535418, 0.1565335024517668, 0.12435807988705627]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  23 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.18725999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.08773092550257867, 0.4572407223503575, 0.10840035011288668, 0.06573641969535418, 0.1565335024517668, 0.12435807988705627]
maxi score, test score, baseline:  0.18725999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.08773092550257867, 0.4572407223503575, 0.10840035011288668, 0.06573641969535418, 0.1565335024517668, 0.12435807988705627]
siam score:  -0.75900006
actor:  1 policy actor:  1  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.18725999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.08773092550257867, 0.4572407223503575, 0.10840035011288668, 0.06573641969535418, 0.1565335024517668, 0.12435807988705627]
Printing some Q and Qe and total Qs values:  [[0.383]
 [0.383]
 [0.57 ]
 [0.383]
 [0.383]
 [0.383]
 [0.383]] [[37.719]
 [37.719]
 [39.945]
 [37.719]
 [37.719]
 [37.719]
 [37.719]] [[1.013]
 [1.013]
 [1.265]
 [1.013]
 [1.013]
 [1.013]
 [1.013]]
maxi score, test score, baseline:  0.18725999999999984 0.6729999999999999 0.6729999999999999
maxi score, test score, baseline:  0.18725999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.08773092550257867, 0.4572407223503575, 0.10840035011288668, 0.06573641969535418, 0.1565335024517668, 0.12435807988705627]
actor:  1 policy actor:  1  step number:  21 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  30.44532440533717
printing an ep nov before normalisation:  43.40471584270214
maxi score, test score, baseline:  0.18725999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.08773092550257867, 0.4572407223503575, 0.10840035011288668, 0.06573641969535418, 0.1565335024517668, 0.12435807988705627]
printing an ep nov before normalisation:  35.11386079498802
actor:  0 policy actor:  1  step number:  24 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  29 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  34 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.18737999999999988 0.6729999999999999 0.6729999999999999
probs:  [0.08773092550257866, 0.45724072235035745, 0.10840035011288666, 0.06573641969535417, 0.15653350245176678, 0.12435807988705636]
maxi score, test score, baseline:  0.18737999999999988 0.6729999999999999 0.6729999999999999
probs:  [0.08773092550257866, 0.45724072235035745, 0.10840035011288666, 0.06573641969535417, 0.15653350245176678, 0.12435807988705636]
printing an ep nov before normalisation:  26.647663116455078
maxi score, test score, baseline:  0.18737999999999988 0.6729999999999999 0.6729999999999999
maxi score, test score, baseline:  0.18737999999999988 0.6729999999999999 0.6729999999999999
probs:  [0.08773092550257866, 0.45724072235035745, 0.10840035011288666, 0.06573641969535417, 0.15653350245176678, 0.12435807988705636]
printing an ep nov before normalisation:  39.850257750264795
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.715]
 [0.58 ]
 [0.58 ]
 [0.58 ]
 [0.58 ]
 [0.58 ]] [[34.137]
 [38.426]
 [36.295]
 [36.295]
 [36.295]
 [36.295]
 [36.295]] [[0.603]
 [0.896]
 [0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]]
actor:  1 policy actor:  1  step number:  22 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18737999999999988 0.6729999999999999 0.6729999999999999
probs:  [0.08773092550257866, 0.45724072235035745, 0.10840035011288666, 0.06573641969535417, 0.15653350245176678, 0.12435807988705636]
maxi score, test score, baseline:  0.18737999999999988 0.6729999999999999 0.6729999999999999
actor:  0 policy actor:  1  step number:  30 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  2  action  0 :  tensor([0.6874, 0.0035, 0.0581, 0.0446, 0.1089, 0.0471, 0.0503],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0024, 0.9779, 0.0031, 0.0050, 0.0022, 0.0011, 0.0082],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0502,     0.0005,     0.7656,     0.0281,     0.0388,     0.0649,
            0.0520], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1267, 0.0022, 0.1381, 0.3319, 0.1110, 0.1259, 0.1643],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1506, 0.0026, 0.0731, 0.0610, 0.5780, 0.0636, 0.0711],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0892, 0.0213, 0.3344, 0.0878, 0.1067, 0.2507, 0.1100],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1501, 0.0053, 0.1730, 0.1300, 0.1348, 0.1360, 0.2706],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  36 total reward:  0.1899999999999995  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  48.68353200285733
printing an ep nov before normalisation:  24.024326260825397
maxi score, test score, baseline:  0.18711999999999987 0.6729999999999999 0.6729999999999999
maxi score, test score, baseline:  0.18711999999999987 0.6729999999999999 0.6729999999999999
probs:  [0.08767753561452439, 0.4572674937844196, 0.10840669099808774, 0.06574026188033487, 0.15654266235334022, 0.1243653553692932]
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.64967591734675
actor:  1 policy actor:  1  step number:  47 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  34.849664857391765
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[ 0.344]
 [ 0.352]
 [ 0.221]
 [ 0.259]
 [ 0.161]
 [-0.135]
 [ 0.34 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.344]
 [ 0.352]
 [ 0.221]
 [ 0.259]
 [ 0.161]
 [-0.135]
 [ 0.34 ]]
actor:  1 policy actor:  1  step number:  35 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.18711999999999987 0.6729999999999999 0.6729999999999999
probs:  [0.08767753561452439, 0.4572674937844196, 0.10840669099808774, 0.06574026188033487, 0.15654266235334022, 0.1243653553692932]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  19.82589355975841
maxi score, test score, baseline:  0.18711999999999987 0.6729999999999999 0.6729999999999999
actor:  1 policy actor:  1  step number:  34 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  30 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18711999999999987 0.6729999999999999 0.6729999999999999
probs:  [0.08767753561452439, 0.4572674937844196, 0.10840669099808774, 0.06574026188033487, 0.15654266235334022, 0.1243653553692932]
Printing some Q and Qe and total Qs values:  [[0.756]
 [0.749]
 [0.756]
 [0.7  ]
 [0.695]
 [0.756]
 [0.756]] [[25.852]
 [27.353]
 [25.852]
 [26.24 ]
 [26.858]
 [25.852]
 [25.852]] [[0.756]
 [0.749]
 [0.756]
 [0.7  ]
 [0.695]
 [0.756]
 [0.756]]
maxi score, test score, baseline:  0.18711999999999987 0.6729999999999999 0.6729999999999999
probs:  [0.08767753561452439, 0.4572674937844196, 0.10840669099808774, 0.06574026188033487, 0.15654266235334022, 0.1243653553692932]
printing an ep nov before normalisation:  47.89159783198166
actor:  1 policy actor:  1  step number:  19 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18711999999999987 0.6729999999999999 0.6729999999999999
maxi score, test score, baseline:  0.18711999999999987 0.6729999999999999 0.6729999999999999
probs:  [0.0876346292177323, 0.45704344741860303, 0.10835362502179116, 0.06570810717469895, 0.15695572328530397, 0.12430446788187055]
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.411]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.402]
 [0.411]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]]
actor:  1 policy actor:  1  step number:  27 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  1  step number:  39 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.54]
 [0.54]
 [0.54]
 [0.54]
 [0.54]
 [0.54]
 [0.54]] [[33.044]
 [33.044]
 [33.044]
 [33.044]
 [33.044]
 [33.044]
 [33.044]] [[1.354]
 [1.354]
 [1.354]
 [1.354]
 [1.354]
 [1.354]
 [1.354]]
printing an ep nov before normalisation:  38.52081403068882
maxi score, test score, baseline:  0.18703999999999987 0.6729999999999999 0.6729999999999999
probs:  [0.0876346292177323, 0.45704344741860303, 0.10835362502179116, 0.06570810717469895, 0.15695572328530397, 0.12430446788187055]
UNIT TEST: sample policy line 217 mcts : [0.    0.918 0.    0.    0.02  0.061 0.   ]
maxi score, test score, baseline:  0.18703999999999987 0.6729999999999999 0.6729999999999999
probs:  [0.0876346292177323, 0.45704344741860303, 0.10835362502179116, 0.06570810717469895, 0.15695572328530397, 0.12430446788187055]
actor:  1 policy actor:  1  step number:  24 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  31 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.408]
 [0.355]
 [0.355]
 [0.355]
 [0.355]
 [0.355]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.355]
 [0.408]
 [0.355]
 [0.355]
 [0.355]
 [0.355]
 [0.355]]
printing an ep nov before normalisation:  31.303214492686454
printing an ep nov before normalisation:  35.040642484548485
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18703999999999987 0.6729999999999999 0.6729999999999999
probs:  [0.0876346292177323, 0.45704344741860303, 0.10835362502179116, 0.06570810717469895, 0.15695572328530397, 0.12430446788187055]
actor:  0 policy actor:  0  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  32.728942608397276
Printing some Q and Qe and total Qs values:  [[0.656]
 [0.728]
 [0.632]
 [0.64 ]
 [0.64 ]
 [0.648]
 [0.64 ]] [[28.818]
 [32.608]
 [28.162]
 [27.53 ]
 [27.53 ]
 [28.207]
 [27.53 ]] [[1.018]
 [1.206]
 [0.973]
 [0.962]
 [0.962]
 [0.991]
 [0.962]]
actor:  1 policy actor:  1  step number:  31 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  30 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.232]
 [0.318]
 [0.303]
 [0.238]
 [0.138]
 [0.201]
 [0.256]] [[32.363]
 [29.922]
 [30.406]
 [32.938]
 [34.842]
 [31.82 ]
 [31.53 ]] [[0.423]
 [0.481]
 [0.472]
 [0.436]
 [0.358]
 [0.386]
 [0.438]]
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.651]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]] [[22.595]
 [25.344]
 [22.595]
 [22.595]
 [22.595]
 [22.595]
 [22.595]] [[1.518]
 [1.877]
 [1.518]
 [1.518]
 [1.518]
 [1.518]
 [1.518]]
maxi score, test score, baseline:  0.18751999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.0876346292177323, 0.45704344741860303, 0.10835362502179116, 0.06570810717469895, 0.15695572328530397, 0.12430446788187055]
maxi score, test score, baseline:  0.18751999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.0876346292177323, 0.45704344741860303, 0.10835362502179116, 0.06570810717469895, 0.15695572328530397, 0.12430446788187055]
actor:  1 policy actor:  1  step number:  38 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.18751999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08758147729557506, 0.45707008521262055, 0.10835993425399312, 0.06571193017991621, 0.15696486601219214, 0.12431170704570299]
actor:  1 policy actor:  1  step number:  34 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
from probs:  [0.08758147729557506, 0.45707008521262055, 0.10835993425399312, 0.06571193017991621, 0.15696486601219214, 0.12431170704570299]
Printing some Q and Qe and total Qs values:  [[0.442]
 [0.442]
 [0.469]
 [0.442]
 [0.442]
 [0.442]
 [0.442]] [[44.216]
 [44.216]
 [39.469]
 [44.216]
 [44.216]
 [44.216]
 [44.216]] [[0.975]
 [0.975]
 [0.919]
 [0.975]
 [0.975]
 [0.975]
 [0.975]]
printing an ep nov before normalisation:  25.323860369760418
printing an ep nov before normalisation:  40.46808874476634
maxi score, test score, baseline:  0.18751999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08758147729557506, 0.45707008521262055, 0.10835993425399312, 0.06571193017991621, 0.15696486601219214, 0.12431170704570299]
maxi score, test score, baseline:  0.18751999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08758147729557506, 0.45707008521262055, 0.10835993425399312, 0.06571193017991621, 0.15696486601219214, 0.12431170704570299]
maxi score, test score, baseline:  0.18751999999999985 0.6729999999999999 0.6729999999999999
maxi score, test score, baseline:  0.18751999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08758147729557506, 0.45707008521262055, 0.10835993425399312, 0.06571193017991621, 0.15696486601219214, 0.12431170704570299]
actor:  1 policy actor:  1  step number:  32 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.18755999999999987 0.6729999999999999 0.6729999999999999
probs:  [0.08758147729557506, 0.45707008521262055, 0.10835993425399312, 0.06571193017991621, 0.15696486601219214, 0.12431170704570299]
printing an ep nov before normalisation:  42.84572209215477
printing an ep nov before normalisation:  47.557071330702954
printing an ep nov before normalisation:  27.986422255517244
maxi score, test score, baseline:  0.18755999999999987 0.6729999999999999 0.6729999999999999
probs:  [0.08758147729557506, 0.45707008521262055, 0.10835993425399312, 0.06571193017991621, 0.15696486601219214, 0.12431170704570299]
printing an ep nov before normalisation:  42.45682668968834
maxi score, test score, baseline:  0.18755999999999987 0.6729999999999999 0.6729999999999999
probs:  [0.08758147729557506, 0.45707008521262055, 0.10835993425399312, 0.06571193017991621, 0.15696486601219214, 0.12431170704570299]
Printing some Q and Qe and total Qs values:  [[0.737]
 [0.737]
 [0.737]
 [0.737]
 [0.737]
 [0.735]
 [0.737]] [[43.458]
 [43.458]
 [43.458]
 [43.458]
 [43.458]
 [44.67 ]
 [43.458]] [[2.611]
 [2.611]
 [2.611]
 [2.611]
 [2.611]
 [2.691]
 [2.611]]
printing an ep nov before normalisation:  12.702815705965405
printing an ep nov before normalisation:  13.150080913488958
using explorer policy with actor:  1
printing an ep nov before normalisation:  13.463112919956194
actor:  1 policy actor:  1  step number:  38 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  22 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  48 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  21.216795934968555
maxi score, test score, baseline:  0.19069999999999984 0.6729999999999999 0.6729999999999999
maxi score, test score, baseline:  0.19069999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.08758147729557503, 0.45707008521262044, 0.1083599342539931, 0.0657119301799162, 0.15696486601219212, 0.12431170704570306]
printing an ep nov before normalisation:  31.009711518073434
maxi score, test score, baseline:  0.19069999999999984 0.6729999999999999 0.6729999999999999
probs:  [0.08758147729557503, 0.45707008521262044, 0.1083599342539931, 0.0657119301799162, 0.15696486601219212, 0.12431170704570306]
actor:  1 policy actor:  1  step number:  20 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18737999999999988 0.6729999999999999 0.6729999999999999
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18737999999999988 0.6729999999999999 0.6729999999999999
probs:  [0.08758147729557503, 0.45707008521262044, 0.1083599342539931, 0.0657119301799162, 0.15696486601219212, 0.12431170704570306]
printing an ep nov before normalisation:  32.62729070818592
Printing some Q and Qe and total Qs values:  [[0.84 ]
 [0.906]
 [0.146]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]] [[35.137]
 [29.915]
 [31.064]
 [32.759]
 [32.759]
 [32.759]
 [32.759]] [[0.84 ]
 [0.906]
 [0.146]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]]
printing an ep nov before normalisation:  35.71463697750599
actor:  1 policy actor:  1  step number:  24 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  1.333
using another actor
from probs:  [0.08758147729557503, 0.45707008521262044, 0.1083599342539931, 0.0657119301799162, 0.15696486601219212, 0.12431170704570306]
actor:  0 policy actor:  0  step number:  34 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  26.73260868470985
maxi score, test score, baseline:  0.18705999999999987 0.6729999999999999 0.6729999999999999
probs:  [0.08758147729557503, 0.45707008521262044, 0.1083599342539931, 0.0657119301799162, 0.15696486601219212, 0.12431170704570306]
maxi score, test score, baseline:  0.18705999999999987 0.6729999999999999 0.6729999999999999
probs:  [0.08758147729557503, 0.45707008521262044, 0.1083599342539931, 0.0657119301799162, 0.15696486601219212, 0.12431170704570306]
printing an ep nov before normalisation:  41.42790042181525
Printing some Q and Qe and total Qs values:  [[0.703]
 [0.785]
 [0.717]
 [0.749]
 [0.689]
 [0.738]
 [0.693]] [[25.274]
 [33.613]
 [25.53 ]
 [33.836]
 [29.021]
 [31.725]
 [25.171]] [[0.961]
 [1.205]
 [0.979]
 [1.173]
 [1.019]
 [1.121]
 [0.948]]
actor:  0 policy actor:  1  step number:  35 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.333
using another actor
maxi score, test score, baseline:  0.18973999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08758147729557503, 0.45707008521262044, 0.1083599342539931, 0.0657119301799162, 0.15696486601219212, 0.12431170704570306]
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.474]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]] [[33.09 ]
 [39.366]
 [33.09 ]
 [33.09 ]
 [33.09 ]
 [33.09 ]
 [33.09 ]] [[1.059]
 [1.141]
 [1.059]
 [1.059]
 [1.059]
 [1.059]
 [1.059]]
printing an ep nov before normalisation:  27.237483777399685
maxi score, test score, baseline:  0.18973999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08758147729557503, 0.45707008521262044, 0.1083599342539931, 0.0657119301799162, 0.15696486601219212, 0.12431170704570306]
actor:  1 policy actor:  1  step number:  39 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18973999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08758147729557503, 0.45707008521262044, 0.1083599342539931, 0.0657119301799162, 0.15696486601219212, 0.12431170704570306]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18973999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08758147729557503, 0.45707008521262044, 0.1083599342539931, 0.0657119301799162, 0.15696486601219212, 0.12431170704570306]
printing an ep nov before normalisation:  39.888548671558915
actions average: 
K:  4  action  0 :  tensor([0.5330, 0.0038, 0.0774, 0.0680, 0.1387, 0.0832, 0.0960],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0047,     0.9590,     0.0097,     0.0046,     0.0005,     0.0013,
            0.0202], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0736, 0.1017, 0.4781, 0.0654, 0.0714, 0.0822, 0.1276],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1345, 0.0434, 0.1242, 0.3036, 0.1355, 0.1274, 0.1316],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1389, 0.1042, 0.1572, 0.1348, 0.1866, 0.1369, 0.1415],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1090, 0.0074, 0.1228, 0.1066, 0.1179, 0.3532, 0.1831],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0704, 0.2465, 0.1648, 0.0829, 0.0667, 0.0995, 0.2693],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  25 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18973999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08758147729557503, 0.45707008521262044, 0.1083599342539931, 0.0657119301799162, 0.15696486601219212, 0.12431170704570306]
maxi score, test score, baseline:  0.18973999999999985 0.6729999999999999 0.6729999999999999
maxi score, test score, baseline:  0.18973999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08758147729557503, 0.45707008521262044, 0.1083599342539931, 0.0657119301799162, 0.15696486601219212, 0.12431170704570306]
maxi score, test score, baseline:  0.18973999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08758147729557503, 0.45707008521262044, 0.1083599342539931, 0.0657119301799162, 0.15696486601219212, 0.12431170704570306]
actor:  1 policy actor:  1  step number:  39 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  0.6  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  30.474388192354695
Printing some Q and Qe and total Qs values:  [[0.575]
 [0.619]
 [0.447]
 [0.575]
 [0.47 ]
 [0.593]
 [0.451]] [[29.211]
 [31.329]
 [26.349]
 [29.211]
 [26.803]
 [30.296]
 [26.758]] [[0.951]
 [1.044]
 [0.758]
 [0.951]
 [0.791]
 [0.994]
 [0.772]]
actor:  1 policy actor:  1  step number:  35 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  21 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.18973999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08758147729557503, 0.45707008521262044, 0.1083599342539931, 0.0657119301799162, 0.15696486601219212, 0.12431170704570306]
maxi score, test score, baseline:  0.18973999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08758147729557503, 0.45707008521262044, 0.1083599342539931, 0.0657119301799162, 0.15696486601219212, 0.12431170704570306]
printing an ep nov before normalisation:  36.402822756922674
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.514]
 [0.53 ]
 [0.53 ]
 [0.53 ]
 [0.53 ]
 [0.53 ]] [[34.7  ]
 [34.435]
 [34.7  ]
 [34.7  ]
 [34.7  ]
 [34.7  ]
 [34.7  ]] [[2.133]
 [2.093]
 [2.133]
 [2.133]
 [2.133]
 [2.133]
 [2.133]]
maxi score, test score, baseline:  0.18973999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08752853839702923, 0.45709661624701664, 0.10836621819989532, 0.06571573786319691, 0.15697397209662928, 0.12431891719623246]
maxi score, test score, baseline:  0.18973999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08752853839702923, 0.45709661624701664, 0.10836621819989532, 0.06571573786319691, 0.15697397209662928, 0.12431891719623246]
maxi score, test score, baseline:  0.18973999999999985 0.6729999999999999 0.6729999999999999
maxi score, test score, baseline:  0.18973999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08752853839702923, 0.45709661624701664, 0.10836621819989532, 0.06571573786319691, 0.15697397209662928, 0.12431891719623246]
Printing some Q and Qe and total Qs values:  [[0.724]
 [0.693]
 [0.613]
 [0.58 ]
 [0.673]
 [0.58 ]
 [0.654]] [[31.554]
 [31.437]
 [26.179]
 [33.974]
 [29.791]
 [33.974]
 [28.195]] [[1.095]
 [1.061]
 [0.856]
 [1.009]
 [1.002]
 [1.009]
 [0.945]]
maxi score, test score, baseline:  0.18973999999999985 0.6729999999999999 0.6729999999999999
maxi score, test score, baseline:  0.18973999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08752853839702923, 0.45709661624701664, 0.10836621819989532, 0.06571573786319691, 0.15697397209662928, 0.12431891719623246]
actor:  1 policy actor:  1  step number:  26 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  21 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  34 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18973999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08752853839702923, 0.45709661624701664, 0.10836621819989532, 0.06571573786319691, 0.15697397209662928, 0.12431891719623246]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.18973999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08752853839702923, 0.45709661624701664, 0.10836621819989532, 0.06571573786319691, 0.15697397209662928, 0.12431891719623246]
actor:  1 policy actor:  1  step number:  26 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  24 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18973999999999985 0.6729999999999999 0.6729999999999999
maxi score, test score, baseline:  0.18973999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08752853839702923, 0.45709661624701664, 0.10836621819989532, 0.06571573786319691, 0.15697397209662928, 0.12431891719623246]
maxi score, test score, baseline:  0.18973999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08752853839702923, 0.45709661624701664, 0.10836621819989532, 0.06571573786319691, 0.15697397209662928, 0.12431891719623246]
printing an ep nov before normalisation:  29.851992982179695
printing an ep nov before normalisation:  39.42690767882118
maxi score, test score, baseline:  0.18973999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08752853839702923, 0.45709661624701664, 0.10836621819989532, 0.06571573786319691, 0.15697397209662928, 0.12431891719623246]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18973999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08752853839702923, 0.45709661624701664, 0.10836621819989532, 0.06571573786319691, 0.15697397209662928, 0.12431891719623246]
actor:  1 policy actor:  1  step number:  31 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  34 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.18973999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08752853839702923, 0.45709661624701664, 0.10836621819989532, 0.06571573786319691, 0.15697397209662928, 0.12431891719623246]
printing an ep nov before normalisation:  34.312038330640405
printing an ep nov before normalisation:  31.155893363971472
printing an ep nov before normalisation:  51.27033651561288
printing an ep nov before normalisation:  45.0201733534761
maxi score, test score, baseline:  0.18973999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08752853839702923, 0.45709661624701664, 0.10836621819989532, 0.06571573786319691, 0.15697397209662928, 0.12431891719623246]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  33 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  29 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18973999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08752853839702923, 0.45709661624701664, 0.10836621819989532, 0.06571573786319691, 0.15697397209662928, 0.12431891719623246]
actor:  1 policy actor:  1  step number:  25 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  24 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.10164619171383
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  23 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7419243
siam score:  -0.74056304
actor:  1 policy actor:  1  step number:  28 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.65 ]
 [0.622]
 [0.548]
 [0.54 ]
 [0.59 ]
 [0.546]
 [0.626]] [[43.393]
 [41.354]
 [41.017]
 [43.967]
 [42.768]
 [35.347]
 [46.347]] [[1.069]
 [1.003]
 [0.923]
 [0.969]
 [0.998]
 [0.817]
 [1.098]]
actor:  1 policy actor:  1  step number:  33 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  27 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.56 ]
 [0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.477]] [[34.63 ]
 [40.564]
 [34.63 ]
 [34.63 ]
 [34.63 ]
 [34.63 ]
 [34.63 ]] [[0.98 ]
 [1.227]
 [0.98 ]
 [0.98 ]
 [0.98 ]
 [0.98 ]
 [0.98 ]]
actor:  0 policy actor:  0  step number:  29 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.667
Starting evaluation
siam score:  -0.74648225
maxi score, test score, baseline:  0.19267999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08742329456865211, 0.45714936059394046, 0.10837871083843212, 0.06572330763092385, 0.15699207521577288, 0.12433325115227867]
printing an ep nov before normalisation:  43.445365958860116
actor:  1 policy actor:  1  step number:  30 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 34.594417572021484
maxi score, test score, baseline:  0.19267999999999985 0.6729999999999999 0.6729999999999999
probs:  [0.08742329456865211, 0.45714936059394046, 0.10837871083843212, 0.06572330763092385, 0.15699207521577288, 0.12433325115227867]
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
Printing some Q and Qe and total Qs values:  [[0.998]
 [0.999]
 [0.999]
 [0.924]
 [0.941]
 [0.924]
 [0.908]] [[33.672]
 [32.373]
 [33.569]
 [32.038]
 [36.77 ]
 [35.868]
 [34.997]] [[0.998]
 [0.999]
 [0.999]
 [0.924]
 [0.941]
 [0.924]
 [0.908]]
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  31.698590277685078
actor:  1 policy actor:  1  step number:  24 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  43.86697469130634
line 256 mcts: sample exp_bonus 38.01882035136642
Printing some Q and Qe and total Qs values:  [[0.613]
 [0.642]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]] [[41.108]
 [44.208]
 [41.108]
 [41.108]
 [41.108]
 [41.108]
 [41.108]] [[2.264]
 [2.485]
 [2.264]
 [2.264]
 [2.264]
 [2.264]
 [2.264]]
maxi score, test score, baseline:  0.19631999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08529101298671816, 0.4788325391003867, 0.10560393604907253, 0.06889292591287766, 0.14720974753676017, 0.1141698384141848]
actor:  1 policy actor:  1  step number:  25 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.19631999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08525568505605201, 0.47885104092013875, 0.10560801251210938, 0.06889558348836872, 0.1472154320723488, 0.11417424595098223]
actor:  0 policy actor:  1  step number:  41 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.19589999999999982 0.6799999999999999 0.6799999999999999
probs:  [0.08525568505605201, 0.47885104092013875, 0.10560801251210938, 0.06889558348836872, 0.1472154320723488, 0.11417424595098223]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.554]] [[38.883]
 [38.883]
 [38.883]
 [38.883]
 [38.883]
 [38.883]
 [38.883]] [[2.221]
 [2.221]
 [2.221]
 [2.221]
 [2.221]
 [2.221]
 [2.221]]
maxi score, test score, baseline:  0.19589999999999982 0.6799999999999999 0.6799999999999999
probs:  [0.08525568505605201, 0.47885104092013875, 0.10560801251210938, 0.06889558348836872, 0.1472154320723488, 0.11417424595098223]
actor:  1 policy actor:  1  step number:  24 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.297]
 [0.388]
 [0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.297]
 [0.388]
 [0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]]
maxi score, test score, baseline:  0.19589999999999982 0.6799999999999999 0.6799999999999999
probs:  [0.08525568505605201, 0.47885104092013875, 0.10560801251210938, 0.06889558348836872, 0.1472154320723488, 0.11417424595098223]
printing an ep nov before normalisation:  28.089377880096436
printing an ep nov before normalisation:  45.32585070333294
actor:  1 policy actor:  1  step number:  31 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.19589999999999982 0.6799999999999999 0.6799999999999999
probs:  [0.08525568505605201, 0.47885104092013875, 0.10560801251210938, 0.06889558348836872, 0.1472154320723488, 0.11417424595098223]
from probs:  [0.08525568505605201, 0.47885104092013875, 0.10560801251210938, 0.06889558348836872, 0.1472154320723488, 0.11417424595098223]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  30 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  27.160002426461666
maxi score, test score, baseline:  0.19551999999999983 0.6799999999999999 0.6799999999999999
probs:  [0.08518545367180873, 0.4788878222539929, 0.10561611645809474, 0.06890086670767845, 0.14722673284189994, 0.11418300806652525]
actor:  1 policy actor:  1  step number:  22 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.19551999999999983 0.6799999999999999 0.6799999999999999
probs:  [0.08518545367180873, 0.4788878222539929, 0.10561611645809474, 0.06890086670767845, 0.14722673284189994, 0.11418300806652525]
actor:  1 policy actor:  1  step number:  24 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]] [[26.363]
 [26.363]
 [26.363]
 [26.363]
 [26.363]
 [26.363]
 [26.363]] [[1.346]
 [1.346]
 [1.346]
 [1.346]
 [1.346]
 [1.346]
 [1.346]]
printing an ep nov before normalisation:  25.365613105066444
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  42.43970568573736
actor:  1 policy actor:  1  step number:  20 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.19245999999999985 0.6799999999999999 0.6799999999999999
probs:  [0.08518545367180873, 0.4788878222539929, 0.10561611645809474, 0.06890086670767845, 0.14722673284189994, 0.11418300806652525]
printing an ep nov before normalisation:  20.97303010096647
maxi score, test score, baseline:  0.19245999999999985 0.6799999999999999 0.6799999999999999
probs:  [0.08518545367180873, 0.4788878222539929, 0.10561611645809474, 0.06890086670767845, 0.14722673284189994, 0.11418300806652525]
actor:  1 policy actor:  1  step number:  33 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.307]
 [0.346]
 [0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.307]
 [0.346]
 [0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]]
printing an ep nov before normalisation:  33.99171745463202
printing an ep nov before normalisation:  32.40234558165097
printing an ep nov before normalisation:  54.07067188579763
maxi score, test score, baseline:  0.19245999999999985 0.6799999999999999 0.6799999999999999
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.5356],
        [0.4884],
        [0.3191],
        [0.4884],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.8152],
        [0.8808]], dtype=torch.float64)
0.950598 0.950598
-0.145559551797 0.3900616716908005
-0.145559551797 0.3428578594393208
-0.067539651597 0.2515993014859336
-0.125759551797 0.36265785943932083
0.950598 0.950598
-0.7128 -0.7128
-0.9702 -0.9702
-0.048519850599000006 0.7667063170405667
-0.08713775079899999 0.7936267166878416
actor:  1 policy actor:  1  step number:  33 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  31.401298140924645
maxi score, test score, baseline:  0.19245999999999985 0.6799999999999999 0.6799999999999999
probs:  [0.08515054852155017, 0.4789061026566752, 0.10562014413682197, 0.06890349247913152, 0.14723234934887136, 0.11418736285694979]
printing an ep nov before normalisation:  28.055674785477503
maxi score, test score, baseline:  0.19245999999999985 0.6799999999999999 0.6799999999999999
probs:  [0.08515054852155017, 0.4789061026566752, 0.10562014413682197, 0.06890349247913152, 0.14723234934887136, 0.11418736285694979]
printing an ep nov before normalisation:  0.08747621989755316
actor:  1 policy actor:  1  step number:  34 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  34.05755664913832
maxi score, test score, baseline:  0.18921999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08515054852155017, 0.4789061026566752, 0.10562014413682197, 0.06890349247913152, 0.14723234934887136, 0.11418736285694979]
printing an ep nov before normalisation:  26.186070212746753
printing an ep nov before normalisation:  32.113443456127456
actor:  1 policy actor:  1  step number:  27 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18921999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08515054852155017, 0.4789061026566752, 0.10562014413682197, 0.06890349247913152, 0.14723234934887136, 0.11418736285694979]
maxi score, test score, baseline:  0.18921999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08515054852155017, 0.4789061026566752, 0.10562014413682197, 0.06890349247913152, 0.14723234934887136, 0.11418736285694979]
actor:  1 policy actor:  1  step number:  33 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.501]
 [0.414]
 [0.403]
 [0.478]
 [0.478]
 [0.399]] [[32.543]
 [40.   ]
 [27.088]
 [27.735]
 [32.543]
 [32.543]
 [27.632]] [[0.95 ]
 [1.202]
 [0.719]
 [0.728]
 [0.95 ]
 [0.95 ]
 [0.721]]
actor:  1 policy actor:  1  step number:  34 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.667
from probs:  [0.08515054852155017, 0.4789061026566752, 0.10562014413682197, 0.06890349247913152, 0.14723234934887136, 0.11418736285694979]
maxi score, test score, baseline:  0.18921999999999986 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.18921999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08515054852155017, 0.4789061026566752, 0.10562014413682197, 0.06890349247913152, 0.14723234934887136, 0.11418736285694979]
printing an ep nov before normalisation:  28.22461985154376
actor:  1 policy actor:  1  step number:  38 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  1.333
using another actor
maxi score, test score, baseline:  0.18921999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08515054852155017, 0.4789061026566752, 0.10562014413682197, 0.06890349247913152, 0.14723234934887136, 0.11418736285694979]
maxi score, test score, baseline:  0.18615999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.08515054852155017, 0.4789061026566752, 0.10562014413682197, 0.06890349247913152, 0.14723234934887136, 0.11418736285694979]
actor:  1 policy actor:  1  step number:  43 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  45.85412237154535
using another actor
maxi score, test score, baseline:  0.18615999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.08515054852155017, 0.4789061026566752, 0.10562014413682197, 0.06890349247913152, 0.14723234934887136, 0.11418736285694979]
actor:  1 policy actor:  1  step number:  22 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  0.667
from probs:  [0.08515054852155017, 0.4789061026566752, 0.10562014413682197, 0.06890349247913152, 0.14723234934887136, 0.11418736285694979]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.967]
 [1.019]
 [1.011]
 [0.967]
 [0.891]
 [0.967]
 [0.967]] [[45.106]
 [40.627]
 [40.002]
 [45.106]
 [45.308]
 [45.106]
 [45.106]] [[0.967]
 [1.019]
 [1.011]
 [0.967]
 [0.891]
 [0.967]
 [0.967]]
maxi score, test score, baseline:  0.18305999999999986 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.18305999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08515054852155017, 0.4789061026566752, 0.10562014413682197, 0.06890349247913152, 0.14723234934887136, 0.11418736285694979]
maxi score, test score, baseline:  0.18305999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08515054852155017, 0.4789061026566752, 0.10562014413682197, 0.06890349247913152, 0.14723234934887136, 0.11418736285694979]
maxi score, test score, baseline:  0.18305999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08515054852155017, 0.4789061026566752, 0.10562014413682197, 0.06890349247913152, 0.14723234934887136, 0.11418736285694979]
maxi score, test score, baseline:  0.18305999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08515054852155017, 0.4789061026566752, 0.10562014413682197, 0.06890349247913152, 0.14723234934887136, 0.11418736285694979]
actor:  0 policy actor:  1  step number:  32 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.18263999999999983 0.6799999999999999 0.6799999999999999
probs:  [0.08515054852155017, 0.4789061026566752, 0.10562014413682197, 0.06890349247913152, 0.14723234934887136, 0.11418736285694979]
siam score:  -0.75274116
actor:  0 policy actor:  0  step number:  32 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18241999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08515054852155017, 0.4789061026566752, 0.10562014413682197, 0.06890349247913152, 0.14723234934887136, 0.11418736285694979]
Printing some Q and Qe and total Qs values:  [[0.849]
 [0.849]
 [0.849]
 [0.849]
 [0.791]
 [0.849]
 [0.849]] [[42.986]
 [42.986]
 [42.986]
 [42.986]
 [40.659]
 [42.986]
 [42.986]] [[2.797]
 [2.797]
 [2.797]
 [2.797]
 [2.559]
 [2.797]
 [2.797]]
maxi score, test score, baseline:  0.18241999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08515054852155017, 0.4789061026566752, 0.10562014413682197, 0.06890349247913152, 0.14723234934887136, 0.11418736285694979]
maxi score, test score, baseline:  0.18241999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08515054852155017, 0.4789061026566752, 0.10562014413682197, 0.06890349247913152, 0.14723234934887136, 0.11418736285694979]
actor:  1 policy actor:  1  step number:  22 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  37 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18241999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08515054852155017, 0.4789061026566752, 0.10562014413682197, 0.06890349247913152, 0.14723234934887136, 0.11418736285694979]
printing an ep nov before normalisation:  38.19366558086593
printing an ep nov before normalisation:  30.251463250436228
maxi score, test score, baseline:  0.18241999999999986 0.6799999999999999 0.6799999999999999
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.363]
 [0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]] [[32.829]
 [35.773]
 [32.829]
 [32.829]
 [32.829]
 [32.829]
 [32.829]] [[0.641]
 [0.693]
 [0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.641]]
maxi score, test score, baseline:  0.18241999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08515054852155017, 0.4789061026566752, 0.10562014413682197, 0.06890349247913152, 0.14723234934887136, 0.11418736285694979]
Printing some Q and Qe and total Qs values:  [[0.359]
 [0.508]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]] [[37.426]
 [38.157]
 [37.426]
 [37.426]
 [37.426]
 [37.426]
 [37.426]] [[0.759]
 [0.923]
 [0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]]
maxi score, test score, baseline:  0.18241999999999986 0.6799999999999999 0.6799999999999999
printing an ep nov before normalisation:  26.905632034028176
actor:  1 policy actor:  1  step number:  40 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  32.54864896879712
actor:  1 policy actor:  1  step number:  51 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  39.717662323354354
maxi score, test score, baseline:  0.17925999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08511578261269358, 0.4789243101363394, 0.10562415574858988, 0.06890610777602435, 0.14723794345083094, 0.11419170027552188]
maxi score, test score, baseline:  0.17925999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08511578261269358, 0.4789243101363394, 0.10562415574858988, 0.06890610777602435, 0.14723794345083094, 0.11419170027552188]
printing an ep nov before normalisation:  27.74920483465384
printing an ep nov before normalisation:  24.12750185960957
actor:  1 policy actor:  1  step number:  38 total reward:  0.14999999999999947  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  42 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.0
from probs:  [0.08511578261269358, 0.4789243101363394, 0.10562415574858988, 0.06890610777602435, 0.14723794345083094, 0.11419170027552188]
maxi score, test score, baseline:  0.17925999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08511578261269358, 0.4789243101363394, 0.10562415574858988, 0.06890610777602435, 0.14723794345083094, 0.11419170027552188]
using explorer policy with actor:  0
actor:  0 policy actor:  0  step number:  34 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17873999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.08511578261269358, 0.4789243101363394, 0.10562415574858988, 0.06890610777602435, 0.14723794345083094, 0.11419170027552188]
printing an ep nov before normalisation:  26.517845281520223
Printing some Q and Qe and total Qs values:  [[0.876]
 [0.892]
 [0.876]
 [0.876]
 [0.876]
 [0.876]
 [0.876]] [[38.54 ]
 [42.144]
 [38.54 ]
 [38.54 ]
 [38.54 ]
 [38.54 ]
 [38.54 ]] [[0.876]
 [0.892]
 [0.876]
 [0.876]
 [0.876]
 [0.876]
 [0.876]]
printing an ep nov before normalisation:  42.18676241122572
printing an ep nov before normalisation:  48.343528614671946
actor:  0 policy actor:  1  step number:  31 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  36 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  45.66154761849171
maxi score, test score, baseline:  0.17849999999999985 0.6799999999999999 0.6799999999999999
probs:  [0.08511578261269358, 0.4789243101363394, 0.10562415574858988, 0.06890610777602435, 0.14723794345083094, 0.11419170027552188]
Printing some Q and Qe and total Qs values:  [[0.177]
 [0.201]
 [0.204]
 [0.148]
 [0.204]
 [0.204]
 [0.202]] [[31.244]
 [29.192]
 [31.198]
 [26.255]
 [31.198]
 [31.198]
 [34.056]] [[0.607]
 [0.575]
 [0.633]
 [0.44 ]
 [0.633]
 [0.633]
 [0.711]]
maxi score, test score, baseline:  0.17849999999999985 0.6799999999999999 0.6799999999999999
probs:  [0.08511578261269358, 0.4789243101363394, 0.10562415574858988, 0.06890610777602435, 0.14723794345083094, 0.11419170027552188]
actor:  1 policy actor:  1  step number:  32 total reward:  0.44999999999999984  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17849999999999985 0.6799999999999999 0.6799999999999999
probs:  [0.08511578261269358, 0.4789243101363394, 0.10562415574858988, 0.06890610777602435, 0.14723794345083094, 0.11419170027552188]
printing an ep nov before normalisation:  43.49428964634798
printing an ep nov before normalisation:  39.43515372591698
Printing some Q and Qe and total Qs values:  [[0.677]
 [0.624]
 [0.566]
 [0.566]
 [0.677]
 [0.566]
 [0.615]] [[35.629]
 [32.739]
 [34.533]
 [34.533]
 [38.234]
 [34.533]
 [33.445]] [[1.089]
 [0.972]
 [0.954]
 [0.954]
 [1.146]
 [0.954]
 [0.979]]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  28 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7486345
maxi score, test score, baseline:  0.17849999999999985 0.6799999999999999 0.6799999999999999
probs:  [0.08511578261269358, 0.4789243101363394, 0.10562415574858988, 0.06890610777602435, 0.14723794345083094, 0.11419170027552188]
actor:  0 policy actor:  0  step number:  23 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  23.41325996931874
maxi score, test score, baseline:  0.17853999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.08511578261269358, 0.4789243101363394, 0.10562415574858988, 0.06890610777602435, 0.14723794345083094, 0.11419170027552188]
maxi score, test score, baseline:  0.17853999999999987 0.6799999999999999 0.6799999999999999
actor:  0 policy actor:  0  step number:  26 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.997]
 [0.327]
 [0.997]
 [0.997]
 [0.997]
 [0.997]
 [0.997]] [[23.069]
 [ 0.005]
 [23.069]
 [23.069]
 [23.069]
 [23.069]
 [23.069]] [[0.997]
 [0.327]
 [0.997]
 [0.997]
 [0.997]
 [0.997]
 [0.997]]
actor:  1 policy actor:  1  step number:  37 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  34 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18131999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.08511578261269358, 0.4789243101363394, 0.10562415574858988, 0.06890610777602435, 0.14723794345083094, 0.11419170027552188]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18131999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.08511578261269358, 0.4789243101363394, 0.10562415574858988, 0.06890610777602435, 0.14723794345083094, 0.11419170027552188]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.18131999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.08511578261269358, 0.4789243101363394, 0.10562415574858988, 0.06890610777602435, 0.14723794345083094, 0.11419170027552188]
actor:  1 policy actor:  1  step number:  19 total reward:  0.7  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  37 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18131999999999987 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.18131999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.08470142749333648, 0.4765898364965034, 0.10510980658560268, 0.06857078725010424, 0.15139256441056753, 0.1136355777638857]
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.538]
 [0.422]
 [0.422]
 [0.422]
 [0.422]
 [0.422]] [[32.88 ]
 [34.423]
 [32.88 ]
 [32.88 ]
 [32.88 ]
 [32.88 ]
 [32.88 ]] [[1.312]
 [1.515]
 [1.312]
 [1.312]
 [1.312]
 [1.312]
 [1.312]]
actor:  1 policy actor:  1  step number:  29 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18131999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.08470142749333648, 0.4765898364965034, 0.10510980658560268, 0.06857078725010424, 0.15139256441056753, 0.1136355777638857]
maxi score, test score, baseline:  0.18131999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.08470142749333648, 0.4765898364965034, 0.10510980658560268, 0.06857078725010424, 0.15139256441056753, 0.1136355777638857]
Printing some Q and Qe and total Qs values:  [[0.749]
 [0.778]
 [0.75 ]
 [0.749]
 [0.752]
 [0.753]
 [0.752]] [[20.87 ]
 [20.508]
 [21.191]
 [21.439]
 [21.8  ]
 [22.322]
 [22.093]] [[0.749]
 [0.778]
 [0.75 ]
 [0.749]
 [0.752]
 [0.753]
 [0.752]]
printing an ep nov before normalisation:  19.18217897415161
actor:  1 policy actor:  1  step number:  39 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18131999999999987 0.6799999999999999 0.6799999999999999
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.558]
 [0.558]
 [0.558]
 [0.584]
 [0.558]
 [0.558]] [[43.068]
 [59.427]
 [59.427]
 [59.427]
 [51.142]
 [59.427]
 [59.427]] [[1.039]
 [1.225]
 [1.225]
 [1.225]
 [1.118]
 [1.225]
 [1.225]]
siam score:  -0.76122266
maxi score, test score, baseline:  0.17813999999999983 0.6799999999999999 0.6799999999999999
actor:  1 policy actor:  1  step number:  34 total reward:  0.16999999999999948  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  25.134111395345748
printing an ep nov before normalisation:  35.23223606887339
actor:  1 policy actor:  1  step number:  30 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
using another actor
actor:  1 policy actor:  1  step number:  28 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.229]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.229]]
actor:  1 policy actor:  1  step number:  25 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17813999999999983 0.6799999999999999 0.6799999999999999
probs:  [0.08466695321848243, 0.47660779507245643, 0.1051137633569931, 0.06857336679481642, 0.15139826566908382, 0.11363985588816777]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 36.483633518218994
maxi score, test score, baseline:  0.17813999999999983 0.6799999999999999 0.6799999999999999
probs:  [0.08466695321848243, 0.47660779507245643, 0.1051137633569931, 0.06857336679481642, 0.15139826566908382, 0.11363985588816777]
maxi score, test score, baseline:  0.17813999999999983 0.6799999999999999 0.6799999999999999
probs:  [0.08466695321848243, 0.47660779507245643, 0.1051137633569931, 0.06857336679481642, 0.15139826566908382, 0.11363985588816777]
printing an ep nov before normalisation:  0.012762619056729818
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.357]] [[24.524]
 [24.524]
 [24.524]
 [24.524]
 [24.524]
 [24.524]
 [24.524]] [[2.024]
 [2.024]
 [2.024]
 [2.024]
 [2.024]
 [2.024]
 [2.024]]
maxi score, test score, baseline:  0.17813999999999983 0.6799999999999999 0.6799999999999999
probs:  [0.08466695321848243, 0.47660779507245643, 0.1051137633569931, 0.06857336679481642, 0.15139826566908382, 0.11363985588816777]
Printing some Q and Qe and total Qs values:  [[0.418]
 [0.468]
 [0.438]
 [0.418]
 [0.464]
 [0.418]
 [0.404]] [[26.638]
 [32.94 ]
 [32.321]
 [26.638]
 [31.96 ]
 [26.638]
 [32.549]] [[0.886]
 [1.23 ]
 [1.171]
 [0.886]
 [1.18 ]
 [0.886]
 [1.148]]
printing an ep nov before normalisation:  0.04462008474547474
siam score:  -0.76415366
actor:  0 policy actor:  1  step number:  45 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17755999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08466695321848243, 0.47660779507245643, 0.1051137633569931, 0.06857336679481642, 0.15139826566908382, 0.11363985588816777]
printing an ep nov before normalisation:  24.1358000121379
actor:  1 policy actor:  1  step number:  33 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17755999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08466695321848243, 0.47660779507245643, 0.1051137633569931, 0.06857336679481642, 0.15139826566908382, 0.11363985588816777]
Printing some Q and Qe and total Qs values:  [[0.807]
 [0.903]
 [0.859]
 [0.859]
 [0.859]
 [0.859]
 [0.859]] [[47.915]
 [35.15 ]
 [35.147]
 [35.147]
 [35.147]
 [35.147]
 [35.147]] [[0.807]
 [0.903]
 [0.859]
 [0.859]
 [0.859]
 [0.859]
 [0.859]]
maxi score, test score, baseline:  0.17755999999999986 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.17755999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08466695321848243, 0.47660779507245643, 0.1051137633569931, 0.06857336679481642, 0.15139826566908382, 0.11363985588816777]
actor:  1 policy actor:  1  step number:  21 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  22 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17755999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08463261593245038, 0.47662568228725166, 0.10511770440554527, 0.06857593608931177, 0.15140394427277445, 0.11364411701266644]
actor:  0 policy actor:  0  step number:  30 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  0.667
using another actor
printing an ep nov before normalisation:  45.387181610477015
printing an ep nov before normalisation:  39.61835473948409
printing an ep nov before normalisation:  28.821885752295294
maxi score, test score, baseline:  0.17723999999999987 0.6799999999999999 0.6799999999999999
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17723999999999987 0.6799999999999999 0.6799999999999999
actions average: 
K:  1  action  0 :  tensor([0.6245, 0.0163, 0.0643, 0.0844, 0.0799, 0.0543, 0.0765],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0051, 0.9755, 0.0077, 0.0026, 0.0019, 0.0022, 0.0050],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0298, 0.0298, 0.8064, 0.0314, 0.0284, 0.0366, 0.0376],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0763, 0.0908, 0.0033, 0.6575, 0.0085, 0.0065, 0.1571],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1109, 0.0808, 0.1541, 0.1259, 0.2680, 0.1326, 0.1277],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0525, 0.0290, 0.1859, 0.0652, 0.0599, 0.5497, 0.0577],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1305, 0.2568, 0.0709, 0.0670, 0.0514, 0.0512, 0.3722],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  34.09381335215962
actor:  1 policy actor:  1  step number:  38 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17723999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.08463261593245038, 0.47662568228725166, 0.10511770440554527, 0.06857593608931177, 0.15140394427277445, 0.11364411701266644]
actor:  0 policy actor:  1  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.333
siam score:  -0.7630956
maxi score, test score, baseline:  0.18029999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.08463261593245038, 0.47662568228725166, 0.10511770440554527, 0.06857593608931177, 0.15140394427277445, 0.11364411701266644]
maxi score, test score, baseline:  0.18029999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.08463261593245038, 0.47662568228725166, 0.10511770440554527, 0.06857593608931177, 0.15140394427277445, 0.11364411701266644]
printing an ep nov before normalisation:  1.1799553290352094
maxi score, test score, baseline:  0.18029999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.08463261593245038, 0.47662568228725166, 0.10511770440554527, 0.06857593608931177, 0.15140394427277445, 0.11364411701266644]
maxi score, test score, baseline:  0.18029999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.08463261593245038, 0.47662568228725166, 0.10511770440554527, 0.06857593608931177, 0.15140394427277445, 0.11364411701266644]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  53 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.296]
 [0.302]
 [0.293]
 [0.282]
 [0.296]
 [0.296]
 [0.296]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.296]
 [0.302]
 [0.293]
 [0.282]
 [0.296]
 [0.296]
 [0.296]]
printing an ep nov before normalisation:  49.34462853204948
maxi score, test score, baseline:  0.18029999999999988 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.18029999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.08463261593245038, 0.47662568228725166, 0.10511770440554527, 0.06857593608931177, 0.15140394427277445, 0.11364411701266644]
printing an ep nov before normalisation:  34.504697296536996
maxi score, test score, baseline:  0.18029999999999988 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.18029999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.08463261593245038, 0.47662568228725166, 0.10511770440554527, 0.06857593608931177, 0.15140394427277445, 0.11364411701266644]
actor:  0 policy actor:  0  step number:  22 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.48894467754436
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.18053999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08459841482034006, 0.47664349856539284, 0.1051216298247888, 0.06857849519456544, 0.15140960035640522, 0.1136483612385077]
actor:  1 policy actor:  1  step number:  50 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.698]
 [0.698]
 [0.714]
 [0.698]
 [0.68 ]
 [0.698]
 [0.703]] [[12.523]
 [12.523]
 [31.052]
 [12.523]
 [29.166]
 [12.523]
 [29.292]] [[1.316]
 [1.316]
 [2.246]
 [1.316]
 [2.119]
 [1.316]
 [2.148]]
printing an ep nov before normalisation:  35.366754481929114
printing an ep nov before normalisation:  29.24424111025047
maxi score, test score, baseline:  0.18053999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08459841482034006, 0.47664349856539284, 0.1051216298247888, 0.06857849519456544, 0.15140960035640522, 0.1136483612385077]
actions average: 
K:  2  action  0 :  tensor([0.5241, 0.0082, 0.0798, 0.0846, 0.1316, 0.0683, 0.1034],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0066, 0.9052, 0.0099, 0.0223, 0.0032, 0.0047, 0.0481],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0534,     0.0005,     0.5908,     0.0627,     0.0587,     0.1608,
            0.0731], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1103, 0.0066, 0.1548, 0.2677, 0.1173, 0.2178, 0.1255],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0838, 0.0062, 0.0723, 0.1436, 0.5301, 0.0877, 0.0763],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0943, 0.0066, 0.2467, 0.1195, 0.1057, 0.3112, 0.1159],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0277, 0.2000, 0.0482, 0.0584, 0.0311, 0.0229, 0.6117],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.777]
 [0.852]
 [0.76 ]
 [0.76 ]
 [0.76 ]
 [0.76 ]
 [0.76 ]] [[35.466]
 [31.533]
 [33.979]
 [33.979]
 [33.979]
 [33.979]
 [33.979]] [[0.777]
 [0.852]
 [0.76 ]
 [0.76 ]
 [0.76 ]
 [0.76 ]
 [0.76 ]]
Printing some Q and Qe and total Qs values:  [[0.897]
 [0.925]
 [0.897]
 [0.897]
 [0.897]
 [0.897]
 [0.898]] [[37.362]
 [39.004]
 [39.038]
 [37.362]
 [37.362]
 [37.362]
 [39.617]] [[0.897]
 [0.925]
 [0.897]
 [0.897]
 [0.897]
 [0.897]
 [0.898]]
maxi score, test score, baseline:  0.18053999999999984 0.6799999999999999 0.6799999999999999
actor:  0 policy actor:  1  step number:  28 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  16.012459993362427
printing an ep nov before normalisation:  24.357932930049433
printing an ep nov before normalisation:  7.090694623457239e-05
actor:  1 policy actor:  1  step number:  72 total reward:  0.0699999999999994  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.18057999999999982 0.6799999999999999 0.6799999999999999
probs:  [0.08459841482034006, 0.47664349856539284, 0.1051216298247888, 0.06857849519456544, 0.15140960035640522, 0.1136483612385077]
printing an ep nov before normalisation:  29.728100299835205
actor:  1 policy actor:  1  step number:  42 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  21 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  29.569584731322063
printing an ep nov before normalisation:  33.64098068693521
actor:  1 policy actor:  1  step number:  30 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  51.88317797646941
maxi score, test score, baseline:  0.18057999999999982 0.6799999999999999 0.6799999999999999
probs:  [0.08459841482034006, 0.47664349856539284, 0.1051216298247888, 0.06857849519456544, 0.15140960035640522, 0.1136483612385077]
printing an ep nov before normalisation:  39.1716097348302
printing an ep nov before normalisation:  32.825493812561035
actor:  1 policy actor:  1  step number:  34 total reward:  0.2699999999999996  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  32.61373375474583
maxi score, test score, baseline:  0.18057999999999982 0.6799999999999999 0.6799999999999999
probs:  [0.08459841482034006, 0.47664349856539284, 0.1051216298247888, 0.06857849519456544, 0.15140960035640522, 0.1136483612385077]
printing an ep nov before normalisation:  41.62516376017
maxi score, test score, baseline:  0.18057999999999982 0.6799999999999999 0.6799999999999999
probs:  [0.08459841482034006, 0.47664349856539284, 0.1051216298247888, 0.06857849519456544, 0.15140960035640522, 0.1136483612385077]
printing an ep nov before normalisation:  38.07265447741772
Printing some Q and Qe and total Qs values:  [[0.188]
 [0.37 ]
 [0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.188]
 [0.37 ]
 [0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.18057999999999982 0.6799999999999999 0.6799999999999999
probs:  [0.08456434907370246, 0.47666124432802237, 0.10512553970751333, 0.06858104417106972, 0.15141523405367516, 0.11365258866601685]
Printing some Q and Qe and total Qs values:  [[0.239]
 [0.367]
 [0.265]
 [0.265]
 [0.296]
 [0.299]
 [0.265]] [[26.657]
 [28.821]
 [27.538]
 [27.538]
 [27.539]
 [27.973]
 [27.538]] [[1.034]
 [1.289]
 [1.111]
 [1.111]
 [1.142]
 [1.171]
 [1.111]]
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.722]
 [0.636]
 [0.635]
 [0.634]
 [0.634]
 [0.633]] [[16.412]
 [18.134]
 [17.004]
 [16.861]
 [16.937]
 [16.94 ]
 [16.633]] [[1.135]
 [1.275]
 [1.154]
 [1.149]
 [1.151]
 [1.151]
 [1.14 ]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  30 total reward:  0.71  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  56 total reward:  0.2699999999999996  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18057999999999982 0.6799999999999999 0.6799999999999999
probs:  [0.0845386409763781, 0.4789473453770161, 0.1046584932347937, 0.06889841181993168, 0.14995459636028566, 0.11300251223159485]
maxi score, test score, baseline:  0.18057999999999982 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.18057999999999982 0.6799999999999999 0.6799999999999999
probs:  [0.0845386409763781, 0.4789473453770161, 0.1046584932347937, 0.06889841181993168, 0.14995459636028566, 0.11300251223159485]
printing an ep nov before normalisation:  24.36251880535796
actor:  1 policy actor:  1  step number:  30 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  1  action  0 :  tensor([0.5555, 0.0161, 0.0832, 0.0466, 0.1617, 0.0459, 0.0911],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0100, 0.9442, 0.0068, 0.0117, 0.0025, 0.0026, 0.0222],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0490,     0.0005,     0.7167,     0.0639,     0.0477,     0.0748,
            0.0474], grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0811, 0.2908, 0.1013, 0.2136, 0.0813, 0.0808, 0.1511],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1654, 0.0038, 0.1540, 0.1220, 0.3093, 0.1269, 0.1185],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0673, 0.1395, 0.1056, 0.0691, 0.0726, 0.4767, 0.0690],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0810, 0.1278, 0.1202, 0.1490, 0.0937, 0.0954, 0.3329],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.18057999999999982 0.6799999999999999 0.6799999999999999
line 256 mcts: sample exp_bonus 21.439819624855186
actor:  0 policy actor:  0  step number:  25 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  42.446808729245
Printing some Q and Qe and total Qs values:  [[0.376]
 [0.46 ]
 [0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.376]] [[30.764]
 [31.883]
 [30.764]
 [30.764]
 [30.764]
 [30.764]
 [30.764]] [[0.833]
 [0.95 ]
 [0.833]
 [0.833]
 [0.833]
 [0.833]
 [0.833]]
actor:  0 policy actor:  1  step number:  25 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18075999999999984 0.6799999999999999 0.6799999999999999
actor:  1 policy actor:  1  step number:  28 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.18075999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.0845386409763781, 0.4789473453770161, 0.1046584932347937, 0.06889841181993168, 0.14995459636028566, 0.11300251223159485]
printing an ep nov before normalisation:  26.518596819274475
printing an ep nov before normalisation:  28.060708045959473
maxi score, test score, baseline:  0.18075999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.0845386409763781, 0.4789473453770161, 0.1046584932347937, 0.06889841181993168, 0.14995459636028566, 0.11300251223159485]
actor:  1 policy actor:  1  step number:  34 total reward:  0.1899999999999995  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18075999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.0845386409763781, 0.4789473453770161, 0.1046584932347937, 0.06889841181993168, 0.14995459636028566, 0.11300251223159485]
printing an ep nov before normalisation:  43.91087055206299
printing an ep nov before normalisation:  29.668757915864447
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]] [[37.692]
 [37.692]
 [37.692]
 [37.692]
 [37.692]
 [37.692]
 [37.692]] [[1.915]
 [1.915]
 [1.915]
 [1.915]
 [1.915]
 [1.915]
 [1.915]]
maxi score, test score, baseline:  0.18075999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.0845386409763781, 0.4789473453770161, 0.1046584932347937, 0.06889841181993168, 0.14995459636028566, 0.11300251223159485]
printing an ep nov before normalisation:  28.17677684337806
printing an ep nov before normalisation:  47.21511242572139
actor:  1 policy actor:  1  step number:  31 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.18075999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.0845386409763781, 0.4789473453770161, 0.1046584932347937, 0.06889841181993168, 0.14995459636028566, 0.11300251223159485]
actor:  1 policy actor:  1  step number:  50 total reward:  0.20999999999999952  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.328]
 [0.201]
 [0.289]
 [0.289]
 [0.289]
 [0.289]
 [0.289]] [[20.673]
 [24.278]
 [18.112]
 [18.112]
 [18.112]
 [18.112]
 [18.112]] [[1.353]
 [1.448]
 [1.156]
 [1.156]
 [1.156]
 [1.156]
 [1.156]]
printing an ep nov before normalisation:  36.69949491042291
printing an ep nov before normalisation:  29.56446499270058
actor:  1 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  27 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  23.908878001181197
from probs:  [0.0845386409763781, 0.4789473453770161, 0.1046584932347937, 0.06889841181993168, 0.14995459636028566, 0.11300251223159485]
actor:  1 policy actor:  1  step number:  37 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  0
maxi score, test score, baseline:  0.17755999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.0845386409763781, 0.4789473453770161, 0.1046584932347937, 0.06889841181993168, 0.14995459636028566, 0.11300251223159485]
printing an ep nov before normalisation:  31.87143041816288
using explorer policy with actor:  0
printing an ep nov before normalisation:  40.34408460639727
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  0 policy actor:  1  step number:  34 total reward:  0.2899999999999996  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  42 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17691999999999983 0.6799999999999999 0.6799999999999999
probs:  [0.0845386409763781, 0.4789473453770161, 0.1046584932347937, 0.06889841181993168, 0.14995459636028566, 0.11300251223159485]
actor:  1 policy actor:  1  step number:  34 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  25.315018494923912
maxi score, test score, baseline:  0.17691999999999983 0.6799999999999999 0.6799999999999999
probs:  [0.0845386409763781, 0.4789473453770161, 0.1046584932347937, 0.06889841181993168, 0.14995459636028566, 0.11300251223159485]
actor:  0 policy actor:  0  step number:  20 total reward:  0.73  reward:  1.0 rdn_beta:  0.667
using another actor
Printing some Q and Qe and total Qs values:  [[0.426]
 [0.517]
 [0.426]
 [0.34 ]
 [0.325]
 [0.342]
 [0.344]] [[28.671]
 [29.098]
 [28.671]
 [29.035]
 [29.343]
 [29.223]
 [28.96 ]] [[0.714]
 [0.814]
 [0.714]
 [0.636]
 [0.627]
 [0.642]
 [0.638]]
actor:  1 policy actor:  1  step number:  25 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  30.351419754490568
Printing some Q and Qe and total Qs values:  [[0.28 ]
 [0.439]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.28 ]
 [0.439]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]]
printing an ep nov before normalisation:  24.9160269810545
maxi score, test score, baseline:  0.17705999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.0845386409763781, 0.4789473453770161, 0.1046584932347937, 0.06889841181993168, 0.14995459636028566, 0.11300251223159485]
actor:  1 policy actor:  1  step number:  32 total reward:  0.2899999999999996  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  31 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  26 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.717187876484335
printing an ep nov before normalisation:  39.96368011455317
maxi score, test score, baseline:  0.17709999999999984 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.17709999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.0845386409763781, 0.4789473453770161, 0.1046584932347937, 0.06889841181993168, 0.14995459636028566, 0.11300251223159485]
actor:  1 policy actor:  1  step number:  43 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  34 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17709999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.0845386409763781, 0.4789473453770161, 0.1046584932347937, 0.06889841181993168, 0.14995459636028566, 0.11300251223159485]
siam score:  -0.7549443
actor:  1 policy actor:  1  step number:  53 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  0.014754248294934769
actor:  1 policy actor:  1  step number:  33 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17709999999999984 0.6799999999999999 0.6799999999999999
actor:  1 policy actor:  1  step number:  22 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  45 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17709999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.0845386409763781, 0.4789473453770161, 0.1046584932347937, 0.06889841181993168, 0.14995459636028566, 0.11300251223159485]
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.579]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]] [[35.988]
 [34.876]
 [30.027]
 [30.027]
 [30.027]
 [30.027]
 [30.027]] [[1.182]
 [1.143]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.17709999999999984 0.6799999999999999 0.6799999999999999
actor:  0 policy actor:  1  step number:  25 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
from probs:  [0.0845386409763781, 0.4789473453770161, 0.1046584932347937, 0.06889841181993168, 0.14995459636028566, 0.11300251223159485]
actor:  0 policy actor:  0  step number:  22 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  37.52707773009495
Printing some Q and Qe and total Qs values:  [[0.957]
 [0.957]
 [0.976]
 [0.957]
 [0.957]
 [0.957]
 [0.957]] [[37.184]
 [37.184]
 [34.558]
 [37.184]
 [37.184]
 [37.184]
 [37.184]] [[0.957]
 [0.957]
 [0.976]
 [0.957]
 [0.957]
 [0.957]
 [0.957]]
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.685]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]] [[29.475]
 [29.814]
 [29.475]
 [29.475]
 [29.475]
 [29.475]
 [29.475]] [[1.432]
 [1.596]
 [1.432]
 [1.432]
 [1.432]
 [1.432]
 [1.432]]
printing an ep nov before normalisation:  38.599595576919455
maxi score, test score, baseline:  0.17693999999999985 0.6799999999999999 0.6799999999999999
actor:  0 policy actor:  0  step number:  19 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.18005999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.0845386409763781, 0.4789473453770161, 0.1046584932347937, 0.06889841181993168, 0.14995459636028566, 0.11300251223159485]
actor:  1 policy actor:  1  step number:  29 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  0
maxi score, test score, baseline:  0.18005999999999986 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.18005999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.0845386409763781, 0.4789473453770161, 0.1046584932347937, 0.06889841181993168, 0.14995459636028566, 0.11300251223159485]
maxi score, test score, baseline:  0.18005999999999986 0.6799999999999999 0.6799999999999999
printing an ep nov before normalisation:  0.005264367310360285
actor:  0 policy actor:  1  step number:  28 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  23 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  39 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.17961999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08450543720681085, 0.4789647246037776, 0.1046622871252365, 0.06890090774830576, 0.14996003433601554, 0.11300660897985365]
actor:  1 policy actor:  1  step number:  22 total reward:  0.71  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17961999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08448150642776542, 0.48115445991363104, 0.10421477366720786, 0.06920489568766383, 0.1485606191079636, 0.11238374519576814]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17961999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08448150642776542, 0.48115445991363104, 0.10421477366720786, 0.06920489568766383, 0.1485606191079636, 0.11238374519576814]
actor:  1 policy actor:  1  step number:  33 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  26.959394479961208
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17653999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08448150642776542, 0.48115445991363104, 0.10421477366720786, 0.06920489568766383, 0.1485606191079636, 0.11238374519576814]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  35 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  24.294594698559024
maxi score, test score, baseline:  0.17653999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08448150642776542, 0.48115445991363104, 0.10421477366720786, 0.06920489568766383, 0.1485606191079636, 0.11238374519576814]
maxi score, test score, baseline:  0.17653999999999986 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.17653999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08448150642776542, 0.48115445991363104, 0.10421477366720786, 0.06920489568766383, 0.1485606191079636, 0.11238374519576814]
actor:  0 policy actor:  0  step number:  37 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.17905999999999989 0.6799999999999999 0.6799999999999999
probs:  [0.08448150642776542, 0.48115445991363104, 0.10421477366720786, 0.06920489568766383, 0.1485606191079636, 0.11238374519576814]
actor:  1 policy actor:  1  step number:  37 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.667
from probs:  [0.08448150642776542, 0.48115445991363104, 0.10421477366720786, 0.06920489568766383, 0.1485606191079636, 0.11238374519576814]
maxi score, test score, baseline:  0.17905999999999989 0.6799999999999999 0.6799999999999999
probs:  [0.08448150642776542, 0.48115445991363104, 0.10421477366720786, 0.06920489568766383, 0.1485606191079636, 0.11238374519576814]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17905999999999989 0.6799999999999999 0.6799999999999999
probs:  [0.08448150642776542, 0.48115445991363104, 0.10421477366720786, 0.06920489568766383, 0.1485606191079636, 0.11238374519576814]
actor:  0 policy actor:  0  step number:  22 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  30.783021450042725
printing an ep nov before normalisation:  44.58232957455913
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  28 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7604858
siam score:  -0.75955003
actor:  1 policy actor:  1  step number:  36 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  21.707656383514404
printing an ep nov before normalisation:  35.669049592477
printing an ep nov before normalisation:  27.409889790621225
actor:  1 policy actor:  1  step number:  36 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.182]
 [0.242]
 [0.196]
 [0.195]
 [0.191]
 [0.191]
 [0.192]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.182]
 [0.242]
 [0.196]
 [0.195]
 [0.191]
 [0.191]
 [0.192]]
Printing some Q and Qe and total Qs values:  [[0.407]
 [0.561]
 [0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]] [[27.048]
 [31.699]
 [27.048]
 [27.048]
 [27.048]
 [27.048]
 [27.048]] [[0.702]
 [0.946]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]]
printing an ep nov before normalisation:  33.584900261634374
actor:  1 policy actor:  1  step number:  39 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.0
actions average: 
K:  2  action  0 :  tensor([0.4377, 0.0049, 0.0755, 0.0775, 0.2190, 0.0645, 0.1209],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0041, 0.9263, 0.0182, 0.0089, 0.0024, 0.0038, 0.0364],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0527, 0.0116, 0.6804, 0.0472, 0.0595, 0.0881, 0.0606],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1039, 0.0109, 0.1099, 0.3782, 0.1458, 0.1099, 0.1414],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1796, 0.0134, 0.1190, 0.0895, 0.3705, 0.0864, 0.1415],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1405, 0.1309, 0.1291, 0.1226, 0.1305, 0.2297, 0.1167],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1094, 0.1078, 0.1732, 0.1418, 0.1183, 0.1136, 0.2359],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  29.214003456978972
printing an ep nov before normalisation:  23.829297990715627
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  42.79004711953935
actor:  1 policy actor:  1  step number:  44 total reward:  0.22999999999999954  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  30 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.18251999999999982 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.18251999999999982 0.6799999999999999 0.6799999999999999
printing an ep nov before normalisation:  47.062043693973294
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.296]
 [0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]] [[27.363]
 [30.174]
 [27.363]
 [27.363]
 [27.363]
 [27.363]
 [27.363]] [[0.761]
 [0.704]
 [0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.761]]
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  41 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  37 total reward:  0.13999999999999946  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18249999999999986 0.6799999999999999 0.6799999999999999
printing an ep nov before normalisation:  44.87783755487762
maxi score, test score, baseline:  0.18249999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08441687665096291, 0.4811884416753909, 0.10422212651171248, 0.06920977525812001, 0.14857110476626295, 0.11239167513755075]
printing an ep nov before normalisation:  0.00021550720504137644
actor:  0 policy actor:  0  step number:  32 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  0.16194569065817177
actor:  1 policy actor:  1  step number:  32 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  35.150085930156735
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.35 ]
 [0.218]
 [0.218]
 [0.218]
 [0.413]
 [0.218]] [[29.589]
 [34.594]
 [32.631]
 [32.631]
 [32.631]
 [28.393]
 [32.631]] [[1.2  ]
 [1.542]
 [1.266]
 [1.266]
 [1.266]
 [1.151]
 [1.266]]
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.31 ]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]] [[43.4  ]
 [48.789]
 [42.337]
 [42.337]
 [42.337]
 [42.337]
 [42.337]] [[0.905]
 [0.643]
 [0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.626]]
actor:  1 policy actor:  1  step number:  30 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18261999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.08441687665096291, 0.4811884416753909, 0.10422212651171248, 0.06920977525812001, 0.14857110476626295, 0.11239167513755075]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.18261999999999987 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.18261999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
actor:  1 policy actor:  1  step number:  38 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  25 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18261999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
maxi score, test score, baseline:  0.18261999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
maxi score, test score, baseline:  0.18261999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
actions average: 
K:  1  action  0 :  tensor([0.5321, 0.0210, 0.0586, 0.0480, 0.2096, 0.0478, 0.0829],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0051, 0.9501, 0.0058, 0.0083, 0.0052, 0.0052, 0.0203],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0577, 0.0526, 0.6343, 0.0579, 0.0589, 0.0715, 0.0670],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1644, 0.0128, 0.1937, 0.1536, 0.1579, 0.1516, 0.1660],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1333, 0.0019, 0.0521, 0.0647, 0.6350, 0.0550, 0.0579],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0766, 0.0014, 0.2845, 0.0796, 0.0847, 0.3571, 0.1161],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1082, 0.0056, 0.1103, 0.1132, 0.1047, 0.1063, 0.4517],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.18261999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
maxi score, test score, baseline:  0.18261999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  32 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.18263999999999983 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
actions average: 
K:  4  action  0 :  tensor([0.4583, 0.0013, 0.1006, 0.0751, 0.1491, 0.1078, 0.1078],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0040, 0.8939, 0.0095, 0.0433, 0.0075, 0.0055, 0.0363],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0617, 0.0945, 0.5917, 0.0520, 0.0521, 0.0684, 0.0797],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1368, 0.0290, 0.1300, 0.2455, 0.1358, 0.1189, 0.2041],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0993, 0.0214, 0.1095, 0.0742, 0.4940, 0.0801, 0.1215],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1732, 0.0079, 0.2635, 0.1082, 0.0994, 0.1609, 0.1870],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1242, 0.0156, 0.1140, 0.1618, 0.1345, 0.1279, 0.3219],
       grad_fn=<DivBackward0>)
siam score:  -0.75634414
actor:  1 policy actor:  1  step number:  38 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17955999999999983 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
printing an ep nov before normalisation:  28.744022807566687
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17955999999999983 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
printing an ep nov before normalisation:  32.3330873877025
maxi score, test score, baseline:  0.17955999999999983 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  31.81347131513977
actor:  0 policy actor:  0  step number:  40 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 32.971552159311315
actor:  1 policy actor:  1  step number:  38 total reward:  0.2699999999999997  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17913999999999983 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
maxi score, test score, baseline:  0.17913999999999983 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.17913999999999983 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
printing an ep nov before normalisation:  21.607128068558684
printing an ep nov before normalisation:  41.56447887294731
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  32.062426933098756
actions average: 
K:  2  action  0 :  tensor([0.6381, 0.0109, 0.0733, 0.0593, 0.0650, 0.0702, 0.0832],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0015, 0.9289, 0.0029, 0.0163, 0.0011, 0.0016, 0.0477],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0508, 0.0009, 0.7565, 0.0417, 0.0451, 0.0592, 0.0458],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0626, 0.1022, 0.0924, 0.3400, 0.1312, 0.1162, 0.1554],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.3637, 0.0019, 0.0597, 0.0739, 0.3558, 0.0800, 0.0650],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0990, 0.0025, 0.1408, 0.1223, 0.1074, 0.4359, 0.0921],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1403, 0.0027, 0.0984, 0.1239, 0.1327, 0.1364, 0.3655],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  32.99775056443528
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  30 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17913999999999983 0.6799999999999999 0.6799999999999999
printing an ep nov before normalisation:  22.80806976705867
printing an ep nov before normalisation:  31.07229953866001
printing an ep nov before normalisation:  30.020542316869417
printing an ep nov before normalisation:  37.428641059764914
printing an ep nov before normalisation:  30.08593352748293
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.556]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]] [[33.943]
 [28.488]
 [33.943]
 [33.943]
 [33.943]
 [33.943]
 [33.943]] [[0.847]
 [0.796]
 [0.847]
 [0.847]
 [0.847]
 [0.847]
 [0.847]]
printing an ep nov before normalisation:  35.93761443454128
actor:  0 policy actor:  0  step number:  27 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  30.49112796673962
actions average: 
K:  3  action  0 :  tensor([0.5194, 0.0047, 0.0885, 0.0875, 0.1067, 0.0950, 0.0982],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0029,     0.9575,     0.0250,     0.0015,     0.0008,     0.0012,
            0.0111], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0344, 0.0059, 0.8029, 0.0335, 0.0418, 0.0467, 0.0349],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0845, 0.1546, 0.0882, 0.2799, 0.1364, 0.1191, 0.1374],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2990, 0.0012, 0.0926, 0.0772, 0.3186, 0.0915, 0.1200],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1227, 0.0118, 0.1958, 0.1064, 0.1224, 0.3314, 0.1094],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0609, 0.2372, 0.0452, 0.0588, 0.0568, 0.0458, 0.4954],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  35 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  29.111333611084557
actor:  1 policy actor:  1  step number:  45 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17993999999999985 0.6799999999999999 0.6799999999999999
actor:  1 policy actor:  1  step number:  36 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  0.667
from probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
maxi score, test score, baseline:  0.17993999999999985 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
printing an ep nov before normalisation:  32.90146905063249
printing an ep nov before normalisation:  0.0017792620445788998
maxi score, test score, baseline:  0.17993999999999985 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
actor:  1 policy actor:  1  step number:  36 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17993999999999985 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
printing an ep nov before normalisation:  20.061690160703233
maxi score, test score, baseline:  0.17993999999999985 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
printing an ep nov before normalisation:  35.966217188194335
printing an ep nov before normalisation:  36.87709433803836
printing an ep nov before normalisation:  37.44188157883081
actor:  0 policy actor:  1  step number:  31 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  42 total reward:  0.3299999999999996  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.85 ]
 [0.925]
 [0.18 ]
 [0.266]
 [0.818]
 [0.165]
 [0.259]] [[39.255]
 [31.904]
 [37.791]
 [37.118]
 [27.251]
 [38.693]
 [41.173]] [[0.85 ]
 [0.925]
 [0.18 ]
 [0.266]
 [0.818]
 [0.165]
 [0.259]]
printing an ep nov before normalisation:  41.64331784622241
printing an ep nov before normalisation:  38.111540322065416
siam score:  -0.7519771
maxi score, test score, baseline:  0.17667999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
actor:  1 policy actor:  1  step number:  34 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  35 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  40.21412709001839
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17667999999999984 0.6799999999999999 0.6799999999999999
printing an ep nov before normalisation:  36.239440410605454
maxi score, test score, baseline:  0.17667999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
printing an ep nov before normalisation:  47.717795464409804
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.407]
 [0.405]
 [0.399]
 [0.387]
 [0.353]
 [0.4  ]] [[33.244]
 [32.601]
 [33.517]
 [33.652]
 [33.711]
 [35.892]
 [33.848]] [[1.897]
 [1.87 ]
 [1.948]
 [1.954]
 [1.947]
 [2.103]
 [1.972]]
printing an ep nov before normalisation:  40.11936913424729
printing an ep nov before normalisation:  34.35484602136509
maxi score, test score, baseline:  0.17667999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
actor:  1 policy actor:  1  step number:  33 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.17667999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
actor:  1 policy actor:  1  step number:  37 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  37 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17667999999999984 0.6799999999999999 0.6799999999999999
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.568]
 [0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.553]] [[35.549]
 [41.192]
 [35.549]
 [35.549]
 [35.549]
 [35.549]
 [35.549]] [[1.42 ]
 [1.697]
 [1.42 ]
 [1.42 ]
 [1.42 ]
 [1.42 ]
 [1.42 ]]
printing an ep nov before normalisation:  45.523134848544984
siam score:  -0.74702364
actor:  1 policy actor:  1  step number:  28 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  34 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  29.681130021618152
printing an ep nov before normalisation:  28.567318062018185
siam score:  -0.74383926
actions average: 
K:  0  action  0 :  tensor([    0.8054,     0.0007,     0.0335,     0.0284,     0.0661,     0.0270,
            0.0390], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0013,     0.9700,     0.0031,     0.0058,     0.0004,     0.0006,
            0.0188], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0477,     0.0002,     0.7341,     0.0429,     0.0496,     0.0792,
            0.0462], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1071, 0.0634, 0.1728, 0.2939, 0.1044, 0.1129, 0.1455],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1304, 0.0088, 0.1010, 0.0836, 0.4873, 0.0800, 0.1088],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1035, 0.0036, 0.1571, 0.0816, 0.0703, 0.4532, 0.1307],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0414, 0.3087, 0.0339, 0.0830, 0.0432, 0.0363, 0.4536],
       grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 34.83934106559757
maxi score, test score, baseline:  0.17667999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
maxi score, test score, baseline:  0.17667999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  0  action  0 :  tensor([    0.6644,     0.0002,     0.0657,     0.0552,     0.0617,     0.0667,
            0.0861], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0025,     0.9814,     0.0022,     0.0023,     0.0003,     0.0007,
            0.0107], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0446, 0.0056, 0.7716, 0.0317, 0.0343, 0.0636, 0.0486],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1046, 0.0046, 0.0950, 0.4413, 0.1358, 0.1167, 0.1020],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1255, 0.0012, 0.1130, 0.1105, 0.4095, 0.1173, 0.1230],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0657, 0.0015, 0.1492, 0.0672, 0.0614, 0.5986, 0.0565],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1143, 0.0243, 0.0913, 0.0786, 0.0479, 0.0517, 0.5919],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  34 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17667999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
maxi score, test score, baseline:  0.17667999999999986 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.17667999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.609]
 [0.585]
 [0.571]
 [0.588]
 [0.586]
 [0.587]] [[38.3  ]
 [38.244]
 [38.397]
 [38.288]
 [38.222]
 [38.942]
 [38.092]] [[0.574]
 [0.609]
 [0.585]
 [0.571]
 [0.588]
 [0.586]
 [0.587]]
actor:  1 policy actor:  1  step number:  33 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  2  action  0 :  tensor([0.4698, 0.0030, 0.0774, 0.1046, 0.1657, 0.1017, 0.0778],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0151, 0.9138, 0.0324, 0.0063, 0.0025, 0.0064, 0.0236],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0214, 0.0236, 0.8391, 0.0215, 0.0210, 0.0534, 0.0200],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0273, 0.0097, 0.0201, 0.7783, 0.0397, 0.0590, 0.0660],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0813, 0.0024, 0.0923, 0.0897, 0.5581, 0.0881, 0.0880],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1178, 0.0021, 0.1282, 0.1514, 0.1415, 0.3592, 0.0998],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1200, 0.0341, 0.1415, 0.1868, 0.1233, 0.1320, 0.2623],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.17667999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
actor:  0 policy actor:  0  step number:  28 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  25 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  0.667
siam score:  -0.74845946
printing an ep nov before normalisation:  34.09092436625848
printing an ep nov before normalisation:  38.50366821935228
printing an ep nov before normalisation:  30.62793232074095
actor:  0 policy actor:  1  step number:  33 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18015999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
maxi score, test score, baseline:  0.18015999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
maxi score, test score, baseline:  0.18015999999999988 0.6799999999999999 0.6799999999999999
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  56 total reward:  0.10999999999999954  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18015999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
printing an ep nov before normalisation:  40.093465814389994
using another actor
maxi score, test score, baseline:  0.18015999999999988 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
actor:  1 policy actor:  1  step number:  34 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17691999999999983 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
maxi score, test score, baseline:  0.17691999999999983 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
maxi score, test score, baseline:  0.17353999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
actor:  1 policy actor:  1  step number:  20 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7659858
printing an ep nov before normalisation:  60.097685234880196
maxi score, test score, baseline:  0.17353999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
Printing some Q and Qe and total Qs values:  [[0.472]
 [0.61 ]
 [0.472]
 [0.472]
 [0.472]
 [0.472]
 [0.472]] [[23.512]
 [33.533]
 [23.512]
 [23.512]
 [23.512]
 [23.512]
 [23.512]] [[1.027]
 [1.995]
 [1.027]
 [1.027]
 [1.027]
 [1.027]
 [1.027]]
actor:  1 policy actor:  1  step number:  33 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.52 ]
 [0.526]
 [0.476]
 [0.501]
 [0.499]
 [0.498]] [[36.531]
 [37.   ]
 [33.265]
 [36.884]
 [36.643]
 [36.278]
 [36.471]] [[0.984]
 [1.047]
 [0.948]
 [1.   ]
 [1.018]
 [1.006]
 [1.01 ]]
maxi score, test score, baseline:  0.17353999999999986 0.6799999999999999 0.6799999999999999
actor:  1 policy actor:  1  step number:  32 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  33 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.17353999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
maxi score, test score, baseline:  0.17353999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
actor:  1 policy actor:  1  step number:  39 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  32.09705541067857
maxi score, test score, baseline:  0.17353999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.35211189022932
maxi score, test score, baseline:  0.17353999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
printing an ep nov before normalisation:  44.85011097776395
actor:  1 policy actor:  1  step number:  34 total reward:  0.2699999999999996  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.094]
 [0.137]
 [0.106]
 [0.099]
 [0.096]
 [0.106]
 [0.101]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.094]
 [0.137]
 [0.106]
 [0.099]
 [0.096]
 [0.106]
 [0.101]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.22999999999999954  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  28 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  40.685481013973
maxi score, test score, baseline:  0.17335999999999985 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.17335999999999985 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
actor:  0 policy actor:  1  step number:  36 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.061 0.571 0.082 0.082 0.082 0.041 0.082]
actor:  1 policy actor:  1  step number:  34 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17645999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.08438475225479676, 0.4812053323971479, 0.10422578126191429, 0.06921220066111858, 0.148576316689589, 0.1123956167354334]
Printing some Q and Qe and total Qs values:  [[0.228]
 [0.323]
 [0.244]
 [0.243]
 [0.243]
 [0.242]
 [0.243]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.228]
 [0.323]
 [0.244]
 [0.243]
 [0.243]
 [0.242]
 [0.243]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  32 total reward:  0.69  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  42.25960011562063
maxi score, test score, baseline:  0.17645999999999987 0.6799999999999999 0.6799999999999999
printing an ep nov before normalisation:  26.42291553725811
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.518]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]] [[30.751]
 [34.738]
 [30.751]
 [30.751]
 [30.751]
 [30.751]
 [30.751]] [[0.646]
 [0.84 ]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]]
maxi score, test score, baseline:  0.17645999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
actor:  0 policy actor:  1  step number:  23 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  31 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17683999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
maxi score, test score, baseline:  0.17683999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
actor:  1 policy actor:  1  step number:  35 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  42.542200530259294
maxi score, test score, baseline:  0.17683999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.48 ]
 [0.381]
 [0.381]
 [0.381]
 [0.381]
 [0.381]] [[21.082]
 [29.559]
 [21.082]
 [21.082]
 [21.082]
 [21.082]
 [21.082]] [[0.647]
 [0.949]
 [0.647]
 [0.647]
 [0.647]
 [0.647]
 [0.647]]
maxi score, test score, baseline:  0.17683999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
printing an ep nov before normalisation:  39.142386899716534
actor:  1 policy actor:  1  step number:  33 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17683999999999986 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.17683999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
printing an ep nov before normalisation:  39.92067335576128
actor:  1 policy actor:  1  step number:  54 total reward:  0.02999999999999936  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  48.5642671584799
actor:  1 policy actor:  1  step number:  26 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  49.497358221549085
maxi score, test score, baseline:  0.17683999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
printing an ep nov before normalisation:  42.299152381184676
maxi score, test score, baseline:  0.17683999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
printing an ep nov before normalisation:  57.31977637062416
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.448]
 [0.353]
 [0.448]
 [0.448]
 [0.448]
 [0.448]] [[44.961]
 [44.961]
 [49.4  ]
 [44.961]
 [44.961]
 [44.961]
 [44.961]] [[1.86]
 [1.86]
 [2.02]
 [1.86]
 [1.86]
 [1.86]
 [1.86]]
maxi score, test score, baseline:  0.17683999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
maxi score, test score, baseline:  0.17683999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
Printing some Q and Qe and total Qs values:  [[-0.08 ]
 [-0.094]
 [-0.08 ]
 [-0.075]
 [-0.074]
 [-0.075]
 [-0.072]] [[27.133]
 [20.668]
 [26.516]
 [27.74 ]
 [27.765]
 [27.904]
 [27.961]] [[1.326]
 [0.977]
 [1.295]
 [1.363]
 [1.365]
 [1.371]
 [1.377]]
Printing some Q and Qe and total Qs values:  [[-0.078]
 [-0.094]
 [-0.085]
 [-0.079]
 [-0.076]
 [-0.076]
 [-0.075]] [[26.674]
 [20.668]
 [26.733]
 [26.684]
 [27.104]
 [27.24 ]
 [27.278]] [[1.304]
 [0.977]
 [1.301]
 [1.304]
 [1.329]
 [1.336]
 [1.339]]
maxi score, test score, baseline:  0.17683999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
actor:  1 policy actor:  1  step number:  40 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  1.667
from probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
actor:  1 policy actor:  1  step number:  31 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.17683999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.409]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.336]
 [0.409]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]]
using explorer policy with actor:  1
siam score:  -0.7650557
actor:  1 policy actor:  1  step number:  24 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17683999999999986 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.17683999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  39 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  37 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17667999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.506]
 [0.407]
 [0.421]
 [0.444]
 [0.404]
 [0.444]] [[29.416]
 [31.499]
 [28.622]
 [28.772]
 [28.338]
 [27.588]
 [28.338]] [[0.563]
 [0.678]
 [0.549]
 [0.566]
 [0.584]
 [0.536]
 [0.584]]
printing an ep nov before normalisation:  34.71068542450741
actor:  0 policy actor:  0  step number:  22 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.17683999999999983 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  36 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  34.98290506586769
printing an ep nov before normalisation:  34.00362730026245
actor:  0 policy actor:  0  step number:  26 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.463]
 [0.447]
 [0.447]
 [0.447]
 [0.447]
 [0.447]] [[35.799]
 [31.029]
 [35.799]
 [35.799]
 [35.799]
 [35.799]
 [35.799]] [[1.09 ]
 [0.958]
 [1.09 ]
 [1.09 ]
 [1.09 ]
 [1.09 ]
 [1.09 ]]
maxi score, test score, baseline:  0.17681999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
maxi score, test score, baseline:  0.17681999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
printing an ep nov before normalisation:  24.171829223632812
actor:  1 policy actor:  1  step number:  32 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17681999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
actor:  1 policy actor:  1  step number:  30 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.17681999999999984 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.17681999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
printing an ep nov before normalisation:  32.38724903794036
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.408]
 [0.337]
 [0.337]
 [0.334]
 [0.339]
 [0.337]] [[29.581]
 [28.052]
 [27.684]
 [36.384]
 [33.561]
 [32.326]
 [27.684]] [[1.026]
 [1.024]
 [0.938]
 [1.298]
 [1.178]
 [1.132]
 [0.938]]
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.526]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]] [[37.466]
 [34.019]
 [37.466]
 [37.466]
 [37.466]
 [37.466]
 [37.466]] [[0.83 ]
 [0.785]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]]
maxi score, test score, baseline:  0.17681999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
actor:  1 policy actor:  1  step number:  30 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17681999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.817536208850854
printing an ep nov before normalisation:  40.40110733048582
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  36 total reward:  0.34999999999999976  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17681999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
maxi score, test score, baseline:  0.17681999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
printing an ep nov before normalisation:  31.754920482635498
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.526]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]] [[28.491]
 [32.813]
 [28.491]
 [28.491]
 [28.491]
 [28.491]
 [28.491]] [[0.802]
 [0.928]
 [0.802]
 [0.802]
 [0.802]
 [0.802]
 [0.802]]
maxi score, test score, baseline:  0.17681999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
actor:  1 policy actor:  1  step number:  54 total reward:  0.04999999999999938  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  44 total reward:  0.16999999999999948  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  38.97516756052082
actions average: 
K:  1  action  0 :  tensor([0.7151, 0.0013, 0.0393, 0.0388, 0.1179, 0.0435, 0.0440],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0010,     0.9701,     0.0017,     0.0052,     0.0004,     0.0005,
            0.0211], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0560, 0.0077, 0.6443, 0.0663, 0.0728, 0.0787, 0.0741],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0838, 0.0040, 0.1118, 0.4057, 0.0946, 0.1350, 0.1652],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.1747,     0.0005,     0.0285,     0.0406,     0.6927,     0.0345,
            0.0286], grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0697, 0.0874, 0.1419, 0.0892, 0.0846, 0.4443, 0.0829],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0644, 0.0765, 0.1030, 0.0702, 0.0452, 0.0740, 0.5668],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  26 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
siam score:  -0.7571103
maxi score, test score, baseline:  0.17681999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
actor:  1 policy actor:  1  step number:  28 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  28 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.191]
 [0.314]
 [0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.191]
 [0.314]
 [0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]]
printing an ep nov before normalisation:  38.68316812125852
maxi score, test score, baseline:  0.17681999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
maxi score, test score, baseline:  0.17681999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17681999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
maxi score, test score, baseline:  0.17681999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
printing an ep nov before normalisation:  32.904744148254395
actor:  0 policy actor:  0  step number:  33 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.17637999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  33.17780714966342
actor:  0 policy actor:  1  step number:  36 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  0.333
from probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
printing an ep nov before normalisation:  24.921016693115234
maxi score, test score, baseline:  0.17617999999999986 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.17617999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
maxi score, test score, baseline:  0.17315999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
maxi score, test score, baseline:  0.17315999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
using explorer policy with actor:  0
actor:  0 policy actor:  0  step number:  26 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  37.26421380326282
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17345999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
actor:  1 policy actor:  1  step number:  22 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17345999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
actor:  0 policy actor:  1  step number:  46 total reward:  0.1899999999999995  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17583999999999983 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
maxi score, test score, baseline:  0.17583999999999983 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
maxi score, test score, baseline:  0.17583999999999983 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
maxi score, test score, baseline:  0.17583999999999983 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
printing an ep nov before normalisation:  36.659841096432096
actor:  1 policy actor:  1  step number:  29 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17583999999999983 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
maxi score, test score, baseline:  0.17583999999999983 0.6799999999999999 0.6799999999999999
actor:  0 policy actor:  1  step number:  38 total reward:  0.12999999999999945  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17481999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
printing an ep nov before normalisation:  42.155151781215274
maxi score, test score, baseline:  0.17481999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  25 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17481999999999984 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.17481999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
actor:  1 policy actor:  1  step number:  35 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  3  action  0 :  tensor([0.7093, 0.0097, 0.0337, 0.0497, 0.1190, 0.0340, 0.0445],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0009,     0.9801,     0.0069,     0.0037,     0.0002,     0.0014,
            0.0069], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0381, 0.0878, 0.5886, 0.0470, 0.0287, 0.1641, 0.0458],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1019, 0.0236, 0.0968, 0.3471, 0.1230, 0.1457, 0.1620],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1019, 0.0045, 0.0643, 0.1294, 0.5515, 0.0861, 0.0623],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0154, 0.0185, 0.2389, 0.0441, 0.0227, 0.5196, 0.1408],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1071, 0.0532, 0.1130, 0.1990, 0.1103, 0.1202, 0.2972],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.17481999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
maxi score, test score, baseline:  0.17481999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08545221450438825, 0.4806440702414082, 0.10410433748902631, 0.06913160689282981, 0.14840312957754193, 0.11226464129480547]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.792]
 [0.792]
 [0.848]
 [0.792]
 [0.792]
 [0.792]
 [0.792]] [[38.032]
 [38.032]
 [35.682]
 [38.032]
 [38.032]
 [38.032]
 [38.032]] [[0.792]
 [0.792]
 [0.848]
 [0.792]
 [0.792]
 [0.792]
 [0.792]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.903]
 [0.914]
 [0.837]
 [0.837]
 [0.857]
 [0.837]
 [0.813]] [[35.256]
 [34.696]
 [48.523]
 [48.523]
 [43.382]
 [48.523]
 [50.672]] [[0.903]
 [0.914]
 [0.837]
 [0.837]
 [0.857]
 [0.837]
 [0.813]]
actor:  1 policy actor:  1  step number:  19 total reward:  0.72  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  17.22978749325186
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.453]
 [0.447]
 [0.447]
 [0.447]
 [0.447]
 [0.447]] [[32.818]
 [35.719]
 [32.818]
 [32.818]
 [32.818]
 [32.818]
 [32.818]] [[1.2  ]
 [1.332]
 [1.2  ]
 [1.2  ]
 [1.2  ]
 [1.2  ]
 [1.2  ]]
printing an ep nov before normalisation:  31.41591654790565
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  25 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.969]
 [0.969]
 [0.969]
 [0.969]
 [0.969]
 [0.969]
 [0.969]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.969]
 [0.969]
 [0.969]
 [0.969]
 [0.969]
 [0.969]
 [0.969]]
maxi score, test score, baseline:  0.17781999999999984 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.17781999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08537718320327013, 0.4843324822286954, 0.1033582671030076, 0.06964373479099982, 0.14606334136488427, 0.11122499130914283]
maxi score, test score, baseline:  0.17781999999999984 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.17781999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08537718320327013, 0.4843324822286954, 0.1033582671030076, 0.06964373479099982, 0.14606334136488427, 0.11122499130914283]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  0
actor:  0 policy actor:  0  step number:  26 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  33 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.02606462007571
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.423]
 [0.42 ]
 [0.324]
 [0.302]
 [0.42 ]
 [0.448]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.401]
 [0.423]
 [0.42 ]
 [0.324]
 [0.302]
 [0.42 ]
 [0.448]]
actor:  0 policy actor:  1  step number:  31 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18059999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.08537718320327013, 0.4843324822286954, 0.1033582671030076, 0.06964373479099982, 0.14606334136488427, 0.11122499130914283]
maxi score, test score, baseline:  0.18059999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.08537718320327013, 0.4843324822286954, 0.1033582671030076, 0.06964373479099982, 0.14606334136488427, 0.11122499130914283]
printing an ep nov before normalisation:  40.521869289290294
printing an ep nov before normalisation:  20.37121697052746
Printing some Q and Qe and total Qs values:  [[0.783]
 [0.856]
 [0.783]
 [0.783]
 [0.819]
 [0.817]
 [0.783]] [[30.581]
 [30.492]
 [30.581]
 [30.581]
 [32.877]
 [31.293]
 [30.581]] [[0.783]
 [0.856]
 [0.783]
 [0.783]
 [0.819]
 [0.817]
 [0.783]]
maxi score, test score, baseline:  0.18059999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.08537718320327013, 0.4843324822286954, 0.1033582671030076, 0.06964373479099982, 0.14606334136488427, 0.11122499130914283]
printing an ep nov before normalisation:  30.65962791442871
printing an ep nov before normalisation:  46.99980341585509
siam score:  -0.75519854
siam score:  -0.75572145
actor:  1 policy actor:  1  step number:  30 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.18059999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.08537718320327024, 0.4843324822286954, 0.1033582671030076, 0.06964373479099982, 0.14606334136488427, 0.11122499130914283]
printing an ep nov before normalisation:  33.97253629804271
siam score:  -0.7562825
maxi score, test score, baseline:  0.18059999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08537718320327024, 0.4843324822286954, 0.1033582671030076, 0.06964373479099982, 0.14606334136488427, 0.11122499130914283]
printing an ep nov before normalisation:  0.014591440153992608
actor:  0 policy actor:  0  step number:  26 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18083999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08537718320327024, 0.4843324822286954, 0.1033582671030076, 0.06964373479099982, 0.14606334136488427, 0.11122499130914283]
printing an ep nov before normalisation:  28.777763843536377
maxi score, test score, baseline:  0.18083999999999986 0.6799999999999999 0.6799999999999999
actor:  1 policy actor:  1  step number:  28 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7557363
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  22 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.17795999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08537718320327013, 0.4843324822286954, 0.1033582671030076, 0.06964373479099982, 0.14606334136488427, 0.11122499130914283]
maxi score, test score, baseline:  0.17795999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08537718320327013, 0.4843324822286954, 0.1033582671030076, 0.06964373479099982, 0.14606334136488427, 0.11122499130914283]
printing an ep nov before normalisation:  47.44388738358847
actor:  1 policy actor:  1  step number:  21 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17795999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08537718320327013, 0.4843324822286954, 0.1033582671030076, 0.06964373479099982, 0.14606334136488427, 0.11122499130914283]
maxi score, test score, baseline:  0.17795999999999984 0.6799999999999999 0.6799999999999999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.382]
 [0.607]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.375]] [[26.078]
 [29.593]
 [26.078]
 [26.078]
 [26.078]
 [26.078]
 [20.713]] [[1.4  ]
 [1.845]
 [1.4  ]
 [1.4  ]
 [1.4  ]
 [1.4  ]
 [1.057]]
actor:  1 policy actor:  1  step number:  28 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.52 ]
 [0.564]
 [0.527]
 [0.523]
 [0.519]
 [0.51 ]
 [0.518]] [[37.109]
 [30.48 ]
 [36.387]
 [36.466]
 [36.863]
 [36.725]
 [36.726]] [[1.747]
 [1.374]
 [1.709]
 [1.71 ]
 [1.731]
 [1.713]
 [1.722]]
actor:  1 policy actor:  1  step number:  29 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  47 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  23 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  27 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17731999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08537718320327013, 0.4843324822286954, 0.1033582671030076, 0.06964373479099982, 0.14606334136488427, 0.11122499130914283]
actor:  1 policy actor:  1  step number:  29 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17731999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08537718320327024, 0.4843324822286954, 0.1033582671030076, 0.06964373479099982, 0.14606334136488427, 0.11122499130914283]
printing an ep nov before normalisation:  42.164615573501194
maxi score, test score, baseline:  0.17731999999999984 0.6799999999999999 0.6799999999999999
line 256 mcts: sample exp_bonus 38.34790861198154
maxi score, test score, baseline:  0.17731999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08537718320327013, 0.4843324822286954, 0.1033582671030076, 0.06964373479099982, 0.14606334136488427, 0.11122499130914283]
actor:  1 policy actor:  1  step number:  35 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  36 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  2.0
siam score:  -0.7618878
actor:  1 policy actor:  1  step number:  35 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17731999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08537718320327013, 0.4843324822286954, 0.1033582671030076, 0.06964373479099982, 0.14606334136488427, 0.11122499130914283]
maxi score, test score, baseline:  0.17731999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08537718320327013, 0.4843324822286954, 0.1033582671030076, 0.06964373479099982, 0.14606334136488427, 0.11122499130914283]
printing an ep nov before normalisation:  34.84399994396299
printing an ep nov before normalisation:  23.218806584676106
printing an ep nov before normalisation:  46.75876310468677
maxi score, test score, baseline:  0.17731999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08537718320327024, 0.4843324822286954, 0.1033582671030076, 0.06964373479099982, 0.14606334136488427, 0.11122499130914283]
Printing some Q and Qe and total Qs values:  [[0.658]
 [0.739]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]] [[38.885]
 [51.772]
 [38.885]
 [38.885]
 [38.885]
 [38.885]
 [38.885]] [[1.159]
 [1.406]
 [1.159]
 [1.159]
 [1.159]
 [1.159]
 [1.159]]
actions average: 
K:  3  action  0 :  tensor([0.5867, 0.0023, 0.1000, 0.0679, 0.0815, 0.0817, 0.0799],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0012,     0.9476,     0.0014,     0.0083,     0.0004,     0.0007,
            0.0404], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0474, 0.1039, 0.6342, 0.0388, 0.0407, 0.0866, 0.0485],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0770, 0.0344, 0.1031, 0.3920, 0.0938, 0.0841, 0.2156],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2124, 0.0076, 0.1275, 0.1038, 0.3643, 0.1234, 0.0609],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1265, 0.0007, 0.2027, 0.0731, 0.1141, 0.4193, 0.0636],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0639, 0.1169, 0.0781, 0.0773, 0.0609, 0.0616, 0.5413],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  33 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.469]
 [0.458]
 [0.458]
 [0.471]
 [0.458]
 [0.458]] [[35.476]
 [36.604]
 [34.316]
 [34.316]
 [40.803]
 [34.316]
 [34.316]] [[0.667]
 [0.708]
 [0.67 ]
 [0.67 ]
 [0.76 ]
 [0.67 ]
 [0.67 ]]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.691]
 [0.56 ]
 [0.573]
 [0.56 ]
 [0.565]
 [0.56 ]] [[35.389]
 [36.072]
 [35.389]
 [34.175]
 [35.389]
 [34.445]
 [35.389]] [[1.502]
 [1.667]
 [1.502]
 [1.453]
 [1.502]
 [1.458]
 [1.502]]
printing an ep nov before normalisation:  23.196280002593994
maxi score, test score, baseline:  0.17731999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08537718320327013, 0.4843324822286954, 0.1033582671030076, 0.06964373479099982, 0.14606334136488427, 0.11122499130914283]
actor:  1 policy actor:  1  step number:  33 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  47.72990901293628
printing an ep nov before normalisation:  32.09828853607178
printing an ep nov before normalisation:  29.921490759817598
printing an ep nov before normalisation:  27.059109210968018
actor:  0 policy actor:  1  step number:  25 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  35.55600785908542
actor:  1 policy actor:  1  step number:  29 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17741999999999983 0.6799999999999999 0.6799999999999999
printing an ep nov before normalisation:  44.881523545731184
maxi score, test score, baseline:  0.17741999999999983 0.6799999999999999 0.6799999999999999
actions average: 
K:  3  action  0 :  tensor([0.6480, 0.0062, 0.0645, 0.0652, 0.0970, 0.0594, 0.0597],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0078, 0.9261, 0.0297, 0.0075, 0.0069, 0.0077, 0.0144],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0801, 0.0555, 0.5620, 0.0511, 0.0549, 0.1125, 0.0838],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1738, 0.0129, 0.1837, 0.1710, 0.1780, 0.1522, 0.1285],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1734, 0.0045, 0.1271, 0.0824, 0.4320, 0.0891, 0.0915],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0657, 0.1589, 0.1570, 0.0838, 0.0817, 0.3682, 0.0848],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1343, 0.0135, 0.1123, 0.1021, 0.1213, 0.0999, 0.4167],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.753]] [[35.133]
 [35.133]
 [35.133]
 [35.133]
 [35.133]
 [35.133]
 [35.133]] [[2.42]
 [2.42]
 [2.42]
 [2.42]
 [2.42]
 [2.42]
 [2.42]]
printing an ep nov before normalisation:  32.22026348114014
actor:  1 policy actor:  1  step number:  40 total reward:  0.16999999999999948  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  33.23033631194217
printing an ep nov before normalisation:  14.783020993754295
Printing some Q and Qe and total Qs values:  [[0.261]
 [0.34 ]
 [0.261]
 [0.261]
 [0.319]
 [0.261]
 [0.261]] [[17.147]
 [24.009]
 [17.147]
 [17.147]
 [22.612]
 [17.147]
 [17.147]] [[0.959]
 [1.753]
 [0.959]
 [0.959]
 [1.587]
 [0.959]
 [0.959]]
actor:  1 policy actor:  1  step number:  32 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17741999999999983 0.6799999999999999 0.6799999999999999
probs:  [0.08537718320327024, 0.4843324822286954, 0.1033582671030076, 0.06964373479099982, 0.14606334136488427, 0.11122499130914283]
Printing some Q and Qe and total Qs values:  [[0.63 ]
 [0.585]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]] [[33.157]
 [33.843]
 [35.501]
 [35.501]
 [35.501]
 [35.501]
 [35.501]] [[2.44]
 [2.45]
 [2.59]
 [2.59]
 [2.59]
 [2.59]
 [2.59]]
actor:  1 policy actor:  1  step number:  30 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.667
siam score:  -0.76212907
maxi score, test score, baseline:  0.17741999999999983 0.6799999999999999 0.6799999999999999
probs:  [0.08537718320327024, 0.4843324822286954, 0.1033582671030076, 0.06964373479099982, 0.14606334136488427, 0.11122499130914283]
maxi score, test score, baseline:  0.17741999999999983 0.6799999999999999 0.6799999999999999
probs:  [0.08537718320327024, 0.4843324822286954, 0.1033582671030076, 0.06964373479099982, 0.14606334136488427, 0.11122499130914283]
actor:  1 policy actor:  1  step number:  17 total reward:  0.74  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.17733999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08522662372731742, 0.4917337316756438, 0.10186118568939721, 0.07067138201049747, 0.1413682703493369, 0.10913880654780712]
printing an ep nov before normalisation:  30.4681413234419
maxi score, test score, baseline:  0.17733999999999986 0.6799999999999999 0.6799999999999999
actor:  1 policy actor:  1  step number:  31 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17733999999999986 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.17733999999999986 0.6799999999999999 0.6799999999999999
probs:  [0.08522662372731742, 0.4917337316756438, 0.10186118568939721, 0.07067138201049747, 0.1413682703493369, 0.10913880654780712]
actor:  0 policy actor:  0  step number:  27 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.333
using another actor
from probs:  [0.08522662372731742, 0.4917337316756438, 0.10186118568939721, 0.07067138201049747, 0.1413682703493369, 0.10913880654780712]
actor:  1 policy actor:  1  step number:  31 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17701999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.08522662372731742, 0.4917337316756438, 0.10186118568939721, 0.07067138201049747, 0.1413682703493369, 0.10913880654780712]
Printing some Q and Qe and total Qs values:  [[0.348]
 [0.389]
 [0.348]
 [0.348]
 [0.348]
 [0.348]
 [0.348]] [[29.054]
 [27.027]
 [29.054]
 [29.054]
 [29.054]
 [29.054]
 [29.054]] [[1.791]
 [1.611]
 [1.791]
 [1.791]
 [1.791]
 [1.791]
 [1.791]]
maxi score, test score, baseline:  0.17701999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.08522662372731742, 0.4917337316756438, 0.10186118568939721, 0.07067138201049747, 0.1413682703493369, 0.10913880654780712]
maxi score, test score, baseline:  0.17701999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.08522662372731742, 0.4917337316756438, 0.10186118568939721, 0.07067138201049747, 0.1413682703493369, 0.10913880654780712]
actor:  1 policy actor:  1  step number:  31 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
from probs:  [0.08522662372731742, 0.4917337316756438, 0.10186118568939721, 0.07067138201049747, 0.1413682703493369, 0.10913880654780712]
maxi score, test score, baseline:  0.17701999999999987 0.6799999999999999 0.6799999999999999
probs:  [0.08522662372731742, 0.4917337316756438, 0.10186118568939721, 0.07067138201049747, 0.1413682703493369, 0.10913880654780712]
Printing some Q and Qe and total Qs values:  [[0.262]
 [0.281]
 [0.26 ]
 [0.134]
 [0.262]
 [0.262]
 [0.262]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.262]
 [0.281]
 [0.26 ]
 [0.134]
 [0.262]
 [0.262]
 [0.262]]
actions average: 
K:  2  action  0 :  tensor([0.9118, 0.0045, 0.0193, 0.0180, 0.0160, 0.0073, 0.0232],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0057,     0.9647,     0.0057,     0.0024,     0.0003,     0.0007,
            0.0206], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0473, 0.0564, 0.6884, 0.0492, 0.0419, 0.0568, 0.0598],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0954, 0.1895, 0.1181, 0.1694, 0.0947, 0.1049, 0.2279],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1347, 0.0969, 0.0623, 0.0657, 0.5167, 0.0583, 0.0655],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([    0.0416,     0.0003,     0.1948,     0.0290,     0.0242,     0.6402,
            0.0699], grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1217, 0.0035, 0.0677, 0.0723, 0.0704, 0.0677, 0.5968],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  1  step number:  40 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  1.333
using another actor
maxi score, test score, baseline:  0.17991999999999983 0.6799999999999999 0.6799999999999999
actor:  0 policy actor:  0  step number:  22 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17987999999999985 0.6799999999999999 0.6799999999999999
probs:  [0.08522662372731742, 0.4917337316756438, 0.10186118568939721, 0.07067138201049747, 0.1413682703493369, 0.10913880654780712]
maxi score, test score, baseline:  0.17987999999999985 0.6799999999999999 0.6799999999999999
probs:  [0.08522662372731742, 0.4917337316756438, 0.10186118568939721, 0.07067138201049747, 0.1413682703493369, 0.10913880654780712]
actor:  0 policy actor:  1  step number:  29 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  35.55553469960361
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.18275999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.08522662372731742, 0.4917337316756438, 0.10186118568939721, 0.07067138201049747, 0.1413682703493369, 0.10913880654780712]
actor:  1 policy actor:  1  step number:  24 total reward:  0.7699999999999999  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  52 total reward:  0.0699999999999994  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  0.0010511979672855887
actor:  1 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 22.34511993451851
maxi score, test score, baseline:  0.18275999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.06948006189463857, 0.48327275662272945, 0.08641275528504626, 0.14038571546697107, 0.12662790208726493, 0.09382080864334977]
actor:  1 policy actor:  1  step number:  35 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  31 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18275999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.06948006189463855, 0.48327275662272934, 0.08641275528504626, 0.1403857154669711, 0.12662790208726493, 0.09382080864334977]
maxi score, test score, baseline:  0.18275999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.06948006189463855, 0.48327275662272934, 0.08641275528504626, 0.1403857154669711, 0.12662790208726493, 0.09382080864334977]
maxi score, test score, baseline:  0.18275999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.06948006189463855, 0.48327275662272934, 0.08641275528504626, 0.1403857154669711, 0.12662790208726493, 0.09382080864334977]
siam score:  -0.7501302
printing an ep nov before normalisation:  25.758044603225965
maxi score, test score, baseline:  0.18275999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.06948006189463855, 0.48327275662272934, 0.08641275528504626, 0.1403857154669711, 0.12662790208726493, 0.09382080864334977]
printing an ep nov before normalisation:  26.21209144592285
printing an ep nov before normalisation:  40.03969668421207
actor:  1 policy actor:  1  step number:  38 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 28.672120888788463
actor:  1 policy actor:  1  step number:  41 total reward:  0.1599999999999996  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  35.332797597921115
actor:  1 policy actor:  1  step number:  26 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18275999999999984 0.6799999999999999 0.6799999999999999
Printing some Q and Qe and total Qs values:  [[0.34 ]
 [0.318]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.34 ]
 [0.318]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]]
maxi score, test score, baseline:  0.18275999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.06948006189463855, 0.48327275662272934, 0.08641275528504626, 0.1403857154669711, 0.12662790208726493, 0.09382080864334977]
printing an ep nov before normalisation:  50.054378013157425
printing an ep nov before normalisation:  32.449543476104736
actor:  1 policy actor:  1  step number:  28 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.18275999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.06948006189463855, 0.48327275662272934, 0.08641275528504626, 0.1403857154669711, 0.12662790208726493, 0.09382080864334977]
actor:  0 policy actor:  1  step number:  33 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  1.333
using another actor
printing an ep nov before normalisation:  40.73247097899958
printing an ep nov before normalisation:  34.751740693250426
maxi score, test score, baseline:  0.17957999999999982 0.6799999999999999 0.6799999999999999
probs:  [0.06948006189463855, 0.48327275662272934, 0.08641275528504626, 0.1403857154669711, 0.12662790208726493, 0.09382080864334977]
maxi score, test score, baseline:  0.17957999999999982 0.6799999999999999 0.6799999999999999
actor:  0 policy actor:  1  step number:  26 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  26 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18275999999999984 0.6799999999999999 0.6799999999999999
probs:  [0.06948006189463855, 0.48327275662272934, 0.08641275528504626, 0.1403857154669711, 0.12662790208726493, 0.09382080864334977]
actor:  0 policy actor:  0  step number:  31 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.18303999999999981 0.6799999999999999 0.6799999999999999
probs:  [0.06948006189463855, 0.48327275662272934, 0.08641275528504626, 0.1403857154669711, 0.12662790208726493, 0.09382080864334977]
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.594609995750496
maxi score, test score, baseline:  0.18303999999999981 0.6799999999999999 0.6799999999999999
probs:  [0.06948006189463855, 0.48327275662272934, 0.08641275528504626, 0.1403857154669711, 0.12662790208726493, 0.09382080864334977]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.469]
 [0.588]
 [0.469]
 [0.469]
 [0.469]
 [0.283]] [[45.036]
 [45.036]
 [44.776]
 [45.036]
 [45.036]
 [45.036]
 [44.655]] [[0.736]
 [0.736]
 [0.852]
 [0.736]
 [0.736]
 [0.736]
 [0.546]]
actor:  1 policy actor:  1  step number:  38 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.18303999999999981 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.18303999999999981 0.6799999999999999 0.6799999999999999
line 256 mcts: sample exp_bonus 29.75074665065103
maxi score, test score, baseline:  0.18303999999999981 0.6799999999999999 0.6799999999999999
probs:  [0.06948006189463857, 0.48327275662272945, 0.08641275528504626, 0.14038571546697107, 0.12662790208726493, 0.09382080864334977]
printing an ep nov before normalisation:  42.04922684218533
printing an ep nov before normalisation:  43.291574373700584
printing an ep nov before normalisation:  37.876181182073346
maxi score, test score, baseline:  0.18303999999999981 0.6799999999999999 0.6799999999999999
probs:  [0.06948006189463855, 0.48327275662272934, 0.08641275528504626, 0.1403857154669711, 0.12662790208726493, 0.09382080864334977]
actor:  1 policy actor:  1  step number:  29 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  27 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.458]
 [0.532]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.458]
 [0.532]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]]
maxi score, test score, baseline:  0.18303999999999981 0.6799999999999999 0.6799999999999999
maxi score, test score, baseline:  0.18303999999999981 0.6799999999999999 0.6799999999999999
probs:  [0.06948006189463855, 0.48327275662272934, 0.08641275528504626, 0.1403857154669711, 0.12662790208726493, 0.09382080864334977]
Starting evaluation
maxi score, test score, baseline:  0.18303999999999981 0.6799999999999999 0.6799999999999999
probs:  [0.06948006189463855, 0.48327275662272934, 0.08641275528504626, 0.1403857154669711, 0.12662790208726493, 0.09382080864334977]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  39.95990442019044
Printing some Q and Qe and total Qs values:  [[0.325]
 [0.547]
 [0.325]
 [0.325]
 [0.325]
 [0.323]
 [0.325]] [[24.732]
 [23.955]
 [24.732]
 [24.732]
 [24.732]
 [30.622]
 [29.557]] [[0.325]
 [0.547]
 [0.325]
 [0.325]
 [0.325]
 [0.323]
 [0.325]]
maxi score, test score, baseline:  0.18303999999999981 0.6799999999999999 0.6799999999999999
probs:  [0.06948006189463855, 0.48327275662272934, 0.08641275528504626, 0.1403857154669711, 0.12662790208726493, 0.09382080864334977]
printing an ep nov before normalisation:  36.49446500443393
printing an ep nov before normalisation:  31.42765536301286
printing an ep nov before normalisation:  37.23629362606484
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  43.138028035990956
printing an ep nov before normalisation:  43.12022517517832
printing an ep nov before normalisation:  37.63937098623199
printing an ep nov before normalisation:  26.728785019058883
siam score:  -0.7527893
printing an ep nov before normalisation:  25.04787445033464
maxi score, test score, baseline:  0.18303999999999981 0.6799999999999999 0.6799999999999999
printing an ep nov before normalisation:  34.168246514790724
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.72  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.72  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.72  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  0.71  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  0.6699999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  32.88061151861579
actor:  0 policy actor:  0  step number:  19 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  19 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  0.0
actor:  1 policy actor:  1  step number:  35 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  29 total reward:  0.72  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  37 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  0.667
using another actor
maxi score, test score, baseline:  0.19541999999999987 0.6805 0.6805
probs:  [0.0699610248958775, 0.4866975085673586, 0.0863517383749005, 0.13914954612900624, 0.12474283515179549, 0.09309734688106167]
maxi score, test score, baseline:  0.19541999999999987 0.6805 0.6805
probs:  [0.0699610248958775, 0.4866975085673586, 0.0863517383749005, 0.13914954612900624, 0.12474283515179549, 0.09309734688106167]
actions average: 
K:  4  action  0 :  tensor([0.5987, 0.0107, 0.0693, 0.0669, 0.1197, 0.0652, 0.0694],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0077,     0.9262,     0.0085,     0.0144,     0.0009,     0.0045,
            0.0380], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1282, 0.0048, 0.3888, 0.1148, 0.1146, 0.1315, 0.1172],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0818, 0.0644, 0.2309, 0.2093, 0.0765, 0.1530, 0.1842],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1061, 0.0008, 0.0391, 0.0535, 0.7116, 0.0432, 0.0457],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0544, 0.0017, 0.2627, 0.0631, 0.0604, 0.5040, 0.0537],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0815, 0.1032, 0.1435, 0.1182, 0.0756, 0.0804, 0.3975],
       grad_fn=<DivBackward0>)
siam score:  -0.75805116
actor:  1 policy actor:  1  step number:  29 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.19541999999999987 0.6805 0.6805
probs:  [0.0699610248958775, 0.4866975085673586, 0.0863517383749005, 0.13914954612900624, 0.12474283515179549, 0.09309734688106167]
actor:  0 policy actor:  1  step number:  31 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  35 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.19477999999999984 0.6805 0.6805
probs:  [0.0699610248958775, 0.4866975085673586, 0.0863517383749005, 0.13914954612900624, 0.12474283515179549, 0.09309734688106167]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.775]
 [0.   ]
 [0.   ]
 [0.775]
 [0.775]
 [0.775]
 [0.775]] [[29.633]
 [ 0.001]
 [ 0.001]
 [29.633]
 [29.633]
 [29.633]
 [29.633]] [[1.442]
 [0.   ]
 [0.   ]
 [1.442]
 [1.442]
 [1.442]
 [1.442]]
actor:  1 policy actor:  1  step number:  25 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]] [[34.739]
 [34.739]
 [34.739]
 [34.739]
 [34.739]
 [34.739]
 [34.739]] [[0.882]
 [0.882]
 [0.882]
 [0.882]
 [0.882]
 [0.882]
 [0.882]]
actor:  1 policy actor:  1  step number:  33 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  3  action  0 :  tensor([0.7095, 0.0028, 0.0714, 0.0606, 0.0568, 0.0514, 0.0475],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0017,     0.9671,     0.0025,     0.0063,     0.0004,     0.0006,
            0.0215], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0978, 0.1238, 0.4150, 0.0905, 0.0796, 0.1068, 0.0865],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1147, 0.0263, 0.1118, 0.3644, 0.1172, 0.1050, 0.1607],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2323, 0.0202, 0.0769, 0.0761, 0.4654, 0.0559, 0.0732],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0710, 0.0181, 0.2623, 0.0581, 0.0432, 0.4761, 0.0711],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0760, 0.0266, 0.0629, 0.0712, 0.0379, 0.0499, 0.6756],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  20 total reward:  0.73  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  28.67539640091804
maxi score, test score, baseline:  0.19477999999999984 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
printing an ep nov before normalisation:  31.319421525336182
maxi score, test score, baseline:  0.19477999999999984 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.999268362382125
maxi score, test score, baseline:  0.19477999999999984 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
actor:  1 policy actor:  1  step number:  33 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  24 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  32 total reward:  0.4299999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.19477999999999984 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
Printing some Q and Qe and total Qs values:  [[0.79]
 [0.79]
 [0.79]
 [0.79]
 [0.79]
 [0.79]
 [0.79]] [[44.425]
 [44.425]
 [44.425]
 [44.425]
 [44.425]
 [44.425]
 [44.425]] [[1.3]
 [1.3]
 [1.3]
 [1.3]
 [1.3]
 [1.3]
 [1.3]]
actor:  1 policy actor:  1  step number:  28 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  29 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  30 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  41.87032222747803
maxi score, test score, baseline:  0.19477999999999984 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
siam score:  -0.7597304
using another actor
from probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
actor:  1 policy actor:  1  step number:  22 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  39.11030212106366
printing an ep nov before normalisation:  37.766269316755086
actor:  1 policy actor:  1  step number:  29 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.19477999999999984 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  25 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  21.790626049041748
actor:  0 policy actor:  1  step number:  28 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.19423999999999986 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
printing an ep nov before normalisation:  34.78727385215864
printing an ep nov before normalisation:  27.586704487641697
maxi score, test score, baseline:  0.19423999999999986 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
Printing some Q and Qe and total Qs values:  [[0.338]
 [0.338]
 [0.416]
 [0.338]
 [0.338]
 [0.338]
 [0.338]] [[34.662]
 [34.662]
 [28.801]
 [34.662]
 [34.662]
 [34.662]
 [34.662]] [[1.671]
 [1.671]
 [1.32 ]
 [1.671]
 [1.671]
 [1.671]
 [1.671]]
printing an ep nov before normalisation:  32.96214031440381
maxi score, test score, baseline:  0.19423999999999986 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
printing an ep nov before normalisation:  34.722056277814865
actor:  1 policy actor:  1  step number:  32 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.19423999999999986 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
maxi score, test score, baseline:  0.19423999999999986 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
printing an ep nov before normalisation:  28.846186599039772
printing an ep nov before normalisation:  27.766179539474543
maxi score, test score, baseline:  0.19423999999999986 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  27 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.19423999999999986 0.6805 0.6805
Printing some Q and Qe and total Qs values:  [[ 0.573]
 [ 0.541]
 [-0.041]
 [ 0.48 ]
 [ 0.503]
 [-0.079]
 [-0.065]] [[31.245]
 [32.796]
 [36.804]
 [32.683]
 [29.405]
 [33.543]
 [37.164]] [[0.895]
 [0.909]
 [0.446]
 [0.844]
 [0.77 ]
 [0.311]
 [0.433]]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  4  action  0 :  tensor([0.3881, 0.0398, 0.1043, 0.1036, 0.1573, 0.1183, 0.0886],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0050, 0.9487, 0.0078, 0.0080, 0.0021, 0.0028, 0.0258],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1183, 0.0342, 0.4146, 0.1261, 0.0986, 0.1194, 0.0888],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0916, 0.0152, 0.2060, 0.4500, 0.0896, 0.0746, 0.0730],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0943, 0.0768, 0.0763, 0.1254, 0.4761, 0.1041, 0.0471],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1355, 0.0191, 0.1578, 0.1511, 0.1537, 0.2703, 0.1126],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0901, 0.0173, 0.0631, 0.0324, 0.0294, 0.0233, 0.7443],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.19423999999999986 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
printing an ep nov before normalisation:  32.334740595917374
maxi score, test score, baseline:  0.19423999999999986 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
printing an ep nov before normalisation:  32.53027530479775
Printing some Q and Qe and total Qs values:  [[0.672]
 [0.716]
 [0.686]
 [0.695]
 [0.665]
 [0.672]
 [0.672]] [[25.5  ]
 [31.325]
 [27.385]
 [27.674]
 [27.124]
 [25.5  ]
 [25.5  ]] [[0.672]
 [0.716]
 [0.686]
 [0.695]
 [0.665]
 [0.672]
 [0.672]]
printing an ep nov before normalisation:  30.617430028119763
printing an ep nov before normalisation:  30.592578928296778
printing an ep nov before normalisation:  40.556983915448484
actor:  0 policy actor:  0  step number:  25 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  37.63548900535112
printing an ep nov before normalisation:  35.66056966471036
maxi score, test score, baseline:  0.18715999999999985 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
printing an ep nov before normalisation:  41.58790104760136
printing an ep nov before normalisation:  40.36638735026763
actor:  1 policy actor:  1  step number:  52 total reward:  0.2699999999999996  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  29.182143211364746
actor:  1 policy actor:  1  step number:  30 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  42 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18715999999999985 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
maxi score, test score, baseline:  0.18715999999999985 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
printing an ep nov before normalisation:  41.44026434190907
printing an ep nov before normalisation:  42.10419377999727
maxi score, test score, baseline:  0.18715999999999985 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
printing an ep nov before normalisation:  30.80159413117914
printing an ep nov before normalisation:  45.75777137950244
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18715999999999985 0.6805 0.6805
actor:  1 policy actor:  1  step number:  27 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.18715999999999985 0.6805 0.6805
actor:  1 policy actor:  1  step number:  39 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  28 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18715999999999985 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
actions average: 
K:  4  action  0 :  tensor([    0.8924,     0.0035,     0.0012,     0.0026,     0.0656,     0.0006,
            0.0342], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0025, 0.9725, 0.0027, 0.0036, 0.0014, 0.0012, 0.0160],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0650, 0.0007, 0.5445, 0.0709, 0.0726, 0.1659, 0.0803],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1929, 0.0046, 0.1851, 0.1514, 0.1430, 0.1441, 0.1789],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1772, 0.0044, 0.1050, 0.1052, 0.3484, 0.1114, 0.1485],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0732, 0.0212, 0.0887, 0.0812, 0.0831, 0.5737, 0.0789],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1231, 0.1662, 0.0950, 0.1652, 0.1140, 0.0949, 0.2416],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  27 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  15.221705436706543
printing an ep nov before normalisation:  28.92133129858234
maxi score, test score, baseline:  0.18715999999999985 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
actor:  1 policy actor:  1  step number:  21 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  27 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.72 ]
 [0.851]
 [0.747]
 [0.747]
 [0.747]
 [0.747]
 [0.747]] [[55.968]
 [40.857]
 [43.773]
 [43.773]
 [43.773]
 [43.773]
 [43.773]] [[0.72 ]
 [0.851]
 [0.747]
 [0.747]
 [0.747]
 [0.747]
 [0.747]]
printing an ep nov before normalisation:  39.11186158657074
maxi score, test score, baseline:  0.18715999999999985 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
printing an ep nov before normalisation:  37.728168959545016
printing an ep nov before normalisation:  51.838313668430004
line 256 mcts: sample exp_bonus 38.221883177480755
line 256 mcts: sample exp_bonus 38.97542103005824
maxi score, test score, baseline:  0.18715999999999985 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
actor:  1 policy actor:  1  step number:  22 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  2  action  0 :  tensor([0.5960, 0.0034, 0.0592, 0.0855, 0.0930, 0.0660, 0.0970],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0046,     0.9626,     0.0034,     0.0099,     0.0007,     0.0008,
            0.0178], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0459,     0.0001,     0.7680,     0.0299,     0.0464,     0.0552,
            0.0545], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1478, 0.0020, 0.1368, 0.3342, 0.1208, 0.1184, 0.1400],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1021, 0.0020, 0.1031, 0.0756, 0.5198, 0.1016, 0.0958],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([    0.1188,     0.0004,     0.1083,     0.0832,     0.0998,     0.4705,
            0.1189], grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1545, 0.0615, 0.0924, 0.0999, 0.1140, 0.0776, 0.4000],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.18715999999999985 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
Printing some Q and Qe and total Qs values:  [[0.943]
 [1.   ]
 [0.943]
 [0.943]
 [0.943]
 [0.943]
 [0.943]] [[18.387]
 [27.137]
 [18.387]
 [18.387]
 [18.387]
 [18.387]
 [18.387]] [[0.943]
 [1.   ]
 [0.943]
 [0.943]
 [0.943]
 [0.943]
 [0.943]]
actor:  0 policy actor:  1  step number:  48 total reward:  0.20999999999999952  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18621999999999983 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
actor:  0 policy actor:  0  step number:  33 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  31.116411670142067
actor:  0 policy actor:  0  step number:  31 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  32 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.18525999999999984 0.6805 0.6805
actor:  1 policy actor:  1  step number:  29 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18525999999999984 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
printing an ep nov before normalisation:  43.43389987945058
printing an ep nov before normalisation:  39.538367504489706
maxi score, test score, baseline:  0.18525999999999984 0.6805 0.6805
actor:  1 policy actor:  1  step number:  22 total reward:  0.6099999999999999  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  48.50904374300335
printing an ep nov before normalisation:  42.94721483244694
printing an ep nov before normalisation:  30.341666043742336
Printing some Q and Qe and total Qs values:  [[0.901]
 [0.901]
 [0.901]
 [0.901]
 [0.901]
 [0.901]
 [0.901]] [[37.052]
 [37.052]
 [37.052]
 [37.052]
 [37.052]
 [37.052]
 [37.052]] [[0.901]
 [0.901]
 [0.901]
 [0.901]
 [0.901]
 [0.901]
 [0.901]]
actor:  1 policy actor:  1  step number:  27 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  25 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  23 total reward:  0.6799999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.18527999999999986 0.6805 0.6805
maxi score, test score, baseline:  0.18527999999999986 0.6805 0.6805
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
siam score:  -0.755912
maxi score, test score, baseline:  0.18527999999999986 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
actor:  1 policy actor:  1  step number:  36 total reward:  0.2899999999999996  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.214]
 [0.226]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.214]
 [0.226]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]]
maxi score, test score, baseline:  0.18527999999999986 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
printing an ep nov before normalisation:  19.818207330316874
Printing some Q and Qe and total Qs values:  [[0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]]
maxi score, test score, baseline:  0.18527999999999986 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18527999999999986 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
actor:  1 policy actor:  1  step number:  33 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  29 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  20.92787197657994
maxi score, test score, baseline:  0.18527999999999986 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
printing an ep nov before normalisation:  33.99571776390076
printing an ep nov before normalisation:  31.839370727539062
actor:  0 policy actor:  1  step number:  39 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  40.66606498368617
actor:  0 policy actor:  0  step number:  25 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  28 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.364628264542254
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  22 total reward:  0.6499999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  36.650381088256836
maxi score, test score, baseline:  0.18439999999999987 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.675]
 [0.631]
 [0.631]
 [0.631]
 [0.631]
 [0.631]] [[33.022]
 [31.078]
 [34.545]
 [34.545]
 [34.545]
 [34.545]
 [34.545]] [[1.878]
 [1.768]
 [1.947]
 [1.947]
 [1.947]
 [1.947]
 [1.947]]
maxi score, test score, baseline:  0.18439999999999987 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
actor:  1 policy actor:  1  step number:  29 total reward:  0.5799999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18439999999999987 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
printing an ep nov before normalisation:  36.521408557891846
actor:  1 policy actor:  1  step number:  34 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  34.54565107544457
actor:  1 policy actor:  1  step number:  33 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18439999999999987 0.6805 0.6805
maxi score, test score, baseline:  0.18439999999999987 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
actor:  1 policy actor:  1  step number:  23 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.18439999999999987 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
actor:  1 policy actor:  1  step number:  35 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  26 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18439999999999987 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.451]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.445]
 [0.451]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]]
maxi score, test score, baseline:  0.18439999999999987 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
maxi score, test score, baseline:  0.18439999999999987 0.6805 0.6805
maxi score, test score, baseline:  0.18439999999999987 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
Printing some Q and Qe and total Qs values:  [[0.734]
 [0.791]
 [0.734]
 [0.734]
 [0.792]
 [0.734]
 [0.791]] [[26.983]
 [14.787]
 [26.983]
 [26.983]
 [16.253]
 [26.983]
 [15.194]] [[1.288]
 [1.095]
 [1.288]
 [1.288]
 [1.125]
 [1.288]
 [1.103]]
actor:  1 policy actor:  1  step number:  27 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  38 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  22 total reward:  0.6299999999999999  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.287]
 [0.287]
 [0.287]
 [0.287]
 [0.287]
 [0.287]
 [0.287]] [[45.527]
 [45.527]
 [45.527]
 [45.527]
 [45.527]
 [45.527]
 [45.527]] [[1.903]
 [1.903]
 [1.903]
 [1.903]
 [1.903]
 [1.903]
 [1.903]]
printing an ep nov before normalisation:  31.191304659975017
Printing some Q and Qe and total Qs values:  [[0.24 ]
 [0.25 ]
 [0.246]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]] [[ 0.   ]
 [22.109]
 [18.089]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.087]
 [0.748]
 [0.626]
 [0.087]
 [0.087]
 [0.087]
 [0.087]]
printing an ep nov before normalisation:  31.31798059688494
maxi score, test score, baseline:  0.18433999999999984 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
actor:  1 policy actor:  1  step number:  24 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  32.64454364776611
Printing some Q and Qe and total Qs values:  [[0.65 ]
 [0.696]
 [0.64 ]
 [0.641]
 [0.641]
 [0.67 ]
 [0.64 ]] [[35.944]
 [39.675]
 [36.457]
 [35.952]
 [35.8  ]
 [34.424]
 [34.706]] [[1.531]
 [1.808]
 [1.552]
 [1.522]
 [1.513]
 [1.456]
 [1.444]]
maxi score, test score, baseline:  0.18433999999999984 0.6805 0.6805
UNIT TEST: sample policy line 217 mcts : [0.184 0.082 0.122 0.082 0.347 0.102 0.082]
printing an ep nov before normalisation:  40.880513722389615
actor:  1 policy actor:  1  step number:  40 total reward:  0.20999999999999952  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18433999999999984 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
actor:  1 policy actor:  1  step number:  37 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18433999999999984 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
maxi score, test score, baseline:  0.18433999999999984 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
actor:  1 policy actor:  1  step number:  33 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.18433999999999984 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
printing an ep nov before normalisation:  31.150732040405273
maxi score, test score, baseline:  0.18433999999999984 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
maxi score, test score, baseline:  0.18433999999999984 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
printing an ep nov before normalisation:  21.39105702587211
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.18433999999999984 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
printing an ep nov before normalisation:  28.592469692230225
maxi score, test score, baseline:  0.18433999999999984 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
actor:  0 policy actor:  0  step number:  33 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  35.394933856500586
actor:  1 policy actor:  1  step number:  36 total reward:  0.48999999999999977  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18421999999999986 0.6805 0.6805
maxi score, test score, baseline:  0.18421999999999986 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
maxi score, test score, baseline:  0.18421999999999986 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
Printing some Q and Qe and total Qs values:  [[0.643]
 [0.695]
 [0.648]
 [0.613]
 [0.603]
 [0.643]
 [0.643]] [[29.887]
 [33.616]
 [28.718]
 [28.66 ]
 [28.934]
 [29.887]
 [29.887]] [[1.032]
 [1.187]
 [1.006]
 [0.969]
 [0.967]
 [1.032]
 [1.032]]
maxi score, test score, baseline:  0.18421999999999986 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
maxi score, test score, baseline:  0.18421999999999986 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
maxi score, test score, baseline:  0.18421999999999986 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.515]
 [0.48 ]
 [0.469]
 [0.466]
 [0.462]
 [0.463]] [[27.559]
 [30.564]
 [29.465]
 [21.699]
 [21.944]
 [22.487]
 [22.018]] [[0.794]
 [0.941]
 [0.869]
 [0.597]
 [0.602]
 [0.616]
 [0.601]]
maxi score, test score, baseline:  0.18421999999999986 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
using explorer policy with actor:  1
actions average: 
K:  4  action  0 :  tensor([0.6410, 0.0026, 0.0489, 0.0707, 0.1105, 0.0704, 0.0560],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0012,     0.9790,     0.0040,     0.0013,     0.0003,     0.0003,
            0.0138], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0748, 0.0737, 0.4028, 0.0824, 0.0851, 0.2089, 0.0722],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1340, 0.0020, 0.1399, 0.1888, 0.2020, 0.1792, 0.1541],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0791, 0.0337, 0.0830, 0.1488, 0.5016, 0.0817, 0.0721],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1150, 0.0009, 0.1220, 0.1594, 0.1671, 0.3222, 0.1133],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1034, 0.1244, 0.1165, 0.1033, 0.0773, 0.0824, 0.3928],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.18421999999999986 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
actor:  1 policy actor:  1  step number:  39 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  49 total reward:  0.0799999999999994  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  36.025738645853764
maxi score, test score, baseline:  0.18421999999999986 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
actor:  0 policy actor:  1  step number:  28 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.369634599127814
printing an ep nov before normalisation:  41.19004224303729
actions average: 
K:  2  action  0 :  tensor([0.5016, 0.0063, 0.0855, 0.1042, 0.1423, 0.0874, 0.0727],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0156, 0.9034, 0.0178, 0.0116, 0.0060, 0.0064, 0.0393],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0120,     0.0005,     0.9229,     0.0114,     0.0117,     0.0305,
            0.0111], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1073, 0.1433, 0.1076, 0.2864, 0.1126, 0.1091, 0.1339],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0921, 0.0007, 0.0731, 0.0692, 0.6350, 0.0733, 0.0567],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0985, 0.0255, 0.2254, 0.1211, 0.0835, 0.3377, 0.1083],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1319, 0.0021, 0.1297, 0.1491, 0.1586, 0.1344, 0.2941],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
siam score:  -0.7533513
actor:  1 policy actor:  1  step number:  32 total reward:  0.4099999999999997  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  31.636034105867303
maxi score, test score, baseline:  0.18387999999999985 0.6805 0.6805
actor:  1 policy actor:  1  step number:  49 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  30 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  0.667
siam score:  -0.75337386
maxi score, test score, baseline:  0.18387999999999985 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
maxi score, test score, baseline:  0.18387999999999985 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
actor:  1 policy actor:  1  step number:  33 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.366]
 [0.376]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]] [[24.203]
 [25.64 ]
 [24.203]
 [24.203]
 [24.203]
 [24.203]
 [24.203]] [[1.426]
 [1.556]
 [1.426]
 [1.426]
 [1.426]
 [1.426]
 [1.426]]
maxi score, test score, baseline:  0.18387999999999985 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
printing an ep nov before normalisation:  23.272448742856643
maxi score, test score, baseline:  0.18387999999999985 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
printing an ep nov before normalisation:  43.31814160568975
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18387999999999985 0.6805 0.6805
maxi score, test score, baseline:  0.18387999999999985 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
printing an ep nov before normalisation:  27.082867116328014
printing an ep nov before normalisation:  33.55691148325715
printing an ep nov before normalisation:  32.527165374117615
maxi score, test score, baseline:  0.18387999999999985 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
actor:  0 policy actor:  0  step number:  21 total reward:  0.5999999999999999  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  0.0015170793687957485
actor:  1 policy actor:  1  step number:  45 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.18707999999999986 0.6805 0.6805
maxi score, test score, baseline:  0.18707999999999986 0.6805 0.6805
maxi score, test score, baseline:  0.18707999999999986 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
Printing some Q and Qe and total Qs values:  [[0.284]
 [0.294]
 [0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]] [[25.949]
 [30.462]
 [25.949]
 [25.949]
 [25.949]
 [25.949]
 [25.949]] [[0.682]
 [0.867]
 [0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.682]]
maxi score, test score, baseline:  0.18707999999999986 0.6805 0.6805
printing an ep nov before normalisation:  35.14478269121776
Printing some Q and Qe and total Qs values:  [[0.941]
 [0.193]
 [0.869]
 [0.869]
 [0.869]
 [0.869]
 [0.869]] [[36.672]
 [35.959]
 [27.061]
 [27.061]
 [27.061]
 [27.061]
 [27.061]] [[0.941]
 [0.193]
 [0.869]
 [0.869]
 [0.869]
 [0.869]
 [0.869]]
maxi score, test score, baseline:  0.1838399999999999 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
printing an ep nov before normalisation:  29.23331308679052
printing an ep nov before normalisation:  38.863009183670485
actor:  1 policy actor:  1  step number:  37 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  45 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  35.927888457380234
maxi score, test score, baseline:  0.1838399999999999 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
printing an ep nov before normalisation:  36.772593758856885
actor:  1 policy actor:  1  step number:  31 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  32 total reward:  0.5499999999999999  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  38 total reward:  0.22999999999999954  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18319999999999984 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
actor:  1 policy actor:  1  step number:  34 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.672]
 [0.575]
 [0.575]
 [0.575]
 [0.528]
 [0.575]
 [0.575]] [[36.464]
 [31.593]
 [31.593]
 [31.593]
 [40.98 ]
 [31.593]
 [31.593]] [[0.879]
 [0.727]
 [0.727]
 [0.727]
 [0.785]
 [0.727]
 [0.727]]
Printing some Q and Qe and total Qs values:  [[0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.376]] [[37.031]
 [37.031]
 [37.031]
 [37.031]
 [37.031]
 [37.031]
 [37.031]] [[1.707]
 [1.707]
 [1.707]
 [1.707]
 [1.707]
 [1.707]
 [1.707]]
from probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.18009999999999984 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
printing an ep nov before normalisation:  36.73518456557769
maxi score, test score, baseline:  0.18009999999999984 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
printing an ep nov before normalisation:  43.23955601103195
maxi score, test score, baseline:  0.18009999999999984 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
actor:  1 policy actor:  1  step number:  28 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18009999999999984 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
actor:  0 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.641]
 [0.446]
 [0.446]
 [0.446]
 [0.446]
 [0.446]] [[45.127]
 [34.938]
 [47.721]
 [47.721]
 [47.721]
 [47.721]
 [47.721]] [[1.701]
 [1.343]
 [1.709]
 [1.709]
 [1.709]
 [1.709]
 [1.709]]
actor:  1 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
siam score:  -0.7572392
maxi score, test score, baseline:  0.18055999999999986 0.6805 0.6805
maxi score, test score, baseline:  0.18055999999999986 0.6805 0.6805
probs:  [0.06822874916015206, 0.47462690743624436, 0.08421284470182575, 0.1357008537267292, 0.14643953563653098, 0.09079110933851761]
actor:  1 policy actor:  1  step number:  31 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  0.001910262652131678
actor:  1 policy actor:  1  step number:  32 total reward:  0.3099999999999996  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  50 total reward:  0.24999999999999956  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  47.60002412355376
siam score:  -0.7530446
actor:  1 policy actor:  1  step number:  32 total reward:  0.5299999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  22 total reward:  0.73  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  31.94898707857111
printing an ep nov before normalisation:  37.09595490766178
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.18055999999999986 0.6805 0.6805
probs:  [0.06658034083992512, 0.46314069948197933, 0.08217750468403062, 0.15660707183187525, 0.14289785582649459, 0.08859652733569495]
line 256 mcts: sample exp_bonus 30.605283919516815
printing an ep nov before normalisation:  27.990847103670923
actor:  1 policy actor:  1  step number:  21 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18055999999999986 0.6805 0.6805
probs:  [0.06658034083992512, 0.46314069948197933, 0.08217750468403062, 0.15660707183187525, 0.14289785582649459, 0.08859652733569495]
printing an ep nov before normalisation:  24.013793468475342
printing an ep nov before normalisation:  41.19242754077512
actor:  1 policy actor:  1  step number:  38 total reward:  0.3899999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18055999999999986 0.6805 0.6805
probs:  [0.06658034083992512, 0.46314069948197933, 0.08217750468403062, 0.15660707183187525, 0.14289785582649459, 0.08859652733569495]
actor:  1 policy actor:  1  step number:  28 total reward:  0.69  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  28.499105070088067
printing an ep nov before normalisation:  42.1818376554643
actor:  1 policy actor:  1  step number:  30 total reward:  0.4299999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18055999999999986 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.11082035813936
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  28 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  2  action  0 :  tensor([0.4449, 0.0136, 0.0911, 0.0836, 0.1577, 0.0609, 0.1482],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0047,     0.9530,     0.0030,     0.0160,     0.0016,     0.0008,
            0.0208], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0649, 0.0059, 0.6994, 0.0705, 0.0482, 0.0528, 0.0583],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1307, 0.0288, 0.1318, 0.3876, 0.1160, 0.0841, 0.1211],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.1917,     0.0002,     0.0620,     0.0778,     0.5383,     0.0498,
            0.0802], grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0994, 0.0013, 0.1441, 0.1190, 0.0987, 0.4691, 0.0685],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0953, 0.0415, 0.0612, 0.4627, 0.1113, 0.0785, 0.1495],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.18055999999999986 0.6805 0.6805
actor:  1 policy actor:  1  step number:  34 total reward:  0.44999999999999973  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]] [[41.588]
 [41.588]
 [41.588]
 [41.588]
 [41.588]
 [41.588]
 [41.588]] [[0.995]
 [0.995]
 [0.995]
 [0.995]
 [0.995]
 [0.995]
 [0.995]]
maxi score, test score, baseline:  0.18055999999999986 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
using explorer policy with actor:  1
siam score:  -0.74843186
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  22 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.18055999999999986 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
Printing some Q and Qe and total Qs values:  [[0.737]
 [0.786]
 [0.747]
 [0.737]
 [0.737]
 [0.736]
 [0.735]] [[33.999]
 [35.295]
 [34.117]
 [34.07 ]
 [33.923]
 [34.184]
 [33.719]] [[1.339]
 [1.41 ]
 [1.351]
 [1.339]
 [1.337]
 [1.341]
 [1.332]]
actor:  1 policy actor:  1  step number:  37 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  22.03902244567871
actor:  1 policy actor:  1  step number:  50 total reward:  0.0699999999999994  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.18055999999999986 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
maxi score, test score, baseline:  0.18055999999999986 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
actor:  0 policy actor:  0  step number:  23 total reward:  0.5199999999999998  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  24 total reward:  0.5699999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.18359999999999982 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
actor:  1 policy actor:  1  step number:  30 total reward:  0.36999999999999966  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  55.49157283866032
printing an ep nov before normalisation:  47.48250920734296
actor:  1 policy actor:  1  step number:  27 total reward:  0.4999999999999998  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  45.79580640747356
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.577]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.44 ]] [[27.396]
 [33.695]
 [30.679]
 [30.679]
 [30.679]
 [30.679]
 [28.014]] [[0.957]
 [1.376]
 [1.159]
 [1.159]
 [1.159]
 [1.159]
 [0.98 ]]
maxi score, test score, baseline:  0.18359999999999982 0.6805 0.6805
maxi score, test score, baseline:  0.18359999999999982 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
maxi score, test score, baseline:  0.18359999999999982 0.6805 0.6805
maxi score, test score, baseline:  0.18359999999999982 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
maxi score, test score, baseline:  0.18359999999999982 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
actor:  1 policy actor:  1  step number:  25 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  0.8528348235742556
actor:  1 policy actor:  1  step number:  44 total reward:  0.2699999999999996  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  3  action  0 :  tensor([0.4939, 0.0075, 0.1050, 0.1147, 0.0759, 0.0887, 0.1144],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0095, 0.9265, 0.0128, 0.0035, 0.0015, 0.0157, 0.0305],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0364,     0.0007,     0.7433,     0.0407,     0.0441,     0.0959,
            0.0389], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0572, 0.0097, 0.0951, 0.6076, 0.0786, 0.0610, 0.0906],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0977, 0.0050, 0.1365, 0.1167, 0.4313, 0.1122, 0.1006],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0629, 0.0012, 0.1601, 0.1099, 0.0751, 0.5118, 0.0791],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1702, 0.0359, 0.1075, 0.1224, 0.0840, 0.0932, 0.3869],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  40.64296192067268
printing an ep nov before normalisation:  43.659296177860924
printing an ep nov before normalisation:  28.589318068543786
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.499]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]] [[34.856]
 [34.453]
 [34.856]
 [34.856]
 [34.856]
 [34.856]
 [34.856]] [[0.758]
 [0.844]
 [0.758]
 [0.758]
 [0.758]
 [0.758]
 [0.758]]
using explorer policy with actor:  1
from probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
actor:  1 policy actor:  1  step number:  41 total reward:  0.19999999999999962  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18359999999999982 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
maxi score, test score, baseline:  0.18359999999999982 0.6805 0.6805
maxi score, test score, baseline:  0.18359999999999982 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
Printing some Q and Qe and total Qs values:  [[0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]] [[38.861]
 [38.861]
 [38.861]
 [38.861]
 [38.861]
 [38.861]
 [38.861]] [[0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]]
maxi score, test score, baseline:  0.18359999999999982 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]] [[52.455]
 [52.455]
 [52.455]
 [52.455]
 [52.455]
 [52.455]
 [52.455]] [[0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]]
maxi score, test score, baseline:  0.18359999999999982 0.6805 0.6805
actor:  0 policy actor:  0  step number:  22 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  1.667
from probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
maxi score, test score, baseline:  0.18677999999999984 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
printing an ep nov before normalisation:  39.828295259768424
maxi score, test score, baseline:  0.18677999999999984 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
actor:  0 policy actor:  1  step number:  27 total reward:  0.5599999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  43.12725882797771
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
actor:  1 policy actor:  1  step number:  32 total reward:  0.34999999999999964  reward:  1.0 rdn_beta:  2.0
siam score:  -0.75499326
printing an ep nov before normalisation:  0.060975643214078445
actor:  0 policy actor:  0  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18697999999999984 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
actor:  0 policy actor:  0  step number:  21 total reward:  0.6399999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18691999999999984 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  31 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.667
from probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
maxi score, test score, baseline:  0.18691999999999984 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
printing an ep nov before normalisation:  33.49811388678106
maxi score, test score, baseline:  0.18691999999999984 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
actor:  0 policy actor:  0  step number:  22 total reward:  0.5899999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18717999999999982 0.6805 0.6805
Printing some Q and Qe and total Qs values:  [[0.821]
 [0.821]
 [0.891]
 [0.705]
 [0.448]
 [0.821]
 [0.765]] [[25.752]
 [25.752]
 [30.802]
 [33.067]
 [34.626]
 [25.752]
 [38.941]] [[0.821]
 [0.821]
 [0.891]
 [0.705]
 [0.448]
 [0.821]
 [0.765]]
from probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  10901
maxi score, test score, baseline:  0.18717999999999982 0.6805 0.6805
printing an ep nov before normalisation:  56.40074650148176
actor:  0 policy actor:  0  step number:  32 total reward:  0.3299999999999996  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  30.521108384704664
maxi score, test score, baseline:  0.18373999999999982 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
printing an ep nov before normalisation:  36.11857421271127
maxi score, test score, baseline:  0.18373999999999982 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
maxi score, test score, baseline:  0.18373999999999982 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
maxi score, test score, baseline:  0.18373999999999982 0.6805 0.6805
siam score:  -0.7521416
maxi score, test score, baseline:  0.18373999999999982 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
printing an ep nov before normalisation:  29.22582612332966
maxi score, test score, baseline:  0.18373999999999982 0.6805 0.6805
maxi score, test score, baseline:  0.18373999999999982 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
actor:  1 policy actor:  1  step number:  30 total reward:  0.5099999999999998  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  38.92740948750758
actor:  1 policy actor:  1  step number:  45 total reward:  0.01999999999999935  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.408]
 [0.335]
 [0.329]
 [0.323]
 [0.358]
 [0.359]] [[55.302]
 [45.792]
 [53.213]
 [53.211]
 [51.675]
 [51.629]
 [53.086]] [[1.66 ]
 [1.366]
 [1.586]
 [1.579]
 [1.513]
 [1.546]
 [1.605]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18373999999999982 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18373999999999982 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
actor:  1 policy actor:  1  step number:  26 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  28 total reward:  0.5499999999999998  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.06 ]
 [-0.026]
 [-0.058]
 [-0.065]
 [-0.062]
 [-0.059]
 [-0.04 ]] [[35.432]
 [36.7  ]
 [35.537]
 [34.855]
 [35.678]
 [34.508]
 [35.423]] [[0.409]
 [0.477]
 [0.413]
 [0.389]
 [0.414]
 [0.385]
 [0.428]]
printing an ep nov before normalisation:  33.584583379022504
maxi score, test score, baseline:  0.18373999999999982 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
printing an ep nov before normalisation:  32.90865122698338
maxi score, test score, baseline:  0.18373999999999982 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
maxi score, test score, baseline:  0.18065999999999982 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
Printing some Q and Qe and total Qs values:  [[0.343]
 [0.388]
 [0.253]
 [0.248]
 [0.275]
 [0.246]
 [0.247]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.343]
 [0.388]
 [0.253]
 [0.248]
 [0.275]
 [0.246]
 [0.247]]
actor:  0 policy actor:  0  step number:  29 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.18013999999999983 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
actor:  0 policy actor:  0  step number:  25 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.1800799999999999 0.6805 0.6805
maxi score, test score, baseline:  0.1800799999999999 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
maxi score, test score, baseline:  0.1800799999999999 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
siam score:  -0.75349265
maxi score, test score, baseline:  0.17689999999999984 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
printing an ep nov before normalisation:  39.46406509236834
actor:  1 policy actor:  1  step number:  44 total reward:  0.02999999999999936  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  33 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]
 [0.596]] [[40.779]
 [40.779]
 [40.779]
 [40.779]
 [40.779]
 [40.779]
 [40.779]] [[1.837]
 [1.837]
 [1.837]
 [1.837]
 [1.837]
 [1.837]
 [1.837]]
maxi score, test score, baseline:  0.17965999999999985 0.6805 0.6805
printing an ep nov before normalisation:  27.924836269167052
actor:  0 policy actor:  1  step number:  19 total reward:  0.6599999999999999  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  35 total reward:  0.5399999999999998  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.63141312565413
maxi score, test score, baseline:  0.18297999999999986 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
maxi score, test score, baseline:  0.18297999999999986 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
printing an ep nov before normalisation:  35.38567389121623
actor:  1 policy actor:  1  step number:  41 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.325]
 [0.401]
 [0.295]
 [0.298]
 [0.242]
 [0.237]
 [0.241]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.325]
 [0.401]
 [0.295]
 [0.298]
 [0.242]
 [0.237]
 [0.241]]
maxi score, test score, baseline:  0.18297999999999986 0.6805 0.6805
maxi score, test score, baseline:  0.18297999999999986 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
maxi score, test score, baseline:  0.18297999999999986 0.6805 0.6805
probs:  [0.06652114759856265, 0.46272823742908054, 0.08210441698394465, 0.1564676798627001, 0.14366079676338472, 0.08851772136232738]
actor:  1 policy actor:  1  step number:  35 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  35.786572047981615
actor:  1 policy actor:  1  step number:  41 total reward:  0.0799999999999994  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  57.30107702916062
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  21 total reward:  0.6199999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.18297999999999986 0.6805 0.6805
actor:  1 policy actor:  1  step number:  30 total reward:  0.46999999999999975  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  45.365669152259834
